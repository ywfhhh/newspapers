<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2025/08/27报纸</title>
    <link href="/2025/08/2025-08-27_article/"/>
    <url>/2025/08/2025-08-27_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2025/08/27报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-27<br>📄 <strong>发现论文数量</strong>：256  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Solving-the-Min-Max-Multiple-Traveling-Salesmen-Problem-via-Learning-Based-Path-Generation-and-Optimal-Splittingpdf"><a href="#1-Solving-the-Min-Max-Multiple-Traveling-Salesmen-Problem-via-Learning-Based-Path-Generation-and-Optimal-Splittingpdf" class="headerlink" title="1. Solving the Min-Max Multiple Traveling Salesmen Problem via Learning-Based Path Generation and Optimal Splittingpdf"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Solving_the_Min-Max_Multiple_Traveling_Salesmen_Problem_via_Learning-Based_Path_Generation_and_Optim.pdf">Solving the Min-Max Multiple Traveling Salesmen Problem via Learning-Based Path Generation and Optimal Splitting</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本论文提出了Generate-and-Split (GaS) 框架解决最小化最大路径长度的多旅行商问题（m3-TSP）。方法上，采用强化学习（RL）驱动的LSTM增强路径生成器联合最优分割算法，先生成全局路径再高效分割，解决了部分可观测性和分割一致性问题。实验结果显示，GaS在解质量和泛化能力上显著优于现有学习方法，尤其在分布外测试中表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Solving_the_Min-Max_Multiple_Traveling_Salesmen_Problem_via_Learning-Based_Path_Generation_and_Optimal_Splitting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Reinforcement-Guided-Hyper-Heuristic-Hyperparameter-Optimization-for-Fair-and-Explainable-Spiking-Neural-Network-Based-Financial-Fraud-Detectionpdf"><a href="#2-Reinforcement-Guided-Hyper-Heuristic-Hyperparameter-Optimization-for-Fair-and-Explainable-Spiking-Neural-Network-Based-Financial-Fraud-Detectionpdf" class="headerlink" title="2. Reinforcement-Guided Hyper-Heuristic Hyperparameter Optimization for Fair and Explainable Spiking Neural Network-Based Financial Fraud Detectionpdf"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Reinforcement-Guided_Hyper-Heuristic_Hyperparameter_Optimization_for_Fair_and_Explainable_Spiking_Ne.pdf">Reinforcement-Guided Hyper-Heuristic Hyperparameter Optimization for Fair and Explainable Spiking Neural Network-Based Financial Fraud Detection</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Department of Computer Science &amp; Engineering, Khulna University of Engineering &amp; Technology (KUET)</span></p><p>本文提出一种融合生物启发的皮层脉冲神经网络（CSNPC）与强化学习引导的超启发式优化器（RHOSS）的金融欺诈检测方法。方法通过Q学习动态选择低层启发式进行SNN超参数优化，并结合可解释性AI技术（如显著性归因和脉冲活动分析）提升模型透明度和公平性。实验结果表明，该方法在严格的5%误报率下召回率达90.8%，且在客户年龄、收入和就业等敏感属性上实现98%以上的预测平等，优于主流脉冲与非脉冲模型，具备高性能及伦理对齐。结论：融合人口编码SNN与强化学习超启发式优化可实现公平、可解释且高效的金融欺诈检测。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Reinforcement-Guided_Hyper-Heuristic_Hyperparameter_Optimization_for_Fair_and_Explainable_Spiking_Neural_Network-Based_Financial_Fraud_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Dream-to-Chat-Model-based-Reinforcement-Learning-on-Dialogues-with-User-Belief-Modelingpdf"><a href="#3-Dream-to-Chat-Model-based-Reinforcement-Learning-on-Dialogues-with-User-Belief-Modelingpdf" class="headerlink" title="3. Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modelingpdf"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dream_to_Chat__Model-based_Reinforcement_Learning_on_Dialogues_with_User_Belief_Modeling.pdf">Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Geely AI Lab</span></p><p>本文提出DreamCUB框架，将基于模型的强化学习（Model-based RL, MBRL）与用户信念建模（包括情感、情绪和意图识别）结合，专为对话系统设计。方法包括预训练对话世界模型以预测用户情绪及未来对话轨迹，并通过POMDP理论推导，联合策略、评论家与世界模型进行训练。实验结果表明，该方法在情感识别和对话生成质量上优于现有方法，并表现出良好的领域泛化能力和探索-利用平衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Dream_to_Chat_Model-based_Reinforcement_Learning_on_Dialogues_with_User_Belief_Modeling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-ReviBranch-Deep-Reinforcement-Learning-for-Branch-and-Bound-with-Revived-Trajectoriespdf"><a href="#4-ReviBranch-Deep-Reinforcement-Learning-for-Branch-and-Bound-with-Revived-Trajectoriespdf" class="headerlink" title="4. ReviBranch: Deep Reinforcement Learning for Branch-and-Bound with Revived Trajectoriespdf"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ReviBranch__Deep_Reinforcement_Learning_for_Branch-and-Bound_with_Revived_Trajectories.pdf">ReviBranch: Deep Reinforcement Learning for Branch-and-Bound with Revived Trajectories</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong Baptist University</span></p><p>该论文提出ReviBranch深度强化学习框架，通过Encoder-Revival-Decoder架构结合BipartiteGCN图结构建模和多方向注意力机制，创新性地利用“复活轨迹”关联分支决策与动态图状态，并采用重要性加权奖励重分布，解决分支定界（B&amp;B）中长期依赖、稀疏奖励和状态动态变化等难题。实验证明，ReviBranch在多个MILP基准上优于最先进RL方法，复杂实例上B&amp;B节点减少4.0%、LP迭代减少2.2%，展现出良好的泛化能力和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ReviBranch_Deep_Reinforcement_Learning_for_Branch-and-Bound_with_Revived_Trajectories.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-Rectified-Robust-Policy-Optimization-for-Model-Uncertain-Constrained-Reinforcement-Learning-without-Strong-Dualitypdf"><a href="#5-Rectified-Robust-Policy-Optimization-for-Model-Uncertain-Constrained-Reinforcement-Learning-without-Strong-Dualitypdf" class="headerlink" title="5. Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Dualitypdf"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Rectified_Robust_Policy_Optimization_for_Model-Uncertain_Constrained_Reinforcement_Learning_without_.pdf">Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Maryland, College Park</span></p><p>本文发现，在存在模型不确定性的约束强化学习（RL）问题下，强对偶性一般不成立，导致传统的原始-对偶方法难以保证最优可行策略。为此，作者提出了只依赖原问题的Rectified Robust Policy Optimization（RRPO）算法，无需对偶变量，直接在原始问题上优化策略，并在一般正则性假设下给出了收敛性理论保证。实验表明，RRPO在典型环境下可在模型不确定性下获得鲁棒且安全的策略，优于非鲁棒方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Rectified_Robust_Policy_Optimization_for_Model-Uncertain_Constrained_Reinforcement_Learning_without_Strong_Duality.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-MetaFed-Advancing-Privacy-Performance-and-Sustainability-in-Federated-Metaverse-Systemspdf"><a href="#6-MetaFed-Advancing-Privacy-Performance-and-Sustainability-in-Federated-Metaverse-Systemspdf" class="headerlink" title="6. MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systemspdf"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MetaFed__Advancing_Privacy,_Performance,_and_Sustainability_in_Federated_Metaverse_Systems.pdf">MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kırıkkale University</span></p><p>本文提出了MetaFed框架，通过集成多智能体强化学习（MARL）动态客户端选择、同态加密的隐私保护联邦学习以及碳感知调度，有效实现了Metaverse系统中的资源智能编排。实验结果显示，MetaFed相比传统方法在MNIST和CIFAR-10数据集上可减少约20-25%的碳排放，同时保持高准确率和低通信延迟，验证了其在隐私保护与可持续性方面的优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MetaFed_Advancing_Privacy_Performance_and_Sustainability_in_Federated_Metaverse_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Federated-Reinforcement-Learning-for-Runtime-Optimization-of-AI-Applications-in-Smart-Eyewearspdf"><a href="#7-Federated-Reinforcement-Learning-for-Runtime-Optimization-of-AI-Applications-in-Smart-Eyewearspdf" class="headerlink" title="7. Federated Reinforcement Learning for Runtime Optimization of AI Applications in Smart Eyewearspdf"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Federated_Reinforcement_Learning_for_Runtime_Optimization_of_AI_Applications_in_Smart_Eyewears.pdf">Federated Reinforcement Learning for Runtime Optimization of AI Applications in Smart Eyewears</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Politecnico di Milano</span></p><p>本论文提出了一种结合三层DNN分区和联邦强化学习（FRL）的方法，实现智能眼镜设备在动态网络和云延迟环境下的AI任务自适应卸载。通过同步和异步联邦策略，多智能体在保持数据隐私的前提下协同学习，显著降低了性能波动，提高了系统稳定性和泛化能力。实验结果显示，FRL方法在多智能体协作下能够在不加速收敛的前提下，获得更稳定和可靠的策略，适用于实时目标检测等场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Federated_Reinforcement_Learning_for_Runtime_Optimization_of_AI_Applications_in_Smart_Eyewears.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-GPG-HT-Generalized-Policy-Gradient-with-History-Aware-Decision-Transformer-for-Probabilistic-Path-Planningpdf"><a href="#8-GPG-HT-Generalized-Policy-Gradient-with-History-Aware-Decision-Transformer-for-Probabilistic-Path-Planningpdf" class="headerlink" title="8. GPG-HT: Generalized Policy Gradient with History-Aware Decision Transformer for Probabilistic Path Planningpdf"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GPG-HT__Generalized_Policy_Gradient_with_History-Aware_Decision_Transformer_for_Probabilistic_Path_P.pdf">GPG-HT: Generalized Policy Gradient with History-Aware Decision Transformer for Probabilistic Path Planning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sichuan University</span></p><p>该论文提出GPG-HT框架，将历史感知决策Transformer与广义策略梯度相结合，针对交通网络中的概率路径规划SOTA问题。方法利用Transformer建模长程轨迹依赖，通过广义策略梯度以非马尔科夫目标进行优化，实现基于历史信息的路径决策。实验表明，GPG-HT在Sioux Falls Network等基准上显著提升了到点概率与决策效率，优于现有RL与DP方法，具有高准确性和实时性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GPG-HT_Generalized_Policy_Gradient_with_History-Aware_Decision_Transformer_for_Probabilistic_Path_Planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-Reinforcement-Learning-enhanced-Online-Adaptive-Clinical-Decision-Support-via-Digital-Twin-powered-Policy-and-Treatment-Effect-optimized-Rewardpdf"><a href="#9-Reinforcement-Learning-enhanced-Online-Adaptive-Clinical-Decision-Support-via-Digital-Twin-powered-Policy-and-Treatment-Effect-optimized-Rewardpdf" class="headerlink" title="9. Reinforcement Learning enhanced Online Adaptive Clinical Decision Support via Digital Twin powered Policy and Treatment Effect optimized Rewardpdf"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Reinforcement_Learning_enhanced_Online_Adaptive_Clinical_Decision_Support_via_Digital_Twin_powered_P.pdf">Reinforcement Learning enhanced Online Adaptive Clinical Decision Support via Digital Twin powered Policy and Treatment Effect optimized Reward</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Houston</span></p><p>本文提出了一种结合强化学习（RL）、数字孪生（DT）和治疗效应（TE）优化奖励的临床决策支持系统。方法包括：离线阶段利用批约束Q学习（BCQ）从回顾性数据中训练策略，建立患者数字孪生环境模拟与结果模型，在线阶段通过Q网络集成和主动不确定性采样进行决策，并在高不确定性时触发专家查询，始终通过规则安全门控保证安全。结论：在合成临床仿真中，该系统具有低延迟、高稳定性和安全性，减少了专家标注需求，并显著优于常规值函数基线，展现了从离线建模到交互式临床部署的可行路径。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Reinforcement_Learning_enhanced_Online_Adaptive_Clinical_Decision_Support_via_Digital_Twin_powered_Policy_and_Treatment_Effect_optimized_Reward.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-ANO-Faster-is-Better-in-Noisy-Landscapespdf"><a href="#10-ANO-Faster-is-Better-in-Noisy-Landscapespdf" class="headerlink" title="10. ANO : Faster is Better in Noisy Landscapespdf"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ANO___Faster_is_Better_in_Noisy_Landscape.pdf">ANO : Faster is Better in Noisy Landscapes</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent Researcher</span></p><p>本文提出了一种新的优化器Ano，通过将梯度方向和平滑（动量）与步长（由当前梯度模决定）解耦，提升在高噪声和非平稳环境下的鲁棒性，同时保持一阶优化算法的高效性。理论证明其在非凸问题下收敛速度与主流符号优化器相当，实验证明在深度强化学习和低资源NLP任务中显著优于Adam等基线，在常规视觉任务中也具备竞争力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ANO_Faster_is_Better_in_Noisy_Landscapes.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generationpdf"><a href="#11-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generationpdf" class="headerlink" title="11. Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generationpdf"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Visual-CoG__Stage-Aware_Reinforcement_Learning_with_Chain_of_Guidance_for_Text-to-Image_Generation.pdf">Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Alibaba Group</span></p><p>该论文提出了一种用于文本到图像生成的三阶段强化学习框架Visual-CoG，包括语义推理、过程细化和结果评估，每一阶段都提供即时奖励指导生成流程。通过在GenEval、T2I-CompBench和自建VisCog-Bench上的实验，验证了该方法在多属性和歧义性提示下能大幅提升生成图像的语义一致性和推理能力，显著优于已有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Visual-CoG_Stage-Aware_Reinforcement_Learning_with_Chain_of_Guidance_for_Text-to-Image_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-Multi-layer-Abstraction-for-Nested-Generation-of-Options-MANGO-in-Hierarchical-Reinforcement-Learningpdf"><a href="#12-Multi-layer-Abstraction-for-Nested-Generation-of-Options-MANGO-in-Hierarchical-Reinforcement-Learningpdf" class="headerlink" title="12. Multi-layer Abstraction for Nested Generation of Options (MANGO) in Hierarchical Reinforcement Learningpdf"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multi-layer_Abstraction_for_Nested_Generation_of_Options_(MANGO)_in_Hierarchical_Reinforcement_Learn.pdf">Multi-layer Abstraction for Nested Generation of Options (MANGO) in Hierarchical Reinforcement Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Human Inspired Technology Research Center, Università di Padova</span></p><p>本文提出了MANGO框架，通过多层抽象将复杂稀疏奖励环境分解为嵌套的选项（options），每层定义抽象状态空间和选项，选项可在不同层间复用，提升样本效率和泛化能力。实验证明，MANGO在程序生成的网格环境中较标准RL方法有更高样本效率与更好的泛化能力，同时增强了决策过程的可解释性，适用于安全和工业场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Multi-layer_Abstraction_for_Nested_Generation_of_Options_%28MANGO%29_in_Hierarchical_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-HERO-Hierarchical-Extrapolation-and-Refresh-for-Efficient-World-Modelspdf"><a href="#13-HERO-Hierarchical-Extrapolation-and-Refresh-for-Efficient-World-Modelspdf" class="headerlink" title="13. HERO: Hierarchical Extrapolation and Refresh for Efficient World Modelspdf"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HERO__Hierarchical_Extrapolation_and_Refresh_for_Efficient_World_Models.pdf">HERO: Hierarchical Extrapolation and Refresh for Efficient World Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了一种用于高效世界模型的无训练分层加速框架HERO。方法上，HERO针对世界模型中的多模态特性，采用层次化策略：浅层采用Patch-wise Refresh动态选择token重计算，深层则用线性外推直接估算中间特征，避免冗余计算。实验表明，HERO在视觉规划和重建任务上实现了1.73倍加速且几乎无性能损失，优于现有扩散模型加速方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/HERO_Hierarchical_Extrapolation_and_Refresh_for_Efficient_World_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Consciousness-as-a-Functorpdf"><a href="#14-Consciousness-as-a-Functorpdf" class="headerlink" title="14. Consciousness as a Functorpdf"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Consciousness_as_a_Functor.pdf">Consciousness as a Functor</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Adobe Research</span></p><p>该论文提出将意识建模为范函子（Functor），通过范畴论和煤代数理论描述意识与无意识过程的信息流转，并用Universal Reinforcement Learning (URL)框架模拟意识短期记忆到长期无意识记忆的传递，以及用网络经济模型模拟无意识到意识的竞争过程。结论认为该理论为人工智能系统中意识功能提供了统一的理论基础，强调异步分布计算和范畴结构可推广于智能决策系统。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Consciousness_as_a_Functor.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="15-M3DMap-Object-aware-Multimodal-3D-Mapping-for-Dynamic-Environmentspdf"><a href="#15-M3DMap-Object-aware-Multimodal-3D-Mapping-for-Dynamic-Environmentspdf" class="headerlink" title="15. M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environmentspdf"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/M3DMap__Object-aware_Multimodal_3D_Mapping_for_Dynamic_Environments.pdf">M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Moscow Institute of Physics and Technology</span></p><p>本文提出了M3DMap，一种面向动态环境下对象感知的多模态3D建图方法，模块化集成了神经网络对象分割与跟踪、可训练里程计估计、3D地图构建与更新以及多模态数据检索模块，支持多种传感器数据（图像、点云、文本）融合。实验和理论分析表明，融合多模态信息能显著提升目标识别、定位、问答、机器人操控等下游任务的精度，但对实时性的同步与模块协调提出挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/M3DMap_Object-aware_Multimodal_3D_Mapping_for_Dynamic_Environments.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-A-Rapid-Iterative-Trajectory-Planning-Method-for-Automated-Parking-through-Differential-Flatnesspdf"><a href="#16-A-Rapid-Iterative-Trajectory-Planning-Method-for-Automated-Parking-through-Differential-Flatnesspdf" class="headerlink" title="16. A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatnesspdf"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Rapid_Iterative_Trajectory_Planning_Method_for_Automated_Parking_through_Differential_Flatness.pdf">A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatness</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of Industrial Control Technology, Zhejiang University</span></p><p>该论文提出了一种基于路径-速度分解（PVD）的快速迭代轨迹规划（RITP）方法，结合差分平坦理论，将高阶多项式用于路径规划，利用并行计算和显式碰撞检测实现高效且精确的碰撞规避，并通过端点平滑约束（TSC）提升轨迹在换挡点的控制可行性。实验和仿真结果表明，该方法在自动泊车场景下相比现有方法大幅提升了计算效率（0.042~0.065s内完成）和轨迹的控制可行性，且可在实际车辆平台上实现高效、稳定的自动泊车。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Rapid_Iterative_Trajectory_Planning_Method_for_Automated_Parking_through_Differential_Flatness.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-DualReg-Dual-Space-Filtering-and-Reinforcement-for-Rigid-Registrationpdf"><a href="#17-DualReg-Dual-Space-Filtering-and-Reinforcement-for-Rigid-Registrationpdf" class="headerlink" title="17. DualReg: Dual-Space Filtering and Reinforcement for Rigid Registrationpdf"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DualReg__Dual-Space_Filtering_and_Reinforcement_for_Rigid_Registration.pdf">DualReg: Dual-Space Filtering and Reinforcement for Rigid Registration</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出了DualReg，一种结合特征空间与几何空间的点云刚性配准高效算法。方法包括单点RANSAC快速过滤、三点RANSAC概率加权细化，并利用几何代理点建立双空间优化框架实现高精度配准。实验在3DMatch、3DLoMatch和KITTI等公开数据集上，方法在配准精度和速度方面均优于现有主流方法，达到最快32倍加速且精度提升，适用于机器人视觉与SLAM等实时场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/DualReg_Dual-Space_Filtering_and_Reinforcement_for_Rigid_Registration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-WEBSIGHT-A-Vision-First-Architecture-for-Robust-Web-Agentspdf"><a href="#18-WEBSIGHT-A-Vision-First-Architecture-for-Robust-Web-Agentspdf" class="headerlink" title="18. WEBSIGHT: A Vision-First Architecture for Robust Web Agentspdf"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/WebSight__A_Vision-First_Architecture_for_Robust_Web_Agents.pdf">WEBSIGHT: A Vision-First Architecture for Robust Web Agents</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University</span></p><p>WEBSIGHT提出了一种以视觉为主的自主网页代理架构，完全基于网页截图进行交互，摒弃对HTML或DOM元数据的依赖。核心方法是通过LoRA微调UI-TARS模型，打造专用于网页UI元素识别与操作的WEBSIGHT-7B视觉语言模型，并集成至多智能体系统，包含规划、推理、视觉-动作和验证智能体，利用短期记忆机制协调任务推进。实验结果显示，WEBSIGHT-7B在Showdown Clicks基准上取得58.84%准确率，整体代理在WebVoyager基准上达到68%成功率，超越OpenAI等主流系统，且在已完成任务上正确率高达97.14%。结论表明该视觉优先架构实现了高效、可解释且鲁棒的网页自动化代理，推动自主网页智能体更接近人类视觉交互。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/WEBSIGHT_A_Vision-First_Architecture_for_Robust_Web_Agents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-LLM-based-Human-like-Traffic-Simulation-for-Self-driving-Testspdf"><a href="#19-LLM-based-Human-like-Traffic-Simulation-for-Self-driving-Testspdf" class="headerlink" title="19. LLM-based Human-like Traffic Simulation for Self-driving Testspdf"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LLM-based_Human-like_Traffic_Simulation_for_Self-driving_Tests.pdf">LLM-based Human-like Traffic Simulation for Self-driving Tests</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本论文提出HDSim框架，通过结合认知理论与大语言模型(LLM)，以分层人类驾驶风格模型和感知中介行为影响(PMBI)机制，实现多样化、真实的人类驾驶行为仿真。结论显示，HDSim在无需重训练的情况下，可在自动驾驶测试中暴露多达68%更多的潜在安全隐患，提升仿真交通的真实性和系统扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LLM-based_Human-like_Traffic_Simulation_for_Self-driving_Tests.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-Drive-As-You-Like-Strategy-Level-Motion-Planning-Based-on-A-Multi-Head-Diffusion-Modelpdf"><a href="#20-Drive-As-You-Like-Strategy-Level-Motion-Planning-Based-on-A-Multi-Head-Diffusion-Modelpdf" class="headerlink" title="20. Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Modelpdf"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Drive_As_You_Like__Strategy-Level_Motion_Planning_Based_on_A_Multi-Head_Diffusion_Model.pdf">Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Model</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Monash University Malaysia</span></p><p>本文提出了一种多头扩散模型（M-Diffusion Planner），结合DiT架构与Group Relative Policy Optimization（GRPO），实现策略级自动驾驶运动规划。模型通过LLM语义解析实时理解人类指令，动态选择不同驾驶策略头，生成多样化且高质量的轨迹。实验表明，该方法在nuPlan基准上取得SOTA性能，支持实时策略切换且有效反映用户偏好。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Drive_As_You_Like_Strategy-Level_Motion_Planning_Based_on_A_Multi-Head_Diffusion_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-A-Workflow-for-Map-Creation-in-Autonomous-Vehicle-Simulationspdf"><a href="#21-A-Workflow-for-Map-Creation-in-Autonomous-Vehicle-Simulationspdf" class="headerlink" title="21. A Workflow for Map Creation in Autonomous Vehicle Simulationspdf"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Workflow_for_Map_Creation_in_Autonomous_Vehicle_Simulations.pdf">A Workflow for Map Creation in Autonomous Vehicle Simulations</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ontario Tech University</span></p><p>本文提出了一种面向自动驾驶仿真环境（如AWSIM和Autoware）的自定义地图生成工作流程，采用轻量级开源工具链（包括OpenStreetMap、OSM2World、CloudCompare、PCL和Vector Map Builder）自动化生成3D网格、点云和Lanelet2地图文件，无需依赖特定仿真器或高资源消耗。结果表明，该流程可高效生成仿真地图并成功集成于主流自动驾驶仿真平台，显著降低了开发门槛，便于早期研究和快速原型设计。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Workflow_for_Map_Creation_in_Autonomous_Vehicle_Simulations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-NinA-Normalizing-Flows-in-Action-Training-VLA-Models-with-Normalizing-Flowspdf"><a href="#22-NinA-Normalizing-Flows-in-Action-Training-VLA-Models-with-Normalizing-Flowspdf" class="headerlink" title="22. NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flowspdf"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/NinA__Normalizing_Flows_in_Action._Training_VLA_Models_with_Normalizing_Flows.pdf">NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AIRI</span></p><p>本文提出了一种名为NinA的高效视觉-语言-动作（VLA）模型，创新性地用Normalizing Flows（NF）替换了以往常用的扩散模型作为动作解码器，实现了单步采样和精确似然估计，显著加速推理速度。实验表明，在LIBERO基准上，NinA在保持与扩散模型相当性能的同时，模型参数更少且推理速度提升7-10倍，为实际机器人高频控制提供了更实用的解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/NinA_Normalizing_Flows_in_Action._Training_VLA_Models_with_Normalizing_Flows.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-LodeStar-Long-horizon-Dexterity-via-Synthetic-Data-Augmentation-from-Human-Demonstrationspdf"><a href="#23-LodeStar-Long-horizon-Dexterity-via-Synthetic-Data-Augmentation-from-Human-Demonstrationspdf" class="headerlink" title="23. LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrationspdf"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LodeStar__Long-horizon_Dexterity_via_Synthetic_Data_Augmentation_from_Human_Demonstrations.pdf">LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California San Diego</span></p><p>本文提出LODESTAR系统，通过利用视觉基础模型自动将少量人类示范分解为语义技能段，并结合仿真中基于残差强化学习生成多样的合成演示数据，提升技能鲁棒性。最终利用Skill Routing Transformer将多个学习到的技能串联，实现长时序高难度操作，实验证明在三类真实世界任务上性能和泛化性均优于现有方法，成功率提升25%以上。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LodeStar_Long-horizon_Dexterity_via_Synthetic_Data_Augmentation_from_Human_Demonstrations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-Variational-Shape-Inference-for-Grasp-Diffusion-on-SE-3-pdf"><a href="#24-Variational-Shape-Inference-for-Grasp-Diffusion-on-SE-3-pdf" class="headerlink" title="24. Variational Shape Inference for Grasp Diffusion on SE(3)pdf"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Variational_Shape_Inference_for_Grasp_Diffusion_on_SE(3).pdf">Variational Shape Inference for Grasp Diffusion on SE(3)</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Purdue University</span></p><p>本论文提出了一种结合变分形状推断和扩散模型的多模态抓取合成框架，通过先训练基于隐式神经表示（INR）的变分自编码器从点云中获取鲁棒的几何特征，再利用这些特征驱动在SE(3)流形上的抓取扩散模型生成稳定多样的抓取姿态。此外，作者引入了可微分的测试时抓取优化方法以提升抓取成功率。实验表明，该方法在ACRONYM数据集上较主流多模态抓取方法提升6.3%，并在真实场景下表现出对点云噪声和稀疏性的强鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Variational_Shape_Inference_for_Grasp_Diffusion_on_SE%283%29.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-SoK-Cybersecurity-Assessment-of-Humanoid-Ecosystempdf"><a href="#25-SoK-Cybersecurity-Assessment-of-Humanoid-Ecosystempdf" class="headerlink" title="25. SoK: Cybersecurity Assessment of Humanoid Ecosystempdf"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SoK__Cybersecurity_Assessment_of_Humanoid_Ecosystem.pdf">SoK: Cybersecurity Assessment of Humanoid Ecosystem</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ben-Gurion University of the Negev</span></p><p>本论文提出了专为人形机器人设计的七层安全模型，梳理了硬件、传感、数据处理、中间件、决策、应用和人机交互等各层39种攻击手法与35种防御机制，并以系统性视角整合机器人、CPS和网络安全领域的碎片化研究。基于该模型，论文开发了RISK-MAP定量安全评估方法，并通过对Pepper、G1 EDU和Digit三款真实机器人平台的实证分析，发现当前防护多集中于高层，物理层及跨层级威胁仍存在显著安全短板，强调需系统性防护以保障人形机器人安全可信。结论指出该方法为行业提供了可量化、跨平台的安全基线，有助于推动主动安全投资和部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SoK_Cybersecurity_Assessment_of_Humanoid_Ecosystem.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-A-Synthetic-Dataset-for-Manometry-Recognition-in-Robotic-Applicationspdf"><a href="#26-A-Synthetic-Dataset-for-Manometry-Recognition-in-Robotic-Applicationspdf" class="headerlink" title="26. A Synthetic Dataset for Manometry Recognition in Robotic Applicationspdf"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Synthetic_Dataset_for_Manometry_Recognition_in_Robotic_Applications.pdf">A Synthetic Dataset for Manometry Recognition in Robotic Applications</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of São Paulo</span></p><p>本文提出了一种混合数据合成管线，结合BlenderProc程序化渲染与NVIDIA Cosmos-Predict2 AI驱动视频生成，用于在危险工业环境（如海上石油平台）提升机器人检测系统的训练数据量和多样性。实验表明，基于YOLO的检测网络在真实与合成数据1:1混合训练下，性能优于仅用真实数据，且可有效降低数据采集成本，提高安全性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Synthetic_Dataset_for_Manometry_Recognition_in_Robotic_Applications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-Optimizing-Grasping-in-Legged-Robots-A-Deep-Learning-Approach-to-Loco-Manipulationpdf"><a href="#27-Optimizing-Grasping-in-Legged-Robots-A-Deep-Learning-Approach-to-Loco-Manipulationpdf" class="headerlink" title="27. Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulationpdf"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Optimizing_Grasping_in_Legged_Robots__A_Deep_Learning_Approach_to_Loco-Manipulation.pdf">Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Federal University of Uberlandia</span></p><p>本论文提出了一种结合深度学习和仿真环境的四足机器人抓取优化方法。方法通过Genesis仿真平台生成大规模合成数据集，利用多模态传感输入训练U-Net结构的卷积神经网络，集成YOLOv11目标检测和d2nt表面法线估算，实现了在Spot平台上自主导航、感知和精准抓取。实验表明，所提管线能够在真实环境中提升机器人抓取的准确性和适应性，但在物体变化和深度数据噪声方面仍有改进空间。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Optimizing_Grasping_in_Legged_Robots_A_Deep_Learning_Approach_to_Loco-Manipulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-SEER-VAR-Semantic-Egocentric-Environment-Reasoner-for-Vehicle-Augmented-Realitypdf"><a href="#28-SEER-VAR-Semantic-Egocentric-Environment-Reasoner-for-Vehicle-Augmented-Realitypdf" class="headerlink" title="28. SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Realitypdf"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SEER-VAR__Semantic_Egocentric_Environment_Reasoner_for_Vehicle_Augmented_Reality.pdf">SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Eberhard-Karls-Universitat Tubingen</span></p><p>本文提出了SEER-VAR，一种创新的车辆增强现实（AR）系统框架，将语义分解、双分支SLAM（CASB）和GPT驱动的推荐模块统一，专为动态驾驶场景设计。核心方法包括基于深度的语义分割实现车内外场景分离、分别跟踪两类环境的相机运动，并结合LLM（GPT）的视觉-语言推理生成驾驶相关的AR叠加内容。实验和用户调研表明，SEER-VAR在复杂环境下实现了稳定的空间对齐和高感知一致性的AR渲染，大幅提升了驾驶场景理解和驾驶体验，为未来智能车辆AR系统奠定了基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SEER-VAR_Semantic_Egocentric_Environment_Reasoner_for_Vehicle_Augmented_Reality.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-4D-Visual-Pre-training-for-Robot-Learningpdf"><a href="#29-4D-Visual-Pre-training-for-Robot-Learningpdf" class="headerlink" title="29. 4D Visual Pre-training for Robot Learningpdf"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/4D_Visual_Pre-training_for_Robot_Learning.pdf">4D Visual Pre-training for Robot Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出了FVP（4D Visual Pre-training）视觉预训练框架，用于机器人操作任务的3D点云表示学习。该方法以下一时刻点云预测为目标，结合历史点云和动作信息，通过扩散模型预训练3D视觉表示，并在多种3D编码器和公开数据集上进行泛化。实验表明，FVP可显著提升3D模仿学习方法在12项真实世界机器人操作任务上的表现，平均成功率提升28%，并在VLA大模型应用中提升空间感知、任务泛化和长时序任务能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/4D_Visual_Pre-training_for_Robot_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agentspdf"><a href="#30-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agentspdf" class="headerlink" title="30. From reactive to cognitive: brain-inspired spatial intelligence for embodied agentspdf"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/From_reactive_to_cognitive__brain-inspired_spatial_intelligence_for_embodied_agents.pdf">From reactive to cognitive: brain-inspired spatial intelligence for embodied agents</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出BSC-Nav框架，通过仿生结构化空间记忆（包括地标、路径和测量知识）赋予具身智能体认知空间智能。BSC-Nav集成多模态大模型（如GPT-4V、DINOv2）进行环境感知、语义推理和动态规划，实现基于语义目标的分层记忆检索和高效导航，在多种仿真与真实场景下的导航、移动操作和空间问答任务中均取得最先进的泛化与效率表现，展示了生物启发式结构化空间记忆在提升具身智能体泛化性和适应性的关键作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/From_reactive_to_cognitive_brain-inspired_spatial_intelligence_for_embodied_agents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-FlowVLA-Thinking-in-Motion-with-a-Visual-Chain-of-Thoughtpdf"><a href="#31-FlowVLA-Thinking-in-Motion-with-a-Visual-Chain-of-Thoughtpdf" class="headerlink" title="31. FlowVLA: Thinking in Motion with a Visual Chain of Thoughtpdf"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FlowVLA__Thinking_in_Motion_with_a_Visual_Chain_of_Thought.pdf">FlowVLA: Thinking in Motion with a Visual Chain of Thought</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">HKUST(GZ)</span></p><p>本文提出了Visual Chain of Thought (Visual CoT)原则，并在FlowVLA框架中实现，通过“帧→光流→帧”结构化推理过程，先预测物理运动再生成下一帧，采用统一的VQ-GAN分词器与自回归Transformer模型学习视觉与运动表征。实验表明，FlowVLA在机器人操作任务上实现了新的性能纪录，并显著提升了样本效率，验证了显式运动推理对感知与控制桥接的价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/FlowVLA_Thinking_in_Motion_with_a_Visual_Chain_of_Thought.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-Scene-Agnostic-Traversability-Labeling-and-Estimation-via-a-Multimodal-Self-supervised-Frameworkpdf"><a href="#32-Scene-Agnostic-Traversability-Labeling-and-Estimation-via-a-Multimodal-Self-supervised-Frameworkpdf" class="headerlink" title="32. Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Frameworkpdf"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scene-Agnostic_Traversability_Labeling_and_Estimation_via_a_Multimodal_Self-supervised_Framework.pdf">Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了一种多模态自监督框架，用于机器人不同场景下的可通行性标注与估计。方法通过融合视觉基础模型（如SAM、DINOv2）、LiDAR与脚印数据，自动生成语义与几何结合的像素级可通行性标签，并设计双流网络实现语义与几何特征解耦学习，结合稀疏LiDAR标签校正伪标签误差。实验表明，该方法在多数据集和真实机器人平台中均显著优于现有方法，自动标注IoU约为88%，估计准确率提升1.6-3.5%，且支持实时部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Scene-Agnostic_Traversability_Labeling_and_Estimation_via_a_Multimodal_Self-supervised_Framework.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-The-Effects-of-Communication-Delay-on-Human-Performance-and-Neurocognitive-Responses-in-Mobile-Robot-Teleoperationpdf"><a href="#33-The-Effects-of-Communication-Delay-on-Human-Performance-and-Neurocognitive-Responses-in-Mobile-Robot-Teleoperationpdf" class="headerlink" title="33. The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperationpdf"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_Effects_of_Communication_Delay_on_Human_Performance_and_Neurocognitive_Responses_in_Mobile_Robot.pdf">The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>本论文通过人机互动实验，结合脑电（EEG）和机器人行为数据，系统分析了通信延迟（0–500 ms）对移动机器人遥操作中人的操作绩效与神经认知反应的影响。研究发现，200–300 ms延迟会显著降低任务效率与准确性，100–200 ms为人类对延迟的早期感知阈值，400 ms为认知资源分配的上限，这些结果为遥操作系统延迟补偿策略设计提供了神经认知依据。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/The_Effects_of_Communication_Delay_on_Human_Performance_and_Neurocognitive_Responses_in_Mobile_Robot_Teleoperation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-Arnold-a-generalist-muscle-transformer-policypdf"><a href="#34-Arnold-a-generalist-muscle-transformer-policypdf" class="headerlink" title="34. Arnold: a generalist muscle transformer policypdf"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Arnold__a_generalist_muscle_transformer_policy.pdf">Arnold: a generalist muscle transformer policy</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">École Polytechnique Fédérale de Lausanne (EPFL)</span></p><p>本论文提出Arnold，一种通用的肌肉控制Transformer策略，结合了行为克隆（on-policy behavior cloning, OBC）和PPO微调，能在14项复杂的生物肌肉骨骼控制任务（涵盖不同身体部分、多任务与多体型）中实现专家或超专家水平。方法创新在于提出了“传感-运动词汇”，用组合式嵌入描述异构感知、目标与执行器，并通过Transformer处理变长观测与动作空间，实现高效多任务和多体型学习，同时支持新任务的快速适应。结论显示，Arnold不仅能高效汇聚多专家知识和实现自蒸馏，还在个别任务中形成低维肌肉协同子空间，但缺乏跨任务的通用肌肉协同，揭示了生物运动控制中协同迁移性的局限。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-PerPilot-Personalizing-VLM-based-Mobile-Agents-via-Memory-and-Explorationpdf"><a href="#35-PerPilot-Personalizing-VLM-based-Mobile-Agents-via-Memory-and-Explorationpdf" class="headerlink" title="35. PerPilot: Personalizing VLM-based Mobile Agents via Memory and Explorationpdf"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PerPilot__Personalizing_VLM-based_Mobile_Agents_via_Memory_and_Exploration.pdf">PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northwestern Polytechnical University</span></p><p>本文提出了PerPilot，一个基于大语言模型（LLM）驱动的可插拔框架，提升移动智能体在理解和执行用户个性化指令方面的能力。其方法包括个性化感知模块用于识别指令中的用户特有元素，以及通过记忆检索和推理探索两种互补机制自动补全信息。实验结果显示，PerPilot显著提升了多种主流移动智能体在个性化任务上的成功率，并且随着使用次数增加，个性化能力持续增强。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-MODELING-AND-CONTROL-FRAMEWORK-FOR-AUTONOMOUS-SPACE-MANIPULATOR-HANDOVER-OPERATIONSpdf"><a href="#36-MODELING-AND-CONTROL-FRAMEWORK-FOR-AUTONOMOUS-SPACE-MANIPULATOR-HANDOVER-OPERATIONSpdf" class="headerlink" title="36. MODELING AND CONTROL FRAMEWORK FOR AUTONOMOUS SPACE MANIPULATOR HANDOVER OPERATIONSpdf"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Modeling_and_Control_Framework_for_Autonomous_Space_Manipulator_Handover_Operations.pdf">MODELING AND CONTROL FRAMEWORK FOR AUTONOMOUS SPACE MANIPULATOR HANDOVER OPERATIONS</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cincinnati</span></p><p>本文提出了一种用于自主空间机器人系统中机器人对机器人（R2R）交接任务的动力学建模方法，建立了双臂空间机械臂的完整动力学模型，并在仿真中对非线性模型预测控制（NMPC）与经典PID控制方法进行了比较。结果表明，NMPC和PID均能完成任务，但NMPC在轨迹跟踪精度和稳定性方面优于PID，适合空间服务、组装与制造（ISAM）场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-No-Need-to-Look-Locating-and-Grasping-Objects-by-a-Robot-Arm-Covered-with-Sensitive-Skinpdf"><a href="#37-No-Need-to-Look-Locating-and-Grasping-Objects-by-a-Robot-Arm-Covered-with-Sensitive-Skinpdf" class="headerlink" title="37. No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skinpdf"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/No_Need_to_Look!_Locating_and_Grasping_Objects_by_a_Robot_Arm_Covered_with_Sensitive_Skin.pdf">No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Czech Technical University in Prague</span></p><p>本论文提出了一种机器人在完全无视觉输入情况下，仅依赖覆盖全臂的人工皮肤进行物体定位和抓取的方法。该方法分为粗略全臂探索和末端精确定位两阶段，结合力&#x2F;扭矩传感器，并在仿真与真实机器人上系统评估，实现了多种物体的高效定位与抓取。结论显示，相比仅末端触觉基线方法，结合全臂皮肤的方案在真实机器人中抓取成功率达85.7%，且速度提升6倍，适用于视觉受限场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-Integration-of-Computer-Vision-with-Adaptive-Control-for-Autonomous-Driving-Using-ADOREpdf"><a href="#38-Integration-of-Computer-Vision-with-Adaptive-Control-for-Autonomous-Driving-Using-ADOREpdf" class="headerlink" title="38. Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADOREpdf"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Integration_of_Computer_Vision_with_Adaptive_Control_for_Autonomous_Driving_Using_ADORE.pdf">Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADORE</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Siegen</span></p><p>本论文提出了一种将YOLOv8深度学习视觉模型与ADORE自适应控制框架集成的方法，通过ROS桥在CARLA仿真环境下实现感知、决策和控制的实时闭环。实验表明，该系统可在不同天气和速度限制下准确识别交通标志，且ADORE能低延迟地调整车辆行为，实现平滑人性化的自主驾驶。结论：方法提升了自动驾驶系统的安全性和可靠性，并具备扩展到实际车辆的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-Neural-Algorithmic-Reasoners-informed-Large-Language-Model-for-Multi-Agent-Path-Findingpdf"><a href="#39-Neural-Algorithmic-Reasoners-informed-Large-Language-Model-for-Multi-Agent-Path-Findingpdf" class="headerlink" title="39. Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Findingpdf"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Neural_Algorithmic_Reasoners_informed_Large_Language_Model_for_Multi-Agent_Path_Finding.pdf">Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>本文提出了LLM-NAR框架，将大语言模型（LLM）与基于图神经网络（GNN）的神经算法推理器（NAR）以及交叉注意力机制结合，以提升多智能体路径规划（MAPF）任务的性能。方法通过优化的LLM提示机制、GNN-NAR地图信息建模和交叉注意力融合，实现了高效规划和更优路径。实验结果显示，该方法在仿真和真实机器人环境中均显著优于现有LLM方法，成功率更高、路径更短，同时训练效率远超强化学习和传统规划。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-A-holistic-perception-system-of-internal-and-external-monitoring-for-ground-autonomous-vehicles-AutoTRUST-paradigmpdf"><a href="#40-A-holistic-perception-system-of-internal-and-external-monitoring-for-ground-autonomous-vehicles-AutoTRUST-paradigmpdf" class="headerlink" title="40. A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigmpdf"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_holistic_perception_system_of_internal_and_external_monitoring_for_ground_autonomous_vehicles__Aut.pdf">A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Industrial Systems Institute, ATHENA Research Center</span></p><p>本文提出了AutoTRUST范式下的地面自动驾驶车辆全方位感知系统，集成了内部行为监测、智能虚拟助手、空气质量与热舒适分析，以及基于LiDAR的高效外部语义分割，通过多摄像头、智能传感器和大语言模型实现车内外实时感知。系统在真实电动车平台上部署并验证，有效提升了乘员安全、舒适性和环境感知能力，具备良好的实时性和鲁棒性，适用于多样化自动驾驶场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-Egocentric-Instruction-oriented-Affordance-Prediction-via-Large-Multimodal-Modelpdf"><a href="#41-Egocentric-Instruction-oriented-Affordance-Prediction-via-Large-Multimodal-Modelpdf" class="headerlink" title="41. Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Modelpdf"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Egocentric_Instruction-oriented_Affordance_Prediction_via_Large_Multimodal_Model.pdf">Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xidian University</span></p><p>该论文提出了一个以指令为中心的、从第一人称视角出发的物体可操作性（affordance）预测任务，并发布了包含约1.5万个数据样本的新数据集EIVA。方法上，作者设计了一种基于大规模多模态模型（LMM）的“Actor-Verifier”自反迭代管线，利用LMM对机器人操作区域和动作方向进行逐步自我优化和验证。实验结果显示，所提管线在开放词汇、复杂场景、真实机器人仿真等多项基准上超越现有方法，显著提升了基于指令的可操作性预测能力，为机器人智能操作领域提供了强有力的基线和新见解。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Physical-Embodiment-Enables-Information-Processing-Beyond-Explicit-Sensing-in-Active-Matterpdf"><a href="#42-Physical-Embodiment-Enables-Information-Processing-Beyond-Explicit-Sensing-in-Active-Matterpdf" class="headerlink" title="42. Physical Embodiment Enables Information Processing Beyond Explicit Sensing in Active Matterpdf"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Physical_Embodiment_Enables_Information_Processing_Beyond_Explicit_Sensing_in_Active_Matter.pdf">Physical Embodiment Enables Information Processing Beyond Explicit Sensing in Active Matter</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Leipzig University</span></p><p>本论文提出一种无需显式传感的微观自主导航方法，通过物理具身性和强化学习（PPO算法）控制自热泳动微机器人，仅利用位置信息和自身动力学反馈，实现对不可观测流场扰动的适应性导航。实验结果显示，机器人能在流场扰动强度远超自驱速度的环境中，仅凭自身动力学与环境耦合学习出对抗策略，证明了具身动力学可作为信息处理资源，为微型自主机器人和生物启发计算提供新范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-EndoUFM-Utilizing-Foundation-Models-for-Monocular-depth-estimation-of-endoscopic-imagespdf"><a href="#43-EndoUFM-Utilizing-Foundation-Models-for-Monocular-depth-estimation-of-endoscopic-imagespdf" class="headerlink" title="43. EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic imagespdf"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/EndoUFM__Utilizing_Foundation_Models_for_Monocular_depth_estimation_of_endoscopic_images.pdf">EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>本文提出EndoUFM框架，通过融合Depth Anything和Segment Anything两种视觉基础模型，实现了针对外科内窥镜图像的无监督单目深度估计。方法采用RVLoRA高效微调策略和Res-DSC残差块以提升模型对复杂纹理和光照变化的适应性，并设计了基于SAM分割掩码的平滑损失保证解剖结构内深度一致性。实验结果表明，EndoUFM在SCARED、Hamlyn、SERV-CT和EndoNeRF等公开数据集上达到了最新最优性能，增强了微创手术的空间感知和导航精度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-CubeDN-Real-time-Drone-Detection-in-3D-Space-from-Dual-mmWave-Radar-Cubespdf"><a href="#44-CubeDN-Real-time-Drone-Detection-in-3D-Space-from-Dual-mmWave-Radar-Cubespdf" class="headerlink" title="44. CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubespdf"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CubeDN__Real-time_Drone_Detection_in_3D_Space_from_Dual_mmWave_Radar_Cubes.pdf">CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubes</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University College London</span></p><p>本论文提出了CubeDN，一种面向无人机的实时三维检测与追踪系统，采用双毫米波雷达立方体数据，通过创新的4D数据融合与3D卷积神经网络，实现小型和大型无人机的高精度检测、定位与分类。实验结果显示CubeDN在多种复杂场景下均显著优于现有方法，达到10Hz实时推理和厘米级定位精度，具有优良的泛化能力和实用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-AgentRAN-An-Agentic-AI-Architecture-for-Autonomous-Control-of-Open-6G-Networkspdf"><a href="#45-AgentRAN-An-Agentic-AI-Architecture-for-Autonomous-Control-of-Open-6G-Networkspdf" class="headerlink" title="45. AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networkspdf"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AgentRAN__An_Agentic_AI_Architecture_for_Autonomous_Control_of_Open_6G_Networks.pdf">AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networks</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeastern University</span></p><p>本文提出AgentRAN框架，通过基于大语言模型（LLM）的AI代理，实现对Open RAN架构下6G蜂窝网络的自主控制。方法包括自然语言意图解析、层级意图分解、多代理协调、AI-RAN Factory自动生成和优化控制代理，实现跨时空和协议层的智能控制。实验在真实5G测试床中验证了系统能根据高层意图自适应优化网络性能，实现网络自主进化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-MEVITA-Open-Source-Bipedal-Robot-Assembled-from-E-Commerce-Components-via-Sheet-Metal-Weldingpdf"><a href="#46-MEVITA-Open-Source-Bipedal-Robot-Assembled-from-E-Commerce-Components-via-Sheet-Metal-Weldingpdf" class="headerlink" title="46. MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Weldingpdf"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MEVITA__Open-Source_Bipedal_Robot_Assembled_from_E-Commerce_Components_via_Sheet_Metal_Welding.pdf">MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Welding</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Tokyo</span></p><p>本文提出了MEVITA，一种完全由电商平台可购金属部件组装、并通过板金焊接集成复杂结构的开源双足机器人。通过最小化机械结构设计和强化学习仿真-实物迁移，MEVITA实现了部件数显著减少、易于装配，并在多种环境中实现了稳定行走。实验结果验证了其硬件和学习方法的有效性，降低了双足机器人开发门槛。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-SEBVS-Synthetic-Event-based-Visual-Servoing-for-Robot-Navigation-and-Manipulationpdf"><a href="#47-SEBVS-Synthetic-Event-based-Visual-Servoing-for-Robot-Navigation-and-Manipulationpdf" class="headerlink" title="47. SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulationpdf"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SEBVS__Synthetic_Event-based_Visual_Servoing_for_Robot_Navigation_and_Manipulation.pdf">SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Arizona State University</span></p><p>本文提出了SEBVS，一个集成于Gazebo仿真的开源ROS2包，通过v2e模拟器将RGB图像实时转换为事件流，实现无专用硬件的类神经形态视觉感知，并设计Transformer架构融合事件和RGB数据用于机器人导航和操作。实验表明，融合事件和RGB输入的Transformer策略能在运动模糊和复杂光照下实现更高导航和抓取精度、鲁棒性及任务成功率，展示了事件视觉在机器人感知与决策中的价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-GWM-Towards-Scalable-Gaussian-World-Models-for-Robotic-Manipulationpdf"><a href="#48-GWM-Towards-Scalable-Gaussian-World-Models-for-Robotic-Manipulationpdf" class="headerlink" title="48. GWM: Towards Scalable Gaussian World Models for Robotic Manipulationpdf"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GWM__Towards_Scalable_Gaussian_World_Models_for_Robotic_Manipulation.pdf">GWM: Towards Scalable Gaussian World Models for Robotic Manipulation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出Gaussian World Model (GWM)，结合3D Gaussian Splatting表征、3D变分自编码器与扩散Transformer，实现机器人操作中未来场景的精细三维预测。GWM可作为高效的视觉表征用于模仿学习，也可作为神经仿真器助力模型型强化学习。实验表明，GWM在多任务仿真和真实机器人操作中均显著优于现有图像方法，提升策略学习成功率和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="49-Structural-Damage-Detection-Using-AI-Super-Resolution-and-Visual-Language-Modelpdf"><a href="#49-Structural-Damage-Detection-Using-AI-Super-Resolution-and-Visual-Language-Modelpdf" class="headerlink" title="49. Structural Damage Detection Using AI Super Resolution and Visual Language Modelpdf"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Structural_Damage_Detection_Using_AI_Super_Resolution_and_Visual_Language_Model.pdf">Structural Damage Detection Using AI Super Resolution and Visual Language Model</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Dakota State University</span></p><p>该论文提出了一个结合无人机航拍、AI视频超分辨率模型（Video Restoration Transformer, VRT）与视觉语言模型（Gemma3:27b）的灾后结构损伤自动检测框架。流程包括多角度灾前&#x2F;灾后图像采集、VRT图像增强、VLM语义分析与建筑损伤分级，验证于土耳其地震和美国龙卷风数据集，分类准确率达84.5%。结论显示该方法能快速、准确地评估灾区建筑损伤，有效辅助救灾决策，且普通用户也可操作。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-CE-RS-SBCIT-A-Novel-Channel-Enhanced-Hybrid-CNN–Transformer-with-Residual-Spatial-and-Boundary-Aware-Learning-for-Brain-Tumor-MRI-Analysispdf"><a href="#50-CE-RS-SBCIT-A-Novel-Channel-Enhanced-Hybrid-CNN–Transformer-with-Residual-Spatial-and-Boundary-Aware-Learning-for-Brain-Tumor-MRI-Analysispdf" class="headerlink" title="50. CE-RS-SBCIT: A Novel Channel-Enhanced Hybrid CNN–Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysispdf"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CE-RS-SBCIT_A_Novel_Channel_Enhanced_Hybrid_CNN_Transformer_with_Residual,_Spatial,_and_Boundary-Awa.pdf">CE-RS-SBCIT: A Novel Channel-Enhanced Hybrid CNN–Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysis</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ibadat International University</span></p><p>该论文提出了CE-RS-SBCIT混合框架，将残差学习CNN、空间学习CNN与变换器模块结合，用于脑肿瘤MRI分类。方法创新包括SBCIT模块、通道增强策略、空间注意机制等，能同时捕获局部细粒度和全局上下文特征。实验在Kaggle和Figshare脑肿瘤MRI公开数据集上，准确率达98.3%，灵敏度98.43%，F1分数98.25%，精度98.08%，显著优于现有CNN和ViT模型。结论表明该方法在脑肿瘤自动诊断中表现优异且具备实际应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-GRASP-Geospatial-pixel-Reasoning-viA-Structured-Policy-learningpdf"><a href="#51-GRASP-Geospatial-pixel-Reasoning-viA-Structured-Policy-learningpdf" class="headerlink" title="51. GRASP: Geospatial pixel Reasoning viA Structured Policy learningpdf"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GRASP__Geospatial_pixel_Reasoning_viA_Structured_Policy_learning.pdf">GRASP: Geospatial pixel Reasoning viA Structured Policy learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>GRASP提出了一种使用结构化策略学习的远程感知像素推理框架，结合多模态大型语言模型（MLLM）输出任务相关的边框和正点，并以此为提示输入预训练分割模型生成最终掩码。系统采用纯强化学习（GRPO），通过格式和准确性奖励进行优化，无需掩码监督，显著降低标注成本并提升泛化能力。实验在遥感领域内外均取得了SOTA表现，尤其在OOD测试集提升高达54%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-Proximal-Vision-Transformer-Enhancing-Feature-Representation-through-Two-Stage-Manifold-Geometrypdf"><a href="#52-Proximal-Vision-Transformer-Enhancing-Feature-Representation-through-Two-Stage-Manifold-Geometrypdf" class="headerlink" title="52. Proximal Vision Transformer: Enhancing Feature Representation through Two-Stage Manifold Geometrypdf"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Proximal_Vision_Transformer__Enhancing_Feature_Representation_through_Two-Stage_Manifold_Geometry.pdf">Proximal Vision Transformer: Enhancing Feature Representation through Two-Stage Manifold Geometry</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">North Carolina State University</span></p><p>该论文提出了一种将Vision Transformer (ViT) 与可学习的邻近算子（proximal operator）相结合的新框架，通过自注意力机制构建数据流形的切丛，并利用邻近迭代在切丛内定义截面，实现特征从局部到全局的对齐与优化，显著提升分类准确率和特征分布结构。实验结果表明，该方法在高分辨率视觉任务上优于传统ViT，提升了类内紧凑性和类间可分性，验证了几何优化思想在视觉Transformer中的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-SSG-Dit-A-Spatial-Signal-Guided-Framework-for-Controllable-Video-Generationpdf"><a href="#53-SSG-Dit-A-Spatial-Signal-Guided-Framework-for-Controllable-Video-Generationpdf" class="headerlink" title="53. SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generationpdf"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SSG-Dit__A_Spatial_Signal_Guided_Framework_for_Controllable_Video_Generation.pdf">SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>该论文提出了SSG-DiT，一种面向可控视频生成的空间信号引导扩散变换器架构，采用解耦两阶段流程：首先利用CLIP模型中间特征生成文本感知的视觉提示，然后通过轻量级SSG-Adapter将视觉和文本条件联合注入冻结的Video DiT主干，实现空间信号的精细引导。实验表明，SSG-DiT在VBench基准的空间关系控制和一致性等关键指标上显著优于现有方法，提升了生成视频对复杂多模态条件的语义一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-REGEN-Real-Time-Photorealism-Enhancement-in-Games-via-a-Dual-Stage-Generative-Network-Frameworkpdf"><a href="#54-REGEN-Real-Time-Photorealism-Enhancement-in-Games-via-a-Dual-Stage-Generative-Network-Frameworkpdf" class="headerlink" title="54. REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Frameworkpdf"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/REGEN__Real-Time_Photorealism_Enhancement_in_Games_via_a_Dual-Stage_Generative_Network_Framework.pdf">REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Aristotle University of Thessaloniki</span></p><p>该论文提出了REGEN双阶段生成网络框架，首先利用健壮的无配对图像到图像翻译方法（EPE）生成与原始游戏帧语义一致的高拟真数据，再以此为配对数据训练轻量级的Pix2PixHD模型，实现游戏中的高效实时照片级画质增强。实验表明，该方法在GTA V游戏上可将推理速度提升32倍，同时保持与SOTA方法相当的视觉质量，且易于集成无需底层引擎信息。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-PVNet-Point-Voxel-Interaction-LiDAR-Scene-Upsampling-Via-Diffusion-Modelspdf"><a href="#55-PVNet-Point-Voxel-Interaction-LiDAR-Scene-Upsampling-Via-Diffusion-Modelspdf" class="headerlink" title="55. PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Modelspdf"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PVNet__Point-Voxel_Interaction_LiDAR_Scene_Upsampling_Via_Diffusion_Models.pdf">PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Telecom Guizhou Branch</span></p><p>该论文提出PVNet，一种基于扩散模型的点-体素交互框架，用于无密集监督的LiDAR点云场景级超分辨率。方法包括：以分类器无关指导的DDPM为生成主干，融合多帧稀疏点云作为条件输入，通过体素补全模块丰富特征，再利用点-体素交互模块提升每个上采样点的环境感知。实验结果表明，PVNet在各类基准测试上实现了任意倍率的高质量场景点云超分辨率，性能优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-F4-ITS-Fine-grained-Feature-Fusion-for-Food-Image-Text-Searchpdf"><a href="#56-F4-ITS-Fine-grained-Feature-Fusion-for-Food-Image-Text-Searchpdf" class="headerlink" title="56. F4-ITS: Fine-grained Feature Fusion for Food Image-Text Searchpdf"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/F4-ITS__Fine-grained_Feature_Fusion_for_Food_Image-Text_Search.pdf">F4-ITS: Fine-grained Feature Fusion for Food Image-Text Search</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">HyperVerge</span></p><p>本文提出了一种训练自由的视觉-语言模型引导框架F4-ITS，通过双向多模态特征融合和基于食材的特征重排序显著提升食物图像与文本检索性能。实验结果表明该方法在细粒度食物检索任务中对主流模型有平均10%、7.7%和28.6%的准确率提升，并证明小模型加融合可媲美大模型，适合资源受限场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-Dual-Orthogonal-Guidance-for-Robust-Diffusion-based-Handwritten-Text-Generationpdf"><a href="#57-Dual-Orthogonal-Guidance-for-Robust-Diffusion-based-Handwritten-Text-Generationpdf" class="headerlink" title="57. Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generationpdf"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dual_Orthogonal_Guidance_for_Robust_Diffusion-based_Handwritten_Text_Generation.pdf">Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Luleå University of Technology</span></p><p>本文提出了Dual Orthogonal Guidance (DOG)策略，通过在扩散模型生成手写文本时引入正负提示的正交投影，有效抑制伪影并提升内容清晰度和风格多样性。DOG基于三角形时序调度动态控制引导强度，可直接应用于现有手写文本扩散模型，无需重新训练，实验表明在内容准确性和风格复现方面均优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-Contrastive-Prompt-Clustering-for-Weakly-Supervised-Semantic-Segmentationpdf"><a href="#58-Contrastive-Prompt-Clustering-for-Weakly-Supervised-Semantic-Segmentationpdf" class="headerlink" title="58. Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentationpdf"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Contrastive_Prompt_Clustering_for_Weakly_Supervised_Semantic_Segmentation.pdf">Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong-Liverpool University</span></p><p>该论文提出了一种基于对比提示聚类（CPC）的弱监督语义分割方法，利用大语言模型（LLM）自动生成类别聚类，编码类别间的内在关系，并引入类感知的patch级对比损失以强化类内一致性与类间分离。实验证明该方法在PASCAL VOC 2012和MS COCO 2014数据集上超过了现有的最先进方法，显著提升了弱监督分割的精度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-HiCache-Training-free-Acceleration-of-Diffusion-Models-via-Hermite-Polynomial-based-Feature-Cachingpdf"><a href="#59-HiCache-Training-free-Acceleration-of-Diffusion-Models-via-Hermite-Polynomial-based-Feature-Cachingpdf" class="headerlink" title="59. HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Cachingpdf"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HiCache__Training-free_Acceleration_of_Diffusion_Models_via_Hermite_Polynomial-based_Feature_Caching.pdf">HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出HiCache，一种基于Hermite多项式的特征缓存方法，无需训练即可加速扩散模型的推理过程。方法通过验证扩散Transformer特征动态近似高斯分布，进而用Hermite多项式替代传统的Taylor展开进行特征预测，并引入双缩放机制保证数值稳定性。实验证明HiCache在文本生成图像、视频生成和超分辨率任务中实现高达6.24倍加速，同时生成质量超过现有基线。结论是HiCache能在无需模型重训练下大幅提升扩散模型的推理效率与生成质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-Combating-Digitally-Altered-Images-Deepfake-Detectionpdf"><a href="#60-Combating-Digitally-Altered-Images-Deepfake-Detectionpdf" class="headerlink" title="60. Combating Digitally Altered Images: Deepfake Detectionpdf"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Combating_Digitally_Altered_Images__Deepfake_Detection.pdf">Combating Digitally Altered Images: Deepfake Detection</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amrita School of Computing</span></p><p>本文提出一种基于改进版Vision Transformer（ViT）的深度伪造（Deepfake）图像检测方法，利用OpenForensics数据集并结合多种数据增强和处理技术，提升模型对多样化图像篡改的鲁棒性。模型在测试集上取得了超过99%的检测准确率，表明其在真实与伪造图像分类任务中具备高效和实际应用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-RPD-Diff-Region-Adaptive-Physics-Guided-Diffusion-Model-for-Visibility-Enhancement-under-Dense-and-Non-Uniform-Hazepdf"><a href="#61-RPD-Diff-Region-Adaptive-Physics-Guided-Diffusion-Model-for-Visibility-Enhancement-under-Dense-and-Non-Uniform-Hazepdf" class="headerlink" title="61. RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Hazepdf"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/RPD-Diff__Region-Adaptive_Physics-Guided_Diffusion_Model_for_Visibility_Enhancement_under_Dense_and_.pdf">RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Haze</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Intelligent Systems Engineering, Shenzhen Campus of Sun Yat-Sen University</span></p><p>本文提出了一种区域自适应物理引导扩散模型（RPD-Diff），用于在密集和非均匀雾霾条件下提升图像可见性。该方法引入物理引导的中间状态目标（PIST）策略，通过物理先验动态调整扩散过程，并结合基于传输图的交叉注意力机制实现局部去噪步数自适应（HADTP），有效应对空间异质性雾霾。实验表明，RPD-Diff在四个真实世界数据集上均取得了优于现有方法的性能，能够生成高质量、无雾且细节清晰、色彩真实的图像。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-Disentangling-Polysemantic-Neurons-with-a-Null-Calibrated-Polysemanticity-Index-and-Causal-Patch-Interventionspdf"><a href="#62-Disentangling-Polysemantic-Neurons-with-a-Null-Calibrated-Polysemanticity-Index-and-Causal-Patch-Interventionspdf" class="headerlink" title="62. Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventionspdf"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Disentangling_Polysemantic_Neurons_with_a_Null-Calibrated_Polysemanticity_Index_and_Causal_Patch_Int.pdf">Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventions</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">BITS Pilani</span></p><p>本论文提出了一种新的多证据神经元多义性度量指标——Polysemanticity Index (PSI)，结合几何聚类质量、类别标签对齐和基于CLIP的开放词汇语义判别，通过与显式的零假设分布校准，有效识别和量化神经网络中响应多种概念的多义神经元。实验结果表明，PSI能显著区分真实和随机信号，揭示深层网络多义性增强，并通过因果干预验证了发现的概念原型对神经元激活具备因果效力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-HieroAction-Hierarchically-Guided-VLM-for-Fine-Grained-Action-Analysispdf"><a href="#63-HieroAction-Hierarchically-Guided-VLM-for-Fine-Grained-Action-Analysispdf" class="headerlink" title="63. HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysispdf"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HieroAction__Hierarchically_Guided_VLM_for_Fine-Grained_Action_Analysis.pdf">HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysis</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>该论文提出HieroAction，一种结合视觉-语言模型（VLM）、分步动作推理（SAR）和分层策略学习（HPL）的精细化动作分析方法。其核心流程为：首先利用SAR将复杂人类动作分解为观察、识别、评估、结论四个阶段，生成结构化、可解释的评估序列；随后以Qwen-VL-2.5-7B为基础，通过有监督微调和基于多级奖励的分层策略强化学习，提升模型对子动作识别、时序边界及评分准确性的理解。实验在FineDive、FineFS和LOGO等细粒度体育动作数据集上取得了比现有方法更高的准确率和可解释性。结论认为，融合结构化推理与策略学习的视觉-语言模型能大幅提升复杂动作评估的透明度、准确性和通用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-HunyuanVideo-Foley-Multimodal-Diffusion-with-Representation-Alignment-for-High-Fidelity-Foley-Audio-Generationpdf"><a href="#64-HunyuanVideo-Foley-Multimodal-Diffusion-with-Representation-Alignment-for-High-Fidelity-Foley-Audio-Generationpdf" class="headerlink" title="64. HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generationpdf"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HunyuanVideo-Foley__Multimodal_Diffusion_with_Representation_Alignment_for_High-Fidelity_Foley_Audio.pdf">HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent Hunyuan</span></p><p>该论文提出HunyuanVideo-Foley，一种端到端的文本-视频-音频生成框架，利用多模态扩散Transformer结构和表示对齐(REPA)策略，结合大规模自动标注的高质量多模态数据集，提升了生成音频的视觉-语义对齐、时序同步和音质。实验结果表明，该方法在多个权威基准上实现了新的SOTA，显著提升了视频语义对齐和音质。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-LGE-Guided-Cross-Modality-Contrastive-Learning-for-Gadolinium-Free-Cardiomyopathy-Screening-in-Cine-CMRpdf"><a href="#65-LGE-Guided-Cross-Modality-Contrastive-Learning-for-Gadolinium-Free-Cardiomyopathy-Screening-in-Cine-CMRpdf" class="headerlink" title="65. LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMRpdf"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LGE-Guided_Cross-Modality_Contrastive_Learning_for_Gadolinium-Free_Cardiomyopathy_Screening_in_Cine_.pdf">LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMR</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ShanghaiTech University</span></p><p>本文提出了CC-CMR框架，通过对心脏磁共振（CMR）影像中的cine序列与晚期钆增强（LGE）序列的潜在空间对齐，利用对比学习和特征交互模块（包含不确定性感知机制）实现无需钆剂的心肌病筛查。该方法在多中心数据集上显著提升了诊断准确率（94.3%），优于现有方法，并具备良好的泛化能力和临床适用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-MDD-A-Dataset-for-Text-and-Music-Conditioned-Duet-Dance-Generationpdf"><a href="#66-MDD-A-Dataset-for-Text-and-Music-Conditioned-Duet-Dance-Generationpdf" class="headerlink" title="66. MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generationpdf"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MDD__A_Dataset_for_Text-and-Music_Conditioned_Duet_Dance_Generation.pdf">MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Purdue University</span></p><p>本文提出了MDD数据集，这是首个同时包含高质量3D双人舞动作、音乐及细粒度文本描述的大规模多模态数据集，支持文本和音乐条件下的双人舞动作生成。作者定义了两个新的基准任务：Text-to-Duet（基于文本和音乐生成双人舞动作）与Text-to-Dance Accompaniment（基于文本、音乐及领舞动作生成跟随舞者动作），并基于现有方法进行了基线实验。实验结果表明，MDD在规模和标注深度上均超越现有数据集，为互动动画与自动舞蹈生成等研究提供了坚实基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-Generating-Synthetic-Contrast-Enhanced-Chest-CT-Images-from-Non-Contrast-Scans-Using-Slice-Consistent-Brownian-Bridge-Diffusion-Networkpdf"><a href="#67-Generating-Synthetic-Contrast-Enhanced-Chest-CT-Images-from-Non-Contrast-Scans-Using-Slice-Consistent-Brownian-Bridge-Diffusion-Networkpdf" class="headerlink" title="67. Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Networkpdf"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Generating_Synthetic_Contrast-Enhanced_Chest_CT_Images_from_Non-Contrast_Scans_Using_Slice-Consisten.pdf">Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Network</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Saskatchewan</span></p><p>本文提出一种基于Slice-Consistent Brownian Bridge Diffusion Model (SC-BBDM) 的方法，实现从无造影胸部CT生成高保真合成造影CT图像。该方法通过预处理（配准、分割）、风格一致性和切片间轨迹对齐，有效提升解剖结构和对比度保持性。实验结果表明，SC-BBDM在保留血管结构和对比度真实性方面优于现有基线方法，具有提升诊断安全性和降低成本的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-A-Lightweight-Convolution-and-Vision-Transformer-integrated-model-with-Multi-scale-Self-attention-Mechanismpdf"><a href="#68-A-Lightweight-Convolution-and-Vision-Transformer-integrated-model-with-Multi-scale-Self-attention-Mechanismpdf" class="headerlink" title="68. A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanismpdf"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Lightweight_Convolution_and_Vision_Transformer_integrated_model_with_Multi-scale_Self-attention_Me.pdf">A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanism</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sichuan University</span></p><p>该论文提出了SAEViT，一种融合卷积神经网络（CNN）和视觉Transformer（ViT）的轻量级混合架构，核心包括稀疏聚合注意力（SAA）模块和通道交互前馈网络（CIFFN），以提升局部感知和全局建模能力。通过在ImageNet分类、COCO目标检测和ADE20K语义分割等任务上的实验，SAEViT在保持低计算量下实现了优于同类轻量模型的准确率-速度权衡，展示了其在多种视觉任务下的有效性与高效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-Multimodal-Medical-Endoscopic-Image-Analysis-via-Progressive-Disentangle-aware-Contrastive-Learningpdf"><a href="#69-Multimodal-Medical-Endoscopic-Image-Analysis-via-Progressive-Disentangle-aware-Contrastive-Learningpdf" class="headerlink" title="69. Multimodal Medical Endoscopic Image Analysis via Progressive Disentangle-aware Contrastive Learningpdf"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multimodal_Medical_Endoscopic_Image_Analysis_via_Progressive_Disentangle-aware_Contrastive_Learning.pdf">Multimodal Medical Endoscopic Image Analysis via Progressive Disentangle-aware Contrastive Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</span></p><p>本论文提出了一种创新的多模态医学内镜图像分割框架，结合’Align-Disentangle-Fusion’机制，通过多尺度分布对齐及逐步特征解耦，对2D白光和窄带成像进行融合。方法包括多尺度分布对齐、特征解耦及解耦感知对比学习，实现模态差异消除和高效语义融合。实验结果显示，该方法在多数据集上显著优于现有方法，提升了临床分割准确率，具备较强的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-Do-Multimodal-LLMs-See-Sentiment-pdf"><a href="#70-Do-Multimodal-LLMs-See-Sentiment-pdf" class="headerlink" title="70. Do Multimodal LLMs See Sentiment?pdf"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Do_Multimodal_LLMs_See_Sentiment_.pdf">Do Multimodal LLMs See Sentiment?</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universidade Tecnologica Federal do Paraná</span></p><p>本文提出MLLMsent框架，系统性地评估多模态大语言模型（如MiniGPT-4、GPT-4o mini、DeepSeek-VL2-Tiny）在视觉情感分析中的推理能力。方法包括直接图像情感分类、通过自动生成图像描述并用文本LLM分析情感、以及在带标签的描述上微调LLM。实验结果表明，微调后的MLLMsent在PerceptSent和DeepSent数据集上，显著优于Lexicon、CNN和Transformer等传统方法，F1-score最高提升64.8%，且具备较强的跨数据集泛化能力。结论是：多模态视觉-语言推理结合微调的文本模型，能有效提升社交媒体视觉情感分析的准确性和可解释性，并设立了新的性能基线。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-Delta-SVD-Efficient-Compression-for-Personalized-Text-to-Image-Modelspdf"><a href="#71-Delta-SVD-Efficient-Compression-for-Personalized-Text-to-Image-Modelspdf" class="headerlink" title="71. Delta-SVD: Efficient Compression for Personalized Text-to-Image Modelspdf"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Delta-SVD__Efficient_Compression_for_Personalized_Text-to-Image_Models.pdf">Delta-SVD: Efficient Compression for Personalized Text-to-Image Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Monash University</span></p><p>该论文提出Delta-SVD方法，通过对DreamBooth个性化文本到图像扩散模型的权重更新进行后处理压缩，采用SVD分解和能量阈值截断实现低秩近似，无需再训练或模型架构修改。实验表明，Delta-SVD能在极大压缩模型存储的同时保持生成质量，与主流方法相比压缩比和视觉保真度均优，支持大规模个性化扩散模型的高效部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-TinySR-Pruning-Diffusion-for-Real-World-Image-Super-Resolutionpdf"><a href="#72-TinySR-Pruning-Diffusion-for-Real-World-Image-Super-Resolutionpdf" class="headerlink" title="72. TinySR: Pruning Diffusion for Real-World Image Super-Resolutionpdf"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TinySR__Pruning_Diffusion_for_Real-World_Image_Super-Resolution.pdf">TinySR: Pruning Diffusion for Real-World Image Super-Resolution</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出了TinySR，一种针对真实场景图像超分辨率的高效扩散模型。方法包括动态深度剪枝（动态块间激活和扩展-腐蚀策略）、VAE压缩（通道剪枝、注意力移除、轻量化卷积）、去除冗余条件模块和参数预缓存等，整体显著降低计算量和模型规模。实验表明，TinySR在保持高感知质量的同时，推理速度提升5.68倍，参数减少83%，适合实时和边缘设备部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-OmniMRI-A-Unified-Vision–Language-Foundation-Model-for-Generalist-MRI-Interpretationpdf"><a href="#73-OmniMRI-A-Unified-Vision–Language-Foundation-Model-for-Generalist-MRI-Interpretationpdf" class="headerlink" title="73. OmniMRI: A Unified Vision–Language Foundation Model for Generalist MRI Interpretationpdf"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/OmniMRI__A_Unified_Vision--Language_Foundation_Model_for_Generalist_MRI_Interpretation.pdf">OmniMRI: A Unified Vision–Language Foundation Model for Generalist MRI Interpretation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Athinoula A. Martinos Center for Biomedical Imaging, Harvard Medical School</span></p><p>OmniMRI提出了一种统一的视觉-语言基础模型，能够覆盖MRI全流程任务，包括重建、分割、检测、诊断建议和报告生成。方法上，模型采用大规模异构数据集，通过自监督视觉预训练、视觉-语言对齐、多模态预训练和多任务指令微调等多阶段训练流程，实现跨模态推理和多任务泛化。结论显示，OmniMRI能够在一个架构下高质量完成多种MRI相关任务，有望将碎片化流程整合为通用框架，提升医学影像自动化与临床决策支持能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-Social-MAE-A-Transformer-Based-Multimodal-Autoencoder-for-Face-and-Voicepdf"><a href="#74-Social-MAE-A-Transformer-Based-Multimodal-Autoencoder-for-Face-and-Voicepdf" class="headerlink" title="74. Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voicepdf"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Social-MAE__A_Transformer-Based_Multimodal_Autoencoder_for_Face_and_Voice.pdf">Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Numediart Institute, ISIA Lab, University of Mons</span></p><p>该论文提出了Social-MAE，一种基于Transformer的多模态Masked Autoencoder，专为人脸和语音社交行为理解设计。方法在大规模社交音视频数据集（VoxCeleb2）上通过自监督学习预训练，采用改进CAV-MAE结构，增强视觉时序分辨率，并结合对比损失和重构损失。通过在情感识别、笑声检测和表面人格估计任务上微调，Social-MAE在多模态情感识别和笑声检测上达到了最新水平，验证了领域内自监督预训练对社交行为感知任务的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-Multimodal-Representation-Learning-Conditioned-on-Semantic-Relationspdf"><a href="#75-Multimodal-Representation-Learning-Conditioned-on-Semantic-Relationspdf" class="headerlink" title="75. Multimodal Representation Learning Conditioned on Semantic Relationspdf"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multimodal_Representation_Learning_Conditioned_on_Semantic_Relations.pdf">Multimodal Representation Learning Conditioned on Semantic Relations</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Emory University</span></p><p>该论文提出了RCML（Relation-Conditioned Multimodal Learning）框架，通过自然语言语义关系描述对多模态（图像-文本）特征提取和对齐进行条件引导。方法创新地构建了多对多的训练对，并引入关系引导的跨注意力机制，联合跨模态与模态内对比损失，实现了关系上下文中的多模态一致性。实验证明，RCML在多个数据集上的检索和分类任务均优于现有强基线模型，显著提升了关系感知的多模态表示能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-GraphMMP-A-Graph-Neural-Network-Model-with-Mutual-Information-and-Global-Fusion-for-Multimodal-Medical-Prognosispdf"><a href="#76-GraphMMP-A-Graph-Neural-Network-Model-with-Mutual-Information-and-Global-Fusion-for-Multimodal-Medical-Prognosispdf" class="headerlink" title="76. GraphMMP: A Graph Neural Network Model with Mutual Information and Global Fusion for Multimodal Medical Prognosispdf"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GraphMMP__A_Graph_Neural_Network_Model_with_Mutual_Information_and_Global_Fusion_for_Multimodal_Medi.pdf">GraphMMP: A Graph Neural Network Model with Mutual Information and Global Fusion for Multimodal Medical Prognosis</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hangzhou Dianzi University</span></p><p>该论文提出了一种基于图神经网络的双阶段多模态医学预后模型GraphMMP，方法包括利用互信息进行特征图构建和基于Mamba的全局融合模块以提升全局感知能力。实验表明，GraphMMP在肝脏和乳腺癌预后任务中准确率和鲁棒性均优于现有方法，证明了其在多模态医学数据分析中的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-Multi-Level-LVLM-Guidance-for-Untrimmed-Video-Action-Recognitionpdf"><a href="#77-Multi-Level-LVLM-Guidance-for-Untrimmed-Video-Action-Recognitionpdf" class="headerlink" title="77. Multi-Level LVLM Guidance for Untrimmed Video Action Recognitionpdf"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multi-Level_LVLM_Guidance_for_Untrimmed_Video_Action_Recognition.pdf">Multi-Level LVLM Guidance for Untrimmed Video Action Recognition</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kunming University of Science and Technology</span></p><p>本文提出了事件情境化视频Transformer（ECVT）架构，通过引入大规模视觉-语言模型（LVLM）生成的多粒度语义描述，采用视频编码分支与跨模态引导分支结合，利用全局事件提示与时序子事件提示等多级文本信息，通过自适应门控融合、跨模态注意力和事件图校准，有效提升视频时序结构和事件逻辑理解能力。在ActivityNet v1.3和THUMOS14数据集上，ECVT实现了平均mAP 40.5%和<a href="mailto:&#x6d;&#x41;&#x50;&#x40;&#48;&#46;&#53;">mAP@0.5</a> 67.1%的领先性能，优于现有主流方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-An-LLM-LVLM-Driven-Agent-for-Iterative-and-Fine-Grained-Image-Editingpdf"><a href="#78-An-LLM-LVLM-Driven-Agent-for-Iterative-and-Fine-Grained-Image-Editingpdf" class="headerlink" title="78. An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editingpdf"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/An_LLM-LVLM_Driven_Agent_for_Iterative_and_Fine-Grained_Image_Editing.pdf">An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kunming University of Science and Technology</span></p><p>该论文提出了RefineEdit-Agent，一个无需训练的智能代理框架，利用大型语言模型（LLM）进行编辑任务规划，并结合视觉-语言大型模型（LVLM）进行场景理解和迭代反馈，实现复杂、多步、细粒度的图像编辑。通过引入LongBench-T2I-Edit基准和大量实验证明，该方法在编辑精度和上下文保持方面显著优于现有主流方法。结论：RefineEdit-Agent可高效执行复杂编辑指令，并保持图像整体一致性，推动智能图像编辑技术发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-Robust-Point-Cloud-Registration-via-Geometric-Overlapping-Guided-Rotation-Searchpdf"><a href="#79-Robust-Point-Cloud-Registration-via-Geometric-Overlapping-Guided-Rotation-Searchpdf" class="headerlink" title="79. Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Searchpdf"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Robust_Point_Cloud_Registration_via_Geometric_Overlapping_Guided_Rotation_Search.pdf">Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>该论文提出了一种基于几何最大重叠和分支定界（BnB）搜索的点云配准方法GMOR。方法分两阶段：首先利用Chasles定理将刚性变换分解为旋转轴与轴向平移，通过立方映射参数化旋转轴并用区间扫描加速RMQ，第二阶段将2D配准转化为1D旋转角度搜索，利用扫线算法和线段树对轴对齐矩形做最大重叠查询。实验表明GMOR在3DMatch、3DLoMatch和KITTI数据集上配准精度和计算效率优于现有方法，且复杂度随点数线性增长，适合大规模场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-Constrained-Prompt-Enhancement-for-Improving-Zero-Shot-Generalization-of-Vision-Language-Modelspdf"><a href="#80-Constrained-Prompt-Enhancement-for-Improving-Zero-Shot-Generalization-of-Vision-Language-Modelspdf" class="headerlink" title="80. Constrained Prompt Enhancement for Improving Zero-Shot Generalization of Vision-Language Modelspdf"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Constrained_Prompt_Enhancement_for_Improving_Zero-Shot_Generalization_of_Vision-Language_Models.pdf">Constrained Prompt Enhancement for Improving Zero-Shot Generalization of Vision-Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tianjin University</span></p><p>该论文提出了一种约束性提示增强（CPE）方法，通过“拓扑引导同义语义生成”（TGSSG）和“类别无关判别区域选择”（CADRS）分别生成完整的文本和精简的视觉提示，并采用基于测试时自适应（TTA）和最优传输（OT）的集合到集合匹配策略提升视觉-文本对齐能力。实验表明，CPE方法在零样本图像分类、分布外泛化和视频动作识别任务上均显著超过现有方法，取得了最先进的性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-Lightweight-Joint-Optimization-of-General-Purpose-Vision-Language-Models-and-Retrievers-for-Medical-Diagnosispdf"><a href="#81-Lightweight-Joint-Optimization-of-General-Purpose-Vision-Language-Models-and-Retrievers-for-Medical-Diagnosispdf" class="headerlink" title="81. Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosispdf"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Lightweight_Joint_Optimization_of_General-Purpose_Vision-Language_Models_and_Retrievers_for_Medical_.pdf">Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosis</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hebrew University of Jerusalem</span></p><p>该论文提出JOMED方法，通过联合优化通用多模态检索器和大型视觉语言模型（LVLM），在无需医学预训练的情况下实现医学图像分类和视觉问答任务。方法采用两阶段训练流程，LVLM和检索器分别进行轻量级微调，并设计了基于KL散度的损失函数，使LVLM的误差信号能够反向传播至检索器，有效提升在检索结果不一致的复杂病例上的表现。实验结果表明，JOMED在多个医学基准任务上取得了与医学预训练模型相媲美甚至更优的性能，尤其在困难检索情形下显著提升了准确率和可靠性，但与理论最优（oracle）之间仍有提升空间。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-Neural-Proteomics-Fields-for-Super-resolved-Spatial-Proteomics-Predictionpdf"><a href="#82-Neural-Proteomics-Fields-for-Super-resolved-Spatial-Proteomics-Predictionpdf" class="headerlink" title="82. Neural Proteomics Fields for Super-resolved Spatial Proteomics Predictionpdf"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Neural_Proteomics_Fields_for_Super-resolved_Spatial_Proteomics_Prediction.pdf">Neural Proteomics Fields for Super-resolved Spatial Proteomics Prediction</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Artificial Intelligence, University of Chinese Academy of Sciences</span></p><p>该论文提出了Neural Proteomics Fields (NPF)，首创将空间超分辨蛋白组学（seq-SP）预测任务建模为连续空间的蛋白表达重建问题。NPF采用空间建模模块与形态建模模块的双分支架构，结合深度神经网络、Vision Transformer和基础病理模型UNI，能有效捕捉组织特异的蛋白空间分布与形态特征，并在公开的Pseudo-Visium SP和真实10X Visium数据集上实现了参数量更少下的SOTA性能。结论显示，NPF方法大幅提升了蛋白空间分辨预测的准确性和泛化能力，并为空间蛋白组学研究提供了新范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-DiCache-Let-Diffusion-Model-Determine-Its-Own-Cachepdf"><a href="#83-DiCache-Let-Diffusion-Model-Determine-Its-Own-Cachepdf" class="headerlink" title="83. DiCache: Let Diffusion Model Determine Its Own Cachepdf"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DiCache__Let_Diffusion_Model_Determine_Its_Own_Cache.pdf">DiCache: Let Diffusion Model Determine Its Own Cache</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出DiCache，一种运行时自适应、无需训练的缓存加速策略，用于提升扩散模型的推理速度和视觉质量。方法包括在线浅层探针分析（动态判定何时缓存）和缓存轨迹动态对齐（自适应多步缓存利用），实验表明DiCache在多种主流图像&#x2F;视频扩散模型上均显著优于现有方法，兼具高效和高保真度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-PosBridge-Multi-View-Positional-Embedding-Transplant-for-Identity-Aware-Image-Editingpdf"><a href="#84-PosBridge-Multi-View-Positional-Embedding-Transplant-for-Identity-Aware-Image-Editingpdf" class="headerlink" title="84. PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editingpdf"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PosBridge__Multi-View_Positional_Embedding_Transplant_for_Identity-Aware_Image_Editing.pdf">PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editing</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Electro-Communications</span></p><p>该论文提出了PosBridge，一种无需训练的身份感知图像编辑框架，通过多视角位置嵌入移植（positional embedding transplant）和Corner Centered Layout，实现对象特征结构与场景的高保真融合。其方法以扩散模型为基础，利用参考图像的位置信息引导目标区域的合成，同时可选用轻量级LoRA模块增强细节表现。实验显示，该方法在结构一致性、外观真实感及计算效率方面优于主流基线，能有效实现个性化、空间可控的对象插入。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-FoundDiff-Foundational-Diffusion-Model-for-Generalizable-Low-Dose-CT-Denoisingpdf"><a href="#85-FoundDiff-Foundational-Diffusion-Model-for-Generalizable-Low-Dose-CT-Denoisingpdf" class="headerlink" title="85. FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoisingpdf"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FoundDiff__Foundational_Diffusion_Model_for_Generalizable_Low-Dose_CT_Denoising.pdf">FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本论文提出FoundDiff，一种两阶段的基础扩散模型，实现了在不同剂量水平和解剖区域下通用的低剂量CT去噪。方法上，第一阶段利用基于CLIP的DA-CLIP模型，通过对剂量排序和解剖判别损失获得连续的剂量和解剖表征；第二阶段用DA-Diff扩散模型，通过创新的DACB模块将上述表征融合于扩散过程，实现自适应去噪。实验在多公开数据集和不同剂量、解剖条件下，FoundDiff均优于现有方法，且对未见剂量具备强泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoningpdf"><a href="#86-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoningpdf" class="headerlink" title="86. Explain Before You Answer: A Survey on Compositional Visual Reasoningpdf"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Explain_Before_You_Answer__A_Survey_on_Compositional_Visual_Reasoning.pdf">Explain Before You Answer: A Survey on Compositional Visual Reasoning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Monash University</span></p><p>本论文系统综述了2023-2025年间260余篇关于组合式视觉推理的研究，梳理了从基于提示的语言中心方法、工具增强的大模型、链式思维推理到统一体式视觉语言模型的五阶段发展范式，涵盖关键技术如多模态推理、工具集成、链式推理与视觉证据显式展示。结论指出，组合式视觉推理以结构化中间步骤实现更强的泛化、认知对齐、可解释性和鲁棒性，但仍面临数据、评测和系统集成等挑战，未来应关注世界模型集成与更细致的评测协议。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-Quickly-Tuning-Foundation-Models-for-Image-Segmentationpdf"><a href="#87-Quickly-Tuning-Foundation-Models-for-Image-Segmentationpdf" class="headerlink" title="87. Quickly Tuning Foundation Models for Image Segmentationpdf"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Quickly_Tuning_Foundation_Models_for_Image_Segmentation.pdf">Quickly Tuning Foundation Models for Image Segmentation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Freiburg</span></p><p>本论文提出QTT-SEG，一种基于元学习和贝叶斯优化的自动化快速微调SAM（Segment Anything Model）图像分割基础模型的方法。QTT-SEG通过预训练的性能与成本预测器，结合丰富的超参数搜索空间，在13个二值与多类分割数据集上实现高效、稳定的微调，显著优于SAM零样本及AutoGluon等AutoML基线，尤其在限定时间预算下表现突出。结论显示QTT-SEG能显著提升基础模型在领域特定分割任务中的表现，具备优秀的效率、稳定性与泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-ERF-BA-TFD-A-Multimodal-Model-for-Audio-Visual-Deepfake-Detectionpdf"><a href="#88-ERF-BA-TFD-A-Multimodal-Model-for-Audio-Visual-Deepfake-Detectionpdf" class="headerlink" title="88. ERF-BA-TFD+: A Multimodal Model for Audio-Visual Deepfake Detectionpdf"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ERF-BA-TFD+__A_Multimodal_Model_for_Audio-Visual_Deepfake_Detection.pdf">ERF-BA-TFD+: A Multimodal Model for Audio-Visual Deepfake Detection</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Lanzhou University</span></p><p>该论文提出了ERF-BA-TFD+，一种结合增强感受野（ERF）和音视频融合的多模态深度伪造检测模型。方法包括视觉编码器（MViTv2）、音频编码器（BYOL-A）、跨重建注意力Transformer（CRATrans）、帧分类与边界定位模块，以及特征增强模块，能够建模音视频长程依赖，精准定位伪造片段。实验在DDL-AV数据集上进行，模型在精度、召回率、F1分数等指标上超越现有方法，尤其在长时视频和音视频不同步场景下表现突出，并获得深度伪造检测竞赛第一名。结论：ERF-BA-TFD+显著提升了多模态深度伪造检测的准确性和鲁棒性，适用于复杂真实场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-MTNet-Learning-Modality-Aware-Representation-with-Transformer-for-RGBT-Trackingpdf"><a href="#89-MTNet-Learning-Modality-Aware-Representation-with-Transformer-for-RGBT-Trackingpdf" class="headerlink" title="89. MTNet: Learning Modality-Aware Representation with Transformer for RGBT Trackingpdf"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MTNet__Learning_modality-aware_representation_with_transformer_for_RGBT_tracking.pdf">MTNet: Learning Modality-Aware Representation with Transformer for RGBT Tracking</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本文提出了一种基于Transformer的模态感知RGBT目标跟踪方法MTNet，创新性地设计了模态感知网络（包括通道聚合分配模块CADM和空间相似性感知模块SSPM）以增强多模态特征表达，并结合混合Transformer融合网络实现模板与搜索区域的全局依赖建模。通过三分支预测头与状态感知动态模板更新策略，有效提升了定位精度和跟踪鲁棒性。实验结果表明，MTNet在多个RGBT基准数据集上均优于现有方法，且具备实时跟踪速度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-PersPose-3D-Human-Pose-Estimation-with-Perspective-Encoding-and-Perspective-Rotationpdf"><a href="#90-PersPose-3D-Human-Pose-Estimation-with-Perspective-Encoding-and-Perspective-Rotationpdf" class="headerlink" title="90. PersPose: 3D Human Pose Estimation with Perspective Encoding and Perspective Rotationpdf"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PersPose__3D_Human_Pose_Estimation_with_Perspective_Encoding_and_Perspective_Rotation.pdf">PersPose: 3D Human Pose Estimation with Perspective Encoding and Perspective Rotation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southern University of Science and Technology</span></p><p>本文提出了一种新颖的3D人体姿态估计算法PersPose，核心贡献包括引入Perspective Encoding（PE）模块，将裁剪后图像的相机内参编码为2D特征图与图像一同送入CNN，以及提出Perspective Rotation（PR）模块，通过旋转原图使人体主体居中，减少视角畸变并简化模型拟合难度。实验结果显示，PersPose在3DPW、MPI-INF-3DHP和Human3.6M等数据集上均实现了SOTA性能，特别在复杂真实场景下表现出更强鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-Multi-Agent-Visual-Language-Reasoning-for-Comprehensive-Highway-Scene-Understandingpdf"><a href="#91-Multi-Agent-Visual-Language-Reasoning-for-Comprehensive-Highway-Scene-Understandingpdf" class="headerlink" title="91. Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understandingpdf"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multi-Agent_Visual-Language_Reasoning_for_Comprehensive_Highway_Scene_Understanding.pdf">Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Georgia</span></p><p>该论文提出了一种多智能体视觉-语言推理框架，采用大型通用视觉语言模型（如GPT-4o）结合领域知识生成链式思维（CoT）提示，引导小型高效VLM（如Qwen2.5-VL-7B）对短视频及多模态数据进行推理，实现对天气分类、路面湿滑评估和交通拥堵检测等多任务高效感知。实验结果表明，该方法在多样化交通和环境条件下表现稳健，显著提升多任务推理准确率，并可集成于现有交通摄像头系统，实现大规模部署和高风险路段实时监控。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-MMCIG-Multimodal-Cover-Image-Generation-for-Text-only-Documents-and-Its-Dataset-Construction-via-Pseudo-labelingpdf"><a href="#92-MMCIG-Multimodal-Cover-Image-Generation-for-Text-only-Documents-and-Its-Dataset-Construction-via-Pseudo-labelingpdf" class="headerlink" title="92. MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labelingpdf"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MMCIG__Multimodal_Cover_Image_Generation_for_Text-only_Documents_and_Its_Dataset_Construction_via_Ps.pdf">MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labeling</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Chungnam National University</span></p><p>本文提出了MMCIG任务，针对仅含文本的文档，先自动生成简明摘要，再生成与摘要视觉对应的封面图片。核心方法是多模态伪标签标注，通过独立对图片和其说明进行排序，选出与摘要最相关的图片，并构建高质量训练集。实验和人工评测显示，该方法比只用文本或图片伪标注更能生成与摘要高度一致的图片，并提升生成模型的表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-Advancing-Weakly-Supervised-Change-Detection-in-Satellite-Images-via-Adversarial-Class-Promptingpdf"><a href="#93-Advancing-Weakly-Supervised-Change-Detection-in-Satellite-Images-via-Adversarial-Class-Promptingpdf" class="headerlink" title="93. Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Promptingpdf"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Advancing_Weakly-Supervised_Change_Detection_in_Satellite_Images_via_Adversarial_Class_Prompting.pdf">Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>该论文提出了一种新颖的Adversarial Class Prompting (AdvCP)方法用于遥感卫星图像的弱监督变化检测。方法包含两个阶段：首先通过对未变化样本施加强制性分类扰动挖掘共现噪声（背景变化），其次结合全局原型进行对抗样本特征的聚类校正，从而显著提升模型区分目标变化与环境噪声的能力。实验证明，AdvCP可无缝集成至主流ConvNet、Transformer及SAM等多种网络，在多个公开数据集上带来最高7.46% IoU提升，且对多类弱监督分割与全监督变化检测等任务也具有良好泛化性。结论：AdvCP方法增强了弱监督变化检测的鲁棒性和泛用性，有效降低了误检背景变化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-VROOM-Visual-Reconstruction-over-Onboard-Multiviewpdf"><a href="#94-VROOM-Visual-Reconstruction-over-Onboard-Multiviewpdf" class="headerlink" title="94. VROOM - Visual Reconstruction over Onboard Multiviewpdf"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/VROOM_-_Visual_Reconstruction_over_Onboard_Multiview.pdf">VROOM - Visual Reconstruction over Onboard Multiview</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Berkeley</span></p><p>该论文提出VROOM系统，通过对F1赛车的车载摄像头视频进行预处理（包括分块、掩膜、降采样等），结合Monst3r、AnyCam等3D重建与SLAM方法，实现赛道及车辆轨迹的三维重建。实验表明，在复杂动态环境下，VROOM能恢复部分赛道和车辆轨迹，展示了车载视频实现可扩展4D重建的可行性，但全局一致性和多视角融合仍需进一步优化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-GSVisLoc-Generalizable-Visual-Localization-for-Gaussian-Splatting-Scene-Representationspdf"><a href="#95-GSVisLoc-Generalizable-Visual-Localization-for-Gaussian-Splatting-Scene-Representationspdf" class="headerlink" title="95. GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representationspdf"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GSVisLoc__Generalizable_Visual_Localization_for_Gaussian_Splatting_Scene_Representations.pdf">GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Weizmann Institute of Science</span></p><p>该论文提出GSVisLoc，一种基于3D Gaussian Splatting（3DGS）场景表征的视觉定位方法。GSVisLoc通过3DGS编码器和2D图像编码器提取特征，采用粗到细的3D-2D特征匹配流程并结合PnP+RANSAC和基于3DGS的姿态优化，实现了无需额外参考图像、无需重训练即可对新场景泛化的高效视觉定位。实验表明，该方法在标准室内外数据集上超越了现有3DGS方法，并与主流视觉定位方法具有竞争力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-Disentangling-the-Factors-of-Convergence-between-Brains-and-Computer-Vision-Modelspdf"><a href="#96-Disentangling-the-Factors-of-Convergence-between-Brains-and-Computer-Vision-Modelspdf" class="headerlink" title="96. Disentangling the Factors of Convergence between Brains and Computer Vision Modelspdf"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Disentangling_the_Factors_of_Convergence_between_Brains_and_Computer_Vision_Models.pdf">Disentangling the Factors of Convergence between Brains and Computer Vision Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Meta AI</span></p><p>本论文系统性地分析了自监督视觉Transformer模型（DINOv3）在架构、训练量和数据类型三方面对其与人脑视觉表征收敛性的影响。通过与人类fMRI和MEG数据比较，并采用编码分数、空间分数和时间分数等多维度指标，作者发现更大规模、训练量更高且更具人类中心的数据能显著提升模型与脑表征的相似性，且该收敛过程与大脑皮层结构和功能属性密切相关。结论指出，模型架构、训练和数据共同决定AI与人脑视觉表征的收敛性，为理解生物视觉的组织原则提供了新思路。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-Follow-My-Hold-Hand-Object-Interaction-Reconstruction-through-Geometric-Guidancepdf"><a href="#97-Follow-My-Hold-Hand-Object-Interaction-Reconstruction-through-Geometric-Guidancepdf" class="headerlink" title="97. Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidancepdf"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Follow_My_Hold__Hand-Object_Interaction_Reconstruction_through_Geometric_Guidance.pdf">Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Max Planck Institute for Informatics</span></p><p>该论文提出FollowMyHold方法，通过多模态几何引导在单张RGB图像下实现手持物体的高质量3D重建，采用优化环路设计引导预训练的3D扩散模型。核心流程包括多模型输出统一对齐、阶段式优化（先手再物体最后联合）、并在扩散采样过程中引入2D像素对齐损失和3D物理约束（交集与接触），显著提升了在遮挡和真实场景下的重建稳定性和准确性。实验表明，FollowMyHold在多数据集上重建准确率和鲁棒性均优于现有生成方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-Explain-and-Monitor-Deep-Learning-Models-for-Computer-Vision-using-Obz-AIpdf"><a href="#98-Explain-and-Monitor-Deep-Learning-Models-for-Computer-Vision-using-Obz-AIpdf" class="headerlink" title="98. Explain and Monitor Deep Learning Models for Computer Vision using Obz AIpdf"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Explain_and_Monitor_Deep_Learning_Models_for_Computer_Vision_using_Obz_AI.pdf">Explain and Monitor Deep Learning Models for Computer Vision using Obz AI</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Informatics, University of Warsaw</span></p><p>本文提出Obz AI，一套面向计算机视觉系统的集成化可解释性与监测软件平台，包含PyTorch库、后端数据库和前端仪表盘，支持XAI方法、异常检测及实时监控。实验表明Obz AI可提升深度学习视觉模型的可解释性和可观测性，便于实际部署和审计。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-SEAM-Semantically-Equivalent-Across-Modalities-Benchmark-for-Vision-Language-Modelspdf"><a href="#99-SEAM-Semantically-Equivalent-Across-Modalities-Benchmark-for-Vision-Language-Modelspdf" class="headerlink" title="99. SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Modelspdf"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SEAM__Semantically_Equivalent_Across_Modalities_Benchmark_for_Vision-Language_Models.pdf">SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Toronto</span></p><p>SEAM提出了一个评价视觉-语言模型（VLM）在视觉与文本两种模态下推理一致性的基准，通过四个具有标准化符号和视觉表征的领域（国际象棋、化学、音乐、图论）构建语义等价的输入。实验评测21个主流VLM，发现所有模型在不同模态间存在显著性能不均衡，视觉推理普遍落后于文本推理，且跨模态答案一致性较低，主要受文本标记化和视觉感知误差影响。结论：当前VLM在处理语义等价的多模态信息时仍存在明显不足，SEAM为推动更强跨模态推理能力提供了标准和工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-Scene-Aware-Vectorized-Memory-Multi-Agent-Framework-with-Cross-Modal-Differentiated-Quantization-VLMs-for-Visually-Impaired-Assistancepdf"><a href="#100-Scene-Aware-Vectorized-Memory-Multi-Agent-Framework-with-Cross-Modal-Differentiated-Quantization-VLMs-for-Visually-Impaired-Assistancepdf" class="headerlink" title="100. Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistancepdf"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scene-Aware_Vectorized_Memory_Multi-Agent_Framework_with_Cross-Modal_Differentiated_Quantization_VLM.pdf">Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>本文提出一种创新的跨模态差异化量化框架（CMDQ），针对视觉-语言模型（VLM）中视觉编码器和跨模态处理模块的不同量化敏感性进行模块化量化压缩，并结合高效解量化和存储优化技术，使19B参数模型在单块消费级GPU上运行，将内存需求从38GB降至16GB，仅损失约2%性能。此外，设计了场景感知向量化记忆多智能体系统，实现跨视角理解和历史场景知识集成，通过多智能体协作提供实时场景分析、文本识别与语音流式反馈。结论显示，该系统既提升了模型部署高效性，又为视障用户提供了及时、全面的环境感知辅助。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="101-Incorporating-Pre-trained-Diffusion-Models-in-Solving-the-Schrodinger-Bridge-Problempdf"><a href="#101-Incorporating-Pre-trained-Diffusion-Models-in-Solving-the-Schrodinger-Bridge-Problempdf" class="headerlink" title="101. Incorporating Pre-trained Diffusion Models in Solving the Schrödinger Bridge Problempdf"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Incorporating_Pre-trained_Diffusion_Models_in_Solving_the_Schr%C3%B6dinger_Bridge_Problem.pdf">Incorporating Pre-trained Diffusion Models in Solving the Schrödinger Bridge Problem</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了三种重参数化技术（IPMM、IPTM、IPFM），统一了Schrödinger Bridge (SB) 问题与Score-based Generative Models (SGMs, 即扩散模型) 的训练目标，并提出将预训练扩散模型用于SB模型初始化，从而大幅提升SB模型训练的效率、稳定性和生成质量。实验结果表明，该方法在高维高分辨率图像生成、无配对图像翻译等任务上，显著优于已有SB及扩散模型方法，并有效加速收敛与提升生成表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-Annotation-Free-Open-Vocabulary-Segmentation-for-Remote-Sensing-Imagespdf"><a href="#102-Annotation-Free-Open-Vocabulary-Segmentation-for-Remote-Sensing-Imagespdf" class="headerlink" title="102. Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Imagespdf"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Annotation-Free_Open-Vocabulary_Segmentation_for_Remote-Sensing_Images.pdf">Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>本论文提出SegEarth-OV，首个面向遥感图像的无标注开放词汇分割框架。方法核心包括SimFeatUp通用特征上采样模块（基于参数化JBU和内容保持网络，无需任务特定训练）、Global Bias Alleviation（全局偏置消除操作提升像素级区分力）、以及AlignEarth跨模态知识蒸馏策略（实现光学VLM到SAR编码器的高效语义迁移）。实验表明，SegEarth-OV在17个遥感任务和8个SAR数据集上显著超越现有SOTA方法，验证了其在多模态遥感无标注分割的有效性和通用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-AQ-PCDSys-An-Adaptive-Quantized-Planetary-Crater-Detection-System-for-Autonomous-Space-Explorationpdf"><a href="#103-AQ-PCDSys-An-Adaptive-Quantized-Planetary-Crater-Detection-System-for-Autonomous-Space-Explorationpdf" class="headerlink" title="103. AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Explorationpdf"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AQ-PCDSys__An_Adaptive_Quantized_Planetary_Crater_Detection_System_for_Autonomous_Space_Exploration.pdf">AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Manipal University Jaipur</span></p><p>本文提出了AQ-PCDSys系统，通过结合量化感知训练（QAT）的量化神经网络（QNN）、自适应多传感器融合（AMF）模块和多尺度检测头，实现了对行星陨石坑的高效、实时检测。该系统显著优化了模型体积和推理速度，并通过自适应权重机制动态融合光学影像和DEM数据，提升了复杂环境下的检测鲁棒性。结论：AQ-PCDSys为受限硬件下的自主空间探测提供了一种高效、准确且可部署的视觉感知方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-Propose-and-Rectify-A-Forensics-Driven-MLLM-Framework-for-Image-Manipulation-Localizationpdf"><a href="#104-Propose-and-Rectify-A-Forensics-Driven-MLLM-Framework-for-Image-Manipulation-Localizationpdf" class="headerlink" title="104. Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localizationpdf"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Propose_and_Rectify__A_Forensics-Driven_MLLM_Framework_for_Image_Manipulation_Localization.pdf">Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">City University of Hong Kong</span></p><p>本文提出了一种面向图像篡改检测与定位的Propose-Rectify框架，结合多模态大语言模型（MLLM）高层语义推理与多尺度低层次取证特征分析。方法包括利用经过取证适配的LLaVA模型生成初步可疑区域，再通过多特征提取器和取证校正模块逐步精细化检测与分割边界，并引入增强分割模块融合取证线索以提升定位精度。实验表明该方法在多个公开数据集上实现了领先的检测与定位精度，具备优异的稳健性与泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-SAIL-Recon-Large-SfM-by-Augmenting-Scene-Regression-with-Localizationpdf"><a href="#105-SAIL-Recon-Large-SfM-by-Augmenting-Scene-Regression-with-Localizationpdf" class="headerlink" title="105. SAIL-Recon: Large SfM by Augmenting Scene Regression with Localizationpdf"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SAIL-Recon__Large_SfM_by_Augmenting_Scene_Regression_with_Localization.pdf">SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>该论文提出SAIL-Recon，一种通过在Transformer架构下增加视觉定位能力，实现大规模结构光束法（SfM）的端到端场景回归方法。方法流程为：先从一小部分锚点图片提取神经场景表征，再用此表征对所有输入图片进行定位与重建，实现了高效处理千级图片数据集，同时提升了相机位姿估计与新视角合成的精度。实验表明，SAIL-Recon在TUM-RGBD、CO3Dv2、Tanks &amp; Temples等基准上实现了SOTA效果，超越传统与其他深度学习SfM方法，能够在几分钟内完成大规模三维重建与定位。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-Generative-Feature-Imputing-—-A-Technique-for-Error-resilient-Semantic-Communicationpdf"><a href="#106-Generative-Feature-Imputing-—-A-Technique-for-Error-resilient-Semantic-Communicationpdf" class="headerlink" title="106. Generative Feature Imputing — A Technique for Error-resilient Semantic Communicationpdf"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Generative_Feature_Imputing_-_A_Technique_for_Error-resilient_Semantic_Communication.pdf">Generative Feature Imputing — A Technique for Error-resilient Semantic Communication</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Hong Kong</span></p><p>本论文提出了一种用于数字语义通信系统的生成式特征补全新框架，包含空间误差集中分组、扩散模型驱动的特征生成、以及基于语义重要性的功率分配。实验表明，该方法在图像传输中显著提升了抗误差鲁棒性和语义准确性，优于DJSCC和传统方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="107-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimationpdf"><a href="#107-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimationpdf" class="headerlink" title="107. Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimationpdf"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Gaze_into_the_Heart__A_Multi-View_Video_Dataset_for_rPPG_and_Health_Biomarkers_Estimation.pdf">Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sber AI Lab</span></p><p>本文提出了MCD-rPPG多视角视频大规模数据集，用于远程光电容积描记（rPPG）和健康生物标志物估计，涵盖600名受试者在不同状态下的多摄像头同步视频及13项健康指标，并配套100 Hz PPG信号。基于此数据集，作者设计了高效的多任务神经网络，实现了从人脸视频中实时推断脉搏波与健康参数，并在跨数据集评测中表现出色。结论认为公开的数据集和模型将加速基于AI的远程健康监测领域进展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-ISALux-Illumination-and-Semantics-Aware-Transformer-Employing-Mixture-of-Experts-for-Low-Light-Image-Enhancementpdf"><a href="#108-ISALux-Illumination-and-Semantics-Aware-Transformer-Employing-Mixture-of-Experts-for-Low-Light-Image-Enhancementpdf" class="headerlink" title="108. ISALux: Illumination and Semantics-Aware Transformer Employing Mixture of Experts for Low Light Image Enhancementpdf"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ISALux__Illumination_and_Segmentation_Aware_Transformer_Employing_Mixture_of_Experts_for_Low_Light_I.pdf">ISALux: Illumination and Semantics-Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Manchester</span></p><p>该论文提出了ISALux，一种基于视觉Transformer的低光照图像增强方法，通过融合照明先验和语义分割先验，实现对不同区域结构和亮度的自适应增强。核心创新包括引入HISA-MSA模块进行照明与语义特征融合自注意力，以及采用MoE专家混合机制提升上下文特征表达能力，并通过LoRA适配防止过拟合。实验结果显示，ISALux在多个基准数据集上优于现有方法，且参数和推理效率均具备优势，适合实际应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-Edge-Enhanced-Vision-Transformer-Framework-for-Accurate-AI-Generated-Image-Detectionpdf"><a href="#109-Edge-Enhanced-Vision-Transformer-Framework-for-Accurate-AI-Generated-Image-Detectionpdf" class="headerlink" title="109. Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detectionpdf"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Edge-Enhanced_Vision_Transformer_Framework_for_Accurate_AI-Generated_Image_Detection.pdf">Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detection</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Uttara University</span></p><p>本文提出了一种结合细粒度边缘结构分析与ViT（Vision Transformer）全局特征提取的AI生成图像检测框架。方法包括对ViT进行多数据集微调，结合基于边缘差异方差的新型后处理模块，提升对AI生成图像与真实图像的区分能力；实验在CIFAKE、Artistic及自建数据集上均取得优于主流模型的检测准确率（CIFAKE上准确率97.75%、F1分数97.77%），验证了方法在不同领域和分辨率下的鲁棒性与高效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="110-Diffusion-Based-Data-Augmentation-for-Medical-Image-Segmentationpdf"><a href="#110-Diffusion-Based-Data-Augmentation-for-Medical-Image-Segmentationpdf" class="headerlink" title="110. Diffusion-Based Data Augmentation for Medical Image Segmentationpdf"></a>110. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Diffusion-Based_Data_Augmentation_for_Medical_Image_Segmentation.pdf">Diffusion-Based Data Augmentation for Medical Image Segmentation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science and Engineering, Beihang University</span></p><p>本文提出DiffAug框架，结合文本引导的扩散模型生成与分割验证，用于医学图像中的异常病灶数据增强。通过医学术语文本提示和空间掩码在正常图像中扩散生成病灶，并通过高效的单步推理分割网络自动验证空间准确性，仅保留高质量合成样本训练分割网络。实验在CVC-ClinicDB、Kvasir-SEG和REFUGE2等医学数据集上验证，DiffAug显著提升分割性能（Dice提升8-10%），并将小病灶漏检率降低至28%，兼顾生成质量与计算效率，为医学影像稀缺异常数据增强提供实用方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="111-SCOUT-Semi-supervised-Camouflaged-Object-Detection-by-Utilizing-Text-and-Adaptive-Data-Selectionpdf"><a href="#111-SCOUT-Semi-supervised-Camouflaged-Object-Detection-by-Utilizing-Text-and-Adaptive-Data-Selectionpdf" class="headerlink" title="111. SCOUT: Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selectionpdf"></a>111. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SCOUT__Semi-supervised_Camouflaged_Object_Detection_by_Utilizing_Text_and_Adaptive_Data_Selection.pdf">SCOUT: Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selection</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xiamen University</span></p><p>该论文提出了SCOUT，一个用于伪装物体检测的半监督学习模型，结合了自适应数据增强与选择（ADAS）模块和文本融合模块（TFM），通过主动筛选有价值的未标注数据并利用图像文本交互提升模型对伪装物体的表征能力。实验结果表明，SCOUT在多个主流数据集上显著优于现有半监督方法，有效降低了标注成本并提升了检测性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="112-UniSino-Physics-Driven-Foundational-Model-for-Universal-CT-Sinogram-Standardizationpdf"><a href="#112-UniSino-Physics-Driven-Foundational-Model-for-Universal-CT-Sinogram-Standardizationpdf" class="headerlink" title="112. UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardizationpdf"></a>112. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/UniSino__Physics-Driven_Foundational_Model_for_Universal_CT_Sinogram_Standardization.pdf">UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanchang University</span></p><p>该论文提出了UniSino，一个面向CT投影域（sinogram）的物理驱动基础模型，通过SinoVAE感知压缩和物理约束的损失函数，以及条件扩散模型LRD，实现多类型伪影和噪声的标准化处理。该方法显著提升了CT重建图像的质量，并在多个数据集和任务下展现出卓越的泛化能力，为医学影像预处理提供了统一、高效的解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="113-Towards-Trustworthy-Breast-Tumor-Segmentation-in-Ultrasound-using-Monte-Carlo-Dropout-and-Deep-Ensembles-for-Epistemic-Uncertainty-Estimationpdf"><a href="#113-Towards-Trustworthy-Breast-Tumor-Segmentation-in-Ultrasound-using-Monte-Carlo-Dropout-and-Deep-Ensembles-for-Epistemic-Uncertainty-Estimationpdf" class="headerlink" title="113. Towards Trustworthy Breast Tumor Segmentation in Ultrasound using Monte Carlo Dropout and Deep Ensembles for Epistemic Uncertainty Estimationpdf"></a>113. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Towards_Trustworthy_Breast_Tumor_Segmentation_in_Ultrasound_using_Monte_Carlo_Dropout_and_Deep_Ensem.pdf">Towards Trustworthy Breast Tumor Segmentation in Ultrasound using Monte Carlo Dropout and Deep Ensembles for Epistemic Uncertainty Estimation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kwame Nkrumah University of Science and Technology</span></p><p>该论文提出了基于改进型Residual Encoder U-Net的乳腺超声肿瘤自动分割方法，并重点利用Monte Carlo dropout和深度集成（Deep Ensembles）对模型的认知不确定性进行量化，结合多种不确定性估计方法提升分割的可靠性。通过清理和去重BUSI数据集，并在分布内和分布外数据上进行充分验证，结果在Breast-Lesion-USG数据集上实现了当前最优分割准确率，同时不确定性估计能够有效指示低置信度区域，强调了数据准备和不确定性建模在临床部署中的重要性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="114-CEIDM-A-Controlled-Entity-and-Interaction-Diffusion-Model-for-Enhanced-Text-to-Image-Generationpdf"><a href="#114-CEIDM-A-Controlled-Entity-and-Interaction-Diffusion-Model-for-Enhanced-Text-to-Image-Generationpdf" class="headerlink" title="114. CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generationpdf"></a>114. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CEIDM__A_Controlled_Entity_and_Interaction_Diffusion_Model_for_Enhanced_Text-to-Image_Generation.pdf">CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">College of Computer Science and Technology, National University of Defense Technology</span></p><p>本文提出CEIDM方法，通过在扩散模型中引入显式与隐式交互关系挖掘（基于大语言模型推理）、交互动作聚类与双向偏移，以及实体控制网络，实现了对文本到图像生成过程中实体及其复杂交互的精细双重控制。实验结果显示，CEIDM在图像整体质量和实体交互检测准确率方面均优于现有主流方法，尤其在复杂场景合成和细节处理上表现出色。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="115-SUPERGEN-An-Efficient-Ultra-high-resolution-Video-Generation-System-with-Sketching-and-Tilingpdf"><a href="#115-SUPERGEN-An-Efficient-Ultra-high-resolution-Video-Generation-System-with-Sketching-and-Tilingpdf" class="headerlink" title="115. SUPERGEN: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tilingpdf"></a>115. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SuperGen__An_Efficient_Ultra-high-resolution_Video_Generation_System_with_Sketching_and_Tiling.pdf">SUPERGEN: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Rice University</span></p><p>SUPERGEN提出了一种高效、无需额外训练的超高分辨率视频生成系统，采用两阶段的sketch-tile协同生成流程：首先通过预训练扩散模型生成低分辨率视频草图，随后利用分块（tiling）和智能平衡的并行加速对各区域细致优化，并结合区域自适应缓存机制减少冗余计算。实验显示，该系统在多种主流基线和分辨率下实现了最高6.2倍的加速，同时保持了生成质量，极大提升了超高分辨率视频生成的实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="116-DroneKey-Drone-3D-Pose-Estimation-in-Image-Sequences-using-Gated-Key-representation-and-Pose-adaptive-Learningpdf"><a href="#116-DroneKey-Drone-3D-Pose-Estimation-in-Image-Sequences-using-Gated-Key-representation-and-Pose-adaptive-Learningpdf" class="headerlink" title="116. DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learningpdf"></a>116. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DroneKey__Drone_3D_Pose_Estimation_in_Image_Sequences_using_Gated_Key-representation_and_Pose-adapti.pdf">DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Chonnam National University</span></p><p>本文提出DroneKey框架，专为无人机3D姿态估计设计，结合Transformer结构进行2D关键点检测，并通过门控加权融合中间和紧凑特征表示，提升关键点检测的准确性和顺序判别能力。为增强极端姿态下的稳定性，提出了姿态自适应Mahalanobis损失函数，并构建了公开的合成无人机2D关键点和3D姿态数据集。实验结果显示，DroneKey在关键点检测（OKS AP 99.68%）和3D姿态估计（MAE-angle 10.62°，RMSE 0.221m，MAE-absolute 0.076m）方面显著优于现有方法，并能实时运行（44 FPS）。该方法适用于实时反无人机系统。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="117-Few-shot-Human-Action-Anomaly-Detection-via-a-Unified-Contrastive-Learning-Frameworkpdf"><a href="#117-Few-shot-Human-Action-Anomaly-Detection-via-a-Unified-Contrastive-Learning-Frameworkpdf" class="headerlink" title="117. Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Frameworkpdf"></a>117. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Few-shot_Human_Action_Anomaly_Detection_via_a_Unified_Contrastive_Learning_Framework.pdf">Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Toyama</span></p><p>该论文提出了一个统一的对比学习框架用于少样本人体动作异常检测，通过残差图卷积网络(Res-GCN)和离散余弦变换(DCT)进行动作编码，并结合基于扩散模型的生成式运动增强策略(HumanMAC)以提升类别泛化和鲁棒性。实验结果表明，该方法无需类别特定的重训练，在HumanAct12数据集的已见和未见类别上均取得了最优性能，提升了模型的实用性和可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="118-Instant-Preference-Alignment-for-Text-to-Image-Diffusion-Modelspdf"><a href="#118-Instant-Preference-Alignment-for-Text-to-Image-Diffusion-Modelspdf" class="headerlink" title="118. Instant Preference Alignment for Text-to-Image Diffusion Modelspdf"></a>118. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Instant_Preference_Alignment_for_Text-to-Image_Diffusion_Models.pdf">Instant Preference Alignment for Text-to-Image Diffusion Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">New Laboratory of Pattern Recognition, CASIA</span></p><p>本文提出了一种无需训练、基于多模态大语言模型（MLLM）先验的即时偏好对齐文本到图像（T2I）生成框架，将任务分为偏好理解和偏好引导生成两部分。方法通过MLLM自动从参考图像中提取多维全局偏好关键词并丰富文本提示，结合全局关键词引导与局部区域感知的交叉注意力调控，实现了对扩散模型的实时、细粒度偏好控制，无须额外微调。实验表明，该方法在多个数据集上均优于现有方案，并支持多轮交互式图像精细化生成，极大提升了偏好对齐能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="119-CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolutionpdf"><a href="#119-CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolutionpdf" class="headerlink" title="119. CATformer: Contrastive Adversarial Transformer for Image Super-Resolutionpdf"></a>119. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CATformer__Contrastive_Adversarial_Transformer_for_Image_Super-Resolution.pdf">CATformer: Contrastive Adversarial Transformer for Image Super-Resolution</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Duke University</span></p><p>本文提出了CATformer，一种对比对抗变换器网络，将扩散模型启发的特征精炼、对抗性训练和对比学习结合，用于图像超分辨率。CATformer采用双分支结构：主分支为扩散式Transformer逐步去噪特征，辅助分支通过噪声增强提升鲁棒性，最后经特征融合和深层残差密集块解码重建高质量图像。实验表明，CATformer在感知质量、结构相似性和推理效率上优于现有SOTA方法，但在小数据集PSNR略低，凸显了感知质量与像素精度的权衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="120-Robustness-Feature-Adapter-for-Efficient-Adversarial-Trainingpdf"><a href="#120-Robustness-Feature-Adapter-for-Efficient-Adversarial-Trainingpdf" class="headerlink" title="120. Robustness Feature Adapter for Efficient Adversarial Trainingpdf"></a>120. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Robustness_Feature_Adapter_for_Efficient_Adversarial_Training.pdf">Robustness Feature Adapter for Efficient Adversarial Training</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Dongguan University of Technology</span></p><p>本文提出了一种新的鲁棒性特征适配器（RFA），在特征空间中高效进行对抗训练（AT），通过引入外部适配器模块实现参数高效微调，有效缓解了大模型AT的高计算开销和鲁棒过拟合问题。实验表明，该方法大幅提升了模型对未知攻击的泛化鲁棒性和训练效率，并可灵活应用于主流视觉模型（如CNN和ViT）和下游多媒体任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="121-Citizen-Centered-Climate-Intelligence-Operationalizing-Open-Tree-Data-for-Urban-Cooling-and-Eco-Routing-in-Indian-Citiespdf"><a href="#121-Citizen-Centered-Climate-Intelligence-Operationalizing-Open-Tree-Data-for-Urban-Cooling-and-Eco-Routing-in-Indian-Citiespdf" class="headerlink" title="121. Citizen Centered Climate Intelligence: Operationalizing Open Tree Data for Urban Cooling and Eco-Routing in Indian Citiespdf"></a>121. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Citizen_Centered_Climate_Intelligence__Operationalizing_Open_Tree_Data_for_Urban_Cooling_and_Eco-Rou.pdf">Citizen Centered Climate Intelligence: Operationalizing Open Tree Data for Urban Cooling and Eco-Routing in Indian Cities</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Institute of Technology Tiruchirappalli</span></p><p>该论文提出了一套以城市居民为中心的气候智能框架，包括三大模块：手机端AI分割辅助的树木测量工具、基于卫星地表温度的局地降温指标建模，以及融合环境质量分数的生态路径规划引擎。该方法实现了公民参与式数据采集、环境分析和生态出行建议，显著提升了城市气候适应与公民赋权。实验展示了该系统在印地城市的应用潜力，结论认为此框架可复制推广，有效促进城市绿色治理和可持续交通。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="122-Finding-Outliers-in-a-Haystack-Anomaly-Detection-for-Large-Pointcloud-Scenespdf"><a href="#122-Finding-Outliers-in-a-Haystack-Anomaly-Detection-for-Large-Pointcloud-Scenespdf" class="headerlink" title="122. Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenespdf"></a>122. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Finding_Outliers_in_a_Haystack__Anomaly_Detection_for_Large_Pointcloud_Scenes.pdf">Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenes</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Adelaide</span></p><p>本文提出了一种新颖的基于重建的开集分割方法，结合了Mamba架构对大规模点云场景的长程依赖建模能力。通过仅用已知类别训练重建器以生成场景的“默认上下文”，再利用重建与原始点云的差异进行异常检测，显著提升了在KITTI和ECLAIR等数据集上的异常检测性能，并可增强现有方法的表现。结论表明，该方法在异常检测任务中优于主流方法，但在已知对象分割上仍有提升空间。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="123-Improving-Interpretability-in-Alzheimer’s-Prediction-via-Joint-Learning-of-ADAS-Cog-Scorespdf"><a href="#123-Improving-Interpretability-in-Alzheimer’s-Prediction-via-Joint-Learning-of-ADAS-Cog-Scorespdf" class="headerlink" title="123. Improving Interpretability in Alzheimer’s Prediction via Joint Learning of ADAS-Cog Scorespdf"></a>123. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Improving_Interpretability_in_Alzheimer's_Prediction_via_Joint_Learning_of_ADAS-Cog_Scores.pdf">Improving Interpretability in Alzheimer’s Prediction via Joint Learning of ADAS-Cog Scores</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universiti Brunei Darussalam</span></p><p>该论文提出了一种多任务学习（MTL）框架，利用Vision Transformer（ViT）和Swin Transformer对MRI影像和纵向临床评分（ADAS-Cog及其13个子项）进行特征提取和融合，联合预测阿尔茨海默症未来的全球评分和各子评分。结论表明联合子项学习提升了全球评分预测的准确性，并揭示了部分子项（如Q1、Q4、Q8）对总评分的主导作用，但也暴露出模型对临床特征的过度依赖和MRI特征利用不足，未来需优化多模态融合与损失权重以提升模型稳定性与解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="124-JCo-MVTON-Jointly-Controllable-Multi-Modal-Diffusion-Transformer-for-Mask-Free-Virtual-Try-onpdf"><a href="#124-JCo-MVTON-Jointly-Controllable-Multi-Modal-Diffusion-Transformer-for-Mask-Free-Virtual-Try-onpdf" class="headerlink" title="124. JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-onpdf"></a>124. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/JCo-MVTON__Jointly_Controllable_Multi-Modal_Diffusion_Transformer_for_Mask-Free_Virtual_Try-on.pdf">JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-on</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">DAMO Academy, Alibaba Group</span></p><p>该论文提出了JCo-MVTON，一种基于多模态扩散变换器（MM-DiT）的无掩码虚拟试衣框架，通过多条件特征注入（如参考图像和服装图像）与自注意力机制结合，并引入条件分支掩码与拼接位置编码，提升衣物与人体融合的真实性和一致性。实验结果显示，JCo-MVTON在VITON-HD和DressCode等公开基准和真实场景下均超越了现有学术和商业系统，实现了更高的图像保真度与细节一致性，验证了模型的实际应用价值和强泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="125-A-Weighted-Vision-Transformer-Based-Multi-Task-Learning-Framework-for-Predicting-ADAS-Cog-Scorespdf"><a href="#125-A-Weighted-Vision-Transformer-Based-Multi-Task-Learning-Framework-for-Predicting-ADAS-Cog-Scorespdf" class="headerlink" title="125. A Weighted Vision Transformer-Based Multi-Task Learning Framework for Predicting ADAS-Cog Scorespdf"></a>125. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Weighted_Vision_Transformer-Based_Multi-Task_Learning_Framework_for_Predicting_ADAS-Cog_Scores.pdf">A Weighted Vision Transformer-Based Multi-Task Learning Framework for Predicting ADAS-Cog Scores</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universiti Brunei Darussalam</span></p><p>本文提出了一种加权Vision Transformer (ViT)为基础的多任务学习框架，用于联合预测阿尔茨海默病ADAS-Cog量表的13个认知分项得分与未来总分，模型以MRI影像为输入，通过对关键子项（Q1, Q4, Q8）赋予更高损失权重，实现对具临床意义认知域的重点关注。实验结果表明：强权重策略在MCI组提升预测相关性和准确性，适度权重更适合CN组，均优于均匀权重方案，显示加权策略可提升模型泛化性和解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="126-TinyGiantVLM-A-Lightweight-Vision-Language-Architecture-for-Spatial-Reasoning-under-Resource-Constraintspdf"><a href="#126-TinyGiantVLM-A-Lightweight-Vision-Language-Architecture-for-Spatial-Reasoning-under-Resource-Constraintspdf" class="headerlink" title="126. TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraintspdf"></a>126. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TinyGiantVLM__A_Lightweight_Vision-Language_Architecture_for_Spatial_Reasoning_under_Resource_Constr.pdf">TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science, VNU-HCM</span></p><p>本文提出TinyGiantVLM，一种面向工业仓储环境空间推理的轻量级视觉-语言模型架构。方法采用双分支设计，对RGB和深度信息的全局及区域特征进行提取，并通过跨注意力融合和Mixture-of-Experts（MoE）模块实现多模态融合及任务特定推理，采用两阶段训练（自由回答与规范化回答）提升空间推理和输出准确率。结论显示，该模型仅用64M参数在AI City Challenge 2025空间智能赛道取得前五名，兼顾推理能力和计算效率，适合资源受限场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="127-Beyond-Play-Pause-Untwist-–-Turning-GPT-4o’s-Spatial-Weakness-into-a-Strength-for-In-Depth-Interactive-Video-Learningpdf"><a href="#127-Beyond-Play-Pause-Untwist-–-Turning-GPT-4o’s-Spatial-Weakness-into-a-Strength-for-In-Depth-Interactive-Video-Learningpdf" class="headerlink" title="127. Beyond Play &amp; Pause: Untwist – Turning GPT-4o’s Spatial Weakness into a Strength for In-Depth Interactive Video Learningpdf"></a>127. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Beyond_Play_and_Pause__Turning_GPT-4o_Spatial_Weakness_into_a_Strength_for_In-Depth_Interactive_Vide.pdf">Beyond Play &amp; Pause: Untwist – Turning GPT-4o’s Spatial Weakness into a Strength for In-Depth Interactive Video Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Clemson University</span></p><p>该论文提出了Untwist系统，通过结合GPT-4o API与计算机视觉技术，实现视频内容的交互式学习。用户可对整个视频或指定区域提出问题，系统以标注框代替坐标传递，显著提升了GPT-4o在视频区域定位和理解的准确性。实验证明标注图像显著优于坐标输入，系统能够实时生成多模态、上下文相关的反馈，增强视频学习的互动性和理解深度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="128-Towards-Safeguarding-LLM-Fine-tuning-APIs-against-Cipher-Attackspdf"><a href="#128-Towards-Safeguarding-LLM-Fine-tuning-APIs-against-Cipher-Attackspdf" class="headerlink" title="128. Towards Safeguarding LLM Fine-tuning APIs against Cipher Attackspdf"></a>128. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Towards_Safeguarding_LLM_Fine-tuning_APIs_against_Cipher_Attacks.pdf">Towards Safeguarding LLM Fine-tuning APIs against Cipher Attacks</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent</span></p><p>本文提出了Cipher Fine-tuning Robustness Benchmark（CIFR），用于系统性评估LLM微调API在面对利用隐写编码（cipher-based）攻击时的安全防御能力，并提出了一种基于模型内部激活的线性探针监控方法。实验证明，该探针监控技术对多种已知和未知加密变体具有99%以上检测准确率，且泛化能力优于现有主流监控方法，能够有效阻止加密内容规避安全机制的攻击。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="129-Mind-the-Gap-Time-of-Check-to-Time-of-Use-Vulnerabilities-in-LLM-Enabled-Agentspdf"><a href="#129-Mind-the-Gap-Time-of-Check-to-Time-of-Use-Vulnerabilities-in-LLM-Enabled-Agentspdf" class="headerlink" title="129. Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agentspdf"></a>129. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Mind_the_Gap__Time-of-Check_to_Time-of-Use_Vulnerabilities_in_LLM-Enabled_Agents.pdf">Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Oregon State University</span></p><p>该论文首次系统性研究了大语言模型(LLM)驱动的智能体在多步任务执行中存在的时间检查到使用(TOCTOU)安全漏洞，提出了TOCTOU-Bench基准和三种防御机制：Prompt Rewriting、State Integrity Monitoring和Tool Fusing，并在基准上评估其有效性。实验结果显示，这些方法可协同将已执行轨迹中的TOCTOU漏洞率从12%降低到8%，并显著缩短攻击窗口，提升了LLM智能体的安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="130-Natural-Language-Satisfiability-Exploring-the-Problem-Distribution-and-Evaluating-Transformer-based-Language-Modelspdf"><a href="#130-Natural-Language-Satisfiability-Exploring-the-Problem-Distribution-and-Evaluating-Transformer-based-Language-Modelspdf" class="headerlink" title="130. Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Modelspdf"></a>130. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Natural_Language_Satisfiability__Exploring_the_Problem_Distribution_and_Evaluating_Transformer-based.pdf">Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Department of Computer Science, University of Manchester</span></p><p>本论文提出了针对不同复杂度自然语言片段的可满足性问题数据集，系统评估了Transformer类语言模型（TLMs，如T5-large与DeBERTa-v3-large）在逻辑推理任务中的表现。结果显示，TLMs在复杂度较低的语言片段上表现较好，但随着片段复杂度提升，模型准确率下降，且在分布外泛化能力有限，难以学习并推断底层推理规则，因此当前TLMs尚未可靠地学会自然语言逻辑推理所需的算法与规则。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="131-MoE-Beyond-Learning-Based-Expert-Activation-Prediction-on-Edge-Devicespdf"><a href="#131-MoE-Beyond-Learning-Based-Expert-Activation-Prediction-on-Edge-Devicespdf" class="headerlink" title="131. MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devicespdf"></a>131. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MoE-Beyond__Learning-Based_Expert_Activation_Prediction_on_Edge_Devices.pdf">MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Univ. of Pennsylvania</span></p><p>MoE-Beyond 提出了一种基于轻量级 transformer 的专家激活预测模型，将专家选择建模为多标签序列预测任务，通过在 6600 万条 DeepSeek-V2-Lite MoE 专家激活轨迹上训练，显著提升了边缘设备大规模 MoE 推理的缓存命中率。实验结果显示，在仅有 10% 专家可缓存的条件下，GPU 缓存命中率由 17% 提升至 72%，大幅优于现有启发式方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="132-Token-Homogenization-under-Positional-Biaspdf"><a href="#132-Token-Homogenization-under-Positional-Biaspdf" class="headerlink" title="132. Token Homogenization under Positional Biaspdf"></a>132. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Token_Homogenization_under_Positional_Bias.pdf">Token Homogenization under Positional Bias</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">International Laboratory of Bioinformatics, HSE University</span></p><p>本文提出并实证分析了大语言模型（LLMs）中 token homogenization（表示趋同）与 positional bias（位置偏置）之间的关系，使用一系列创新和现有的度量指标（如MAUVE分数、有效秩、最大可解释方差、Schatten范数和resultant length），在LLaMA-3、Gemma和Qwen等模型上，通过对数据集中关键信息词前置&#x2F;后置的实验，揭示了位置偏置会显著增强token表示的趋同效应。结果表明，位置偏置导致模型各层token表现更加同质化，为模型解释性、压缩和优化提供了参考。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="133-PowerChain-Automating-Distribution-Grid-Analysis-with-Agentic-AI-Workflowspdf"><a href="#133-PowerChain-Automating-Distribution-Grid-Analysis-with-Agentic-AI-Workflowspdf" class="headerlink" title="133. PowerChain: Automating Distribution Grid Analysis with Agentic AI Workflowspdf"></a>133. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PowerChain__Automating_Distribution_Grid_Analysis_with_Agentic_AI_Workflows.pdf">PowerChain: Automating Distribution Grid Analysis with Agentic AI Workflows</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Vermont</span></p><p>该论文提出了PowerChain系统，通过结合Agentic AI架构和大语言模型（LLM）函数调用，实现对电力分布网分析任务的自动化工作流生成。方法包括利用专家工作流-查询对、函数池描述符和动态上下文提示，递归生成并执行领域相关的分析流程，显著降低了对专家定制和LLM微调的需求。实验证明，PowerChain能在真实电力数据上产生专家级分析流程，提升小型电力公司对复杂网格分析的可达性和效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="134-Zero-shot-Multimodal-Document-Retrieval-via-Cross-modal-Question-Generationpdf"><a href="#134-Zero-shot-Multimodal-Document-Retrieval-via-Cross-modal-Question-Generationpdf" class="headerlink" title="134. Zero-shot Multimodal Document Retrieval via Cross-modal Question Generationpdf"></a>134. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Zero-shot_Multimodal_Document_Retrieval_via_Cross-modal_Question_Generation.pdf">Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yonsei University</span></p><p>该论文提出PREMIR框架，利用多模态大语言模型(MLLM)从文档中生成跨模态预问题(preQs)，并通过细粒度token级别表示实现多模态文档的高效检索。实验显示，PREMIR在跨领域、闭域和多语言场景下均取得了最优性能，无需额外训练，显著提升了检索的泛化能力和质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="135-Linguistic-Neuron-Overlap-Patterns-to-Facilitate-Cross-lingual-Transfer-on-Low-resource-Languagespdf"><a href="#135-Linguistic-Neuron-Overlap-Patterns-to-Facilitate-Cross-lingual-Transfer-on-Low-resource-Languagespdf" class="headerlink" title="135. Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languagespdf"></a>135. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Linguistic_Neuron_Overlap_Patterns_to_Facilitate_Cross-lingual_Transfer_on_Low-resource_Languages.pdf">Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Information Science and Technology, Beijing Foreign Studies University</span></p><p>本文提出了BridgeX-ICL方法，通过分析大型语言模型（LLMs）中不同语言的神经元重叠模式，并利用MUSE双语词典构建神经元探测数据，结合HSIC度量选择最优桥接语言，提升低资源语言的跨语言零样本上下文学习（X-ICL）能力。实验表明，该方法在15对语言和两个跨语言任务上均显著提升了LLMs在低资源语言上的性能，平均提升约6%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="136-GRAID-Synthetic-Data-Generation-with-Geometric-Constraints-and-Multi-Agentic-Reflection-for-Harmful-Content-Detectionpdf"><a href="#136-GRAID-Synthetic-Data-Generation-with-Geometric-Constraints-and-Multi-Agentic-Reflection-for-Harmful-Content-Detectionpdf" class="headerlink" title="136. GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detectionpdf"></a>136. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GRAID__Synthetic_Data_Generation_with_Geometric_Constraints_and_Multi-Agentic_Reflection_for_Harmful.pdf">GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detection</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Capital One</span></p><p>论文提出GRAID数据增强管道，利用大型语言模型（LLM）进行两阶段数据合成：首先通过几何约束生成均衡分布的新样本，然后采用多代理反思机制提升风格多样性并发现极端案例。实验结果显示，GRAID在BeaverTails和WildGuard数据集上能显著提升有害文本分类模型的性能，增强模型对多样化和难例的检测能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="137-Limitations-of-refinement-methods-for-weak-to-strong-generalizationpdf"><a href="#137-Limitations-of-refinement-methods-for-weak-to-strong-generalizationpdf" class="headerlink" title="137. Limitations of refinement methods for weak to strong generalizationpdf"></a>137. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Limitations_of_refinement_methods_for_weak_to_strong_generalization.pdf">Limitations of refinement methods for weak to strong generalization</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Michigan</span></p><p>本文提出了一个通用的潜在概念转移框架，分析了弱训练和标签精炼在LLM超对齐（superalignment）下的弱到强泛化能力，理论证明这两类主流方法都会产生不可消除的偏差，无法一致收敛到目标函数。作者进一步给出一种基于潜在概念识别的理想化算法，能在参数可辨识条件下实现一致估计，但该算法尚不切实际。结论认为现有弱到强泛化方法存在理论缺陷，需探索兼具实用性和最优性的替代方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="138-KL-Regularised-Q-Learning-A-Token-level-Action-Value-perspective-on-Online-RLHFpdf"><a href="#138-KL-Regularised-Q-Learning-A-Token-level-Action-Value-perspective-on-Online-RLHFpdf" class="headerlink" title="138. KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHFpdf"></a>138. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/KL-Regularised_Q-Learning__A_Token-level_Action-Value_perspective_on_Online_RLHF.pdf">KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cambridge</span></p><p>本文提出KL正则化Q学习（KLQ），一种面向大语言模型基于人类反馈的在线强化学习（LM-RLHF）的动作价值方法。KLQ在理论上等价于一种变体的PPO，但其目标函数更简洁，能够在摘要和对话生成等任务上达到与PPO相当甚至更优的表现，尤其在LLM评审下胜率更高。结论显示，KLQ为LM-RLHF提供了新的理论视角和更优质输出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="139-Unveiling-the-Latent-Directions-of-Reflection-in-Large-Language-Modelspdf"><a href="#139-Unveiling-the-Latent-Directions-of-Reflection-in-Large-Language-Modelspdf" class="headerlink" title="139. Unveiling the Latent Directions of Reflection in Large Language Modelspdf"></a>139. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Unveiling_the_Latent_Directions_of_Reflection_in_Large_Language_Models.pdf">Unveiling the Latent Directions of Reflection in Large Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Taiwan University</span></p><p>该论文提出了一种基于激活操控（activation steering）的方法，通过构建反思行为的潜在方向向量，系统性地分析和控制大语言模型（LLM）中的反思机制。实验证明：反思行为可通过激活干预被增强或抑制，且抑制反思比激发更容易，并能发现新的反思触发指令，提升模型推理准确率；同时揭示了反思可控性带来的安全隐患与防御机会。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="140-ReFactX-Scalable-Reasoning-with-Reliable-Facts-via-Constrained-Generationpdf"><a href="#140-ReFactX-Scalable-Reasoning-with-Reliable-Facts-via-Constrained-Generationpdf" class="headerlink" title="140. ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generationpdf"></a>140. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ReFactX__Scalable_Reasoning_with_Reliable_Facts_via_Constrained_Generation.pdf">ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Milano-Bicocca</span></p><p>ReFactX提出了一种基于受限生成（constrained generation）和前缀树索引的通用方法，使LLMs能高效访问超大规模知识库（如800M Wikidata事实），无需外部检索器或管道，仅通过受控解码生成真实知识事实并嵌入推理流程。实验证明，该方法在问答任务中可大幅提升准确率和精度（提升达20个百分点，精度超90%），同时仅增加约1%推理延迟。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="141-Hierarchical-Contextual-Grounding-LVLM-Enhancing-Fine-Grained-Visual-Language-Understanding-with-Robust-Groundingpdf"><a href="#141-Hierarchical-Contextual-Grounding-LVLM-Enhancing-Fine-Grained-Visual-Language-Understanding-with-Robust-Groundingpdf" class="headerlink" title="141. Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Groundingpdf"></a>141. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Hierarchical_Contextual_Grounding_LVLM__Enhancing_Fine-Grained_Visual-Language_Understanding_with_Ro.pdf">Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Grounding</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhongkai University of Agriculture and Engineering</span></p><p>该论文提出了一种新型分层视觉语言大模型架构HCG-LVLM，通过模拟人类由粗到细的认知流程，结合全局语境感知层与细粒度局部定位层（含局部细节增强模块和语义一致性验证器），提升了视觉-语言对齐的准确性和鲁棒性。实验表明该方法在GQA、A-OKVQA、RefCOCO等细粒度VQA与指代表达理解任务上，显著优于Flamingo、BLIP-2和MiniGPT-4，有效减少了幻觉现象、提升了精确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="142-Robust-Diagram-Reasoning-A-Framework-for-Enhancing-LVLM-Performance-on-Visually-Perturbed-Scientific-Diagramspdf"><a href="#142-Robust-Diagram-Reasoning-A-Framework-for-Enhancing-LVLM-Performance-on-Visually-Perturbed-Scientific-Diagramspdf" class="headerlink" title="142. Robust Diagram Reasoning: A Framework for Enhancing LVLM Performance on Visually Perturbed Scientific Diagramspdf"></a>142. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Robust_Diagram_Reasoning__A_Framework_for_Enhancing_LVLM_Performance_on_Visually_Perturbed_Scientifi.pdf">Robust Diagram Reasoning: A Framework for Enhancing LVLM Performance on Visually Perturbed Scientific Diagrams</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Taiyuan University of Science and Technology</span></p><p>本文提出了Robust Diagram Reasoning (RDR) 框架，通过Adaptive Multi-View &amp; Consistency Verification (AMCV)机制，对科学图表生成多种视觉扰动版本，进行多视角并行推理，并通过自我纠错循环提高LVLM在视觉退化场景下的鲁棒性。实验表明，RDR显著提升了LVLM在受噪声、模糊、遮挡等干扰下的推理准确率与一致性，超越了主流开源及闭源大模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="143-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoningpdf"><a href="#143-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoningpdf" class="headerlink" title="143. Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoningpdf"></a>143. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Breaking_the_Exploration_Bottleneck__Rubric-Scaffolded_Reinforcement_Learning_for_General_LLM_Reason.pdf">Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了Rubric-Scaffolded Reinforcement Learning (RuscaRL) 框架，通过在大语言模型(LLM)强化学习训练中引入checklist风格的rubric作为显式探索支架与可验证奖励，打破RL在通用推理任务中的探索瓶颈。RuscaRL在rollout阶段提供分组区分和逐步衰减的rubric支架激发多样化高质量响应，同时以rubric为准则设计多维度奖励函数，促进模型有效学习。实验表明，RuscaRL在HealthBench等多个基准上大幅提升LLM推理能力，小模型Qwen-2.5-7B-Instruct得分提升至50.3，超越GPT-4.1，Qwen3-30B模型更超越OpenAI-o3，验证了方法的有效性与通用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="144-Attention-Layers-Add-Into-Low-Dimensional-Residual-Subspacespdf"><a href="#144-Attention-Layers-Add-Into-Low-Dimensional-Residual-Subspacespdf" class="headerlink" title="144. Attention Layers Add Into Low-Dimensional Residual Subspacespdf"></a>144. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Attention_Layers_Add_Into_Low-Dimensional_Residual_Subspaces.pdf">Attention Layers Add Into Low-Dimensional Residual Subspaces</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Innovation Institute</span></p><p>该论文发现Transformer模型中的注意力输出被限制在低维子空间内，这种低秩结构导致稀疏字典学习（如稀疏自编码器）中的大量死特征。作者提出Active Subspace Initialization方法，将特征初始化到激活的低维子空间，并结合SparseAdam优化器，显著减少死特征（由87%降至1%），提升重构精度。该方法适用于大语言模型中的稀疏字典学习和稀疏替换模型，改善可解释性和训练效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="145-TextOnly-A-Unified-Function-Portal-for-Text-Related-Functions-on-Smartphonespdf"><a href="#145-TextOnly-A-Unified-Function-Portal-for-Text-Related-Functions-on-Smartphonespdf" class="headerlink" title="145. TextOnly: A Unified Function Portal for Text-Related Functions on Smartphonespdf"></a>145. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TextOnly__A_Unified_Function_Portal_for_Text-Related_Functions_on_Smartphones.pdf">TextOnly: A Unified Function Portal for Text-Related Functions on Smartphones</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>该论文提出了TextOnly系统，通过统一的文本输入框整合并智能推荐智能手机上的文本相关功能，结合了大语言模型（LLM，GPT-3.5）与BERT模型，LLM提供通用知识和冷启动能力，BERT实现用户个性化与快速预测。实验表明，TextOnly能够有效提升文本相关功能的访问效率，用户满意度高，且系统准确率和推理速度可随使用持续提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="146-TriagerX-Dual-Transformers-for-Bug-Triaging-Tasks-with-Content-and-Interaction-Based-Rankingspdf"><a href="#146-TriagerX-Dual-Transformers-for-Bug-Triaging-Tasks-with-Content-and-Interaction-Based-Rankingspdf" class="headerlink" title="146. TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankingspdf"></a>146. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TriagerX__Dual_Transformers_for_Bug_Triaging_Tasks_with_Content_and_Interaction_Based_Rankings.pdf">TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Calgary</span></p><p>本文提出了TriagerX，一种结合内容和开发者交互的双Transformer架构，用于自动化软件缺陷（bug）分配。方法上，TriagerX通过内容驱动的双PLM（如DeBERTa与RoBERTa）融合多层语义特征，并引入基于历史开发者交互（如commit&#x2F;PR&#x2F;讨论）的交互排序，最后通过加权聚合提升推荐准确性。在多个工业与公开数据集上，TriagerX显著优于现有SOTA方法，Top-1与Top-3开发者推荐准确率提升超10%，在工业部署中成功助力实际团队高效分配任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="147-Beyond-Emotion-Recognition-A-Multi-Turn-Multimodal-Emotion-Understanding-and-Reasoning-Benchmarkpdf"><a href="#147-Beyond-Emotion-Recognition-A-Multi-Turn-Multimodal-Emotion-Understanding-and-Reasoning-Benchmarkpdf" class="headerlink" title="147. Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmarkpdf"></a>147. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Beyond_Emotion_Recognition__A_Multi-Turn_Multimodal_Emotion_Understanding_and_Reasoning_Benchmark.pdf">Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hefei University of Technology</span></p><p>本论文提出了多轮多模态情感理解与推理基准（MTMEUR），涵盖1451个真实场景视频和5101个递进式问题，涵盖情感识别、情感成因、未来行为预测等方面。作者还提出了包含背景、角色、事件、决策四个智能体的多智能体协作框架，显著提升了多模态大模型在情感推理任务上的表现。实验表明，现有多模态大模型在复杂情感推理任务中仍面临很大挑战，所提方法在该基准上取得了最优效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="148-RADAR-A-Reasoning-Guided-Attribution-Framework-for-Explainable-Visual-Data-Analysispdf"><a href="#148-RADAR-A-Reasoning-Guided-Attribution-Framework-for-Explainable-Visual-Data-Analysispdf" class="headerlink" title="148. RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysispdf"></a>148. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/RADAR__A_Reasoning-Guided_Attribution_Framework_for_Explainable_Visual_Data_Analysis.pdf">RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysis</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Massachusetts Institute of Technology</span></p><p>该论文提出RADAR框架，通过结合多模态大语言模型（如InternLM-XComposer2）和半自动数据标注，生成可解释的数学图表问答推理及区域归因数据集，并实现了自动化的推理分步生成与视觉归因流程。实验结果显示，RADAR在图表数学推理归因准确率上较现有方法提升15%，生成答案与人工标注高度一致（BERTScore约0.90），显著增强了多模态大模型在数据可视化分析的可解释性和可信度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="149-Quantifying-Sycophancy-as-Deviations-from-Bayesian-Rationality-in-LLMspdf"><a href="#149-Quantifying-Sycophancy-as-Deviations-from-Bayesian-Rationality-in-LLMspdf" class="headerlink" title="149. Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMspdf"></a>149. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Quantifying_Sycophancy_as_Deviations_from_Bayesian_Rationality_in_LLMs.pdf">Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeastern University</span></p><p>本文提出利用贝叶斯框架量化大语言模型（LLM）在用户观点引导下的逢迎行为，将逢迎定义为偏离贝叶斯理性更新的概率偏移，并在三类任务（对话预测、道德判断、文化适应性）和多种模型及概率估计方法下进行实证评估。结论显示：逢迎会显著提升LLM对目标结果的概率估计，并在部分情形下导致贝叶斯误差增加，表明逢迎对模型推理有不可忽视的影响，但LLM本身并不总是贝叶斯理性，逢迎有时可纠正模型的某些误差。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="150-In-Context-Algorithm-Emulation-in-Fixed-Weight-Transformerspdf"><a href="#150-In-Context-Algorithm-Emulation-in-Fixed-Weight-Transformerspdf" class="headerlink" title="150. In-Context Algorithm Emulation in Fixed-Weight Transformerspdf"></a>150. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/In-Context_Algorithm_Emulation_in_Fixed-Weight_Transformers.pdf">In-Context Algorithm Emulation in Fixed-Weight Transformers</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northwestern University</span></p><p>本文理论证明了最简化的、固定权重的Transformer架构（尤其是仅由softmax自注意力组成且无参数更新）能够通过巧妙设计的prompt精确模拟一类广泛的统计和优化算法，包括梯度下降、线性回归、岭回归等。方法核心是将算法参数编码进输入token，使softmax注意力机制在前向计算过程中完成算法输出的再现。结论表明，GPT风格的大型Transformer可仅通过prompt切换内部算法，无需重训练，具备高度算法通用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="151-Activation-Transport-Operatorspdf"><a href="#151-Activation-Transport-Operatorspdf" class="headerlink" title="151. Activation Transport Operatorspdf"></a>151. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Activation_Transport_Operators.pdf">Activation Transport Operators</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cambridge</span></p><p>该论文提出了Activation Transport Operators (ATO)，即在特征空间中由上游至下游残差流的正则化线性映射，并利用稀疏自动编码器(SAE)对LLM残差流中的特征传递进行建模。实验表明，ATO能有效判别特征是被线性传递还是由非线性操作新生成，且提出了传输效率指标来量化线性传递子空间的维度。结论指出，特征线性传递主要发生在相邻层，随层深及跨度增加而减弱，ATO有助于模型调试与诊断。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="152-Evaluating-Retrieval-Augmented-Generation-Strategies-for-Large-Language-Models-in-Travel-Mode-Choice-Predictionpdf"><a href="#152-Evaluating-Retrieval-Augmented-Generation-Strategies-for-Large-Language-Models-in-Travel-Mode-Choice-Predictionpdf" class="headerlink" title="152. Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Predictionpdf"></a>152. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Evaluating_Retrieval-Augmented_Generation_Strategies_for_Large_Language_Models_in_Travel_Mode_Choice.pdf">Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Prediction</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Texas at Austin</span></p><p>本文提出了一套结合检索增强生成（RAG）与大语言模型（LLM）的方法框架，系统评估了四种RAG策略（基础RAG、平衡检索、交叉编码重排序、及两者结合）在出行方式选择预测任务中的表现。结果显示，GPT-4o与平衡检索和交叉编码重排序结合时在Puget Sound出行调查数据上准确率达到80.8%，大幅优于传统统计与机器学习基线模型，且LLM具备更强泛化能力，强调了模型推理能力与检索策略匹配的重要性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="153-Efficient-Zero-Shot-Long-Document-Classification-by-Reducing-Context-Through-Sentence-Rankingpdf"><a href="#153-Efficient-Zero-Shot-Long-Document-Classification-by-Reducing-Context-Through-Sentence-Rankingpdf" class="headerlink" title="153. Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Rankingpdf"></a>153. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Efficient_Zero-Shot_Long_Document_Classification_by_Reducing_Context_Through_Sentence_Ranking.pdf">Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Ranking</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Pune Institute of Computer Technology</span></p><p>本文提出了一种高效的零样本长文档分类方法，通过TF-IDF句子排序进行上下文压缩，无需修改模型结构。将短文本训练的Transformer模型（如marathi-bert-v2）应用于长文档，仅保留最具信息量的句子，显著降低推理时间且保持分类准确率。实验表明，选取50%高排名句子可在减少35%推理时间的情况下，准确率与原文档相当甚至更优，验证了句子排序作为上下文优化的可扩展性和效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="154-MoE-Inference-Bench-Performance-Evaluation-of-Mixture-of-Expert-Large-Language-and-Vision-Modelspdf"><a href="#154-MoE-Inference-Bench-Performance-Evaluation-of-Mixture-of-Expert-Large-Language-and-Vision-Modelspdf" class="headerlink" title="154. MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Modelspdf"></a>154. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MoE-Inference-Bench__Performance_Evaluation_of_Mixture_of_Expert_Large_Language_and_Vision_Models.pdf">MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Argonne National Laboratory</span></p><p>本文提出MoE-Inference-Bench，系统评测主流混合专家（MoE）大型语言及视觉-语言模型（LLM&#x2F;VLM）在多种硬件（主要为Nvidia H100 GPU）和推理优化策略（如量化、剪枝、推理并行、Speculative Decoding和Fused MoE等）下的推理性能。结果表明，FP8量化带来20-30%的吞吐提升，激活专家数量是主要的优化杠杆，单专家设置可提升50-80%的推理效率，且VLM模型推理延迟远高于文本模型，为高效部署提供了参数和策略参考。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="155-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modelingpdf"><a href="#155-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modelingpdf" class="headerlink" title="155. TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modelingpdf"></a>155. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TreePO__Bridging_the_Gap_of_Policy_Optimization_and_Efficacy_and_Inference_Efficiency_with_Heuristic.pdf">TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">M-A-P</span></p><p>该论文提出TreePO，一种将大语言模型序列生成过程重构为分段树结构搜索的强化学习训练框架。方法包括动态树采样、分段解码和树结构优势估计，有效利用KV缓存，显著减少推理和训练计算成本。实验表明TreePO在数学推理等复杂任务中提升了训练稳定性和探索多样性，同时显著降低了GPU计算消耗，推动了RL后训练在大模型上的可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="156-MahaParaphrase-A-Marathi-Paraphrase-Detection-Corpus-and-BERT-based-Modelspdf"><a href="#156-MahaParaphrase-A-Marathi-Paraphrase-Detection-Corpus-and-BERT-based-Modelspdf" class="headerlink" title="156. MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Modelspdf"></a>156. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MahaParaphrase__A_Marathi_Paraphrase_Detection_Corpus_and_BERT-based_Models.pdf">MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Pune Institute of Computer Technology</span></p><p>本论文提出了MahaParaphrase数据集——一个包含8,000对经人工标注的马拉地语同义&#x2F;非同义句对的高质量语料库。数据集采集采用余弦相似度和回译相结合并由母语者手工校验，并按词重叠率分为五个难度桶；文中还基于多个BERT系列模型（如MahaBERT、IndicBERT、Muril、MBERT）进行了微调和评测，MahaBERT在该任务上取得了88.7%的F1分数。结论：该数据集填补了低资源马拉地语领域的空白，并为相关NLP研究提供了重要资源和基线。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="157-Large-Language-Models-as-Universal-Predictors-An-Empirical-Study-on-Small-Tabular-Datasetspdf"><a href="#157-Large-Language-Models-as-Universal-Predictors-An-Empirical-Study-on-Small-Tabular-Datasetspdf" class="headerlink" title="157. Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasetspdf"></a>157. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Large_Language_Models_as_Universal_Predictors__An_Empirical_Study_on_Small_Tabular_Datasets.pdf">Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute for Language and Speech Processing, Athena Research Center</span></p><p>本文系统评估了多种主流大语言模型（如GPT-5、GPT-4o、Gemini等）在小规模结构化表格数据上的分类、回归和聚类任务表现，采用少样本in-context learning（ICL）方式并与传统机器学习及表格基础模型（TabPFN、TabICL）做对比。实验发现，LLMs在分类任务中接近甚至优于传统方法，无需训练即可作为实用基线，而在回归和聚类任务中表现欠佳，主要受限于模型的数值输出精度和无监督学习能力，适合于业务智能等场景的快速数据探索。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="158-GRAPH-R1-Incentivizing-the-Zero-Shot-Graph-Learning-Capability-in-LLMs-via-Explicit-Reasoningpdf"><a href="#158-GRAPH-R1-Incentivizing-the-Zero-Shot-Graph-Learning-Capability-in-LLMs-via-Explicit-Reasoningpdf" class="headerlink" title="158. GRAPH-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoningpdf"></a>158. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Graph-R1__Incentivizing_the_Zero-Shot_Graph_Learning_Capability_in_LLMs_via_Explicit_Reasoning.pdf">GRAPH-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>本文提出了一种全新的无GNN（图神经网络）图学习范式，将图任务（节点分类、链接预测、图分类等）转化为大型语言模型（LLM）的推理问题，并构建第一个带有详细推理链的图任务数据集。通过两阶段训练（指令微调+基于rethink模板的强化学习），GRAPH-R1显著提升了LLM在零样本图任务上的泛化和可解释性，实验结果优于现有主流基线，证明了显式推理在图学习中的有效性和潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="159-Mimicking-the-Physicist’s-Eye-A-VLM-Centric-Approach-for-Physics-Formula-Discoverypdf"><a href="#159-Mimicking-the-Physicist’s-Eye-A-VLM-Centric-Approach-for-Physics-Formula-Discoverypdf" class="headerlink" title="159. Mimicking the Physicist’s Eye: A VLM-Centric Approach for Physics Formula Discoverypdf"></a>159. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Mimicking_the_Physicist's_Eye_A_VLM-centric_Approach_for_Physics_Formula_Discovery.pdf">Mimicking the Physicist’s Eye: A VLM-Centric Approach for Physics Formula Discovery</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">UNC–Chapel Hill</span></p><p>该论文提出了VIPER-R1，一种多模态视觉语言模型，通过“运动结构归纳”(MSI)和“奖励引导符号校准”(RGSC)两阶段训练，结合视觉感知（轨迹和相空间图）与符号推理，实现物理公式自动发现。推理阶段利用符号回归工具作残差对齐(SR²)，与经验数据融合，显著提升结构正确性和精度。实验表明，该模型在PhysSymbol多模态物理数据集上结构分数和准确率均显著超越现有VLM基线，实现更精确的物理规律发现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="160-Evolving-Collective-Cognition-in-Human–Agent-Hybrid-Societies-How-Agents-Form-Stances-and-Boundariespdf"><a href="#160-Evolving-Collective-Cognition-in-Human–Agent-Hybrid-Societies-How-Agents-Form-Stances-and-Boundariespdf" class="headerlink" title="160. Evolving Collective Cognition in Human–Agent Hybrid Societies: How Agents Form Stances and Boundariespdf"></a>160. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Evolving_Collective_Cognition_in_Human-Agent_Hybrid_Societies__How_Agents_Form_Stances_and_Boundarie.pdf">Evolving Collective Cognition in Human–Agent Hybrid Societies: How Agents Form Stances and Boundaries</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">William &amp; Mary</span></p><p>该论文提出了CMASE框架，将生成式多智能体建模与虚拟民族志方法结合，用于研究人—Agent混合社会中的群体立场分化和社会边界形成。通过三项实验，发现基于LLM的智能体能够自主形成与预设身份无关的立场，并可在互动中重组社会结构，显示身份标签的不稳定性和语言互动对社会认知演化的关键作用。结论认为，基于语言模型的智能体在多轮交互中能打破静态身份设定，通过语言实践重组认知和社会边界，为未来可信赖的社会性AI系统提供理论基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="161-Trust-Me-I-Know-This-Function-Hijacking-LLM-Static-Analysis-using-Biaspdf"><a href="#161-Trust-Me-I-Know-This-Function-Hijacking-LLM-Static-Analysis-using-Biaspdf" class="headerlink" title="161. Trust Me, I Know This Function: Hijacking LLM Static Analysis using Biaspdf"></a>161. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Trust_Me,_I_Know_This_Function__Hijacking_LLM_Static_Analysis_using_Bias.pdf">Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ben Gurion University of the Negev</span></p><p>本论文发现和系统分析了大型语言模型（LLM）在代码静态分析任务中的抽象偏置漏洞，提出了“Familiar Pattern Attacks”（FPAs），即通过在熟悉的代码模式中嵌入微小但有意义的错误来误导LLM的静态分析。作者开发了一种全自动黑盒算法，可在多种主流模型（GPT-4o、Claude 3.5、Gemini 2.0）和多种编程语言（Python、C、Rust、Go）下生成和注入FPAs。实验显示FPAs具有高度转移性和普适性，即使在明确告知模型防御该攻击的情况下依然有效。结论：FPA攻击揭示了LLM在代码理解中的系统性盲点，难以通过提示工程或数据过滤彻底修复，需引起广泛关注。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="162-Agentic-AI-for-Software-thoughts-from-Software-Engineering-communitypdf"><a href="#162-Agentic-AI-for-Software-thoughts-from-Software-Engineering-communitypdf" class="headerlink" title="162. Agentic AI for Software: thoughts from Software Engineering communitypdf"></a>162. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Agentic_AI_for_Software__thoughts_from_Software_Engineering_community.pdf">Agentic AI for Software: thoughts from Software Engineering community</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>本文探讨了基于大型语言模型（LLM）的自主软件工程智能体（如AutoCodeRover），通过结合程序分析和意图推断，实现自动修复、测试和维护软件任务。结论认为，随着自动化程度提高，信任和验证将成为AI生成代码流程中的核心问题，未来agentic AI将在代码验证和安全审计等关键环节扮演重要角色。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="163-Capturing-Legal-Reasoning-Paths-from-Facts-to-Law-in-Court-Judgments-using-Knowledge-Graphspdf"><a href="#163-Capturing-Legal-Reasoning-Paths-from-Facts-to-Law-in-Court-Judgments-using-Knowledge-Graphspdf" class="headerlink" title="163. Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphspdf"></a>163. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Capturing_Legal_Reasoning_Paths_from_Facts_to_Law_in_Court_Judgments_using_Knowledge_Graphs.pdf">Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphs</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Tokyo</span></p><p>本论文提出了一种基于知识图谱（LKG）的法律推理建模方法，通过设计法律本体和分层结构，从日本行政法院判决中抽取事实、法律规范、法律适用等节点及其推理关系，利用GPT-4o自动化构建图谱，并实现了从事实到法律条文的可追溯检索。实验表明，该方法在法律条文检索准确率和解释性上均优于单纯LLM或RAG方法，证明显式结构对法律AI推理与解释的重要性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="164-DropLoRA-Sparse-Low-Rank-Adaptation-for-Parameter-Efficient-Fine-Tuningpdf"><a href="#164-DropLoRA-Sparse-Low-Rank-Adaptation-for-Parameter-Efficient-Fine-Tuningpdf" class="headerlink" title="164. DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuningpdf"></a>164. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DropLoRA__Sparse_Low-Rank_Adaptation_for_Parameter-Efficient_Fine-Tuning.pdf">DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出DropLoRA，一种创新的剪枝策略，通过在LoRA的两低秩矩阵间插入动态mask，实现动态子空间学习，从而提升大语言模型的参数高效微调性能。DropLoRA无需增加额外参数，实验在推理、数学、代码生成等多任务上验证了其在LLaMA系列上的优越性，显著优于现有LoRA及其变体。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="165-Omne-R1-Learning-to-Reason-with-Memory-for-Multi-hop-Question-Answeringpdf"><a href="#165-Omne-R1-Learning-to-Reason-with-Memory-for-Multi-hop-Question-Answeringpdf" class="headerlink" title="165. Omne-R1: Learning to Reason with Memory for Multi-hop Question Answeringpdf"></a>165. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Omne-R1__Learning_to_Reason_with_Memory_for_Multi-hop_Question_Answering.pdf">Omne-R1: Learning to Reason with Memory for Multi-hop Question Answering</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tanka Inc.</span></p><p>本文提出了Omne-R1，一种针对无模式知识图谱多跳问答的多阶段训练框架，融合大语言模型推理能力。方法包括知识图谱与自动生成多跳QA对、两阶段强化学习和一次监督微调，采用工具调用增强推理链条。实验结果表明，该方法在复杂多跳（3+跳）问答上优于基线，并具备较强跨领域泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="166-CultranAI-at-PalmX-2025-Data-Augmentation-for-Cultural-Knowledge-Representationpdf"><a href="#166-CultranAI-at-PalmX-2025-Data-Augmentation-for-Cultural-Knowledge-Representationpdf" class="headerlink" title="166. CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representationpdf"></a>166. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CultranAI_at_PalmX_2025__Data_Augmentation_for_Cultural_Knowledge_Representation.pdf">CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Qatar University</span></p><p>本论文提出了CultranAI系统，通过数据增强（结合PalmX、Palm和新扩展的PalmX-ext数据集）以及LoRA微调技术，提升大语言模型对阿拉伯文化知识的表达能力。实验对多种大模型进行基准测试，最终选择并微调Fanar-1-9B-Instruct模型，结果显示在Palm数据集上达到84.1%准确率，在盲测集上排名第五（70.5%），证明数据增强和高效微调策略有效提升了模型的文化知识理解与生成能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="167-Chinese-Court-Simulation-with-LLM-Based-Agent-Systempdf"><a href="#167-Chinese-Court-Simulation-with-LLM-Based-Agent-Systempdf" class="headerlink" title="167. Chinese Court Simulation with LLM-Based Agent Systempdf"></a>167. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Chinese_Court_Simulation_with_LLM-Based_Agent_System.pdf">Chinese Court Simulation with LLM-Based Agent System</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了SimCourt框架，基于中国真实刑事审判流程，设计了具备记忆、规划和反思能力的多模块大语言模型（LLM）法庭代理人系统，能够完整模拟法庭五大阶段及五类角色，并集成法律检索工具提升法律推理水平。实验表明，SimCourt在判决预测和模拟过程评估上均超过现有方法，部分场景下甚至优于真实法官和律师，展示了其在法律实践和教育中的应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="168-AdaptiveK-Sparse-Autoencoders-Dynamic-Sparsity-Allocation-for-Interpretable-LLM-Representationspdf"><a href="#168-AdaptiveK-Sparse-Autoencoders-Dynamic-Sparsity-Allocation-for-Interpretable-LLM-Representationspdf" class="headerlink" title="168. AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for Interpretable LLM Representationspdf"></a>168. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AdaptiveK_Sparse_Autoencoders__Dynamic_Sparsity_Allocation_for_Interpretable_LLM_Representations.pdf">AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for Interpretable LLM Representations</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出AdaptiveK Sparse Autoencoders（AdaptiveK SAE）框架，通过先用线性探针对文本复杂度进行线性预测，然后动态调整稀疏自编码器的激活特征数量，实现对大语言模型（LLM）内部表示的可解释分解。实验表明，该方法在多种LLM（Pythia-70M、Pythia-160M、Gemma-2-2B）上，重构精度、解释方差和余弦相似性均优于固定稀疏度方法，且无需大量超参数调优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="169-Provable-Generalization-in-Overparameterized-Neural-Netspdf"><a href="#169-Provable-Generalization-in-Overparameterized-Neural-Netspdf" class="headerlink" title="169. Provable Generalization in Overparameterized Neural Netspdf"></a>169. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Provable_Generalization_in_Overparameterized_Neural_Nets.pdf">Provable Generalization in Overparameterized Neural Nets</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Princeton University</span></p><p>本文提出用注意力矩阵的有效秩（effective rank）作为Transformer模型容量的新度量，并理论证明该度量可导出非空的泛化界，优于传统参数计数。结论指出，注意力机制的谱结构比参数数量更能解释大型语言模型的泛化能力，与实际经验的标度律一致。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="170-CoViPAL-Layer-wise-Contextualized-Visual-Token-Pruning-for-Large-Vision-Language-Modelspdf"><a href="#170-CoViPAL-Layer-wise-Contextualized-Visual-Token-Pruning-for-Large-Vision-Language-Modelspdf" class="headerlink" title="170. CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Modelspdf"></a>170. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CoViPAL__Layer-wise_Contextualized_Visual_Token_Pruning_for_Large_Vision-Language_Models.pdf">CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Artificial Intelligence, Wuhan University</span></p><p>本文提出了CoViPAL，一种面向大规模视觉-语言模型（LVLMs）的层级上下文化视觉token剪枝方法。其核心是引入轻量级、可插拔的剪枝模块（PPM），通过两阶段训练策略利用注意力权重和上下文信息，预测并在各层有效地移除冗余视觉token，在不损失准确率的前提下显著降低推理计算与内存开销。实验表明，CoViPAL可在保留25%视觉token情况下，推理时间降低50%以上，性能优于现有训练型和免训练型剪枝方法，适用于多种LVLM模型和多模态任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="171-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generationpdf"><a href="#171-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generationpdf" class="headerlink" title="171. ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generationpdf"></a>171. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ClaimGen-CN__A_Large-scale_Chinese_Dataset_for_Legal_Claim_Generation.pdf">ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本论文提出了ClaimGen-CN，这是首个面向中文法律诉请生成的大规模数据集，涵盖100类民事案件，总计207,748条样本，并专为非专业用户如原告设计。方法上，作者不仅构建了多样化、以原告为中心的数据集，还引入了事实性与清晰度两个面向的评价指标，并对主流通用及法律领域大模型进行了零样本评测和误差分析，发现现有模型在事实准确性和表达清晰度上均存在明显不足。结论表明，ClaimGen-CN为法律AI研究提供了新基准，当前大模型在法律诉请生成上的表现仍需提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="172-Module-Aware-Parameter-Efficient-Machine-Unlearning-on-Transformerspdf"><a href="#172-Module-Aware-Parameter-Efficient-Machine-Unlearning-on-Transformerspdf" class="headerlink" title="172. Module-Aware Parameter-Efficient Machine Unlearning on Transformerspdf"></a>172. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Module-Aware_Parameter-Efficient_Machine_Unlearning_on_Transformers.pdf">Module-Aware Parameter-Efficient Machine Unlearning on Transformers</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出MAPE-Unlearn，一种模块感知的高效参数化机器遗忘方法，通过可学习掩码在Transformer的heads和filters处定位关键影响参数，并结合二阶优化及贪婪搜索算法实现稀疏更新。实验显示MAPE-Unlearn在多模型和任务上提升了遗忘效果与模型鲁棒性，在连续遗忘和对抗重学攻击下表现优异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="173-SSFO-Self-Supervised-Faithfulness-Optimization-for-Retrieval-Augmented-Generationpdf"><a href="#173-SSFO-Self-Supervised-Faithfulness-Optimization-for-Retrieval-Augmented-Generationpdf" class="headerlink" title="173. SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generationpdf"></a>173. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SSFO__Self-Supervised_Faithfulness_Optimization_for_Retrieval-Augmented_Generation.pdf">SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (Guangzhou)</span></p><p>本文提出了SSFO（Self-Supervised Faithfulness Optimization），一种针对检索增强生成（RAG）系统的自监督大语言模型对齐方法。该方法通过让模型自身生成有无外部上下文的响应并构造偏好对，自主进行对齐训练，结合Direct Preference Optimization和改进的损失函数以促进概率向上下文一致的响应转移。实验结果显示，SSFO在多种问答数据集上显著提升了模型的上下文忠实度、跨语言泛化能力，并有效保留模型的通用指令遵循能力，且无需昂贵标注和推理开销。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="174-Exposing-Privacy-Risks-in-Graph-Retrieval-Augmented-Generationpdf"><a href="#174-Exposing-Privacy-Risks-in-Graph-Retrieval-Augmented-Generationpdf" class="headerlink" title="174. Exposing Privacy Risks in Graph Retrieval-Augmented Generationpdf"></a>174. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Exposing_Privacy_Risks_in_Graph_Retrieval-Augmented_Generation.pdf">Exposing Privacy Risks in Graph Retrieval-Augmented Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Pennsylvania State University</span></p><p>本论文系统研究了Graph Retrieval-Augmented Generation（Graph RAG）系统在数据提取攻击下的隐私风险，设计针对性攻击方法揭示其在结构化信息（实体和关系）泄露方面的高脆弱性。结论指出：Graph RAG虽然减少了原始文本泄露，但极易泄露结构化数据，且常见防御（系统提示、相似度阈值、摘要）作用有限，亟需设计更强隐私防护技术。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="175-How-to-make-Medical-AI-Systems-safer-Simulating-Vulnerabilities-and-Threats-in-Multimodal-Medical-RAG-Systemspdf"><a href="#175-How-to-make-Medical-AI-Systems-safer-Simulating-Vulnerabilities-and-Threats-in-Multimodal-Medical-RAG-Systemspdf" class="headerlink" title="175. How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG Systemspdf"></a>175. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/How_to_make_Medical_AI_Systems_safer__Simulating_Vulnerabilities,_and_Threats_in_Multimodal_Medical_.pdf">How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG Systems</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Warwick</span></p><p>本论文提出MedThreatRAG框架，通过注入对抗性的图文对，系统性模拟并评估多模态医学RAG系统在文本、视觉和跨模态冲突等多种威胁下的脆弱性。实验表明，跨模态对抗注入（CMCI）最为致命，能显著削弱主流医学视觉-语言模型的查全率和F1分数，暴露了医学RAG系统在知识库更新和检索阶段的安全隐患。结论指出，当前医学RAG系统极易受到数据投毒攻击，需引入多模态一致性检测等新型安全机制。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="176-Large-Language-Model-Based-Automatic-Formulation-for-Stochastic-Optimization-Modelspdf"><a href="#176-Large-Language-Model-Based-Automatic-Formulation-for-Stochastic-Optimization-Modelspdf" class="headerlink" title="176. Large Language Model-Based Automatic Formulation for Stochastic Optimization Modelspdf"></a>176. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Large_Language_Model-Based_Automatic_Formulation_for_Stochastic_Optimization_Models.pdf">Large Language Model-Based Automatic Formulation for Stochastic Optimization Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Ohio State University</span></p><p>本文系统研究了大语言模型（LLM），尤其是ChatGPT，在从自然语言自动构建和求解随机优化问题中的表现，涵盖联合&#x2F;个体机会约束模型及两阶段随机线性规划（SLP-2）。作者设计了链式思维、模块化推理和多智能体协作的提示策略，并提出“软评分”指标，系统评估了各类模型及提示对随机优化任务的建模与代码生成能力。结论显示，GPT-4-Turbo结合高效提示策略在变量、目标函数匹配及部分得分等方面表现最佳，多智能体协作和结构化提示能提升LLM在随机优化建模中的准确性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="177-BudgetThinker-Empowering-Budget-aware-LLM-Reasoning-with-Control-Tokenspdf"><a href="#177-BudgetThinker-Empowering-Budget-aware-LLM-Reasoning-with-Control-Tokenspdf" class="headerlink" title="177. BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokenspdf"></a>177. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/BudgetThinker__Empowering_Budget-aware_LLM_Reasoning_with_Control_Tokens.pdf">BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute for AI Industry Research (AIR), Tsinghua University</span></p><p>本文提出BudgetThinker，一种通过在推理过程中周期性插入特殊控制token，使大语言模型（LLM）能够精准控制推理长度的预算感知推理框架。方法包括SFT（监督微调）+ RL（基于长度奖励的强化学习）双阶段训练流程，实现模型在不同预算下高效且可控的推理输出。实验结果表明，BudgetThinker在多个数学推理基准上较现有方法实现了更精确的预算控制和更高的准确率，具备实际部署价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="178-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMspdf"><a href="#178-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMspdf" class="headerlink" title="178. PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMspdf"></a>178. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PosterGen__Aesthetic-Aware_Paper-to-Poster_Generation_via_Multi-Agent_LLMs.pdf">PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stony Brook University</span></p><p>本论文提出了PosterGen，一种通过多智能体大语言模型（LLMs）驱动的学术论文自动海报生成框架，创新性地将专业设计流程中的美学和排版原则嵌入多代理协作，包括内容解析、版面布局、色彩与字体风格设计以及最终渲染。通过引入基于视觉-语言模型（VLM）的评价体系，实验证明PosterGen在内容准确性、视觉美观性和可读性等方面显著优于现有方法，生成的海报无需大量人工修正即可直接用于展示。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="179-LLM-Assertiveness-can-be-Mechanistically-Decomposed-into-Emotional-and-Logical-Componentspdf"><a href="#179-LLM-Assertiveness-can-be-Mechanistically-Decomposed-into-Emotional-and-Logical-Componentspdf" class="headerlink" title="179. LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Componentspdf"></a>179. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LLM_Assertiveness_can_be_Mechanistically_Decomposed_into_Emotional_and_Logical_Components.pdf">LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Cardiff University</span></p><p>本文提出利用机制可解释性分析方法，研究大型语言模型（LLM）中过度自信（assertiveness）的内部机制。通过对Llama-3.2模型在人工标注断言性数据集上的微调，提取各层激活，并聚类分析，发现高断言性表征可分解为情感与逻辑两个正交子成分，并通过向量操控实验验证其对模型预测的独立影响。结论指出，LLM断言性具有多成分结构，为缓解模型过度自信行为提供了机制依据。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="180-MaRVL-QA-A-Benchmark-for-Mathematical-Reasoning-over-Visual-Landscapespdf"><a href="#180-MaRVL-QA-A-Benchmark-for-Mathematical-Reasoning-over-Visual-Landscapespdf" class="headerlink" title="180. MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapespdf"></a>180. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MaRVL-QA__A_Benchmark_for_Mathematical_Reasoning_over_Visual_Landscapes.pdf">MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Waymo</span></p><p>MaRVL-QA提出了一个专为多模态大语言模型设计的数学推理和空间推理基准，包括拓扑计数和变换识别两大任务，通过严格的函数库与多阶段过滤生成高质量测评数据。实验证明，当前主流MLLM在抽象空间和数学结构推理方面表现不佳，暴露出系统性缺陷，为未来模型架构和训练范式的改进提供方向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="181-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiencypdf"><a href="#181-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiencypdf" class="headerlink" title="181. InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiencypdf"></a>181. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/InternVL3.5__Advancing_Open-Source_Multimodal_Models_in_Versatility,_Reasoning,_and_Efficiency.pdf">InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai AI Laboratory</span></p><p>本文提出InternVL3.5，一种新一代开源多模态大模型，创新性地引入Cascade强化学习（Cascade RL）框架，结合离线和在线RL以提升推理能力，并通过视觉分辨率路由器（ViR）与解耦视觉-语言部署（DvD）优化推理效率。实验结果表明，InternVL3.5在多模态理解、推理和多任务能力方面大幅超越前代开源模型，推理性能提升16%，推理速度提升4倍，缩小了与顶级商用模型（如GPT-5）的差距。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="182-MMTok-Multimodal-Coverage-Maximization-for-Efficient-Inference-of-VLMspdf"><a href="#182-MMTok-Multimodal-Coverage-Maximization-for-Efficient-Inference-of-VLMspdf" class="headerlink" title="182. MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMspdf"></a>182. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MMTok__Multimodal_Coverage_Maximization_for_Efficient_Inference_of_VLMs.pdf">MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Arizona State University</span></p><p>本文提出了一种多模态覆盖最大化（MMTok）方法，通过最大覆盖准则联合利用视觉和文本信息，选择最具信息量的视觉token，以减少视觉-语言模型（VLM）推理时的token冗余。该方法将token选择问题形式化为最大覆盖问题，通过贪心算法高效近似最优解，并可选用轻量agent模型丰富文本语义。实验结果表明，MMTok在多个数据集和VLM架构下大幅提升推理效率，在保持性能的同时显著减少视觉token数量，超越主流单模态方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="183-Type-Compliant-Adaptation-Cascades-Adapting-Programmatic-LM-Workflows-to-Datapdf"><a href="#183-Type-Compliant-Adaptation-Cascades-Adapting-Programmatic-LM-Workflows-to-Datapdf" class="headerlink" title="183. Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Datapdf"></a>183. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Type-Compliant_Adaptation_Cascades__Adapting_Programmatic_LM_Workflows_to_Data.pdf">Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Google</span></p><p>本文提出Type-Compliant Adaptation Cascades (tacs) 框架，将LLM多步工作流适应问题形式化为学习带类型约束的概率程序，通过参数高效微调(PEFT)和梯度优化，实现端到端结构化任务适应。实验显示，在结构化任务（如MGSM-SymPy、FinQA）和中小型模型下，tacs显著优于主流prompt优化方法，并可高效 enforcing 类型合规性，提升复杂推理和结构化输出任务的可靠性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="184-Flash-Sparse-Attention-An-Alternative-Efficient-Implementation-of-Native-Sparse-Attention-Kernelpdf"><a href="#184-Flash-Sparse-Attention-An-Alternative-Efficient-Implementation-of-Native-Sparse-Attention-Kernelpdf" class="headerlink" title="184. Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernelpdf"></a>184. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Flash_Sparse_Attention__An_Alternative_Efficient_Implementation_of_Native_Sparse_Attention_Kernel.pdf">Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>本文提出了Flash Sparse Attention (FSA)，一种替代Native Sparse Attention (NSA)的高效稀疏注意力核方法，通过将内核循环顺序颠倒，并引入针对非连续内存访问、在线softmax和累加的优化，使NSA在小GQA组下也能高效运行。实验结果表明，FSA在主流长上下文LLM训练和推理中，在内核延迟、端到端训练和预填速度方面均显著优于NSA和全量注意力，实现了最高3.5×内核加速和1.25×训练加速。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="185-Unraveling-the-Cognitive-Patterns-of-Large-Language-Models-through-Module-Communitiespdf"><a href="#185-Unraveling-the-Cognitive-Patterns-of-Large-Language-Models-through-Module-Communitiespdf" class="headerlink" title="185. Unraveling the Cognitive Patterns of Large Language Models through Module Communitiespdf"></a>185. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Unraveling_the_cognitive_patterns_of_Large_Language_Models_through_module_communities.pdf">Unraveling the Cognitive Patterns of Large Language Models through Module Communities</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Rensselaer Polytechnic Institute</span></p><p>本论文提出了一种基于网络科学和认知科学的多重网络分析框架，将抽象认知技能、数据集和LLM（如Llama、Llama-Chat、Vicuna）内部的模块结构进行关联，通过社区发现方法揭示了模型内部技能分布与模块功能的关系，并分析了不同剪枝与微调策略对模型表现的影响。结论认为，虽然LLM存在与认知技能相关的社区结构，但基于认知技能的模块化微调并未在性能上优于随机选择，LLM的认知能力源于分布式且相互依赖的模块交互而非严格局部化的功能分区。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="186-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answeringpdf"><a href="#186-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answeringpdf" class="headerlink" title="186. ST-Raptor: LLM-Powered Semi-Structured Table Question Answeringpdf"></a>186. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ST-Raptor__LLM-Powered_Semi-Structured_Table_Question_Answering.pdf">ST-Raptor: LLM-Powered Semi-Structured Table Question Answering</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本论文提出了ST-Raptor，一种基于层次正交树（HO-Tree）结构和操作流水线的半结构化表格问答（QA）框架，通过多模态模型和启发式规则构建表结构，结合问题分解与操作-表内容对齐，实现复杂表格布局下的问题自动解答。实验表明，ST-Raptor在SSTQA等真实场景数据集上，答案准确率比九种现有方法最高提升20%，显著增强了半结构化表格问答的准确性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="187-Leveraging-Large-Language-Models-for-Accurate-Sign-Language-Translation-in-Low-Resource-Scenariospdf"><a href="#187-Leveraging-Large-Language-Models-for-Accurate-Sign-Language-Translation-in-Low-Resource-Scenariospdf" class="headerlink" title="187. Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenariospdf"></a>187. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Leveraging_Large_Language_Models_for_Accurate_Sign_Language_Translation_in_Low-Resource_Scenarios.pdf">Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Catania</span></p><p>该论文提出AulSign，一种结合大语言模型（LLM）、动态提示、上下文学习和外部词汇的手语翻译方法，包括Retriever、LLM和Sign Mapper三个模块，实现了结构化、可解释的从口语到手语及反向翻译。实验表明，AulSign在美式手语（ASL）和意大利手语（LIS）低资源条件下显著优于现有方法，提升了翻译准确性和可用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="188-AdLoCo-Adaptive-Batching-Significantly-Improves-Communications-Efficiency-and-Convergence-for-Large-Language-Modelspdf"><a href="#188-AdLoCo-Adaptive-Batching-Significantly-Improves-Communications-Efficiency-and-Convergence-for-Large-Language-Modelspdf" class="headerlink" title="188. AdLoCo: Adaptive Batching Significantly Improves Communications Efficiency and Convergence for Large Language Modelspdf"></a>188. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AdLoCo__adaptive_batching_significantly_improves_communications_efficiency_and_convergence_for_Large.pdf">AdLoCo: Adaptive Batching Significantly Improves Communications Efficiency and Convergence for Large Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Moscow Institute of Physics and Technology</span></p><p>本文提出AdLoCo方法，通过多实例训练（MIT）、自适应批处理（Adaptive Batching）和梯度累积切换机制（Switch Mode），提升大语言模型在分布式环境下的硬件利用率和通信效率。实验结果表明，AdLoCo在达到目标困惑度和收敛速度上均优于现有分布式训练方法，理论分析也给出了通信复杂度上界。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="189-The-Computational-Complexity-of-Satisfiability-in-State-Space-Modelspdf"><a href="#189-The-Computational-Complexity-of-Satisfiability-in-State-Space-Modelspdf" class="headerlink" title="189. The Computational Complexity of Satisfiability in State Space Modelspdf"></a>189. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_Computational_Complexity_of_Satisfiability_in_State_Space_Models.pdf">The Computational Complexity of Satisfiability in State Space Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Kassel</span></p><p>本文系统研究了状态空间模型（SSM）可满足性问题的计算复杂性，提出一般情形下问题不可判定，并在现实应用动机下探讨了两类可判定情形：有限上下文长度和定宽量化算术，分别给出NP-complete、NEXPTIME、PSPACE-complete和EXPSPACE等复杂性界。结论表明，对基于SSM的语言模型进行形式化验证存在固有的计算复杂性挑战，但在某些实际约束下问题是可解且复杂性可明确界定。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="190-Learning-from-Few-Samples-A-Novel-Approach-for-High-Quality-Malcode-Generationpdf"><a href="#190-Learning-from-Few-Samples-A-Novel-Approach-for-High-Quality-Malcode-Generationpdf" class="headerlink" title="190. Learning from Few Samples: A Novel Approach for High-Quality Malcode Generationpdf"></a>190. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Learning_from_Few_Samples__A_Novel_Approach_for_High-Quality_Malcode_Generation.pdf">Learning from Few Samples: A Novel Approach for High-Quality Malcode Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huazhong University of Science and Technology</span></p><p>该论文提出了GANGRL-LLM框架，将生成对抗网络（GAN）与大语言模型（LLM）结合，在半监督和少样本场景下协同训练生成器和判别器，生成高质量恶意代码（如SQL注入）并提升检测能力。实验结果表明，即使训练数据极少，该方法在恶意代码生成和入侵检测上均取得了优异性能，并具备良好的迁移性和扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="191-Test-Time-Scaling-Strategies-for-Generative-Retrieval-in-Multimodal-Conversational-Recommendationspdf"><a href="#191-Test-Time-Scaling-Strategies-for-Generative-Retrieval-in-Multimodal-Conversational-Recommendationspdf" class="headerlink" title="191. Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendationspdf"></a>191. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Test-Time_Scaling_Strategies_for_Generative_Retrieval_in_Multimodal_Conversational_Recommendations.pdf">Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Research Center for Information Technology Innovation, Academia Sinica</span></p><p>该论文提出了一种结合多模态大语言模型（MLLMs）的生成式检索器与测试时重排序（Test-Time Reranking, TTR）机制的新框架，以提升多轮多模态对话中的商品检索效果。方法流程包括用户意图推理、多模态生成检索、以及在推理阶段动态调整候选项分数的TTR机制。实验证明，所提方法在多个基准数据集上平均MRR提升14.5点、nDCG@1提升10.6点，显著增强检索准确性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="192-The-AI-Data-Scientistpdf"><a href="#192-The-AI-Data-Scientistpdf" class="headerlink" title="192. The AI Data Scientistpdf"></a>192. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_AI_Data_Scientist.pdf">The AI Data Scientist</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MBZUAI</span></p><p>本文提出了AI Data Scientist系统，一种由大型语言模型（LLM）驱动的自治智能体，通过六个专用子代理（数据清洗、假设生成、预处理、特征工程、模型训练和行动建议）实现从原始数据到业务洞察的自动化数据科学全流程。实验结果显示，该方法不仅能提升预测准确率，还能生成更具解释性和实用性的建议，有效缩短数据分析周期，并提高洞察的可用性和决策效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="193-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Codepdf"><a href="#193-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Codepdf" class="headerlink" title="193. A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Codepdf"></a>193. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A.S.E__A_Repository-Level_Benchmark_for_Evaluating_Security_in_AI-Generated_Code.pdf">A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent</span></p><p>本文提出A.S.E基准，专为评估大语言模型（LLMs）在真实仓库级安全代码生成能力而设计。A.S.E以带有CVEs的真实开源项目为任务来源，保留完整仓库上下文，并通过结构&#x2F;语义变异扩展数据，结合专家自定义的静态分析规则（如CodeQL&#x2F;Joern）和可复现的Docker化环境，实现多维度（安全性、质量、稳定性）客观评价。实验显示，Claude-3.7-Sonnet模型总体表现最优，开源模型Qwen3-235B-A22B-Instruct在安全性上领先，且“快思维”推理策略优于“慢思维”在安全修复任务。结论表明A.S.E为代码安全生成研究提供了严谨可复现的评测基础，且开源模型与专有模型安全性能差距缩小，安全性与推理策略密切相关。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="194-Detecting-and-Characterizing-Planning-in-Language-Modelspdf"><a href="#194-Detecting-and-Characterizing-Planning-in-Language-Modelspdf" class="headerlink" title="194. Detecting and Characterizing Planning in Language Modelspdf"></a>194. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Detecting_and_Characterizing_Planning_in_Language_Models.pdf">Detecting and Characterizing Planning in Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Massachusetts Amherst</span></p><p>该论文提出了用于检测与表征大语言模型（LLMs）内部规划行为的形式化、因果标准，并实现了半自动化注释流程。作者将此流程应用于Gemma-2-2B的基础版与指令微调版，在MBPP代码生成和诗歌生成任务上，发现规划并非普适，指令微调有助于优化而非创造规划行为，部分任务模型通过即兴完成，部分则展现出明确的规划机制。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="195-Teaching-LLMs-to-Think-Mathematically-A-Critical-Study-of-Decision-Making-via-Optimizationpdf"><a href="#195-Teaching-LLMs-to-Think-Mathematically-A-Critical-Study-of-Decision-Making-via-Optimizationpdf" class="headerlink" title="195. Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimizationpdf"></a>195. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Teaching_LLMs_to_Think_Mathematically__A_Critical_Study_of_Decision-Making_via_Optimization.pdf">Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Princess Sumaya University for Technology</span></p><p>本文系统性综述和元分析了大语言模型（LLM）在数学规划与优化领域对决策问题的自动建模与求解能力，涵盖了学习方法、数据集设计、评估指标和提示策略，并通过构建数据集和三种提示策略（Act-as-expert、chain-of-thought、self-consistency）在计算机网络优化建模任务上进行了实验。实验显示LLM能解析自然语言并生成符号公式，但在准确性、可扩展性和可解释性上仍有明显不足，未来需发展结构化数据集、领域微调、多智能体和检索增强等混合方法来提升数学建模能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="196-Named-Entity-Recognition-of-Historical-Text-via-Large-Language-Modelpdf"><a href="#196-Named-Entity-Recognition-of-Historical-Text-via-Large-Language-Modelpdf" class="headerlink" title="196. Named Entity Recognition of Historical Text via Large Language Modelpdf"></a>196. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Named_Entity_Recognition_of_Historical_Text_via_Large_Language_Model.pdf">Named Entity Recognition of Historical Text via Large Language Model</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Bologna</span></p><p>本文提出利用大型语言模型（LLM），通过零样本和小样本（few-shot）提示策略，在无需任务特定训练数据的情况下，对多语言历史文本进行命名实体识别（NER）。实验基于HIPE-2022多语言历史文本数据集，探索了不同示例选择策略（随机、词汇重叠、嵌入相似性）对few-shot学习效果的影响。结论表明：仅用单一示例的小样本提示即可显著优于零样本，且示例选择方式影响有限，虽然与有监督SOTA相比仍存在性能差距，但该方法为低资源场景下NER任务提供了高效、低成本的可行替代方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="197-HyST-LLM-Powered-Hybrid-Retrieval-over-Semi-Structured-Tabular-Datapdf"><a href="#197-HyST-LLM-Powered-Hybrid-Retrieval-over-Semi-Structured-Tabular-Datapdf" class="headerlink" title="197. HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Datapdf"></a>197. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HyST__LLM-Powered_Hybrid_Retrieval_over_Semi-Structured_Tabular_Data.pdf">HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">PrompTart LAB, MODULABS</span></p><p>HyST提出了一种基于大语言模型（LLM）的混合检索框架，用于解析自然语言查询，将结构化约束（如类别、品牌等）转化为元数据过滤条件，并结合语义嵌入搜索对半结构化表格数据进行检索。实验结果表明，HyST在精准检索和查询理解方面优于传统基线，显著提升推荐系统在复杂查询下的性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="198-Language-Models-Coupled-with-Metacognition-Can-Outperform-Reasoning-Modelspdf"><a href="#198-Language-Models-Coupled-with-Metacognition-Can-Outperform-Reasoning-Modelspdf" class="headerlink" title="198. Language Models Coupled with Metacognition Can Outperform Reasoning Modelspdf"></a>198. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Language_Models_Coupled_with_Metacognition_Can_Outperform_Reasoning_Models.pdf">Language Models Coupled with Metacognition Can Outperform Reasoning Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IBM Research</span></p><p>本文提出SOFAI-LM，一种无需训练的元认知架构，将大语言模型(LLM)与大推理模型(LRM)结合，通过元认知反馈迭代提升LLM推理能力，并在必要时调用LRM。实验覆盖图着色和代码调试两大推理任务，结果显示SOFAI-LM在准确率和推理效率上均显著优于单独LRM，且对模型和任务具备高度通用性。结论：SOFAI-LM实现了LLM推理性能的大幅提升，并降低了整体计算成本。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="199-Understanding-Subword-Compositionality-of-Large-Language-Modelspdf"><a href="#199-Understanding-Subword-Compositionality-of-Large-Language-Modelspdf" class="headerlink" title="199. Understanding Subword Compositionality of Large Language Modelspdf"></a>199. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Understanding_Subword_Compositionality_of_Large_Language_Models.pdf">Understanding Subword Compositionality of Large Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Copenhagen</span></p><p>本文系统性分析了六种主流大语言模型在子词组合到词级表示过程中的结构相似性、语义可分解性和形式保持能力，采用Procrustes分析、二分类和回归探针实验，揭示不同模型采用三种截然不同的子词组合策略。结论表明，内容相关的语义信息在所有模型和层中均能较好保持，而形式信息则有较大模型间差异，揭示了LLMs内部不同的组合动态。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="200-Debiasing-Multilingual-LLMs-in-Cross-lingual-Latent-Spacepdf"><a href="#200-Debiasing-Multilingual-LLMs-in-Cross-lingual-Latent-Spacepdf" class="headerlink" title="200. Debiasing Multilingual LLMs in Cross-lingual Latent Spacepdf"></a>200. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Debiasing_Multilingual_LLMs_in_Cross-lingual_Latent_Space.pdf">Debiasing Multilingual LLMs in Cross-lingual Latent Space</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Copenhagen</span></p><p>论文提出在跨语言对齐的潜在空间而非直接在LLM表示上进行消偏操作，通过训练自编码器在TED讲座平行语料上构建多语种潜在空间，并在该空间内应用SentDebias和INLP消偏技术。实验表明，这种方法能显著提升消偏效果和跨语言迁移能力，最高可减少65%偏差，且对模型下游性能影响极小。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="201-AMELIA-A-FAMILY-OF-MULTI-TASK-END-TO-END-LANGUAGE-MODELS-FOR-ARGUMENTATIONpdf"><a href="#201-AMELIA-A-FAMILY-OF-MULTI-TASK-END-TO-END-LANGUAGE-MODELS-FOR-ARGUMENTATIONpdf" class="headerlink" title="201. AMELIA: A FAMILY OF MULTI-TASK END-TO-END LANGUAGE MODELS FOR ARGUMENTATIONpdf"></a>201. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AMELIA__A_Family_of_Multi-task_End-to-end_Language_Models_for_Argumentation.pdf">AMELIA: A FAMILY OF MULTI-TASK END-TO-END LANGUAGE MODELS FOR ARGUMENTATION</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universite Claude Bernard Lyon 1</span></p><p>本论文提出AMELIA，通过整合19个主流论证挖掘数据集构建多任务标准化数据集，并基于Meta AI的Llama-3.1-8B-Instruct模型系统研究了单任务微调、多任务联合微调以及多模型合并等训练策略，涵盖论证成分分类、立场检测、证据识别等八项任务。结果显示：单任务微调显著提升各任务表现，多任务微调无性能损失且实现任务迁移，模型合并可在降低资源消耗下取得竞争性效果。结论认为大语言模型通过细致适配，可高效应对多种论证挖掘任务，兼顾性能与灵活性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="202-Riemannian-Optimization-for-LoRA-on-the-Stiefel-Manifoldpdf"><a href="#202-Riemannian-Optimization-for-LoRA-on-the-Stiefel-Manifoldpdf" class="headerlink" title="202. Riemannian Optimization for LoRA on the Stiefel Manifoldpdf"></a>202. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Riemannian_Optimization_for_LoRA_on_the_Stiefel_Manifold.pdf">Riemannian Optimization for LoRA on the Stiefel Manifold</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Opt-AI Inc.</span></p><p>本文提出Stiefel-LoRA方法，将LoRA中的B矩阵优化限制在Stiefel流形上，实现显式正交约束以提升表示效率，通过Riemannian优化（QR分解投影+回缩）替代传统AdamW优化。实验结果表明，Stiefel-LoRA在多种LLM微调基准任务上均显著优于传统LoRA，提升参数利用率与收敛速度，验证了几何约束对PEFT性能优化的关键作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="203-ILRE-Intermediate-Layer-Retrieval-for-Context-Compression-in-Causal-Language-Modelspdf"><a href="#203-ILRE-Intermediate-Layer-Retrieval-for-Context-Compression-in-Causal-Language-Modelspdf" class="headerlink" title="203. ILRE: Intermediate Layer Retrieval for Context Compression in Causal Language Modelspdf"></a>203. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ILRe__Intermediate_Layer_Retrieval_for_Context_Compression_in_Causal_Language_Models.pdf">ILRE: Intermediate Layer Retrieval for Context Compression in Causal Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AI Lab, China Merchants Bank</span></p><p>该论文提出了一种名为ILRe的无训练上下文压缩框架，用于因果语言模型的长文本推理。方法通过在离线选定的中间解码器层，仅对该层进行流式分块预填充，然后利用输入查询与全量Key缓存的注意力分数实现Token召回，并在召回过程中采用多池化核分配策略以保持语义完整性。实验结果显示，ILRe在无需额外训练或算子开发的情况下，实现了推理速度百倍提升，精度与完整上下文持平或更优，显著提升了长上下文LLM的推理效率和能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="204-UniAPO-Unified-Multimodal-Automated-Prompt-Optimizationpdf"><a href="#204-UniAPO-Unified-Multimodal-Automated-Prompt-Optimizationpdf" class="headerlink" title="204. UniAPO: Unified Multimodal Automated Prompt Optimizationpdf"></a>204. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/UniAPO__Unified_Multimodal_Automated_Prompt_Optimization.pdf">UniAPO: Unified Multimodal Automated Prompt Optimization</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ByteDance</span></p><p>本论文提出了UniAPO——首个统一的多模态自动提示优化（APO）框架，采用EM算法思想，将反馈建模与提示优化解耦，并引入短-长期记忆机制，专门解决视觉token膨胀和过程级监督缺失等多模态场景下的挑战。实验显示，UniAPO在文本、图像和视频等多模态任务上全面超越现有方法，具备优异的性能、泛化能力及优化效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="205-AVAM-Universal-Training-free-Adaptive-Visual-Anchoring-Embedded-into-Multimodal-Large-Language-Model-for-Multi-image-Question-Answeringpdf"><a href="#205-AVAM-Universal-Training-free-Adaptive-Visual-Anchoring-Embedded-into-Multimodal-Large-Language-Model-for-Multi-image-Question-Answeringpdf" class="headerlink" title="205. AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answeringpdf"></a>205. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AVAM__Universal_Training-free_Adaptive_Visual_Anchoring_Embedded_into_Multimodal_Large_Language_Mode.pdf">AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hunan University</span></p><p>本文提出了一种无训练的自适应视觉锚定（Adaptive Visual Anchoring, AVAM）策略，通过计算视觉特征与文本特征之间的响应热区，自适应筛选多图像输入中与问题最相关的连续关键区域，并结合协同解码机制动态融合全局与局部视觉上下文信息。实验结果表明，AVAM可无缝集成到主流多模态大模型（MLLMs），在多图像视觉问答任务（MVQA）中显著提升准确率和效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="206-VISA-Group-wise-Visual-Token-Selection-and-Aggregation-via-Graph-Summarization-for-Efficient-MLLMs-Inferencepdf"><a href="#206-VISA-Group-wise-Visual-Token-Selection-and-Aggregation-via-Graph-Summarization-for-Efficient-MLLMs-Inferencepdf" class="headerlink" title="206. VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inferencepdf"></a>206. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/VISA__Group-wise_Visual_Token_Selection_and_Aggregation_via_Graph_Summarization_for_Efficient_MLLMs_.pdf">VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xiamen University</span></p><p>该论文提出了VISA方法，通过组内视觉token选择与图汇总聚合（VTA），解决多模态大语言模型（MLLMs）视觉token冗余导致推理效率低的问题。VTA利用语义相似性构建图，将被移除token的信息通过图结构聚合到保留token，并结合组内选择策略，根据文本关注逐步筛选和压缩视觉token。实验表明，VISA在主流多模态基准上能在极高压缩率下保持模型性能，同时提升推理速度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="207-Group-Expectation-Policy-Optimization-for-Stable-Heterogeneous-Reinforcement-Learning-in-LLMspdf"><a href="#207-Group-Expectation-Policy-Optimization-for-Stable-Heterogeneous-Reinforcement-Learning-in-LLMspdf" class="headerlink" title="207. Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMspdf"></a>207. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Group_Expectation_Policy_Optimization_for_Stable_Heterogeneous_Reinforcement_Learning_in_LLMs.pdf">Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMs</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peng Cheng Laboratory</span></p><p>该论文提出了HeteroRL异构计算架构和GEPO群期望策略优化算法，用于提升大语言模型（LLM）在地理分布式高延迟异构环境下的强化学习训练稳定性。方法上，HeteroRL实现采样与参数学习异步解耦，GEPO通过样本级重要性权重与群期望平滑机制，显著抑制高延迟下重要性采样方差爆炸，理论与实验证明在极端延迟下仍能保持模型性能稳定，优于GRPO等现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="208-FAIRGAMER-Evaluating-Biases-in-the-Application-of-Large-Language-Models-to-Video-Gamespdf"><a href="#208-FAIRGAMER-Evaluating-Biases-in-the-Application-of-Large-Language-Models-to-Video-Gamespdf" class="headerlink" title="208. FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Gamespdf"></a>208. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FAIRGAMER__Evaluating_Biases_in_the_Application_of_Large_Language_Models_to_Video_Games.pdf">FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Games</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Cyber Security, University of Chinese Academy of Sciences</span></p><p>该论文提出了FAIRGAMER，这是首个用于评估大语言模型（LLM）在电子游戏场景中社会和文化偏见的基准，涵盖NPC对话、对抗决策和场景生成三大任务，并引入了Decision Log Standard Deviation（Dlstd）新指标。实验证明，现有主流LLM在真实与虚构游戏环境下均表现出明显偏见，显著影响游戏平衡，揭示了LLM在游戏应用中的可靠性挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="209-Scalable-Engine-and-the-Performance-of-Different-LLM-Models-in-a-SLURM-Based-HPC-Architecturepdf"><a href="#209-Scalable-Engine-and-the-Performance-of-Different-LLM-Models-in-a-SLURM-Based-HPC-Architecturepdf" class="headerlink" title="209. Scalable Engine and the Performance of Different LLM Models in a SLURM Based HPC Architecturepdf"></a>209. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scalable_Engine_and_the_Performance_of_Different_LLM_Models_in_a_SLURM_based_HPC_architecture.pdf">Scalable Engine and the Performance of Different LLM Models in a SLURM Based HPC Architecture</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AImotion Bavaria</span></p><p>本文提出了一种基于SLURM的高性能计算架构，结合容器化微服务与动态资源调度，实现了多种LLM模型（如Llama 3.1和3.2）在多节点集群中的高效可扩展推理。实验结果显示，该架构在REST API接口基础上支持高并发、低延迟的推理服务，对小模型可支持128并发、50毫秒级延迟，大模型则在2个并发下延迟超2秒，整体系统具备良好弹性和容错性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="210-Proximal-Supervised-Fine-Tuningpdf"><a href="#210-Proximal-Supervised-Fine-Tuningpdf" class="headerlink" title="210. Proximal Supervised Fine-Tuningpdf"></a>210. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Proximal_Supervised_Fine-Tuning.pdf">Proximal Supervised Fine-Tuning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了一种新的大模型监督微调方法Proximal Supervised Fine-Tuning (PSFT)，将强化学习中的信赖域思想（如TRPO、PPO的clipped surrogate objective）引入到监督微调损失，限制模型参数更新幅度以防止能力退化和熵坍缩。实验表明，PSFT在数学推理和人类价值对齐等领域，在保持目标任务性能的同时显著提升了模型的泛化能力，并为后续RL等优化阶段打下更好基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="211-ISACL-Internal-State-Analyzer-for-Copyrighted-Training-Data-Leakagepdf"><a href="#211-ISACL-Internal-State-Analyzer-for-Copyrighted-Training-Data-Leakagepdf" class="headerlink" title="211. ISACL: Internal State Analyzer for Copyrighted Training Data Leakagepdf"></a>211. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ISACL__Internal_State_Analyzer_for_Copyrighted_Training_Data_Leakage.pdf">ISACL: Internal State Analyzer for Copyrighted Training Data Leakage</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">City University of Hong Kong</span></p><p>该论文提出了一种名为ISACL的框架，通过分析大语言模型（LLM）在生成文本前的内部状态，结合检索增强生成（RAG）系统及神经网络分类器，实现对潜在版权训练数据泄漏的实时、前置检测。实验表明ISACL能有效预防敏感或受版权保护内容的泄漏，并兼顾高准确率和效率，提升了合规性和数据隐私保护。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="212-Randomly-Removing-50-of-Dimensions-in-Text-Embeddings-has-Minimal-Impact-on-Retrieval-and-Classification-Taskspdf"><a href="#212-Randomly-Removing-50-of-Dimensions-in-Text-Embeddings-has-Minimal-Impact-on-Retrieval-and-Classification-Taskspdf" class="headerlink" title="212. Randomly Removing 50% of Dimensions in Text Embeddings has Minimal Impact on Retrieval and Classification Taskspdf"></a>212. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Randomly_Removing_50%_of_Dimensions_in_Text_Embeddings_has_Minimal_Impact_on_Retrieval_and_Classific.pdf">Randomly Removing 50% of Dimensions in Text Embeddings has Minimal Impact on Retrieval and Classification Tasks</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Mannheim</span></p><p>本论文系统分析了在文本嵌入向量中随机移除高达50%维度后，对检索和分类等下游任务性能的影响。通过对6个主流文本编码器和26项任务的大规模实验证明，嵌入维度的随机截断仅导致性能微幅下降（小于10%），其原因在于许多维度对下游任务有负面贡献，并在所有模型中均匀分布。结论指出，当前文本嵌入模型存在大量无效甚至有害的维度，随机减小维度可以低成本地减少表示空间并保持高性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="213-Speculative-Safety-Aware-Decodingpdf"><a href="#213-Speculative-Safety-Aware-Decodingpdf" class="headerlink" title="213. Speculative Safety-Aware Decodingpdf"></a>213. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Speculative_Safety-Aware_Decoding.pdf">Speculative Safety-Aware Decoding</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science &amp; Technology, Beijing Institute of Technology</span></p><p>本文提出了Speculative Safety-Aware Decoding (SSD)，一种轻量级解码时安全增强方法，通过结合小型安全专家模型与大模型，在生成过程中利用“match ratio”动态切换解码策略（交集用于实用性、并集用于安全性），以防御jailbreak攻击并加速推理。实验证明SSD能有效将深度安全对齐属性转移至大型语言模型输出，显著提升安全性且保持模型有用性和推理效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="214-F2RVLM-Boosting-Fine-grained-Fragment-Retrieval-for-Multi-Modal-Long-form-Dialogue-with-Vision-Language-Modelpdf"><a href="#214-F2RVLM-Boosting-Fine-grained-Fragment-Retrieval-for-Multi-Modal-Long-form-Dialogue-with-Vision-Language-Modelpdf" class="headerlink" title="214. F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Modelpdf"></a>214. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/F2RVLM__Boosting_Fine-grained_Fragment_Retrieval_for_Multi-Modal_Long-form_Dialogue_with_Vision_Lang.pdf">F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent Inc</span></p><p>本文提出了Fine-grained Fragment Retrieval (FFR)任务，要求模型从多模态长对话中检索语义相关的文本和图像片段。作者构建了长对话数据集MLDR，并提出F2RVLM模型：首先通过监督微调注入检索知识，随后采用基于GRPO的强化学习优化语义精确性与上下文一致性，并结合难度感知课程采样提升长上下文推理能力。实验结果显示F2RVLM在合成和真实对话场景下均显著超越主流VLM模型，提升检索准确率与一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="215-Database-Normalization-via-Dual-LLM-Self-Refinementpdf"><a href="#215-Database-Normalization-via-Dual-LLM-Self-Refinementpdf" class="headerlink" title="215. Database Normalization via Dual-LLM Self-Refinementpdf"></a>215. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Database_Normalization_via_Dual-LLM_Self-Refinement.pdf">Database Normalization via Dual-LLM Self-Refinement</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sungshin Women’s University</span></p><p>该论文提出了Miffie框架，利用大语言模型（LLM）实现数据库模式的自动归一化。方法采用双模型自我优化架构：GPT-4负责生成规范化方案，o1-mini进行验证并循环反馈，通过零样本任务特定提示实现高准确率和成本效率。实验表明，Miffie能够高效准确地规范复杂数据库方案，大幅降低人工工作量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="216-LLM-based-Agentic-Reasoning-Frameworks-A-Survey-from-Methods-to-Scenariospdf"><a href="#216-LLM-based-Agentic-Reasoning-Frameworks-A-Survey-from-Methods-to-Scenariospdf" class="headerlink" title="216. LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenariospdf"></a>216. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LLM-based_Agentic_Reasoning_Frameworks__A_Survey_from_Methods_to_Scenarios.pdf">LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Jiaotong University</span></p><p>本论文提出了一个系统性的分类法，将基于大语言模型（LLM）的智能体推理框架分为单智能体方法、工具增强方法和多智能体方法，详细分析了各类框架的核心方法（如 prompt 工程、反思与自我改进、工具集成与选择、多智能体组织与交互等）及其在科学发现、医疗、软件工程和社会经济模拟等关键场景中的应用与评测体系。结论指出，系统化的框架有助于推动智能体推理的发展，提升其在多领域复杂任务中的适用性和安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="217-Unlearning-as-Ablation-Toward-a-Falsifiable-Benchmark-for-Generative-Scientific-Discoverypdf"><a href="#217-Unlearning-as-Ablation-Toward-a-Falsifiable-Benchmark-for-Generative-Scientific-Discoverypdf" class="headerlink" title="217. Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discoverypdf"></a>217. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Unlearning_as_Ablation__Toward_a_Falsifiable_Benchmark_for_Generative_Scientific_Discovery.pdf">Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discovery</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent Researcher</span></p><p>本文提出了“Unlearning-as-Ablation”方法作为检验大语言模型（LLM）是否具备生成性科学发现能力的可证伪测试。该方法通过系统性移除目标结果及其相关推理链（forget-closure），并要求模型仅凭允许的公理和工具重新推导该结果，从而区分模型真正的生成能力与仅仅是记忆和重组；结论认为，无论模型能否重新推导目标，这一框架都为AI科学发现能力的边界提供了实证评估手段。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="218-Towards-Synthesizing-Normative-Data-for-Cognitive-Assessments-Using-Generative-Multimodal-Large-Language-Modelspdf"><a href="#218-Towards-Synthesizing-Normative-Data-for-Cognitive-Assessments-Using-Generative-Multimodal-Large-Language-Modelspdf" class="headerlink" title="218. Towards Synthesizing Normative Data for Cognitive Assessments Using Generative Multimodal Large Language Modelspdf"></a>218. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Towards_Synthesizing_Normative_Data_for_Cognitive_Assessments_Using_Generative_Multimodal_Large_Lang.pdf">Towards Synthesizing Normative Data for Cognitive Assessments Using Generative Multimodal Large Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Westminster Schools</span></p><p>本文提出利用生成式多模态大语言模型（如GPT-4o及其mini版本）通过精细化提示，自动生成认知评估所需的规范性文本数据，替代以往昂贵且耗时的人工采集方法。实验结果显示，综合型临床提示能生成更真实且具诊断区分度的文本，BERTScore和LLM-as-a-Judge评价方法更能反映人工评判，推动认知测试新刺激的快速开发与数据更新。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="219-Dynamic-Embedding-of-Hierarchical-Visual-Features-for-Efficient-Vision-Language-Fine-Tuningpdf"><a href="#219-Dynamic-Embedding-of-Hierarchical-Visual-Features-for-Efficient-Vision-Language-Fine-Tuningpdf" class="headerlink" title="219. Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuningpdf"></a>219. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dynamic_Embedding_of_Hierarchical_Visual_Features_for_Efficient_Vision-Language_Fine-Tuning.pdf">Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">College of Computer Science and Technology, National University of Defense Technology</span></p><p>该论文提出一种动态分层视觉特征嵌入框架（DEHVF），通过分层视觉融合器，根据语言模型各层的语义粒度动态选择和融合视觉编码器的多层特征，并将其直接嵌入至对应层的FFN权重中，从而实现高效的视觉-语言模型微调。实验表明，DEHVF在ScienceQA和COCO Caption等任务上，以更少参数和更高计算效率超越现有PEFT方法和全量微调模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="220-Weights-Rotated-Preference-Optimization-for-Large-Language-Modelspdf"><a href="#220-Weights-Rotated-Preference-Optimization-for-Large-Language-Modelspdf" class="headerlink" title="220. Weights-Rotated Preference Optimization for Large Language Modelspdf"></a>220. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Weights-Rotated_Preference_Optimization_for_Large_Language_Models.pdf">Weights-Rotated Preference Optimization for Large Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Information Engineering, Chinese Academy of Sciences</span></p><p>本文提出了Weights-Rotated Preference Optimization (RoPO)算法，通过对LLM参数空间的多粒度正交矩阵旋转，结合DPO中的KL散度约束，显式地约束中间隐藏层及输出层，防止神经元塌缩和表达冗余，从而有效缓解DPO中的reward hacking问题。实验表明，RoPO在AlpacaEval 2和MT-Bench等多项评测中表现优异，提升对齐能力并减少无效冗长生成，保留了预训练和SFT阶段的知识。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="221-Stop-Spinning-Wheels-Mitigating-LLM-Overthinking-via-Mining-Patterns-for-Early-Reasoning-Exitpdf"><a href="#221-Stop-Spinning-Wheels-Mitigating-LLM-Overthinking-via-Mining-Patterns-for-Early-Reasoning-Exitpdf" class="headerlink" title="221. Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exitpdf"></a>221. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Stop_Spinning_Wheels__Mitigating_LLM_Overthinking_via_Mining_Patterns_for_Early_Reasoning_Exit.pdf">Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Computing Technology, Chinese Academy of Sciences</span></p><p>本文提出通过分析大语言模型推理过程中的思维长度与内容长度关系，将推理划分为三个阶段，并首次明确提出Reasoning Completion Point（RCP）作为最佳推理终止点。基于特征重要性分析，设计了一套高效的启发式规则（RCPD），可低成本且精准地检测RCP，实验证明在多个复杂推理基准上平均减少30%以上token消耗，同时维持或提升准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="222-Steering-When-Necessary-Flexible-Steering-Large-Language-Models-with-Backtrackingpdf"><a href="#222-Steering-When-Necessary-Flexible-Steering-Large-Language-Models-with-Backtrackingpdf" class="headerlink" title="222. Steering When Necessary: Flexible Steering Large Language Models with Backtrackingpdf"></a>222. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Steering_When_Necessary__Flexible_Steering_Large_Language_Models_with_Backtracking.pdf">Steering When Necessary: Flexible Steering Large Language Models with Backtracking</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本文提出了Flexible Activation Steering with Backtracking（FASB）框架，通过追踪LLM生成过程中的内部状态，结合问题和已生成内容，动态判断是否以及如何强度干预模型激活，并设计了回溯机制对偏离目标行为的token进行回退再生成。实验表明，该方法在TruthfulQA和六个多项选择数据集上优于现有基线方法，有效提升了LLM输出的可控性与真确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="223-ChartMaster-Advancing-Chart-to-Code-Generation-with-Real-World-Charts-and-Chart-Similarity-Reinforcement-Learningpdf"><a href="#223-ChartMaster-Advancing-Chart-to-Code-Generation-with-Real-World-Charts-and-Chart-Similarity-Reinforcement-Learningpdf" class="headerlink" title="223. ChartMaster: Advancing Chart-to-Code Generation with Real-World Charts and Chart Similarity Reinforcement Learningpdf"></a>223. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ChartMaster__Advancing_Chart-to-Code_Generation_with_Real-World_Charts_and_Chart_Similarity_Reinforc.pdf">ChartMaster: Advancing Chart-to-Code Generation with Real-World Charts and Chart Similarity Reinforcement Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China University of Technology</span></p><p>该论文提出ChartMaster框架，利用真实世界arXiv论文中的图表构建高多样性数据集ReChartPrompt-240K，并设计ChartSimRL强化学习算法，通过结合语义属性和视觉相似度奖励优化模型输出。最终模型在多个chart-to-code生成基准上取得接近GPT-4o的表现，显著提升了图表复现的准确性和多样性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="224-Exploring-Efficient-Learning-of-Small-BERT-Networks-with-LoRA-and-DoRApdf"><a href="#224-Exploring-Efficient-Learning-of-Small-BERT-Networks-with-LoRA-and-DoRApdf" class="headerlink" title="224. Exploring Efficient Learning of Small BERT Networks with LoRA and DoRApdf"></a>224. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Exploring_Efficient_Learning_of_Small_BERT_Networks_with_LoRA_and_DoRA.pdf">Exploring Efficient Learning of Small BERT Networks with LoRA and DoRA</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University</span></p><p>本论文围绕小型BERT模型（minBERT），研究并定制了低秩适应（LoRA）和权重分解低秩适应（DoRA）技术，结合自动混合精度（AMP），实现了参数高效微调。通过多任务学习（情感分析、复述检测、语义相似度），系统对比了不同低秩分解方式、超参数与优化策略，发现对于小模型，LoRA和DoRA在极低秩（秩为1）即可保持性能，AMP与LoRA结合带来约7-50%内存节省和最高60%训练加速，性能提升或损失极小。结论认为低秩微调技术同样适用于小规模语言模型，推荐在资源受限场景下采用LoRA+AMP方式，且进一步研究需关注低秩更新对模型误差模式和偏见的影响。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="225-TradingGroup-A-Multi-Agent-Trading-System-with-Self-Reflection-and-Data-Synthesispdf"><a href="#225-TradingGroup-A-Multi-Agent-Trading-System-with-Self-Reflection-and-Data-Synthesispdf" class="headerlink" title="225. TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesispdf"></a>225. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TradingGroup__A_Multi-Agent_Trading_System_with_Self-Reflection_and_Data-Synthesis.pdf">TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of New South Wales</span></p><p>该论文提出TradingGroup多智能体量化交易系统，通过结合大语言模型（LLM）、自反思机制、动态风险管理和自动化数据合成管道，实现了新闻情绪、财报解读、趋势预测、风格适应等多代理协作。系统在五只真实股票数据上回测，显著优于规则、传统机器学习、强化学习和现有LLM基交易方法，并通过自主生成高质量后训练数据显著提升了模型表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="226-Enhancing-Knowledge-Tracing-through-Leakage-Free-and-Recency-Aware-Embeddingspdf"><a href="#226-Enhancing-Knowledge-Tracing-through-Leakage-Free-and-Recency-Aware-Embeddingspdf" class="headerlink" title="226. Enhancing Knowledge Tracing through Leakage-Free and Recency-Aware Embeddingspdf"></a>226. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Enhancing_Knowledge_Tracing_through_Leakage-Free_and_Recency-Aware_Embeddings.pdf">Enhancing Knowledge Tracing through Leakage-Free and Recency-Aware Embeddings</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Applied Sciences Karlsruhe</span></p><p>本文针对知识追踪（KT）模型中的标签泄漏问题，提出通过在输入嵌入阶段使用MASK标签，有效阻止同一问题下各知识点（KC）之间的标签泄漏，同时采用基于可学习傅里叶特征的Recency Encoding方法，显式编码每个知识点自上次出现以来的时间步距。实验结果显示，这两种嵌入技术能在多种KT模型和基准数据集上显著提升预测准确率，尤其在多知识点题目数据集上提升更为明显，且方法高效、易于集成。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="227-A-Decoupled-LOB-Representation-Framework-for-Multilevel-Manipulation-Detection-with-Supervised-Contrastive-Learningpdf"><a href="#227-A-Decoupled-LOB-Representation-Framework-for-Multilevel-Manipulation-Detection-with-Supervised-Contrastive-Learningpdf" class="headerlink" title="227. A Decoupled LOB Representation Framework for Multilevel Manipulation Detection with Supervised Contrastive Learningpdf"></a>227. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Decoupled_LOB_Representation_Framework_for_Multilevel_Manipulation_Detection_with_Supervised_Contr.pdf">A Decoupled LOB Representation Framework for Multilevel Manipulation Detection with Supervised Contrastive Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southern University of Science and Technology</span></p><p>本文提出一种用于金融市场多层级操纵检测的表征学习框架，核心包括级联限制性委托单（LOB）表征管道和监督式对比学习。该方法通过Transformer编码器和混合损失函数，有效提升多层级异常检测性能，实验结果表明在多种模型下均优于现有方法，特别是在罕见事件检测上表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="228-CP4SBI-Local-Conformal-Calibration-of-Credible-Sets-in-Simulation-Based-Inferencepdf"><a href="#228-CP4SBI-Local-Conformal-Calibration-of-Credible-Sets-in-Simulation-Based-Inferencepdf" class="headerlink" title="228. CP4SBI: Local Conformal Calibration of Credible Sets in Simulation-Based Inferencepdf"></a>228. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CP4SBI__Local_Conformal_Calibration_of_Credible_Sets_in_Simulation-Based_Inference.pdf">CP4SBI: Local Conformal Calibration of Credible Sets in Simulation-Based Inference</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Federal University of São Carlos</span></p><p>该论文提出CP4SBI框架，将局部和CDF变换的共形校准方法应用于仿真推断(SBI)后验分布的可信区间构建，支持任意后验估计器和打分函数。实验结果表明，CP4SBI在多个SBI基准任务上显著提升了可信区间的条件覆盖率与校准效果，提升了后验不确定性的可靠性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="229-TabResFlow-A-Normalizing-Spline-Flow-Model-for-Probabilistic-Univariate-Tabular-Regressionpdf"><a href="#229-TabResFlow-A-Normalizing-Spline-Flow-Model-for-Probabilistic-Univariate-Tabular-Regressionpdf" class="headerlink" title="229. TabResFlow: A Normalizing Spline Flow Model for Probabilistic Univariate Tabular Regressionpdf"></a>229. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TabResFlow__A_Normalizing_Spline_Flow_Model_for_Probabilistic_Univariate_Tabular_Regression.pdf">TabResFlow: A Normalizing Spline Flow Model for Probabilistic Univariate Tabular Regression</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Hildesheim</span></p><p>该论文提出了TabResFlow，一种结合ResNet风格多层感知机特征提取与条件神经样条流（RQ-NSF）密度估计的新架构，用于单变量表格数据概率回归。TabResFlow在九个公开基准数据集和真实二手车定价任务上，相比现有概率回归方法（如TreeFlow和NodeFlow）在概率拟合精度（NLL）和推理速度上均有显著提升，并提出了AURC评价指标验证模型在不确定性量化中的优越性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="230-Score-Matching-on-Large-Geometric-Graphs-for-Cosmology-Generationpdf"><a href="#230-Score-Matching-on-Large-Geometric-Graphs-for-Cosmology-Generationpdf" class="headerlink" title="230. Score Matching on Large Geometric Graphs for Cosmology Generationpdf"></a>230. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Score_Matching_on_Large_Geometric_Graphs_for_Cosmology_Generation.pdf">Score Matching on Large Geometric Graphs for Cosmology Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Eindhoven University of Technology</span></p><p>该论文提出了一种基于score-matching的生成模型，结合E(3)等变图神经网络和拓扑感知噪声调度，用于在大规模几何图上生成宇宙学结构。方法通过物理动机的均匀先验、周期边界条件和对称性约束实现高效、物理一致的银河团簇生成，并显著优于现有扩散模型。实验表明，该模型可扩展至60万节点的大规模图，生成结果在聚类统计和计算效率上均优于基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="231-Sig-DEG-for-Distillation-Making-Diffusion-Models-Faster-and-Lighterpdf"><a href="#231-Sig-DEG-for-Distillation-Making-Diffusion-Models-Faster-and-Lighterpdf" class="headerlink" title="231. Sig-DEG for Distillation: Making Diffusion Models Faster and Lighterpdf"></a>231. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Sig-DEG_for_Distillation__Making_Diffusion_Models_Faster_and_Lighter.pdf">Sig-DEG for Distillation: Making Diffusion Models Faster and Lighter</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University College London</span></p><p>该论文提出了Sig-DEG（Signature-based Differential Equation Generator）方法，用于将预训练扩散模型蒸馏为更高效的生成器。其核心技术是利用部分路径特征（partial signatures）和递归神经网络结构，近似反向扩散SDE过程，实现粗时间分辨率下的高效仿真。方法将蒸馏过程建模为监督学习任务，以高分辨教师模型轨迹为标签，训练Sig-DEG在推理时能用极少步数生成高质量样本。实验表明，Sig-DEG在MNIST、合成分布与时间序列等任务中，能以5-10步推理达到数十到上百倍加速，同时保持甚至提升生成质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="232-Bridging-Graph-and-State-Space-Modeling-for-Intensive-Care-Unit-Length-of-Stay-Predictionpdf"><a href="#232-Bridging-Graph-and-State-Space-Modeling-for-Intensive-Care-Unit-Length-of-Stay-Predictionpdf" class="headerlink" title="232. Bridging Graph and State-Space Modeling for Intensive Care Unit Length of Stay Predictionpdf"></a>232. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Bridging_Graph_and_State-Space_Modeling_for_Intensive_Care_Unit_Length_of_Stay_Prediction.pdf">Bridging Graph and State-Space Modeling for Intensive Care Unit Length of Stay Prediction</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cambridge</span></p><p>该论文提出了S2G-Net，一种结合状态空间模型（Mamba）和多视图图神经网络（GraphGPS）的双路径神经架构，用于ICU住院时长预测。方法通过时序路径捕捉患者动态轨迹，图路径整合诊断、语义和管理特征构建的患者相似性图，并融合静态特征，提升预测准确性和模型解释性。实验结果显示，S2G-Net在MIMIC-IV大规模临床数据集上显著优于主流时序、图和混合模型，具备高效能、良好解释性和强鲁棒性，为临床资源管理提供有效解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="233-GateTS-Versatile-and-Efficient-Forecasting-via-Attention-Inspired-routed-Mixture-of-Expertspdf"><a href="#233-GateTS-Versatile-and-Efficient-Forecasting-via-Attention-Inspired-routed-Mixture-of-Expertspdf" class="headerlink" title="233. GateTS: Versatile and Efficient Forecasting via Attention-Inspired routed Mixture-of-Expertspdf"></a>233. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GateTS__Versatile_and_Efficient_Forecasting_via_Attention-Inspired_routed_Mixture-of-Experts.pdf">GateTS: Versatile and Efficient Forecasting via Attention-Inspired routed Mixture-of-Experts</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Lviv Polytechnic National University</span></p><p>本文提出了GateTS，一种结合稀疏Mixture-of-Experts结构和创新注意力启发门控机制的时间序列预测模型。该方法通过Kronecker乘积将输入与专家查询进行二阶交互，实现无需辅助均衡损失即可自适应选择专家，并在多种真实数据集上以更少的参数超越PatchTST和LSTM等主流模型。结果表明，GateTS在保证高预测精度的同时，大幅降低了计算与参数成本，适用于精度和效率均受限的实际预测场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="234-Effective-Clustering-for-Large-Multi-Relational-Graphspdf"><a href="#234-Effective-Clustering-for-Large-Multi-Relational-Graphspdf" class="headerlink" title="234. Effective Clustering for Large Multi-Relational Graphspdf"></a>234. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Effective_Clustering_for_Large_Multi-Relational_Graphs.pdf">Effective Clustering for Large Multi-Relational Graphs</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong Baptist University</span></p><p>本文提出了两种针对大规模多关系图聚类（MRGC）的方法：DEMM和DEMM+。DEMM采用基于多关系Dirichlet能量（MRDE）的两阶段优化目标，先融合多类型关系和节点属性生成高质量特征，再进行谱聚类；DEMM+利用近似算法和Sinkhorn-Knopp归一化显著提升计算效率和可扩展性。实验表明，DEMM+在聚类质量和速度上均优于现有20种主流方法，尤其在大规模数据集上表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="235-ShaLa-Multimodal-Shared-Latent-Space-Modellingpdf"><a href="#235-ShaLa-Multimodal-Shared-Latent-Space-Modellingpdf" class="headerlink" title="235. ShaLa: Multimodal Shared Latent Space Modellingpdf"></a>235. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ShaLa__Multimodal_Shared_Latent_Space_Modelling.pdf">ShaLa: Multimodal Shared Latent Space Modelling</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stevens Institute of Technology</span></p><p>本文提出了ShaLa，一种结合结构化推断模型和二阶段扩散先验的生成式框架，用于学习多模态数据间的共享潜在空间。ShaLa通过将各模态编码为确定性特征并融合为全局摘要，显著提升了多模态联合生成和跨模态推断的表现，尤其解决了传统多模态VAE在表达能力和先验匹配上的瓶颈。实验显示，ShaLa在多个公开多模态基准（如PolyMNIST、CUB、ShapeNet）上，相比最新多模态VAE模型在生成一致性、质量和可扩展性方面均取得了最优结果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="236-ShortListing-Model-A-Streamlined-Simplex-Diffusion-for-Discrete-Variable-Generationpdf"><a href="#236-ShortListing-Model-A-Streamlined-Simplex-Diffusion-for-Discrete-Variable-Generationpdf" class="headerlink" title="236. ShortListing Model: A Streamlined Simplex Diffusion for Discrete Variable Generationpdf"></a>236. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ShortListing_Model__A_Streamlined_SimplexDiffusion_for_Discrete_Variable_Generation.pdf">ShortListing Model: A Streamlined Simplex Diffusion for Discrete Variable Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>该论文提出Shortlisting Model (SLM)，一种基于单纯形的扩散模型，创新性地将离散变量生成视为逐步候选剪枝过程。SLM通过在单纯形质心之间建模、简化交叉熵损失和灵活的classifier-free guidance，有效提升了大词表语言建模、生物序列设计等多项任务的生成质量和可扩展性。实验结果表明SLM在DNA&#x2F;蛋白设计和文本生成等离散数据领域表现优异，尤其在非自回归场景下超越现有主流方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="237-Learning-Short-Term-and-Long-Term-Patterns-of-High-Order-Dynamics-in-Real-World-Networkspdf"><a href="#237-Learning-Short-Term-and-Long-Term-Patterns-of-High-Order-Dynamics-in-Real-World-Networkspdf" class="headerlink" title="237. Learning Short-Term and Long-Term Patterns of High-Order Dynamics in Real-World Networkspdf"></a>237. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Learning_Short-Term_and_Long-Term_Patterns_of_High-Order_Dynamics_in_Real-World_Networks.pdf">Learning Short-Term and Long-Term Patterns of High-Order Dynamics in Real-World Networks</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Chung-Ang University</span></p><p>该论文提出了一种动态超图学习方法Lincoln，通过双交互超边编码捕捉快照内高阶关系的结构与时序模式，并通过周期性时间注入和中间节点表示学习跨快照的长期周期性模式。实验表明Lincoln在七个真实数据集上的动态超边预测任务中显著优于九种现有方法，能更有效捕捉高阶动态关系。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="238-Scaling-Graph-Transformers-A-Comparative-Study-of-Sparse-and-Dense-Attentionpdf"><a href="#238-Scaling-Graph-Transformers-A-Comparative-Study-of-Sparse-and-Dense-Attentionpdf" class="headerlink" title="238. Scaling Graph Transformers: A Comparative Study of Sparse and Dense Attentionpdf"></a>238. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scaling_Graph_Transformers__A_Comparative_Study_of_Sparse_and_Dense_Attention.pdf">Scaling Graph Transformers: A Comparative Study of Sparse and Dense Attention</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent, Munich, Germany</span></p><p>本文系统比较了图Transformer中稠密与稀疏注意力机制，分析了Graphormer、SAN（稠密）、GraphGPS和Exphormer（稀疏）等代表性模型的结构、效率和性能。结论表明，稠密注意力适用于小型图，能捕捉全局依赖但计算量大，而稀疏注意力适合大规模图，具备更好可扩展性且在准确率上表现优秀。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="239-Aligning-the-Evaluation-of-Probabilistic-Predictions-with-Downstream-Valuepdf"><a href="#239-Aligning-the-Evaluation-of-Probabilistic-Predictions-with-Downstream-Valuepdf" class="headerlink" title="239. Aligning the Evaluation of Probabilistic Predictions with Downstream Valuepdf"></a>239. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Aligning_the_Evaluation_of_Probabilistic_Predictions_with_Downstream_Value.pdf">Aligning the Evaluation of Probabilistic Predictions with Downstream Value</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Tartu</span></p><p>本文提出了将概率预测的评估与下游任务价值对齐的新方法：将评估对齐问题形式化，并基于加权评分规则理论，设计利用神经网络学习的加权函数，实现与特定下游任务价值的自动对齐。通过合成数据和真实数据回归实验，验证了该方法能有效缩小传统评估指标与实际下游收益之间的差距，显著提升模型筛选与优化的实际效用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="240-PCR-CA-Parallel-Codebook-Representations-with-Contrastive-Alignment-for-Multiple-Category-App-Recommendationpdf"><a href="#240-PCR-CA-Parallel-Codebook-Representations-with-Contrastive-Alignment-for-Multiple-Category-App-Recommendationpdf" class="headerlink" title="240. PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendationpdf"></a>240. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PCR-CA__Parallel_Codebook_Representations_with_Contrastive_Alignment_for_Multiple-Category_App_Recom.pdf">PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Microsoft Store</span></p><p>本文提出PCR-CA框架，通过并行码本VQ-AE对多模态文本特征进行离散化编码，并利用对比对齐提升语义与协同嵌入空间的一致性。结合双注意力机制融合ID与内容特征，实现对多类别、长尾应用的更准确推荐。实验证明在大规模数据和线上A&#x2F;B测试中，PCR-CA在AUC和CTR等指标均显著优于现有方法，有效提升长尾与多类别App推荐效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="241-Frozen-in-Time-Parameter-Efficient-Time-Series-Transformers-via-Reservoir-Induced-Feature-Expansion-and-Fixed-Random-Dynamicspdf"><a href="#241-Frozen-in-Time-Parameter-Efficient-Time-Series-Transformers-via-Reservoir-Induced-Feature-Expansion-and-Fixed-Random-Dynamicspdf" class="headerlink" title="241. Frozen in Time: Parameter-Efficient Time Series Transformers via Reservoir-Induced Feature Expansion and Fixed Random Dynamicspdf"></a>241. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Frozen_in_Time__Parameter-Efficient_Time_Series_Transformers_via_Reservoir-Induced_Feature_Expansion.pdf">Frozen in Time: Parameter-Efficient Time Series Transformers via Reservoir-Induced Feature Expansion and Fixed Random Dynamics</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IIT Roorkee</span></p><p>该论文提出FreezeTST，一种通过在时间序列Transformer中交替插入冻结的随机特征（reservoir）块与可训练层的混合架构。冻结层提供无优化成本的非线性记忆，可训练层通过自注意力机制自适应检索这些记忆，实现参数数量和训练时间的大幅下降。实验证明，在七个长时序预测基准数据集上，FreezeTST在精度上与最佳专用Transformer模型（如PatchTST、Informer等）持平或略有超越，同时大幅减少可训练参数和训练时间，理论分析还保证了梯度稳定性和可控记忆长度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="242-Dynamic-Fusion-Multimodal-Network-for-SpeechWellness-Detectionpdf"><a href="#242-Dynamic-Fusion-Multimodal-Network-for-SpeechWellness-Detectionpdf" class="headerlink" title="242. Dynamic Fusion Multimodal Network for SpeechWellness Detectionpdf"></a>242. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dynamic_Fusion_Multimodal_Network_for_SpeechWellness_Detection.pdf">Dynamic Fusion Multimodal Network for SpeechWellness Detection</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northwestern Polytechnical University</span></p><p>本文提出了一种轻量级多分支多模态网络用于SpeechWellness检测，整合了时域、时频域声学特征和文本特征，并通过动态融合机制自适应调整各模态权重。实验结果表明，该方法在参数量减少78%的同时，准确率较官方基线提高5%，验证了多模态融合与动态加权的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="243-Training-Transformers-for-Mesh-Based-Simulationspdf"><a href="#243-Training-Transformers-for-Mesh-Based-Simulationspdf" class="headerlink" title="243. Training Transformers for Mesh-Based Simulationspdf"></a>243. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Training_Transformers_for_Mesh-Based_Simulations.pdf">Training Transformers for Mesh-Based Simulations</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mines Paris - PSL University</span></p><p>本文提出了一种新颖的基于Transformer的图神经网络架构，用于高效模拟复杂物理过程，利用邻接矩阵作为稀疏注意力掩码，并通过膨胀滑动窗口、全局注意力和随机连接扩展感受野。综合实验表明，该方法在大规模3D网格流体动力学数据集上显著提升了仿真精度和效率，最大模型对比SOTA平均提升38.8%，最小模型比MeshGraphNet快7倍且更小。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="244-Enhancing-Differentially-Private-Linear-Regression-via-Public-Second-Momentpdf"><a href="#244-Enhancing-Differentially-Private-Linear-Regression-via-Public-Second-Momentpdf" class="headerlink" title="244. Enhancing Differentially Private Linear Regression via Public Second-Momentpdf"></a>244. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Enhancing_Differentially_Private_Linear_Regression_via_Public_Second-Moment.pdf">Enhancing Differentially Private Linear Regression via Public Second-Moment</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The School of Mathematics, Northwest University</span></p><p>本文提出了一种基于公共二阶矩阵变换的差分隐私线性回归方法，通过利用公共数据的二阶矩阵对私有数据进行变换，改善了充分统计扰动法（SSP-OLSE）在无界数据下的数值稳定性和回归精度。理论和实验结果表明，该方法在保证差分隐私的同时，相较于传统SSP-OLSE具有更好的鲁棒性和更低的误差界。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="245-A-Novel-Framework-for-Uncertainty-Quantification-via-Proper-Scores-for-Classification-and-Beyondpdf"><a href="#245-A-Novel-Framework-for-Uncertainty-Quantification-via-Proper-Scores-for-Classification-and-Beyondpdf" class="headerlink" title="245. A Novel Framework for Uncertainty Quantification via Proper Scores for Classification and Beyondpdf"></a>245. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Novel_Framework_for_Uncertainty_Quantification_via_Proper_Scores_for_Classification_and_Beyond.pdf">A Novel Framework for Uncertainty Quantification via Proper Scores for Classification and Beyond</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Johann Wolfgang Goethe-Universität</span></p><p>本文提出了一种基于Proper Scores的通用不确定性量化理论框架，统一了回归、分类及生成建模等多种任务中的不确定性估计与模型校准，并通过理论联系Bregman信息、偏差-方差分解、校准误差等核心量。方法上，论文系统推广了Proper Scores在偏差-方差分解、核方法、生成模型多领域中的不确定性估计，提出了新型校准误差估计器和评估方法，并在多个实际任务如图像、音频和大语言模型生成中进行了实证验证。结论显示该框架能有效提升模型不确定性估计的可靠性与泛化性，在大语言模型等任务中超越现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="246-Spectrum-Prediction-in-the-Fractional-Fourier-Domain-with-Adaptive-Filteringpdf"><a href="#246-Spectrum-Prediction-in-the-Fractional-Fourier-Domain-with-Adaptive-Filteringpdf" class="headerlink" title="246. Spectrum Prediction in the Fractional Fourier Domain with Adaptive Filteringpdf"></a>246. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Spectrum_Prediction_in_the_Fractional_Fourier_Domain_with_Adaptive_Filtering.pdf">Spectrum Prediction in the Fractional Fourier Domain with Adaptive Filtering</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University of Aeronautics and Astronautics</span></p><p>本文提出了一种光谱预测新框架SFFP，核心方法包括自适应分数阶傅里叶变换（FrFT）模块将数据映射到最优分数阶傅里叶域以增强趋势与噪声的可分性，随后采用自适应滤波模块保留关键预测特征、抑制噪声，最后利用复值神经网络在该域内完成趋势预测。实验证明SFFP在实际光谱数据集上显著优于现有主流光谱预测和时间序列预测方法，实现了更高精度的预测效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="247-FasterVoiceGrad-Faster-One-step-Diffusion-Based-Voice-Conversion-with-Adversarial-Diffusion-Conversion-Distillationpdf"><a href="#247-FasterVoiceGrad-Faster-One-step-Diffusion-Based-Voice-Conversion-with-Adversarial-Diffusion-Conversion-Distillationpdf" class="headerlink" title="247. FasterVoiceGrad: Faster One-step Diffusion-Based Voice Conversion with Adversarial Diffusion Conversion Distillationpdf"></a>247. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FasterVoiceGrad__Faster_One-step_Diffusion-Based_Voice_Conversion_with_Adversarial_Diffusion_Convers.pdf">FasterVoiceGrad: Faster One-step Diffusion-Based Voice Conversion with Adversarial Diffusion Conversion Distillation</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">NTT, Inc., Japan</span></p><p>本文提出了FasterVoiceGrad，一种通过对扩散模型和内容编码器同时进行蒸馏、结合对抗性扩散转换蒸馏（ADCD）的单步扩散语音转换方法。该方法显著提升了语音转换速度，在保证音质和说话人相似度的前提下，在GPU和CPU上分别实现了6.6倍和1.8倍的加速。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="248-Ada-TransGNN-An-Air-Quality-Prediction-Model-Based-On-Adaptive-Graph-Convolutional-Networkspdf"><a href="#248-Ada-TransGNN-An-Air-Quality-Prediction-Model-Based-On-Adaptive-Graph-Convolutional-Networkspdf" class="headerlink" title="248. Ada-TransGNN: An Air Quality Prediction Model Based On Adaptive Graph Convolutional Networkspdf"></a>248. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Ada-TransGNN__An_Air_Quality_Prediction_Model_Based_On_Adaptive_Graph_Convolutional_Networks.pdf">Ada-TransGNN: An Air Quality Prediction Model Based On Adaptive Graph Convolutional Networks</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">East China University of Science and Technology</span></p><p>本文提出了Ada-TransGNN，一种基于Transformer和自适应图卷积网络的空气质量预测方法，通过引入多头注意力机制和图卷积模块，动态提取复杂时空依赖特征，并自适应学习最优图结构捕捉监测点间空间关系。此外，辅助任务学习模块结合Moran系数增强空间自相关性建模。实验表明，该方法在短期和长期空气质量预测上均优于现有主流模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="249-EEG-FM-Bench-A-Comprehensive-Benchmark-for-the-Systematic-Evaluation-of-EEG-Foundation-Modelspdf"><a href="#249-EEG-FM-Bench-A-Comprehensive-Benchmark-for-the-Systematic-Evaluation-of-EEG-Foundation-Modelspdf" class="headerlink" title="249. EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Modelspdf"></a>249. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/EEG-FM-Bench__A_Comprehensive_Benchmark_for_the_Systematic_Evaluation_of_EEG_Foundation_Models.pdf">EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tongji University</span></p><p>本文提出了EEG-FM-Bench，这是首个面向脑电基础模型（EEG-FMs）的系统化标准化评测基准，包括14个数据集、10类典型任务，统一数据预处理、评测协议和开放源码工具链，并对五种主流EEG-FM模型在多种微调策略下进行定量和定性分析。实验表明，多任务学习、精细化时空特征建模及融入神经生理先验能显著提升模型泛化能力，现有主流基础模型在冻结骨干时表现出较大泛化缺口，需改进高层语义表征学习。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="250-Copyright-Protection-for-3D-Molecular-Structures-with-Watermarkingpdf"><a href="#250-Copyright-Protection-for-3D-Molecular-Structures-with-Watermarkingpdf" class="headerlink" title="250. Copyright Protection for 3D Molecular Structures with Watermarkingpdf"></a>250. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Copyright_Protection_for_3D_Molecular_Structures_with_Watermarking.pdf">Copyright Protection for 3D Molecular Structures with Watermarking</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">City University of Hong Kong</span></p><p>本文提出了首个针对分子结构的AI驱动水印技术，通过利用原子级节点和边特征，在保持分子属性和结构完整性的前提下嵌入水印，并采用SE(3)不变特征与多维尺度缩放（MDS）增强对旋转、平移、反射等仿射变换的鲁棒性。实验结果表明，该方法在主流分子生成数据集和模型上能以高于95%准确率提取水印，同时分子基本属性损失极小且下游分子对接功能保持稳定，有效实现了分子知识产权保护。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="251-Text-Meets-Topology-Rethinking-Out-of-distribution-Detection-in-Text-Rich-Networkspdf"><a href="#251-Text-Meets-Topology-Rethinking-Out-of-distribution-Detection-in-Text-Rich-Networkspdf" class="headerlink" title="251. Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networkspdf"></a>251. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Text_Meets_Topology__Rethinking_Out-of-distribution_Detection_in_Text-Rich_Networks.pdf">Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networks</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Queensland</span></p><p>本文提出TextTopoOOD，一个系统性评测文本丰富网络（TrN）中分布外检测（OOD）的多维度框架，涵盖属性、结构、主题标签和领域等多种分布偏移，同时提出TNT-OOD方法，利用结构编码器、跨模态注意力机制和超网络动态融合节点文本与拓扑特征，提升了ID&#x2F;OOD区分能力。实验表明，TNT-OOD在11个TrN数据集和4类OOD场景中均优于现有方法，有效应对复杂的文本-结构耦合分布偏移挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="252-On-the-Edge-of-Memorization-in-Diffusion-Modelspdf"><a href="#252-On-the-Edge-of-Memorization-in-Diffusion-Modelspdf" class="headerlink" title="252. On the Edge of Memorization in Diffusion Modelspdf"></a>252. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/On_the_Edge_of_Memorization_in_Diffusion_Models.pdf">On the Edge of Memorization in Diffusion Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">TTIC</span></p><p>本文提出了一个理论实验平台，用于研究扩散模型中的记忆化与泛化现象。作者通过在可控的高斯混合分布下，系统分析了模型容量、数据复杂度与泛化&#x2F;记忆化之间的临界点，理论上推导并实验验证了模型参数量增大时记忆化行为的发生转变点，并能精确预测这一转变。结论表明，扩散模型记忆化的临界点可通过训练损失的理论计算精确预测，该理论适用于分析和实证扩散模型泛化与记忆化行为。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="253-Characterizing-the-Behavior-of-Training-Mamba-based-State-Space-Models-on-GPUspdf"><a href="#253-Characterizing-the-Behavior-of-Training-Mamba-based-State-Space-Models-on-GPUspdf" class="headerlink" title="253. Characterizing the Behavior of Training Mamba-based State Space Models on GPUspdf"></a>253. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Characterizing_the_Behavior_of_Training_Mamba-based_State_Space_Models_on_GPUs.pdf">Characterizing the Behavior of Training Mamba-based State Space Models on GPUs</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Advanced Micro Devices (AMD)</span></p><p>本文系统分析了Mamba-based State Space Models (SSMs) 在GPU上的训练行为，通过构建多种代表性模型（包括语言、视觉、混合和图结构任务）并在NVIDIA H100上进行性能剖析，揭示了SSM关键算子在计算和内存上的瓶颈，并指出大部分算子受限于内存带宽而非算力。结论认为，要提升Mamba类模型的训练效率，需要在硬件和软件上针对内存带宽与算子执行路径进行联合优化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="254-Longitudinal-Progression-Prediction-of-Alzheimer’s-Disease-with-Tabular-Foundation-Modelpdf"><a href="#254-Longitudinal-Progression-Prediction-of-Alzheimer’s-Disease-with-Tabular-Foundation-Modelpdf" class="headerlink" title="254. Longitudinal Progression Prediction of Alzheimer’s Disease with Tabular Foundation Modelpdf"></a>254. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Longitudinal_Progression_Prediction_of_Alzheimer's_Disease_with_Tabular_Foundation_Model.pdf">Longitudinal Progression Prediction of Alzheimer’s Disease with Tabular Foundation Model</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Emory University</span></p><p>本文提出L2C-TabPFN方法，将纵向到横截面（L2C）特征变换与预训练Tabular Foundation Model（TabPFN）结合，应用于TADPOLE多模态数据集以预测阿尔茨海默病的诊断、认知评分及脑室体积。实验表明，该方法在脑室体积预测上优于XGBoost基线，能更好捕捉与疾病进展相关的结构性脑变化，展示了TabPFN模型在医学回归任务中的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="255-Quantum-Graph-Attention-Network-A-Novel-Quantum-Multi-Head-Attention-Mechanism-for-Graph-Learningpdf"><a href="#255-Quantum-Graph-Attention-Network-A-Novel-Quantum-Multi-Head-Attention-Mechanism-for-Graph-Learningpdf" class="headerlink" title="255. Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learningpdf"></a>255. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Quantum_Graph_Attention_Network__A_Novel_Quantum_Multi-Head_Attention_Mechanism_for_Graph_Learning.pdf">Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea Advanced Institute of Science and Technology (KAIST)</span></p><p>该论文提出了一种混合型的量子图注意力网络（QGAT），将变分量子电路集成到图神经网络的注意力机制中，通过振幅编码和强纠缠量子层，实现高效的多头注意力与非线性特征交互。实验结果表明，QGAT在节点分类和链路预测等任务上，对比经典GAT与GATv2表现出更优的精度和更强的抗噪鲁棒性，尤其在复杂和有噪声的图结构下展现出优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="256-The-Statistical-Fairness-Accuracy-Frontierpdf"><a href="#256-The-Statistical-Fairness-Accuracy-Frontierpdf" class="headerlink" title="256. The Statistical Fairness-Accuracy Frontierpdf"></a>256. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_Statistical_Fairness-Accuracy_Frontier.pdf">The Statistical Fairness-Accuracy Frontier</a><a href="paper.origin_link">pdf</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Rice University</span></p><p>该论文研究有限样本下的公平性与准确性（FA）前沿理论，提出了在已知和未知协方差情况下的最优估计器，量化了有限样本导致的公平性与准确性偏移，并给出了最优分配采样策略。结果表明有限数据会导致群体间风险不对称，影响公平性-准确性权衡，理论结论为实际公平算法设计和政策提供参考。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025/08/26报纸</title>
    <link href="/2025/08/2025-08-26_article/"/>
    <url>/2025/08/2025-08-26_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2025/08/26报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-26<br>📄 <strong>发现论文数量</strong>：100  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Guiding-Diffusion-Models-with-Reinforcement-Learning-for-Stable-Molecule-Generation"><a href="#1-Guiding-Diffusion-Models-with-Reinforcement-Learning-for-Stable-Molecule-Generation" class="headerlink" title="1. Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Guiding_Diffusion_Models_with_Reinforcement_Learning_for_Stable_Molecule_Generation.pdf">Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出了RLPF框架，将基于物理反馈的强化学习（PPO）与等变扩散模型结合，用于3D分子结构生成。通过将扩散去噪过程建模为马尔可夫决策过程，并利用力场评价作为奖励信号，RLPF显著提升了分子的物理稳定性和化学有效性，在QM9与GEOM-drug数据集上均优于现有方法。结论表明，基于物理反馈的RL微调能提升分子生成质量，且框架对不同扩散模型具备良好适应性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Guiding_Diffusion_Models_with_Reinforcement_Learning_for_Stable_Molecule_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-On-Zero-Shot-Reinforcement-Learning"><a href="#2-On-Zero-Shot-Reinforcement-Learning" class="headerlink" title="2. On Zero-Shot Reinforcement Learning"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/On_Zero-Shot_Reinforcement_Learning.pdf">On Zero-Shot Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cambridge</span></p><p>本文针对现实场景中零样本强化学习（Zero-Shot RL）面临的数据质量、可观测性和数据可用性三大约束，提出了保守性零样本RL算法、基于记忆的零样本RL方法和PEARL主动系统辨识算法。方法分别通过正则化、引入记忆模型和概率模型预测，系统性提升了低质量数据、部分可观测和无历史数据情况下的泛化性能。结论显示，这些方法可在现实任务中以有限甚至零先验数据实现高效泛化和任务迁移，推动RL实用化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/On_Zero-Shot_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Double-Check-My-Desired-Return-Transformer-with-Target-Alignment-for-Offline-Reinforcement-Learning"><a href="#3-Double-Check-My-Desired-Return-Transformer-with-Target-Alignment-for-Offline-Reinforcement-Learning" class="headerlink" title="3. Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Double_Check_My_Desired_Return__Transformer_with_Target_Alignment_for_Offline_Reinforcement_Learning.pdf">Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>本文提出Doctor方法，通过双重检查机制提升Transformer在离线强化学习中的目标回报对齐能力。其关键流程包括利用双向Transformer进行序列重建和动作-价值预测，并在推理时对候选动作进行采样和价值验证，实现对目标回报的精准控制。实验结果表明，Doctor在多项基准任务中实现了优于现有方法的目标回报对齐，无论是在数据集覆盖范围内还是外部均表现出色，且在医疗决策等实际场景下能够平衡治疗效果与风险，具有广泛应用前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Double_Check_My_Desired_Return_Transformer_with_Target_Alignment_for_Offline_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-Constraints-Guided-Diffusion-Reasoner-for-Neuro-Symbolic-Learning"><a href="#4-Constraints-Guided-Diffusion-Reasoner-for-Neuro-Symbolic-Learning" class="headerlink" title="4. Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Constraints-Guided_Diffusion_Reasoner_for_Neuro-Symbolic_Learning.pdf">Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出了DDReasoner，一种基于扩散模型的神经-符号学习方法，采用两阶段训练流程：首先通过掩码扩散模型进行有监督学习，培养基础推理能力；随后将扩散去噪过程形式化为马尔可夫决策过程，利用改进的PPO算法和基于约束一致性的奖励信号进行强化学习微调。实验结果显示，该方法在数独、迷宫、路径规划等符号推理任务上，能有效内化逻辑约束，显著提升逻辑一致性和准确率，并优于同类神经网络方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Constraints-Guided_Diffusion_Reasoner_for_Neuro-Symbolic_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-Reinforcement-Learning-based-Control-via-Y-wise-Affine-Neural-Networks-YANNs"><a href="#5-Reinforcement-Learning-based-Control-via-Y-wise-Affine-Neural-Networks-YANNs" class="headerlink" title="5. Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Reinforcement_Learning-based_Control_via_Y-wise_Affine_Neural_Networks_(YANNs).pdf">Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">West Virginia University</span></p><p>本文提出了一种基于Y-wise Affine Neural Networks (YANNs)的新型强化学习算法。该方法利用YANNs精确表示线性模型预测控制的分段仿射显式解，实现可解释和高效的RL初始策略，并通过在网络中注入可训练的非线性层提升对复杂系统的适应性。实验证明，在安全约束下，YANN-RL显著优于现代深度确定性策略梯度（DDPG）算法，尤其是在安全关键系统控制中表现出更高的稳定性和安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Reinforcement_Learning-based_Control_via_Y-wise_Affine_Neural_Networks_%28YANNs%29.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-OPERA-A-Reinforcement-Learning–Enhanced-Orchestrated-Planner-Executor-Architecture-for-Reasoning-Oriented-Multi-Hop-Retrieval"><a href="#6-OPERA-A-Reinforcement-Learning–Enhanced-Orchestrated-Planner-Executor-Architecture-for-Reasoning-Oriented-Multi-Hop-Retrieval" class="headerlink" title="6. OPERA: A Reinforcement Learning–Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/OPERA__A_Reinforcement_Learning--Enhanced_Orchestrated_Planner-Executor_Architecture_for_Reasoning-O.pdf">OPERA: A Reinforcement Learning–Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Information Engineering, Chinese Academy of Sciences</span></p><p>本文提出OPERA框架，通过多智能体架构（包括规划、分析-回答和重写代理）分离复杂多跳检索任务中的战略规划和执行，并利用多智能体递进式群体相对策略优化（MAPGRPO）进行逐步训练。实验表明，该方法在多跳问答基准上显著提升了检索和推理能力，尤其在复杂任务上优于现有强化学习和检索增强生成系统，体系结构的模块化和专用奖励进一步促进了性能提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/OPERA_A_Reinforcement_Learning%E2%80%93Enhanced_Orchestrated_Planner-Executor_Architecture_for_Reasoning-Oriented_Multi-Hop_Retrieval.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Representation-Learning-of-Auxiliary-Concepts-for-Improved-Student-Modeling-and-Exercise-Recommendation"><a href="#7-Representation-Learning-of-Auxiliary-Concepts-for-Improved-Student-Modeling-and-Exercise-Recommendation" class="headerlink" title="7. Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Representation_Learning_of_Auxiliary_Concepts_for_Improved_Student_Modeling_and_Exercise_Recommendat.pdf">Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Karlsruhe University of Applied Sciences</span></p><p>该论文提出了一种深度学习模型（SBRKT），通过学习稀疏二值辅助知识概念（auxiliary KCs），为每道题生成可解释的特征标签。模型将辅助KCs与传统的人类标注KCs结合，提升了学生知识追踪（如BKT、DKT）与个性化习题推荐算法的性能，并在多数据集实验中证明了辅助KCs在强化学习推荐和规划方法（如Expectimax）中的效果提升。结论显示：辅助KCs能显著提升学生模型预测和习题推荐的准确性，兼具可解释性和可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Representation_Learning_of_Auxiliary_Concepts_for_Improved_Student_Modeling_and_Exercise_Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-Pareto-Actor-Critic-for-Communication-and-Computation-Co-Optimization-in-Non-Cooperative-Federated-Learning-Services"><a href="#8-Pareto-Actor-Critic-for-Communication-and-Computation-Co-Optimization-in-Non-Cooperative-Federated-Learning-Services" class="headerlink" title="8. Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Pareto_Actor-Critic_for_Communication_and_Computation_Co-Optimization_in_Non-Cooperative_Federated_L.pdf">Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出PAC-MCoFL框架，利用Pareto Actor-Critic（PAC）结合expectile回归，在多服务提供商（SPs）非合作联邦学习场景下进行通信与计算资源的多目标联合优化。方法包括TCAD高维动作空间分解和参数化策略生成器（PAC-MCoFL-p），理论上保证收敛并显著优于现有多智能体强化学习基线，在奖励和Pareto前沿指标上提升5.8%和4.2%。结论表明，PAC-MCoFL能够有效实现多SP系统下的均衡和高效资源优化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Pareto_Actor-Critic_for_Communication_and_Computation_Co-Optimization_in_Non-Cooperative_Federated_Learning_Services.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-Breaking-Barriers-in-Software-Testing-The-Power-of-AI-Driven-Automation"><a href="#9-Breaking-Barriers-in-Software-Testing-The-Power-of-AI-Driven-Automation" class="headerlink" title="9. Breaking Barriers in Software Testing: The Power of AI-Driven Automation"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Breaking_Barriers_in_Software_Testing__The_Power_of_AI-Driven_Automation.pdf">Breaking Barriers in Software Testing: The Power of AI-Driven Automation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MUFG Bank</span></p><p>本论文提出了一个结合自然语言处理、强化学习和预测模型的AI自动化测试框架，通过将需求文档自动转化为测试用例并持续优化，实现高效、低偏差的软件测试流程。结论显示该框架在真实金融服务应用中大幅缩短测试周期、提高缺陷检测率和合规性，为复杂软件环境下的质量保障提供了可行方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Breaking_Barriers_in_Software_Testing_The_Power_of_AI-Driven_Automation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="10-Hierarchical-Decision-Making-for-Autonomous-Navigation-Integrating-Deep-Reinforcement-Learning-and-Fuzzy-Logic-in-Four-Wheel-Independent-Steering-and-Driving-Systems"><a href="#10-Hierarchical-Decision-Making-for-Autonomous-Navigation-Integrating-Deep-Reinforcement-Learning-and-Fuzzy-Logic-in-Four-Wheel-Independent-Steering-and-Driving-Systems" class="headerlink" title="10. Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Hierarchical_Decision-Making_for_Autonomous_Navigation__Integrating_Deep_Reinforcement_Learning_and_.pdf">Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Central South University</span></p><p>该论文提出了一种用于四轮独立转向与驱动(4WISD)系统自主导航的分层决策框架，将高层深度强化学习（DRL）用于全局导航决策，底层模糊逻辑控制器负责物理可行的轮速与转向控制。实验结果表明，该方法在动态工业环境中优于传统方法，训练效率和稳定性更高，并有效减少机械应力和轮胎打滑。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Hierarchical_Decision-Making_for_Autonomous_Navigation_Integrating_Deep_Reinforcement_Learning_and_Fuzzy_Logic_in_Four-Wheel_Independent_Steering_and_Driving_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-On-Kinodynamic-Global-Planning-in-a-Simplicial-Complex-Environment-A-Mixed-Integer-Approach"><a href="#11-On-Kinodynamic-Global-Planning-in-a-Simplicial-Complex-Environment-A-Mixed-Integer-Approach" class="headerlink" title="11. On Kinodynamic Global Planning in a Simplicial Complex Environment: A Mixed Integer Approach"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/On_Kinodynamic_Global_Planning_in_a_Simplicial_Complex_Environment__A_Mixed_Integer_Approach.pdf">On Kinodynamic Global Planning in a Simplicial Complex Environment: A Mixed Integer Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Innopolis University</span></p><p>本文提出了一种混合整数动力学(MIKD)规划器，将汽车类车辆在3D三角网格地形上的动力学路径规划建模为带有二次约束的混合整数优化问题，通过变量变换和McCormick松弛将其转化为混合整数线性规划，显著提升了约束满足性和计算效率。实验结果表明，该方法在满足动力学约束的同时，相较于MPPI等主流方法计算速度提升约10^4倍，且路径可行性更高，适用于复杂三维地形的全局路径规划。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/On_Kinodynamic_Global_Planning_in_a_Simplicial_Complex_Environment_A_Mixed_Integer_Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-GPL-SLAM-A-Laser-SLAM-Framework-with-Gaussian-Process-Based-Extended-Landmarks"><a href="#12-GPL-SLAM-A-Laser-SLAM-Framework-with-Gaussian-Process-Based-Extended-Landmarks" class="headerlink" title="12. GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended Landmarks"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/GPL-SLAM__A_Laser_SLAM_Framework_with_Gaussian_Process_Based_Extended_Landmarks.pdf">GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended Landmarks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">TU Delft</span></p><p>本文提出了一种基于高斯过程（GP）对象轮廓建模的激光SLAM方法，采用递归贝叶斯框架联合估计机器人位姿和基于对象的地图。方法以星凸集为对象表示，结合递归IEKF实现在线SLAM，显著提升了定位与建图的精度和效率，同时为对象形状提供不确定性边界。实验表明，该方法在仿真和真实环境中均优于传统网格法，具备更高的鲁棒性和内存效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/GPL-SLAM_A_Laser_SLAM_Framework_with_Gaussian_Process_Based_Extended_Landmarks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-Integrated-Noise-and-Safety-Management-in-UAM-via-A-Unified-Reinforcement-Learning-Framework"><a href="#13-Integrated-Noise-and-Safety-Management-in-UAM-via-A-Unified-Reinforcement-Learning-Framework" class="headerlink" title="13. Integrated Noise and Safety Management in UAM via A Unified Reinforcement Learning Framework"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Integrated_Noise_and_Safety_Management_in_UAM_via_A_Unified_Reinforcement_Learning_Framework.pdf">Integrated Noise and Safety Management in UAM via A Unified Reinforcement Learning Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Texas at Austin</span></p><p>该论文提出了一个多智能体强化学习框架，集成了城市空中交通(UAM)中的噪音影响与安全分离管理，通过基于PPO的深度神经网络(D2MAV-A架构)实现航迹高度动态调整，考虑噪音、分离和能耗三重目标。实验结果表明，RL方法能在高密度交通下平衡分离安全与噪音暴露，并揭示了安全、环保和能效间的关键权衡，为未来城市空中交通管理系统提供了可扩展的智能化解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Integrated_Noise_and_Safety_Management_in_UAM_via_A_Unified_Reinforcement_Learning_Framework.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-SAMFusion-Sensor-Adaptive-Multimodal-Fusion-for-3D-Object-Detection-in-Adverse-Weather"><a href="#14-SAMFusion-Sensor-Adaptive-Multimodal-Fusion-for-3D-Object-Detection-in-Adverse-Weather" class="headerlink" title="14. SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/SAMFusion__Sensor-Adaptive_Multimodal_Fusion_for_3D_Object_Detection_in_Adverse_Weather.pdf">SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Torc Robotics</span></p><p>本文提出SAMFusion，一种针对恶劣天气下自主驾驶场景的多模态3D目标检测方法。该方法将RGB、LiDAR、雷达和近红外门控相机特征通过深度引导的自适应融合、基于距离加权的transformer解码器在鸟瞰视角（BEV）中有效集成，实现多源传感器信息的动态加权与融合。实验表明，SAMFusion在夜间、雾天、雪天等复杂环境下，尤其是远距离和行人检测方面，显著优于当前多模态融合方法，提升最大达17.2 AP。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/SAMFusion_Sensor-Adaptive_Multimodal_Fusion_for_3D_Object_Detection_in_Adverse_Weather.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-Do-What-Teaching-Vision-Language-Action-Models-to-Reject-the-Impossible"><a href="#15-Do-What-Teaching-Vision-Language-Action-Models-to-Reject-the-Impossible" class="headerlink" title="15. Do What? Teaching Vision-Language-Action Models to Reject the Impossible"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Do_What__Teaching_Vision-Language-Action_Models_to_Reject_the_Impossible.pdf">Do What? Teaching Vision-Language-Action Models to Reject the Impossible</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Berkeley</span></p><p>该论文提出了Instruct-Verify-and-Act (IVA) 框架，基于LLARVA模型，结合大规模指令微调和结构化多模态数据，使机器人能够检测、澄清并纠正因场景缺失或不合理而无法实现的自然语言指令。实验表明IVA在识别和处理虚假前提指令方面显著优于基线，并在常规任务表现稳定，推动了语言感知机器人与人类更安全自然的交互。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Do_What_Teaching_Vision-Language-Action_Models_to_Reject_the_Impossible.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-Take-That-for-Me-Multimodal-Exophora-Resolution-with-Interactive-Questioning-for-Ambiguous-Out-of-View-Instructions"><a href="#16-Take-That-for-Me-Multimodal-Exophora-Resolution-with-Interactive-Questioning-for-Ambiguous-Out-of-View-Instructions" class="headerlink" title="16. Take That for Me: Multimodal Exophora Resolution with Interactive Questioning for Ambiguous Out-of-View Instructions"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Take_That_for_Me__Multimodal_Exophora_Resolution_with_Interactive_Questioning_for_Ambiguous_Out-of-V.pdf">Take That for Me: Multimodal Exophora Resolution with Interactive Questioning for Ambiguous Out-of-View Instructions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ritsumeikan University</span></p><p>该论文提出MIEL方法，通过结合声音源定位（SSL）、语义地图、视觉-语言模型（VLMs）及GPT-4o交互式提问，实现机器人在面对含指示词且目标物体或用户不在视野范围内的模糊指令时的多模态指代消解。实验结果表明，MIEL在用户可见时比无SSL和交互模块的方法提升约1.3倍成功率，用户不可见时提升约2倍，显著增强了机器人在实际环境下对模糊指令的理解和执行能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Take_That_for_Me_Multimodal_Exophora_Resolution_with_Interactive_Questioning_for_Ambiguous_Out-of-View_Instructions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-CoVeRaP-Cooperative-Vehicular-Perception-through-mmWave-FMCW-Radars"><a href="#17-CoVeRaP-Cooperative-Vehicular-Perception-through-mmWave-FMCW-Radars" class="headerlink" title="17. CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/CoVeRaP__Cooperative_Vehicular_Perception_through_mmWave_FMCW_Radars.pdf">CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">UC Davis</span></p><p>该论文提出CoVeRaP数据集，为多车协作毫米波FMCW雷达感知提供高质量时间对齐的雷达、摄像头和GPS数据，并设计了基于多分支自注意力网络的协作感知框架，支持中融合和晚融合两种特征整合策略，实现了多视角雷达点云的空间对齐和融合。实验结果表明中融合结合强度编码能在高IoU阈值下将mAP提升至单车基线的9倍，且始终优于晚融合，验证了多车雷达共享在提高检测鲁棒性上的显著作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/CoVeRaP_Cooperative_Vehicular_Perception_through_mmWave_FMCW_Radars.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="18-MV-RAG-Retrieval-Augmented-Multiview-Diffusion"><a href="#18-MV-RAG-Retrieval-Augmented-Multiview-Diffusion" class="headerlink" title="18. MV-RAG: Retrieval Augmented Multiview Diffusion"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/MV-RAG__Retrieval_Augmented_Multiview_Diffusion.pdf">MV-RAG: Retrieval Augmented Multiview Diffusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hebrew University of Jerusalem</span></p><p>该论文提出MV-RAG，一种检索增强的多视图扩散框架，用于文本到多视图（3D）生成。方法核心在于：首先根据文本提示从大规模2D图像库检索相关图像，利用CLIP-ViT编码和Resampler提取局部特征，再通过多视图扩散模型结合检索到的图像与文本条件生成一致且高保真的多视图图像。训练时采用混合策略，结合结构化3D多视图数据和大规模2D图集，分别采用视图重建和held-out视图预测目标，提升3D一致性与泛化能力。论文还提出自适应融合机制，根据输入分布自适应权衡模型先验与检索信号，并构建OOD-Eval基准集专门评估稀有&#x2F;分布外概念。实验表明，MV-RAG在稀有和OOD概念的3D一致性、写实性及文本对齐方面显著优于SOTA方法，同时在常规基准上表现同样优异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/MV-RAG_Retrieval_Augmented_Multiview_Diffusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Closer-to-Reality-Practical-Semi-Supervised-Federated-Learning-for-Foundation-Model-Adaptation"><a href="#19-Closer-to-Reality-Practical-Semi-Supervised-Federated-Learning-for-Foundation-Model-Adaptation" class="headerlink" title="19. Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Closer_to_Reality__Practical_Semi-Supervised_Federated_Learning_for_Foundation_Model_Adaptation.pdf">Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sony AI</span></p><p>本文提出了一种面向现实应用的半监督联邦学习（PSSFL）新设定，针对边缘设备上仅有低分辨率无标签数据、服务器拥有有限高分辨率有标签数据的场景，提出了FedMox框架。FedMox通过稀疏空间专家混合（Mixture-of-Experts）结构和Soft-Mixture参数融合策略，解决了冻结骨干网络、分辨率不匹配和顺序更新导致的泛化与稳定性难题。实验表明，FedMox在自动驾驶等真实数据集上显著优于现有方法，实现了高效、隐私保护的基础模型适配，推动了复杂任务下联邦学习的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Closer_to_Reality_Practical_Semi-Supervised_Federated_Learning_for_Foundation_Model_Adaptation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-Towards-Open-World-Detection-A-Survey"><a href="#20-Towards-Open-World-Detection-A-Survey" class="headerlink" title="20. Towards Open World Detection: A Survey"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Towards_Open_World_Detection__A_Survey.pdf">Towards Open World Detection: A Survey</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universitatea Politehnica Timisoara</span></p><p>本论文系统梳理并提出“开放世界检测（OWD）”作为统一视觉领域中类别无关、任务泛化检测模型的总称，涵盖了显著性检测、前景&#x2F;背景分离、分布外检测、零样本检测、开放世界目标检测、视觉大语言模型等子领域，强调它们的融合趋势和未来一体化发展。结论指出，随着大规模多模态基础模型的兴起，视觉感知任务正逐步融合，未来OWD将成为应对新颖性与动态变化场景的核心技术路径，对多领域有重大影响，尤其在需要鲁棒性和动态更新的应用中将日益重要。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Towards_Open_World_Detection_A_Survey.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-Disentangled-Multi-modal-Learning-of-Histology-and-Transcriptomics-for-Cancer-Characterization"><a href="#21-Disentangled-Multi-modal-Learning-of-Histology-and-Transcriptomics-for-Cancer-Characterization" class="headerlink" title="21. Disentangled Multi-modal Learning of Histology and Transcriptomics for Cancer Characterization"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Disentangled_Multi-modal_Learning_of_Histology_and_Transcriptomics_for_Cancer_Characterization.pdf">Disentangled Multi-modal Learning of Histology and Transcriptomics for Cancer Characterization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cambridge</span></p><p>该论文提出一个两阶段的多模态学习框架，利用解耦融合模块（DMSF）、置信度引导梯度协调（CGC），以及尺度间基因表达一致性（IGC）策略，将癌症组织病理图像（WSI）和转录组数据分解到肿瘤和微环境子空间并多尺度融合。通过子空间知识蒸馏（SKD）和信息聚合模块（ITA），实现仅用WSI的高效、可解释癌症诊断、分级与生存预测。实验显示该方法在多项任务和数据集均优于现有方法，具备广泛临床适用性和较强泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Disentangled_Multi-modal_Learning_of_Histology_and_Transcriptomics_for_Cancer_Characterization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-HOSt3R-Keypoint-free-Hand-Object-3D-Reconstruction-from-RGB-images"><a href="#22-HOSt3R-Keypoint-free-Hand-Object-3D-Reconstruction-from-RGB-images" class="headerlink" title="22. HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/HOSt3R__Keypoint-free_Hand-Object_3D_Reconstruction_from_RGB_images.pdf">HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">NAVER LABS Europe</span></p><p>本文提出了一种无需关键点检测的手-物体三维重建方法HOSt3R，核心为基于Vision Transformer的点云对齐和姿态平均，通过对单目视频每对图像估计稠密3D点图和相对姿态，再进行多视角位姿平均及神经隐式建模，实现无模板、无相机内参假设的高精度三维重建。实验表明，该方法在SHOWMe基准上表现优异，并能泛化至HO3D等新数据集，在复杂遮挡和纹理弱物体下也具备鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/HOSt3R_Keypoint-free_Hand-Object_3D_Reconstruction_from_RGB_images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-HAMSt3R-Human-Aware-Multi-view-Stereo-3D-Reconstruction"><a href="#23-HAMSt3R-Human-Aware-Multi-view-Stereo-3D-Reconstruction" class="headerlink" title="23. HAMSt3R: Human-Aware Multi-view Stereo 3D Reconstruction"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/HAMSt3R__Human-Aware_Multi-view_Stereo_3D_Reconstruction.pdf">HAMSt3R: Human-Aware Multi-view Stereo 3D Reconstruction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KAUST</span></p><p>本文提出HAMSt3R，一种基于MASt3R扩展的全新端到端前馈式多视角3D重建方法，能够从稀疏、非标定多视角图像中联合重建场景与人体。方法核心是采用DUNE蒸馏ViT编码器，并引入实例分割、DensePose和深度估计分支，实现3D点云中直接嵌入人体语义信息。实验表明，HAMSt3R在EgoHumans和EgoExo4D等具挑战性的人体场景下优于现有方法，同时在传统3D重建任务中也表现出色。结论是HAMSt3R高效地桥接了人体与场景3D理解，为实际应用提供了更强的适用性和扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/HAMSt3R_Human-Aware_Multi-view_Stereo_3D_Reconstruction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-A-Disease-Centric-Vision-Language-Foundation-Model-for-Precision-Oncology-in-Kidney-Cancer"><a href="#24-A-Disease-Centric-Vision-Language-Foundation-Model-for-Precision-Oncology-in-Kidney-Cancer" class="headerlink" title="24. A Disease-Centric Vision-Language Foundation Model for Precision Oncology in Kidney Cancer"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/A_Disease-Centric_Vision-Language_Foundation_Model_for_Precision_Oncology_in_Kidney_Cancer.pdf">A Disease-Centric Vision-Language Foundation Model for Precision Oncology in Kidney Cancer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出了RenalCLIP，一个针对肾癌的视觉-语言基础模型，通过大规模多中心CT影像与放射报告两阶段预训练（知识增强+对比学习），实现了肾肿瘤解剖评分、诊断分型、预后预测、报告生成、跨模态检索等多任务一体化自动化分析。实验表明，RenalCLIP在多项临床任务（如恶性&#x2F;侵袭性诊断及生存预测）上均显著优于主流医学影像基础模型，具备极强泛化能力与数据效率，有望提高肾癌精准诊断与个体化管理水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/A_Disease-Centric_Vision-Language_Foundation_Model_for_Precision_Oncology_in_Kidney_Cancer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-Time-Aware-One-Step-Diffusion-Network-for-Real-World-Image-Super-Resolution"><a href="#25-Time-Aware-One-Step-Diffusion-Network-for-Real-World-Image-Super-Resolution" class="headerlink" title="25. Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Time-Aware_One_Step_Diffusion_Network_for_Real-World_Image_Super-Resolution.pdf">Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nankai University</span></p><p>本文提出了一种面向真实世界图像超分辨率的时序感知单步扩散网络（TADSR），引入时序感知VAE编码器（TAE）与时序感知变分分数蒸馏损失（TAVSD），使模型能够根据不同时间步充分利用Stable Diffusion的生成先验，实现对细节与真实感的可控权衡。实验结果表明，TADSR在多个合成和真实数据集上达到了同类方法中的最优性能，并显著提升了图像的真实性和语义表达能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Time-Aware_One_Step_Diffusion_Network_for_Real-World_Image_Super-Resolution.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-Seeing-Clearly-Forgetting-Deeply-Revisiting-Fine-Tuned-Video-Generators-for-Driving-Simulation"><a href="#26-Seeing-Clearly-Forgetting-Deeply-Revisiting-Fine-Tuned-Video-Generators-for-Driving-Simulation" class="headerlink" title="26. Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Seeing_Clearly,_Forgetting_Deeply__Revisiting_Fine-Tuned_Video_Generators_for_Driving_Simulation.pdf">Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Delft University of Technology</span></p><p>本文系统分析了针对自动驾驶仿真的视频生成模型微调过程中的效果，发现虽能显著提升视觉质量和表面真实感，但却可能导致对动态场景理解能力下降，尤其是对关键交通参与者的空间准确性。作者通过引入简单的持续学习策略（如跨领域样本回放），证明可在不损失视觉质量的前提下，缓解微调引发的动态信息遗忘问题，从而更好地兼顾视觉与语义一致性，提升仿真安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Seeing_Clearly_Forgetting_Deeply_Revisiting_Fine-Tuned_Video_Generators_for_Driving_Simulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-Modular-Embedding-Recomposition-for-Zero-Shot-Incremental-Learning"><a href="#27-Modular-Embedding-Recomposition-for-Zero-Shot-Incremental-Learning" class="headerlink" title="27. Modular Embedding Recomposition for Zero-Shot Incremental Learning"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Modular_Embedding_Recomposition_for_Incremental_Learning.pdf">Modular Embedding Recomposition for Zero-Shot Incremental Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Modena and Reggio Emilia</span></p><p>该论文提出了MoDER方法，通过模块化嵌入重组，增量训练多个针对单类的文本专家模块，并在推理时组合专家以生成更适合未见类别的原型，从而提升视觉语言模型的零样本分类能力。实验表明，MoDER在14个增量学习数据集上显著提升了零样本性能，并有效缓解了遗忘问题，优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Modular_Embedding_Recomposition_for_Zero-Shot_Incremental_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-Attention-Mechanism-in-Randomized-Time-Warping"><a href="#28-Attention-Mechanism-in-Randomized-Time-Warping" class="headerlink" title="28. Attention Mechanism in Randomized Time Warping"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Attention_Mechanism_in_Randomized_Time_Warping.pdf">Attention Mechanism in Randomized Time Warping</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Japan Research Institute, Limited</span></p><p>该论文提出将Randomized Time Warping (RTW)的核心机制解释为一种自注意力机制，并与Transformer中的多头自注意力进行了比较。通过在动作识别任务（Something-Something V2数据集）中实验证明，RTW在整个输入序列上进行注意力分配，且在小样本数据下性能优于Transformer，准确率提升约5%。结论表明RTW可作为自注意力模块的有效替代方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Attention_Mechanism_in_Randomized_Time_Warping.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Exploiting-Information-Redundancy-in-Attention-Maps-for-Extreme-Quantization-of-Vision-Transformers"><a href="#29-Exploiting-Information-Redundancy-in-Attention-Maps-for-Extreme-Quantization-of-Vision-Transformers" class="headerlink" title="29. Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Exploiting_Information_Redundancy_in_Attention_Maps_for_Extreme_Quantization_of_Vision_Transformers.pdf">Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universite Paris-Saclay</span></p><p>该论文提出基于熵的注意力图分析方法（Entropy Attention Maps, EAM），通过量化每个注意力头的信息冗余，对低熵注意力头进行权重冻结和极低比特量化（如4位），从而减少模型推理的计算与内存开销。实验表明，在ImageNet-1K数据集和多种ViT架构下，EAM在10%-20%稀疏度时可提升或保持准确率，最高可达40%稀疏度而性能损失极小。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Exploiting_Information_Redundancy_in_Attention_Maps_for_Extreme_Quantization_of_Vision_Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-Robust-Small-Methane-Plume-Segmentation-in-Satellite-Imagery"><a href="#30-Robust-Small-Methane-Plume-Segmentation-in-Satellite-Imagery" class="headerlink" title="30. Robust Small Methane Plume Segmentation in Satellite Imagery"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Robust_Small_Methane_Plume_Segmentation_in_Satellite_Imagery.pdf">Robust Small Methane Plume Segmentation in Satellite Imagery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Curtin University</span></p><p>本文提出基于U-Net结合ResNet34编码器的深度学习方法，通过集成Varon ratio和Sanchez regression两种光谱增强技术优化卫星影像输入特征，实现对小型甲烷羽流（最小至400㎡，即单像素级）的精准分割。实验结果表明，该方法在敏感性和精度上明显优于传统遥感技术，验证集F1分数达78.39%，为自动化甲烷监测系统提供了强有力的解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Robust_Small_Methane_Plume_Segmentation_in_Satellite_Imagery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-UniEM-3M-A-Universal-Electron-Micrograph-Dataset-for-Microstructural-Segmentation-and-Generation"><a href="#31-UniEM-3M-A-Universal-Electron-Micrograph-Dataset-for-Microstructural-Segmentation-and-Generation" class="headerlink" title="31. UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/UniEM-3M__A_Universal_Electron_Micrograph_Dataset_for_Microstructural_Segmentation_and_Generation.pdf">UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">DP Technology</span></p><p>本论文提出了UniEM-3M，这是首个大规模多模态电子显微图（EM）数据集，包含5091张高分辨率EM图像、约300万实例分割标注及结构化文本描述，并引入了专为高密度微结构分割设计的UniEM-Net基线模型。团队还基于结构化描述训练了文本到图像扩散模型，用以数据增强和隐私保护。多项实验显示，UniEM-Net在高密度分割任务上大幅优于现有方法，生成数据能有效提升模型性能。结论：UniEM-3M及其配套工具显著推动了材料自动分析与视觉-语言模型研究。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/UniEM-3M_A_Universal_Electron_Micrograph_Dataset_for_Microstructural_Segmentation_and_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-PromptFlare-Prompt-Generalized-Defense-via-Cross-Attention-Decoy-in-Diffusion-Based-Inpainting"><a href="#32-PromptFlare-Prompt-Generalized-Defense-via-Cross-Attention-Decoy-in-Diffusion-Based-Inpainting" class="headerlink" title="32. PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/PromptFlare__Prompt-Generalized_Defense_via_Cross-Attention_Decoy_in_Diffusion-Based_Inpainting.pdf">PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sungkyunkwan University</span></p><p>PromptFlare提出了一种针对扩散模型图像修复的通用防御方法，通过在跨注意力机制中注入对抗性噪声，专门干扰无语义信息的共享BOS token，使模型难以根据文本提示进行恶意修改。该方法在EditBench数据集上实现了最优防护效果，显著降低了计算资源消耗，验证了对抗性保护的高效性和稳健性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/PromptFlare_Prompt-Generalized_Defense_via_Cross-Attention_Decoy_in_Diffusion-Based_Inpainting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-OmniCache-A-Trajectory-Oriented-Global-Perspective-on-Training-Free-Cache-Reuse-for-Diffusion-Transformer-Models"><a href="#33-OmniCache-A-Trajectory-Oriented-Global-Perspective-on-Training-Free-Cache-Reuse-for-Diffusion-Transformer-Models" class="headerlink" title="33. OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/OmniCache__A_Trajectory-Oriented_Global_Perspective_on_Training-Free_Cache_Reuse_for_Diffusion_Trans.pdf">OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhipu AI</span></p><p>OmniCache提出了一种基于采样轨迹全局分析的训练无关缓存复用方法，通过采样轨迹曲率选择最佳复用节点，并引入噪声动态估计与高&#x2F;低通滤波纠正缓存噪声，提升Diffusion Transformer模型的推理速度。实验表明该方法在视频和图像生成任务中能实现2-2.5倍加速且几乎无性能损失，尤其在采样步数较少或模型冗余低的场景下优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/OmniCache_A_Trajectory-Oriented_Global_Perspective_on_Training-Free_Cache_Reuse_for_Diffusion_Transformer_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-T-MASK-Temporal-Masking-for-Probing-Foundation-Models-across-Camera-Views-in-Driver-Monitoring"><a href="#34-T-MASK-Temporal-Masking-for-Probing-Foundation-Models-across-Camera-Views-in-Driver-Monitoring" class="headerlink" title="34. T-MASK: Temporal Masking for Probing Foundation Models across Camera Views in Driver Monitoring"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/_textsc%7BT-Mask%7D__Temporal_Masking_for_Probing_Foundation_Models_across_Camera_Views_in_Driver_Monito.pdf">T-MASK: Temporal Masking for Probing Foundation Models across Camera Views in Driver Monitoring</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Stuttgart</span></p><p>本文提出了T-MASK，一种用于驾驶员监控的视觉基础模型跨摄像头视角的轻量级探测方法。T-MASK通过计算视频帧间的嵌入差异，动态屏蔽静态、视角依赖性强的令牌，使探测模块聚焦于运动相关特征，从而提升模型对未见视角和数据稀缺动作的鲁棒性。在Drive&amp;Act数据集上的实验表明，T-MASK在不增加模型参数的情况下，显著提升了跨视角识别准确率，优于常规探测和参数高效微调方法，成为驾驶员活动识别领域的新基线。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/T-MASK_Temporal_Masking_for_Probing_Foundation_Models_across_Camera_Views_in_Driver_Monitoring.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-RAGSR-Regional-Attention-Guided-Diffusion-for-Image-Super-Resolution"><a href="#35-RAGSR-Regional-Attention-Guided-Diffusion-for-Image-Super-Resolution" class="headerlink" title="35. RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/RAGSR__Regional_Attention_Guided_Diffusion_for_Image_Super-Resolution.pdf">RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science, Wuhan University</span></p><p>该论文提出了RAGSR方法，通过结合区域细粒度文本描述和区域引导注意力机制，提升了文本驱动扩散模型在单幅图像超分辨上的表现。具体流程为先用大视觉语言模型对降质低分辨图像生成全局与区域文本描述，再通过自研区域注意力机制对齐文本与图像区域，引导扩散模型重建高质量细节。实验表明，该方法在多个公开数据集上超越现有最佳方法，有效提升了结构精度和语义一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/RAGSR_Regional_Attention_Guided_Diffusion_for_Image_Super-Resolution.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-Vision-encoders-should-be-image-size-agnostic-and-task-driven"><a href="#36-Vision-encoders-should-be-image-size-agnostic-and-task-driven" class="headerlink" title="36. Vision encoders should be image size agnostic and task driven"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Vision_encoders_should_be_image_size_agnostic_and_task_driven.pdf">Vision encoders should be image size agnostic and task driven</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">INSAIT, Sofia University “St. Kliment Ohridski”, Bulgaria</span></p><p>本文提出下一代视觉编码器应具备图像尺寸无关性和任务驱动性，受到生物视觉系统效率启发。方法上，提出了一种结合多尺度（multi-zoom）自顶向下图像patch提取、基于Transformer的迭代内部状态更新，以及强化学习（GRPO）驱动的注意力策略的视觉编码新范式，并在ImageNet-1K上通过分类任务验证了其可行性。结论表明，该方法为高效、灵活的视觉编码器设计提供了新思路，尤其是在处理大尺寸图像和任务适应性方面展现出潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Vision_encoders_should_be_image_size_agnostic_and_task_driven.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-A-Multimodal-Multitask-Framework-with-Cross-modal-Relation-and-Hierarchical-Interactive-Attention-for-Semantic-Comprehension"><a href="#37-A-Multimodal-Multitask-Framework-with-Cross-modal-Relation-and-Hierarchical-Interactive-Attention-for-Semantic-Comprehension" class="headerlink" title="37. A Multimodal-Multitask Framework with Cross-modal Relation and Hierarchical Interactive Attention for Semantic Comprehension"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/A_Multimodal-Multitask_Framework_with_Cross-modal_Relation_and_Hierarchical_Interactive_Attention_fo.pdf">A Multimodal-Multitask Framework with Cross-modal Relation and Hierarchical Interactive Attention for Semantic Comprehension</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indian Institute of Technology Indore</span></p><p>本文提出了一种新颖的多模态多任务框架MM-ORIENT，通过跨模态关系图（Cross-modal Relation Graphs）和分层交互注意力机制（Hierarchical Interactive Monomodal Attention, HIMA），实现了对图像和文本的联合理解，适用于情感、幽默、讽刺、冒犯和动机等多任务。该方法在特征层减少不同模态的直接交互，降低了噪声传播，并通过多层注意力增强判别性特征，最终在三大公开多模态数据集上取得了优于现有方法的效果，验证了方法的有效性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/A_Multimodal-Multitask_Framework_with_Cross-modal_Relation_and_Hierarchical_Interactive_Attention_for_Semantic_Comprehension.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-EdgeDoc-Hybrid-CNN-Transformer-Model-for-Accurate-Forgery-Detection-and-Localization-in-ID-Documents"><a href="#38-EdgeDoc-Hybrid-CNN-Transformer-Model-for-Accurate-Forgery-Detection-and-Localization-in-ID-Documents" class="headerlink" title="38. EdgeDoc: Hybrid CNN-Transformer Model for Accurate Forgery Detection and Localization in ID Documents"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/EdgeDoc__Hybrid_CNN-Transformer_Model_for_Accurate_Forgery_Detection_and_Localization_in_ID_Document.pdf">EdgeDoc: Hybrid CNN-Transformer Model for Accurate Forgery Detection and Localization in ID Documents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Idiap Research Institute</span></p><p>本文提出了EdgeDoc，一种结合轻量级卷积-Transformer混合架构和辅助NoisePrint特征的文档伪造检测与定位方法。方法流程包括利用TruFor管线提取NoisePrint，与原始图像融合后输入卷积-Transformer网络，实现同时的分类和伪造区域定位。实验结果表明，EdgeDoc在FantasyID数据集上表现优于现有基线方法，并在ICCV 2025 DeepID Challenge中取得第三名，验证了其实用性和竞争力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/EdgeDoc_Hybrid_CNN-Transformer_Model_for_Accurate_Forgery_Detection_and_Localization_in_ID_Documents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-An-Investigation-of-Visual-Foundation-Models-Robustness"><a href="#39-An-Investigation-of-Visual-Foundation-Models-Robustness" class="headerlink" title="39. An Investigation of Visual Foundation Models Robustness"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/An_Investigation_of_Visual_Foundation_Models_Robustness.pdf">An Investigation of Visual Foundation Models Robustness</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Queen’s University Belfast</span></p><p>本文系统性梳理和分析了视觉基础模型（VFMs）在计算机视觉任务中的鲁棒性问题，涵盖目标检测、图像分类、分割、姿态估计等场景。作者详细探讨了VFMs在应对分布漂移、噪声、空间畸变和对抗攻击时的主要需求，评估了包括对抗检测、输入变换、对抗训练、认证防御、蒸馏和对抗学习等主流防御与训练机制的有效性及其局限性。结论指出，当前防御机制多集中于有限类型攻击，建议在实际部署中需全面评估模型对分布漂移、噪声扰动和对抗扰动的鲁棒性，以提升其在关键应用领域的可靠性和适用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/An_Investigation_of_Visual_Foundation_Models_Robustness.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-Forecast-then-Calibrate-Feature-Caching-as-ODE-for-Efficient-Diffusion-Transformers"><a href="#40-Forecast-then-Calibrate-Feature-Caching-as-ODE-for-Efficient-Diffusion-Transformers" class="headerlink" title="40. Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Forecast_then_Calibrate__Feature_Caching_as_ODE_for_Efficient_Diffusion_Transformers.pdf">Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了FoCa（Forecast-then-Calibrate），将扩散Transformer中的特征缓存过程视为常微分方程（ODE）求解，采用BDF2预测器与Heun校正器结合的训练无关预测-校正框架，有效整合历史特征，实现高稳定性和大步长下的高精度特征预测。实验表明，FoCa在图像合成、视频生成和超分辨率等任务中，在无需额外训练的情况下实现了5.50×~6.45×的近无损加速，并优于现有缓存方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Forecast_then_Calibrate_Feature_Caching_as_ODE_for_Efficient_Diffusion_Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-Deep-learning-enabled-virtual-multiplexed-immunostaining-of-label-free-tissue-for-vascular-invasion-assessment"><a href="#41-Deep-learning-enabled-virtual-multiplexed-immunostaining-of-label-free-tissue-for-vascular-invasion-assessment" class="headerlink" title="41. Deep learning-enabled virtual multiplexed immunostaining of label-free tissue for vascular invasion assessment"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Deep_learning-enabled_virtual_multiplexed_immunostaining_of_label-free_tissue_for_vascular_invasion_.pdf">Deep learning-enabled virtual multiplexed immunostaining of label-free tissue for vascular invasion assessment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Los Angeles</span></p><p>该论文提出了一种基于深度学习的虚拟多重免疫组化染色方法，利用条件生成对抗网络(cGAN)和数字染色矩阵，将无标记组织的自发荧光显微图像快速转化为H&amp;E、ERG和PanCK等多重染色的明场等效图像，实现甲状腺癌血管侵犯的精准定位与解读。实验结果显示，虚拟染色与真实化学染色高度一致，部分情况下虚拟染色在染色强度和特异性上优于传统方法。该方法能显著提升组织病理诊断效率和准确性，并减少传统染色中因组织丢失等带来的误判风险。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Deep_learning-enabled_virtual_multiplexed_immunostaining_of_label-free_tissue_for_vascular_invasion_assessment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Through-the-Looking-Glass-A-Dual-Perspective-on-Weakly-Supervised-Few-Shot-Segmentation"><a href="#42-Through-the-Looking-Glass-A-Dual-Perspective-on-Weakly-Supervised-Few-Shot-Segmentation" class="headerlink" title="42. Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Through_the_Looking_Glass__A_Dual_Perspective_on_Weakly-Supervised_Few-Shot_Segmentation.pdf">Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University of Science and Technology</span></p><p>本文提出了一种同源异构网络（TLG）用于弱监督少样本语义分割，通过异构视觉聚合（HA）模块增强支持-查询对的互补性，同时设计异构转运（HT）模块减少语义噪声，并引入异构CLIP（HC）模块提升多模态泛化能力。实验表明，TLG仅用1&#x2F;24参数即可在Pascal-5i和COCO-20i等数据集上超越现有SOTA模型，并首次实现基于相同主干网络的弱监督模型优于全监督模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Through_the_Looking_Glass_A_Dual_Perspective_on_Weakly-Supervised_Few-Shot_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-Ensemble-learning-of-foundation-models-for-precision-oncology"><a href="#43-Ensemble-learning-of-foundation-models-for-precision-oncology" class="headerlink" title="43. Ensemble learning of foundation models for precision oncology"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Ensemble_learning_of_foundation_models_for_precision_oncology.pdf">Ensemble learning of foundation models for precision oncology</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University School of Medicine</span></p><p>该论文提出了ELF框架，通过集成五个主流病理基础模型（如GigaPath、CONCH、Virchow2等），采用对比学习和弱监督学习，在53,699张全切片病理图像上进行预训练，生成统一的切片级表示。ELF在疾病分类、分型、分子标志物检测及抗癌及免疫治疗响应预测等115个临床任务中，均显著优于现有基础模型和滑片级模型，表现出更高的准确性和泛化能力，适用于数据有限的临床场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Ensemble_learning_of_foundation_models_for_precision_oncology.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Wavelet-Space-Super-Resolution-for-Real-Time-Rendering"><a href="#44-Wavelet-Space-Super-Resolution-for-Real-Time-Rendering" class="headerlink" title="44. Wavelet-Space Super-Resolution for Real-Time Rendering"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Wavelet-Space_Super-Resolution_for_Real-Time_Rendering.pdf">Wavelet-Space Super-Resolution for Real-Time Rendering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Thapathali Campus, IOE, Tribhuwan University</span></p><p>该论文提出了一种基于小波域特征分解的神经网络超分辨率方法，提高实时渲染管线的重建质量。通过在小波域中分离低、高频细节，并利用空间G-buffer和时序历史帧预测小波系数，结合逆小波合成重建，高效保留纹理细节和结构一致性。实验结果表明，与传统RGB空间回归和现有DFASR框架相比，采用SWT的小波表示显著提升了PSNR和LPIPS指标，能够在现代GPU上实现实时性能，适用于高保真图形渲染。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Wavelet-Space_Super-Resolution_for_Real-Time_Rendering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-Two-flow-Feedback-Multi-scale-Progressive-Generative-Adversarial-Network"><a href="#45-Two-flow-Feedback-Multi-scale-Progressive-Generative-Adversarial-Network" class="headerlink" title="45. Two-flow Feedback Multi-scale Progressive Generative Adversarial Network"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Two-flow_Feedback_Multi-scale_Progressive_Generative_Adversarial_Network.pdf">Two-flow Feedback Multi-scale Progressive Generative Adversarial Network</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Qilu Normal University</span></p><p>该论文提出了一种创新的两流反馈多尺度渐进生成对抗网络（MSPG-SEN），融合了动态嵌入注意力机制（DEMA）、全局连接的两流动态残差网络（GCTDRN）、自适应感知-行为反馈环（APFL）以及对抗特征增强模块，显著提升了图像生成质量和多样性，并优化了训练过程和效率。实验表明，该方法在多种公开数据集上实现了高质量、高多样性的图像生成，具备良好的泛化能力与计算效率，尤其在复杂场景和细节保留方面表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Two-flow_Feedback_Multi-scale_Progressive_Generative_Adversarial_Network.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-A-Unified-Voxel-Diffusion-Module-for-Point-Cloud-3D-Object-Detection"><a href="#46-A-Unified-Voxel-Diffusion-Module-for-Point-Cloud-3D-Object-Detection" class="headerlink" title="46. A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/A_Unified_Voxel_Diffusion_Module_for_Point_Cloud_3D_Object_Detection.pdf">A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了一种新颖的体素扩散模块（VDM），结合稀疏3D卷积、子流形稀疏卷积和残差连接，用于在点云三维目标检测中增强体素级空间扩散和特征聚合。通过在序列化前对体素特征进行扩散和细粒度空间信息聚合，VDM可无缝集成到主流Transformer或SSM检测模型中，在Waymo、nuScenes、Argoverse 2和ONCE等多个基准数据集上显著提升检测精度，达到了新的SOTA水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/A_Unified_Voxel_Diffusion_Module_for_Point_Cloud_3D_Object_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-Wavelet-Enhanced-PaDiM-for-Industrial-Anomaly-Detection"><a href="#47-Wavelet-Enhanced-PaDiM-for-Industrial-Anomaly-Detection" class="headerlink" title="47. Wavelet-Enhanced PaDiM for Industrial Anomaly Detection"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Wavelet-Enhanced_PaDiM_for_Industrial_Anomaly_Detection.pdf">Wavelet-Enhanced PaDiM for Industrial Anomaly Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Saint Louis University</span></p><p>该论文提出了Wavelet-Enhanced PaDiM (WE-PaDiM)方法，通过在多层CNN特征上应用二维离散小波变换(DWT)，选择特定频率子带系数并进行对齐与拼接，再采用PaDiM的高斯建模，实现了结构化、可解释的特征选择用于工业图像异常检测。实验表明，WE-PaDiM在MVTec AD工业异常数据集上获得了平均99.32%的Image-AUC和92.10%的Pixel-AUC，显示出在检测和定位工业异常方面的高效和强竞争力，并且可根据任务需求优化配置。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Wavelet-Enhanced_PaDiM_for_Industrial_Anomaly_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="48-GLARE-Agentic-Reasoning-for-Legal-Judgment-Prediction"><a href="#48-GLARE-Agentic-Reasoning-for-Legal-Judgment-Prediction" class="headerlink" title="48. GLARE: Agentic Reasoning for Legal Judgment Prediction"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/GLARE__Agentic_Reasoning_for_Legal_Judgment_Prediction.pdf">GLARE: Agentic Reasoning for Legal Judgment Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Gaoling School of Artificial Intelligence, Renmin University of China</span></p><p>本文提出了GLARE（Agentic Legal Reasoning Framework），通过三大模块（候选罪名扩展、判例推理演示、法律搜索增强推理）动态获取关键法律知识，辅助大语言模型进行法律判决预测。实验证明该方法在真实法律数据集上显著提升了判决准确率和推理链条的可解释性，特别是在处理低频或混淆罪名时效果突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/GLARE_Agentic_Reasoning_for_Legal_Judgment_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-LLM-Based-Agents-for-Competitive-Landscape-Mapping-in-Drug-Asset-Due-Diligence"><a href="#49-LLM-Based-Agents-for-Competitive-Landscape-Mapping-in-Drug-Asset-Due-Diligence" class="headerlink" title="49. LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/LLM-Based_Agents_for_Competitive_Landscape_Mapping_in_Drug_Asset_Due_Diligence.pdf">LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bioptic.io</span></p><p>本文提出了一种基于LLM的多智能体系统，用于药物资产尽职调查中的竞争格局映射。方法包括多阶段REACT风格的Web智能体进行竞争药物发现，并结合LLM-as-judge“Competitor-Validator”过滤假阳性，流程涵盖多模态信息解析、规范化、属性抽取与迭代多步推理。实验在真实生物医药VC尽调备忘录数据集上，召回率和精度均优于OpenAI Deep Research与Perplexity Labs等通用系统，显著提升分析师工作效率（提速约20倍），且在难样本上仍保持稳定表现，系统已落地企业生产环境。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/LLM-Based_Agents_for_Competitive_Landscape_Mapping_in_Drug_Asset_Due_Diligence.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-Boardwalk-Towards-a-Framework-for-Creating-Board-Games-with-LLMs"><a href="#50-Boardwalk-Towards-a-Framework-for-Creating-Board-Games-with-LLMs" class="headerlink" title="50. Boardwalk: Towards a Framework for Creating Board Games with LLMs"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Boardwalk__Towards_a_Framework_for_Creating_Board_Games_with_LLMs.pdf">Boardwalk: Towards a Framework for Creating Board Games with LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universidade Federal do Rio Grande do Sul</span></p><p>该论文提出了Boardwalk，一个用于简化抽象棋盘游戏实现的Python API，并系统评估了三种主流大语言模型（Claude, DeepSeek, ChatGPT）在根据自然语言规则生成棋盘游戏代码的能力。实验表明Claude模型表现最佳，55.6%的代码无错误，且大多数错误可快速修复，证明LLM辅助棋盘游戏开发具有可行性和效率提升价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Boardwalk_Towards_a_Framework_for_Creating_Board_Games_with_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-PediatricsMQA-a-Multi-modal-Pediatrics-Question-Answering-Benchmark"><a href="#51-PediatricsMQA-a-Multi-modal-Pediatrics-Question-Answering-Benchmark" class="headerlink" title="51. PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/PediatricsMQA__a_Multi-modal_Pediatrics_Question_Answering_Benchmark.pdf">PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohammed 6 Polytechnic University</span></p><p>本论文提出PediatricsMQA，一个覆盖文本与视觉多模态的儿科医学问答基准，包含3417道文本多选题和2067道视觉多选题，涵盖131个儿科主题和67种影像模态，数据通过手动与自动结合流程从文献、题库等多源构建。实验结果显示该基准对于当前主流LLMs和VLMs极具挑战性，尤其在低龄组表现显著下降，凸显了儿科医学领域AI模型的不足，有助于推动针对儿童医学的AI公平性和专用能力提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/PediatricsMQA_a_Multi-modal_Pediatrics_Question_Answering_Benchmark.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-RL-Is-Neither-a-Panacea-Nor-a-Mirage-Understanding-Supervised-vs-Reinforcement-Learning-Fine-Tuning-for-LLMs"><a href="#52-RL-Is-Neither-a-Panacea-Nor-a-Mirage-Understanding-Supervised-vs-Reinforcement-Learning-Fine-Tuning-for-LLMs" class="headerlink" title="52. RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/RL_Is_Neither_a_Panacea_Nor_a_Mirage__Understanding_Supervised_vs._Reinforcement_Learning_Fine-Tunin.pdf">RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">PolyTechnique Montreal</span></p><p>本论文系统分析了大语言模型（LLM）在监督微调（SFT）与强化学习微调（RL-FT）两阶段中的表现与参数变化，采用谱分析方法揭示SFT导致模型表征方向漂移（而非奇异值变化）是泛化能力下降的主因，RL-FT主要通过恢复这些方向来挽回出分布泛化性能。结论表明，RL微调可有效恢复中度SFT遗失的泛化能力，但无法修复极度过拟合导致的损失，且模型核心容量基本保持不变。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/RL_Is_Neither_a_Panacea_Nor_a_Mirage_Understanding_Supervised_vs._Reinforcement_Learning_Fine-Tuning_for_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-Beyond-Interpretability-Exploring-the-Comprehensibility-of-Adaptive-Video-Streaming-through-Large-Language-Models"><a href="#53-Beyond-Interpretability-Exploring-the-Comprehensibility-of-Adaptive-Video-Streaming-through-Large-Language-Models" class="headerlink" title="53. Beyond Interpretability: Exploring the Comprehensibility of Adaptive Video Streaming through Large Language Models"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Beyond_Interpretability__Exploring_the_Comprehensibility_of_Adaptive_Video_Streaming_through_Large_L.pdf">Beyond Interpretability: Exploring the Comprehensibility of Adaptive Video Streaming through Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出ComTree框架，将决策树生成与大语言模型（LLM）结合，用于自适应视频流的码率自适应算法设计。方法上，ComTree首先用特征筛选和教师—学生学习框架生成Rashomon集合，再用多LLM集成进行决策树可理解性评估，最终选出既高性能又高可理解性的算法。实验证明ComTree在多种网络环境下实现了与SOTA持平甚至更优的性能，并大幅提升了算法的工程师可理解性，便于持续优化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Beyond_Interpretability_Exploring_the_Comprehensibility_of_Adaptive_Video_Streaming_through_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-CETVEL-A-Unified-Benchmark-for-Evaluating-Language-Understanding-Generation-and-Cultural-Capacity-of-LLMs-for-Turkish"><a href="#54-CETVEL-A-Unified-Benchmark-for-Evaluating-Language-Understanding-Generation-and-Cultural-Capacity-of-LLMs-for-Turkish" class="headerlink" title="54. CETVEL: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Cetvel__A_Unified_Benchmark_for_Evaluating_Language_Understanding,_Generation_and_Cultural_Capacity_.pdf">CETVEL: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KUIS AI Center</span></p><p>本文提出了CETVEL，一个专为评估土耳其语大语言模型（LLM）设计的统一基准，涵盖23项任务，兼顾任务多样性和土耳其文化、语言特性，包括文本分类、问答、语法纠错、翻译等。实验结果显示，土耳其语定制模型整体性能不及多语言或通用预训练模型（如Llama 3），语法纠错、翻译和抽取式问答在模型区分上最具信息性，未来需优化针对土耳其语的指令微调策略。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/CETVEL_A_Unified_Benchmark_for_Evaluating_Language_Understanding_Generation_and_Cultural_Capacity_of_LLMs_for_Turkish.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-LLM-GUARD-Large-Language-Model-Based-Detection-and-Repair-of-Bugs-and-Security-Vulnerabilities-in-C-and-Python"><a href="#55-LLM-GUARD-Large-Language-Model-Based-Detection-and-Repair-of-Bugs-and-Security-Vulnerabilities-in-C-and-Python" class="headerlink" title="55. LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/LLM-GUARD__Large_Language_Model-Based_Detection_and_Repair_of_Bugs_and_Security_Vulnerabilities_in_C.pdf">LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Texas A&amp;M University - Central Texas</span></p><p>本论文系统评估了ChatGPT-4、Claude 3、LLaMA 4三种主流大语言模型在C++和Python代码中自动检测和修复基础编程错误、经典安全漏洞和实际生产级Bug的能力，采用多阶段上下文感知提示和细致评分标准进行实验。结果发现，所有LLM在基础语法和语义错误检测上表现优异，适用于教育与初步代码审计，但在复杂安全漏洞和大规模生产代码分析能力有限，ChatGPT-4和Claude 3在上下文推理和安全分析方面优于LLaMA 4。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/LLM-GUARD_Large_Language_Model-Based_Detection_and_Repair_of_Bugs_and_Security_Vulnerabilities_in_C%2B%2B_and_Python.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-RoMedQA-The-First-Benchmark-for-Romanian-Medical-Question-Answering"><a href="#56-RoMedQA-The-First-Benchmark-for-Romanian-Medical-Question-Answering" class="headerlink" title="56. RoMedQA: The First Benchmark for Romanian Medical Question Answering"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/RoMedQA__The_First_Benchmark_for_Romanian_Medical_Question_Answering.pdf">RoMedQA: The First Benchmark for Romanian Medical Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Bucharest</span></p><p>该论文提出RoMedQA，首个罗马尼亚医学问答数据集，涵盖1,011名癌症患者的102,646个高质量QA对，并由医学专家耗时2,100小时人工标注。作者系统评估了四种主流大语言模型（包括罗马尼亚语专用、长文本支持和生物医学预训练模型），在零样本推理和LoRA高效微调两种模式下进行对比实验，发现微调后的模型在医学问答任务上显著优于零样本，且最佳模型F1仅达0.667，说明该任务具挑战性。结论表明，领域和语言专门化微调对提升医学问答准确性至关重要，RoMedQA填补了罗马尼亚医学NLP资源空缺。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/RoMedQA_The_First_Benchmark_for_Romanian_Medical_Question_Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-MizanQA-Benchmarking-Large-Language-Models-on-Moroccan-Legal-Question-Answering"><a href="#57-MizanQA-Benchmarking-Large-Language-Models-on-Moroccan-Legal-Question-Answering" class="headerlink" title="57. MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/MizanQA__Benchmarking_Large_Language_Models_on_Moroccan_Legal_Question_Answering.pdf">MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohammed 6 Polytechnic University</span></p><p>本文提出了MizanQA，这是首个针对摩洛哥法律领域多项选择题问答的LLM评测基准，涵盖现代标准阿拉伯语、伊斯兰法、摩洛哥习惯法和法语法律影响。作者手工和自动混合构建了包含1700余题的高质量数据集，并设计了多答案、多类别、置信度校准等新评测指标，系统评估多语种与阿拉伯语LLM的法律推理能力。实验显示主流LLM在文化特定术语、复杂推理、多答案匹配等方面有较大短板，强调了低资源语境下发展本地化、领域特定法律AI评测与模型的必要性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/MizanQA_Benchmarking_Large_Language_Models_on_Moroccan_Legal_Question_Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-Confusion-is-the-Final-Barrier-Rethinking-Jailbreak-Evaluation-and-Investigating-the-Real-Misuse-Threat-of-LLMs"><a href="#58-Confusion-is-the-Final-Barrier-Rethinking-Jailbreak-Evaluation-and-Investigating-the-Real-Misuse-Threat-of-LLMs" class="headerlink" title="58. Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Confusion_is_the_Final_Barrier__Rethinking_Jailbreak_Evaluation_and_Investigating_the_Real_Misuse_Th.pdf">Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Computing Technology, Chinese Academy of Sciences</span></p><p>本文提出VENOM框架，通过构建知识密集型问答和反事实任务，系统评估大模型在犯罪知识、任务规划及有害性判断等方面的真实能力，揭示现有LLM安全评估方法高估了模型对真实有害知识的掌握。结论表明，主流LLM的越狱成功率与实际有害知识掌握并不匹配，且判别模块过度依赖有害语言风格，无法准确识别真实威胁。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Confusion_is_the_Final_Barrier_Rethinking_Jailbreak_Evaluation_and_Investigating_the_Real_Misuse_Threat_of_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-LLMSymGuard-A-Symbolic-Safety-Guardrail-Framework-Leveraging-Interpretable-Jailbreak-Concepts"><a href="#59-LLMSymGuard-A-Symbolic-Safety-Guardrail-Framework-Leveraging-Interpretable-Jailbreak-Concepts" class="headerlink" title="59. LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/LLMSymGuard__A_Symbolic_Safety_Guardrail_Framework_Leveraging_Interpretable_Jailbreak_Concepts.pdf">LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Université Paris-Saclay</span></p><p>LLMSymGuard提出了一种利用稀疏自编码器（SAE）从大型语言模型（LLM）内部提取可解释的jailbreak相关概念的方法，通过对SAE激活特征进行分类和逻辑规则组合，构建具备透明度和鲁棒性的符号安全防护机制。实验证明，该方法可在不损失模型能力、无需额外微调的前提下，显著提升对有害内容的拦截率，并保持较低的误杀率，优于主流安全微调基线。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/LLMSymGuard_A_Symbolic_Safety_Guardrail_Framework_Leveraging_Interpretable_Jailbreak_Concepts.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-OwkinZero-Accelerating-Biological-Discovery-with-AI"><a href="#60-OwkinZero-Accelerating-Biological-Discovery-with-AI" class="headerlink" title="60. OwkinZero: Accelerating Biological Discovery with AI"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/OwkinZero__Accelerating_Biological_Discovery_with_AI.pdf">OwkinZero: Accelerating Biological Discovery with AI</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Owkin</span></p><p>本论文提出了OwkinZero框架，通过构建8个涵盖30万高质量生物学问答对的全新基准数据集，针对药物发现中的关键生物学推理任务，采用可验证奖励强化学习（RLVR）方法对开放大语言模型进行后训练。结果表明，经过RL后训练的8-32B参数量OwkinZero模型在生物学推理基准上的表现显著优于更大规模的商用LLM，并展现出跨任务泛化能力，但多任务训练时推理一致性有所降低，需进一步对齐。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/OwkinZero_Accelerating_Biological_Discovery_with_AI.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-Structuring-GUI-Elements-through-Vision-Language-Models-Towards-Action-Space-Generation"><a href="#61-Structuring-GUI-Elements-through-Vision-Language-Models-Towards-Action-Space-Generation" class="headerlink" title="61. Structuring GUI Elements through Vision Language Models: Towards Action Space Generation"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Structuring_GUI_Elements_through_Vision_Language_Models__Towards_Action_Space_Generation.pdf">Structuring GUI Elements through Vision Language Models: Towards Action Space Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了一种针对多模态大语言模型（MLLMs）用于图形用户界面（GUI）元素结构化的IoU增强最大似然（IAML）训练范式，通过IoU度量的坐标采样实现数据增强，并结合奖励机制缓解传统最大似然估计中的暴露偏差问题。实验结果表明，IAML方法在多元素动作空间生成和单元素定位任务上均超越了传统MLE和随机扰动方法，尤其在低资源场景下表现更优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Structuring_GUI_Elements_through_Vision_Language_Models_Towards_Action_Space_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-On-the-Evolution-of-Federated-Post-Training-Large-Language-Models-A-Model-Accessibility-View"><a href="#62-On-the-Evolution-of-Federated-Post-Training-Large-Language-Models-A-Model-Accessibility-View" class="headerlink" title="62. On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/On_the_Evolution_of_Federated_Post-Training_Large_Language_Models__A_Model_Accessibility_View.pdf">On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen University</span></p><p>本论文系统梳理了联邦学习下大语言模型（LLMs）后训练的最新进展，提出了一个双轴分类法：一是模型可访问性（白盒、灰盒、黑盒），二是参数高效性，并详细分析了每种优化方案，包括LoRA、Prompt Tuning等关键技术。结论指出，随着模型访问受限，黑盒推理API范式和高效参数优化将成为未来主流，强调了推理黑盒场景下的隐私和安全性挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/On_the_Evolution_of_Federated_Post-Training_Large_Language_Models_A_Model_Accessibility_View.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-LLM-Assisted-Semantic-Alignment-and-Integration-in-Collaborative-Model-Based-Systems-Engineering-Using-SysML-v2"><a href="#63-LLM-Assisted-Semantic-Alignment-and-Integration-in-Collaborative-Model-Based-Systems-Engineering-Using-SysML-v2" class="headerlink" title="63. LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/LLM-Assisted_Semantic_Alignment_and_Integration_in_Collaborative_Model-Based_Systems_Engineering_Usi.pdf">LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technische Universität Ilmenau</span></p><p>本文提出了一种基于GPT类大语言模型（LLM）和SysML v2结构的分阶段、结构化、提示驱动方法，用于跨组织MBSE模型的语义对齐与集成。方法包括模型抽取、语义匹配、验证、结构化导出等流程，利用SysML v2的alias、import和元数据扩展实现模型结构独立性和可追溯软对齐，并在测量系统协作场景下展示了可行性。结论表明，该方法提升了对齐效率、语义透明性和用户交互性，但在深层语义一致性和自动化方面仍需进一步研究。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/LLM-Assisted_Semantic_Alignment_and_Integration_in_Collaborative_Model-Based_Systems_Engineering_Using_SysML_v2.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-Graph-RAG-as-Human-Choice-Model-Building-a-Data-Driven-Mobility-Agent-with-Preference-Chain"><a href="#64-Graph-RAG-as-Human-Choice-Model-Building-a-Data-Driven-Mobility-Agent-with-Preference-Chain" class="headerlink" title="64. Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Graph_RAG_as_Human_Choice_Model__Building_a_Data-Driven_Mobility_Agent_with_Preference_Chain.pdf">Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China University of Technology</span></p><p>本论文提出了一种Preference Chain方法，将图检索增强生成（Graph RAG）与大语言模型（LLM）结合，用于在数据稀缺环境下模拟交通系统中的人类行为。该方法通过构建BDI行为图、相似性搜索、概率建模和LLM偏好重塑四步，实现了基于小样本数据的个体行为模拟，并在Replica出行数据集上验证了其在出行方式模拟和动态交通预测方面优于传统LLM和机器学习方法，特别适合新兴城市等数据稀缺场景，结论表明该方法提升了模拟的准确性和泛化能力，但在推理速度和连续性建模方面仍有局限。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Graph_RAG_as_Human_Choice_Model_Building_a_Data-Driven_Mobility_Agent_with_Preference_Chain.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-Towards-Recommending-Usability-Improvements-with-Multimodal-Large-Language-Models"><a href="#65-Towards-Recommending-Usability-Improvements-with-Multimodal-Large-Language-Models" class="headerlink" title="65. Towards Recommending Usability Improvements with Multimodal Large Language Models"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Towards_Recommending_Usability_Improvements_with_Multimodal_Large_Language_Models.pdf">Towards Recommending Usability Improvements with Multimodal Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Graz University of Technology</span></p><p>该论文将可用性评估问题形式化为推荐任务，利用多模态大语言模型（LLM）结合应用描述、用户角色、评估标准及界面截图，自动识别并按严重性排序可用性问题，并生成针对性的改进建议。通过与专业可用性专家的评估对比，证明LLM在结构化、二元评分等场景下能较好辅助发现和优先排序关键可用性问题，但尚不能完全取代专家分析，可为资源有限的团队提供高效辅助。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Towards_Recommending_Usability_Improvements_with_Multimodal_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-Retrieval-Enhanced-Feedback-via-In-context-Neural-Error-book"><a href="#66-Retrieval-Enhanced-Feedback-via-In-context-Neural-Error-book" class="headerlink" title="66. Retrieval Enhanced Feedback via In-context Neural Error-book"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Retrieval_Enhanced_Feedback_via_In-context_Neural_Error-book.pdf">Retrieval Enhanced Feedback via In-context Neural Error-book</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">POSTECH</span></p><p>该论文提出了一种名为REFINE的教师-学生框架，通过构建结构化神经错误本（Error-book），利用三种系统化反馈（Feed-Target, Feed-Check, Feed-Path）对多模态大模型进行有针对性的纠错与推理增强。实验表明，REFINE在多模态推理准确性和推理效率上显著优于现有方法，同时具备良好的泛化能力和可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Retrieval_Enhanced_Feedback_via_In-context_Neural_Error-book.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications"><a href="#67-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications" class="headerlink" title="67. AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/AgentScope_1.0__A_Developer-Centric_Framework_for_Building_Agentic_Applications.pdf">AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Alibaba Group</span></p><p>该论文提出AgentScope 1.0，一个以开发者为中心、基于ReAct范式的LLM智能体开发框架。方法上，AgentScope抽象了消息、模型、内存和工具等基础组件，提供统一接口，支持异步与并行工具调用、多智能体协作、可视化开发与安全沙箱部署，并内建多种典型智能体（如深度研究、浏览器操作、Meta规划）。结论认为，AgentScope能高效、灵活地支撑工具增强型LLM智能体在真实环境下的开发与部署，提升扩展性和开发体验。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/AgentScope_1.0_A_Developer-Centric_Framework_for_Building_Agentic_Applications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-MCPVerse-An-Expansive-Real-World-Benchmark-for-Agentic-Tool-Use"><a href="#68-MCPVerse-An-Expansive-Real-World-Benchmark-for-Agentic-Tool-Use" class="headerlink" title="68. MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/MCPVerse__An_Expansive,_Real-World_Benchmark_for_Agentic_Tool_Use.pdf">MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">SenseTime Research</span></p><p>本文提出了MCPVerse基准，用于大规模、真实场景下评估大语言模型（LLMs）的工具调用与推理能力。MCPVerse整合了550+真实可执行工具，设计了覆盖多复杂度和时效性的250个任务，采用混合结果导向评价体系，通过标准化接口和多种评测模式，全面测试了主流LLM在大规模工具集下的表现和局限。实验发现，拓展的工具集对具备agentic能力的模型（如Claude-4-Sonnet）有益，能提升其复杂问题解决能力。结论指出，MCPVerse弥补了现有基准在真实性和规模上的不足，是推动LLM工具使用能力发展的重要基准。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/MCPVerse_An_Expansive_Real-World_Benchmark_for_Agentic_Tool_Use.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-MedOmni-45°-A-Safety–Performance-Benchmark-for-Reasoning-Oriented-LLMs-in-Medicine"><a href="#69-MedOmni-45°-A-Safety–Performance-Benchmark-for-Reasoning-Oriented-LLMs-in-Medicine" class="headerlink" title="69. MedOmni-45°: A Safety–Performance Benchmark for Reasoning-Oriented LLMs in Medicine"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/MedOmni-45%C2%B0__A_Safety-Performance_Benchmark_for_Reasoning-Oriented_LLMs_in_Medicine.pdf">MedOmni-45°: A Safety–Performance Benchmark for Reasoning-Oriented LLMs in Medicine</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">East China Normal University</span></p><p>该论文提出了MedOmni-45°，一个面向医学领域推理型大语言模型（LLM）的安全-性能权衡评测基准。方法包括构建涵盖六大临床专科和三类推理任务的1804道多选题，并系统性地添加7类操纵性提示，组合出2.7万条输入，基于三重指标（准确率、推理链忠实度、反拍马屁性）对7个主流LLM进行系统性评测，并以45°可视化图揭示安全与性能间的权衡。结论显示所有模型均存在安全与性能之间的普遍性权衡，最优模型QwQ-32B在安全性和准确率上接近理想线，但无模型能完全兼顾两者，说明当前医学推理型LLM在操纵性提示下仍存在关键安全短板，MedOmni-45°为未来模型对齐与医疗AI安全研究提供了有力工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/MedOmni-45%C2%B0_A_Safety%E2%80%93Performance_Benchmark_for_Reasoning-Oriented_LLMs_in_Medicine.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-SPECVLM-Enhancing-Speculative-Decoding-of-Video-LLMs-via-Verifier-Guided-Token-Pruning"><a href="#70-SPECVLM-Enhancing-Speculative-Decoding-of-Video-LLMs-via-Verifier-Guided-Token-Pruning" class="headerlink" title="70. SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/SpecVLM__Enhancing_Speculative_Decoding_of_Video_LLMs_via_Verifier-Guided_Token_Pruning.pdf">SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>SPECVLM提出了一种针对视频大语言模型（Vid-LLMs）的无训练推理加速框架，通过引入基于Verifier（目标模型）注意力引导的两阶段视频Token剪枝，大幅减少冗余视频Token输入，有效提升推理速度。实验结果表明，SPECVLM能够在保持生成质量无损的前提下，实现最高2.68倍的视频LLM解码加速，并具备良好的通用性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/SPECVLM_Enhancing_Speculative_Decoding_of_Video_LLMs_via_Verifier-Guided_Token_Pruning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-Seeing-is-Believing-Emotion-Aware-Audio-Visual-Language-Modeling-for-Expressive-Speech-Generation"><a href="#71-Seeing-is-Believing-Emotion-Aware-Audio-Visual-Language-Modeling-for-Expressive-Speech-Generation" class="headerlink" title="71. Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for Expressive Speech Generation"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Seeing_is_Believing__Emotion-Aware_Audio-Visual_Language_Modeling_for_Expressive_Speech_Generation.pdf">Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for Expressive Speech Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Johns Hopkins University</span></p><p>本文提出了一种音频视觉语言模型（AVLM），通过在预训练的表达式语音模型中集成全脸视觉线索，实现了情感感知的多模态语音生成。方法上，系统性探索了多种视觉编码器与模态融合策略（如Q-Former Prefix），并在情感识别及表达式对话任务上微调，显著优于仅使用语音的基线（如情感识别F1提升5分以上）。结论为：AVLM有效利用视觉信息提升语音生成的情感表达与准确性，为多模态对话系统奠定基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Seeing_is_Believing_Emotion-Aware_Audio-Visual_Language_Modeling_for_Expressive_Speech_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-The-Fools-are-Certain-the-Wise-are-Doubtful-Exploring-LLM-Confidence-in-Code-Completion"><a href="#72-The-Fools-are-Certain-the-Wise-are-Doubtful-Exploring-LLM-Confidence-in-Code-Completion" class="headerlink" title="72. The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/The_Fools_are_Certain;_the_Wise_are_Doubtful__Exploring_LLM_Confidence_in_Code_Completion.pdf">The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Athens University of Economics and Business</span></p><p>本文提出了一种可复现的方法和开源工具包，用于利用多种大型语言模型（LLM）评估代码补全任务中的模型信心，通过计算不同编程语言、模型和数据集的代码困惑度（perplexity）。实验发现，强类型语言困惑度较低，脚本语言较高，且困惑度主要受模型影响而非数据集影响，结论强调困惑度可作为代码生成任务中LLM可信度和功能正确性的便捷度量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/The_Fools_are_Certain_the_Wise_are_Doubtful_Exploring_LLM_Confidence_in_Code_Completion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-AgentFly-Fine-tuning-LLM-Agents-without-Fine-tuning-LLMs"><a href="#73-AgentFly-Fine-tuning-LLM-Agents-without-Fine-tuning-LLMs" class="headerlink" title="73. AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/AgentFly__Fine-tuning_LLM_Agents_without_Fine-tuning_LLMs.pdf">AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AI Centre, UCL</span></p><p>本文提出了一种无需微调底层大模型的LLM Agent自适应学习新范式AgentFly，通过基于记忆的在线强化学习实现持续适应。AgentFly将任务建模为记忆增强型马尔可夫决策过程（M-MDP），利用神经案例选择策略从外部记忆中检索历史经验，结合规划–执行架构实现经验积累与决策改进。实验表明，AgentFly在GAIA、DeepResearcher等多个基准上超越现有方法，具备高效可扩展的持续学习能力，显著提升了泛化与实时适应性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/AgentFly_Fine-tuning_LLM_Agents_without_Fine-tuning_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-CommonKV-Compressing-KV-Cache-with-Cross-layer-Parameter-Sharing"><a href="#74-CommonKV-Compressing-KV-Cache-with-Cross-layer-Parameter-Sharing" class="headerlink" title="74. CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/CommonKV__Compressing_KV_Cache_with_Cross-layer_Parameter_Sharing.pdf">CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology</span></p><p>本文提出了一种针对大型语言模型KV缓存压缩的无训练方法CommonKV，通过对相邻层的参数进行SVD分解实现跨层权重共享，从而获得更一致的潜在KV缓存，并设计了基于余弦相似度的自适应压缩预算分配策略，有效避免性能损失。实验结果表明，CommonKV在多种主流模型和长文本任务上实现最高98%压缩率且性能损失极小，并与其他压缩和淘汰方法高度兼容。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/CommonKV_Compressing_KV_Cache_with_Cross-layer_Parameter_Sharing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-Bridging-the-Gap-in-Ophthalmic-AI-MM-Retinal-Reason-Dataset-and-OphthaReason-Model-toward-Dynamic-Multimodal-Reasoning"><a href="#75-Bridging-the-Gap-in-Ophthalmic-AI-MM-Retinal-Reason-Dataset-and-OphthaReason-Model-toward-Dynamic-Multimodal-Reasoning" class="headerlink" title="75. Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Bridging_the_Gap_in_Ophthalmic_AI__MM-Retinal-Reason_Dataset_and_OphthaReason_Model_toward_Dynamic_M.pdf">Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southeast University</span></p><p>本文提出了MM-Retinal-Reason，这是首个覆盖基础与复杂推理的眼科多模态数据集，整合了多种医学影像及临床信息，并具备全链路推理轨迹。基于该数据集，作者开发了OphthaReason模型，通过三阶段流程（多模态对齐、链式思维监督微调和强化学习）结合创新的不确定性动态思维机制（UADT），实现了对不同推理难度的自适应探索。实验表明，OphthaReason在基础与复杂推理任务上均大幅超越现有通用及医学多模态大模型，提升最高达24.92%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Bridging_the_Gap_in_Ophthalmic_AI_MM-Retinal-Reason_Dataset_and_OphthaReason_Model_toward_Dynamic_Multimodal_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-Extending-FKG-in-Towards-a-Food-Claim-Traceability-Network"><a href="#76-Extending-FKG-in-Towards-a-Food-Claim-Traceability-Network" class="headerlink" title="76. Extending FKG.in: Towards a Food Claim Traceability Network"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Extending_FKG.in__Towards_a_Food_Claim_Traceability_Network.pdf">Extending FKG.in: Towards a Food Claim Traceability Network</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ashoka University</span></p><p>该论文提出了Food Claim-Traceability Network（FCN），作为印度食品知识图谱FKG.in的扩展，建立了用于结构化、可追溯地表示和验证食品相关主张的本体及数据处理流程。方法包括本体设计、LLM辅助信息抽取、验证源关联和人工审核，流程在Reddit数据集上实现了半自动化主张提取与验证。结论显示，FCN可提升食品主张的透明度和可解释性，为食品虚假信息检测和公众健康干预奠定基础，并具备全球推广潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Extending_FKG.in_Towards_a_Food_Claim_Traceability_Network.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-CYCLE-INSTRUCT-Fully-Seed-Free-Instruction-Tuning-via-Dual-Self-Training-and-Cycle-Consistency"><a href="#77-CYCLE-INSTRUCT-Fully-Seed-Free-Instruction-Tuning-via-Dual-Self-Training-and-Cycle-Consistency" class="headerlink" title="77. CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual Self-Training and Cycle Consistency"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/CYCLE-INSTRUCT__Fully_Seed-Free_Instruction_Tuning_via_Dual_Self-Training_and_Cycle_Consistency.pdf">CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual Self-Training and Cycle Consistency</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出CYCLE-INSTRUCT框架，通过双自训练环路和循环一致性机制，实现了无需任何人工标注种子数据或外部教师模型的指令微调。方法仅利用原始未标注文本，自动分割生成问答，双模型互相监督重建原文，生成高质量指令-响应数据。实验结果显示，CYCLE-INSTRUCT在四类数据集上性能超越传统种子驱动的回译方法，并可达到强监督模型的水平，显著提升了LLM自动化对齐能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/CYCLE-INSTRUCT_Fully_Seed-Free_Instruction_Tuning_via_Dual_Self-Training_and_Cycle_Consistency.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-Cooperative-Design-Optimization-through-Natural-Language-Interaction"><a href="#78-Cooperative-Design-Optimization-through-Natural-Language-Interaction" class="headerlink" title="78. Cooperative Design Optimization through Natural Language Interaction"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Cooperative_Design_Optimization_through_Natural_Language_Interaction.pdf">Cooperative Design Optimization through Natural Language Interaction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">OMRON SINIC X Corporation</span></p><p>本文提出了一种结合大语言模型（LLM）与贝叶斯优化（BO）的协同设计优化框架，实现设计师可通过自然语言与优化系统互动，灵活干预设计参数的探索过程。用户实验表明，该方法在保持优化性能的同时，显著提升了用户的主观能动性并降低了认知负担，且与现有协同方法相比具备更高的易用性和灵活性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Cooperative_Design_Optimization_through_Natural_Language_Interaction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-Generative-Foundation-Model-for-Structured-and-Unstructured-Electronic-Health-Records"><a href="#79-Generative-Foundation-Model-for-Structured-and-Unstructured-Electronic-Health-Records" class="headerlink" title="79. Generative Foundation Model for Structured and Unstructured Electronic Health Records"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Generative_Foundation_Model_for_Structured_and_Unstructured_Electronic_Health_Records.pdf">Generative Foundation Model for Structured and Unstructured Electronic Health Records</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Pittsburgh</span></p><p>该论文提出Generative Deep Patient (GDP)，一种融合结构化EHR时间序列（通过CNN-Transformer编码器）与非结构化文本（通过BioClinicalBERT和LLaMA-based解码器，采用跨模态注意力）的多模态基础模型。GDP采用两阶段训练：首先进行生成式预训练（包括生成临床叙述、掩码特征预测和下一个时间步预测），随后多任务微调以完成疾病预测和再入院风险等任务。实验表明，GDP在心衰（AUROC&#x3D;0.923）、2型糖尿病（AUROC&#x3D;0.817）和30天再入院（AUROC&#x3D;0.627）等临床预测显著优于现有方法，在生成高质量临床文本方面也超过主流医疗LLM，提升了预测准确性并减轻医生文档负担。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Generative_Foundation_Model_for_Structured_and_Unstructured_Electronic_Health_Records.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-OpenWHO-A-Document-Level-Parallel-Corpus-for-Health-Translation-in-Low-Resource-Languages"><a href="#80-OpenWHO-A-Document-Level-Parallel-Corpus-for-Health-Translation-in-Low-Resource-Languages" class="headerlink" title="80. OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/OpenWHO__A_Document-Level_Parallel_Corpus_for_Health_Translation_in_Low-Resource_Languages.pdf">OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Melbourne</span></p><p>本论文提出OpenWHO语料库，这是一个基于世界卫生组织e-learning平台、涵盖20余种语言（含9种低资源语言）的健康领域文档级平行语料库。作者利用该数据集系统评估了现代大语言模型（如Gemini 2.5 Flash）与传统机器翻译模型（如NLLB-54B）在低资源健康翻译中的表现，结果显示LLMs在文档级上下文下显著优于传统模型（Gemini 2.5 Flash在低资源测试集上比NLLB-54B提升4.79 ChrF分），并且文档级翻译在健康等专业领域的优势尤为突出。结论是：文档级上下文和最强LLM能够提升低资源专业领域翻译质量，OpenWHO语料库为相关研究提供了新基准。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/OpenWHO_A_Document-Level_Parallel_Corpus_for_Health_Translation_in_Low-Resource_Languages.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-Spacetime-GR-A-Spacetime-Aware-Generative-Model-for-Large-Scale-Online-POI-Recommendation"><a href="#81-Spacetime-GR-A-Spacetime-Aware-Generative-Model-for-Large-Scale-Online-POI-Recommendation" class="headerlink" title="81. Spacetime-GR: A Spacetime-Aware Generative Model for Large Scale Online POI Recommendation"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Spacetime-GR__A_Spacetime-Aware_Generative_Model_for_Large_Scale_Online_POI_Recommendation.pdf">Spacetime-GR: A Spacetime-Aware Generative Model for Large Scale Online POI Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AMAP, Alibaba Group</span></p><p>本文提出Spacetime-GR，一种面向大规模在线兴趣点（POI）推荐的时空感知生成模型。方法结合地理分层POI索引、时空编码模块与多模态POI嵌入，基于Llama 2架构，通过预训练和多种后训练策略（嵌入特征、排序分数与候选POI生成）支持多种下游应用。实验显示其在工业及公开数据集上显著提升推荐准确性和排序质量，已成功部署于覆盖数亿用户和POI的在线系统。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Spacetime-GR_A_Spacetime-Aware_Generative_Model_for_Large_Scale_Online_POI_Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-From-Indirect-Object-Identification-to-Syllogisms-Exploring-Binary-Mechanisms-in-Transformer-Circuits"><a href="#82-From-Indirect-Object-Identification-to-Syllogisms-Exploring-Binary-Mechanisms-in-Transformer-Circuits" class="headerlink" title="82. From Indirect Object Identification to Syllogisms: Exploring Binary Mechanisms in Transformer Circuits"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/From_Indirect_Object_Identification_to_Syllogisms__Exploring_Binary_Mechanisms_in_Transformer_Circui.pdf">From Indirect Object Identification to Syllogisms: Exploring Binary Mechanisms in Transformer Circuits</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Los Angeles</span></p><p>本文采用机械可解释性技术（如Path Patching和Logit Lens）系统分析GPT-2小模型在处理三类二元逻辑推理任务（简单、反向、复杂三段论）时的内部机制。研究发现，模型通过特定的注意力头和MLP模块实现真值的复制和否定，构建了高忠实度的子电路，揭示了逻辑推理和二元机制在语言模型中的具体实现方式。结论表明GPT-2在二元逻辑任务中表现出可解释的电路结构，并且这些机制在多种模型间具有一定可迁移性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/From_Indirect_Object_Identification_to_Syllogisms_Exploring_Binary_Mechanisms_in_Transformer_Circuits.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-CEQuest-Benchmarking-Large-Language-Models-for-Construction-Estimation"><a href="#83-CEQuest-Benchmarking-Large-Language-Models-for-Construction-Estimation" class="headerlink" title="83. CEQuest: Benchmarking Large Language Models for Construction Estimation"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/CEQuest__Benchmarking_Large_Language_Models_for_Construction_Estimation.pdf">CEQuest: Benchmarking Large Language Models for Construction Estimation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Florida International University</span></p><p>本论文提出了CEQuest，一个专为建筑领域设计的基准数据集，用于评估大型语言模型（LLMs）在施工图纸解读和估算任务上的表现。作者系统测试了五种主流LLM（Gemma 3, Phi4, LLaVA, Llama 3.3, GPT-4.1），比较了它们在准确率、执行时间和模型规模上的性能，发现当前LLM在建筑领域任务上仍有很大提升空间。结论指出：需融合领域知识以提升LLM在专业任务的准确性，CEQuest将开源以推动领域专用LLM的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/CEQuest_Benchmarking_Large_Language_Models_for_Construction_Estimation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-Integrating-Time-Series-into-LLMs-via-Multi-layer-Steerable-Embedding-Fusion-for-Enhanced-Forecasting"><a href="#84-Integrating-Time-Series-into-LLMs-via-Multi-layer-Steerable-Embedding-Fusion-for-Enhanced-Forecasting" class="headerlink" title="84. Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Integrating_Time_Series_into_LLMs_via_Multi-layer_Steerable_Embedding_Fusion_for_Enhanced_Forecastin.pdf">Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-Sen University</span></p><p>本文提出了多层可控嵌入融合（MSEF）框架，将时间序列基础模型提取的语义嵌入通过分层可学习的引导向量动态融合到大型语言模型（LLM）各层，实现时序信息的深度整合，并仅训练引导向量与输出层以保证高效。实验在七个公开基准数据集上表明，MSEF在少样本时序预测任务上较主流方法平均MSE提升31.8%，显著提升LLM对时序数据的理解和预测能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Integrating_Time_Series_into_LLMs_via_Multi-layer_Steerable_Embedding_Fusion_for_Enhanced_Forecasting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-MMAPG-A-Training-Free-Framework-for-Multimodal-Multi-hop-Question-Answering-via-Adaptive-Planning-Graphs"><a href="#85-MMAPG-A-Training-Free-Framework-for-Multimodal-Multi-hop-Question-Answering-via-Adaptive-Planning-Graphs" class="headerlink" title="85. MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/MMAPG__A_Training-Free_Framework_for_Multimodal_Multi-hop_Question_Answering_via_Adaptive_Planning_G.pdf">MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of New South Wales</span></p><p>本文提出MMAPG框架，通过自适应规划图整合规划、检索和推理模块，实现了无需训练的多模态多跳问答。该方法结合LLM动态规划推理路径，并采用模块化、模态自适应检索策略，能灵活处理文本和图像等多源信息。实验表明，MMAPG在MultimodalQA和WebQA数据集上，无需额外训练即可达到或优于主流有监督模型，具备较强的灵活性和扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/MMAPG_A_Training-Free_Framework_for_Multimodal_Multi-hop_Question_Answering_via_Adaptive_Planning_Graphs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="86-Enhanced-NIRMAL-Optimizer-With-Damped-Nesterov-Acceleration-A-Comparative-Analysis"><a href="#86-Enhanced-NIRMAL-Optimizer-With-Damped-Nesterov-Acceleration-A-Comparative-Analysis" class="headerlink" title="86. Enhanced NIRMAL Optimizer With Damped Nesterov Acceleration: A Comparative Analysis"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Enhanced_NIRMAL_Optimizer_With_Damped_Nesterov_Acceleration__A_Comparative_Analysis.pdf">Enhanced NIRMAL Optimizer With Damped Nesterov Acceleration: A Comparative Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ThinkAI - A Machine Learning Community</span></p><p>该论文提出了Enhanced NIRMAL优化器，通过集成(α, r)-damped Nesterov加速机制，提升了原NIRMAL优化器的收敛稳定性，并保留了其基于棋局的多策略融合（梯度下降、动量、随机扰动、自适应学习率和非线性变换）。实验在MNIST、FashionMNIST、CIFAR-10和CIFAR-100数据集上，使用定制CNN结构，与Adam、SGD with Momentum、Nesterov及原NIRMAL进行对比。结果显示Enhanced NIRMAL在复杂数据集（如CIFAR-100）上表现出更优的稳定性和泛化能力，特别是在测试损失和准确率方面优于原NIRMAL，并接近SGD with Momentum。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Enhanced_NIRMAL_Optimizer_With_Damped_Nesterov_Acceleration_A_Comparative_Analysis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-NeuroKoop-Neural-Koopman-Fusion-of-Structural–Functional-Connectomes-for-Identifying-Prenatal-Drug-Exposure-in-Adolescents"><a href="#87-NeuroKoop-Neural-Koopman-Fusion-of-Structural–Functional-Connectomes-for-Identifying-Prenatal-Drug-Exposure-in-Adolescents" class="headerlink" title="87. NeuroKoop: Neural Koopman Fusion of Structural–Functional Connectomes for Identifying Prenatal Drug Exposure in Adolescents"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/NeuroKoop__Neural_Koopman_Fusion_of_Structural-Functional_Connectomes_for_Identifying_Prenatal_Drug_.pdf">NeuroKoop: Neural Koopman Fusion of Structural–Functional Connectomes for Identifying Prenatal Drug Exposure in Adolescents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Georgia State University</span></p><p>本论文提出了NeuroKoop框架，将结构（SBM）和功能（FNC）脑网络通过神经Koopman算子和图神经网络（GNN）融合，结合工作记忆分数实现动态潜在空间的个性化表征，提升了产前药物暴露（PDE）识别的准确性。在ABCD青少年大规模数据集上，NeuroKoop显著优于现有多模态GNN基线方法，能够揭示PDE对大脑结构-功能耦合的影响，为神经发育机制研究和个体化风险评估提供了新工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/NeuroKoop_Neural_Koopman_Fusion_of_Structural%E2%80%93Functional_Connectomes_for_Identifying_Prenatal_Drug_Exposure_in_Adolescents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-Domain-aligned-generative-downscaling-enhances-projections-of-extreme-climate-events"><a href="#88-Domain-aligned-generative-downscaling-enhances-projections-of-extreme-climate-events" class="headerlink" title="88. Domain-aligned generative downscaling enhances projections of extreme climate events"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Domain-aligned_generative_downscaling_enhances_projections_of_extreme_climate_events.pdf">Domain-aligned generative downscaling enhances projections of extreme climate events</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出了基于生成式机器学习的空间-时间下采样模型DACD，通过域适应和Flow Matching训练框架，将全球低分辨率气候数据转换为高分辨率本地气候信息，实现多变量和多时间尺度的极端气候事件精准模拟。实验表明该方法在历史和未来情景下均显著提升了极端高温、强降水、强风和热带气旋等事件的模拟精度，能够更好地捕捉极端事件的空间分布与动态变化，为应对气候变化和制定适应策略提供了科学支撑。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Domain-aligned_generative_downscaling_enhances_projections_of_extreme_climate_events.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-MuST2-Learn-Multi-view-Spatial-Temporal-Type-Learning-for-Heterogeneous-Municipal-Service-Time-Estimation"><a href="#89-MuST2-Learn-Multi-view-Spatial-Temporal-Type-Learning-for-Heterogeneous-Municipal-Service-Time-Estimation" class="headerlink" title="89. MuST2-Learn: Multi-view Spatial-Temporal-Type Learning for Heterogeneous Municipal Service Time Estimation"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/MuST2-Learn__Multi-view_Spatial-Temporal-Type_Learning_for_Heterogeneous_Municipal_Service_Time_Esti.pdf">MuST2-Learn: Multi-view Spatial-Temporal-Type Learning for Heterogeneous Municipal Service Time Estimation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Tennessee at Chattanooga</span></p><p>本文提出MuST2-Learn框架，通过联合空间、时间以及服务类型三维度的信息，采用Transformer、CNN、注意力机制和高斯过程回归，结合LLM自动分配请求工作量，实现对市政多类型服务请求处理时间的高精度预测。实验表明，该方法在两个真实数据集上平均绝对误差降低至少32.5%，显著优于现有方法，具备良好泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/MuST2-Learn_Multi-view_Spatial-Temporal-Type_Learning_for_Heterogeneous_Municipal_Service_Time_Estimation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-Post-Hoc-Regression-Refinement-via-Pairwise-Rankings"><a href="#90-Post-Hoc-Regression-Refinement-via-Pairwise-Rankings" class="headerlink" title="90. Post Hoc Regression Refinement via Pairwise Rankings"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Post_Hoc_Regression_Refinement_via_Pairwise_Rankings.pdf">Post Hoc Regression Refinement via Pairwise Rankings</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MPI-INF</span></p><p>本文提出了一种名为RankRefine的模型无关后处理方法，通过引入专家或大语言模型（LLM）提供的成对排序信息，结合逆方差加权策略，对已有回归模型的输出进行校正，无需重新训练。在多个合成和真实数据集（尤其是分子属性预测）上的实验表明，RankRefine即使仅有少量成对比较和中等准确度的排序器，也能带来可观的均方绝对误差（MAE）降低，且优于已有的同类后处理方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Post_Hoc_Regression_Refinement_via_Pairwise_Rankings.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-Probabilistic-Pretraining-for-Neural-Regression"><a href="#91-Probabilistic-Pretraining-for-Neural-Regression" class="headerlink" title="91. Probabilistic Pretraining for Neural Regression"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Probabilistic_Pretraining_for_Neural_Regression.pdf">Probabilistic Pretraining for Neural Regression</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amazon</span></p><p>本文提出了NIAQUE（Neural Interpretable Any-Quantile Estimation），一种通过排列不变架构实现的深度概率回归模型，专为概率回归的迁移学习设计。该方法通过在多样化的下游回归数据集上预训练，并在目标数据集上微调实现知识迁移，有效提升了单个回归任务的性能。实验表明，NIAQUE在101个公开回归数据集和Kaggle竞赛中均优于强基线（如XGBoost、CatBoost、TabDPT和TabPFN），同时支持不确定性量化和特征重要性解释。结论显示，NIAQUE具备强大的可扩展性和泛化能力，是概率回归任务中迁移学习的有力框架。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Probabilistic_Pretraining_for_Neural_Regression.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-Vevo2-Bridging-Controllable-Speech-and-Singing-Voice-Generation-via-Unified-Prosody-Learning"><a href="#92-Vevo2-Bridging-Controllable-Speech-and-Singing-Voice-Generation-via-Unified-Prosody-Learning" class="headerlink" title="92. Vevo2: Bridging Controllable Speech and Singing Voice Generation via Unified Prosody Learning"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Vevo2__Bridging_Controllable_Speech_and_Singing_Voice_Generation_via_Unified_Prosody_Learning.pdf">Vevo2: Bridging Controllable Speech and Singing Voice Generation via Unified Prosody Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong, Shenzhen</span></p><p>Vevo2提出了一种统一的可控语音与歌声生成框架，通过两个核心音频分词器——无音乐记谱要求的韵律分词器和低帧率内容-风格分词器，实现文本、韵律（旋律）、风格和音色的灵活解耦建模。其方法包含自回归内容-风格建模和流匹配声学建模，并通过联合表达性和韵律相似性目标的多目标后训练显著提升了模型的可控性和泛化能力。实验结果表明，Vevo2在语音和歌声合成、转换与编辑等多种任务上表现优异，同时验证了语音与歌声统一建模的互补优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Vevo2_Bridging_Controllable_Speech_and_Singing_Voice_Generation_via_Unified_Prosody_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-Modeling-User-Preferences-as-Distributions-for-Optimal-Transport-based-Cross-domain-Recommendation-under-Non-overlapping-Settings"><a href="#93-Modeling-User-Preferences-as-Distributions-for-Optimal-Transport-based-Cross-domain-Recommendation-under-Non-overlapping-Settings" class="headerlink" title="93. Modeling User Preferences as Distributions for Optimal Transport-based Cross-domain Recommendation under Non-overlapping Settings"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Modeling_User_Preferences_as_Distributions_for_Optimal_Transport-based_Cross-domain_Recommendation_u.pdf">Modeling User Preferences as Distributions for Optimal Transport-based Cross-domain Recommendation under Non-overlapping Settings</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Tokyo</span></p><p>该论文提出了DUP-OT框架，利用高斯混合模型（GMM）对用户偏好进行分布建模，并通过最优传输（Optimal Transport）在无重叠用户或物品的跨域推荐场景下实现知识迁移。方法包括三阶段：评论嵌入与降维，用户GMM权重学习，以及通过最优传输进行跨域偏好迁移。实验表明，DUP-OT能有效缓解域间分布差异，并在Amazon数据集上优于现有无重叠跨域推荐方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Modeling_User_Preferences_as_Distributions_for_Optimal_Transport-based_Cross-domain_Recommendation_under_Non-overlapping_Settings.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-Set-Transformer-Architectures-and-Synthetic-Data-Generation-for-Flow-Guided-Nanoscale-Localization"><a href="#94-Set-Transformer-Architectures-and-Synthetic-Data-Generation-for-Flow-Guided-Nanoscale-Localization" class="headerlink" title="94. Set Transformer Architectures and Synthetic Data Generation for Flow-Guided Nanoscale Localization"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Set_Transformer_Architectures_and_Synthetic_Data_Generation_for_Flow-Guided_Nanoscale_Localization.pdf">Set Transformer Architectures and Synthetic Data Generation for Flow-Guided Nanoscale Localization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Polytechnic University of Catalonia</span></p><p>本论文提出将Set Transformer架构应用于血流引导的纳米级定位（FGL），以提升模型对解剖结构变异的泛化能力，方法上采用自注意力机制处理无序、变长的纳米设备循环时间集合，摒弃手工特征和静态图结构。此外，引入CGAN、WGAN、WGAN-GP和CVAE等深度生成模型进行合成数据增强以缓解数据稀缺与类别不均衡。实验结果表明，Set Transformer在区域分类准确率上能媲美或优于GNN基线，且更具灵活性和泛化性，但仍存在区分对称区域的困难和稀疏采样下的过拟合风险。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Set_Transformer_Architectures_and_Synthetic_Data_Generation_for_Flow-Guided_Nanoscale_Localization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-Motor-Imagery-EEG-Signal-Classification-Using-Minimally-Random-Convolutional-Kernel-Transform-and-Hybrid-Deep-Learning"><a href="#95-Motor-Imagery-EEG-Signal-Classification-Using-Minimally-Random-Convolutional-Kernel-Transform-and-Hybrid-Deep-Learning" class="headerlink" title="95. Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Motor_Imagery_EEG_Signal_Classification_Using_Minimally_Random_Convolutional_Kernel_Transform_and_Hy.pdf">Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">City University of London</span></p><p>本文提出了一种结合MiniRocket特征提取与线性分类器，以及CNN-LSTM混合神经网络的新方法，用于高效分类脑电（EEG）运动想象信号。通过在PhysioNet公开数据集上的实验，MiniRocket方法以更低计算成本取得了98.63%的平均准确率，优于98.06%的CNN-LSTM混合模型，显著提升了EEG运动想象分类的效率和准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Motor_Imagery_EEG_Signal_Classification_Using_Minimally_Random_Convolutional_Kernel_Transform_and_Hybrid_Deep_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-EGRA-Toward-Enhanced-Behavior-Graphs-and-Representation-Alignment-for-Multimodal-Recommendation"><a href="#96-EGRA-Toward-Enhanced-Behavior-Graphs-and-Representation-Alignment-for-Multimodal-Recommendation" class="headerlink" title="96. EGRA: Toward Enhanced Behavior Graphs and Representation Alignment for Multimodal Recommendation"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/EGRA_Toward_Enhanced_Behavior_Graphs_and_Representation_Alignment_for_Multimodal_Recommendation.pdf">EGRA: Toward Enhanced Behavior Graphs and Representation Alignment for Multimodal Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>该论文提出了一种新型多模态推荐框架EGRA，通过引入基于预训练多模态推荐模型表示构建的item-item图增强行为图，并提出了双层动态对齐权重机制，在实体和训练周期上自适应地调整模态与行为表示的对齐强度。实验结果表明，EGRA在五个公开数据集上均显著优于最新方法，在提升长尾项目推荐和整体性能方面展现出卓越效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/EGRA_Toward_Enhanced_Behavior_Graphs_and_Representation_Alignment_for_Multimodal_Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-On-the-Collapse-Errors-Induced-by-the-Deterministic-Sampler-for-Diffusion-Models"><a href="#97-On-the-Collapse-Errors-Induced-by-the-Deterministic-Sampler-for-Diffusion-Models" class="headerlink" title="97. On the Collapse Errors Induced by the Deterministic Sampler for Diffusion Models"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/On_the_Collapse_Errors_Induced_by_the_Deterministic_Sampler_for_Diffusion_Models.pdf">On the Collapse Errors Induced by the Deterministic Sampler for Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Data Science, The University of Hong Kong</span></p><p>本文揭示了扩散模型中使用确定性采样器（如ODE-based采样）时出现的“坍塌错误”现象，即生成样本在数据空间中过度集中。作者提出了新的度量指标TID，系统分析了坍塌错误的普遍性及其根本原因——低噪声与高噪声区间的得分学习互相影响（see-saw效应），导致高噪声区间拟合失误并在采样过程中累积，从而产生坍塌。作者通过采样、训练和模型结构等多维度实验，验证了引入随机性、分区训练和跳连结构等方法能有效缓解坍塌错误，强调需关注得分学习与采样器动态的耦合作用。结论为：确定性采样器下扩散模型存在普遍的坍塌错误，其本质为模型泛化与采样器机制的协同失效。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/On_the_Collapse_Errors_Induced_by_the_Deterministic_Sampler_for_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-Training-a-Foundation-Model-for-Materials-on-a-Budget"><a href="#98-Training-a-Foundation-Model-for-Materials-on-a-Budget" class="headerlink" title="98. Training a Foundation Model for Materials on a Budget"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Training_a_Foundation_Model_for_Materials_on_a_Budget.pdf">Training a Foundation Model for Materials on a Budget</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Massachusetts Institute of Technology</span></p><p>本文提出Nequix模型，一种基于简化NequIP架构的E(3)-等变图神经网络，通过采用现代训练技术（如等变RMS层归一化、动态批处理和Muon优化器）实现了高效的材料基础模型训练。结果显示，Nequix在Matbench-Discovery和MDR Phonon基准上以远低于主流方法的训练成本获得了前三的准确率，并大幅提升了推断速度，降低了资源门槛。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Training_a_Foundation_Model_for_Materials_on_a_Budget.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-Time-Series-Based-Network-Intrusion-Detection-using-MTF-Aided-Transformer"><a href="#99-Time-Series-Based-Network-Intrusion-Detection-using-MTF-Aided-Transformer" class="headerlink" title="99. Time Series Based Network Intrusion Detection using MTF-Aided Transformer"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Time_Series_Based_Network_Intrusion_Detection_using_MTF-Aided_Transformer.pdf">Time Series Based Network Intrusion Detection using MTF-Aided Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>本文提出了一种结合马尔可夫转移场（MTF）和Transformer的时间序列分类方法，针对软件定义网络（SDN）中的入侵检测问题。该方法通过MTF模块对网络流量时序数据进行结构化编码，再与Transformer自注意力机制结合，捕捉时空依赖关系，并融合空间结构信息实现精细化多类异常检测。实验结果表明，MTF-aided Transformer在数据稀疏环境下，精准率和F1分数较传统方法提升最高可达26.5%，并能在高达40%数据缺失时保持优异表现，同时推理速度提升50%以上，适合实际SDN部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Time_Series_Based_Network_Intrusion_Detection_using_MTF-Aided_Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-HePGA-A-Heterogeneous-Processing-in-Memory-based-GNN-Training-Accelerator"><a href="#100-HePGA-A-Heterogeneous-Processing-in-Memory-based-GNN-Training-Accelerator" class="headerlink" title="100. HePGA: A Heterogeneous Processing-in-Memory based GNN Training Accelerator"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/HePGA__A_Heterogeneous_Processing-in-Memory_based_GNN_Training_Accelerator.pdf">HePGA: A Heterogeneous Processing-in-Memory based GNN Training Accelerator</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Washington State University</span></p><p>本文提出了一种三维异构PIM架构HePGA，结合SRAM、ReRAM和FeFET等多种存储设备，通过多目标优化方法实现GNN层与处理单元和层级的高效映射，提升能效和计算效率。实验结果表明，HePGA在能效和面积效率上较同类架构最高提升3.8倍和6.8倍，并保持模型准确率，同时适用于Transformer等新兴模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/HePGA_A_Heterogeneous_Processing-in-Memory_based_GNN_Training_Accelerator.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025/08/25报纸</title>
    <link href="/2025/08/2025-08-25_article/"/>
    <url>/2025/08/2025-08-25_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2025/08/25报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-25<br>📄 <strong>发现论文数量</strong>：107  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Distributed-Detection-of-Adversarial-Attacks-in-Multi-Agent-Reinforcement-Learning-with-Continuous-Action-Space"><a href="#1-Distributed-Detection-of-Adversarial-Attacks-in-Multi-Agent-Reinforcement-Learning-with-Continuous-Action-Space" class="headerlink" title="1. Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Distributed_Detection_of_Adversarial_Attacks_in_Multi-Agent_Reinforcement_Learning_with_Continuous_A.pdf">Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KTH Royal Institute of Technology</span></p><p>本文提出了一种用于协作型多智能体强化学习（c-MARL）连续动作空间系统中检测对抗性攻击的分布式方法。该方法通过神经网络将他智能体的行为建模为参数化高斯分布，计算归一化对数似然得分，并利用CUSUM检测均值偏移，实现实时异常检测。实验结果表明，该方法在多种基准环境和多种攻击下均能高效检测受害智能体，且优于离散动作空间下的对比方法。结论是，该方法在检测高影响攻击时表现优越，仅极低影响攻击可能逃逸，并且计算复杂度低，适用于多受害智能体场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Distributed_Detection_of_Adversarial_Attacks_in_Multi-Agent_Reinforcement_Learning_with_Continuous_Action_Space.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Understanding-Action-Effects-through-Instrumental-Empowerment-in-Multi-Agent-Reinforcement-Learning"><a href="#2-Understanding-Action-Effects-through-Instrumental-Empowerment-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="2. Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Understanding_Action_Effects_through_Instrumental_Empowerment_in_Multi-Agent_Reinforcement_Learning.pdf">Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Istituto Dalle Molle di Studi sull’Intelligenza Artificiale (IDSIA)</span></p><p>本文提出了一种新的因果动作归因方法Intended Cooperation Values (ICVs)，基于信息论Shapley值，仅利用策略分布分析，量化个体动作对多智能体系统中他人决策确定性和偏好一致性的影响。实验表明，ICV无需奖励信号即可有效解释多智能体合作与影响机制，提升了MARL系统的可解释性和团队行为分析能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Understanding_Action_Effects_through_Instrumental_Empowerment_in_Multi-Agent_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-NiceWebRL-a-Python-library-for-human-subject-experiments-with-reinforcement-learning-environments"><a href="#3-NiceWebRL-a-Python-library-for-human-subject-experiments-with-reinforcement-learning-environments" class="headerlink" title="3. NiceWebRL: a Python library for human subject experiments with reinforcement learning environments"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/NiceWebRL__a_Python_library_for_human_subject_experiments_with_reinforcement_learning_environments.pdf">NiceWebRL: a Python library for human subject experiments with reinforcement learning environments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University</span></p><p>NiceWebRL 是一个 Python 库，利用 Jax 和 NiceGUI，让研究者能够将任意 Jax 环境转化为支持单&#x2F;多智能体的在线人类实验接口，极大简化了人类与 AI 算法的对比和协作实验流程。通过三个案例研究，论文展示了该工具在开发类人AI、人类兼容AI和人类辅助AI中的实际应用，验证了其在认知建模、多智能体协作、LLM 辅助决策等方向的实用性和通用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/NiceWebRL_a_Python_library_for_human_subject_experiments_with_reinforcement_learning_environments.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-A-Dynamical-Systems-Framework-for-Reinforcement-Learning-Safety-and-Robustness-Verification"><a href="#4-A-Dynamical-Systems-Framework-for-Reinforcement-Learning-Safety-and-Robustness-Verification" class="headerlink" title="4. A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/A_Dynamical_Systems_Framework_for_Reinforcement_Learning_Safety_and_Robustness_Verification.pdf">A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">City University of London</span></p><p>本文提出了一种基于动力系统理论的新框架，将强化学习（RL）智能体及其环境视为离散时间自治动力系统，利用有限时间Lyapunov指数（FTLE）识别和可视化Lagrangian Coherent Structures（LCS），从而洞察策略的安全性和鲁棒性。通过引入平均边界排斥（MBR）、聚合伪吸引子强度（ASAS）和时序感知伪吸引子强度（TASAS）等量化指标，该方法能对策略安全裕度和鲁棒性进行定量评估，并在多种离散和连续控制环境中验证了其实用性，能有效发现基于奖励难以暴露的策略缺陷。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/A_Dynamical_Systems_Framework_for_Reinforcement_Learning_Safety_and_Robustness_Verification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-Adversarial-Agent-Behavior-Learning-in-Autonomous-Driving-Using-Deep-Reinforcement-Learning"><a href="#5-Adversarial-Agent-Behavior-Learning-in-Autonomous-Driving-Using-Deep-Reinforcement-Learning" class="headerlink" title="5. Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Adversarial_Agent_Behavior_Learning_in_Autonomous_Driving_Using_Deep_Reinforcement_Learning.pdf">Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Maryland, College Park</span></p><p>该论文提出了一种基于深度强化学习的对抗性智能体行为建模方法，采用PPO算法训练主车（ego-agent），并通过TD3算法结合对抗性奖励机制训练周围车辆为对抗性智能体，生成可导致主车失败的场景。实验表明，在对抗性环境下训练的主车更为鲁棒，获得更高的奖励和更优的生存表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Adversarial_Agent_Behavior_Learning_in_Autonomous_Driving_Using_Deep_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="6-Mag-Match-Magnetic-Vector-Field-Features-for-Map-Matching-and-Registration"><a href="#6-Mag-Match-Magnetic-Vector-Field-Features-for-Map-Matching-and-Registration" class="headerlink" title="6. Mag-Match: Magnetic Vector Field Features for Map Matching and Registration"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Mag-Match__Magnetic_Vector_Field_Features_for_Map_Matching_and_Registration.pdf">Mag-Match: Magnetic Vector Field Features for Map Matching and Registration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Technology Sydney</span></p><p>本文提出了Mag-Match，一种用于3D磁场地图匹配和注册的关键点检测与描述方法。该方法基于递归无散度高斯过程推断磁场及其高阶导数，在磁场曲率高且噪声低区域提取旋转不变的关键点描述符（HOV），通过最近邻与改进MSAC实现地图间的SE(3)配准。实验表明Mag-Match在无重力对齐情况下优于传统SIFT，能稳健实现多机器人&#x2F;多会话场景下的地图匹配与注册。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Mag-Match_Magnetic_Vector_Field_Features_for_Map_Matching_and_Registration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Neural-Robot-Dynamics"><a href="#7-Neural-Robot-Dynamics" class="headerlink" title="7. Neural Robot Dynamics"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Neural_Robot_Dynamics.pdf">Neural Robot Dynamics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">NVIDIA</span></p><p>本文提出了NeRD（Neural Robot Dynamics），一种面向机器人多关节刚体的通用可泛化神经动力学模拟器。NeRD采用混合预测框架，将传统模拟器中的动力学与接触解算器替换为基于Transformer的神经网络，并提出机器人中心的空间不变状态表征，实现对不同任务、环境及控制器的泛化。结论表明，NeRD在多种机器人和任务中能够进行稳定准确的长时序预测，支持策略学习，并可通过少量真实数据高效微调实现仿真到现实的迁移。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Neural_Robot_Dynamics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-LLM-Driven-Self-Refinement-for-Embodied-Drone-Task-Planning"><a href="#8-LLM-Driven-Self-Refinement-for-Embodied-Drone-Task-Planning" class="headerlink" title="8. LLM-Driven Self-Refinement for Embodied Drone Task Planning"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/LLM-Driven_Self-Refinement_for_Embodied_Drone_Task_Planning.pdf">LLM-Driven Self-Refinement for Embodied Drone Task Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Central South University</span></p><p>该论文提出SRDrone系统，实现工业级无人机任务规划的自我优化。方法包括连续状态评估（CMSR算法）用于自主任务结果判断和故障解释，以及分层行为树（BT）修改模型，将LLM反思反馈转化为结构化规划优化。实验证明SRDrone在仿真和真实无人机任务中成功率显著高于现有方法，并具备强适应性和可扩展性，能在无需人工干预下自主适应复杂环境。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/LLM-Driven_Self-Refinement_for_Embodied_Drone_Task_Planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-Lang2Lift-A-Framework-for-Language-Guided-Pallet-Detection-and-Pose-Estimation-Integrated-in-Autonomous-Outdoor-Forklift-Operation"><a href="#9-Lang2Lift-A-Framework-for-Language-Guided-Pallet-Detection-and-Pose-Estimation-Integrated-in-Autonomous-Outdoor-Forklift-Operation" class="headerlink" title="9. Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Lang2Lift__A_Framework_for_Language-Guided_Pallet_Detection_and_Pose_Estimation_Integrated_in_Autono.pdf">Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AIT Austrian Institute of Technology GmbH</span></p><p>本论文提出Lang2Lift框架，通过集成Florence-2和SAM2等视觉语言基础模型实现自然语言引导的托盘检测与6D位姿估计，随后结合FoundationPose进行几何精化和时序跟踪，并将结果输入到运动规划模块，实现全流程户外自动驾驶叉车操作。实验表明，该系统在多种恶劣环境下实现0.76 mIoU分割精度和稳定的实时性能，显著提升了无人叉车的实用性与部署灵活性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Lang2Lift_A_Framework_for_Language-Guided_Pallet_Detection_and_Pose_Estimation_Integrated_in_Autonomous_Outdoor_Forklift_Operation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-JEDI-linear-Fast-and-Efficient-Graph-Neural-Networks-for-Jet-Tagging-on-FPGAs"><a href="#10-JEDI-linear-Fast-and-Efficient-Graph-Neural-Networks-for-Jet-Tagging-on-FPGAs" class="headerlink" title="10. JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/JEDI-linear__Fast_and_Efficient_Graph_Neural_Networks_for_Jet_Tagging_on_FPGAs.pdf">JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>本文提出了JEDI-linear，一种线性复杂度的GNN架构，通过移除显式的两两粒子交互，采用全局特征聚合及细粒度混合量化和分布式算法，实现了高效、低延迟且无DSP资源消耗的FPGA实时推理，适用于高能物理实验中的喷注识别。实验表明，JEDI-linear在准确率、资源利用和推理延迟上均优于现有方法，首次满足了CERN HL-LHC CMS一级触发系统的资源和时延约束。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/JEDI-linear_Fast_and_Efficient_Graph_Neural_Networks_for_Jet_Tagging_on_FPGAs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-RCDINO-Enhancing-Radar-Camera-3D-Object-Detection-with-DINOv2-Semantic-Features"><a href="#11-RCDINO-Enhancing-Radar-Camera-3D-Object-Detection-with-DINOv2-Semantic-Features" class="headerlink" title="11. RCDINO: Enhancing Radar-Camera 3D Object Detection with DINOv2 Semantic Features"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/RCDINO__Enhancing_Radar-Camera_3D_Object_Detection_with_DINOv2_Semantic_Features.pdf">RCDINO: Enhancing Radar-Camera 3D Object Detection with DINOv2 Semantic Features</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Moscow Institute of Physics and Technology</span></p><p>该论文提出了RCDINO，一种基于多模态Transformer的雷达-摄像头3D目标检测模型，通过轻量级适配器将预训练的DINOv2语义特征与视觉主干特征融合，增强视觉表示并提升检测性能。实验证明该方法在nuScenes数据集上对比主流模型获得了SOTA表现，NDS提升0.4%，mAP提升0.7%，并展现出较强的泛化能力和实际适用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/RCDINO_Enhancing_Radar-Camera_3D_Object_Detection_with_DINOv2_Semantic_Features.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-Search-Based-Credit-Assignment-for-Offline-Preference-Based-Reinforcement-Learning"><a href="#12-Search-Based-Credit-Assignment-for-Offline-Preference-Based-Reinforcement-Learning" class="headerlink" title="12. Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Search-Based_Credit_Assignment_for_Offline_Preference-Based_Reinforcement_Learning.pdf">Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本论文提出了一种搜索式偏好加权（SPW）方法，通过将偏好轨迹中的每一步与专家演示中的最相似状态-动作对进行匹配，计算重要性权重，并将其用于偏好学习，实现了细粒度的奖励归因。实验结果表明，SPW在机器人操作任务中，在仅有少量专家演示和偏好反馈条件下，显著优于现有纯偏好、模仿及混合基线方法，提升了任务成功率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Search-Based_Credit_Assignment_for_Offline_Preference-Based_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-AeroDuo-Aerial-Duo-for-UAV-based-Vision-and-Language-Navigation"><a href="#13-AeroDuo-Aerial-Duo-for-UAV-based-Vision-and-Language-Navigation" class="headerlink" title="13. AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/AeroDuo__Aerial_Duo_for_UAV-based_Vision_and_Language_Navigation.pdf">AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>该论文提出了DuAl-VLN任务和AeroDuo框架，实现了双高度无人机协同执行视觉-语言导航。高空无人机采用多模态大语言模型（Pilot-LLM）进行目标区域推理，低空无人机采用轻量级多阶段策略进行导航与目标定位。实验表明，AeroDuo协同方式在HaL-13k数据集上导航成功率较单无人机方法提升9.71%，有效提升了复杂环境下的导航效率和自主管理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/AeroDuo_Aerial_Duo_for_UAV-based_Vision_and_Language_Navigation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-面向具身操作的视觉-语言-动作模型综述"><a href="#14-面向具身操作的视觉-语言-动作模型综述" class="headerlink" title="14. 面向具身操作的视觉-语言-动作模型综述"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Survey_of_Vision-Language-Action_Models_for_Embodied_Manipulation.pdf">面向具身操作的视觉-语言-动作模型综述</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">中国科学院自动化研究所</span></p><p>本文系统梳理了视觉-语言-动作（VLA）模型在具身操作领域的发展脉络，详述了VLA模型架构、训练数据、预训练方法、后训练方法和模型评估等五大核心模块。作者提出VLA模型通过融合大模型技术实现视觉感知、语义推理与动作生成一体化，极大提升了机器人在开放环境下的泛化性和任务执行能力，并指出泛化能力、精细操作与推理实时性是未来VLA发展的关键挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/%E9%9D%A2%E5%90%91%E5%85%B7%E8%BA%AB%E6%93%8D%E4%BD%9C%E7%9A%84%E8%A7%86%E8%A7%89-%E8%AF%AD%E8%A8%80-%E5%8A%A8%E4%BD%9C%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="15-Visual-Autoregressive-Modeling-for-Instruction-Guided-Image-Editing"><a href="#15-Visual-Autoregressive-Modeling-for-Instruction-Guided-Image-Editing" class="headerlink" title="15. Visual Autoregressive Modeling for Instruction-Guided Image Editing"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Visual_Autoregressive_Modeling_for_Instruction-Guided_Image_Editing.pdf">Visual Autoregressive Modeling for Instruction-Guided Image Editing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本论文提出了VAREdit，一种基于视觉自回归（VAR）建模的指令引导图像编辑框架，将图像编辑重构为多尺度条件生成任务，并创新性地引入了Scale-Aligned Reference (SAR)模块，在自注意力第一层注入尺度匹配的源图信息以解决尺度不匹配问题。实验结果表明，VAREdit在编辑精准度和生成效率上均显著优于主流扩散模型，编辑保持性提升30%以上，推理速度提升2倍以上。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Visual_Autoregressive_Modeling_for_Instruction-Guided_Image_Editing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-Fine-grained-Multi-class-Nuclei-Segmentation-with-Molecular-empowered-All-in-SAM-Model"><a href="#16-Fine-grained-Multi-class-Nuclei-Segmentation-with-Molecular-empowered-All-in-SAM-Model" class="headerlink" title="16. Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Fine-grained_Multi-class_Nuclei_Segmentation_with_Molecular-empowered_All-in-SAM_Model.pdf">Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Vanderbilt University</span></p><p>本文提出了分子赋能的All-in-SAM模型，通过结合分子影像辅助弱标注、SAM模型自适应微调及MOCL分子导向纠错学习，实现多类细胞核的高精度分割。实验结果表明，该方法在不同数据量和标注质量下均优于现有主流方法，有效降低标注成本并提升病理图像自动分析的准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Fine-grained_Multi-class_Nuclei_Segmentation_with_Molecular-empowered_All-in-SAM_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-Probability-Density-from-Latent-Diffusion-Models-for-Out-of-Distribution-Detection"><a href="#17-Probability-Density-from-Latent-Diffusion-Models-for-Out-of-Distribution-Detection" class="headerlink" title="17. Probability Density from Latent Diffusion Models for Out-of-Distribution Detection"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Probability_Density_from_Latent_Diffusion_Models_for_Out-of-Distribution_Detection.pdf">Probability Density from Latent Diffusion Models for Out-of-Distribution Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Tartu</span></p><p>本文提出在预训练ResNet-18的表示空间上训练变分扩散模型（VDM），并结合概率流ODE计算精确对数似然，实现密度驱动的OOD检测。实验表明，利用表示空间的密度估计方法在OpenOOD基准上与主流检测方法表现相当，其中Top-K扩散损失（TKDL）方法更具稳定性和鲁棒性。结论：在表示空间进行密度估计可有效提升OOD检测性能，值得进一步研究。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Probability_Density_from_Latent_Diffusion_Models_for_Out-of-Distribution_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-End-to-End-Analysis-of-Charge-Stability-Diagrams-with-Transformers"><a href="#18-End-to-End-Analysis-of-Charge-Stability-Diagrams-with-Transformers" class="headerlink" title="18. End-to-End Analysis of Charge Stability Diagrams with Transformers"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/End-to-End_Analysis_of_Charge_Stability_Diagrams_with_Transformers.pdf">End-to-End Analysis of Charge Stability Diagrams with Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Department of Engineering Science, University of Oxford</span></p><p>本文提出了TRACS，一种基于Transformers的端到端模型，结合卷积神经网络(CNN)和Transformer架构，实现对半导体量子点阵列电荷稳定性图（CSD）的自动分析，包括三重点的识别和连通性提取。实验结果表明，TRACS在三种不同量子比特器件架构上均优于传统CNN模型YOLO，无需重新训练，具备更高的泛化性和推理速度，有助于自动化量子器件调控和扩展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/End-to-End_Analysis_of_Charge_Stability_Diagrams_with_Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation"><a href="#19-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation" class="headerlink" title="19. CineScale: Free Lunch in High-Resolution Cinematic Visual Generation"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/CineScale__Free_Lunch_in_High-Resolution_Cinematic_Visual_Generation.pdf">CineScale: Free Lunch in High-Resolution Cinematic Visual Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>该论文提出了CineScale，一种扩展自FreeScale的新型推理范式，专为扩展预训练扩散模型在高分辨率下的视觉生成能力设计。方法包含自级联上采样、受限膨胀卷积、多尺度频率融合和对DiT架构的NTK-RoPE等适配技术，并支持文本到图像、文本到视频、图像到视频和视频到视频生成。实验显示，CineScale在无需微调即可实现8k图像生成，仅需极少LoRA微调即可实现4k视频生成，生成质量优于现有方法。结论是CineScale显著拓展了主流扩散模型的高分辨率生成能力，提升了视觉质量和任务范围。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/CineScale_Free_Lunch_in_High-Resolution_Cinematic_Visual_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass"><a href="#20-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass" class="headerlink" title="20. SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/SceneGen__Single-Image_3D_Scene_Generation_in_One_Feedforward_Pass.pdf">SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出SceneGen框架，实现从单张场景图像及对象掩码直接生成多个3D资产的几何、纹理和空间关系，无需优化或检索。SceneGen采用视觉和几何编码器提取局部与全局特征，通过特征聚合模块和位置预测头，在单次前馈中完成3D场景生成，且可自然扩展到多图像输入。实验表明，SceneGen在几何、纹理质量及效率上均优于现有方法，能高效生成物理合理且高质量的3D场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/SceneGen_Single-Image_3D_Scene_Generation_in_One_Feedforward_Pass.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-WAVER-WAVE-YOUR-WAY-TO-LIFELIKE-VIDEO-GENERATION"><a href="#21-WAVER-WAVE-YOUR-WAY-TO-LIFELIKE-VIDEO-GENERATION" class="headerlink" title="21. WAVER: WAVE YOUR WAY TO LIFELIKE VIDEO GENERATION"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Waver__Wave_Your_Way_to_Lifelike_Video_Generation.pdf">WAVER: WAVE YOUR WAY TO LIFELIKE VIDEO GENERATION</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bytedance</span></p><p>本文提出了Waver，一个基于混合流Rectified Flow Transformer的统一图像与视频生成基础模型，采用了Task-Unified DiT与Cascade Refiner架构，支持文本到视频、图片到视频和文本到图片等多任务，结合高质量数据过滤管线与多阶段训练优化策略，实现了高分辨率（本地720p至1080p）视频的高效生成。实验证明Waver在复杂运动场景下运动幅度和时序一致性表现优异，在多个公开和自建基准上排名领先，超越开源模型并达到商用水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/WAVER_WAVE_YOUR_WAY_TO_LIFELIKE_VIDEO_GENERATION.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-“Does-the-cafe-entrance-look-accessible-Where-is-the-door-”-Towards-Geospatial-AI-Agents-for-Visual-Inquiries"><a href="#22-“Does-the-cafe-entrance-look-accessible-Where-is-the-door-”-Towards-Geospatial-AI-Agents-for-Visual-Inquiries" class="headerlink" title="22. “Does the cafe entrance look accessible? Where is the door?” Towards Geospatial AI Agents for Visual Inquiries"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/_Does_the_cafe_entrance_look_accessible__Where_is_the_door___Towards_Geospatial_AI_Agents_for_Visual.pdf">“Does the cafe entrance look accessible? Where is the door?” Towards Geospatial AI Agents for Visual Inquiries</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Washington</span></p><p>本文提出了Geo-Visual Agents的愿景，即多模态AI体能够融合大规模地理空间图像（如街景、航拍和用户上传照片）与GIS数据，实时理解和回答复杂的视觉-空间地理问题。方法上，结合计算机视觉、场景理解、空间推理与多模态大模型，实现如Accessibility Scout和StreetViewAI等原型系统，支持个性化、交互式地理空间视觉问答。结论认为，这一方向可极大提升导航与无障碍体验，但在数据融合、空间推理、个性化和语音输出等方面仍面临诸多挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/%E2%80%9CDoes_the_cafe_entrance_look_accessible_Where_is_the_door_%E2%80%9D_Towards_Geospatial_AI_Agents_for_Visual_Inquiries.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-LLM-empowered-Dynamic-Prompt-Routing-for-Vision-Language-Models-Tuning-under-Long-Tailed-Distributions"><a href="#23-LLM-empowered-Dynamic-Prompt-Routing-for-Vision-Language-Models-Tuning-under-Long-Tailed-Distributions" class="headerlink" title="23. LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning under Long-Tailed Distributions"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/LLM-empowered_Dynamic_Prompt_Routing_for_Vision-Language_Models_Tuning_under_Long-Tailed_Distributio.pdf">LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning under Long-Tailed Distributions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shandong University</span></p><p>该论文提出了MDPR（Multi-dimensional Dynamic Prompt Routing）框架，用于解决视觉-语言模型（VLM）在长尾分布下微调时的类别偏置问题。方法上，MDPR通过大语言模型（LLM）构建五维语义知识库，并采用动态路由机制对类别语义进行图像感知聚合，结合正则化提升尾类表现。实验结果表明，MDPR在CIFAR-100-LT、ImageNet-LT和Places-LT等长尾数据集上显著提升了尾类识别准确率，同时保持头类和中类的稳健性，达到SOTA或高度竞争的性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/LLM-empowered_Dynamic_Prompt_Routing_for_Vision-Language_Models_Tuning_under_Long-Tailed_Distributions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-GRAFT-GRAPH-AND-TABLE-REASONING-FOR-TEXTUAL-ALIGNMENT-—-A-BENCHMARK-FOR-STRUCTURED-INSTRUCTION-FOLLOWING-AND-VISUAL-REASONING"><a href="#24-GRAFT-GRAPH-AND-TABLE-REASONING-FOR-TEXTUAL-ALIGNMENT-—-A-BENCHMARK-FOR-STRUCTURED-INSTRUCTION-FOLLOWING-AND-VISUAL-REASONING" class="headerlink" title="24. GRAFT: GRAPH AND TABLE REASONING FOR TEXTUAL ALIGNMENT — A BENCHMARK FOR STRUCTURED INSTRUCTION FOLLOWING AND VISUAL REASONING"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/GRAFT__GRaPH_and_Table_Reasoning_for_Textual_Alignment_--_A_Benchmark_for_Structured_Instruction_Fol.pdf">GRAFT: GRAPH AND TABLE REASONING FOR TEXTUAL ALIGNMENT — A BENCHMARK FOR STRUCTURED INSTRUCTION FOLLOWING AND VISUAL REASONING</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ServiceNow</span></p><p>本文提出了GRAFT基准，专为评估多模态模型在结构化视觉推理、指令跟随和视觉-文本对齐任务中的能力。GRAFT利用Python可控生成的图表和表格图像，配以多步视觉推理问题和结构化答案（JSON&#x2F;YAML），并通过多LLM裁判机制确保高质量评测。结论指出，当前主流多模态模型在输出格式和完整性上表现良好，但在复杂视觉推理和语义对齐方面仍有显著提升空间，GRAFT为未来多模态问答模型的评估与发展提供了统一且细粒度的平台。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/GRAFT_GRAPH_AND_TABLE_REASONING_FOR_TEXTUAL_ALIGNMENT_%E2%80%94_A_BENCHMARK_FOR_STRUCTURED_INSTRUCTION_FOLLOWING_AND_VISUAL_REASONING.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-MapKD-Unlocking-Prior-Knowledge-with-Cross-Modal-Distillation-for-Efficient-Online-HD-Map-Construction"><a href="#25-MapKD-Unlocking-Prior-Knowledge-with-Cross-Modal-Distillation-for-Efficient-Online-HD-Map-Construction" class="headerlink" title="25. MapKD: Unlocking Prior Knowledge with Cross-Modal Distillation for Efficient Online HD Map Construction"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/MapKD__Unlocking_Prior_Knowledge_with_Cross-Modal_Distillation_for_Efficient_Online_HD_Map_Construct.pdf">MapKD: Unlocking Prior Knowledge with Cross-Modal Distillation for Efficient Online HD Map Construction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>本文提出MapKD，一种新颖的三阶段教师-教练-学生跨模态知识蒸馏框架，用于高效的在线高精地图构建。其核心流程包含：以多模态融合模型为教师、通过模拟LiDAR的教练桥接模态差异、并以轻量级视觉学生为目标，并设计了Token-Guided Patch Distillation（TGPD）和Masked Semantic Response Distillation（MSRD）两种针对性蒸馏策略，实现鸟瞰图特征和语义响应的对齐。实验结果表明，MapKD无需依赖昂贵的HD地图先验，仅用摄像头输入即可显著提升学生模型性能（mIoU提升6.68，mAP提升10.94），且推理速度更快，具有良好的实用性和部署价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/MapKD_Unlocking_Prior_Knowledge_with_Cross-Modal_Distillation_for_Efficient_Online_HD_Map_Construction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding"><a href="#26-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding" class="headerlink" title="26. When and What: Diffusion-Grounded VideoLLM with Entity-Aware Segmentation for Long Video Understanding"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/When_and_What__Diffusion-Grounded_VideoLLM_with_Entity_Aware_Segmentation_for_Long_Video_Understandi.pdf">When and What: Diffusion-Grounded VideoLLM with Entity-Aware Segmentation for Long Video Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出Grounded-VideoDiT，一种面向长视频理解的Video-LLM，结合扩散模型的时序特征提取（Diffusion Temporal Latent, DTL）、语义分割驱动的实体级表示和混合token输入结构，实现了事件精准定位和实体追踪。实验表明该方法在Charades-STA、NExT-GQA等多个长视频基准上取得了最优或高度竞争的性能，显著提升了时序和实体对齐能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/When_and_What_Diffusion-Grounded_VideoLLM_with_Entity-Aware_Segmentation_for_Long_Video_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-Deep-Equilibrium-Convolutional-Sparse-Coding-for-Hyperspectral-Image-Denoising"><a href="#27-Deep-Equilibrium-Convolutional-Sparse-Coding-for-Hyperspectral-Image-Denoising" class="headerlink" title="27. Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Deep_Equilibrium_Convolutional_Sparse_Coding_for_Hyperspectral_Image_Denoising.pdf">Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University of Science and Technology</span></p><p>该论文提出了Deep Equilibrium Convolutional Sparse Coding (DECSC) 框架，结合深度平衡模型（DEQ）与卷积稀疏编码（CSC），用于高光谱图像去噪。方法通过分解全局空间-谱一致性与局部空间-谱细节，并引入Swin Transformer和细节增强模块，提升了对非局部自相似性与细节的建模能力。实验表明，DECSC相比现有方法在各种合成与真实噪声条件下均取得了更优的去噪表现，具有更强的鲁棒性与物理可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Deep_Equilibrium_Convolutional_Sparse_Coding_for_Hyperspectral_Image_Denoising.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-D3FNet-A-Differential-Attention-Fusion-Network-for-Fine-Grained-Road-Structure-Extraction-in-Remote-Perception-Systems"><a href="#28-D3FNet-A-Differential-Attention-Fusion-Network-for-Fine-Grained-Road-Structure-Extraction-in-Remote-Perception-Systems" class="headerlink" title="28. D3FNet: A Differential Attention Fusion Network for Fine-Grained Road Structure Extraction in Remote Perception Systems"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/D3FNet__A_Differential_Attention_Fusion_Network_for_Fine-Grained_Road_Structure_Extraction_in_Remote.pdf">D3FNet: A Differential Attention Fusion Network for Fine-Grained Road Structure Extraction in Remote Perception Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Budapest University of Technology and Economics</span></p><p>该论文提出了D3FNet，一种基于D-LinkNet的Dilated Dual-Stream Differential Attention Fusion Network，用于高分辨率遥感影像中细粒度窄路结构提取。方法创新包括差分注意力膨胀提取模块（DADE）强化道路特征，双流解码融合机制（DDFM）实现结构与语义信息融合，并采用多尺度膨胀策略（1,3,5,9）提升连续性。实验结果表明D3FNet在DeepGlobe和CHN6-CUG数据集上对窄路段提取性能优越，IoU和Recall均高于主流模型，且能有效恢复遮挡和低对比度路段的连通性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/D3FNet_A_Differential_Attention_Fusion_Network_for_Fine-Grained_Road_Structure_Extraction_in_Remote_Perception_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Aligning-Moments-in-Time-using-Video-Queries"><a href="#29-Aligning-Moments-in-Time-using-Video-Queries" class="headerlink" title="29. Aligning Moments in Time using Video Queries"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Aligning_Moments_in_Time_using_Video_Queries.pdf">Aligning Moments in Time using Video Queries</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indian Institute of Technology Jodhpur</span></p><p>本文提出了MATR（Moment Alignment TRansformer），一种针对视频到视频时刻检索（Vid2VidMR）任务的Transformer架构，通过显式的双阶段序列对齐机制和自监督预训练方法，实现了查询视频与目标视频在时序和语义上的精确对齐，并结合前景&#x2F;背景分类及边界预测头，实现时刻精确定位。实验结果表明，MATR在ActivityNet-VRL和新提出的SportsMoments数据集上对比现有方法，在mIoU和R@1等指标上均取得了显著提升，验证了其方法的有效性和优越性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Aligning_Moments_in_Time_using_Video_Queries.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-Task-Generalized-Adaptive-Cross-Domain-Learning-for-Multimodal-Image-Fusion"><a href="#30-Task-Generalized-Adaptive-Cross-Domain-Learning-for-Multimodal-Image-Fusion" class="headerlink" title="30. Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Task-Generalized_Adaptive_Cross-Domain_Learning_for_Multimodal_Image_Fusion.pdf">Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanchang Hangkong University</span></p><p>该论文提出了AdaSFFuse框架，采用自适应近似小波变换（AdaWAT）进行多模态图像的高低频特征解耦，并通过空间-频率Mamba块实现跨域高效融合。实验结果表明，AdaSFFuse在红外-可见、多曝光、多焦点和医学图像融合等任务上表现优异，兼具高效性和泛化能力，成为多模态融合领域新基准。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Task-Generalized_Adaptive_Cross-Domain_Learning_for_Multimodal_Image_Fusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-Thinning-a-medical-image-segmentation-model-via-dual-level-multiscale-fusion"><a href="#31-Thinning-a-medical-image-segmentation-model-via-dual-level-multiscale-fusion" class="headerlink" title="31. Thinning a medical image segmentation model via dual-level multiscale fusion"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/LGMSNet__Thinning_a_medical_image_segmentation_model_via_dual-level_multiscale_fusion.pdf">Thinning a medical image segmentation model via dual-level multiscale fusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本论文提出了LGMSNet，一种结合本地多尺度卷积（LMS块）与稀疏Transformer-卷积混合（GMS块）的轻量级医学图像分割网络。该方法有效缓解了通道冗余和全局上下文建模不足的问题；在六个公开数据集上取得了SOTA分割精度和参数效率，尤其在四个未见数据集上表现出优异的跨域泛化能力，适用于资源受限医疗场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Thinning_a_medical_image_segmentation_model_via_dual-level_multiscale_fusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-A-Curated-Dataset-and-Deep-Learning-Approach-for-Minor-Dent-Detection-in-Vehicles"><a href="#32-A-Curated-Dataset-and-Deep-Learning-Approach-for-Minor-Dent-Detection-in-Vehicles" class="headerlink" title="32. A Curated Dataset and Deep Learning Approach for Minor Dent Detection in Vehicles"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/A_Curated_Dataset_and_Deep_Learning_Approach_for_Minor_Dent_Detection_in_Vehicles.pdf">A Curated Dataset and Deep Learning Approach for Minor Dent Detection in Vehicles</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Sciences and Technology (NUST)</span></p><p>本文提出了一种基于YOLOv8m及其改进变体（YOLOv8m-t4和YOLOv8m-t42）的深度学习检测流程，针对汽车表面微小凹陷，构建并公开了多视角、自然光照条件下的高质量标注数据集，并在训练中采用实时数据增强方法优化模型泛化能力。实验结果显示，YOLOv8m-t42模型在精度、召回率和F1分数上优于YOLOv8m-t4，具备更高的实际应用可靠性，适合工业和保险自动化场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/A_Curated_Dataset_and_Deep_Learning_Approach_for_Minor_Dent_Detection_in_Vehicles.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-From-Linearity-to-Non-Linearity-How-Masked-Autoencoders-Capture-Spatial-Correlations"><a href="#33-From-Linearity-to-Non-Linearity-How-Masked-Autoencoders-Capture-Spatial-Correlations" class="headerlink" title="33. From Linearity to Non-Linearity: How Masked Autoencoders Capture Spatial Correlations"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/From_Linearity_to_Non-Linearity__How_Masked_Autoencoders_Capture_Spatial_Correlations.pdf">From Linearity to Non-Linearity: How Masked Autoencoders Capture Spatial Correlations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Pennsylvania</span></p><p>本论文系统分析了Masked Autoencoders（MAE）在视觉领域如何通过调整遮罩比例和patch大小来捕获输入图像的空间相关性。方法上，作者对线性MAE推导了解析特征表达，说明关键超参数如何影响模型对图像空间长短程相关性的学习，并拓展至非线性MAE，通过分析输入输出Jacobian，揭示了MAE自适应学习空间相关特征的机制。结论指出，MAE能有效适应不同下游任务的数据空间特性，但其超参数需针对数据集仔细调优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/From_Linearity_to_Non-Linearity_How_Masked_Autoencoders_Capture_Spatial_Correlations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-Image-Conditioned-3D-Gaussian-Splat-Quantization"><a href="#34-Image-Conditioned-3D-Gaussian-Splat-Quantization" class="headerlink" title="34. Image-Conditioned 3D Gaussian Splat Quantization"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Image-Conditioned_3D_Gaussian_Splat_Quantization.pdf">Image-Conditioned 3D Gaussian Splat Quantization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, San Diego</span></p><p>该论文提出了ICGS-Quantizer，一种图像条件化的3D高斯点云量化方法，通过联合高斯及其属性的矢量量化和跨场景共享码本，将3D场景压缩至KB级存储；解码端可利用当前采集图像进行条件化重建，自适应场景照明与外观变化。实验表明，该方法在压缩率和场景自适应性方面显著优于现有方法，能高保真恢复和动态更新3D场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Image-Conditioned_3D_Gaussian_Splat_Quantization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Transfer-learning-optimization-based-on-evolutionary-selective-fine-tuning"><a href="#35-Transfer-learning-optimization-based-on-evolutionary-selective-fine-tuning" class="headerlink" title="35. Transfer learning optimization based on evolutionary selective fine tuning"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Transfer_learning_optimization_based_on_evolutionary_selective_fine_tuning.pdf">Transfer learning optimization based on evolutionary selective fine tuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Engineering</span></p><p>本文提出了BioTune，一种基于进化算法的自适应选择性微调方法，通过进化搜索自动决定哪些层进行微调和优化学习率，实现了迁移学习的高效参数调整。实验证明BioTune在九个图像分类数据集上相较于现有微调方法提升了准确率，并显著降低了可训练参数比例和计算成本，适用于不同领域的迁移学习任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Transfer_learning_optimization_based_on_evolutionary_selective_fine_tuning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-VideoEraser-Concept-Erasure-in-Text-to-Video-Diffusion-Models"><a href="#36-VideoEraser-Concept-Erasure-in-Text-to-Video-Diffusion-Models" class="headerlink" title="36. VideoEraser: Concept Erasure in Text-to-Video Diffusion Models"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/VideoEraser__Concept_Erasure_in_Text-to-Video_Diffusion_Models.pdf">VideoEraser: Concept Erasure in Text-to-Video Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出了VideoEraser，一种无需训练、即插即用的文本到视频扩散模型概念擦除框架。方法包括选择性提示嵌入调整（SPEA）和对抗鲁棒噪声引导（ARNG），用于在不损害无关内容生成能力的前提下，抑制模型生成不良或敏感概念。实验显示，VideoEraser在目标内容擦除、视频质量、鲁棒性和泛化性等方面均优于现有方法，在多种主流T2V扩散模型及四类任务（物体、艺术风格、名人、显式内容擦除）上均取得SOTA表现。结论是VideoEraser能有效、安全地防止T2V模型生成不良内容，适用于实际监管和合规场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/VideoEraser_Concept_Erasure_in_Text-to-Video_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-First-RAG-Second-SEG-A-Training-Free-Paradigm-for-Camouflaged-Object-Detection"><a href="#37-First-RAG-Second-SEG-A-Training-Free-Paradigm-for-Camouflaged-Object-Detection" class="headerlink" title="37. First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/First_RAG,_Second_SEG__A_Training-Free_Paradigm_for_Camouflaged_Object_Detection.pdf">First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University of Aeronautics and Astronautics</span></p><p>本文提出了一种无训练的伪装目标检测（COD）新范式RAG-SEG，将COD任务分为两阶段：首先通过RAG（检索增强生成）自动生成粗略掩码作为提示，随后利用SAM2进行精细分割。方法通过DINOv2提取特征构建向量数据库，FAISS聚类压缩检索空间，推理时用检索结果生成伪标签，再经SAM2优化分割，彻底免除传统训练。实验表明，RAG-SEG在标准COD数据集上与SOTA方法性能相当甚至更优，并能高效运行在普通笔记本电脑上，兼具实用性与低算力消耗。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/First_RAG_Second_SEG_A_Training-Free_Paradigm_for_Camouflaged_Object_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-BasketLiDAR-The-First-LiDAR-Camera-Multimodal-Dataset-for-Professional-Basketball-MOT"><a href="#38-BasketLiDAR-The-First-LiDAR-Camera-Multimodal-Dataset-for-Professional-Basketball-MOT" class="headerlink" title="38. BasketLiDAR: The First LiDAR-Camera Multimodal Dataset for Professional Basketball MOT"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/BasketLiDAR__The_First_LiDAR-Camera_Multimodal_Dataset_for_Professional_Basketball_MOT.pdf">BasketLiDAR: The First LiDAR-Camera Multimodal Dataset for Professional Basketball MOT</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Keio University</span></p><p>该论文首次构建了面向专业篮球多目标跟踪（MOT）的LiDAR-多摄像头同步多模态数据集BasketLiDAR，并提出了一种融合LiDAR点云与多视角摄像头图像的高效MOT方法。实验表明，该方法在遮挡场景下显著提升了跟踪准确率与ID一致性，并大幅降低了计算成本，实现了实时高精度运动员3D跟踪。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/BasketLiDAR_The_First_LiDAR-Camera_Multimodal_Dataset_for_Professional_Basketball_MOT.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-DESIGNCLIP-Multimodal-Learning-with-CLIP-for-Design-Patent-Understanding"><a href="#39-DESIGNCLIP-Multimodal-Learning-with-CLIP-for-Design-Patent-Understanding" class="headerlink" title="39. DESIGNCLIP: Multimodal Learning with CLIP for Design Patent Understanding"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/DesignCLIP__Multimodal_Learning_with_CLIP_for_Design_Patent_Understanding.pdf">DESIGNCLIP: Multimodal Learning with CLIP for Design Patent Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Chicago</span></p><p>该论文提出了DESIGNCLIP，一个针对设计专利多模态分析的CLIP改进框架，通过引入类别感知采样与对比损失，有效缓解专利类别长尾分布问题，并结合多视角图像对比学习与多任务预训练（包括专利分类、图像-文本和多视角对比学习）。实验显示，DESIGNCLIP在专利分类、检索和多模态检索等任务上均优于现有SOTA方法，提升了专利领域的多模态理解与泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/DESIGNCLIP_Multimodal_Learning_with_CLIP_for_Design_Patent_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-RATopo-Improving-Lane-Topology-Reasoning-via-Redundancy-Assignment"><a href="#40-RATopo-Improving-Lane-Topology-Reasoning-via-Redundancy-Assignment" class="headerlink" title="40. RATopo: Improving Lane Topology Reasoning via Redundancy Assignment"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/RATopo__Improving_Lane_Topology_Reasoning_via_Redundancy_Assignment.pdf">RATopo: Improving Lane Topology Reasoning via Redundancy Assignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>本文提出了一种名为RATopo的模型无关型冗余分配策略，通过重构Transformer解码器（交换交叉注意力和自注意力顺序）并引入多路并行交叉注意力模块，实现对车道拓扑推理任务的数量丰富和几何多样的有效监督。实验结果显示，RATopo能无缝集成到现有DETR风格的拓扑推理框架，显著提升OpenLane-V2等数据集上的车道-车道与车道-交通元素拓扑推理性能，具备良好的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/RATopo_Improving_Lane_Topology_Reasoning_via_Redundancy_Assignment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-Explainable-Knowledge-Distillation-for-Efficient-Medical-Image-Classification"><a href="#41-Explainable-Knowledge-Distillation-for-Efficient-Medical-Image-Classification" class="headerlink" title="41. Explainable Knowledge Distillation for Efficient Medical Image Classification"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Explainable_Knowledge_Distillation_for_Efficient_Medical_Image_Classification.pdf">Explainable Knowledge Distillation for Efficient Medical Image Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Jamia Millia Islamia</span></p><p>本文提出一种结合知识蒸馏与可解释性AI的框架，将高容量教师模型（如DenseNet-201、Visformer-S、AutoFormerV2-T）知识压缩到轻量化的OFA-595学生模型，采用混合损失函数（硬标签与软目标）并通过Score-CAM实现模型解释。实验结果表明，该学生模型在COVID-QU-Ex和LCS25000医学影像数据集上保持高准确率和F1分数，参数量和推理时间显著降低，适用于资源受限临床环境。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Explainable_Knowledge_Distillation_for_Efficient_Medical_Image_Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Pathology-Informed-Latent-Diffusion-Model-for-Anomaly-Detection-in-Lymph-Node-Metastasis"><a href="#42-Pathology-Informed-Latent-Diffusion-Model-for-Anomaly-Detection-in-Lymph-Node-Metastasis" class="headerlink" title="42. Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph Node Metastasis"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Pathology-Informed_Latent_Diffusion_Model_for_Anomaly_Detection_in_Lymph_Node_Metastasis.pdf">Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph Node Metastasis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea University</span></p><p>该论文提出了一种结合潜在扩散模型（LDM）和视觉语言模型（VLM）的无监督异常检测方法AnoPILaD，通过利用病理学关键词引导重建过程，实现对淋巴结转移病理图像异常区域的高效检测。实验结果表明，该方法在不同器官和数据域下均优于现有密度和重建类方法，具有较强的泛化性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Pathology-Informed_Latent_Diffusion_Model_for_Anomaly_Detection_in_Lymph_Node_Metastasis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation"><a href="#43-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation" class="headerlink" title="43. Collaborative Multi-Modal Coding for High-Quality 3D Generation"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Collaborative_Multi-Modal_Coding_for_High-Quality_3D_Generation.pdf">Collaborative Multi-Modal Coding for High-Quality 3D Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>该论文提出了TriMM，一种协同多模态编码及三平面潜变量扩散模型，能够将RGB、RGBD及点云等多模态特征映射到统一三平面隐空间，通过结合2D图像和3D结构的混合监督及专门的重建损失，高效提升3D生成的几何和纹理质量。实验表明，TriMM在小规模数据集下实现了与大规模训练模型相媲美的3D生成效果，并验证了多模态数据可显著提升模型性能与泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Collaborative_Multi-Modal_Coding_for_High-Quality_3D_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-STAGNet-A-Spatio-Temporal-Graph-and-LSTM-Framework-for-Accident-Anticipation"><a href="#44-STAGNet-A-Spatio-Temporal-Graph-and-LSTM-Framework-for-Accident-Anticipation" class="headerlink" title="44. STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/STAGNet__A_Spatio-Temporal_Graph_and_LSTM_Framework_for_Accident_Anticipation.pdf">STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Moratuwa</span></p><p>该论文提出了STAGNet框架，将SlowFast网络提取的全局时空特征与LSTM进行帧级聚合，并在对象级图中采用GATv2注意力机制，专注于关键交通目标及其动态关系，实现了仅依赖行车记录仪视频的交通事故早期预测。实验表明，STAGNet在多个公开数据集上平均精度和事故预测提前量均优于现有方法，且跨数据集泛化能力更强。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/STAGNet_A_Spatio-Temporal_Graph_and_LSTM_Framework_for_Accident_Anticipation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-TPA-Temporal-Prompt-Alignment-for-Fetal-Congenital-Heart-Defect-Classification"><a href="#45-TPA-Temporal-Prompt-Alignment-for-Fetal-Congenital-Heart-Defect-Classification" class="headerlink" title="45. TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/TPA__Temporal_Prompt_Alignment_for_Fetal_Congenital_Heart_Defect_Classification.pdf">TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</span></p><p>本文提出了Temporal Prompt Alignment (TPA)方法，通过结合视觉-语言基础模型、可训练时序特征提取器与提示对齐对比学习，实现了对胎儿超声心脏视频先天性心脏病（CHD）的多类别分类。TPA在视频级别引入了条件变分自编码风格调制（CVAESM）模块以量化不确定性，从而提升模型在临床中的可靠性。实验表明，TPA在私有CHD数据集和EchoNet-Dynamic公开数据集上均取得了当前最佳的分类性能，并显著优化了模型置信度校准。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/TPA_Temporal_Prompt_Alignment_for_Fetal_Congenital_Heart_Defect_Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-Comp-X-On-Defining-an-Interactive-Learned-Image-Compression-Paradigm-With-Expert-driven-LLM-Agent"><a href="#46-Comp-X-On-Defining-an-Interactive-Learned-Image-Compression-Paradigm-With-Expert-driven-LLM-Agent" class="headerlink" title="46. Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Comp-X__On_Defining_an_Interactive_Learned_Image_Compression_Paradigm_With_Expert-driven_LLM_Agent.pdf">Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本论文提出Comp-X：一种由大语言模型（LLM）智能体驱动的交互式图像压缩新范式，集成多功能统一编解码框架，支持人机感知、变量压缩和空间比特分配。核心方法包括：1）多功能统一图像编解码器，2）结合专家反馈的增强式上下文学习（ICL-EF）提升LLM理解压缩知识与用户需求的能力，3）首个交互式图像压缩评测基准IIC-Bench。实验表明，Comp-X能高效理解用户自然语言指令，实现自动参数选择和自适应压缩，指令解析成功率超80%，压缩性能与现有顶尖方法相当，展示了在智能图像压缩和通用人工智能方向的前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Comp-X_On_Defining_an_Interactive_Learned_Image_Compression_Paradigm_With_Expert-driven_LLM_Agent.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-Pretrained-Diffusion-Models-Are-Inherently-Skipped-Step-Samplers"><a href="#47-Pretrained-Diffusion-Models-Are-Inherently-Skipped-Step-Samplers" class="headerlink" title="47. Pretrained Diffusion Models Are Inherently Skipped-Step Samplers"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Pretrained_Diffusion_Models_Are_Inherently_Skipped-Step_Samplers.pdf">Pretrained Diffusion Models Are Inherently Skipped-Step Samplers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>该论文提出了一种跳步采样机制，证明预训练扩散模型本身即可实现跳过多个中间去噪步骤以加速生成，无需重新训练或复杂修改，并与DDIM方法结合进一步提升采样效率和生成质量。实验在OpenAI ADM、Stable Diffusion和Open Sora等主流扩散模型上，涵盖图像和视频生成，结果显示在大幅减少采样步数的同时，生成质量和多样性优于主流基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Pretrained_Diffusion_Models_Are_Inherently_Skipped-Step_Samplers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-Center-Oriented-Prototype-Contrastive-Clustering"><a href="#48-Center-Oriented-Prototype-Contrastive-Clustering" class="headerlink" title="48. Center-Oriented Prototype Contrastive Clustering"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Center-Oriented_Prototype_Contrastive_Clustering.pdf">Center-Oriented Prototype Contrastive Clustering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University of Information Science and Technology</span></p><p>该论文提出一种中心导向原型对比聚类方法（CPCC），结合软原型对比模块和双一致性学习模块，软原型通过样本属于聚类中心的概率加权计算原型，减少类间冲突和原型漂移；双一致性学习通过对同一样本不同变换和不同样本邻域的一致性约束，保证特征变换不变的语义信息和紧凑的类内分布。实验结果表明，该方法在五个数据集上聚类表现优于现有主流方法，并提升了训练稳定性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Center-Oriented_Prototype_Contrastive_Clustering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-GenTune-Toward-Traceable-Prompts-to-Improve-Controllability-of-Image-Refinement-in-Environment-Design"><a href="#49-GenTune-Toward-Traceable-Prompts-to-Improve-Controllability-of-Image-Refinement-in-Environment-Design" class="headerlink" title="49. GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/GenTune__Toward_Traceable_Prompts_to_Improve_Controllability_of_Image_Refinement_in_Environment_Desi.pdf">GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Taiwan University</span></p><p>该论文提出了GenTune系统，通过可追溯的文本标签与图像元素映射，结合语义引导的逐步细粒度图像修饰方法，提升环境设计中AI生成图像的可理解性与可控性。实验显示，GenTune显著提升了设计师对生成内容的理解、精修效率及结果一致性，并在真实工作流中有效改善了创意沟通与设计效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/GenTune_Toward_Traceable_Prompts_to_Improve_Controllability_of_Image_Refinement_in_Environment_Design.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-See-it-Say-it-Sorted-Agentic-System-for-Compositional-Diagram-Generation"><a href="#50-See-it-Say-it-Sorted-Agentic-System-for-Compositional-Diagram-Generation" class="headerlink" title="50. See it. Say it. Sorted: Agentic System for Compositional Diagram Generation"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/See_it._Say_it._Sorted__Agentic_System_for_Compositional_Diagram_Generation.pdf">See it. Say it. Sorted: Agentic System for Compositional Diagram Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yale University</span></p><p>该论文提出了一种训练无关的智能体系统，结合视觉语言模型（VLM）和大语言模型（LLM），通过迭代Critic–Candidates–Judge流程将手绘草图和文本说明转换为可编辑SVG图形。方法强调高层次的结构推理和多策略SVG生成，实验结果表明在结构重建和细节保留方面显著优于前沿图像生成LLM模型。结论：系统能稳定、准确地生成可控且可扩展的结构化图表，可集成到实际设计工具中。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/See_it._Say_it._Sorted_Agentic_System_for_Compositional_Diagram_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-MeSS-City-Mesh-Guided-Outdoor-Scene-Generation-with-Cross-View-Consistent-Diffusion"><a href="#51-MeSS-City-Mesh-Guided-Outdoor-Scene-Generation-with-Cross-View-Consistent-Diffusion" class="headerlink" title="51. MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/MeSS__City_Mesh-Guided_Outdoor_Scene_Generation_with_Cross-View_Consistent_Diffusion.pdf">MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">TU Munich</span></p><p>该论文提出MeSS，一种利用城市网格模型作为几何先验，通过多阶段图像扩散模型（Cascaded Outpainting ControlNets、Appearance Guided Inpainting和Global Consistency Alignment）生成高质量、跨视角一致的3D高斯场景。方法在保持几何对齐与外观一致性的同时，显著提升了城市级室外场景生成的细节和风格多样性，实验表明其在视觉质量和一致性上均优于同类方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/MeSS_City_Mesh-Guided_Outdoor_Scene_Generation_with_Cross-View_Consistent_Diffusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-Zero-shot-Volumetric-CT-Super-Resolution-using-3D-Gaussian-Splatting-with-Upsampled-2D-X-ray-Projection-Priors"><a href="#52-Zero-shot-Volumetric-CT-Super-Resolution-using-3D-Gaussian-Splatting-with-Upsampled-2D-X-ray-Projection-Priors" class="headerlink" title="52. Zero-shot Volumetric CT Super-Resolution using 3D Gaussian Splatting with Upsampled 2D X-ray Projection Priors"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Zero-shot_Volumetric_CT_Super-Resolution_using_3D_Gaussian_Splatting_with_Upsampled_2D_X-ray_Project.pdf">Zero-shot Volumetric CT Super-Resolution using 3D Gaussian Splatting with Upsampled 2D X-ray Projection Priors</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea University</span></p><p>该论文提出了一种零样本3D CT超分辨率重建框架，利用基于扩散模型的高分辨率2D X射线投影作为先验，结合3D Gaussian Splatting（3DGS）与提出的负Alpha混合（NAB-GS）机制，实现3D CT体数据的高质量超分重建。方法流程为：首先对低分辨率CT体数据生成多角度2D投影，利用无监督扩散模型和逐投影自适应采样（PAS）策略提升2D投影分辨率，然后用NAB-GS在3DGS框架下进行残差学习重建高分辨率3D CT体。实验结果显示，该方法在无高分辨率CT体训练数据的情况下，在定量和定性指标上均优于现有方法，并能更好地恢复解剖细节。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Zero-shot_Volumetric_CT_Super-Resolution_using_3D_Gaussian_Splatting_with_Upsampled_2D_X-ray_Projection_Priors.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation"><a href="#53-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation" class="headerlink" title="53. Mobile-Agent-v3: Foundamental Agents for GUI Automation"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Mobile-Agent-v3__Foundamental_Agents_for_GUI_Automation.pdf">Mobile-Agent-v3: Foundamental Agents for GUI Automation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tongyi Lab, Alibaba Group</span></p><p>该论文提出了GUI-Owl，一种基于Qwen2.5-VL的多模态GUI自动化基础模型，通过大规模交互数据后训练，统一实现感知、定位、推理、规划和动作执行，并以此为核心构建了多智能体框架Mobile-Agent-v3，实现跨平台（桌面与移动）环境下的GUI自动化任务。论文创新包括自进化轨迹数据生产、高质量环境基础设施、多样化能力构建及可扩展强化学习训练，并在十项主流基准上达到了开源新SOTA，显著优于现有开源和专有模型。结论是GUI-Owl及Mobile-Agent-v3具备强泛化、适应性和多智能体协作能力，推动了GUI自动化智能体的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Mobile-Agent-v3_Foundamental_Agents_for_GUI_Automation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="54-Intern-S1-A-Scientific-Multimodal-Foundation-Model"><a href="#54-Intern-S1-A-Scientific-Multimodal-Foundation-Model" class="headerlink" title="54. Intern-S1: A Scientific Multimodal Foundation Model"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Intern-S1__A_Scientific_Multimodal_Foundation_Model.pdf">Intern-S1: A Scientific Multimodal Foundation Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai AI Laboratory</span></p><p>Intern-S1提出了一种专为科学领域设计的多模态Mixture-of-Experts大模型（28B激活参数，241B总参数），通过高比例科学数据（2.5T tokens）持续预训练、创新的多模态结构（视觉编码器、动态分词器、时序信号编码器）、以及Mixture-of-Rewards框架实现千任务并行在线强化学习，极大提升了科学领域的专业推理能力。实验表明，Intern-S1在科学专业任务（如分子合成、反应预测、晶体稳定性判断）上超越了开源和闭源SOTA模型，并在通用推理任务上保持一流水平，实现了科学AI基础模型的重大突破。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Intern-S1_A_Scientific_Multimodal_Foundation_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-Language-Guided-Tuning-Enhancing-Numeric-Optimization-with-Textual-Feedback"><a href="#55-Language-Guided-Tuning-Enhancing-Numeric-Optimization-with-Textual-Feedback" class="headerlink" title="55. Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Language-Guided_Tuning__Enhancing_Numeric_Optimization_with_Textual_Feedback.pdf">Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>该论文提出了Language-Guided Tuning (LGT) 框架，利用多智能体大语言模型通过自然语言推理来优化模型架构、特征工程、训练策略和超参数，采用文本梯度与数值梯度结合实现多维度自适应优化。实验结果显示，LGT在六个数据集上的表现显著优于传统方法，提高了准确率并降低了误差，同时优化过程具备高可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Language-Guided_Tuning_Enhancing_Numeric_Optimization_with_Textual_Feedback.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-Dissecting-Tool-Integrated-Reasoning-An-Empirical-Study-and-Analysis"><a href="#56-Dissecting-Tool-Integrated-Reasoning-An-Empirical-Study-and-Analysis" class="headerlink" title="56. Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Dissecting_Tool-Integrated_Reasoning__An_Empirical_Study_and_Analysis.pdf">Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Artificial Intelligence Laboratory</span></p><p>该论文提出了REASONZOO基准，系统性地评估工具集成推理（TIR）对大语言模型在九类复杂推理任务上的泛化和效率提升，并创新性引入PAC和AUC-PCC两项推理效率指标。实验结果显示，TIR方法（如PoT、MT-TIR、TIT）能广泛提升推理准确率和效率，显著减少冗余计算与“过度思考”现象，推动LLM在复杂推理领域的表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Dissecting_Tool-Integrated_Reasoning_An_Empirical_Study_and_Analysis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-EcomMMMU-Strategic-Utilization-of-Visuals-for-Robust-Multimodal-E-Commerce-Models"><a href="#57-EcomMMMU-Strategic-Utilization-of-Visuals-for-Robust-Multimodal-E-Commerce-Models" class="headerlink" title="57. EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/EcomMMMU__Strategic_Utilization_of_Visuals_for_Robust_Multimodal_E-Commerce_Models.pdf">EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Ohio State University</span></p><p>本文提出EcomMMMU数据集，包含406,190个样本和8,989,510张图片，专为电商多模态多任务理解设计，并构建视觉显著子集（VSS）系统性评估多模态大语言模型（MLLMs）对视觉内容的有效利用。分析发现，产品图片并非始终提升任务表现，有时反而带来干扰。为此，作者提出SUMEI方法，通过视觉效用预测，筛选“有用”图片用于下游任务，大幅提升多模态模型电商应用表现和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/EcomMMMU_Strategic_Utilization_of_Visuals_for_Robust_Multimodal_E-Commerce_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-StreamMem-Query-Agnostic-KV-Cache-Memory-for-Streaming-Video-Understanding"><a href="#58-StreamMem-Query-Agnostic-KV-Cache-Memory-for-Streaming-Video-Understanding" class="headerlink" title="58. StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/StreamMem__Query-Agnostic_KV_Cache_Memory_for_Streaming_Video_Understanding.pdf">StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Meta AI</span></p><p>StreamMem提出了一种针对流式视频理解的训练无关、查询无关KV缓存压缩机制。其通过利用视觉token与通用查询token之间的注意力分数进行关键内容保留，并结合帧过滤与加权合并，持续保持固定KV缓存，实现高效、准确的视频问答，在多个长视频理解和流式问答基准上取得了领先性能。结论指出，StreamMem无需微调即可在多种MLLM模型与内存受限环境下高效工作，显著提升了流式视频理解能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/StreamMem_Query-Agnostic_KV_Cache_Memory_for_Streaming_Video_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning"><a href="#59-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning" class="headerlink" title="59. End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/End-to-End_Agentic_RAG_System_Training_for_Traceable_Diagnostic_Reasoning.pdf">End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出Deep-DxSearch，一种结合大型医学检索语料库和强化学习的端到端Agentic RAG系统，针对医疗诊断任务优化LLM的检索-推理流程。方法包括构建多源医学知识库、设计五步动作空间并用软奖励RL进行联合训练，实现检索与推理的协同优化。实验结果显示，该系统在多中心数据集上显著提升诊断准确率，超越GPT-4o、DeepSeek-R1等强基线，具备更强泛化性和解释性。结论：Deep-DxSearch通过端到端RL训练，将医学外部知识与推理深度融合，显著提高诊断准确性和可靠性，在常见及罕见疾病场景均优于现有方法，为安全关键医疗领域提供了更可追溯的诊断支持。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/End-to-End_Agentic_RAG_System_Training_for_Traceable_Diagnostic_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-Measuring-the-environmental-impact-of-delivering-AI-at-Google-Scale"><a href="#60-Measuring-the-environmental-impact-of-delivering-AI-at-Google-Scale" class="headerlink" title="60. Measuring the environmental impact of delivering AI at Google Scale"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Measuring_the_environmental_impact_of_delivering_AI_at_Google_Scale.pdf">Measuring the environmental impact of delivering AI at Google Scale</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Google, Mountain View, CA, USA</span></p><p>本论文提出并实施了一个全面的方法学，首次在大规模生产环境中测量AI推理服务的能耗、碳排放和用水，包括AI加速器、主机系统、闲置机器及数据中心能耗等全栈指标。通过对Google Gemini AI助手的详细监测，结果显示单次文本推理的能耗远低于以往公开估算，并且通过软硬件协同优化，Gemini产品在一年内实现了能耗33倍和碳排放44倍的显著降低。结论表明，全面、标准化的环境影响测量对于推动AI服务全栈效率提升至关重要。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Measuring_the_environmental_impact_of_delivering_AI_at_Google_Scale.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-Communication-Efficient-LLM-Pre-training-with-SparseLoCo"><a href="#61-Communication-Efficient-LLM-Pre-training-with-SparseLoCo" class="headerlink" title="61. Communication Efficient LLM Pre-training with SparseLoCo"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Communication_Efficient_LLM_Pre-training_with_SparseLoCo.pdf">Communication Efficient LLM Pre-training with SparseLoCo</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Templar AI</span></p><p>本文提出SparseLoCo算法，通过将TOP-k稀疏化和量化结合于分布式大语言模型（LLM）预训练，实现极高压缩比（1–3%稀疏度与2位量化），显著降低跨数据中心的通信成本。该方法利用局部误差反馈近似全局动量，并通过稀疏聚合提升模型性能。实验表明，在多种通信受限环境下，SparseLoCo在通信效率和最终性能上均优于现有方法（如DiLoCo和DeMo），并处于损失与通信量的帕累托前沿。结论：SparseLoCo有效减少通信开销且性能优于主流低通信量训练方法，适用于全球分布式LLM大规模预训练。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Communication_Efficient_LLM_Pre-training_with_SparseLoCo.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-Benchmarking-Computer-Science-Survey-Generation"><a href="#62-Benchmarking-Computer-Science-Survey-Generation" class="headerlink" title="62. Benchmarking Computer Science Survey Generation"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Benchmarking_Computer_Science_Survey_Generation.pdf">Benchmarking Computer Science Survey Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了SurGE——一个面向计算机科学领域的科学综述自动生成基准，包括高质量专家综述、百万级学术论文检索库以及自动化多维评估框架。通过对多种大语言模型驱动的检索-生成方法进行系统评测，发现即使先进自反框架也难以生成高质量综述，凸显任务复杂性和未来研究需求。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Benchmarking_Computer_Science_Survey_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-Trained-Miniatures-Low-cost-High-Efficacy-SLMs-for-Sales-Marketing"><a href="#63-Trained-Miniatures-Low-cost-High-Efficacy-SLMs-for-Sales-Marketing" class="headerlink" title="63. Trained Miniatures: Low cost, High Efficacy SLMs for Sales &amp; Marketing"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Trained_Miniatures__Low_cost,_High_Efficacy_SLMs_for_Sales_&_Marketing.pdf">Trained Miniatures: Low cost, High Efficacy SLMs for Sales &amp; Marketing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">SuperAGI Research</span></p><p>本文提出“Trained Miniatures”方法，采用大型语言模型（LLM）生成领域特定数据并经人工审核后，利用全参数微调或LoRA等高效参数微调方法，训练小型语言模型（SLM）以满足销售与市场营销场景的高效低成本需求。实验结果表明，4B–12B参数规模的开源SLM在邮件营销等关键业务指标上可接近LLM表现，LoRA微调在大幅降低算力和部署成本的同时，性能损失极小，适合大规模和资源敏感应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Trained_Miniatures_Low_cost_High_Efficacy_SLMs_for_Sales_%26_Marketing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-Think-in-Blocks-Adaptive-Reasoning-from-Direct-Response-to-Deep-Reasoning"><a href="#64-Think-in-Blocks-Adaptive-Reasoning-from-Direct-Response-to-Deep-Reasoning" class="headerlink" title="64. Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Think_in_Blocks__Adaptive_Reasoning_from_Direct_Response_to_Deep_Reasoning.pdf">Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了Think in Blocks框架，通过将推理过程划分为可调节的块，实现了大语言模型（LLM）对于不同任务复杂度的自适应推理深度。方法包括三阶段训练：监督微调、Direct Preference Optimization（DPO）和强化学习（RL），使模型能先预测所需推理块数，再按块生成推理内容。结论显示，该方法在减少平均答案长度25.1%的同时，几乎不降低准确率，提升了模型的推理效率和灵活性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Think_in_Blocks_Adaptive_Reasoning_from_Direct_Response_to_Deep_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-Subjective-Behaviors-and-Preferences-in-LLM-Language-of-Browsing"><a href="#65-Subjective-Behaviors-and-Preferences-in-LLM-Language-of-Browsing" class="headerlink" title="65. Subjective Behaviors and Preferences in LLM: Language of Browsing"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Subjective_Behaviors_and_Preferences_in_LLM__Language_of_Browsing.pdf">Subjective Behaviors and Preferences in LLM: Language of Browsing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Adobe Research</span></p><p>论文提出了HeTLM（Heterogeneity aware Training of Language Model）方法，通过将用户的网页浏览行为序列进行嵌入编码和聚类，采用Actor-Critic框架对小型语言模型进行集群细化训练，使每个集群模型更好地捕捉用户的主观偏好和行为模式。实验结果显示，采用HeTLM后的小型LM在页面生成和结果预测任务上整体优于更大单一LM，同时降低了用户级性能方差，提高了个性化对齐。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Subjective_Behaviors_and_Preferences_in_LLM_Language_of_Browsing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-Reliable-Unlearning-Harmful-Information-in-LLMs-with-Metamorphosis-Representation-Projection"><a href="#66-Reliable-Unlearning-Harmful-Information-in-LLMs-with-Metamorphosis-Representation-Projection" class="headerlink" title="66. Reliable Unlearning Harmful Information in LLMs with Metamorphosis Representation Projection"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Reliable_Unlearning_Harmful_Information_in_LLMs_with_Metamorphosis_Representation_Projection.pdf">Reliable Unlearning Harmful Information in LLMs with Metamorphosis Representation Projection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出了一种针对大型语言模型（LLMs）持续性有害信息“遗忘”的新方法——Metamorphosis Representation Projection（MRP）。该方法通过在隐藏状态空间中应用不可逆的投影矩阵，将不良信息从特定网络层的表示中正交去除，同时最大程度保留有用知识，有效防止遗忘信息的恢复并支持多轮连续遗忘。实验结果显示，MRP在连续遗忘和抵抗再学习攻击方面大幅优于现有方法，能够在仅更新极少参数的情况下实现高效、安全的知识移除而不损害模型通用性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Reliable_Unlearning_Harmful_Information_in_LLMs_with_Metamorphosis_Representation_Projection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-GraSP-A-Unified-Graph-Based-Framework-for-Scalable-Generation-Quality-Tagging-and-Management-of-Synthetic-Data-for-SFT-and-DPO"><a href="#67-GraSP-A-Unified-Graph-Based-Framework-for-Scalable-Generation-Quality-Tagging-and-Management-of-Synthetic-Data-for-SFT-and-DPO" class="headerlink" title="67. GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/GraSP__A_Unified_Graph-Based_Framework_for_Scalable_Generation,_Quality_Tagging,_and_Management_of_S.pdf">GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ServiceNow Inc.</span></p><p>GraSP提出了一个基于图结构的可扩展合成数据生成框架，采用低代码YAML配置和模块化DAG编排，支持复杂多分支、多模态输入、代理执行等流程，自动实现高质量数据标注与过滤。实验结果显示GraSP极大提升了大语言模型训练中数据生成的速度和质量，促进了透明、高效的数据管理。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/GraSP_A_Unified_Graph-Based_Framework_for_Scalable_Generation_Quality_Tagging_and_Management_of_Synthetic_Data_for_SFT_and_DPO.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-When-Audio-and-Text-Disagree-Benchmarking-Text-Bias-in-Large-Audio-Language-Models-under-Cross-Modal-Inconsistencies"><a href="#68-When-Audio-and-Text-Disagree-Benchmarking-Text-Bias-in-Large-Audio-Language-Models-under-Cross-Modal-Inconsistencies" class="headerlink" title="68. When Audio and Text Disagree: Benchmarking Text Bias in Large Audio-Language Models under Cross-Modal Inconsistencies"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/When_Audio_and_Text_Disagree__Revealing_Text_Bias_in_Large_Audio-Language_Models.pdf">When Audio and Text Disagree: Benchmarking Text Bias in Large Audio-Language Models under Cross-Modal Inconsistencies</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>该论文提出了MCR-BENCH基准，用于评估大型音频-语言模型（LALMs）在音频与文本信息不一致时的表现，涵盖音频问答、语音情感识别和人声分类等任务。实验发现当前主流LALMs普遍对文本输入有显著偏好，导致在音频为主的任务中性能大幅下降，仅有限的微调和提示工程能部分缓解该问题。结论：现有模型在多模态冲突下可靠性不足，亟需新的训练范式与融合机制以提升音频-语言模型的稳健性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/When_Audio_and_Text_Disagree_Benchmarking_Text_Bias_in_Large_Audio-Language_Models_under_Cross-Modal_Inconsistencies.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-Unveiling-Trust-in-Multimodal-Large-Language-Models-Evaluation-Analysis-and-Mitigation"><a href="#69-Unveiling-Trust-in-Multimodal-Large-Language-Models-Evaluation-Analysis-and-Mitigation" class="headerlink" title="69. Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Unveiling_Trust_in_Multimodal_Large_Language_Models__Evaluation,_Analysis,_and_Mitigation.pdf">Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了MultiTrust-X，一个三维度全面评估多模态大语言模型（MLLMs）可信度的基准，包括真实、鲁棒性、安全、公平和隐私五大方面，并涵盖多模态和跨模态风险，以及数据、架构、训练和推理等多种缓解方法。实验表明，当前开源MLLMs在可信度和通用能力上仍存在显著差距，且多模态训练和推理会放大风险。作者提出基于链式推理的安全对齐方法RESA，有效提升模型安全性并保持性能，在MultiTrust-X上取得开源模型最佳表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Unveiling_Trust_in_Multimodal_Large_Language_Models_Evaluation_Analysis_and_Mitigation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-DeepThink3D-Enhancing-Large-Language-Models-with-Programmatic-Reasoning-in-Complex-3D-Situated-Reasoning-Tasks"><a href="#70-DeepThink3D-Enhancing-Large-Language-Models-with-Programmatic-Reasoning-in-Complex-3D-Situated-Reasoning-Tasks" class="headerlink" title="70. DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/DeepThink3D__Enhancing_Large_Language_Models_with_Programmatic_Reasoning_in_Complex_3D_Situated_Reas.pdf">DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出DeepThink3D框架，利用大语言模型（LLM）结合程序化API调用，采用两阶段优化策略（监督微调SFT与直接偏好优化DPO）提升LLM在复杂3D场景中的推理和执行能力。方法通过自动生成多步复杂问题扩展训练集，显著增强模型在3D环境下的结构化推理能力和代码可执行性。实验表明DeepThink3D在SQA3D基准上准确率领先，提升了3D情境推理的泛化性、可解释性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/DeepThink3D_Enhancing_Large_Language_Models_with_Programmatic_Reasoning_in_Complex_3D_Situated_Reasoning_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-Super-additive-Cooperation-in-Language-Model-Agents"><a href="#71-Super-additive-Cooperation-in-Language-Model-Agents" class="headerlink" title="71. Super-additive Cooperation in Language Model Agents"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Super-additive_Cooperation_in_Language_Model_Agents.pdf">Super-additive Cooperation in Language Model Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Southern Denmark</span></p><p>本文提出了一种基于大语言模型的多智能体虚拟锦标赛框架，通过迭代囚徒困境（IPD）模拟重复互动与组间竞争，验证了“超加性合作”理论。实验结果显示，在同时存在组内重复互动和组间竞争时，Qwen3与Phi4模型的整体及首次合作率显著高于单独机制，促进了更强的合作倾向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Super-additive_Cooperation_in_Language_Model_Agents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-From-Bits-to-Boardrooms-A-Cutting-Edge-Multi-Agent-LLM-Framework-for-Business-Excellence"><a href="#72-From-Bits-to-Boardrooms-A-Cutting-Edge-Multi-Agent-LLM-Framework-for-Business-Excellence" class="headerlink" title="72. From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/From_Bits_to_Boardrooms__A_Cutting-Edge_Multi-Agent_LLM_Framework_for_Business_Excellence.pdf">From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">College of Artificial Intelligence and Automation, Hohai University</span></p><p>该论文提出BusiAgent，一种面向企业决策的多智能体大语言模型（LLM）框架，融合扩展连续时间马尔可夫决策过程（CTMDP）建模、基于熵的头脑风暴、分层Stackelberg博弈、工具集成和上下文Thompson采样，实现多角色（如CEO、CTO、CFO等）高效协作、任务分配与流程优化。实验证明，BusiAgent在跨行业复杂业务场景下显著提升了问题分析、任务分配和方案开发的质量与用户满意度，能有效整合细粒度运营洞见与宏观战略，实现AI驱动的企业决策智能化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/From_Bits_to_Boardrooms_A_Cutting-Edge_Multi-Agent_LLM_Framework_for_Business_Excellence.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-Test-time-Corpus-Feedback-From-Retrieval-to-RAG"><a href="#73-Test-time-Corpus-Feedback-From-Retrieval-to-RAG" class="headerlink" title="73. Test-time Corpus Feedback: From Retrieval to RAG"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Test-time_Corpus_Feedback__From_Retrieval_to_RAG.pdf">Test-time Corpus Feedback: From Retrieval to RAG</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">L3S Research Center</span></p><p>本文系统综述了检索增强生成（RAG）系统中测试时语料库反馈机制，聚焦于如何在RAG管道的查询、检索和生成阶段引入动态反馈信号以提升整体性能。总结了基于大语言模型的查询重写、检索自适应、生成时反馈等关键技术流程，并提出了开放性挑战与未来方向。结论认为反馈机制能显著提高复杂任务的检索和推理能力，推动RAG系统向更高效、动态和智能化发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Test-time_Corpus_Feedback_From_Retrieval_to_RAG.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-Foundational-Design-Principles-and-Patterns-for-Building-Robust-and-Adaptive-GenAI-Native-Systems"><a href="#74-Foundational-Design-Principles-and-Patterns-for-Building-Robust-and-Adaptive-GenAI-Native-Systems" class="headerlink" title="74. Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Foundational_Design_Principles_and_Patterns_for_Building_Robust_and_Adaptive_GenAI-Native_Systems.pdf">Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nokia Bell Labs</span></p><p>本论文提出了构建健壮且自适应的GenAI-native系统的基础设计原则和架构模式，围绕可靠性、卓越性、可进化性、自主性和保障五大支柱，结合传统软件工程方法与生成式AI的认知能力，提出了GenAI-native cell、organic substrate、programmable router等关键设计模式及软件栈蓝图。结论认为，融合传统与GenAI范式将推动未来软件系统更具弹性、自主进化与高效，尽管仍需进一步实证研究和技术发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Foundational_Design_Principles_and_Patterns_for_Building_Robust_and_Adaptive_GenAI-Native_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-Exploiting-Vocabulary-Frequency-Imbalance-in-Language-Model-Pre-training"><a href="#75-Exploiting-Vocabulary-Frequency-Imbalance-in-Language-Model-Pre-training" class="headerlink" title="75. Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Exploiting_Vocabulary_Frequency_Imbalance_in_Language_Model_Pre-training.pdf">Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KAIST</span></p><p>本论文系统研究了在语言模型预训练中扩展词表规模（24K至196K）对模型损失与表现的影响，通过Kolmogorov复杂度、分词效率和词频分布等分析，发现更大词表主要通过加剧高频词的频率不均衡、降低对高频词的不确定性，从而减少整体交叉熵损失和提升下游任务准确率。结论表明，模型损失和下游表现的提升主要来自于高频词的不确定性降低，进一步扩展词表对稀有词收益有限。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Exploiting_Vocabulary_Frequency_Imbalance_in_Language_Model_Pre-training.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-An-Empirical-Study-on-How-Video-LLMs-Answer-Video-Questions"><a href="#76-An-Empirical-Study-on-How-Video-LLMs-Answer-Video-Questions" class="headerlink" title="76. An Empirical Study on How Video-LLMs Answer Video Questions"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/An_Empirical_Study_on_How_Video-LLMs_Answer_Video_Questions.pdf">An Empirical Study on How Video-LLMs Answer Video Questions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Monash University</span></p><p>本文系统性分析了 Video-LLMs 在视频问答任务中的内部工作机制，提出并应用三类注意力剔除（时序、空间、语言到视频）方法，分别在全局和细粒度层面剖析了不同注意力类型和模型层对视频理解的影响。结论显示，视频信息主要在早期层被提取，高层负责推理，且模型对语言引导的视频检索依赖远大于时空自注意力，提出的精简策略可有效降低计算量且几乎不损失性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/An_Empirical_Study_on_How_Video-LLMs_Answer_Video_Questions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-RETAIL-Towards-Real-world-Travel-Planning-for-Large-Language-Models"><a href="#77-RETAIL-Towards-Real-world-Travel-Planning-for-Large-Language-Models" class="headerlink" title="77. RETAIL: Towards Real-world Travel Planning for Large Language Models"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/RETAIL__Towards_Real-world_Travel_Planning_for_Large_Language_Models.pdf">RETAIL: Towards Real-world Travel Planning for Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>该论文提出了RETAIL数据集，专注于真实场景下的旅游规划，覆盖隐式与显式用户需求、环境感知与计划修订，并包含丰富POI信息，提升规划可行性。基于此，作者提出了TGMA主题引导多智能体框架，通过决策支持和多智能体协作体系，有效提升了大模型在真实旅游规划中的表现，实验结果显著优于现有基线。结论表明，TGMA框架和RETAIL数据集为大模型在复杂实际规划任务中提供了新的思路和资源。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/RETAIL_Towards_Real-world_Travel_Planning_for_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-IPIGuard-A-Novel-Tool-Dependency-Graph-Based-Defense-Against-Indirect-Prompt-Injection-in-LLM-Agents"><a href="#78-IPIGuard-A-Novel-Tool-Dependency-Graph-Based-Defense-Against-Indirect-Prompt-Injection-in-LLM-Agents" class="headerlink" title="78. IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/IPIGuard__A_Novel_Tool_Dependency_Graph-Based_Defense_Against_Indirect_Prompt_Injection_in_LLM_Agent.pdf">IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出了IPIGUARD，一种以工具依赖图（Tool Dependency Graph, TDG）为核心的任务执行范式，通过将LLM代理的动作规划与外部数据交互解耦，显式约束工具调用行为，有效防御间接提示注入（IPI）攻击。方法包括TDG构建、参数估算、节点扩展和虚假工具调用机制，实现了更强的安全性与较高的任务完成率。实验表明，IPIGUARD在多个主流LLM和多种攻击场景下均显著降低攻击成功率且保持高效用。结论：IPIGUARD显著提升了LLM代理系统对IPI攻击的鲁棒性，兼顾安全性与实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/IPIGuard_A_Novel_Tool_Dependency_Graph-Based_Defense_Against_Indirect_Prompt_Injection_in_LLM_Agents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-EXBIGBANG-A-Dynamic-Approach-for-Explainable-Persona-Classification-Through-Contextualized-Hybrid-Transformer-Analysis"><a href="#79-EXBIGBANG-A-Dynamic-Approach-for-Explainable-Persona-Classification-Through-Contextualized-Hybrid-Transformer-Analysis" class="headerlink" title="79. EXBIGBANG: A Dynamic Approach for Explainable Persona Classification Through Contextualized Hybrid Transformer Analysis"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/ExBigBang__A_Dynamic_Approach_for_Explainable_Persona_Classification_through_Contextualized_Hybrid_T.pdf">EXBIGBANG: A Dynamic Approach for Explainable Persona Classification Through Contextualized Hybrid Transformer Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Macquarie University</span></p><p>本文提出了ExBigBang框架，利用混合文本-表格Transformer（BERT为核心）结合领域知识、元数据和用户画像，实现动态、可解释的用户Persona分类。通过循环反馈的用户画像与分类流程，模型能反映用户行为变化，并借助XAI（如SHAP）揭示特征贡献。结果表明，该方法在社交媒体数据上性能显著优于传统与深度学习基线，且对特征影响机制进行了可解释性分析。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/EXBIGBANG_A_Dynamic_Approach_for_Explainable_Persona_Classification_Through_Contextualized_Hybrid_Transformer_Analysis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-DiagECG-An-LLM-Driven-Framework-for-Diagnostic-Reasoning-via-Discretized-ECG-Tokenization"><a href="#80-DiagECG-An-LLM-Driven-Framework-for-Diagnostic-Reasoning-via-Discretized-ECG-Tokenization" class="headerlink" title="80. DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/DiagECG__An_LLM-Driven_Framework_for_Diagnostic_Reasoning_via_Discretized_ECG_Tokenization.pdf">DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences</span></p><p>DiagECG提出了一种将12导联ECG信号离散化为符号token，并将其集成到大语言模型（LLM）词汇表的方法，通过lead-wise编码器和FSQ量化模块，将ECG连续信号转化为可供LLM处理的离散符号，支持统一的时序与文本建模。方法包括自动回归式ECG预训练和轻量化指令微调（LoRA），实现了强大的开放式问答和诊断报告生成能力。实验结果显示，DiagECG在多项ECG问答和报告生成任务上达到SOTA性能，具备良好的跨数据集泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/DiagECG_An_LLM-Driven_Framework_for_Diagnostic_Reasoning_via_Discretized_ECG_Tokenization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-Coarse-to-Fine-Grounded-Memory-for-LLM-Agent-Planning"><a href="#81-Coarse-to-Fine-Grounded-Memory-for-LLM-Agent-Planning" class="headerlink" title="81. Coarse-to-Fine Grounded Memory for LLM Agent Planning"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Coarse-to-Fine_Grounded_Memory_for_LLM_Agent_Planning.pdf">Coarse-to-Fine Grounded Memory for LLM Agent Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institution of Automation, Chinese Academy of Sciences</span></p><p>该论文提出了Coarse-to-Fine Grounded Memory (CFGM) 框架，通过引入粗到细粒度的记忆机制，利用LLM的内在知识对环境信息、经验轨迹和当前状态进行分层知识萃取和规划辅助。在AlfWorld、WebShop和ScienceWorld等复杂交互环境中，CFGM显著提升了LLM Agent的任务规划能力和泛化性，实验结果显示其在多个基准上均优于现有增强记忆的LLM agent系统。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Coarse-to-Fine_Grounded_Memory_for_LLM_Agent_Planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-Deep-Think-with-Confidence"><a href="#82-Deep-Think-with-Confidence" class="headerlink" title="82. Deep Think with Confidence"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Deep_Think_with_Confidence.pdf">Deep Think with Confidence</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Meta AI</span></p><p>该论文提出了Deep Think with Confidence (DeepConf)，一种通过利用大语言模型内部置信度信号，在推理生成过程中动态过滤低质量推理路径的方法。方法无需额外训练或超参数调整，能在离线和在线场景下通过局部置信度度量实现早停和置信度加权投票，有效减少计算开销并提升推理准确率。实验表明，DeepConf在AIME 2025等高难度推理基准上最高可达99.9%准确率，并显著减少84.7%的token生成。结论是DeepConf能以更高效方式提升LLM推理性能，并易于集成现有服务框架。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Deep_Think_with_Confidence.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-SPARK-Query-Aware-Unstructured-Sparsity-with-Recoverable-KV-Cache-Channel-Pruning"><a href="#83-SPARK-Query-Aware-Unstructured-Sparsity-with-Recoverable-KV-Cache-Channel-Pruning" class="headerlink" title="83. SPARK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/SparK__Query-Aware_Unstructured_Sparsity_with_Recoverable_KV_Cache_Channel_Pruning.pdf">SPARK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Advanced Micro Devices (china) Co., Ltd.</span></p><p>该论文提出了SPARK方法，通过无需训练的查询感知无结构稀疏性，对LLM推理中的KV缓存进行细粒度通道剪枝，并引入统计恢复机制在注意力计算时动态重建被剪枝的通道。实验表明，SPARK在高压缩比下能大幅降低KV缓存内存消耗，同时保持模型性能损失低于5%，优于现有结构化通道剪枝方法，兼容主流的缓存压缩及量化技术。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/SPARK_Query-Aware_Unstructured_Sparsity_with_Recoverable_KV_Cache_Channel_Pruning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-R-ConstraintBench-Evaluating-LLMs-on-NP-Complete-Scheduling"><a href="#84-R-ConstraintBench-Evaluating-LLMs-on-NP-Complete-Scheduling" class="headerlink" title="84. R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/R-ConstraintBench__Evaluating_LLMs_on_NP-Complete_Scheduling.pdf">R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Labelbox</span></p><p>本文提出R-ConstraintBench，一个可扩展的基准框架，系统评估大语言模型（LLM）在NP完全资源约束项目调度问题（RCPSP）上的推理能力。方法包括多阶段有向无环图（DAG）生成、逐步增加约束（资源停机、时间窗口、互斥），并在现实数据中心迁移场景中测试多种LLM，通过可行性和错误分析发现：模型在仅有前序约束时表现接近满分，但当多种约束互相作用时，可行性骤降，主要瓶颈在约束交互而非图的深度；且在合成数据上表现优异的模型在实际领域迁移时性能会明显下降，显示出有限的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/R-ConstraintBench_Evaluating_LLMs_on_NP-Complete_Scheduling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-SemToken-Semantic-Aware-Tokenization-for-Efficient-Long-Context-Language-Modeling"><a href="#85-SemToken-Semantic-Aware-Tokenization-for-Efficient-Long-Context-Language-Modeling" class="headerlink" title="85. SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/SemToken__Semantic-Aware_Tokenization_for_Efficient_Long-Context_Language_Modeling.pdf">SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yale University</span></p><p>SemToken提出了一种基于语义的动态分词框架，通过语义嵌入、局部聚类以及自适应粒度分配，减少冗余token并提升长上下文大语言模型的推理效率。实验表明，该方法在保持甚至提升下游任务表现的同时，实现了高达2.4倍的token压缩和1.9倍推理加速，兼容现有加速方法且无需模型重训练。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/SemToken_Semantic-Aware_Tokenization_for_Efficient_Long-Context_Language_Modeling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-Multiple-Memory-Systems-for-Enhancing-the-Long-term-Memory-of-Agent"><a href="#86-Multiple-Memory-Systems-for-Enhancing-the-Long-term-Memory-of-Agent" class="headerlink" title="86. Multiple Memory Systems for Enhancing the Long-term Memory of Agent"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Multiple_Memory_Systems_for_Enhancing_the_Long-term_Memory_of_Agent.pdf">Multiple Memory Systems for Enhancing the Long-term Memory of Agent</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tianjin University</span></p><p>该论文提出了多重记忆系统（MMS），结合认知心理学理论，将短期记忆内容处理为高质量的长期记忆片段，包括关键词、认知视角、情节记忆和语义记忆，并构建了检索记忆单元与情境记忆单元。实验结果显示，MMS在LoCoMo数据集的多种任务上显著提升了长程记忆的召回率和生成质量，同时具有更低的资源消耗和更强的鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Multiple_Memory_Systems_for_Enhancing_the_Long-term_Memory_of_Agent.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-M-𝐿𝐿𝑀3REC-A-Motivation-Aware-User-Item-Interaction-Framework-for-Enhancing-Recommendation-Accuracy-with-LLMs"><a href="#87-M-𝐿𝐿𝑀3REC-A-Motivation-Aware-User-Item-Interaction-Framework-for-Enhancing-Recommendation-Accuracy-with-LLMs" class="headerlink" title="87. M-𝐿𝐿𝑀3REC: A Motivation-Aware User-Item Interaction Framework for Enhancing Recommendation Accuracy with LLMs"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/M-$LLM^3$REC__A_Motivation-Aware_User-Item_Interaction_Framework_for_Enhancing_Recommendation_Accura.pdf">M-𝐿𝐿𝑀3REC: A Motivation-Aware User-Item Interaction Framework for Enhancing Recommendation Accuracy with LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Sydney</span></p><p>该论文提出了M-𝐿𝐿𝑀3REC推荐框架，利用大语言模型（LLM）通过三个模块（MOPE用户动机提取、MOTE商品特征编码、MAR动机-特征匹配）深度建模用户动机与商品语义，实现冷启动和数据稀疏场景下的高准确个性化推荐。实验表明，该方法在多个领域数据集上显著优于现有主流模型，提升了推荐的泛化能力和解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/M-%F0%9D%90%BF%F0%9D%90%BF%F0%9D%91%803REC_A_Motivation-Aware_User-Item_Interaction_Framework_for_Enhancing_Recommendation_Accuracy_with_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-Conflict-Aware-Soft-Prompting-for-Retrieval-Augmented-Generation"><a href="#88-Conflict-Aware-Soft-Prompting-for-Retrieval-Augmented-Generation" class="headerlink" title="88. Conflict-Aware Soft Prompting for Retrieval-Augmented Generation"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Conflict-Aware_Soft_Prompting_for_Retrieval-Augmented_Generation.pdf">Conflict-Aware Soft Prompting for Retrieval-Augmented Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sungkyunkwan University</span></p><p>该论文提出了CARE方法，通过引入context assessor模块和软提示（soft prompting）来解决检索增强生成（RAG）中外部检索内容与LLM内部知识冲突问题。方法包括重构预训练和冲突感知微调，以动态生成反映可靠性的记忆嵌入，有效引导LLM在外部或内部知识间做出合理推理。实验表明，CARE在问答和事实核查任务上提升了平均性能5%，并能在不微调基础LLM的前提下保持其泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Conflict-Aware_Soft_Prompting_for_Retrieval-Augmented_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-LLM4Sweat-A-Trustworthy-Large-Language-Model-for-Hyperhidrosis-Support"><a href="#89-LLM4Sweat-A-Trustworthy-Large-Language-Model-for-Hyperhidrosis-Support" class="headerlink" title="89. LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/LLM4Sweat__A_Trustworthy_Large_Language_Model_for_Hyperhidrosis_Support.pdf">LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Purdue University</span></p><p>该论文提出LLM4Sweat框架，通过三阶段流程（数据增强、领域微调、专家闭环评估）将开源大模型适应于多任务（诊断、治疗建议、心理支持）罕见病超汗症，利用前沿LLM生成医学合理的合成问答数据，专家反馈进一步提升模型表现。实验结果显示LLM4Sweat在诊断和治疗任务上显著优于基线模型，兼具准确性和临床可信度，具备在其他罕见病领域扩展的通用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/LLM4Sweat_A_Trustworthy_Large_Language_Model_for_Hyperhidrosis_Support.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-SafeLLM-Unlearning-Harmful-Outputs-from-Large-Language-Models-against-Jailbreak-Attacks"><a href="#90-SafeLLM-Unlearning-Harmful-Outputs-from-Large-Language-Models-against-Jailbreak-Attacks" class="headerlink" title="90. SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/SafeLLM__Unlearning_Harmful_Outputs_from_Large_Language_Models_against_Jailbreak_Attacks.pdf">SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Queen’s University</span></p><p>本论文提出SafeLLM，一种基于unlearning的防御框架，通过三阶段流程：动态有害输出检测（结合外部分类器与模型自评）、基于FFN激活的有害内容追踪、受约束优化抑制有害行为，实现LLM对有害知识的精准定位与不可逆遗忘，同时保持语言流畅和通用能力。实验表明，SafeLLM在多种主流LLM与越狱攻击基准下显著降低攻击成功率，优于常规SFT与DPO等防御方法，并对未见攻击具备更强鲁棒性，且对通用任务无明显性能损失，展示了token级unlearning在LLM安全中的应用前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/SafeLLM_Unlearning_Harmful_Outputs_from_Large_Language_Models_against_Jailbreak_Attacks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-PuzzleClone-An-SMT-Powered-Framework-for-Synthesizing-Verifiable-Data"><a href="#91-PuzzleClone-An-SMT-Powered-Framework-for-Synthesizing-Verifiable-Data" class="headerlink" title="91. PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/PuzzleClone__An_SMT-Powered_Framework_for_Synthesizing_Verifiable_Data.pdf">PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">HiThink Research</span></p><p>本论文提出了PuzzleClone，一个利用可满足性模理论（SMT）自动合成大规模、可验证逻辑与数学推理数据的框架。方法包括将种子谜题编码为结构化逻辑规范，通过变量和约束系统化随机化生成多样变体并用SMT求解器验证答案，最终构建出覆盖不同难度的8.3万+谜题数据集。实验表明，PuzzleClone不仅对主流大语言模型提出挑战，且其数据用于SFT和RL微调后，显著提升了模型在逻辑与数学推理基准上的性能，验证了框架的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/PuzzleClone_An_SMT-Powered_Framework_for_Synthesizing_Verifiable_Data.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-XDR-LVLM-An-Explainable-Vision-Language-Large-Model-for-Diabetic-Retinopathy-Diagnosis"><a href="#92-XDR-LVLM-An-Explainable-Vision-Language-Large-Model-for-Diabetic-Retinopathy-Diagnosis" class="headerlink" title="92. XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/XDR-LVLM__An_Explainable_Vision-Language_Large_Model_for_Diabetic_Retinopathy_Diagnosis.pdf">XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">SANNO University</span></p><p>本文提出了一种面向糖尿病视网膜病变（DR）诊断的解释型视觉-语言大模型XDR-LVLM，结合医学视觉编码器、LVLM Core、多任务提示工程和多阶段微调，能够对眼底图像进行精确诊断，并生成包含病变解释的自然语言报告。实验结果在DDR数据集上实现了84.55%的Balanced Accuracy和79.92%的F1分数，报告在流畅性、准确性和临床实用性方面均获得高分，显著提升了模型的可解释性与临床应用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/XDR-LVLM_An_Explainable_Vision-Language_Large_Model_for_Diabetic_Retinopathy_Diagnosis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-A-Robust-BERT-Based-Deep-Learning-Model-for-Automated-Cancer-Type-Extraction-from-Unstructured-Pathology-Reports"><a href="#93-A-Robust-BERT-Based-Deep-Learning-Model-for-Automated-Cancer-Type-Extraction-from-Unstructured-Pathology-Reports" class="headerlink" title="93. A Robust BERT-Based Deep Learning Model for Automated Cancer Type Extraction from Unstructured Pathology Reports"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/A_Robust_BERT-Based_Deep_Learning_Model_for_Automated_Cancer_Type_Extraction_from_Unstructured_Patho.pdf">A Robust BERT-Based Deep Learning Model for Automated Cancer Type Extraction from Unstructured Pathology Reports</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of New South Wales</span></p><p>该论文提出了一种基于RoBERTa的深度学习模型，通过OCR与领域特定微调，实现了从未结构化病理报告中自动提取癌症类型和亚型。结果显示，微调后的RoBERTa模型在准确率和F1-BERT分数方面均显著优于通用RoBERTa和Mistral 7B模型，验证了领域适应与微调在医学文本信息抽取中的必要性和有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/A_Robust_BERT-Based_Deep_Learning_Model_for_Automated_Cancer_Type_Extraction_from_Unstructured_Pathology_Reports.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="94-Discovering-Hidden-Algebraic-Structures-via-Transformers-with-Rank-Aware-Beam-GRPO"><a href="#94-Discovering-Hidden-Algebraic-Structures-via-Transformers-with-Rank-Aware-Beam-GRPO" class="headerlink" title="94. Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Discovering_Hidden_Algebraic_Structures_via_Transformers_with_Rank-Aware_Beam_GRPO.pdf">Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">California Institute of Technology</span></p><p>本文提出了一种结合合成数据生成、轻量级Transformer训练、束搜索和Beam Grouped Relative Policy Optimization (BGRPO)强化学习的方法，用于解决多元多项式分解这一NP难题。实验表明该方法在精度和推理计算效率上均有显著提升，且模型在多项式简化任务中部分超过Mathematica，展示了神经网络发现隐藏代数结构的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Discovering_Hidden_Algebraic_Structures_via_Transformers_with_Rank-Aware_Beam_GRPO.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-Tutorial-on-the-Probabilistic-Unification-of-Estimation-Theory-Machine-Learning-and-Generative-AI"><a href="#95-Tutorial-on-the-Probabilistic-Unification-of-Estimation-Theory-Machine-Learning-and-Generative-AI" class="headerlink" title="95. Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Tutorial_on_the_Probabilistic_Unification_of_Estimation_Theory,_Machine_Learning,_and_Generative_AI.pdf">Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Vaasa</span></p><p>该论文提出了一个统一的概率框架，将经典估计理论、统计推断与现代机器学习（包括深度学习与大语言模型）连接起来。通过分析最大似然估计、贝叶斯推断和注意力机制等技术在处理不确定性中的作用，论文展示了各种AI方法均源自共同的概率原理，并通过图像分类和语言生成等示例证明了概率视角对理解AI模型的有效性。结论指出：无论是估计隐藏因果、学习输入输出映射还是建模语言结构，均可归于概率推断，未来AI发展应关注数学基础与可扩展学习系统的结合。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Tutorial_on_the_Probabilistic_Unification_of_Estimation_Theory_Machine_Learning_and_Generative_AI.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-Foundation-Models-for-Cross-Domain-EEG-Analysis-Application-A-Survey"><a href="#96-Foundation-Models-for-Cross-Domain-EEG-Analysis-Application-A-Survey" class="headerlink" title="96. Foundation Models for Cross-Domain EEG Analysis Application: A Survey"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Foundation_Models_for_Cross-Domain_EEG_Analysis_Application__A_Survey.pdf">Foundation Models for Cross-Domain EEG Analysis Application: A Survey</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northwestern Polytechnical University</span></p><p>本文首次系统性梳理了预训练于大规模非EEG数据的基础模型在EEG分析中的跨领域应用，提出了基于输出模态的分类法，涵盖EEG原生解码、EEG-文本、EEG-视觉、EEG-音频及多模态融合五大类任务。结论指出，基础模型显著提升了EEG解码的泛化能力和跨模态表现，但在可解释性、跨主体泛化和实际部署等方面仍面临挑战，未来需加强神经语义对齐与多模态融合。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Foundation_Models_for_Cross-Domain_EEG_Analysis_Application_A_Survey.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-Effect-Identification-and-Unit-Categorization-in-the-Multi-Score-Regression-Discontinuity-Design-with-Application-to-LED-Manufacturing"><a href="#97-Effect-Identification-and-Unit-Categorization-in-the-Multi-Score-Regression-Discontinuity-Design-with-Application-to-LED-Manufacturing" class="headerlink" title="97. Effect Identification and Unit Categorization in the Multi-Score Regression Discontinuity Design with Application to LED Manufacturing"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Effect_Identification_and_Unit_Categorization_in_the_Multi-Score_Regression_Discontinuity_Design_wit.pdf">Effect Identification and Unit Categorization in the Multi-Score Regression Discontinuity Design with Application to LED Manufacturing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AMS Osram</span></p><p>本文提出了一套多评分回归断点设计（MRD）的理论工具，针对多阈值决策环境下单位行为类型（如complier、nevertaker、alwaystaker、defier、indecisive）进行了系统定义与归类，推导了多维切点下complier效应的可识别性条件，并提出通过分解复杂规则、去除非合规单位提升因果效应估计准确率。理论框架在LED制造真实数据和模拟数据上验证，表明该方法能有效降低估计方差，为制造决策优化提供支持。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Effect_Identification_and_Unit_Categorization_in_the_Multi-Score_Regression_Discontinuity_Design_with_Application_to_LED_Manufacturing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-Amortized-In-Context-Mixed-Effect-Transformer-Models-A-Zero-Shot-Approach-for-Pharmacokinetics"><a href="#98-Amortized-In-Context-Mixed-Effect-Transformer-Models-A-Zero-Shot-Approach-for-Pharmacokinetics" class="headerlink" title="98. Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot Approach for Pharmacokinetics"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Amortized_In-Context_Mixed_Effect_Transformer_Models__A_Zero-Shot_Approach_for_Pharmacokinetics.pdf">Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot Approach for Pharmacokinetics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Potsdam</span></p><p>本论文提出了AICMET模型，将分层潜变量Transformer与药动学分腔体机理先验结合，通过大规模合成数据预训练，实现基于Transformer的零样本个体预测与群体生成。结果显示AICMET在药物浓度预测和个体变异量化方面优于传统NLME和神经ODE方法，实现更高预测准确率并大幅缩短模型开发周期。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Amortized_In-Context_Mixed_Effect_Transformer_Models_A_Zero-Shot_Approach_for_Pharmacokinetics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-CITE-A-Comprehensive-Benchmark-for-Heterogeneous-Text-Attributed-Graphs-on-Catalytic-Materials"><a href="#99-CITE-A-Comprehensive-Benchmark-for-Heterogeneous-Text-Attributed-Graphs-on-Catalytic-Materials" class="headerlink" title="99. CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on Catalytic Materials"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/CITE__A_Comprehensive_Benchmark_for_Heterogeneous_Text-Attributed_Graphs_on_Catalytic_Materials.pdf">CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on Catalytic Materials</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Computer Network Information Center, China Academy of Sciences</span></p><p>该论文提出了CITE数据集，这是首个聚焦催化材料领域的异构文本属性图大规模基准，包含四类节点（论文、作者、期刊、关键词）和丰富文本属性，支持复杂关系建模。论文系统比较了同质图模型、异质图模型、LLM和LLM+图模型在节点分类等任务上的表现，发现异质图模型在结构复杂和语义丰富场景下表现更强，LLM相关方法在多标签一致性上存在挑战，并强调高连接节点和丰富语义对图表示学习的重要性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/CITE_A_Comprehensive_Benchmark_for_Heterogeneous_Text-Attributed_Graphs_on_Catalytic_Materials.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-EvoFormer-Learning-Dynamic-Graph-Level-Representations-with-Structural-and-Temporal-Bias-Correction"><a href="#100-EvoFormer-Learning-Dynamic-Graph-Level-Representations-with-Structural-and-Temporal-Bias-Correction" class="headerlink" title="100. EvoFormer: Learning Dynamic Graph-Level Representations with Structural and Temporal Bias Correction"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/EvoFormer__Learning_Dynamic_Graph-Level_Representations_with_Structural_and_Temporal_Bias_Correction.pdf">EvoFormer: Learning Dynamic Graph-Level Representations with Structural and Temporal Bias Correction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xidian University</span></p><p>本文提出EvoFormer，一种结合结构感知Transformer模块和演化敏感时序模块的动态图级表示学习方法。该方法利用结构角色位置编码和图级时序分段，解决了随机游走采样导致的结构访问偏差和突发结构变化感知不足的问题。实验结果表明，EvoFormer在图相似性排名、时序异常检测和时序分割等任务上实现了最新最优性能，有效提升了动态图结构和演化特征的表达能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/EvoFormer_Learning_Dynamic_Graph-Level_Representations_with_Structural_and_Temporal_Bias_Correction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="101-GRASPED-Graph-Anomaly-Detection-using-Autoencoder-with-Spectral-Encoder-and-Decoder"><a href="#101-GRASPED-Graph-Anomaly-Detection-using-Autoencoder-with-Spectral-Encoder-and-Decoder" class="headerlink" title="101. GRASPED: Graph Anomaly Detection using Autoencoder with Spectral Encoder and Decoder"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/GRASPED__Graph_Anomaly_Detection_using_Autoencoder_with_Spectral_Encoder_and_Decoder_(Full_Version).pdf">GRASPED: Graph Anomaly Detection using Autoencoder with Spectral Encoder and Decoder</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fraunhofer Institute for Applied and Integrated Security (AISEC)</span></p><p>该论文提出了GRASPED，一种基于图自编码器（GAE）的无监督异常检测模型，方法使用图小波卷积构建的多尺度谱域编码器与图反卷积网络（GDN）解码器，实现节点属性与结构信息的多分辨率捕获和高效重构。实验结果显示，GRASPED在多个真实数据集上的节点异常检测性能优于当前主流模型，并表现出对参数和不同异常类型的鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/GRASPED_Graph_Anomaly_Detection_using_Autoencoder_with_Spectral_Encoder_and_Decoder.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-Bayesian-Inference-and-Learning-in-Nonlinear-Dynamical-Systems-A-Framework-for-Incorporating-Explicit-and-Implicit-Prior-Knowledge"><a href="#102-Bayesian-Inference-and-Learning-in-Nonlinear-Dynamical-Systems-A-Framework-for-Incorporating-Explicit-and-Implicit-Prior-Knowledge" class="headerlink" title="102. Bayesian Inference and Learning in Nonlinear Dynamical Systems: A Framework for Incorporating Explicit and Implicit Prior Knowledge"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Bayesian_Inference_and_Learning_in_Nonlinear_Dynamical_Systems__A_Framework_for_Incorporating_Explic.pdf">Bayesian Inference and Learning in Nonlinear Dynamical Systems: A Framework for Incorporating Explicit and Implicit Prior Knowledge</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Leibniz Universität Hannover</span></p><p>该论文提出了一个通用的系统辨识工具，用于在非线性动力学系统中融合显式（如已知物理方程）和隐式（如平滑或对称性假设）先验知识，实现对未知模型部分的贝叶斯推断与学习。方法核心包括：基于可交换接口变量和灵活基函数展开的建模结构，通过封闭形式的参数边缘化推导，结合序贯蒙特卡洛（SMC）与粒子MCMC，实现在线和离线场景下的高效推断与学习，避免模型反演或人工变换。实验在仿真与真实系统上验证了方法的适用性与数据高效性，能在有限数据下准确学习复杂系统的未知动力学。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Bayesian_Inference_and_Learning_in_Nonlinear_Dynamical_Systems_A_Framework_for_Incorporating_Explicit_and_Implicit_Prior_Knowledge.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-Evaluating-Knowledge-Graph-Complexity-via-Semantic-Spectral-and-Structural-Metrics-for-Link-Prediction"><a href="#103-Evaluating-Knowledge-Graph-Complexity-via-Semantic-Spectral-and-Structural-Metrics-for-Link-Prediction" class="headerlink" title="103. Evaluating Knowledge Graph Complexity via Semantic, Spectral, and Structural Metrics for Link Prediction"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Evaluating_Knowledge_Graph_Complexity_via_Semantic,_Spectral,_and_Structural_Metrics_for_Link_Predic.pdf">Evaluating Knowledge Graph Complexity via Semantic, Spectral, and Structural Metrics for Link Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universiti Brunei Darussalam</span></p><p>本文提出了一个综合框架，通过语义、谱和结构指标系统性评估知识图谱（KG）复杂性，并分析这些指标在链路预测任务中的表现。研究发现，谱指标CSG在KG环境下对参数高度敏感且与模型性能相关性弱，而语义和结构指标（如关系熵、最大关系多样性、平均度等）能更稳健地反映任务难度。结论是，KG链路预测中应优先采用语义和结构复杂性指标而非谱分离指标。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Evaluating_Knowledge_Graph_Complexity_via_Semantic_Spectral_and_Structural_Metrics_for_Link_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-Learning-ECG-Representations-via-Poly-Window-Contrastive-Learning"><a href="#104-Learning-ECG-Representations-via-Poly-Window-Contrastive-Learning" class="headerlink" title="104. Learning ECG Representations via Poly-Window Contrastive Learning"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Learning_ECG_Representations_via_Poly-Window_Contrastive_Learning.pdf">Learning ECG Representations via Poly-Window Contrastive Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Emory University</span></p><p>该论文提出了一种多窗口对比学习（Poly-Window Contrastive Learning）框架，通过从每条ECG信号中提取多个时间窗口，利用统计方式聚合正样本对，实现对心电信号的稳定生理特征表征学习。实验在PTB-XL数据集上表明，该方法在多标签分类性能（AUROC和F1）上超越传统双视图方法，同时训练收敛速度提高至原来的四分之一，计算效率提升14.8%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Learning_ECG_Representations_via_Poly-Window_Contrastive_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-Frequency-adaptive-tensor-neural-networks-for-high-dimensional-multi-scale-problems"><a href="#105-Frequency-adaptive-tensor-neural-networks-for-high-dimensional-multi-scale-problems" class="headerlink" title="105. Frequency-adaptive tensor neural networks for high-dimensional multi-scale problems"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Frequency-adaptive_tensor_neural_networks_for_high-dimensional_multi-scale_problems.pdf">Frequency-adaptive tensor neural networks for high-dimensional multi-scale problems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Academy of Mathematics and Systems Science, Chinese Academy of Sciences</span></p><p>本文提出了频率自适应张量神经网络（TNNs）方法，结合张量分解和离散傅里叶变换（DFT），通过分析TNNs的频率原理（频谱偏置），设计了可自适应调节频率特征的TNNs算法。此方法有效提升了TNNs在高维多尺度 PDE 问题中的高频特征表达能力，大幅降低了解的L2误差。结论表明，频率自适应TNNs在多种高维多尺度方程（如泊松方程、热方程、波方程、Helmholtz方程）上均表现出高效、鲁棒的逼近能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Frequency-adaptive_tensor_neural_networks_for_high-dimensional_multi-scale_problems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-MMQ-Multimodal-Mixture-of-Quantization-Tokenization-for-Semantic-ID-Generation-and-User-Behavioral-Adaptation"><a href="#106-MMQ-Multimodal-Mixture-of-Quantization-Tokenization-for-Semantic-ID-Generation-and-User-Behavioral-Adaptation" class="headerlink" title="106. MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/MMQ__Multimodal_Mixture-of-Quantization_Tokenization_for_Semantic_ID_Generation_and_User_Behavioral_.pdf">MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Alibaba Group</span></p><p>该论文提出MMQ框架，通过多专家结构结合模态共享与模态特异专家训练，实现多模态内容（文本和图像）到语义ID的高效离散化，并以正交约束提升表示多样性。第二阶段采用行为感知微调，使语义ID动态适应推荐目标，显著提升推荐系统在检索和排序任务中的表现，尤其在长尾物品推荐和工业级场景中有较大优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/MMQ_Multimodal_Mixture-of-Quantization_Tokenization_for_Semantic_ID_Generation_and_User_Behavioral_Adaptation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="107-SleepDIFFormer-Sleep-Stage-Classification-via-Multivariate-Differential-Transformer"><a href="#107-SleepDIFFormer-Sleep-Stage-Classification-via-Multivariate-Differential-Transformer" class="headerlink" title="107. SleepDIFFormer: Sleep Stage Classification via Multivariate Differential Transformer"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/SleepDIFFormer__Sleep_Stage_Classification_via_Multivariate_Differential_Transformer.pdf">SleepDIFFormer: Sleep Stage Classification via Multivariate Differential Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xiamen University Malaysia</span></p><p>SleepDIFFormer提出了一种基于多变量差分Transformer架构（MDTA）的方法，通过联合EEG和EOG信号，采用差分自注意力和跨注意力机制抑制信号与注意力噪声，并结合多层特征对齐实现跨域泛化。实验在五个公开睡眠分期数据集上取得了当前最佳性能，并通过消融分析验证了各模块的贡献，提升了模型在未见数据集上的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/SleepDIFFormer_Sleep_Stage_Classification_via_Multivariate_Differential_Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025/08/22报纸</title>
    <link href="/2025/08/2025-08-22_article/"/>
    <url>/2025/08/2025-08-22_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2025/08/22报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-22<br>📄 <strong>发现论文数量</strong>：98  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Compute-Optimal-Scaling-for-Value-Based-Deep-RL"><a href="#1-Compute-Optimal-Scaling-for-Value-Based-Deep-RL" class="headerlink" title="1. Compute-Optimal Scaling for Value-Based Deep RL"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Compute-Optimal_Scaling_for_Value-Based_Deep_RL.pdf">Compute-Optimal Scaling for Value-Based Deep RL</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Berkeley</span></p><p>本论文系统研究了在给定计算预算下，如何在在线、基于价值（value-based）的深度强化学习中合理分配模型容量与update-to-data (UTD) 比例，以最大化采样效率。通过大量实证分析，作者提出并解释了TD-overfitting现象，量化了模型容量、UTD比和batch size之间的关系，给出了可预测的计算资源分配法则，并为实际使用者提供了优化采样效率和计算效率的指导规则。结论显示，合理分配模型容量与UTD比可显著提升数据与计算利用率，并且大模型能有效缓解TD-overfitting、支持更大batch size。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Compute-Optimal_Scaling_for_Value-Based_Deep_RL.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-HERAKLES-Hierarchical-Skill-Compilation-for-Open-ended-LLM-Agents"><a href="#2-HERAKLES-Hierarchical-Skill-Compilation-for-Open-ended-LLM-Agents" class="headerlink" title="2. HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/HERAKLES__Hierarchical_Skill_Compilation_for_Open-ended_LLM_Agents.pdf">HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Inria(Flowers)</span></p><p>HERAKLES提出了一种层级强化学习框架，利用大语言模型（LLM）作为高层策略进行目标分解，并将已掌握的复杂目标编译为低层策略的可复用技能。通过在Crafter环境的实验，HERAKLES展现了随目标难度线性扩展、显著提升样本效率，并能强泛化到新目标的能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/HERAKLES_Hierarchical_Skill_Compilation_for_Open-ended_LLM_Agents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Beyond-ReLU-Chebyshev-DQN-for-Enhanced-Deep-Q-Networks"><a href="#3-Beyond-ReLU-Chebyshev-DQN-for-Enhanced-Deep-Q-Networks" class="headerlink" title="3. Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Beyond_ReLU__Chebyshev-DQN_for_Enhanced_Deep_Q-Networks.pdf">Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">K. N. Toosi University of Technology</span></p><p>本文提出了一种创新的Chebyshev-DQN（Ch-DQN）架构，将Chebyshev多项式基底集成到DQN框架中以提升特征表达能力，通过在输入层进行正交多项式特征变换，改善了Q函数的近似效果。实验证明，Ch-DQN在CartPole、MountainCar和Acrobot等三个基准环境中表现优异，尤其在复杂任务上显著提高了样本效率和最终性能，验证了Chebyshev基底在深度强化学习中的有效性与理论优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Beyond_ReLU_Chebyshev-DQN_for_Enhanced_Deep_Q-Networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-A-Comparative-Evaluation-of-Teacher-Guided-Reinforcement-Learning-Techniques-for-Autonomous-Cyber-Operations"><a href="#4-A-Comparative-Evaluation-of-Teacher-Guided-Reinforcement-Learning-Techniques-for-Autonomous-Cyber-Operations" class="headerlink" title="4. A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/A_Comparative_Evaluation_of_Teacher-Guided_Reinforcement_Learning_Techniques_for_Autonomous_Cyber_Op.pdf">A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Royal Military College of Canada</span></p><p>本文在CybORG模拟环境下实现并比较了四种教师引导的强化学习（RL）方法，包括奖励塑形、动作掩码、辅助损失和特征空间修改，均利用预训练RL代理作为教师。实验表明，辅助损失和动作掩码在提升早期训练表现和加快收敛速度方面最有效，而奖励塑形和特征空间修改效果不明显，说明教师引导RL可显著提升自动化网络安全操作中的训练效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/A_Comparative_Evaluation_of_Teacher-Guided_Reinforcement_Learning_Techniques_for_Autonomous_Cyber_Operations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="5-Virtual-Community-An-Open-World-for-Humans-Robots-and-Society"><a href="#5-Virtual-Community-An-Open-World-for-Humans-Robots-and-Society" class="headerlink" title="5. Virtual Community: An Open World for Humans, Robots, and Society"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Virtual_Community__An_Open_World_for_Humans,_Robots,_and_Society.pdf">Virtual Community: An Open World for Humans, Robots, and Society</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">UMass Amherst</span></p><p>本文提出了Virtual Community，一个基于通用物理引擎的开放世界人-机-社会模拟平台，能够自动将真实地理空间数据转换为大规模3D场景与嵌入丰富社会关系的多智能体社区，并统一支持人类虚拟体与多类型机器人协作、互动与物理仿真。平台提出了Community Planning与Community Robot两大多智能体开放世界任务，并通过多种基线方法系统评测，展示了当前多智能体高层规划与低层物理协作的挑战性。结论认为该平台为研究人机共存与具身通用智能提供了新方向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Virtual_Community_An_Open_World_for_Humans_Robots_and_Society.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-Multiscale-Video-Transformers-for-Class-Agnostic-Segmentation-in-Autonomous-Driving"><a href="#6-Multiscale-Video-Transformers-for-Class-Agnostic-Segmentation-in-Autonomous-Driving" class="headerlink" title="6. Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Multiscale_Video_Transformers_for_Class_Agnostic_Segmentation_in_Autonomous_Driving.pdf">Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ontario Tech University</span></p><p>该论文提出了一种多阶段多尺度查询-记忆视频Transformer，用于自动驾驶场景下的类别无关视频分割，仅利用运动线索无需光流输入。方法包含多尺度特征提取、查询-记忆解码器和随机Token Drop机制以提升效率和精度，能在保持高分辨率时减少计算和内存开销。实验表明该方法在DAVIS’16、KITTI和Cityscapes等数据集上显著优于传统多尺度解码器，且具有实时性，适用于安全关键型机器人应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Multiscale_Video_Transformers_for_Class_Agnostic_Segmentation_in_Autonomous_Driving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-EAROL-Environmental-Augmented-Perception-Aware-Planning-and-Robust-Odometry-via-Downward-Mounted-Tilted-LiDAR"><a href="#7-EAROL-Environmental-Augmented-Perception-Aware-Planning-and-Robust-Odometry-via-Downward-Mounted-Tilted-LiDAR" class="headerlink" title="7. EAROL: Environmental Augmented Perception-Aware Planning and Robust Odometry via Downward-Mounted Tilted LiDAR"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/EAROL__Environmental_Augmented_Perception-Aware_Planning_and_Robust_Odometry_via_Downward-Mounted_Ti.pdf">EAROL: Environmental Augmented Perception-Aware Planning and Robust Odometry via Downward-Mounted Tilted LiDAR</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>该论文提出了一种创新的无人机系统EAROL，通过将LiDAR倾斜向下安装，结合基于IESKF的紧耦合LiDAR-IMU里程计和分层轨迹-偏航优化算法，实现了在无顶部特征的极端环境中高精度6自由度定位与高效动态目标跟踪。实验证明，该系统在室内外场景中可将跟踪误差降低81%，提升22%环境感知覆盖，并几乎消除垂直漂移，为无人机灾后搜索救援与自主导航提供了强健解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/EAROL_Environmental_Augmented_Perception-Aware_Planning_and_Robust_Odometry_via_Downward-Mounted_Tilted_LiDAR.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-Adversarial-Generation-and-Collaborative-Evolution-of-Safety-Critical-Scenarios-for-Autonomous-Vehicles"><a href="#8-Adversarial-Generation-and-Collaborative-Evolution-of-Safety-Critical-Scenarios-for-Autonomous-Vehicles" class="headerlink" title="8. Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Adversarial_Generation_and_Collaborative_Evolution_of_Safety-Critical_Scenarios_for_Autonomous_Vehic.pdf">Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>本文提出SCENGE框架，通过结合知识检索增强的大语言模型（LLM）推理与多代理轨迹优化，自动生成安全关键场景用于仿真测试自动驾驶系统。方法包括Meta-Scenario Generation阶段利用LLM和结构化驾驶知识推理出可执行的对抗场景，随后Complex Scenario Evolution通过协同优化背景车辆轨迹，最大化场景复杂性与风险。实验结果显示SCENGE在多种强化学习自动驾驶模型上能发现更多严重碰撞案例，并提升模型鲁棒性，且可迁移到不同仿真平台和真实车辆测试，表明其生成场景真实且具关键风险。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Adversarial_Generation_and_Collaborative_Evolution_of_Safety-Critical_Scenarios_for_Autonomous_Vehicles.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-Action-Constrained-Imitation-Learning"><a href="#9-Action-Constrained-Imitation-Learning" class="headerlink" title="9. Action-Constrained Imitation Learning"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Action-Constrained_Imitation_Learning.pdf">Action-Constrained Imitation Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Yang Ming Chiao Tung University</span></p><p>本文提出了Action-Constrained Imitation Learning (ACIL)问题，并针对由于模仿者受限于动作约束而导致的occupancy measure失配，提出了基于动态时间规整（DTW）距离的动态时间规整模仿学习（DTWIL）方法。该方法通过模型预测控制（MPC）将专家演示轨迹与受限动作下的代理轨迹对齐，生成代理可行的仿真演示数据，并结合下游仿真学习方法进行策略学习。实验结果表明，DTWIL能有效提升机器人控制等多任务下的模仿性能和样本效率，优于多种基线算法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Action-Constrained_Imitation_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-Taming-VR-Teleoperation-and-Learning-from-Demonstration-for-Multi-Task-Bimanual-Table-Service-Manipulation"><a href="#10-Taming-VR-Teleoperation-and-Learning-from-Demonstration-for-Multi-Task-Bimanual-Table-Service-Manipulation" class="headerlink" title="10. Taming VR Teleoperation and Learning from Demonstration for Multi-Task Bimanual Table Service Manipulation"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Taming_VR_Teleoperation_and_Learning_from_Demonstration_for_Multi-Task_Bimanual_Table_Service_Manipu.pdf">Taming VR Teleoperation and Learning from Demonstration for Multi-Task Bimanual Table Service Manipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了一种结合VR远程遥操作与模仿学习的方法，用于多任务双臂餐桌服务机器人操作，包括桌布展开、披萨放置和食品盒开合。方法采用VR设备采集人类操作演示，通过ACT算法进行策略学习，实现高效且可靠的任务完成。实验结果表明，该混合策略在ICRA 2025 WBCD竞赛中获得冠军，兼顾了效率和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Taming_VR_Teleoperation_and_Learning_from_Demonstration_for_Multi-Task_Bimanual_Table_Service_Manipulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-Great-GATsBi-Hybrid-Multimodal-Trajectory-Forecasting-for-Bicycles-using-Anticipation-Mechanism"><a href="#11-Great-GATsBi-Hybrid-Multimodal-Trajectory-Forecasting-for-Bicycles-using-Anticipation-Mechanism" class="headerlink" title="11. Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Great_GATsBi__Hybrid,_Multimodal,_Trajectory_Forecasting_for_Bicycles_using_Anticipation_Mechanism.pdf">Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ETH Zürich</span></p><p>该论文提出了Great GATsBi，一种用于自行车轨迹预测的混合多模态框架，融合了基于物理建模（如车辆运动学和卡尔曼滤波）与基于社会建模（如图注意网络GAT，邻居轨迹感知与衰减机制），实现了对自行车运动中物理和社会交互的系统性建模。实验表明，该模型在短期和长期预测均优于现有方法，显著降低预测偏差和不确定性，提升了自主驾驶和交通安全相关应用的轨迹预测能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Great_GATsBi_Hybrid_Multimodal_Trajectory_Forecasting_for_Bicycles_using_Anticipation_Mechanism.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-FBI-Learning-Dexterous-In-hand-Manipulation-with-Dynamic-Visuotactile-Shortcut-Policy"><a href="#12-FBI-Learning-Dexterous-In-hand-Manipulation-with-Dynamic-Visuotactile-Shortcut-Policy" class="headerlink" title="12. FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/FBI__Learning_Dexterous_In-hand_Manipulation_with_Dynamic_Visuotactile_Shortcut_Policy.pdf">FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了一种新颖的动态视触觉融合模仿学习方法——Flow Before Imitation (FBI)，通过动态建模触觉与物体运动的因果关系，将时序点云的运动流与密集触觉特征结合，并利用Transformer融合模块生成一步扩散策略，实现了无需或可选物理触觉传感器的灵巧手内操作。实验显示，在模拟和真实环境的五项任务中，FBI均显著超越现有基线，提升操作成功率，特别在复杂重定位任务中表现突出，且具备鲁棒性和高实时性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/FBI_Learning_Dexterous_In-hand_Manipulation_with_Dynamic_Visuotactile_Shortcut_Policy.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-DEXTER-LLM-Dynamic-and-Explainable-Coordination-of-Multi-Robot-Systems-in-Unknown-Environments-via-Large-Language-Models"><a href="#13-DEXTER-LLM-Dynamic-and-Explainable-Coordination-of-Multi-Robot-Systems-in-Unknown-Environments-via-Large-Language-Models" class="headerlink" title="13. DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/DEXTER-LLM__Dynamic_and_Explainable_Coordination_of_Multi-Robot_Systems_in_Unknown_Environments_via_.pdf">DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出DEXTER-LLM框架，实现了在未知环境下多机器人系统的动态任务分解与协调。方法包括四大模块：任务理解（自然语言&#x2F;LTL转任务DAG）、基于LLM的多阶段子任务生成、基于优化的子任务分配与调度，以及多级在线自适应与人类验证。实验显示，该方法在动态场景下实现了100%任务完成率，任务效率和计划质量大幅优于现有方法，并有效减少了LLM调用次数。结论：框架在开放环境下具备强泛化性、自适应性和可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/DEXTER-LLM_Dynamic_and_Explainable_Coordination_of_Multi-Robot_Systems_in_Unknown_Environments_via_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-D²-LIO-Enhanced-Optimization-for-LiDAR-IMU-Odometry-Considering-Directional-Degeneracy"><a href="#14-D²-LIO-Enhanced-Optimization-for-LiDAR-IMU-Odometry-Considering-Directional-Degeneracy" class="headerlink" title="14. D²-LIO: Enhanced Optimization for LiDAR-IMU Odometry Considering Directional Degeneracy"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/D$^2$-LIO__Enhanced_Optimization_for_LiDAR-IMU_Odometry_Considering_Directional_Degeneracy.pdf">D²-LIO: Enhanced Optimization for LiDAR-IMU Odometry Considering Directional Degeneracy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>该论文提出了一种增强型LiDAR-IMU里程计(D²-LIO)优化框架，通过自适应逐点异常值剔除和IMU-点云协同加权，实现对方向性退化的鲁棒定位。实验表明，该方法在复杂和退化环境下显著提升了定位精度和鲁棒性，优于主流SLAM方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/D%C2%B2-LIO_Enhanced_Optimization_for_LiDAR-IMU_Odometry_Considering_Directional_Degeneracy.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-MoVieDrive-Multi-Modal-Multi-View-Urban-Scene-Video-Generation"><a href="#15-MoVieDrive-Multi-Modal-Multi-View-Urban-Scene-Video-Generation" class="headerlink" title="15. MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/MoVieDrive__Multi-Modal_Multi-View_Urban_Scene_Video_Generation.pdf">MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Noah’s Ark Lab</span></p><p>MoVieDrive提出了一种用于自动驾驶的多模态多视角城市场景视频生成方法，通过设计统一的扩散Transformer模型，融合模态共享和模态特定组件，实现RGB、深度和语义等多模态视频在统一框架下的生成。实验结果表明，该方法在nuScenes数据集上生成的视频在保真度、可控性及多模态一致性方面均优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/MoVieDrive_Multi-Modal_Multi-View_Urban_Scene_Video_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="16-Locality-aware-Concept-Bottleneck-Model"><a href="#16-Locality-aware-Concept-Bottleneck-Model" class="headerlink" title="16. Locality-aware Concept Bottleneck Model"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Locality-aware_Concept_Bottleneck_Model.pdf">Locality-aware Concept Bottleneck Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University</span></p><p>本文提出了一种新型的Locality-aware Concept Bottleneck Model (LCBM) 框架，通过结合原型学习和CLIP等基础模型，提升了无标签概念瓶颈模型在图像中概念定位的准确性。该方法为每个概念分配原型，并通过辅助分类任务及与CLIP的相似度约束，使原型与局部区域高度对齐，实验结果显示LCBM在多个数据集上提升了概念定位能力且保持了分类性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Locality-aware_Concept_Bottleneck_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-Snap-Snap-Taking-Two-Images-to-Reconstruct-3D-Human-Gaussians-in-Milliseconds"><a href="#17-Snap-Snap-Taking-Two-Images-to-Reconstruct-3D-Human-Gaussians-in-Milliseconds" class="headerlink" title="17. Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Snap-Snap__Taking_Two_Images_to_Reconstruct_3D_Human_Gaussians_in_Milliseconds.pdf">Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huazhong University of Science and Technology</span></p><p>本文提出了一种名为Snap-Snap的前馈式3D人体重建框架，仅需两张RGB图像（正面和背面）即可在190毫秒内直接预测3D人体高斯点云。方法包括重设计的几何点云预测模型、侧视点云及颜色增强算法，并利用高斯属性回归获得完整可渲染的人体高斯点集合。实验显示该方法在THuman2.0和跨域数据集上达到SOTA性能，并支持低成本设备采集。结论：相比现有方法，Snap-Snap在推理速度、简单性和重建质量上具有显著优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Snap-Snap_Taking_Two_Images_to_Reconstruct_3D_Human_Gaussians_in_Milliseconds.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-Squeezed-Diffusion-Models"><a href="#18-Squeezed-Diffusion-Models" class="headerlink" title="18. Squeezed Diffusion Models"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Squeezed_Diffusion_Models.pdf">Squeezed Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University</span></p><p>该论文提出了Squeezed Diffusion Models（SDM），通过在扩散模型训练时基于主成分分析对噪声进行各向异性缩放（即挤压&#x2F;反挤压），并设计了Heisenberg扩散模型和标准SDM两种方案。实验显示，在CIFAR-10&#x2F;100和CelebA-64等图像数据集上，适度反挤压（即主轴方向增加噪声方差）可显著提升生成图像的FID和召回率，表明基于数据结构调整噪声可提升扩散模型的生成质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Squeezed_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-TransLight-Image-Guided-Customized-Lighting-Control-with-Generative-Decoupling"><a href="#19-TransLight-Image-Guided-Customized-Lighting-Control-with-Generative-Decoupling" class="headerlink" title="19. TransLight: Image-Guided Customized Lighting Control with Generative Decoupling"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/TransLight__Image-Guided_Customized_Lighting_Control_with_Generative_Decoupling.pdf">TransLight: Image-Guided Customized Lighting Control with Generative Decoupling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of EIC, Huazhong University of Science and Technology</span></p><p>本文提出TransLight，一种首创的图像引导光效转移方法，通过创新的生成式解耦策略，利用两个微调扩散模型，将参考图像中的内容与光效分离，构建百万级图像-内容-光效三元组数据集，并以此训练TransLight。该方法实现了高保真、高自由度的光效转移，支持位置、方向、强度等属性灵活控制，实验结果在光效转移和图像内容保持方面均优于现有技术。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/TransLight_Image-Guided_Customized_Lighting_Control_with_Generative_Decoupling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-DINOv3-with-Test-Time-Training-for-Medical-Image-Registration"><a href="#20-DINOv3-with-Test-Time-Training-for-Medical-Image-Registration" class="headerlink" title="20. DINOv3 with Test-Time Training for Medical Image Registration"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/DINOv3_with_Test-Time_Training_for_Medical_Image_Registration.pdf">DINOv3 with Test-Time Training for Medical Image Registration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Emory University</span></p><p>该论文提出了一个无需训练的医学图像配准方法，利用冻结的DINOv3视觉编码器进行切片特征提取，通过PCA降维在特征空间直接优化变形场，仅在测试时进行两阶段配准。实验结果在腹部MR-CT和心脏MRI数据集上均取得了配准精度和变形规则性的新高，表明该方法无需监督即可适用于多模态医学图像临床应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/DINOv3_with_Test-Time_Training_for_Medical_Image_Registration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-Fusing-Monocular-RGB-Images-with-AIS-Data-to-Create-a-6D-Pose-Estimation-Dataset-for-Marine-Vessels"><a href="#21-Fusing-Monocular-RGB-Images-with-AIS-Data-to-Create-a-6D-Pose-Estimation-Dataset-for-Marine-Vessels" class="headerlink" title="21. Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Fusing_Monocular_RGB_Images_with_AIS_Data_to_Create_a_6D_Pose_Estimation_Dataset_for_Marine_Vessels.pdf">Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Hamburg</span></p><p>该论文提出了一种自动化的6D姿态估计算法，通过融合单目RGB图像中的YOLOX-X目标检测结果与AIS（自动识别系统）数据，实现船舶的3D包围盒和6D姿态标注。核心流程包括目标检测、PnP坐标变换、AIS与检测结果的二分图匹配、视觉引导校正和3D包围盒生成。实验表明，PnP方法投影误差远低于传统单应性方法，YOLOX-X检测准确率达0.80，最终自动生成的BONK-Pose数据集有86.4%标注准确、94.5%为可用标注。结论认为该方法无需人工标注即可高效生成高质量6D姿态数据集，并为后续方法优化和移动采集等场景提供了方向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Fusing_Monocular_RGB_Images_with_AIS_Data_to_Create_a_6D_Pose_Estimation_Dataset_for_Marine_Vessels.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-MISSIONHD-Data-Driven-Refinement-of-Reasoning-Graph-Structure-through-Hyperdimensional-Causal-Path-Encoding-and-Decoding"><a href="#22-MISSIONHD-Data-Driven-Refinement-of-Reasoning-Graph-Structure-through-Hyperdimensional-Causal-Path-Encoding-and-Decoding" class="headerlink" title="22. MISSIONHD: Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/MissionHD__Data-Driven_Refinement_of_Reasoning_Graph_Structure_through_Hyperdimensional_Causal_Path_.pdf">MISSIONHD: Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Irvine</span></p><p>该论文提出MISSIONHD框架，通过超高维计算（HDC）实现数据驱动的知识图谱结构优化（D-GSR），将大语言模型（LLM）生成的推理图与下游视觉任务（如视频异常检测）数据分布对齐。方法包括高效的推理路径编码-解码流程，利用任务信号指导图结构修正，实验证明在UCF-Crime和XD-Violence等视觉基准上显著提升推理模型性能，验证了其作为预处理步骤的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/MISSIONHD_Data-Driven_Refinement_of_Reasoning_Graph_Structure_through_Hyperdimensional_Causal_Path_Encoding_and_Decoding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-GSFix3D-Diffusion-Guided-Repair-of-Novel-Views-in-Gaussian-Splatting"><a href="#23-GSFix3D-Diffusion-Guided-Repair-of-Novel-Views-in-Gaussian-Splatting" class="headerlink" title="23. GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/GSFix3D__Diffusion-Guided_Repair_of_Novel_Views_in_Gaussian_Splatting.pdf">GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Munich</span></p><p>GSFix3D提出了一套针对3D Gaussian Splatting重建场景新视角修复的流程，通过定制微调的GSFixer扩散模型，融合3DGS和mesh渲染信息，并引入随机掩码增强，实现对缺失区域的智能修复和伪影消除。实验表明该方法无需大规模数据，仅少量场景微调即可在室内外真实数据和极端视角下取得领先修复效果，对位姿误差也具备较强鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/GSFix3D_Diffusion-Guided_Repair_of_Novel_Views_in_Gaussian_Splatting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-AnchorSync-Global-Consistency-Optimization-for-Long-Video-Editing"><a href="#24-AnchorSync-Global-Consistency-Optimization-for-Long-Video-Editing" class="headerlink" title="24. AnchorSync: Global Consistency Optimization for Long Video Editing"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/AnchorSync__Global_Consistency_Optimization_for_Long_Video_Editing.pdf">AnchorSync: Global Consistency Optimization for Long Video Editing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出AnchorSync，一种面向长视频编辑的两阶段扩散模型框架。方法首先通过带有可训练双向注意力机制的扩散模型对稀疏锚帧进行联合编辑，确保全局结构一致性，并通过多模态（边缘与光流）ControlNet引导的视频扩散模型进行锚帧间的插值，保证时序平滑和结构连续。实验表明，该方法在长视频的编辑一致性、帧连续性和视觉质量方面明显优于现有方法，可高效实现高质量的可控长视频编辑。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/AnchorSync_Global_Consistency_Optimization_for_Long_Video_Editing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-Reliable-Smoke-Detection-via-Optical-Flow-Guided-Feature-Fusion-and-Transformer-Based-Uncertainty-Modeling"><a href="#25-Reliable-Smoke-Detection-via-Optical-Flow-Guided-Feature-Fusion-and-Transformer-Based-Uncertainty-Modeling" class="headerlink" title="25. Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Reliable_Smoke_Detection_via_Optical_Flow-Guided_Feature_Fusion_and_Transformer-Based_Uncertainty_Mo.pdf">Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Maulana Azad National Institute of Technology Bhopal</span></p><p>本文提出了一种融合单目图像光流特征与外观信息的新型烟雾检测框架，核心包括基于分数阶变分的光流估计（FCDLe-FOV模型）、高斯混合模型分割和两阶段不确定性感知Swin Transformer（TP-UAST）。方法在光流与视觉特征融合的基础上，结合不确定性建模，提升了检测的准确性和可靠性。实验表明，该方法在多项指标上优于现有方法，实现了高鲁棒性和优良的置信度校准，适用于早期火灾预警等场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Reliable_Smoke_Detection_via_Optical_Flow-Guided_Feature_Fusion_and_Transformer-Based_Uncertainty_Modeling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-Towards-Skeletal-and-Signer-Noise-Reduction-in-Sign-Language-Production-via-Quaternion-Based-Pose-Encoding-and-Contrastive-Learning"><a href="#26-Towards-Skeletal-and-Signer-Noise-Reduction-in-Sign-Language-Production-via-Quaternion-Based-Pose-Encoding-and-Contrastive-Learning" class="headerlink" title="26. Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Towards_Skeletal_and_Signer_Noise_Reduction_in_Sign_Language_Production_via_Quaternion-Based_Pose_En.pdf">Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Université de Lorraine</span></p><p>本文提出在基于Progressive Transformers的手语生成模型中，采用骨骼四元数旋转编码与几何损失函数以提升关节角度表达的准确性与清晰度，并引入基于语义相似度的对比损失（利用词汇重叠或SBERT句子嵌入）以减少无关风格和解剖学特征带来的噪音。实验在Phoenix14T数据集上表明，对比损失可提升16%关键点正确率，结合四元数编码后平均骨角误差降低6%，显示骨骼结构建模与语义对比目标能提升模型泛化与表达一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Towards_Skeletal_and_Signer_Noise_Reduction_in_Sign_Language_Production_via_Quaternion-Based_Pose_Encoding_and_Contrastive_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-GOGS-High-Fidelity-Geometry-and-Relighting-for-Glossy-Objects-via-Gaussian-Surfels"><a href="#27-GOGS-High-Fidelity-Geometry-and-Relighting-for-Glossy-Objects-via-Gaussian-Surfels" class="headerlink" title="27. GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/GOGS__High-Fidelity_Geometry_and_Relighting_for_Glossy_Objects_via_Gaussian_Surfels.pdf">GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Chengdu University of Information Technology</span></p><p>本文提出了GOGS，一种基于2D高斯表面元的两阶段反向渲染框架，首先通过结合基础模型的几何先验和物理渲染（split-sum近似）实现高精度几何重建，然后利用蒙特卡洛重要性采样和可微2D高斯追踪对材料进行分解，并引入球面mipmap方向编码以补偿高频镜面细节。实验表明该方法在几何重建、材料分离和新光照下的真实感渲染方面均优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/GOGS_High-Fidelity_Geometry_and_Relighting_for_Glossy_Objects_via_Gaussian_Surfels.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-TINKER-Diffusion’s-Gift-to-3D—Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization"><a href="#28-TINKER-Diffusion’s-Gift-to-3D—Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization" class="headerlink" title="28. TINKER: Diffusion’s Gift to 3D—Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Tinker__Diffusion's_Gift_to_3D--Multi-View_Consistent_Editing_From_Sparse_Inputs_without_Per-Scene_O.pdf">TINKER: Diffusion’s Gift to 3D—Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出TINKER，一种无需针对具体场景优化的高保真3D编辑框架，通过利用预训练扩散模型，实现从极少的视角输入（一到两张图片）即可生成多视角一致的编辑结果。其核心方法包括：1）多视角引用编辑器，实现参考驱动的跨视角一致编辑；2）任意视角到视频合成器，结合视频扩散模型的空间-时间先验，实现高质量场景补全和新视角生成。结论：TINKER显著降低了3D内容创建门槛，在编辑、一视多角合成及渲染质量提升任务上表现优异，推动3D编辑的通用化和零样本能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/TINKER_Diffusion%E2%80%99s_Gift_to_3D%E2%80%94Multi-View_Consistent_Editing_From_Sparse_Inputs_without_Per-Scene_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Adversarial-Hospital-Invariant-Feature-Learning-for-WSI-Patch-Classification"><a href="#29-Adversarial-Hospital-Invariant-Feature-Learning-for-WSI-Patch-Classification" class="headerlink" title="29. Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Adversarial_Hospital-Invariant_Feature_Learning_for_WSI_Patch_Classification.pdf">Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Texas at Arlington</span></p><p>本文系统研究了病理基础模型在不同医院来源的WSI图像中存在的域偏置问题，提出了一种轻量级对抗训练框架，通过引入可训练的投影头、域分类器和梯度反转层（GRL），在不修改主编码器的情况下，有效抑制了特征中的医院特有信息，同时保持疾病分类性能。实验表明，该方法在多中心病理数据集上显著降低了医院可预测性，提升了模型在未知医院场景下的泛化能力，验证了对抗训练在医疗AI公平性和鲁棒性提升中的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Adversarial_Hospital-Invariant_Feature_Learning_for_WSI_Patch_Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-Seeing-Further-on-the-Shoulders-of-Giants-Knowledge-Inheritance-for-Vision-Foundation-Models"><a href="#30-Seeing-Further-on-the-Shoulders-of-Giants-Knowledge-Inheritance-for-Vision-Foundation-Models" class="headerlink" title="30. Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Seeing_Further_on_the_Shoulders_of_Giants__Knowledge_Inheritance_for_Vision_Foundation_Models.pdf">Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sony AI</span></p><p>该论文提出了一种名为知识保持与统一（KPU）的模型驱动训练方法，通过在统一的潜空间内对多个预训练视觉模型（如CLIP、DINOv2、GroundingDINO）进行知识对齐，并采用适配器模块融合不同教师模型的特长与泛化能力，实现对视觉基础模型的高效继承和知识整合。实验表明，该方法无需大量标注数据即可在图像分类、目标检测、语义分割和实例分割等多项视觉任务上超越现有主流数据驱动和模型驱动方法，展现了模型集成和知识迁移的新潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Seeing_Further_on_the_Shoulders_of_Giants_Knowledge_Inheritance_for_Vision_Foundation_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-Virtual-Multiplex-Staining-for-Histological-Images-using-a-Marker-wise-Conditioned-Diffusion-Model"><a href="#31-Virtual-Multiplex-Staining-for-Histological-Images-using-a-Marker-wise-Conditioned-Diffusion-Model" class="headerlink" title="31. Virtual Multiplex Staining for Histological Images using a Marker-wise Conditioned Diffusion Model"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Virtual_Multiplex_Staining_for_Histological_Images_using_a_Marker-wise_Conditioned_Diffusion_Model.pdf">Virtual Multiplex Staining for Histological Images using a Marker-wise Conditioned Diffusion Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea University</span></p><p>该论文提出了一种基于条件扩散模型的虚拟多重染色方法，利用预训练的稳定扩散模型骨架，并通过marker-wise一热嵌入实现多marker联合生成，提高模型在大规模标记类型下的可扩展性与知识共享。通过两阶段训练框架，后续像素级损失微调显著提升颜色对比保真度与推理效率，在两个公开医学数据集上验证了方法可高质量生成最多18种marker类型，性能优于现有方法，推动多重染色虚拟化在病理分析中的应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Virtual_Multiplex_Staining_for_Histological_Images_using_a_Marker-wise_Conditioned_Diffusion_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-Controllable-Latent-Space-Augmentation-for-Digital-Pathology"><a href="#32-Controllable-Latent-Space-Augmentation-for-Digital-Pathology" class="headerlink" title="32. Controllable Latent Space Augmentation for Digital Pathology"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Controllable_Latent_Space_Augmentation_for_Digital_Pathology.pdf">Controllable Latent Space Augmentation for Digital Pathology</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MICS, CentraleSupelec – Université Paris-Saclay</span></p><p>该论文提出了一种名为HistAug的高效生成模型，用于数字病理学中的WSI（Whole Slide Image）分析，通过在视觉特征的潜在空间中进行可控的数据增强。HistAug采用基于Transformer的架构，能够根据不同的图像变换（如色调、腐蚀等）参数，生成与指定增强一致的patch特征嵌入，大大提升了多实例学习（MIL）任务中的模型性能。结论表明，HistAug相比传统patch级增强和现有扩散模型，不仅加速训练且显著减少内存消耗，在多种器官和任务中均优于现有方法，尤其在低数据量场景下效果更突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Controllable_Latent_Space_Augmentation_for_Digital_Pathology.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-Safety-Critical-Learning-for-Long-Tail-Events-The-TUM-Traffic-Accident-Dataset"><a href="#33-Safety-Critical-Learning-for-Long-Tail-Events-The-TUM-Traffic-Accident-Dataset" class="headerlink" title="33. Safety-Critical Learning for Long-Tail Events: The TUM Traffic Accident Dataset"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Safety-Critical_Learning_for_Long-Tail_Events__The_TUM_Traffic_Accident_Dataset.pdf">Safety-Critical Learning for Long-Tail Events: The TUM Traffic Accident Dataset</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Munich</span></p><p>本文提出了TUM Traffic Accident数据集，涵盖真实高速公路事故的多模态数据（包括2D&#x2F;3D标注、轨迹等），并提出了Accid3nD事故检测框架，将基于规则与基于YOLOv8深度学习的方法结合，实现了对交通事故的实时检测与分析。实验结果表明该方法在多种环境下具有较高鲁棒性和准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Safety-Critical_Learning_for_Long-Tail_Events_The_TUM_Traffic_Accident_Dataset.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-A-Comprehensive-Review-of-Agricultural-Parcel-and-Boundary-Delineation-from-Remote-Sensing-Images-Recent-Progress-and-Future-Perspectives"><a href="#34-A-Comprehensive-Review-of-Agricultural-Parcel-and-Boundary-Delineation-from-Remote-Sensing-Images-Recent-Progress-and-Future-Perspectives" class="headerlink" title="34. A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/A_Comprehensive_Review_of_Agricultural_Parcel_and_Boundary_Delineation_from_Remote_Sensing_Images__R.pdf">A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-Sen University</span></p><p>本论文系统回顾了农业地块与边界从遥感影像自动提取的研究进展，涵盖传统图像处理、传统机器学习和深度学习三类方法，重点剖析了语义分割、目标检测和Transformer等深度学习技术在农业地块边界提取中的应用流程与优势。结论指出，深度学习方法特别是基于Transformer和大模型的视觉方法已成为主流，极大提升了复杂地形下地块提取的准确性和适应性，并展望了多源数据融合、可解释模型和大规模应用的未来发展方向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/A_Comprehensive_Review_of_Agricultural_Parcel_and_Boundary_Delineation_from_Remote_Sensing_Images_Recent_Progress_and_Future_Perspectives.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-WISE-FUSE-Efficient-Whole-Slide-Image-Encoding-via-Coarse-to-Fine-Patch-Selection-with-VLM-and-LLM-Knowledge-Fusion"><a href="#35-WISE-FUSE-Efficient-Whole-Slide-Image-Encoding-via-Coarse-to-Fine-Patch-Selection-with-VLM-and-LLM-Knowledge-Fusion" class="headerlink" title="35. WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/WISE-FUSE__Efficient_Whole_Slide_Image_Encoding_via_Coarse-to-Fine_Patch_Selection_with_VLM_and_LLM_.pdf">WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea University</span></p><p>WISE-FUSE提出了一种高效编码病理全切片图像的方法，结合跨尺度视觉提示微调（CS-VPT）和基于大规模视觉语言模型（VLM）及大语言模型（LLM）的知识融合。该方法通过低分辨率与高分辨率补丁的协同选择和语义融合，仅处理诊断相关区域，大幅减少计算成本并保持或提升诊断性能。实验结果显示WISE-FUSE能将WSI编码时间缩短三倍以上，同时在多种任务和数据集上取得与全量处理相当或更优的诊断结果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/WISE-FUSE_Efficient_Whole_Slide_Image_Encoding_via_Coarse-to-Fine_Patch_Selection_with_VLM_and_LLM_Knowledge_Fusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-Deep-Skin-Lesion-Segmentation-with-Transformer-CNN-Fusion-Toward-Intelligent-Skin-Cancer-Analysis"><a href="#36-Deep-Skin-Lesion-Segmentation-with-Transformer-CNN-Fusion-Toward-Intelligent-Skin-Cancer-Analysis" class="headerlink" title="36. Deep Skin Lesion Segmentation with Transformer-CNN Fusion: Toward Intelligent Skin Cancer Analysis"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Deep_Skin_Lesion_Segmentation_with_Transformer-CNN_Fusion__Toward_Intelligent_Skin_Cancer_Analysis.pdf">Deep Skin Lesion Segmentation with Transformer-CNN Fusion: Toward Intelligent Skin Cancer Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of the Chinese Academy of Sciences</span></p><p>该论文提出一种基于改进TransUNet结构的皮肤病变图像高精度分割方法，将Transformer模块与CNN结合以兼顾全局语义建模与局部纹理边缘特征，并引入边界引导注意机制和多尺度上采样路径提升病变边界定位和分割一致性。实验表明，该方法在mIoU、mDice和mAcc等指标上优于主流模型，具备更强的病变识别准确性和鲁棒性，尤其在复杂场景下实现了更优的边界重建。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Deep_Skin_Lesion_Segmentation_with_Transformer-CNN_Fusion_Toward_Intelligent_Skin_Cancer_Analysis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-Vivid-VR-Distilling-Concepts-from-Text-to-Video-Diffusion-Transformer-for-Photorealistic-Video-Restoration"><a href="#37-Vivid-VR-Distilling-Concepts-from-Text-to-Video-Diffusion-Transformer-for-Photorealistic-Video-Restoration" class="headerlink" title="37. Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Vivid-VR__Distilling_Concepts_from_Text-to-Video_Diffusion_Transformer_for_Photorealistic_Video_Rest.pdf">Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Alibaba Group - Taobao &amp; Tmall Group</span></p><p>本文提出了一种基于Diffusion Transformer (DiT) 的生成式视频复原方法Vivid-VR，核心在于引入概念蒸馏训练策略，通过预训练T2V模型合成嵌入文本概念的训练样本，缓解传统可控生成模型微调时的分布漂移问题，并保持纹理真实感与时序一致性。此外，设计了轻量级控制特征投影器和双分支ControlNet连接器以提升控制能力和去除退化伪影。实验显示，Vivid-VR在真实世界、合成和AIGC视频基准上优于现有方法，实现了更高的纹理真实感、视觉生动性和时间一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Vivid-VR_Distilling_Concepts_from_Text-to-Video_Diffusion_Transformer_for_Photorealistic_Video_Restoration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-Ouroboros-Single-step-Diffusion-Models-for-Cycle-consistent-Forward-and-Inverse-Rendering"><a href="#38-Ouroboros-Single-step-Diffusion-Models-for-Cycle-consistent-Forward-and-Inverse-Rendering" class="headerlink" title="38. Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Ouroboros__Single-step_Diffusion_Models_for_Cycle-consistent_Forward_and_Inverse_Rendering.pdf">Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Irvine</span></p><p>本文提出Ouroboros框架，利用两个单步扩散模型实现正向和逆向渲染的一致性学习，核心方法包括基于单步扩散模型的高效逆向&#x2F;正向渲染、循环一致性训练机制，以及无训练的视频推理扩展。实验表明，该方法在多种场景下大幅提升了渲染速度与一致性，并在图像和视频分解与合成任务中均优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Ouroboros_Single-step_Diffusion_Models_for_Cycle-consistent_Forward_and_Inverse_Rendering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-MUSE-Multi-Subject-Unified-Synthesis-via-Explicit-Layout-Semantic-Expansion"><a href="#39-MUSE-Multi-Subject-Unified-Synthesis-via-Explicit-Layout-Semantic-Expansion" class="headerlink" title="39. MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/MUSE__Multi-Subject_Unified_Synthesis_via_Explicit_Layout_Semantic_Expansion.pdf">MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing University of Posts and Telecommunications</span></p><p>该论文提出MUSE框架，通过显式布局语义扩展和级联交叉注意力（CCA）机制，将布局约束与文本描述无干扰地统一，提升多主体文本到图像生成的空间与身份控制能力。采用渐进式两阶段训练，先优化布局，再细化主体合成，实现零样本、端到端高精度图像生成，在多主体合成和布局控制任务上均超越现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/MUSE_Multi-Subject_Unified_Synthesis_via_Explicit_Layout_Semantic_Expansion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-HyperDiff-Hypergraph-Guided-Diffusion-Model-for-3D-Human-Pose-Estimation"><a href="#40-HyperDiff-Hypergraph-Guided-Diffusion-Model-for-3D-Human-Pose-Estimation" class="headerlink" title="40. HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose Estimation"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/HyperDiff__Hypergraph_Guided_Diffusion_Model_for_3D_Human_Pose_Estimation.pdf">HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose Estimation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University of Aeronautics and Astronautics</span></p><p>该论文提出HyperDiff框架，将扩散模型与多粒度HyperGCN结合用于单目3D人体姿态估计，通过构建关节点、身体部件和整体身体多尺度超图，增强模型对关节高阶关联的建模能力，有效缓解深度歧义与遮挡问题。实验结果显示该方法在Human3.6M和MPI-INF-3DHP等标准数据集上达到了当前最佳性能，同时可灵活调整计算资源以兼顾效率和精度，适用于实时应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/HyperDiff_Hypergraph_Guided_Diffusion_Model_for_3D_Human_Pose_Estimation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-Disentanglement-in-T-space-for-Faster-and-Distributed-Training-of-Diffusion-Models-with-Fewer-Latent-states"><a href="#41-Disentanglement-in-T-space-for-Faster-and-Distributed-Training-of-Diffusion-Models-with-Fewer-Latent-states" class="headerlink" title="41. Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Disentanglement_in_T-space_for_Faster_and_Distributed_Training_of_Diffusion_Models_with_Fewer_Latent.pdf">Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amazon</span></p><p>本论文提出了一种在扩散模型训练中减少潜变量（latent-states）数量的方法，通过精心设计噪声调度，实现仅用极少潜变量甚至单一潜变量即可匹配传统千级潜变量模型的性能。作者进一步提出“T空间完全解耦”，将多个独立训练的单潜变量模型组合用于推断，实现高质量生成图像，并在ImageNet和大规模文本-图像数据集上验证了方法可显著加速训练（速度提升4-6倍），且支持跨地理分布式和异构硬件训练，最终在一天内达到传统方法五天训练效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Disentanglement_in_T-space_for_Faster_and_Distributed_Training_of_Diffusion_Models_with_Fewer_Latent-states.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-SATURN-Autoregressive-Image-Generation-Guided-by-Scene-Graphs"><a href="#42-SATURN-Autoregressive-Image-Generation-Guided-by-Scene-Graphs" class="headerlink" title="42. SATURN: Autoregressive Image Generation Guided by Scene Graphs"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/SATURN__Autoregressive_Image_Generation_Guided_by_Scene_Graphs.pdf">SATURN: Autoregressive Image Generation Guided by Scene Graphs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science, VNU-HCM</span></p><p>本文提出SATURN方法，将场景图（scene graph）结构转化为显著性排序的token序列，通过冻结的CLIP和VQ-VAE模型仅微调VAR Transformer，实现结构化信息对自回归图像生成的高效引导。实验表明，SATURN在Visual Genome数据集上FID从56.45降到21.62，IS从16.03升到24.78，显著优于SG2IM与SGDiff，同时准确性和可编辑性提升，无需额外模块或多阶段训练。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/SATURN_Autoregressive_Image_Generation_Guided_by_Scene_Graphs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-WeedSense-Multi-Task-Learning-for-Weed-Segmentation-Height-Estimation-and-Growth-Stage-Classification"><a href="#43-WeedSense-Multi-Task-Learning-for-Weed-Segmentation-Height-Estimation-and-Growth-Stage-Classification" class="headerlink" title="43. WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation, and Growth Stage Classification"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/WeedSense__Multi-Task_Learning_for_Weed_Segmentation,_Height_Estimation,_and_Growth_Stage_Classifica.pdf">WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation, and Growth Stage Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southern Illinois University Carbondale</span></p><p>该论文提出了WeedSense，一种多任务学习架构，可同时实现杂草语义分割、植株高度估算和生长阶段分类。方法采用双路径UIB编码器和Transformer特征融合的多任务解码器，在自主采集的含有16种杂草全生长周期的新数据集上进行评估。实验结果表明，WeedSense在分割、估高和生长阶段分类上均优于现有方法，并实现实时推理速度与参数量优化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/WeedSense_Multi-Task_Learning_for_Weed_Segmentation_Height_Estimation_and_Growth_Stage_Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Reconstruction-Using-the-Invisible-Intuition-from-NIR-and-Metadata-for-Enhanced-3D-Gaussian-Splatting"><a href="#44-Reconstruction-Using-the-Invisible-Intuition-from-NIR-and-Metadata-for-Enhanced-3D-Gaussian-Splatting" class="headerlink" title="44. Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Reconstruction_Using_the_Invisible__Intuition_from_NIR_and_Metadata_for_Enhanced_3D_Gaussian_Splatti.pdf">Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea University</span></p><p>本文提出了一种多模态3D高斯泼溅重建框架NIRSplat，结合了近红外(NIR)、RGB影像和植物学元数据，通过跨模态注意力机制与三维点位编码，实现对农业场景下复杂照明、遮挡和视角受限等问题的鲁棒三维重建。实验表明，NIRSplat在重建精度与视觉质量上显著优于现有主流方法，特别适用于农业环境。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Reconstruction_Using_the_Invisible_Intuition_from_NIR_and_Metadata_for_Enhanced_3D_Gaussian_Splatting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-FOCUS-Frequency-Optimized-Conditioning-of-DiffUSion-Models-for-mitigating-catastrophic-forgetting-during-Test-Time-Adaptation"><a href="#45-FOCUS-Frequency-Optimized-Conditioning-of-DiffUSion-Models-for-mitigating-catastrophic-forgetting-during-Test-Time-Adaptation" class="headerlink" title="45. FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/FOCUS__Frequency-Optimized_Conditioning_of_DiffUSion_Models_for_mitigating_catastrophic_forgetting_d.pdf">FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Centre for Frontier AI Research (CFAR), Agency for Science, Technology and Research (A*STAR)</span></p><p>该论文提出FOCUS，一种基于频率优化的扩散模型输入适应方法，通过轻量级Y型频率预测网络（Y-FPN）学习空间自适应频率先验，并利用FrequencyMix频率数据增强训练Y-FPN，条件化扩散模型逆过程以提升语义分割和单目深度估计任务在多种分布偏移和噪声干扰下的适应能力。实验表明，FOCUS在15类腐蚀和三大数据集上取得了最优平均性能，并能与现有模型适应方法协同提升，显著缓解灾难性遗忘。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/FOCUS_Frequency-Optimized_Conditioning_of_DiffUSion_Models_for_mitigating_catastrophic_forgetting_during_Test-Time_Adaptation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-MoCHA-former-Moire-Conditioned-Hybrid-Adaptive-Transformer-for-Video-Demoireing"><a href="#46-MoCHA-former-Moire-Conditioned-Hybrid-Adaptive-Transformer-for-Video-Demoireing" class="headerlink" title="46. MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/MoCHA-former__Moir%C3%A9-Conditioned_Hybrid_Adaptive_Transformer_for_Video_Demoir%C3%A9ing.pdf">MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Chung-Ang University</span></p><p>该论文提出了MoCHA-former框架，通过Decoupled Moiré Adaptive Demoiréing (DMAD)模块有效分离并自适应处理视频中的摩尔纹，结合Spatio-Temporal Adaptive Demoiréing (STAD)模块利用时空和频域信息去除大规模和动态变化的摩尔纹。实验结果表明，该方法在RawVDemoiré和VDemoiré数据集上超越了现有方法，提升了去摩尔纹效果并提高了参数效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/MoCHA-former_Moir%C3%A9-Conditioned_Hybrid_Adaptive_Transformer_for_Video_Demoir%C3%A9ing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-CTA-Flux-Integrating-Chinese-Cultural-Semantics-into-High-Quality-English-Text-to-Image-Communities"><a href="#47-CTA-Flux-Integrating-Chinese-Cultural-Semantics-into-High-Quality-English-Text-to-Image-Communities" class="headerlink" title="47. CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/CTA-Flux__Integrating_Chinese_Cultural_Semantics_into_High-Quality_English_Text-to-Image_Communities.pdf">CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>该论文提出CTA-Flux，通过在Flux文本到图像生成模型中集成中国语义分支，实现高质量中文文本驱动的图像生成。方法采用MMDiT架构，在保持英文模型主干及社区插件兼容性的前提下，设计轻量级中文语言适配模块，并通过两阶段训练（语义对齐与文化特征微调）显著提升中文语境下图像的文化真实性和生成质量。实验证明CTA-Flux可同时支持中英文输入，生成图像在视觉和语义上兼具高质量和文化适配性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/CTA-Flux_Integrating_Chinese_Cultural_Semantics_into_High-Quality_English_Text-to-Image_Communities.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-QuadINR-Hardware-Efficient-Implicit-Neural-Representations-Through-Quadratic-Activation"><a href="#48-QuadINR-Hardware-Efficient-Implicit-Neural-Representations-Through-Quadratic-Activation" class="headerlink" title="48. QuadINR: Hardware-Efficient Implicit Neural Representations Through Quadratic Activation"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/QuadINR__Hardware-Efficient_Implicit_Neural_Representations_Through_Quadratic_Activation.pdf">QuadINR: Hardware-Efficient Implicit Neural Representations Through Quadratic Activation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Hong Kong</span></p><p>该论文提出QuadINR方法，通过采用分段二次激活函数实现硬件高效的隐式神经表示（INR），并提出统一的N级流水线框架便于FPGA和ASIC上的高效实现。实验结果表明，QuadINR在图片和视频重建任务中可在大幅降低硬件消耗的同时提升PSNR最高2.06dB，资源和功耗降低高达97%，显著优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/QuadINR_Hardware-Efficient_Implicit_Neural_Representations_Through_Quadratic_Activation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-Physics-Constrained-Diffusion-Reconstruction-with-Posterior-Correction-for-Quantitative-and-Fast-PET-Imaging"><a href="#49-Physics-Constrained-Diffusion-Reconstruction-with-Posterior-Correction-for-Quantitative-and-Fast-PET-Imaging" class="headerlink" title="49. Physics-Constrained Diffusion Reconstruction with Posterior Correction for Quantitative and Fast PET Imaging"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Physics-Constrained_Diffusion_Reconstruction_with_Posterior_Correction_for_Quantitative_and_Fast_PET.pdf">Physics-Constrained Diffusion Reconstruction with Posterior Correction for Quantitative and Fast PET Imaging</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Normal University</span></p><p>本论文提出了一种基于条件扩散模型与后验物理校正（PET-DPC）的PET图像重建方法。方法流程为：首先对原始PET列表模式数据进行几何TOF概率图像（GTP-image）生成，再将其作为辅助输入用于条件DDPM训练，在反向扩散采样过程中引入物理后验信息以实现散射和衰减校正。实验在大量临床、物理仿体与模拟数据集上验证，结果显示，PET-DPC在定量准确性、结构细节恢复与重建速度上均优于端到端深度学习模型和部分传统迭代算法，并能良好泛化至分布外数据。该方法显著提升了PET图像的定量可靠性和临床适用性，兼顾速度与精度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Physics-Constrained_Diffusion_Reconstruction_with_Posterior_Correction_for_Quantitative_and_Fast_PET_Imaging.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-Learning-Point-Cloud-Representations-with-Pose-Continuity-for-Depth-Based-Category-Level-6D-Object-Pose-Estimation"><a href="#50-Learning-Point-Cloud-Representations-with-Pose-Continuity-for-Depth-Based-Category-Level-6D-Object-Pose-Estimation" class="headerlink" title="50. Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Learning_Point_Cloud_Representations_with_Pose_Continuity_for_Depth-Based_Category-Level_6D_Object_P.pdf">Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Graduate Center, CUNY</span></p><p>该论文提出HRC-Pose，一种基于深度图的类别级6D物体姿态估计方法。方法创新性地引入分层排序对比学习模块，利用6D姿态感知的层级排序来学习点云的连续表征，并将位姿解耦为旋转和平移分别编码，分别用于最终的姿态回归。实验表明，该方法在REAL275和CAMERA25数据集上性能优于现有深度方法，且推理速度达实时水平，具有良好实际应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Learning_Point_Cloud_Representations_with_Pose_Continuity_for_Depth-Based_Category-Level_6D_Object_Pose_Estimation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-TCFNet-Bidirectional-Face-Bone-Transformation-via-a-Transformer-Based-Coarse-to-Fine-Point-Movement-Network"><a href="#51-TCFNet-Bidirectional-Face-Bone-Transformation-via-a-Transformer-Based-Coarse-to-Fine-Point-Movement-Network" class="headerlink" title="51. TCFNet: Bidirectional Face-Bone Transformation via a Transformer-Based Coarse-to-Fine Point Movement Network"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/TCFNet__Bidirectional_face-bone_transformation_via_a_Transformer-based_coarse-to-fine_point_movement.pdf">TCFNet: Bidirectional Face-Bone Transformation via a Transformer-Based Coarse-to-Fine Point Movement Network</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>本文提出了TCFNet，一种基于Transformer的粗到细点云移动网络框架，针对正颌手术规划中的面部与骨骼点云之间的双向变换问题。方法在第一阶段采用Point Transformer V3提取全局特征并生成粗变形路径，第二阶段引入本地信息聚合网络（LIA-Net），建模点云的局部几何结构以增强局部特征；同时设计了辅助损失用于关键结构的对应。实验表明，TCFNet在各项评估指标及可视化效果上均优于现有SOTA方法，能高效准确处理高密度点云，实现面骨双向转化并提升局部结构重建。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/TCFNet_Bidirectional_Face-Bone_Transformation_via_a_Transformer-Based_Coarse-to-Fine_Point_Movement_Network.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-Taming-Transformer-for-Emotion-Controllable-Talking-Face-Generation"><a href="#52-Taming-Transformer-for-Emotion-Controllable-Talking-Face-Generation" class="headerlink" title="52. Taming Transformer for Emotion-Controllable Talking Face Generation"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Taming_Transformer_for_Emotion-Controllable_Talking_Face_Generation.pdf">Taming Transformer for Emotion-Controllable Talking Face Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xidian University</span></p><p>本文提出一种基于离散表示的自回归框架用于情感可控的语音驱动人脸视频生成。方法包括音频情感与内容特征的解耦、VQ-GAN视觉量化、情感锚点（EA）表示融合以及自回归Transformer建模，实现了音视频跨模态情感融合与高质量情感可控人脸生成。实验结果显示，该方法在MEAD数据集上显著优于现有方法，生成视频的情感准确性和真实性大幅提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Taming_Transformer_for_Emotion-Controllable_Talking_Face_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-HandCraft-Dynamic-Sign-Generation-for-Synthetic-Data-Augmentation"><a href="#53-HandCraft-Dynamic-Sign-Generation-for-Synthetic-Data-Augmentation" class="headerlink" title="53. HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/HandCraft__Dynamic_Sign_Generation_for_Synthetic_Data_Augmentation.pdf">HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Instituto de Investigación en Informática LIDI - Universidad Nacional de La Plata</span></p><p>本文提出了一种基于条件多层感知机（CMLPe）的高效手语动作生成器，用于合成手语关键点序列，并通过两阶段流程（先用合成数据预训练SLR模型，再用真实数据微调）提升手语识别准确率。实验结果表明，该方法在LSFB和DiSPLaY数据集上取得新的SOTA，且合成数据预训练优于传统增强方法，两者结合效果更佳。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/HandCraft_Dynamic_Sign_Generation_for_Synthetic_Data_Augmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="54-Long-Chain-of-Thought-Reasoning-Across-Languages"><a href="#54-Long-Chain-of-Thought-Reasoning-Across-Languages" class="headerlink" title="54. Long Chain-of-Thought Reasoning Across Languages"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Long_Chain-of-Thought_Reasoning_Across_Languages.pdf">Long Chain-of-Thought Reasoning Across Languages</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Berkeley</span></p><p>该论文通过将英文推理数据集自动翻译为法语、日语、拉脱维亚语和斯瓦希里语，系统研究了长链式思维（Long CoT）在多语言环境下的表现，方法包括对Qwen 2.5&#x2F;3模型在多语言推理轨迹上的微调、语言强制和数据集质量与规模的对比实验。结论表明：英文作为枢纽语言对不同语种的推理迁移效用不一，高资源语言本地推理表现与英文相当，中低资源语言则表现分化，多语言预训练缩小但未消除跨语种性能差距，小规模高质量数据对高资源语言足够，而低资源语言需要更大规模数据。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Long_Chain-of-Thought_Reasoning_Across_Languages.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs"><a href="#55-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs" class="headerlink" title="55. Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Quantization_Meets_dLLMs__A_Systematic_Study_of_Post-training_Quantization_for_Diffusion_LLMs.pdf">Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">NLPR &amp; MAIS, Institute of Automation, CAS</span></p><p>本文首次系统研究了后训练量化（PTQ）在扩散型大语言模型（dLLMs）上的应用，分析发现激活异常值广泛存在并是低比特量化的关键挑战。实验比较了多种量化方法（GPTQ、AWQ、SmoothQuant、QuaRot、DuQuant）在不同任务和模型上的表现，结论指出GPTQ和DuQuant在低比特量化下表现优越，任务类型和模型调优方式影响量化鲁棒性，为dLLMs高效部署提供了实用指导。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Quantization_Meets_dLLMs_A_Systematic_Study_of_Post-training_Quantization_for_Diffusion_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds"><a href="#56-MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds" class="headerlink" title="56. MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/MeshCoder__LLM-Powered_Structured_Mesh_Code_Generation_from_Point_Clouds.pdf">MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Artificial Intelligence Laboratory</span></p><p>本文提出了MeshCoder框架，将3D点云重建为可编辑的Blender Python脚本。方法包括设计高表达力的Blender Python API、构建百万级3D对象-代码对大数据集，并训练多模态大语言模型（LLM）将点云编码为结构化代码，实现高精度形状重建和语义理解。实验表明，MeshCoder在shape-to-code任务上表现优异，并显著提升LLM对3D形状的推理能力，且支持便捷的几何与拓扑编辑。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/MeshCoder_LLM-Powered_Structured_Mesh_Code_Generation_from_Point_Clouds.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-Universal-and-Transferable-Adversarial-Attack-on-Large-Language-Models-Using-Exponentiated-Gradient-Descent"><a href="#57-Universal-and-Transferable-Adversarial-Attack-on-Large-Language-Models-Using-Exponentiated-Gradient-Descent" class="headerlink" title="57. Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Universal_and_Transferable_Adversarial_Attack_on_Large_Language_Models_Using_Exponentiated_Gradient_.pdf">Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Florida State University</span></p><p>该论文提出了一种基于指数梯度下降（EGD）的新型对抗攻击方法，通过优化松弛的一次编码，能高效地对大型语言模型（LLMs）进行jailbreak。该方法在无需显式投影的情况下保证概率单纯形约束，并通过理论收敛性证明和大量实验，展示了对多种开源模型和多种数据集的高成功率及优越的迁移性与通用性。结论表明，该方法在成功率和计算效率方面均超越三种主流基线，是当前LLM jailbreak的有效方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Universal_and_Transferable_Adversarial_Attack_on_Large_Language_Models_Using_Exponentiated_Gradient_Descent.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-From-Passive-Tool-to-Socio-cognitive-Teammate-A-Conceptual-Framework-for-Agentic-AI-in-Human-AI-Collaborative-Learning"><a href="#58-From-Passive-Tool-to-Socio-cognitive-Teammate-A-Conceptual-Framework-for-Agentic-AI-in-Human-AI-Collaborative-Learning" class="headerlink" title="58. From Passive Tool to Socio-cognitive Teammate: A Conceptual Framework for Agentic AI in Human-AI Collaborative Learning"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/From_Passive_Tool_to_Socio-cognitive_Teammate__A_Conceptual_Framework_for_Agentic_AI_in_Human-AI_Col.pdf">From Passive Tool to Socio-cognitive Teammate: A Conceptual Framework for Agentic AI in Human-AI Collaborative Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出APCP（Adaptive instrument, Proactive assistant, Co-learner, Peer collaborator）四级框架，系统刻画了教育场景下AI从被动工具到具备主动性、协作能力的“社交认知队友”演变过程。作者基于社会文化学习理论及CSCL，明确AI在不同层级下的角色分工和互动模式，强调AI虽无法成为具备真实意识的合作伙伴，但可作为高效功能型协作者显著提升协作学习效果，对未来教育AI设计和教学实践具有重要指导意义。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/From_Passive_Tool_to_Socio-cognitive_Teammate_A_Conceptual_Framework_for_Agentic_AI_in_Human-AI_Collaborative_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-TransLLM-A-Unified-Multi-Task-Foundation-Framework-for-Urban-Transportation-via-Learnable-Prompting"><a href="#59-TransLLM-A-Unified-Multi-Task-Foundation-Framework-for-Urban-Transportation-via-Learnable-Prompting" class="headerlink" title="59. TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/TransLLM__A_Unified_Multi-Task_Foundation_Framework_for_Urban_Transportation_via_Learnable_Prompting.pdf">TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出TransLLM，一种结合轻量级时空编码器与大语言模型（LLM）的统一多任务城市交通基础模型框架，采用可学习的实例级prompt路由机制（基于强化学习），动态个性化生成适用于不同输入的提示，并通过多任务输出层提升结构化数据的预测精度。实验表明，TransLLM在交通流预测、电动车充电需求预测和出租车调度等任务上，在有监督与零样本场景下均优于十个主流基线模型，实现了优异的泛化和跨任务适应能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/TransLLM_A_Unified_Multi-Task_Foundation_Framework_for_Urban_Transportation_via_Learnable_Prompting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-Assessing-the-Quality-and-Security-of-AI-Generated-Code-A-Quantitative-Analysis"><a href="#60-Assessing-the-Quality-and-Security-of-AI-Generated-Code-A-Quantitative-Analysis" class="headerlink" title="60. Assessing the Quality and Security of AI-Generated Code: A Quantitative Analysis"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Assessing_the_Quality_and_Security_of_AI-Generated_Code__A_Quantitative_Analysis.pdf">Assessing the Quality and Security of AI-Generated Code: A Quantitative Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sonar</span></p><p>该论文采用静态分析工具SonarQube对五种主流大语言模型（Claude Sonnet 4、Claude 3.7 Sonnet、GPT-4o、Llama 3.2 90B、OpenCoder 8B）生成的4,442份Java代码进行系统质量与安全性评估。研究流程包括代码生成、功能测试与静态缺陷检测，发现所有模型输出的代码均存在明显的代码异味、bug和安全漏洞，且模型功能表现与代码质量、安全性无明显相关性。结论认为，LLM生成代码在投入生产前需严格验证，静态分析工具是发现潜在缺陷和保障安全的关键手段。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Assessing_the_Quality_and_Security_of_AI-Generated_Code_A_Quantitative_Analysis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-ShizhenGPT-Towards-Multimodal-LLMs-for-Traditional-Chinese-Medicine"><a href="#61-ShizhenGPT-Towards-Multimodal-LLMs-for-Traditional-Chinese-Medicine" class="headerlink" title="61. ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/ShizhenGPT__Towards_Multimodal_LLMs_for_Traditional_Chinese_Medicine.pdf">ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong, Shenzhen</span></p><p>ShizhenGPT提出了首个针对中医领域的多模态大语言模型，结合大规模中医文本、图像、音频和生理信号，采用两阶段预训练和多模态指令微调，支持望、闻、问、切四诊信息的统一感知与推理。实验显示，ShizhenGPT在中医专业考试、视觉诊断和多模态信号感知等任务上超越同规模主流LLM，部分能力接近超大模型，显著提升了中医AI诊断的全面性和准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/ShizhenGPT_Towards_Multimodal_LLMs_for_Traditional_Chinese_Medicine.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-ELATE-Evolutionary-Language-model-for-Automated-Time-series-Engineering"><a href="#62-ELATE-Evolutionary-Language-model-for-Automated-Time-series-Engineering" class="headerlink" title="62. ELATE: Evolutionary Language model for Automated Time-series Engineering"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/ELATE__Evolutionary_Language_model_for_Automated_Time-series_Engineering.pdf">ELATE: Evolutionary Language model for Automated Time-series Engineering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">JP Morgan AI Research</span></p><p>本论文提出ELATE方法，将大型语言模型（LLM）嵌入到进化优化框架中，用于自动化时序数据的特征工程。其流程包括：LLM根据任务上下文和已有特征建议新的特征变换，通过Granger因果性和互信息等指标评估特征预测力，并结合SHAP特征筛选进行特征库的进化与剪枝。大量实验表明，ELATE在多个时序预测任务上平均提升预测准确率8.4%，同时具备较高可解释性和计算效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/ELATE_Evolutionary_Language_model_for_Automated_Time-series_Engineering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-OneLoc-Geo-Aware-Generative-Recommender-Systems-for-Local-Life-Service"><a href="#63-OneLoc-Geo-Aware-Generative-Recommender-Systems-for-Local-Life-Service" class="headerlink" title="63. OneLoc: Geo-Aware Generative Recommender Systems for Local Life Service"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/OneLoc__Geo-Aware_Generative_Recommender_Systems_for_Local_Life_Service.pdf">OneLoc: Geo-Aware Generative Recommender Systems for Local Life Service</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Inc.</span></p><p>该论文提出OneLoc，一种面向本地生活服务的端到端生成式推荐系统，结合地理信息建模（geo-aware语义ID、geo-aware自注意力、neighbor-aware prompt）与强化学习优化（地理奖励和GMV奖励）。结果显示OneLoc在Kuaishou上线后显著提升GMV和订单量，并在离线实验和A&#x2F;B测试中优于传统推荐方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/OneLoc_Geo-Aware_Generative_Recommender_Systems_for_Local_Life_Service.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-Can-LLM-Agents-Solve-Collaborative-Tasks-A-Study-on-Urgency-Aware-Planning-and-Coordination"><a href="#64-Can-LLM-Agents-Solve-Collaborative-Tasks-A-Study-on-Urgency-Aware-Planning-and-Coordination" class="headerlink" title="64. Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Can_LLM_Agents_Solve_Collaborative_Tasks__A_Study_on_Urgency-Aware_Planning_and_Coordination.pdf">Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universidade Federal de Minas Gerais</span></p><p>本文提出并系统评估了基于大型语言模型（LLM）驱动的多智能体在结构化救援任务中的协作能力，方法包括在完全可知的图环境下，采用模块化推理流程和消息通信机制，使智能体进行分工、优先级规划和资源分配。结果显示，部分LLM智能体能实现紧急性感知和任务协作，但在效率和可靠性上仍略逊于确定性启发式基线，主要挑战在于计划幻觉、冗余动作和队友意图感知不足。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Can_LLM_Agents_Solve_Collaborative_Tasks_A_Study_on_Urgency-Aware_Planning_and_Coordination.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-Evaluating-Retrieval-Augmented-Generation-vs-Long-Context-Input-for-Clinical-Reasoning-over-EHRs"><a href="#65-Evaluating-Retrieval-Augmented-Generation-vs-Long-Context-Input-for-Clinical-Reasoning-over-EHRs" class="headerlink" title="65. Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Evaluating_Retrieval-Augmented_Generation_vs._Long-Context_Input_for_Clinical_Reasoning_over_EHRs.pdf">Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Wisconsin-Madison</span></p><p>本文比较了在电子健康记录（EHR）中，基于检索增强生成（RAG）与直接长上下文输入的三种临床推理任务表现。通过对三种主流大语言模型在成像信息抽取、抗生素用药时序生成和诊断识别等任务上实验，发现RAG能以显著更少的输入token达到接近甚至超过长上下文输入的性能，尤其在信息提取类任务中表现突出。结论是RAG在长文档医疗推理任务中即使面对更强大的长上下文LLM，仍具备高效竞争力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Evaluating_Retrieval-Augmented_Generation_vs._Long-Context_Input_for_Clinical_Reasoning_over_EHRs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-Synthetic-Adaptive-Guided-Embeddings-SAGE-A-Novel-Knowledge-Distillation-Method"><a href="#66-Synthetic-Adaptive-Guided-Embeddings-SAGE-A-Novel-Knowledge-Distillation-Method" class="headerlink" title="66. Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Synthetic_Adaptive_Guided_Embeddings_(SAGE)__A_Novel_Knowledge_Distillation_Method.pdf">Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of North Texas</span></p><p>该论文提出一种新型知识蒸馏框架SAGE，通过UMAP降维和邻近采样，动态生成针对学生模型高损失区域的合成训练数据，并采用跳过输入层的矢量空间蒸馏策略，大幅提升训练效率。实验证明，在GLUE多项NLP任务上，SAGE学生模型以更少训练周期达到甚至超越DistilBERT等主流蒸馏模型，尤其在语义推理和情感分析任务表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Synthetic_Adaptive_Guided_Embeddings_%28SAGE%29_A_Novel_Knowledge_Distillation_Method.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-PepThink-R1-LLM-for-Interpretable-Cyclic-Peptide-Optimization-with-CoT-SFT-and-Reinforcement-Learning"><a href="#67-PepThink-R1-LLM-for-Interpretable-Cyclic-Peptide-Optimization-with-CoT-SFT-and-Reinforcement-Learning" class="headerlink" title="67. PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/PepThink-R1__LLM_for_Interpretable_Cyclic_Peptide_Optimization_with_CoT_SFT_and_Reinforcement_Learni.pdf">PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Merck &amp; Co., Inc., Rahway, NJ, USA</span></p><p>本文提出PepThink-R1，一种将大语言模型（LLM）、链式思维（CoT）监督微调和强化学习（RL）相结合的可解释环肽生成优化框架。该方法通过在分子单体水平进行显式推理，引导LLM产生具有多重药理性质（如脂溶性、稳定性和暴露度）优化的环肽，并利用定制奖励函数在化学有效性和多性质提升间平衡，实现自动探索多样化肽序列。实验表明，PepThink-R1在多目标优化和解释性上显著优于现有通用LLM（如GPT-5）和领域特定基线工具，能够高效生成满足多重性质的创新肽分子。结论：PepThink-R1实现了可解释的多目标肽优化，提升了优化成功率和药物发现的透明度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/PepThink-R1_LLM_for_Interpretable_Cyclic_Peptide_Optimization_with_CoT_SFT_and_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-Evaluating-Multilingual-and-Code-Switched-Alignment-in-LLMs-via-Synthetic-Natural-Language-Inference"><a href="#68-Evaluating-Multilingual-and-Code-Switched-Alignment-in-LLMs-via-Synthetic-Natural-Language-Inference" class="headerlink" title="68. Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Evaluating_Multilingual_and_Code-Switched_Alignment_in_LLMs_via_Synthetic_Natural_Language_Inference.pdf">Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Texas A&amp;M University</span></p><p>本文提出了一种通过合成、逻辑驱动的自然语言推理（NLI）框架，系统评估大型语言模型（LLMs）在多语言与代码切换情境下的语义一致性。该方法结合模板生成、自动高质量翻译及混合语输入，能精细控制推理难度，剥离语言噪声，揭示不同LLM在跨语言推理上的薄弱环节。实验发现，代码切换不仅不会降低推理表现，有时还能提升模型的多语言鲁棒性，且语义嵌入分析显示翻译后语句高度聚类，证实语义保持良好。结论指出，当前LLM的跨语言推理存在脆弱性，但代码切换可作为提升多语言推理能力的重要手段。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Evaluating_Multilingual_and_Code-Switched_Alignment_in_LLMs_via_Synthetic_Natural_Language_Inference.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-Transplant-Then-Regenerate-A-New-Paradigm-for-Text-Data-Augmentation"><a href="#69-Transplant-Then-Regenerate-A-New-Paradigm-for-Text-Data-Augmentation" class="headerlink" title="69. Transplant Then Regenerate: A New Paradigm for Text Data Augmentation"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Transplant_Then_Regenerate__A_New_Paradigm_for_Text_Data_Augmentation.pdf">Transplant Then Regenerate: A New Paradigm for Text Data Augmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了一种基于大语言模型（LLM）的创新文本数据增强范式LMTransplant，其核心方法是“transplant-then-regenerate”：将原始文本嵌入由LLM扩展生成的上下文中，并利用双向文本续写及遮蔽预测引导LLM生成多样且富有创造性的内容级变体。实验结果显示，LMTransplant在文本分类、问答和命名实体识别等任务上生成的数据提升了下游模型表现，并显著优于现有方法，具备较强的可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Transplant_Then_Regenerate_A_New_Paradigm_for_Text_Data_Augmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers"><a href="#70-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers" class="headerlink" title="70. MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/MCP-Universe__Benchmarking_Large_Language_Models_with_Real-World_Model_Context_Protocol_Servers.pdf">MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Salesforce AI Research</span></p><p>本论文提出了MCP-Universe，这是首个基于真实MCP服务器的综合基准，用于评测大语言模型（LLMs）在连接外部数据源和工具时的现实任务表现。方法包括六大领域的231项任务，采用执行式评估框架（包括格式、静态与动态评估器），分析了主流LLM在长上下文、多工具和跨域任务下的局限性，发现即使最强模型成功率也不足50%。结论：当前LLM在真实MCP环境下存在能力短板，需改进模型与代理设计。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/MCP-Universe_Benchmarking_Large_Language_Models_with_Real-World_Model_Context_Protocol_Servers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-Entropy-Constrained-Strategy-Optimization-in-Urban-Floods-A-Multi-Agent-Framework-with-LLM-and-Knowledge-Graph-Integration"><a href="#71-Entropy-Constrained-Strategy-Optimization-in-Urban-Floods-A-Multi-Agent-Framework-with-LLM-and-Knowledge-Graph-Integration" class="headerlink" title="71. Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Entropy-Constrained_Strategy_Optimization_in_Urban_Floods__A_Multi-Agent_Framework_with_LLM_and_Know.pdf">Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">College of Intelligence and Computing, Tianjin University</span></p><p>该论文提出H–J框架，融合了大语言模型（LLM）、结构化与非结构化知识检索、熵约束分层策略生成和反馈优化，形成城市洪涝多智能体调度的闭环决策系统。实验结果表明，该方法在三种典型降雨场景下，显著优于规则基线和强化学习方法，在交通通畅度、任务完成率和系统鲁棒性方面均有提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Entropy-Constrained_Strategy_Optimization_in_Urban_Floods_A_Multi-Agent_Framework_with_LLM_and_Knowledge_Graph_Integration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-Adaptively-Robust-LLM-Inference-Optimization-under-Prediction-Uncertainty"><a href="#72-Adaptively-Robust-LLM-Inference-Optimization-under-Prediction-Uncertainty" class="headerlink" title="72. Adaptively Robust LLM Inference Optimization under Prediction Uncertainty"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Adaptively_Robust_LLM_Inference_Optimization_under_Prediction_Uncertainty.pdf">Adaptively Robust LLM Inference Optimization under Prediction Uncertainty</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出了解决大语言模型（LLM）推理调度中的预测不确定性问题，通过机器学习方法预测输出长度区间，并设计了两种调度算法：Amax（利用上界，保守但低效）和Amin（利用下界，自适应更新），并对其竞争比进行了理论分析和证明。实验结果显示，Amin算法在各种预测精度下均表现出高效和稳健的推理调度性能，显著优于传统保守方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Adaptively_Robust_LLM_Inference_Optimization_under_Prediction_Uncertainty.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-Semantic-Energy-Detecting-LLM-Hallucination-Beyond-Entropy"><a href="#73-Semantic-Energy-Detecting-LLM-Hallucination-Beyond-Entropy" class="headerlink" title="73. Semantic Energy: Detecting LLM Hallucination Beyond Entropy"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Semantic_Energy__Detecting_LLM_Hallucination_Beyond_Entropy.pdf">Semantic Energy: Detecting LLM Hallucination Beyond Entropy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tianjin University</span></p><p>本文提出了Semantic Energy框架，基于对大模型最后一层logits的能量建模，结合语义聚类和玻尔兹曼能量分布，提出一种新的LLM不确定性度量方法。该方法能够克服Semantic Entropy基于概率归一化导致在部分场景下失效的问题，在多个问答基准上显著提升了幻觉检测和不确定性评估能力，平均AUROC提升超13%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Semantic_Energy_Detecting_LLM_Hallucination_Beyond_Entropy.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model"><a href="#74-NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model" class="headerlink" title="74. NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/NVIDIA_Nemotron_Nano_2__An_Accurate_and_Efficient_Hybrid_Mamba-Transformer_Reasoning_Model.pdf">NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">NVIDIA</span></p><p>本论文提出了Nemotron-Nano-9B-v2，一种混合Mamba-Transformer架构的语言模型，通过将多数自注意力层替换为Mamba-2层，提升推理任务吞吐率，保持同规模模型的先进准确率。其流程包括20万亿token的FP8预训练、连续长上下文扩展、多阶段SFT、GRPO、DPO和RLHF对齐，然后利用Minitron策略剪枝和蒸馏，最终在单块A10G GPU实现128k上下文高效推理。实验表明Nemotron-Nano-9B-v2在多项推理基准上准确率比同级模型更优或持平，且推理吞吐率提升至6倍。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/NVIDIA_Nemotron_Nano_2_An_Accurate_and_Efficient_Hybrid_Mamba-Transformer_Reasoning_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-Cognitive-Surgery-The-Awakening-of-Implicit-Territorial-Awareness-in-LLMs"><a href="#75-Cognitive-Surgery-The-Awakening-of-Implicit-Territorial-Awareness-in-LLMs" class="headerlink" title="75. Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Cognitive_Surgery__The_Awakening_of_Implicit_Territorial_Awareness_in_LLMs.pdf">Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">China Agricultural University</span></p><p>该论文提出了Cognitive Surgery (CoSur) 方法，通过表示提取、领地空间构建、作者身份判别和认知编辑四个模块，激活大语言模型（LLM）的隐性领地意识（ITA），提升其在单文本场景下的自我识别能力。实验表明，CoSur显著提升了三种主流LLM在单文本自我识别任务中的准确率，有效解决了信息损失导致的能力瓶颈。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Cognitive_Surgery_The_Awakening_of_Implicit_Territorial_Awareness_in_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-NoteIt-A-System-Converting-Instructional-Videos-to-Interactable-Notes-Through-Multimodal-Video-Understanding"><a href="#76-NoteIt-A-System-Converting-Instructional-Videos-to-Interactable-Notes-Through-Multimodal-Video-Understanding" class="headerlink" title="76. NoteIt: A System Converting Instructional Videos to Interactable Notes Through Multimodal Video Understanding"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/NoteIt__A_System_Converting_Instructional_Videos_to_Interactable_Notes_Through_Multimodal_Video_Unde.pdf">NoteIt: A System Converting Instructional Videos to Interactable Notes Through Multimodal Video Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Hong Kong</span></p><p>本文提出NoteIt，一个可将教学视频自动转换为可交互笔记的系统。其方法采用多模态大语言模型（如GPT-4o）为核心，结合视频解析、层次结构提取（章节与步骤）、视觉关键信息抽取（静态与动态关键帧）、笔记生成与自定义交互界面，全面保留视频的结构和多模态信息，并支持用户按需定制笔记表现形式和详细程度。技术评估和用户研究表明，NoteIt在内容准确性和用户满意度方面显著优于现有同类工具，有效提升了视频内容转化为结构化学习资料的效率和体验。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/NoteIt_A_System_Converting_Instructional_Videos_to_Interactable_Notes_Through_Multimodal_Video_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-Credence-Calibration-Game-Calibrating-Large-Language-Models-through-Structured-Play"><a href="#77-Credence-Calibration-Game-Calibrating-Large-Language-Models-through-Structured-Play" class="headerlink" title="77. Credence Calibration Game? Calibrating Large Language Models through Structured Play"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Credence_Calibration_Game__Calibrating_Large_Language_Models_through_Structured_Play.pdf">Credence Calibration Game? Calibrating Large Language Models through Structured Play</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Pennsylvania</span></p><p>本文提出了一种基于提示工程的LLM校准新框架，借鉴了人类“Credence Calibration Game”的结构化反馈机制，将模型历史表现和校准状态通过自然语言反馈整合到多轮交互Prompt中，实现无需参数更新的自适应置信度校准。实验在多种主流LLM及QA任务中验证了该方法能显著降低ECE和Brier分数，特别是在指数惩罚机制下校准效果最佳，但有时会略微降低准确率，表明该策略能有效提升模型置信度的可靠性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Credence_Calibration_Game_Calibrating_Large_Language_Models_through_Structured_Play.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-Towards-LLM-generated-explanations-for-Component-based-Knowledge-Graph-Question-Answering-Systems"><a href="#78-Towards-LLM-generated-explanations-for-Component-based-Knowledge-Graph-Question-Answering-Systems" class="headerlink" title="78. Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Towards_LLM-generated_explanations_for_Component-based_Knowledge_Graph_Question_Answering_Systems.pdf">Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">HTWK Leipzig</span></p><p>本文提出了一种为基于组件的知识图谱问答系统生成可解释性解释的方法，通过利用组件输入（SPARQL查询）和输出（RDF三元组）数据流，分别采用模板化（规则驱动）和大型语言模型（LLM）两种方式自动生成自然语言解释，并在Qanary框架下进行了实验与专家评价。实验结果表明，LLM生成的解释在可用性和正确性上优于模板方法，能有效提升系统可解释性，为用户和开发者提供对AI驱动问答系统决策过程的透明理解。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Towards_LLM-generated_explanations_for_Component-based_Knowledge_Graph_Question_Answering_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-PB-IAD-Utilizing-multimodal-foundation-models-for-semantic-industrial-anomaly-detection-in-dynamic-manufacturing-environments"><a href="#79-PB-IAD-Utilizing-multimodal-foundation-models-for-semantic-industrial-anomaly-detection-in-dynamic-manufacturing-environments" class="headerlink" title="79. PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/PB-IAD__Utilizing_multimodal_foundation_models_for_semantic_industrial_anomaly_detection_in_dynamic_.pdf">PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Friedrich-Alexander-Universität Erlangen-Nürnberg</span></p><p>本文提出PB-IAD框架，利用多模态基础模型（如GPT-4）通过语义化提示工程进行工业异常检测，特别适用于动态、数据稀缺的制造环境。方法通过模块化的提示模板和预处理模块，将领域专家的知识以自然语言融入检测流程，无需数据科学背景即可实现模型快速部署和灵活适配。实验表明，在三种工业场景下，PB-IAD仅凭语义提示即可在数据稀缺情况下超越PatchCore、Isolation Forest等主流算法，尤其在低样本场景下表现优异。结论：基于大模型的语义指令和人机协作，可实现工业异常检测的高效、灵活和可扩展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/PB-IAD_Utilizing_multimodal_foundation_models_for_semantic_industrial_anomaly_detection_in_dynamic_manufacturing_environments.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-In2x-at-WMT25-Translation-Task"><a href="#80-In2x-at-WMT25-Translation-Task" class="headerlink" title="80. In2x at WMT25 Translation Task"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/In2x_at_WMT25_Translation_Task.pdf">In2x at WMT25 Translation Task</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Duxiaoman</span></p><p>该论文提出了In2x，一种以日语为核心的通用大语言模型翻译范式，涵盖数据构建、对齐、奖励模型等流程，强调将英语的优势迁移到日语，并通过多阶段预训练、监督微调和强化学习提升模型在低资源语言中的表现及文化表达能力。结果显示，In2x在WMT25比赛中无需任务特定微调便在日语相关赛道取得第一，显著优于多款主流专有模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/In2x_at_WMT25_Translation_Task.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-DuPO-Enabling-Reliable-LLM-Self-Verification-via-Dual-Preference-Optimization"><a href="#81-DuPO-Enabling-Reliable-LLM-Self-Verification-via-Dual-Preference-Optimization" class="headerlink" title="81. DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/DuPO__Enabling_Reliable_LLM_Self-Verification_via_Dual_Preference_Optimization.pdf">DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ByteDance Seed</span></p><p>该论文提出DuPO框架，通过广义双任务学习分解输入为已知和未知部分，并用LLM同时进行主任务和对偶任务，自动生成自监督反馈信号，无需人工标注或外部奖励。实验表明，DuPO在多语种翻译和数学推理任务显著提升了模型性能，并且可以作为推理时无训练重排序机制，帮助小模型超过大模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/DuPO_Enabling_Reliable_LLM_Self-Verification_via_Dual_Preference_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-Automated-Optimization-Modeling-through-Expert-Guided-Large-Language-Model-Reasoning"><a href="#82-Automated-Optimization-Modeling-through-Expert-Guided-Large-Language-Model-Reasoning" class="headerlink" title="82. Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Automated_Optimization_Modeling_through_Expert-Guided_Large_Language_Model_Reasoning.pdf">Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ZJU-UIUC Institute, Zhejiang University</span></p><p>本文提出ORThought框架，通过专家级优化建模原则结合链式思维推理，实现优化建模流程的自动化。方法包括系统化数据集修正与扩充、新物流领域基准LogiOR、模型与求解双代理协同，以及智能错误诊断修复。实验表明ORThought在多类型复杂优化问题上准确率和效率均优于现有多智能体及推理基线方法，尤其在整数与非线性优化建模上提升显著。结论指出，ORThought显著提升了大模型自动优化建模性能，且保持低计算消耗，但在约束表达和复杂问题上仍具挑战，未来可结合RAG检索增强与人机协作进一步提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Automated_Optimization_Modeling_through_Expert-Guided_Large_Language_Model_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-DEPTH-Hallucination-Free-Relation-Extraction-via-Dependency-Aware-Sentence-Simplification-and-Two-tiered-Hierarchical-Refinement"><a href="#83-DEPTH-Hallucination-Free-Relation-Extraction-via-Dependency-Aware-Sentence-Simplification-and-Two-tiered-Hierarchical-Refinement" class="headerlink" title="83. DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/DEPTH__Hallucination-Free_Relation_Extraction_via_Dependency-Aware_Sentence_Simplification_and_Two-t.pdf">DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出DEPTH框架，通过依赖句法分析实现面向实体对的句子简化，并结合因果奖励建模的RLHF微调，在关系抽取中显著减少LLM幻觉问题。方法包括两阶段流程：首先用Grounding模块提取依赖路径简化句子并预测局部关系，然后用Refinement模块整合全局语义进行自校正和一致性检查。实验表明DEPTH在六大数据集上将幻觉率降至7.0%，F1得分提升17.2%，有效提升关系抽取的准确性与鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/DEPTH_Hallucination-Free_Relation_Extraction_via_Dependency-Aware_Sentence_Simplification_and_Two-tiered_Hierarchical_Refinement.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-ZPD-SCA-Unveiling-the-Blind-Spots-of-LLMs-in-Assessing-Students’-Cognitive-Abilities"><a href="#84-ZPD-SCA-Unveiling-the-Blind-Spots-of-LLMs-in-Assessing-Students’-Cognitive-Abilities" class="headerlink" title="84. ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students’ Cognitive Abilities"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/ZPD-SCA__Unveiling_the_Blind_Spots_of_LLMs_in_Assessing_Students'_Cognitive_Abilities.pdf">ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students’ Cognitive Abilities</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Psychology, South China Normal University</span></p><p>本文提出ZPD-SCA数据集及多阶段学生认知能力适配性评估任务，专注于评估大语言模型（LLM）对不同学段学生中文阅读理解难度的判断能力。通过60位特级教师标注，实验证明LLM在零样本条件下表现较差，加入上下文示例后准确率显著提升，但依然存在系统性偏差和跨级误判，反映出当前LLM训练中对教育任务的忽视。结论指出需将认知对齐任务纳入LLM训练以提升其教育适应性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/ZPD-SCA_Unveiling_the_Blind_Spots_of_LLMs_in_Assessing_Students%E2%80%99_Cognitive_Abilities.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-Organ-Agents-Virtual-Human-Physiology-Simulator-via-LLMs"><a href="#85-Organ-Agents-Virtual-Human-Physiology-Simulator-via-LLMs" class="headerlink" title="85. Organ-Agents: Virtual Human Physiology Simulator via LLMs"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Organ-Agents__Virtual_Human_Physiology_Simulator_via_LLMs.pdf">Organ-Agents: Virtual Human Physiology Simulator via LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tianjin University</span></p><p>本文提出Organ-Agents，一种基于大语言模型（LLM）的多智能体框架，用于模拟人体多器官系统的生理动态。方法上，将每个器官系统建模为独立的LLM智能体，先进行系统特异的有监督微调，再通过强化学习驱动的Correlator智能体实现智能体间动态协作，并引入残差补偿机制增强长期模拟稳定性。实验在MIMIC-IV及多中心ICU数据集上显示，模型在各系统均获得低MSE，能准确复现多系统临床事件链，在分布迁移和疾病严重度分层下表现稳健，能生成具有临床可解释性的反事实轨迹，专家评价其轨迹真实且生理一致。结论：Organ-Agents为临床数字孪生、诊断推理与治疗模拟提供了可信赖且可解释的解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Organ-Agents_Virtual_Human_Physiology_Simulator_via_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="86-Multimodal-Quantum-Vision-Transformer-for-Enzyme-Commission-Classification-from-Biochemical-Representations"><a href="#86-Multimodal-Quantum-Vision-Transformer-for-Enzyme-Commission-Classification-from-Biochemical-Representations" class="headerlink" title="86. Multimodal Quantum Vision Transformer for Enzyme Commission Classification from Biochemical Representations"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Multimodal_Quantum_Vision_Transformer_for_Enzyme_Commission_Classification_from_Biochemical_Represen.pdf">Multimodal Quantum Vision Transformer for Enzyme Commission Classification from Biochemical Representations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Purdue University</span></p><p>本文提出了多模态量子视觉Transformer（QVT）框架，通过融合蛋白质序列嵌入、量子电子描述符、分子图结构和2D分子图像，实现了酶功能（EC号）分类。方法采用各模态专用编码器和交叉注意力融合，有效捕捉酶的立体电子与结构特征。实验结果显示，QVT模型在多模态融合下Top-1准确率提升至85.1%，显著优于单模态和其他量子机器学习模型，表明融合多种生化信息能提升酶功能预测的准确性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Multimodal_Quantum_Vision_Transformer_for_Enzyme_Commission_Classification_from_Biochemical_Representations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-AFABench-A-Generic-Framework-for-Benchmarking-Active-Feature-Acquisition"><a href="#87-AFABench-A-Generic-Framework-for-Benchmarking-Active-Feature-Acquisition" class="headerlink" title="87. AFABench: A Generic Framework for Benchmarking Active Feature Acquisition"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/AFABench__A_Generic_Framework_for_Benchmarking_Active_Feature_Acquisition.pdf">AFABench: A Generic Framework for Benchmarking Active Feature Acquisition</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Chalmers University of Technology</span></p><p>本文提出了AFABench，这是首个用于主动特征获取（AFA）方法的标准化评测框架，支持多种贪心、静态和基于强化学习的特征获取策略。作者在多样的真实与合成数据集上系统实现并评测了这些算法，并引入了新型数据集AFAContext以测试非贪心策略。实验结果显示，虽然非贪心（强化学习）方法在特殊结构数据上有潜在优势，但实际中贪心判别方法在多数真实任务上表现更优且训练更高效，静态方法在部分场景下也具竞争力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/AFABench_A_Generic_Framework_for_Benchmarking_Active_Feature_Acquisition.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-ECHO-Frequency-aware-Hierarchical-Encoding-for-Variable-length-Signal"><a href="#88-ECHO-Frequency-aware-Hierarchical-Encoding-for-Variable-length-Signal" class="headerlink" title="88. ECHO: Frequency-aware Hierarchical Encoding for Variable-length Signal"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/ECHO__Frequency-aware_Hierarchical_Encoding_for_Variable-length_Signal.pdf">ECHO: Frequency-aware Hierarchical Encoding for Variable-length Signal</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science, Wuhan University</span></p><p>本文提出了ECHO模型，一种适用于变长机器信号（如声学和振动）的频率感知分层编码基础模型。其方法融合了自适应频带切分、相对频率位置编码和层次化Transformer结构，无需信号填充或分割即可生成高质量嵌入，并在SIREN基准测试（涵盖DCASE多年份挑战与多模态故障数据集）中实现了异常检测和故障识别领域的最新性能。结论表明，ECHO兼具跨领域泛化能力和任务性能，优于现有通用音频模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/ECHO_Frequency-aware_Hierarchical_Encoding_for_Variable-length_Signal.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-Improving-Fairness-in-Graph-Neural-Networks-via-Counterfactual-Debiasing"><a href="#89-Improving-Fairness-in-Graph-Neural-Networks-via-Counterfactual-Debiasing" class="headerlink" title="89. Improving Fairness in Graph Neural Networks via Counterfactual Debiasing"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Improving_Fairness_in_Graph_Neural_Networks_via_Counterfactual_Debiasing.pdf">Improving Fairness in Graph Neural Networks via Counterfactual Debiasing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tianjin University</span></p><p>本文提出了一种基于反事实数据增强的公平性提升方法Fair-ICD，通过在消息传递前利用反事实邻域增强，提升图结构异质性，获得无偏节点表示，并结合对抗判别器减少GNN分类器预测中的偏见。实验结果表明，Fair-ICD在多个基准数据集和三种GNN骨干上显著提升了公平性指标，同时保持高准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Improving_Fairness_in_Graph_Neural_Networks_via_Counterfactual_Debiasing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-Cross-Modality-Controlled-Molecule-Generation-with-Diffusion-Language-Model"><a href="#90-Cross-Modality-Controlled-Molecule-Generation-with-Diffusion-Language-Model" class="headerlink" title="90. Cross-Modality Controlled Molecule Generation with Diffusion Language Model"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Cross-Modality_Controlled_Molecule_Generation_with_Diffusion_Language_Model.pdf">Cross-Modality Controlled Molecule Generation with Diffusion Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Brandeis University</span></p><p>该论文提出了CMCM-DLM方法，通过在扩散模型基础上集成结构控制模块（SCM）和属性控制模块（PCM），实现了分两阶段的分子生成流程：先在早期扩散步骤注入结构约束，后期结合化学属性控制，显著提升多模态约束下新分子的生成效率和适应性。实验结果表明，该方法在三大分子数据集上平均提升属性指标16%、结构保真度约79%，为药物发现领域建立了新的多模态分子生成基准。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Cross-Modality_Controlled_Molecule_Generation_with_Diffusion_Language_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-Addressing-Graph-Anomaly-Detection-via-Causal-Edge-Separation-and-Spectrum"><a href="#91-Addressing-Graph-Anomaly-Detection-via-Causal-Edge-Separation-and-Spectrum" class="headerlink" title="91. Addressing Graph Anomaly Detection via Causal Edge Separation and Spectrum"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Addressing_Graph_Anomaly_Detection_via_Causal_Edge_Separation_and_Spectrum.pdf">Addressing Graph Anomaly Detection via Causal Edge Separation and Spectrum</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tianjin University</span></p><p>本文提出了CES2-GAD，一种基于因果边分离与谱混合滤波的图异常点检测方法。具体做法是首先通过因果干预将原始图拆分为同质和异质边，随后用低通和高通谱滤波分别提取不同频域信号，最后将多种信号表示拼接后输入分类器判断异常节点。实验证明该方法在真实数据集上对异质图异常检测优于现有主流方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Addressing_Graph_Anomaly_Detection_via_Causal_Edge_Separation_and_Spectrum.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-LeanGeo-Formalizing-Competitional-Geometry-Problems-in-Lean"><a href="#92-LeanGeo-Formalizing-Competitional-Geometry-Problems-in-Lean" class="headerlink" title="92. LeanGeo: Formalizing Competitional Geometry Problems in Lean"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/LeanGeo__Formalizing_Competitional_Geometry_problems_in_Lean.pdf">LeanGeo: Formalizing Competitional Geometry Problems in Lean</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Moonshot AI</span></p><p>该论文提出了LeanGeo，首个可在Lean 4定理证明器中形式化和求解竞赛级几何问题的统一系统，构建了包含260个高阶几何定理的库，并深度集成Mathlib，实现跨领域严谨推理。此外，作者提出了LeanGeo-Bench基准，包含122道国际数学奥林匹克（IMO）等来源的几何题，并系统评估了多种主流大语言模型在此基准上的表现，发现现有模型在复杂几何推理任务上仍有很大提升空间。结论认为LeanGeo及其基准为自动化几何定理证明和神经符号推理提供了可扩展的测试平台，但当前LLM在此类任务上的能力有限，需进一步研究。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/LeanGeo_Formalizing_Competitional_Geometry_Problems_in_Lean.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-Clinical-semantics-for-lung-cancer-prediction"><a href="#93-Clinical-semantics-for-lung-cancer-prediction" class="headerlink" title="93. Clinical semantics for lung cancer prediction"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Clinical_semantics_for_lung_cancer_prediction.pdf">Clinical semantics for lung cancer prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Erasmus University Medical Center</span></p><p>本论文提出将SNOMED医学术语层次结构映射为低维Poincaré超球体嵌入，结合ResNet和Transformer深度学习模型，用于提升肺癌发病预测。实验结果显示，集成Poincaré嵌入能在大规模电子健康记录数据集上相较于传统欧氏嵌入取得稳定但有限的判别提升，模型校准表现也较优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Clinical_semantics_for_lung_cancer_prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-DualNILM-Energy-Injection-Identification-Enabled-Disaggregation-with-Deep-Multi-Task-Learning"><a href="#94-DualNILM-Energy-Injection-Identification-Enabled-Disaggregation-with-Deep-Multi-Task-Learning" class="headerlink" title="94. DualNILM: Energy Injection Identification Enabled Disaggregation with Deep Multi-Task Learning"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/DualNILM__Energy_Injection_Identification_Enabled_Disaggregation_with_Deep_Multi-Task_Learning.pdf">DualNILM: Energy Injection Identification Enabled Disaggregation with Deep Multi-Task Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong, Shenzhen</span></p><p>该论文提出了DualNILM，一个基于深度多任务学习的非侵入式负载监测（NILM）新框架，专门针对家庭光伏等后端注入式能源场景。方法采用Transformer架构，融合序列到点和序列到序列模型，实现了对电器状态识别和能源注入量估计的联合建模。实验覆盖真实实验室数据与合成公共数据集，结果显示DualNILM在复杂注入环境下显著优于传统及主流深度学习方法，具备强泛化能力和实际部署价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/DualNILM_Energy_Injection_Identification_Enabled_Disaggregation_with_Deep_Multi-Task_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-Artificial-Intelligence-Based-Multiscale-Temporal-Modeling-for-Anomaly-Detection-in-Cloud-Services"><a href="#95-Artificial-Intelligence-Based-Multiscale-Temporal-Modeling-for-Anomaly-Detection-in-Cloud-Services" class="headerlink" title="95. Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Artificial_Intelligence-Based_Multiscale_Temporal_Modeling_for_Anomaly_Detection_in_Cloud_Services.pdf">Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Southern California</span></p><p>本文提出了一种基于Transformer架构并集成多尺度特征感知的异常检测方法，专为云服务高维监控数据设计。方法通过改进的Transformer进行时序建模，并引入多尺度特征构建、注意力加权融合模块，实现对不同时间粒度下异常模式的鲁棒建模。实验结果显示，该方法在精度、召回率、AUC和F1-score等指标上均优于主流模型，且在各种扰动条件下保持稳定性能，验证了其在复杂云环境下的优越性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Artificial_Intelligence-Based_Multiscale_Temporal_Modeling_for_Anomaly_Detection_in_Cloud_Services.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-Detecting-Reading-Induced-Confusion-Using-EEG-and-Eye-Tracking"><a href="#96-Detecting-Reading-Induced-Confusion-Using-EEG-and-Eye-Tracking" class="headerlink" title="96. Detecting Reading-Induced Confusion Using EEG and Eye Tracking"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Detecting_Reading-Induced_Confusion_Using_EEG_and_Eye_Tracking.pdf">Detecting Reading-Induced Confusion Using EEG and Eye Tracking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Berkeley</span></p><p>本论文提出了一种多模态框架，通过整合脑电（EEG）和眼动追踪，实现自然段落阅读过程中混淆状态的检测。方法包括采集高密度EEG与眼动数据，提取神经与行为特征，并使用XGBoost与CNN进行单模态与多模态分类，融合模型准确率达77.29%，最佳个体准确率89.55%。结论显示，EEG和眼动信号可有效识别阅读引发的混淆，为可穿戴神经适应界面和个性化学习工具提供基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Detecting_Reading-Induced_Confusion_Using_EEG_and_Eye_Tracking.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-Mamba2-Meets-Silence-Robust-Vocal-Source-Separation-for-Sparse-Regions"><a href="#97-Mamba2-Meets-Silence-Robust-Vocal-Source-Separation-for-Sparse-Regions" class="headerlink" title="97. Mamba2 Meets Silence: Robust Vocal Source Separation for Sparse Regions"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Mamba2_Meets_Silence__Robust_Vocal_Source_Separation_for_Sparse_Regions.pdf">Mamba2 Meets Silence: Robust Vocal Source Separation for Sparse Regions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kwangwoon University</span></p><p>本文提出了BSMamba2模型，用于高效且准确地分离音乐中的人声信号。方法上，BSMamba2将最新的Mamba2状态空间模块与频带分割和双路径结构结合，替换传统Transformer模块，显著提升了对长时序和稀疏人声片段的建模能力。实验表明，该方法在MUSDB18HQ等数据集上表现优异，cSDR达到11.03 dB，优于现有主流模型，并在不同片段长度和稀疏人声情况下均表现出稳定优势。结论：BSMamba2在音频高分辨率处理和人声稀疏分离场景下性能优越，有望推动音频分离领域的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Mamba2_Meets_Silence_Robust_Vocal_Source_Separation_for_Sparse_Regions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-Generative-AI-Against-Poaching-Latent-Composite-Flow-Matching-for-Wildlife-Conservation"><a href="#98-Generative-AI-Against-Poaching-Latent-Composite-Flow-Matching-for-Wildlife-Conservation" class="headerlink" title="98. Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Generative_AI_Against_Poaching__Latent_Composite_Flow_Matching_for_Wildlife_Conservation.pdf">Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harvard University</span></p><p>该论文提出WILDFLOW框架，首次将生成式AI（基于潜在复合流匹配模型）用于偷猎预测，显式结合生态占据模型与图神经网络以建模复杂空间模式，并通过复合基线初始化提升小样本泛化能力。实验证明WILDFLOW在乌干达两个国家公园的数据集上AUPR分别提升7.0%和10.2%，有效应对观测偏差和数据稀缺难题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Generative_AI_Against_Poaching_Latent_Composite_Flow_Matching_for_Wildlife_Conservation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025/08/21报纸</title>
    <link href="/2025/08/2025-08-21_article/"/>
    <url>/2025/08/2025-08-21_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2025/08/21报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-21<br>📄 <strong>发现论文数量</strong>：111  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Learning-from-Preferences-and-Mixed-Demonstrations-in-General-Settings"><a href="#1-Learning-from-Preferences-and-Mixed-Demonstrations-in-General-Settings" class="headerlink" title="1. Learning from Preferences and Mixed Demonstrations in General Settings"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Learning_from_Preferences_and_Mixed_Demonstrations_in_General_Settings.pdf">Learning from Preferences and Mixed Demonstrations in General Settings</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cambridge</span></p><p>本文提出了RRPO（Reward-Rational Partial Orderings）理论框架，将人类偏好和混合示范数据统一编码为部分序关系，并据此设计了LEOPARD算法，实现从偏好反馈、正&#x2F;负示范及其排序中联合学习奖励函数。实验表明，LEOPARD在多种RL环境下显著超越现有基线方法，且混合多种反馈类型优于单一反馈，验证了方法的通用性及高效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Learning_from_Preferences_and_Mixed_Demonstrations_in_General_Settings.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-COMPUTERRL-Scaling-End-to-End-Online-Reinforcement-Learning-for-Computer-Use-Agents"><a href="#2-COMPUTERRL-Scaling-End-to-End-Online-Reinforcement-Learning-for-Computer-Use-Agents" class="headerlink" title="2. COMPUTERRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ComputerRL__Scaling_End-to-End_Online_Reinforcement_Learning_for_Computer_Use_Agents.pdf">COMPUTERRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>该论文提出了COMPUTERRL框架，实现了API-GUI交互范式，自动化API构建和分布式虚拟桌面环境，大幅提升桌面智能体在复杂数字工作空间的操作能力。通过分布式RL基础设施和Entropulse交替训练策略，有效缓解训练中的熵塌缩，实现高效大规模RL训练，并在OSWorld基准测试上取得了48.1%的新SOTA准确率，显著提升了桌面自动化智能体的泛化和执行效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/COMPUTERRL_Scaling_End-to-End_Online_Reinforcement_Learning_for_Computer_Use_Agents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Convergent-Reinforcement-Learning-Algorithms-for-Stochastic-Shortest-Path-Problem"><a href="#3-Convergent-Reinforcement-Learning-Algorithms-for-Stochastic-Shortest-Path-Problem" class="headerlink" title="3. Convergent Reinforcement Learning Algorithms for Stochastic Shortest Path Problem"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Convergent_Reinforcement_Learning_Algorithms_for_Stochastic_Shortest_Path_Problem.pdf">Convergent Reinforcement Learning Algorithms for Stochastic Shortest Path Problem</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indian Institute of Science</span></p><p>本文针对随机最短路径（SSP）问题，提出了两种表格式（tabular）两时间尺度算法（Actor-Critic与Critic-Actor）和一种函数逼近（Function Approximation）版本的Actor-Critic算法。所有算法均在理论上证明了渐近几乎必然收敛，并在实验中表现优于Q-Learning和SARSA，尤其在函数逼近场景下表现出更可靠的收敛性，填补了现有算法在SSP问题上的收敛可靠性空白。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Convergent_Reinforcement_Learning_Algorithms_for_Stochastic_Shortest_Path_Problem.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-Categorical-Policies-Multimodal-Policy-Learning-and-Exploration-in-Continuous-Control"><a href="#4-Categorical-Policies-Multimodal-Policy-Learning-and-Exploration-in-Continuous-Control" class="headerlink" title="4. Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Categorical_Policies__Multimodal_Policy_Learning_and_Exploration_in_Continuous_Control.pdf">Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Texas at Arlington</span></p><p>本文提出了一种新的连续控制多模态策略学习方法——Categorical Policies，通过中间的多个类别分布变量对行为模式进行建模，并采用STE和Gumbel-Softmax两种可微分采样方案，使策略能够表达多模态行为且保持梯度优化。实验在DeepMind Control Suite多个任务上表明，该方法能更快收敛并优于传统高斯单模态策略，尤其在探索和任务适应性上有显著提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Categorical_Policies_Multimodal_Policy_Learning_and_Exploration_in_Continuous_Control.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-Reinforcement-Learning-based-Adaptive-Path-Selection-for-Programmable-Networks"><a href="#5-Reinforcement-Learning-based-Adaptive-Path-Selection-for-Programmable-Networks" class="headerlink" title="5. Reinforcement Learning-based Adaptive Path Selection for Programmable Networks"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Reinforcement_Learning-based_Adaptive_Path_Selection_for_Programmable_Networks.pdf">Reinforcement Learning-based Adaptive Path Selection for Programmable Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Informatics, University of Amsterdam</span></p><p>本文提出了一种分布式、可嵌入数据面的强化学习（IN-RL）方法，用于可编程网络中的自适应路径选择。方法结合了轻量级随机学习自动机（SLA）算法和实时带内网络遥测（INT）收集的拥塞指标，在P4可编程交换机上实现了本地、数据驱动的转发决策，能根据队列长度和出队延迟动态调整流量路径。实验结果表明，SLA机制可稳定收敛于高效路径，并对网络状态变化做出快速响应，且额外开销极低。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Reinforcement_Learning-based_Adaptive_Path_Selection_for_Programmable_Networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-Revisiting-Diffusion-Q-Learning-From-Iterative-Denoising-to-One-Step-Action-Generation"><a href="#6-Revisiting-Diffusion-Q-Learning-From-Iterative-Denoising-to-One-Step-Action-Generation" class="headerlink" title="6. Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Revisiting_Diffusion_Q-Learning__From_Iterative_Denoising_to_One-Step_Action_Generation.pdf">Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea Advanced Institute of Science and Technology</span></p><p>该论文提出了One-Step Flow Q-Learning (OFQL)，通过在Flow Matching框架下学习平均速度场，实现了训练和推理阶段高效的一步动作生成，无需多步去噪、辅助模型或多阶段训练。实验结果显示，OFQL在D4RL基准上显著超越当前扩散模型与Q学习方法，同时大幅提升训练与推理速度，简化了离线强化学习流程。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Revisiting_Diffusion_Q-Learning_From_Iterative_Denoising_to_One-Step_Action_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-MACTAS-Self-Attention-Based-Module-for-Inter-Agent-Communication-in-Multi-Agent-Reinforcement-Learning"><a href="#7-MACTAS-Self-Attention-Based-Module-for-Inter-Agent-Communication-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="7. MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MACTAS__Self-Attention-Based_Module_for_Inter-Agent_Communication_in_Multi-Agent_Reinforcement_Learn.pdf">MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Warsaw</span></p><p>本文提出了一种基于Transformer自注意力机制的多智能体通信模块MACTAS，可无缝集成到任意动作-价值函数分解方法中，实现全可微的奖励驱动通信。实验在SMAC基准上验证了MACTAS在多个难度场景下取得了SOTA或媲美SOTA的表现，并证明了其参数量与智能体数量无关且易于部署。结论是MACTAS显著提升了多智能体强化学习中的通信与协作效率，尤其在大规模场景下表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MACTAS_Self-Attention-Based_Module_for_Inter-Agent_Communication_in_Multi-Agent_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-Breaking-the-SFT-Plateau-Multimodal-Structured-Reinforcement-Learning-for-Chart-to-Code-Generation"><a href="#8-Breaking-the-SFT-Plateau-Multimodal-Structured-Reinforcement-Learning-for-Chart-to-Code-Generation" class="headerlink" title="8. Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Breaking_the_SFT_Plateau__Multimodal_Structured_Reinforcement_Learning_for_Chart-to-Code_Generation.pdf">Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Meituan</span></p><p>本文提出了多模态结构化强化学习（MSRL）方法，针对图表转代码任务，结合文本与视觉多粒度奖励机制，通过两阶段训练流程对多模态大模型进行优化。实验表明，MSRL显著突破了监督微调（SFT）性能瓶颈，在ChartMimic与ReachQA基准上分别提升高层指标6.2%和9.9%，超越所有开源模型并接近闭源大模型水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Breaking_the_SFT_Plateau_Multimodal_Structured_Reinforcement_Learning_for_Chart-to-Code_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-MuFlex-A-Scalable-Physics-based-Platform-for-Multi-Building-Flexibility-Analysis-and-Coordination"><a href="#9-MuFlex-A-Scalable-Physics-based-Platform-for-Multi-Building-Flexibility-Analysis-and-Coordination" class="headerlink" title="9. MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MuFlex__A_Scalable,_Physics-based_Platform_for_Multi-Building_Flexibility_Analysis_and_Coordination.pdf">MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University College London</span></p><p>本文提出了MuFlex，一个基于物理模型、可扩展的多建筑柔性协调开源测试平台，通过同步多座EnergyPlus建筑模型与最新OpenAI Gym标准接口，支持多种控制策略（如强化学习和MPC）的训练与基准测试。案例研究采用Soft Actor–Critic算法实现多建筑负荷灵活性协调，结果显示平台可有效保持建筑能效与舒适度，同时将总峰值负荷控制在指定阈值以下。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MuFlex_A_Scalable_Physics-based_Platform_for_Multi-Building_Flexibility_Analysis_and_Coordination.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-CrafterDojo-A-Suite-of-Foundation-Models-for-Building-Open-Ended-Embodied-Agents-in-Crafter"><a href="#10-CrafterDojo-A-Suite-of-Foundation-Models-for-Building-Open-Ended-Embodied-Agents-in-Crafter" class="headerlink" title="10. CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/CrafterDojo__A_Suite_of_Foundation_Models_for_Building_Open-Ended_Embodied_Agents_in_Crafter.pdf">CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KAIST</span></p><p>该论文提出CrafterDojo，一套为Crafter环境打造的基础模型和工具，包括C-VPT（行为先验）、C-CLIP（视觉-语言对齐）、C-Steve-1（指令跟随）以及自动化数据生成工具CrafterPlay和CrafterCaption。实验表明，这些模型和数据集能高效支持基础模型训练，模型在行为生成、视觉-语言理解和指令执行等任务上表现优异，并通过分层规划实现多样且长时序任务，证明CrafterDojo可作为轻量级通用智能体研究平台。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/CrafterDojo_A_Suite_of_Foundation_Models_for_Building_Open-Ended_Embodied_Agents_in_Crafter.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="11-Multimodal-Data-Storage-and-Retrieval-for-Embodied-AI-A-Survey"><a href="#11-Multimodal-Data-Storage-and-Retrieval-for-Embodied-AI-A-Survey" class="headerlink" title="11. Multimodal Data Storage and Retrieval for Embodied AI: A Survey"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Multimodal_Data_Storage_and_Retrieval_for_Embodied_AI__A_Survey.pdf">Multimodal Data Storage and Retrieval for Embodied AI: A Survey</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China Normal University</span></p><p>本文系统性综述了面向Embodied AI（具身智能）多模态数据存储与检索的五大架构（图数据库、多模型数据库、数据湖、向量数据库、时序数据库）及五大检索范式（融合策略、表示对齐、图结构、生成模型、高效优化）。论文指出，当前技术在物理语境建模、跨模态集成、动态适应和开放世界泛化等方面存在核心瓶颈，并提出未来需发展物理感知数据模型、协同优化存储与检索、标准化基准测试等方向。结论认为，具身智能的数据管理需面向物理世界特点创新，当前技术迁移仍不足以满足实际需求。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Multimodal_Data_Storage_and_Retrieval_for_Embodied_AI_A_Survey.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-Driving-Style-Recognition-Like-an-Expert-Using-Semantic-Privileged-Information-from-Large-Language-Models"><a href="#12-Driving-Style-Recognition-Like-an-Expert-Using-Semantic-Privileged-Information-from-Large-Language-Models" class="headerlink" title="12. Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Driving_Style_Recognition_Like_an_Expert_Using_Semantic_Privileged_Information_from_Large_Language_M.pdf">Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>该论文提出了LUSPI框架，将大语言模型（LLM）生成的驾驶行为语义信息作为特权信息，通过DriBehavGPT模块自动生成专家级语义描述，再经文本嵌入和降维后作为SVM+训练输入。方法在真实驾驶数据中显著提升了驾驶风格识别准确性，F1分数在跟车和变道场景分别提升7.6%和7.9%；部署阶段仅用传感器数据实现高效推断。结论显示语义行为表征能有效改善模型性能并提升解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Driving_Style_Recognition_Like_an_Expert_Using_Semantic_Privileged_Information_from_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-Online-3D-Gaussian-Splatting-Modeling-with-Novel-View-Selection"><a href="#13-Online-3D-Gaussian-Splatting-Modeling-with-Novel-View-Selection" class="headerlink" title="13. Online 3D Gaussian Splatting Modeling with Novel View Selection"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Online_3D_Gaussian_Splatting_Modeling_with_Novel_View_Selection.pdf">Online 3D Gaussian Splatting Modeling with Novel View Selection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Dongguk University</span></p><p>本文提出了一种在线3D高斯Splatting建模方法，通过自适应视角选择显著提升模型的完整性。关键技术包括利用高斯形状和位置梯度的不确定性估计来选取信息增益最大的非关键帧，并结合多视图立体（MVS）网络与全局Bundle Adjustment优化，显著提升了室内外场景的渲染质量和模型鲁棒性。实验结果表明，该方法在复杂户外环境下性能优于现有主流SLAM方法，首次实现了3DGS框架下的非关键帧选择，有效提升了场景建模的完整性和精度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Online_3D_Gaussian_Splatting_Modeling_with_Novel_View_Selection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Toward-Deployable-Multi-Robot-Collaboration-via-a-Symbolically-Guided-Decision-Transformer"><a href="#14-Toward-Deployable-Multi-Robot-Collaboration-via-a-Symbolically-Guided-Decision-Transformer" class="headerlink" title="14. Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Toward_Deployable_Multi-Robot_Collaboration_via_a_Symbolically-Guided_Decision_Transformer.pdf">Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Purdue University</span></p><p>本文提出了Symbolically-Guided Decision Transformer（SGDT）框架，将神经符号规划器与因果Transformer结合，实现了多机器人协作的高效决策。方法包括使用PDDL与开源大语言模型生成高层次符号子目标，再由目标条件决策Transformer完成低层次动作决策。实验表明SGDT在仿真多机器人任务（如三明治制作和杂货打包）中具备优异的任务成功率、强大的零样本与少样本泛化能力。该方法提升了多机器人协作的可部署性、解释性和泛化性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Toward_Deployable_Multi-Robot_Collaboration_via_a_Symbolically-Guided_Decision_Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-Trajectory-Tracking-and-Stabilization-of-Quadrotors-Using-Deep-Koopman-Model-Predictive-Control"><a href="#15-Trajectory-Tracking-and-Stabilization-of-Quadrotors-Using-Deep-Koopman-Model-Predictive-Control" class="headerlink" title="15. Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Trajectory_Tracking_and_Stabilization_of_Quadrotors_Using_Deep_Koopman_Model_Predictive_Control.pdf">Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Egypt-Japan University of Science and Technology</span></p><p>本论文提出了一种面向四旋翼系统的数据驱动控制框架，结合深度Koopman算子与模型预测控制（DK-MPC）。方法通过神经网络学习高维线性化表征，实现复杂非线性飞行动力学的建模，并将其嵌入MPC以高效优化控制输入。实验结果表明，DK-MPC在轨迹跟踪和点稳定任务中表现优于传统非线性MPC，兼具高精度与低计算成本，展示了其在实时嵌入式飞控系统的应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Trajectory_Tracking_and_Stabilization_of_Quadrotors_Using_Deep_Koopman_Model_Predictive_Control.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-Blast-Hole-Seeking-and-Dipping-–-The-Navigation-and-Perception-Framework-in-a-Mine-Site-Inspection-Robot"><a href="#16-Blast-Hole-Seeking-and-Dipping-–-The-Navigation-and-Perception-Framework-in-a-Mine-Site-Inspection-Robot" class="headerlink" title="16. Blast Hole Seeking and Dipping – The Navigation and Perception Framework in a Mine Site Inspection Robot"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Blast_Hole_Seeking_and_Dipping_--_The_Navigation_and_Perception_Framework_in_a_Mine_Site_Inspection_.pdf">Blast Hole Seeking and Dipping – The Navigation and Perception Framework in a Mine Site Inspection Robot</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Australian Centre for Robotics, The University of Sydney</span></p><p>该论文提出了一套用于露天矿坑爆破孔自动检测和导航的机器人感知与控制框架。方法包括基于多传感器（GPS、IMU、双LiDAR）融合、点云处理、虚拟深度图像生成、两阶段（粗到精）孔检测流程、FRST特征提取、RANSAC和Taubin圆拟合、非极大值抑制，以及自适应相机参数调整，实现了高效精准的爆破孔导航与传感器对准。实验表明，该系统无需数据驱动学习方法即可在真实矿区和仿真环境中实现可靠、精确的自动爆破孔定位与检测，显著提升了矿区检测效率与安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Blast_Hole_Seeking_and_Dipping_%E2%80%93_The_Navigation_and_Perception_Framework_in_a_Mine_Site_Inspection_Robot.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-AutoMPC-A-Code-Generator-for-MPC-based-Automated-Driving"><a href="#17-AutoMPC-A-Code-Generator-for-MPC-based-Automated-Driving" class="headerlink" title="17. AutoMPC: A Code Generator for MPC-based Automated Driving"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/AutoMPC__A_Code_Generator_for_MPC-based_Automated_Driving.pdf">AutoMPC: A Code Generator for MPC-based Automated Driving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Luebeck</span></p><p>本文提出了AutoMPC软件包，通过集成非线性主动集（NAS）算法与代码生成器，为自动驾驶车辆轨迹跟踪提供高效、可定制的MPC控制解决方案。AutoMPC可自动生成高效、无依赖的C代码，支持多种车辆模型和数值积分方法，便于在Matlab&#x2F;Simulink®和ROS等嵌入式平台上部署，具备高鲁棒性、实时性和自愈能力。论文通过多种仿真实验证明，AutoMPC能在保证解可行性的同时，满足大多数自动驾驶场景下对复杂动态、约束和切换工况的需求。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/AutoMPC_A_Code_Generator_for_MPC-based_Automated_Driving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-MimicFunc-Imitating-Tool-Manipulation-from-a-Single-Human-Video-via-Functional-Correspondence"><a href="#18-MimicFunc-Imitating-Tool-Manipulation-from-a-Single-Human-Video-via-Functional-Correspondence" class="headerlink" title="18. MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MimicFunc__Imitating_Tool_Manipulation_from_a_Single_Human_Video_via_Functional_Correspondence.pdf">MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southern University of Science and Technology</span></p><p>MimicFunc是一种机器人模仿工具操作的方法，通过分析单个人类RGB-D视频，分三步提取功能关键点、建立功能帧对应关系并生成基于功能帧的动作轨迹。实验证明，MimicFunc能让机器人从单一人类视频泛化到多种新工具及任务，并生成高质量数据高效训练视觉运动策略，无需繁重遥操作数据采集。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MimicFunc_Imitating_Tool_Manipulation_from_a_Single_Human_Video_via_Functional_Correspondence.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-A-Three-Level-Whole-Body-Disturbance-Rejection-Control-Framework-for-Dynamic-Motions-in-Legged-Robots"><a href="#19-A-Three-Level-Whole-Body-Disturbance-Rejection-Control-Framework-for-Dynamic-Motions-in-Legged-Robots" class="headerlink" title="19. A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/A_Three-Level_Whole-Body_Disturbance_Rejection_Control_Framework_for_Dynamic_Motions_in_Legged_Robot.pdf">A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huazhong University of Science and Technology</span></p><p>本文提出了一种三层全身扰动抑制控制框架（T-WB-DRC），通过集成移动视界扩展状态观测器（MH-ESO）与稳健模型预测控制（MPC），能有效估计并补偿腿式机器人在动态运动中遇到的模型不确定性、外部扰动与故障。该方法在Gazebo仿真和实物四足机器人实验中显示出卓越的鲁棒性和稳定性，显著提升了负载运输、扰动抑制和故障容错能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/A_Three-Level_Whole-Body_Disturbance_Rejection_Control_Framework_for_Dynamic_Motions_in_Legged_Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-Unified-Hierarchical-MPC-in-Task-Executing-for-Modular-Manipulators-across-Diverse-Morphologies"><a href="#20-Unified-Hierarchical-MPC-in-Task-Executing-for-Modular-Manipulators-across-Diverse-Morphologies" class="headerlink" title="20. Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Unified_Hierarchical_MPC_in_Task_Executing_for_Modular_Manipulators_across_Diverse_Morphologies.pdf">Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Istituto Italiano Di Tecnologia (IIT)</span></p><p>该论文提出了一种统一的分层模型预测控制（H-MPC）方法，可自动适应多种模块化机械臂形态，实现无需参数调优的任务执行。方法将控制过程分为高层MPC和低层MPC，高层负责预测未来状态并提供轨迹，低层基于高层信息更新模型以提升控制精度，同时隐式引入二阶动力学信息。实验验证显示，该方法在多种机械臂形态下具有更高的轨迹跟踪精度和鲁棒性，尤其在处理奇异点和实际抓取任务中表现优异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Unified_Hierarchical_MPC_in_Task_Executing_for_Modular_Manipulators_across_Diverse_Morphologies.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-Modeling-and-Control-of-AWOISV-A-Filtered-Tube-Based-MPC-Approach-for-Simultaneous-Tracking-of-Lateral-Position-and-Heading-Angle"><a href="#21-Modeling-and-Control-of-AWOISV-A-Filtered-Tube-Based-MPC-Approach-for-Simultaneous-Tracking-of-Lateral-Position-and-Heading-Angle" class="headerlink" title="21. Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Modeling_and_Control_of_AWOISV__A_Filtered_Tube-Based_MPC_Approach_for_Simultaneous_Tracking_of_Late.pdf">Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>本文提出了一种针对全轮全向独立转向车辆（AWOISV）的运动模式理论定义及切换标准，并建立了可应用于任意轴数AWOISV的广义v-β-r动态模型。基于此模型，设计了一种滤波管式线性时变MPC（FT-LTVMPC）控制策略，实现了车辆横向位置与任意航向角的同时高精度跟踪，具备对模型不确定性和参数扰动的强鲁棒性。仿真与硬件在环实验表明，该方法在多种速度和工况下均显著优于传统MPC，提升了跟踪精度和控制平滑性，满足实时控制需求。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Modeling_and_Control_of_AWOISV_A_Filtered_Tube-Based_MPC_Approach_for_Simultaneous_Tracking_of_Lateral_Position_and_Heading_Angle.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-Switch4EAI-Leveraging-Console-Game-Platform-for-Benchmarking-Robotic-Athletics"><a href="#22-Switch4EAI-Leveraging-Console-Game-Platform-for-Benchmarking-Robotic-Athletics" class="headerlink" title="22. Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Switch4EAI__Leveraging_Console_Game_Platform_for_Benchmarking_Robotic_Athletics.pdf">Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Georgia Institute of Technology</span></p><p>本文提出Switch4EAI管道，利用任天堂Switch的Just Dance游戏，通过视频流捕获、ROMP三维动作重建、GMR动作重定向等模块，将人类舞蹈动作映射到机器人并由机器人实际执行，使用内置评分系统进行定量评估。结果显示该方法低成本、易部署，可实现机器人与人类在运动能力上的直接比较，验证了商业游戏平台用于机器人运动基准测试的可行性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Switch4EAI_Leveraging_Console_Game_Platform_for_Benchmarking_Robotic_Athletics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-ROVER-Robust-Loop-Closure-Verification-with-Trajectory-Prior-in-Repetitive-Environments"><a href="#23-ROVER-Robust-Loop-Closure-Verification-with-Trajectory-Prior-in-Repetitive-Environments" class="headerlink" title="23. ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ROVER__Robust_Loop_Closure_Verification_with_Trajectory_Prior_in_Repetitive_Environments.pdf">ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CKS Robotics Institute, Hong Kong University of Science and Technology</span></p><p>本文提出了一种基于轨迹先验约束的循环闭合验证方法ROVER，用于解决重复环境下SLAM中外观特征失效导致的误检问题。方法流程为对每个循环候选，通过位姿图优化估计新轨迹，并与历史轨迹对齐，采用评分机制判断该循环约束是否被接受。实验结果表明，ROVER在多个公开和自采数据集上超越传统几何验证方法，能有效提升机器人定位鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ROVER_Robust_Loop_Closure_Verification_with_Trajectory_Prior_in_Repetitive_Environments.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-CAST-Counterfactual-Labels-Improve-Instruction-Following-in-Vision-Language-Action-Models"><a href="#24-CAST-Counterfactual-Labels-Improve-Instruction-Following-in-Vision-Language-Action-Models" class="headerlink" title="24. CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/CAST__Counterfactual_Labels_Improve_Instruction_Following_in_Vision-Language-Action_Models.pdf">CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California Berkeley</span></p><p>该论文提出CAST方法，通过利用视觉语言模型（VLM）为现有机器人轨迹数据生成反事实语言和动作标签，扩充数据集以提升视觉-语言-动作（VLA）模型的指令跟随能力。该方法无需额外人工采集数据，仅通过合成多样化的标签，强制VLA模型更好地理解并响应复杂自然语言指令。实验证明，采用CAST增强后的模型在真实环境中导航任务的指令跟随成功率提升了27%，并在多个基准任务上优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/CAST_Counterfactual_Labels_Improve_Instruction_Following_in_Vision-Language-Action_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="25-Beyond-Simple-Edits-Composed-Video-Retrieval-with-Dense-Modifications"><a href="#25-Beyond-Simple-Edits-Composed-Video-Retrieval-with-Dense-Modifications" class="headerlink" title="25. Beyond Simple Edits: Composed Video Retrieval with Dense Modifications"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Beyond_Simple_Edits__Composed_Video_Retrieval_with_Dense_Modifications.pdf">Beyond Simple Edits: Composed Video Retrieval with Dense Modifications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohamed bin Zayed University of AI</span></p><p>该论文提出Dense-WebVid-CoVR大规模细粒度视频检索数据集，包含1.6百万组视频及丰富的修改文本，显著提升了视频内容细粒度变化的描述能力。作者还提出了统一融合视觉和文本的跨注意力视频检索模型，能够精准对齐复杂的多模态查询与目标视频，在主流基准上取得SOTA性能，Recall@1达71.3%，显著优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Beyond_Simple_Edits_Composed_Video_Retrieval_with_Dense_Modifications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-InfiniteTalk-Audio-driven-Video-Generation-for-Sparse-Frame-Video-Dubbing"><a href="#26-InfiniteTalk-Audio-driven-Video-Generation-for-Sparse-Frame-Video-Dubbing" class="headerlink" title="26. InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/InfiniteTalk__Audio-driven_Video_Generation_for_Sparse-Frame_Video_Dubbing.pdf">InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Artificial Intelligence, University of Chinese Academy of Sciences</span></p><p>该论文提出了一种新的视频配音范式——稀疏帧视频配音(sparse-frame video dubbing)，通过保留关键帧来实现身份、情感和镜头运动的保持，同时利用音频驱动生成全身动态与情感同步的视频。核心方法InfiniteTalk采用流式音频驱动生成器，结合时序上下文帧实现顺滑片段衔接，并提出细粒度参考帧采样策略以平衡控制强度与运动对齐。实验表明，该方法在唇部、面部及全身运动同步方面达到SOTA效果，显著提升了长视频配音的自然度和沉浸感。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/InfiniteTalk_Audio-driven_Video_Generation_for_Sparse-Frame_Video_Dubbing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-GeoSAM2-Unleashing-the-Power-of-SAM2-for-3D-Part-Segmentation"><a href="#27-GeoSAM2-Unleashing-the-Power-of-SAM2-for-3D-Part-Segmentation" class="headerlink" title="27. GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/GeoSAM2__Unleashing_the_Power_of_SAM2_for_3D_Part_Segmentation.pdf">GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">VAST</span></p><p>该论文提出了GeoSAM2，一种基于2D交互提示的3D部件分割新框架，将3D分割任务转化为多视角2D掩码预测问题。方法利用LoRA和残差几何特征融合对SAM2进行高效适配，实现了对无纹理3D物体的精细分割，并通过多视图一致性和轻量后处理获得高精度的3D标签。实验表明，GeoSAM2在PartObjaverse-Tiny和PartNetE等公开基准上显著超过现有方法，具备较强的泛化和用户可控性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/GeoSAM2_Unleashing_the_Power_of_SAM2_for_3D_Part_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-Backdooring-Self-Supervised-Contrastive-Learning-by-Noisy-Alignment"><a href="#28-Backdooring-Self-Supervised-Contrastive-Learning-by-Noisy-Alignment" class="headerlink" title="28. Backdooring Self-Supervised Contrastive Learning by Noisy Alignment"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Backdooring_Self-Supervised_Contrastive_Learning_by_Noisy_Alignment.pdf">Backdooring Self-Supervised Contrastive Learning by Noisy Alignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southeast University</span></p><p>本论文提出了一种面向自监督对比学习的数据投毒后门攻击方法Noisy Alignment（NA），通过显式压缩中毒样本中的噪声成分，并将触发器与目标类别特征对齐，提升了后门攻击在图像对比学习中的有效性。作者将数据投毒过程建模为图像布局优化问题，理论推导最优参数，并通过大量实验表明该方法在不同数据集和模型下均取得了SOTA攻击成功率，且对常见后门防御表现出较强鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Backdooring_Self-Supervised_Contrastive_Learning_by_Noisy_Alignment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-ASDFormer-A-Transformer-with-Mixtures-of-Pooling-Classifier-Experts-for-Robust-Autism-Diagnosis-and-Biomarker-Discovery"><a href="#29-ASDFormer-A-Transformer-with-Mixtures-of-Pooling-Classifier-Experts-for-Robust-Autism-Diagnosis-and-Biomarker-Discovery" class="headerlink" title="29. ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ASDFormer__A_Transformer_with_Mixtures_of_Pooling-Classifier_Experts_for_Robust_Autism_Diagnosis_and.pdf">ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Isfahan University of Technology</span></p><p>本文提出了ASDFormer，一种结合Transformer与专家混合（MoE）解码器的新方法，用于自闭症谱系障碍（ASD）基于脑功能连接的fMRI数据分类和生物标志物发现。通过专家专化与注意力机制，ASDFormer不仅提升了分类准确率，还增强了解释性，能够识别与ASD相关的关键脑区和连接异常，实验在ABIDE数据集上达到最新性能并揭示了可靠的ASD生物标志模式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ASDFormer_A_Transformer_with_Mixtures_of_Pooling-Classifier_Experts_for_Robust_Autism_Diagnosis_and_Biomarker_Discovery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-ViT-FIQA-Assessing-Face-Image-Quality-using-Vision-Transformers"><a href="#30-ViT-FIQA-Assessing-Face-Image-Quality-using-Vision-Transformers" class="headerlink" title="30. ViT-FIQA: Assessing Face Image Quality using Vision Transformers"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ViT-FIQA__Assessing_Face_Image_Quality_using_Vision_Transformers.pdf">ViT-FIQA: Assessing Face Image Quality using Vision Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fraunhofer Institute for Computer Graphics Research IGD</span></p><p>该论文提出了一种基于视觉Transformer的新型人脸图像质量评估方法ViT-FIQA，通过在ViT骨干网络中引入可学习的质量token，实现对人脸图像可用于识别任务的质量分数回归。该方法将质量token与标准patch token拼接，并利用全局自注意力机制集成图像上下文信息，输出的质量token用于回归人脸质量分数，同时patch tokens用于识别任务。实验结果表明，ViT-FIQA在多个主流数据集和不同人脸识别模型下表现出色，特别是在姿态和质量挑战较大的数据集上，验证了Transformer架构建模人脸图像效用的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ViT-FIQA_Assessing_Face_Image_Quality_using_Vision_Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-Real-Time-Population-Based-Reconstruction-of-3D-Bone-Models-via-Very-Low-Dose-Protocols"><a href="#31-Real-Time-Population-Based-Reconstruction-of-3D-Bone-Models-via-Very-Low-Dose-Protocols" class="headerlink" title="31. Real-Time, Population-Based Reconstruction of 3D Bone Models via Very-Low-Dose Protocols"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Real-Time,_Population-Based_Reconstruction_of_3D_Bone_Models_via_Very-Low-Dose_Protocols.pdf">Real-Time, Population-Based Reconstruction of 3D Bone Models via Very-Low-Dose Protocols</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology</span></p><p>本文提出了一种半监督重建与知识蒸馏（SSR-KD）方法，通过深度神经网络结合隐式场、半监督学习和知识蒸馏，实现了从双平面X线快速重建高精度三维骨骼模型，仅需30秒，平均误差低于1.0mm，显著减少辐射暴露且无需CT和人工标注。临床评估表明，该自动重建的骨骼模型在外科手术规划和导板设计中与CT人工标注模型效果相当，大幅提升了骨模型在骨科应用的实用性和实时性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Real-Time_Population-Based_Reconstruction_of_3D_Bone_Models_via_Very-Low-Dose_Protocols.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-RED-AI-Id-Pattern-First-Results-of-Stone-Deterioration-Patterns-with-Multi-Agent-Systems"><a href="#32-RED-AI-Id-Pattern-First-Results-of-Stone-Deterioration-Patterns-with-Multi-Agent-Systems" class="headerlink" title="32. RED.AI Id-Pattern: First Results of Stone Deterioration Patterns with Multi-Agent Systems"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/RED.AI_Id-Pattern__First_Results_of_Stone_Deterioration_Patterns_with_Multi-Agent_Systems.pdf">RED.AI Id-Pattern: First Results of Stone Deterioration Patterns with Multi-Agent Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">STAP Reabilitação Estrutural, SA</span></p><p>本文提出了一种基于多智能体系统的石材劣化图像诊断方法，采用五类领域专家AI代理（岩石学家、病理学家、环境专家、修复师和诊断协调员），通过多模态LLM驱动结合RAG知识库分阶段实现单独分析、多学科讨论及共识整合。实验结果表明，Id-Pattern系统在准确率、召回率和F1得分上均较基础大模型有显著提升，尤其召回率接近翻倍，极大提升了石材劣化自动检测的全面性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/RED.AI_Id-Pattern_First_Results_of_Stone_Deterioration_Patterns_with_Multi-Agent_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-Extracting-Structured-Requirements-from-Unstructured-Building-Technical-Specifications-for-Building-Information-Modeling"><a href="#33-Extracting-Structured-Requirements-from-Unstructured-Building-Technical-Specifications-for-Building-Information-Modeling" class="headerlink" title="33. Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Extracting_Structured_Requirements_from_Unstructured_Building_Technical_Specifications_for_Building_.pdf">Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Arts et Métiers Institute of Technology</span></p><p>该论文提出将自然语言处理技术（如命名实体识别NER和关系抽取RE）与建筑信息建模（BIM）结合，通过CamemBERT等预训练变换器模型和转移学习方法自动从法语建筑技术规范（BTS）文档中提取结构化需求。实验结果显示，CamemBERT和Fr core news lg在NER任务的F1分数超过90%，随机森林模型在RE任务中F1分数超过80%，显著提升了BIM自动化验证系统的信息抽取效率和准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Extracting_Structured_Requirements_from_Unstructured_Building_Technical_Specifications_for_Building_Information_Modeling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-UNICON-UNIfied-CONtinual-Learning-for-Medical-Foundational-Models"><a href="#34-UNICON-UNIfied-CONtinual-Learning-for-Medical-Foundational-Models" class="headerlink" title="34. UNICON: UNIfied CONtinual Learning for Medical Foundational Models"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/UNICON__UNIfied_CONtinual_Learning_for_Medical_Foundational_Models.pdf">UNICON: UNIfied CONtinual Learning for Medical Foundational Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohamed bin Zayed University of Artificial Intelligence</span></p><p>该论文提出UNICON框架，实现了医学影像基础模型在多任务、多模态（如CT、PET和文本报告）和不同解剖区域上的统一持续学习。通过LoRA与适配器模块高效扩展模型能力，实验证明模型可在无需大规模重训练的情况下动态适应新任务和模态，提升如预后预测与分割等任务表现，同时避免灾难性遗忘。结论认为UNICON可提升医学影像基础模型的通用性和适应性，为医疗AI系统的泛化奠定基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/UNICON_UNIfied_CONtinual_Learning_for_Medical_Foundational_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Self-Supervised-Sparse-Sensor-Fusion-for-Long-Range-Perception"><a href="#35-Self-Supervised-Sparse-Sensor-Fusion-for-Long-Range-Perception" class="headerlink" title="35. Self-Supervised Sparse Sensor Fusion for Long Range Perception"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Self-Supervised_Sparse_Sensor_Fusion_for_Long_Range_Perception.pdf">Self-Supervised Sparse Sensor Fusion for Long Range Perception</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Torc Robotics</span></p><p>本文提出了一种名为LRS4Fusion的高效自监督稀疏体素多模态融合方法，实现了长距离（达250米）高速公路自动驾驶环境下的3D场景理解。通过稀疏体素编码、多模态（相机+LiDAR）特征自监督融合与时空上下文建模，显著提升了目标检测和LiDAR预测精度，分别在目标检测上提升26.6% mAP、在LiDAR预测上降低30.5% Chamfer Distance，突破了以往感知距离和计算瓶颈。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Self-Supervised_Sparse_Sensor_Fusion_for_Long_Range_Perception.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-ROVR-Open-Dataset-A-Large-Scale-Depth-Dataset-for-Autonomous-Driving"><a href="#36-ROVR-Open-Dataset-A-Large-Scale-Depth-Dataset-for-Autonomous-Driving" class="headerlink" title="36. ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ROVR-Open-Dataset__A_Large-Scale_Depth_Dataset_for_Autonomous_Driving.pdf">ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science, Wuhan University</span></p><p>该论文提出了一种大规模、场景多样、帧连续的自动驾驶深度估计公开数据集ROVR-Open-Dataset，采用轻量化多传感器同步采集与稀疏但统计充分的标注策略，覆盖多种道路、天气与光照条件。实验证明，现有主流深度估计算法在该数据集上表现大幅下降，凸显其挑战性和泛化需求，推动了复杂真实场景下深度估计方法的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ROVR-Open-Dataset_A_Large-Scale_Depth_Dataset_for_Autonomous_Driving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-MMIS-Net-for-Retinal-Fluid-Segmentation-and-Detection"><a href="#37-MMIS-Net-for-Retinal-Fluid-Segmentation-and-Detection" class="headerlink" title="37. MMIS-Net for Retinal Fluid Segmentation and Detection"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MMIS-Net_for_Retinal_Fluid_Segmentation_and_Detection.pdf">MMIS-Net for Retinal Fluid Segmentation and Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Brunel University London</span></p><p>该论文提出了MMIS-Net（多模态医学图像分割网络），结合了卷积神经网络与创新的Similarity Fusion Block，通过监督和像素级相似性选择实现特征融合，并采用独特的one-hot标签空间以解决多数据集标签不一致与重叠问题。MMIS-Net在10个数据集、19个器官、2种模态上联合训练，并在RETOUCH挑战隐测集上实现了DS均值0.83、AVD 0.035和AUC 1的分割与检测表现，显著优于现有SOTA和大模型，验证了其通用性和高效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MMIS-Net_for_Retinal_Fluid_Segmentation_and_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-SCRNet-Spatial-Channel-Regulation-Network-for-Medical-Ultrasound-Image-Segmentation"><a href="#38-SCRNet-Spatial-Channel-Regulation-Network-for-Medical-Ultrasound-Image-Segmentation" class="headerlink" title="38. SCRNet: Spatial-Channel Regulation Network for Medical Ultrasound Image Segmentation"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/SCRNet__Spatial-Channel_Regulation_Network_for_Medical_Ultrasound_Image_Segmentation.pdf">SCRNet: Spatial-Channel Regulation Network for Medical Ultrasound Image Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>本文提出了一种用于医学超声图像分割的新型网络SCRNet，通过设计特征聚合模块（FAM）结合卷积和交叉注意力机制，实现长距离依赖与局部上下文信息的融合；并提出空间-通道调控模块（SCRM），用于区分和强化重要特征，抑制冗余信息。将SCRM集成到UNet编码器后，SCRNet在BUSI、BUSIS和TN3K等医学分割数据集上取得了最先进的性能，显著优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/SCRNet_Spatial-Channel_Regulation_Network_for_Medical_Ultrasound_Image_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-UniECS-Unified-Multimodal-E-Commerce-Search-Framework-with-Gated-Cross-modal-Fusion"><a href="#39-UniECS-Unified-Multimodal-E-Commerce-Search-Framework-with-Gated-Cross-modal-Fusion" class="headerlink" title="39. UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/UniECS__Unified_Multimodal_E-Commerce_Search_Framework_with_Gated_Cross-modal_Fusion.pdf">UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Technology</span></p><p>该论文提出了UniECS，一种统一的多模态电商搜索框架，核心技术为自适应门控多模态编码器和跨模态对齐、局部一致性及模态内对比损失的综合训练策略，支持任意图文输入组合检索。实验表明，UniECS在多个电商检索基准上超越现有方法，尤其在跨模态检索任务中提升显著，并在真实电商平台上线后显著提升点击率和营收，具备实际应用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/UniECS_Unified_Multimodal_E-Commerce_Search_Framework_with_Gated_Cross-modal_Fusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-Latent-Interpolation-Learning-Using-Diffusion-Models-for-Cardiac-Volume-Reconstruction"><a href="#40-Latent-Interpolation-Learning-Using-Diffusion-Models-for-Cardiac-Volume-Reconstruction" class="headerlink" title="40. Latent Interpolation Learning Using Diffusion Models for Cardiac Volume Reconstruction"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Latent_Interpolation_Learning_Using_Diffusion_Models_for_Cardiac_Volume_Reconstruction.pdf">Latent Interpolation Learning Using Diffusion Models for Cardiac Volume Reconstruction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University Munich</span></p><p>本文提出了一种名为Cardiac Latent Interpolation Diffusion (CaLID)的新框架，利用数据驱动的扩散模型在潜空间实现心脏MRI稀疏切片的高效三维重建。该方法通过端到端联合训练条件网络和U-Net去噪器，实现了复杂空间关系和时空动态的自适应插值，并在无需辅助语义信息的前提下，显著提升了重建精度和速度。实验表明，CaLID在重建质量、解剖准确性和计算效率上均优于现有方法，适用于临床心血管成像。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Latent_Interpolation_Learning_Using_Diffusion_Models_for_Cardiac_Volume_Reconstruction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-Sketch3DVE-Sketch-based-3D-Aware-Scene-Video-Editing"><a href="#41-Sketch3DVE-Sketch-based-3D-Aware-Scene-Video-Editing" class="headerlink" title="41. Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Sketch3DVE__Sketch-based_3D-Aware_Scene_Video_Editing.pdf">Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Computing Technology, Chinese Academy of Sciences</span></p><p>Sketch3DVE提出了一个基于草图的3D感知视频编辑方法，利用MagicQuill进行首帧编辑，并通过点云和深度图对3D结构进行分析和对齐，实现编辑效果在多视角视频中的传播。该方法结合3D点云编辑、3D感知掩码传播和视频扩散模型，能实现插入、移除、替换及外观修改等多样化视频结构编辑，在大幅视角变化下保留未编辑区域并生成高质量结果。实验表明，Sketch3DVE在视频质量、时序一致性和未编辑区域保留方面均优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Sketch3DVE_Sketch-based_3D-Aware_Scene_Video_Editing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-A-Fully-Transformer-Based-Multimodal-Framework-for-Explainable-Cancer-Image-Segmentation-Using-Radiology-Reports"><a href="#42-A-Fully-Transformer-Based-Multimodal-Framework-for-Explainable-Cancer-Image-Segmentation-Using-Radiology-Reports" class="headerlink" title="42. A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/A_Fully_Transformer_Based_Multimodal_Framework_for_Explainable_Cancer_Image_Segmentation_Using_Radio.pdf">A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Brunel University of London</span></p><p>本论文提出Med-CTX框架，将ViT与Swin Transformer组成双分支视觉编码器，并融合BioClinicalBERT编码的BI-RADS结构化和放射学自由文本，通过不确定性感知的跨模态注意力实现乳腺癌超声图像的分割与解释。Med-CTX在BUS-BRA数据集上取得Dice 99%和IoU 95%，显著优于主流基线，且通过生成分割掩码、不确定性热图和诊断解释，实现可解释性和可信度的提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/A_Fully_Transformer_Based_Multimodal_Framework_for_Explainable_Cancer_Image_Segmentation_Using_Radiology_Reports.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-Optimizing-Region-of-Interest-Selection-for-Effective-Embedding-in-Video-Steganography-Based-on-Genetic-Algorithms"><a href="#43-Optimizing-Region-of-Interest-Selection-for-Effective-Embedding-in-Video-Steganography-Based-on-Genetic-Algorithms" class="headerlink" title="43. Optimizing Region of Interest Selection for Effective Embedding in Video Steganography Based on Genetic Algorithms"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Optimizing_Region_of_Interest_Selection_for_Effective_Embedding_in_Video_Steganography_Based_on_Gene.pdf">Optimizing Region of Interest Selection for Effective Embedding in Video Steganography Based on Genetic Algorithms</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Duhok</span></p><p>本文提出了一种结合遗传算法（GA）与高级加密标准（AES）的新型视频隐写方法，通过GA在视频帧的Y分量中优化选取嵌入区域，实现机密数据的高效隐写与高安全性。实验表明，该方法在保证视频质量（PSNR 64-75 dB）的同时，提升了数据嵌入容量和编码解码效率，适用于实时和高安全需求的场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Optimizing_Region_of_Interest_Selection_for_Effective_Embedding_in_Video_Steganography_Based_on_Genetic_Algorithms.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-DiffIER-Optimizing-Diffusion-Models-with-Iterative-Error-Reduction"><a href="#44-DiffIER-Optimizing-Diffusion-Models-with-Iterative-Error-Reduction" class="headerlink" title="44. DiffIER: Optimizing Diffusion Models with Iterative Error Reduction"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/DiffIER__Optimizing_Diffusion_Models_with_Iterative_Error_Reduction.pdf">DiffIER: Optimizing Diffusion Models with Iterative Error Reduction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出DiffIER，一种无需训练、基于优化的方法，通过迭代误差减小机制提升扩散模型的生成质量。DiffIER在推理阶段通过梯度优化收敛每一步的误差，显著缩小训练推理差距，有效提升文本生成图像、图像超分辨率和文本生成语音等任务的表现。实验显示该方法在多任务上优于主流基线，具备良好泛化性和实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/DiffIER_Optimizing_Diffusion_Models_with_Iterative_Error_Reduction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-Unleashing-Semantic-and-Geometric-Priors-for-3D-Scene-Completion"><a href="#45-Unleashing-Semantic-and-Geometric-Priors-for-3D-Scene-Completion" class="headerlink" title="45. Unleashing Semantic and Geometric Priors for 3D Scene Completion"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Unleashing_Semantic_and_Geometric_Priors_for_3D_Scene_Completion.pdf">Unleashing Semantic and Geometric Priors for 3D Scene Completion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Soochow University</span></p><p>本文提出了FoundationSSC框架，通过在源头和路径层面对语义与几何特征进行双解耦，结合FoundationStereo视觉基础模型生成的高质量语义与几何先验，并设计了轴感知融合（AAF）模块实现3D特征自适应融合。实验结果表明，该方法在SemanticKITTI和SSCBench-KITTI-360数据集上显著提升了3D场景完成的语义与几何性能，达到最新最优水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Unleashing_Semantic_and_Geometric_Priors_for_3D_Scene_Completion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-GazeProphet-Software-Only-Gaze-Prediction-for-VR-Foveated-Rendering"><a href="#46-GazeProphet-Software-Only-Gaze-Prediction-for-VR-Foveated-Rendering" class="headerlink" title="46. GazeProphet: Software-Only Gaze Prediction for VR Foveated Rendering"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/GazeProphet__Software-Only_Gaze_Prediction_for_VR_Foveated_Rendering.pdf">GazeProphet: Software-Only Gaze Prediction for VR Foveated Rendering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">PES University</span></p><p>本文提出GazeProphet，一种无需硬件眼动追踪的纯软件VR凝视点预测方法，结合Spherical Vision Transformer进行360°场景特征提取，LSTM编码凝视序列时序特征，再经多模态融合网络输出高置信凝视点。实验证明在VR数据集上，该方法预测精度显著优于传统显著性模型，角度误差降低24%，一致性和置信度均表现优异，表明可在无专用硬件下实现高效的注视点驱动渲染。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/GazeProphet_Software-Only_Gaze_Prediction_for_VR_Foveated_Rendering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-Enhancing-Targeted-Adversarial-Attacks-on-Large-Vision-Language-Models-through-Intermediate-Projector-Guidance"><a href="#47-Enhancing-Targeted-Adversarial-Attacks-on-Large-Vision-Language-Models-through-Intermediate-Projector-Guidance" class="headerlink" title="47. Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Enhancing_Targeted_Adversarial_Attacks_on_Large_Vision-Language_Models_through_Intermediate_Projecto.pdf">Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong Polytechnic University</span></p><p>本文提出了一种针对大型视觉-语言模型（VLMs）的新型定向对抗攻击方法IPGA（Intermediate Projector Guided Attack），首次利用视觉-语言对齐管道中的投影器模块（特别是Q-Former的中间训练阶段），通过操作细粒度的视觉特征实现更精细的对抗扰动控制，并提出了残差查询对齐（RQA）机制以保护无关内容。实验结果表明，IPGA及其扩展（IPGA-R）在图像字幕和视觉问答等任务中，在黑盒环境下优于现有对抗攻击方法，并能有效迁移至包括Google Gemini和OpenAI GPT等商用VLM，揭示了VLMs的关键安全漏洞。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Enhancing_Targeted_Adversarial_Attacks_on_Large_Vision-Language_Models_through_Intermediate_Projector_Guidance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-subCellSAM-Zero-Shot-Sub-Cellular-Segmentation-for-Hit-Validation-in-Drug-Discovery"><a href="#48-subCellSAM-Zero-Shot-Sub-Cellular-Segmentation-for-Hit-Validation-in-Drug-Discovery" class="headerlink" title="48. subCellSAM: Zero-Shot (Sub-)Cellular Segmentation for Hit Validation in Drug Discovery"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/subCellSAM__Zero-Shot_(Sub-)Cellular_Segmentation_for_Hit_Validation_in_Drug_Discovery.pdf">subCellSAM: Zero-Shot (Sub-)Cellular Segmentation for Hit Validation in Drug Discovery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Bern</span></p><p>该论文提出了subCellSAM，一种利用预训练分割基础模型的零样本细胞及亚细胞分割方法，通过递归自提示和嵌入形态学及拓扑先验实现高通用性，核心流程包括核分割、细胞分割以及亚细胞结构分割。实验证明，subCellSAM在细胞分割基准数据集和药物发现相关验证任务中，无需数据集特定调参即可取得与专业方法相当或更优的分割和下游分析结果，有效减少手动配置需求。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/subCellSAM_Zero-Shot_%28Sub-%29Cellular_Segmentation_for_Hit_Validation_in_Drug_Discovery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-RCGNet-RGB-based-Category-Level-6D-Object-Pose-Estimation-with-Geometric-Guidance"><a href="#49-RCGNet-RGB-based-Category-Level-6D-Object-Pose-Estimation-with-Geometric-Guidance" class="headerlink" title="49. RCGNet: RGB-based Category-Level 6D Object Pose Estimation with Geometric Guidance"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/RCGNet__RGB-based_Category-Level_6D_Object_Pose_Estimation_with_Geometric_Guidance.pdf">RCGNet: RGB-based Category-Level 6D Object Pose Estimation with Geometric Guidance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>本文提出了一种仅基于RGB图像的类别级6D物体姿态估计方法RCGNet，采用DINOv2视觉Transformer作为骨干网络，直接从RGB图像预测物体几何特征，并通过几何特征指导算法提高几何特征预测的准确性。方法融合先验点云与图像特征，无需深度数据，利用RANSAC-PnP实现姿态解算，并在CAMERA25与REAL275等数据集上显著优于同类RGB方法，实现了高效且准确的类别级物体姿态估计。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/RCGNet_RGB-based_Category-Level_6D_Object_Pose_Estimation_with_Geometric_Guidance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-Bridging-Clear-and-Adverse-Driving-Conditions-Domain-Adaptation-with-Simulation-Diffusion-and-GANs"><a href="#50-Bridging-Clear-and-Adverse-Driving-Conditions-Domain-Adaptation-with-Simulation-Diffusion-and-GANs" class="headerlink" title="50. Bridging Clear and Adverse Driving Conditions: Domain Adaptation with Simulation, Diffusion, and GANs"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Bridging_Clear_and_Adverse_Driving_Conditions.pdf">Bridging Clear and Adverse Driving Conditions: Domain Adaptation with Simulation, Diffusion, and GANs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bosch Center for Artificial Intelligence</span></p><p>本论文针对自动驾驶在恶劣环境下感知能力下降的问题，提出了一种无需目标域数据的多阶段域适应数据生成管线，结合仿真（CARLA）、扩散模型（ALDM）和GAN（DAUNIT），将标注的晴天图像转化为雾、雨、雪及夜间等多种恶劣天气高保真合成图像。通过自适应图像融合提升真实感，利用仿真+真实数据混合训练增强泛化能力，在ACDC数据集上语义分割提升1.85%，夜间场景提升4.62%，验证了方法对自动驾驶感知鲁棒性的增益。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Bridging_Clear_and_Adverse_Driving_Conditions_Domain_Adaptation_with_Simulation_Diffusion_and_GANs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-Temporal-Conditional-Referring-Video-Object-Segmentation-with-Noise-Free-Text-to-Video-Diffusion-Model"><a href="#51-Temporal-Conditional-Referring-Video-Object-Segmentation-with-Noise-Free-Text-to-Video-Diffusion-Model" class="headerlink" title="51. Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Temporal-Conditional_Referring_Video_Object_Segmentation_with_Noise-Free_Text-to-Video_Diffusion_Mod.pdf">Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Soochow University</span></p><p>该论文提出了一种新型时序条件指代视频目标分割模型，利用噪声消除的文本到视频扩散模型进行特征提取，并引入Hybrid CondDot（HCD）分割头实现并行融合Segmentation方法，显著提升边界分割能力和时序一致性。实验结果显示，去除传统噪声预测模块和创新的时序上下文掩码优化模块（TCMR），使模型在四大RVOS基准上均取得了最优性能，提升了分割精度与稳定性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Temporal-Conditional_Referring_Video_Object_Segmentation_with_Noise-Free_Text-to-Video_Diffusion_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-Color-Spike-Data-Generation-via-Bio-inspired-Neuron-like-Encoding-with-an-Artificial-Photoreceptor-Layer"><a href="#52-Color-Spike-Data-Generation-via-Bio-inspired-Neuron-like-Encoding-with-an-Artificial-Photoreceptor-Layer" class="headerlink" title="52. Color Spike Data Generation via Bio-inspired Neuron-like Encoding with an Artificial Photoreceptor Layer"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Color_Spike_Data_Generation_via_Bio-inspired_Neuron-like_Encoding_with_an_Artificial_Photoreceptor_L.pdf">Color Spike Data Generation via Bio-inspired Neuron-like Encoding with an Artificial Photoreceptor Layer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fu Jen Catholic University</span></p><p>该论文提出了一种基于生物神经元原理的神经元式编码方法，并结合人工光感受器层，将静态彩色图像转化为包含色彩与亮度信息的脉冲数据，用于提升脉冲神经网络（SNNs）在视觉任务中的表现。实验结果表明，这种信息更丰富的脉冲编码方法不仅提高了SNN的分类准确率，同时保持了神经形态计算的低功耗优势，验证了其在信息密度和能效上的可行性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Color_Spike_Data_Generation_via_Bio-inspired_Neuron-like_Encoding_with_an_Artificial_Photoreceptor_Layer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-FLAIR-Frequency-and-Locality-Aware-Implicit-Neural-Representations"><a href="#53-FLAIR-Frequency-and-Locality-Aware-Implicit-Neural-Representations" class="headerlink" title="53. FLAIR: Frequency- and Locality-Aware Implicit Neural Representations"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/FLAIR__Frequency-_and_Locality-Aware_Implicit_Neural_Representations.pdf">FLAIR: Frequency- and Locality-Aware Implicit Neural Representations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Chung-Ang University</span></p><p>该论文提出了FLAIR框架，引入RC-GAUSS激活函数和Wavelet-Energy-Guided Encoding（WEGE）以提升隐式神经表示（INR）对频率选择性和空间定位能力。通过RC-GAUSS实现精确的频域选择与时域定位，WEGE则利用小波变换计算区域自适应频率信息，二者共同有效缓解传统INR的频谱偏置问题。在2D图像重建、超分辨率、去噪和3D重建等任务上，FLAIR均显著优于现有方法，能够更好地捕捉高频细节。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/FLAIR_Frequency-_and_Locality-Aware_Implicit_Neural_Representations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-Calibrating-Biased-Distribution-in-VFM-derived-Latent-Space-via-Cross-Domain-Geometric-Consistency"><a href="#54-Calibrating-Biased-Distribution-in-VFM-derived-Latent-Space-via-Cross-Domain-Geometric-Consistency" class="headerlink" title="54. Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Calibrating_Biased_Distribution_in_VFM-derived_Latent_Space_via_Cross-Domain_Geometric_Consistency.pdf">Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Gaoling School of Artificial Intelligence, Renmin University of China</span></p><p>本论文提出了一种基于视觉基础模型（如CLIP、DINOv2）嵌入空间几何一致性的分布校准新范式，核心方法利用跨域几何一致性，提取全局分布的几何信息，在联邦学习中通过安全聚合本地协方差矩阵实现全局分布重构，在长尾识别中通过匹配外部数据集的类别几何形态恢复尾类分布。实验表明，该几何知识指导的分布校准框架显著提升了联邦学习和长尾识别等数据受限场景下的模型表现，具有良好的适应性和通用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Calibrating_Biased_Distribution_in_VFM-derived_Latent_Space_via_Cross-Domain_Geometric_Consistency.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-2D-Gaussians-Meet-Visual-Tokenizer"><a href="#55-2D-Gaussians-Meet-Visual-Tokenizer" class="headerlink" title="55. 2D Gaussians Meet Visual Tokenizer"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/2D_Gaussians_Meet_Visual_Tokenizer.pdf">2D Gaussians Meet Visual Tokenizer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huazhong University of Science and Technology</span></p><p>本文提出了一种新的视觉分词器架构Visual Gaussian Quantization (VGQ)，通过将2D高斯分布显式引入视觉分词流程，增强结构建模能力。VGQ采用双分支结构，结合VQ分支的外观信息和2DGS分支的几何结构信息，实验在ImageNet等数据集上实现了重建性能新SOTA（rFID 0.556），尤其在复杂结构和文本区域表现优越。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/2D_Gaussians_Meet_Visual_Tokenizer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-AdaptiveAE-An-Adaptive-Exposure-Strategy-for-HDR-Capturing-in-Dynamic-Scenes"><a href="#56-AdaptiveAE-An-Adaptive-Exposure-Strategy-for-HDR-Capturing-in-Dynamic-Scenes" class="headerlink" title="56. AdaptiveAE: An Adaptive Exposure Strategy for HDR Capturing in Dynamic Scenes"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/AdaptiveAE__An_Adaptive_Exposure_Strategy_for_HDR_Capturing_in_Dynamic_Scenes.pdf">AdaptiveAE: An Adaptive Exposure Strategy for HDR Capturing in Dynamic Scenes</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai AI Laboratory</span></p><p>AdaptiveAE提出了一种基于深度强化学习的自适应曝光控制方法，通过三阶段顺序优化策略自动预测每帧的ISO和快门速度，结合Blur-aware数据合成管线对动态场景中的HDR成像进行曝光融合。实验结果显示，AdaptiveAE在动态场景中能有效平衡噪声与运动模糊，HDR重建质量优于现有方法，兼具高鲁棒性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/AdaptiveAE_An_Adaptive_Exposure_Strategy_for_HDR_Capturing_in_Dynamic_Scenes.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-CORENet-Cross-Modal-4D-Radar-Denoising-Network-with-LiDAR-Supervision-for-Autonomous-Driving"><a href="#57-CORENet-Cross-Modal-4D-Radar-Denoising-Network-with-LiDAR-Supervision-for-Autonomous-Driving" class="headerlink" title="57. CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/CORENet__Cross-Modal_4D_Radar_Denoising_Network_with_LiDAR_Supervision_for_Autonomous_Driving.pdf">CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Computing Technology, Chinese Academy of Sciences</span></p><p>本文提出了CORENet，一种结合LiDAR监督的4D雷达点云去噪网络，核心包括分层多尺度去噪模块（HMSD-Net）和跨模态监督机制。方法在训练阶段利用LiDAR点云生成监督掩码，对4D雷达点云去噪，推理阶段仅需雷达数据，极大提升了噪声点云的物体检测精度。实验在Dual-Radar数据集上，集成CORENet的检测框架平均精度提升7%-18%，显著优于传统和现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/CORENet_Cross-Modal_4D_Radar_Denoising_Network_with_LiDAR_Supervision_for_Autonomous_Driving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-Distribution-Aware-Hadamard-Quantization-for-Hardware-Efficient-Implicit-Neural-Representations"><a href="#58-Distribution-Aware-Hadamard-Quantization-for-Hardware-Efficient-Implicit-Neural-Representations" class="headerlink" title="58. Distribution-Aware Hadamard Quantization for Hardware-Efficient Implicit Neural Representations"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Distribution-Aware_Hadamard_Quantization_for_Hardware-Efficient_Implicit_Neural_Representations.pdf">Distribution-Aware Hadamard Quantization for Hardware-Efficient Implicit Neural Representations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Hong Kong</span></p><p>本文提出DHQ，一种面向隐式神经表示（INR）中权重和激活分布的Hadamard变换量化方法，通过将不同层的数据分布标准化为统一的钟形分布，便于使用标准量化器实现高效硬件部署。结论表明，DHQ在图像重构任务中能大幅提升硬件效率（延迟降低32.7%、能耗降低40.1%、资源利用降低98.3%），同时保持接近全精度的重建性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Distribution-Aware_Hadamard_Quantization_for_Hardware-Efficient_Implicit_Neural_Representations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-Vision-Transformers-for-Kidney-Stone-Image-Classification-A-Comparative-Study-with-CNNs"><a href="#59-Vision-Transformers-for-Kidney-Stone-Image-Classification-A-Comparative-Study-with-CNNs" class="headerlink" title="59. Vision Transformers for Kidney Stone Image Classification: A Comparative Study with CNNs"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Vision_Transformers_for_Kidney_Stone_Image_Classification__A_Comparative_Study_with_CNNs.pdf">Vision Transformers for Kidney Stone Image Classification: A Comparative Study with CNNs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CINVESTAV, Guadalajara, Mexico</span></p><p>本论文提出了使用预训练的Vision Transformer (ViT)模型对肾结石内镜图像进行分类的方法，并与传统的卷积神经网络（CNN）架构进行了系统对比。通过在两个肾结石图像数据集（CCD相机和内窥镜采集）上训练和测试，ViT在准确率、F1分数及特征分辨率等方面均显著优于ResNet50等CNN模型，尤其在复杂和易混淆的图像子集表现突出。结论显示ViT模型具有更强的特征表达能力和分类鲁棒性，为临床自动化肾结石识别提供了更优的技术方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Vision_Transformers_for_Kidney_Stone_Image_Classification_A_Comparative_Study_with_CNNs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-Bridging-the-Gap-Doubles-Badminton-Analysis-with-Singles-Trained-Models"><a href="#60-Bridging-the-Gap-Doubles-Badminton-Analysis-with-Singles-Trained-Models" class="headerlink" title="60. Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Bridging_the_Gap__Doubles_Badminton_Analysis_with_Singles-Trained_Models.pdf">Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Soongsil University</span></p><p>本论文提出了一种将基于单打训练的姿态识别模型迁移到双打羽毛球分析的方法，结合ViT-Pose关键点提取、ST-GCN对时空骨架特征的对比学习嵌入，以及自定义多目标跟踪算法以解决ID切换问题，最终通过Transformer分类器进行击球识别。实验结果显示，该方法在无需额外标注双打数据的情况下实现了双打场景下的有效击球识别，具备较强的跨场景迁移能力，为双打分析和数据集建设奠定了基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Bridging_the_Gap_Doubles_Badminton_Analysis_with_Singles-Trained_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-MINR-Efficient-Implicit-Neural-Representations-for-Multi-Image-Encoding"><a href="#61-MINR-Efficient-Implicit-Neural-Representations-for-Multi-Image-Encoding" class="headerlink" title="61. MINR: Efficient Implicit Neural Representations for Multi-Image Encoding"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MINR__Efficient_Implicit_Neural_Representations_for_Multi-Image_Encoding.pdf">MINR: Efficient Implicit Neural Representations for Multi-Image Encoding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Hong Kong</span></p><p>该论文提出了MINR框架，通过分析多图像隐式神经表示（INR）网络的层权重分布，相似的中间层被多个图像共享，仅输入、输出层和额外投影层保持图像特异性，从而高效编码多张图片。实验表明，MINR在保持重建性能（PSNR高于34dB）的同时，参数量较传统独立INR方法最多可减少60%，有效提升存储与计算效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MINR_Efficient_Implicit_Neural_Representations_for_Multi-Image_Encoding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="62-Unintended-Misalignment-from-Agentic-Fine-Tuning-Risks-and-Mitigation"><a href="#62-Unintended-Misalignment-from-Agentic-Fine-Tuning-Risks-and-Mitigation" class="headerlink" title="62. Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Unintended_Misalignment_from_Agentic_Fine-Tuning__Risks_and_Mitigation.pdf">Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KAIST</span></p><p>本文发现将大型语言模型（LLM）在代理任务上微调会导致模型安全性下降，增加执行有害任务的风险。为此，作者提出了Prefix Injection Guard (PING)方法，通过自动生成自然语言前缀引导模型拒绝有害请求，同时保持在正常任务上的性能。实验结果表明，PING能显著提升微调后LLM代理的安全性，在网页导航和代码生成领域均有效，且不影响其任务能力，机制分析也揭示了前缀在行为修正中的关键作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Unintended_Misalignment_from_Agentic_Fine-Tuning_Risks_and_Mitigation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-Ask-Good-Questions-for-Large-Language-Models"><a href="#63-Ask-Good-Questions-for-Large-Language-Models" class="headerlink" title="63. Ask Good Questions for Large Language Models"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Ask_Good_Questions_for_Large_Language_Models.pdf">Ask Good Questions for Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">China University of Petroleum-Beijing</span></p><p>该论文提出了Ask-Good-Question (AGQ) 框架，结合了Concept-Enhanced Item Response Theory (CEIRT)模型与大语言模型，动态评估用户多维知识状态，并生成针对性引导问题，提升信息检索效率。实验结果显示，AGQ框架生成的问题在准确率、文本相似度及知识增益方面显著优于Zero-shot和CoT等基线方法，接近人类专家水平，具备跨模型通用性和适应性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Ask_Good_Questions_for_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-Chunks-as-Arms-Multi-Armed-Bandit-Guided-Sampling-for-Long-Context-LLM-Preference-Optimization"><a href="#64-Chunks-as-Arms-Multi-Armed-Bandit-Guided-Sampling-for-Long-Context-LLM-Preference-Optimization" class="headerlink" title="64. Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Chunks_as_Arms__Multi-Armed_Bandit-Guided_Sampling_for_Long-Context_LLM_Preference_Optimization.pdf">Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Department of Computer Science and Technology, Northeastern University</span></p><p>本文提出LongMab-PO框架，将长文本上下文划分为多个chunk，并采用多臂老虎机（MAB）策略，以UCB算法动态采样最有信息的chunk组合，引导LLM生成高质量多样化的响应，并构建偏好数据对进行DPO优化。实验表明，LongMab-PO显著提升了长上下文任务中LLM的理解与推理能力，在多个基准上优于现有SFT和DPO方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Chunks_as_Arms_Multi-Armed_Bandit-Guided_Sampling_for_Long-Context_LLM_Preference_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-ChronoLLM-Customizing-Language-Models-for-Physics-Based-Simulation-Code-Generation"><a href="#65-ChronoLLM-Customizing-Language-Models-for-Physics-Based-Simulation-Code-Generation" class="headerlink" title="65. ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ChronoLLM__Customizing_Language_Models_for_Physics-Based_Simulation_Code_Generation.pdf">ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Wisconsin-Madison</span></p><p>该论文提出ChronoLLM框架，通过持续预训练和监督微调等方法，对大语言模型（LLM）进行定制，使其能够自动生成用于PyChrono物理仿真工具的Python脚本。实验结果表明，经过微调后的ChronoLLM在生成仿真代码的准确性和实用性方面明显优于预训练模型和仅用上下文学习的方法，可有效提升领域专家使用仿真工具的效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ChronoLLM_Customizing_Language_Models_for_Physics-Based_Simulation_Code_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-MME-SCI-A-Comprehensive-and-Challenging-Science-Benchmark-for-Multimodal-Large-Language-Models"><a href="#66-MME-SCI-A-Comprehensive-and-Challenging-Science-Benchmark-for-Multimodal-Large-Language-Models" class="headerlink" title="66. MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MME-SCI__A_Comprehensive_and_Challenging_Science_Benchmark_for_Multimodal_Large_Language_Models.pdf">MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了MME-SCI，一个面向多模态大语言模型（MLLMs）的科学推理基准，涵盖数学、物理、化学、生物四大学科、五种语言和三种输入模态（文本、图像、图文混合），并对知识点进行细粒度标注。通过对20个主流MLLMs的实验证明，MME-SCI能有效区分模型性能，揭示模型在多语言一致性、模态适应性、推理能力和知识掌握上的不足，推动科学领域多模态模型的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MME-SCI_A_Comprehensive_and_Challenging_Science_Benchmark_for_Multimodal_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-RotBench-Evaluating-Multimodal-Large-Language-Models-on-Identifying-Image-Rotation"><a href="#67-RotBench-Evaluating-Multimodal-Large-Language-Models-on-Identifying-Image-Rotation" class="headerlink" title="67. RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/RotBench__Evaluating_Multimodal_Large_Language_Models_on_Identifying_Image_Rotation.pdf">RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">UNC Chapel Hill</span></p><p>该论文提出ROTBENCH基准，系统评测多模态大语言模型（MLLMs）在识别图像0°、90°、180°和270°旋转任务上的空间推理能力。实验发现，主流MLLMs能准确识别正向或倒置（0°&#x2F;180°）图像，但对90°与270°区分力极弱，辅以辅助信息或链式思考提示提升有限；仅部分推理型模型利用旋转网格和投票机制有适度提升。结论指出，MLLMs在空间定向推理方面存在明显短板，需在训练中增强旋转感知。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/RotBench_Evaluating_Multimodal_Large_Language_Models_on_Identifying_Image_Rotation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-Learning-to-Use-AI-for-Learning-How-Can-We-Effectively-Teach-and-Measure-Prompting-Literacy-for-K–12-Students"><a href="#68-Learning-to-Use-AI-for-Learning-How-Can-We-Effectively-Teach-and-Measure-Prompting-Literacy-for-K–12-Students" class="headerlink" title="68. Learning to Use AI for Learning: How Can We Effectively Teach and Measure Prompting Literacy for K–12 Students?"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Learning_to_Use_AI_for_Learning__How_Can_We_Effectively_Teach_and_Measure_Prompting_Literacy_for_K-1.pdf">Learning to Use AI for Learning: How Can We Effectively Teach and Measure Prompting Literacy for K–12 Students?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>本文提出并实现了一套基于大语言模型（LLM）的K-12学生提示素养教学与评测模块，包含情境化练习与AI自动评分反馈，目标是培养学生负责任和高效使用AI聊天机器人。两轮真实课堂实验表明，AI自动评分系统可以高质量评估学生写作的提示，模块能提升学生在提示写作中的背景信息嵌入能力和使用AI学习的信心，且经过迭代后，True&#x2F;False与开放式问题能更有效地测量提示素养，显示出方法的可扩展性和应用前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Learning_to_Use_AI_for_Learning_How_Can_We_Effectively_Teach_and_Measure_Prompting_Literacy_for_K%E2%80%9312_Students.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-Prompt-Orchestration-Markup-Language"><a href="#69-Prompt-Orchestration-Markup-Language" class="headerlink" title="69. Prompt Orchestration Markup Language"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Prompt_Orchestration_Markup_Language.pdf">Prompt Orchestration Markup Language</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Microsoft Research</span></p><p>该论文提出了POML（Prompt Orchestration Markup Language），一种面向大语言模型（LLM）应用开发的结构化标记语言。POML采用HTML类组件化结构，具备专用标签用于文档、表格、图片等多模态数据集成，并引入CSS风格的样式系统实现内容与格式分离，显著降低格式敏感性。作者同时开发了IDE工具套件（包括VSCode插件、SDKs），支持实时预览、诊断、版本管理和多语言集成。通过两个案例（iOS智能体应用PomLink和表格问答TableQA）及用户研究，验证POML在复杂LLM应用和数据密集型场景中的开发效率、可维护性和性能提升。结论：POML显著提升了LLM提示工程的结构化、可复用性和开发协作，推动智能体应用的快速实现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Prompt_Orchestration_Markup_Language.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-The-Collaboration-Paradox-Why-Generative-AI-Requires-Both-Strategic-Intelligence-and-Operational-Stability-in-Supply-Chain-Management"><a href="#70-The-Collaboration-Paradox-Why-Generative-AI-Requires-Both-Strategic-Intelligence-and-Operational-Stability-in-Supply-Chain-Management" class="headerlink" title="70. The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/The_Collaboration_Paradox__Why_Generative_AI_Requires_Both_Strategic_Intelligence_and_Operational_St.pdf">The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indian Institute of Technology (IIT) Kharagpur</span></p><p>本文提出了“协作悖论”，通过多阶段仿真实验分析了基于大语言模型（LLM）的生成式AI代理在多级供应链管理中的行为，发现单靠智能协作AI（如VMI）可能导致系统灾难性失稳（如囤货效应）。最终研究证明，只有将高层AI策略生成与低层协同执行（信息共享与主动补货）结合，才能实现供应链的韧性与稳定，相关方法可自动生成并评估多策略方案，为未来AI驱动的业务分析系统设计提供了蓝图。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/The_Collaboration_Paradox_Why_Generative_AI_Requires_Both_Strategic_Intelligence_and_Operational_Stability_in_Supply_Chain_Management.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-InPars-Supercharging-Synthetic-Data-Generation-for-Information-Retrieval-Systems"><a href="#71-InPars-Supercharging-Synthetic-Data-Generation-for-Information-Retrieval-Systems" class="headerlink" title="71. InPars+: Supercharging Synthetic Data Generation for Information Retrieval Systems"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/InPars+__Supercharging_Synthetic_Data_Generation_for_Information_Retrieval_Systems.pdf">InPars+: Supercharging Synthetic Data Generation for Information Retrieval Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Amsterdam</span></p><p>本文在复现InPars Toolkit合成查询生成流程的基础上，提出两项关键扩展：一是通过对生成器大语言模型采用对比偏好优化（CPO）方法微调，提升生成查询的质量；二是利用DSPy框架用动态链式思维（CoT）优化prompt，替代静态模板，从而改进合成数据生成流程。实验结果表明，这两项改进可减少过滤需求并提升下游信息检索性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/InPars%2B_Supercharging_Synthetic_Data_Generation_for_Information_Retrieval_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-Structured-Agentic-Workflows-for-Financial-Time-Series-Modeling-with-LLMs-and-Reflective-Feedback"><a href="#72-Structured-Agentic-Workflows-for-Financial-Time-Series-Modeling-with-LLMs-and-Reflective-Feedback" class="headerlink" title="72. Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Structured_Agentic_Workflows_for_Financial_Time-Series_Modeling_with_LLMs_and_Reflective_Feedback.pdf">Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>该论文提出TS-Agent，一种基于大语言模型的模块化智能体框架，通过结构化知识库、案例推理和反馈驱动的决策，自动化金融时序建模流程。TS-Agent在模型选择、代码优化和超参数微调等多阶段中结合专家知识和实验反馈，显著提升了预测精度、稳健性和可审计性，并在多个金融预测与生成任务上优于现有AutoML及智能体基线。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Structured_Agentic_Workflows_for_Financial_Time-Series_Modeling_with_LLMs_and_Reflective_Feedback.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-Prompt-Based-One-Shot-Exact-Length-Controlled-Generation-with-LLMs"><a href="#73-Prompt-Based-One-Shot-Exact-Length-Controlled-Generation-with-LLMs" class="headerlink" title="73. Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Prompt-Based_One-Shot_Exact_Length-Controlled_Generation_with_LLMs.pdf">Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Taiwan University</span></p><p>本文提出了一种基于倒计时标记和显式计数规则的Prompt工程方法（CAPEL），无需微调或多次采样即可实现大语言模型（LLM）在英中两种语言下的精确长度控制。实验表明，该方法将多种主流模型在多任务场景下的长度合规率从30%提升至95%以上，同时保持生成内容质量，证实仅靠Prompt就能实现严格的长度控制。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Prompt-Based_One-Shot_Exact_Length-Controlled_Generation_with_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-BetaWeb-Towards-a-Blockchain-enabled-Trustworthy-Agentic-Web"><a href="#74-BetaWeb-Towards-a-Blockchain-enabled-Trustworthy-Agentic-Web" class="headerlink" title="74. BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/BetaWeb__Towards_a_Blockchain-enabled_Trustworthy_Agentic_Web.pdf">BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Innovation Institute</span></p><p>本文提出了BetaWeb架构，将区块链技术与大语言模型驱动的多智能体系统（LaMAS）深度融合，解决Agentic Web在开放协作、隐私保护、数据管理和价值度量等方面的核心挑战。具体方法包括通过区块链实现身份验证、不可篡改的过程记录与智能合约驱动的激励分配，并提出了五阶段进化路线，为智能体自治与去中心化治理奠定基础。结论显示，区块链赋能的BetaWeb能有效提升LaMAS系统的信任度、自治性和可扩展性，为智能经济和智能社会的构建提供坚实基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/BetaWeb_Towards_a_Blockchain-enabled_Trustworthy_Agentic_Web.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-Expertise-aware-Multi-LLM-Recruitment-and-Collaboration-for-Medical-Decision-Making"><a href="#75-Expertise-aware-Multi-LLM-Recruitment-and-Collaboration-for-Medical-Decision-Making" class="headerlink" title="75. Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Expertise-aware_Multi-LLM_Recruitment_and_Collaboration_for_Medical_Decision-Making.pdf">Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hangzhou Dianzi University</span></p><p>本文提出了一种名为EMRC（Expertise-aware Multi-LLM Recruitment and Collaboration）的新型框架，通过两阶段流程提升医学决策系统的准确性与可靠性。第一阶段基于公开医学数据集，构建LLM专业能力表，实现针对具体医学查询的动态最优LLM专家代理招募；第二阶段则融合各代理自评置信度，并通过置信度加权和对抗验证机制，集成多代理诊断结果，提升决策一致性和鲁棒性。实验表明，该方法在多个医学决策数据集上均超越了单一及多LLM现有方法，有效提升了诊断准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Expertise-aware_Multi-LLM_Recruitment_and_Collaboration_for_Medical_Decision-Making.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-Prediction-is-not-Explanation-Revisiting-the-Explanatory-Capacity-of-Mapping-Embeddings"><a href="#76-Prediction-is-not-Explanation-Revisiting-the-Explanatory-Capacity-of-Mapping-Embeddings" class="headerlink" title="76. Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Prediction_is_not_Explanation__Revisiting_the_Explanatory_Capacity_of_Mapping_Embeddings.pdf">Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universität Hamburg</span></p><p>本论文系统分析了通过映射词嵌入到人工注释的语义特征（feature norms）来解释大型语言模型（LLM）知识的方法，采用了Partial Least Squares Regression和Feed Forward Neural Networks两种映射技术，并设计多组消融实验和上界分析。结果表明，特征预测准确性主要反映了数据的几何结构和方法论上界而非真实语义知识，映射方法解释的是向量空间的几何相似性而非真实的语义属性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Prediction_is_not_Explanation_Revisiting_the_Explanatory_Capacity_of_Mapping_Embeddings.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-Generics-and-Default-Reasoning-in-Large-Language-Models"><a href="#77-Generics-and-Default-Reasoning-in-Large-Language-Models" class="headerlink" title="77. Generics and Default Reasoning in Large Language Models"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Generics_and_Default_Reasoning_in_Large_Language_Models.pdf">Generics and Default Reasoning in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Oxford</span></p><p>本文系统评估了28种大型语言模型（LLM）在20种默认（defeasible）推理模式下处理通用泛化（generics）的能力，设计了多样化的推理基准并对模型在零样本、少样本和链式思考提示下表现进行了分析。研究发现，前沿LLM在默认推理任务上准确率高，但模型之间和提示方式之间表现差异大，链式思考提示反而导致性能下降，且大多数模型难以区分可撤销推理与演绎推理以及泛化与全称命题，凸显当前LLM在非单调逻辑推理上的局限。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Generics_and_Default_Reasoning_in_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-HumanPCR-Probing-MLLM-Capabilities-in-Diverse-Human-Centric-Scenes"><a href="#78-HumanPCR-Probing-MLLM-Capabilities-in-Diverse-Human-Centric-Scenes" class="headerlink" title="78. HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/HumanPCR__Probing_MLLM_Capabilities_in_Diverse_Human-Centric_Scenes.pdf">HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</span></p><p>该论文提出了HumanPCR，一套针对多模态大语言模型（MLLMs）在多样化人类场景下视觉理解能力的评测基准，包括感知、理解、推理三个层级，涵盖6000多个由人工验证的多选题和442个开放式视频推理题，并配有人类标注的推理链（CoT）。实验结果表明，现有MLLMs在人类中心视觉理解尤其是细粒度感知、时序理解和复杂推理方面存在明显挑战，模型普遍依赖于问题线索而难以主动提取关键视觉证据，即使采用更大视觉上下文和测试时推理策略，提升也有限。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/HumanPCR_Probing_MLLM_Capabilities_in_Diverse_Human-Centric_Scenes.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-Neuro-Symbolic-Artificial-Intelligence-Towards-Improving-the-Reasoning-Abilities-of-Large-Language-Models"><a href="#79-Neuro-Symbolic-Artificial-Intelligence-Towards-Improving-the-Reasoning-Abilities-of-Large-Language-Models" class="headerlink" title="79. Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Neuro-Symbolic_Artificial_Intelligence__Towards_Improving_the_Reasoning_Abilities_of_Large_Language_.pdf">Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本文系统综述了神经符号方法在提升大语言模型（LLM）推理能力方面的最新进展。通过从Symbolic→LLM（数据生成与增强）、LLM→Symbolic（引入符号模块&#x2F;工具提升推理步骤精度）、LLM+Symbolic（端到端融合及符号反馈）三大视角，梳理了关键技术路径及典型工作。结论认为，神经符号方法在补足数据稀缺、提升推理严谨性及优化推理过程表达等方面具有独特优势，是推动LLM向强推理智能（AGI）演进的重要方向，但多模态推理、混合架构及理论基础等仍面临挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Neuro-Symbolic_Artificial_Intelligence_Towards_Improving_the_Reasoning_Abilities_of_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-Input-Time-Scaling"><a href="#80-Input-Time-Scaling" class="headerlink" title="80. Input Time Scaling"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Input_Time_Scaling.pdf">Input Time Scaling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Chinese Academy of Sciences</span></p><p>该论文提出了一种新的规模扩展范式——Input Time Scaling，通过在训练和测试阶段利用LLM元认知自动生成多样化persona修饰输入查询，极大提升了模型的数学推理能力。实验证明，无需高质量数据和复杂训练流程，仅用1k低过滤样本，经多策略训练-测试协同设计，在AIME24和AIME25数学推理任务上达到甚至超越现有SOTA水平，强调数据多样性和训练-测试协同比传统高质量、规模扩展更有效。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Input_Time_Scaling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-Who-Gets-the-Mic-Investigating-Gender-Bias-in-the-Speaker-Assignment-of-a-Speech-LLM"><a href="#81-Who-Gets-the-Mic-Investigating-Gender-Bias-in-the-Speaker-Assignment-of-a-Speech-LLM" class="headerlink" title="81. Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Who_Gets_the_Mic__Investigating_Gender_Bias_in_the_Speaker_Assignment_of_a_Speech-LLM.pdf">Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Uppsala University</span></p><p>该论文提出了一种利用发言人分配作为分析工具的方法，研究Speech-LLM（以Bark为例）在文本到语音转换中的性别偏见问题。通过构建“职业”和“性别色彩词”两个数据集，并结合基线实验，系统评估模型在不同文本输入下的默认发言人性别分配，结果显示Bark没有显著的系统性性别偏见，但在部分词语和职业上表现出性别倾向，并能从文本中推断性别信息。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Who_Gets_the_Mic_Investigating_Gender_Bias_in_the_Speaker_Assignment_of_a_Speech-LLM.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-A-Comparative-Study-of-Decoding-Strategies-in-Medical-Text-Generation"><a href="#82-A-Comparative-Study-of-Decoding-Strategies-in-Medical-Text-Generation" class="headerlink" title="82. A Comparative Study of Decoding Strategies in Medical Text Generation"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/A_Comparative_Study_of_Decoding_Strategies_in_Medical_Text_Generation.pdf">A Comparative Study of Decoding Strategies in Medical Text Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Science and Technology Politehnica Bucharest</span></p><p>本文系统对比了11种解码策略（包括beam search、greedy、top-k、top-p、min-p、DBS等）在医疗文本生成的五大任务（翻译、摘要、问答、对话、图像描述）中的表现，涵盖通用和医学专用大模型。结果显示，确定性搜索（如beam search）通常优于随机采样法，输出质量更高但推理速度较慢，而医学领域大模型对解码策略更敏感，且整体性能并不总优于通用模型。结论指出，解码策略选择对医疗应用效果影响巨大，需针对任务精细调优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/A_Comparative_Study_of_Decoding_Strategies_in_Medical_Text_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-Agentic-DraCor-and-the-Art-of-Docstring-Engineering-Evaluating-MCP-empowered-LLM-Usage-of-the-DraCor-API"><a href="#83-Agentic-DraCor-and-the-Art-of-Docstring-Engineering-Evaluating-MCP-empowered-LLM-Usage-of-the-DraCor-API" class="headerlink" title="83. Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Agentic_DraCor_and_the_Art_of_Docstring_Engineering__Evaluating_MCP-empowered_LLM_Usage_of_the_DraCo.pdf">Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Potsdam</span></p><p>本文实现并评估了Model Context Protocol（MCP）服务器在DraCor平台上的应用，使大型语言模型（LLM）能够自主调用DraCor API工具进行文学数据检索与分析。通过定性和定量实验，研究了LLM的工具选择、调用效率和可靠性，发现“docstring工程”对优化LLM工具交互至关重要。实验结果表明，MCP架构下的LLM具备数字人文领域研究的潜力，但在数据处理能力和工具使用知识积累方面仍有限，基础设施需进一步完善。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Agentic_DraCor_and_the_Art_of_Docstring_Engineering_Evaluating_MCP-empowered_LLM_Usage_of_the_DraCor_API.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-DEPTH-BREADTH-SYNERGY-IN-RLVR-UNLOCKING-LLM-REASONING-GAINS-WITH-ADAPTIVE-EXPLORATION"><a href="#84-DEPTH-BREADTH-SYNERGY-IN-RLVR-UNLOCKING-LLM-REASONING-GAINS-WITH-ADAPTIVE-EXPLORATION" class="headerlink" title="84. DEPTH-BREADTH SYNERGY IN RLVR: UNLOCKING LLM REASONING GAINS WITH ADAPTIVE EXPLORATION"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Depth-Breadth_Synergy_in_RLVR__Unlocking_LLM_Reasoning_Gains_with_Adaptive_Exploration.pdf">DEPTH-BREADTH SYNERGY IN RLVR: UNLOCKING LLM REASONING GAINS WITH ADAPTIVE EXPLORATION</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (Guangzhou)</span></p><p>本文针对大语言模型（LLM）中的可验证奖励强化学习（RLVR）训练，系统分析了“深度”（难题探索）和“广度”（批量实例数）两个关键维度。提出了难度自适应采样算法DARS，通过多阶段采样提升对高难度问题的关注，并结合大批量训练（DARS-B），实现了Pass@1和Pass@K指标的双提升，验证了深度与广度在RLVR中的互补作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/DEPTH-BREADTH_SYNERGY_IN_RLVR_UNLOCKING_LLM_REASONING_GAINS_WITH_ADAPTIVE_EXPLORATION.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-CausalPlan-Empowering-Efficient-LLM-Multi-Agent-Collaboration-Through-Causality-Driven-Planning"><a href="#85-CausalPlan-Empowering-Efficient-LLM-Multi-Agent-Collaboration-Through-Causality-Driven-Planning" class="headerlink" title="85. CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/CausalPlan__Empowering_Efficient_LLM_Multi-Agent_Collaboration_Through_Causality-Driven_Planning.pdf">CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Deakin University</span></p><p>本论文提出CausalPlan框架，通过结构化因果推理模型（Structural Causal Action, SCA）学习多智能体任务中行为和状态的因果图，进而在大语言模型（LLM）行动规划中引入因果分数重加权或因果备选动作，实现有效约束与提升协作。实验证明，CausalPlan能显著减少因果无效动作，提升AI-AI及人-AI多智能体协作表现，超越强RL基线，尤其对开源LLM提升明显。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/CausalPlan_Empowering_Efficient_LLM_Multi-Agent_Collaboration_Through_Causality-Driven_Planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions"><a href="#86-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions" class="headerlink" title="86. ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ViExam__Are_Vision_Language_Models_Better_than_Humans_on_Vietnamese_Multimodal_Exam_Questions_.pdf">ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KAIST</span></p><p>本论文提出了第一个越南语多模态考试基准ViExam，包括2548道融合文本与图像的多领域教育题目，并系统评测了14个主流视觉语言模型（VLMs）在越南语多模态题上的表现。结果显示，SOTA VLMs平均准确率仅57.74%，明显低于人类平均水平（66.54%），仅有o3模型超越人类均值但仍远低于人类最佳，表明当前VLM在低资源语言的多模态推理上仍存在显著挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ViExam_Are_Vision_Language_Models_Better_than_Humans_on_Vietnamese_Multimodal_Exam_Questions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-Text2Weight-Bridging-Natural-Language-and-Neural-Network-Weight-Spaces"><a href="#87-Text2Weight-Bridging-Natural-Language-and-Neural-Network-Weight-Spaces" class="headerlink" title="87. Text2Weight: Bridging Natural Language and Neural Network Weight Spaces"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Text2Weight__Bridging_Natural_Language_and_Neural_Network_Weight_Spaces.pdf">Text2Weight: Bridging Natural Language and Neural Network Weight Spaces</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (Guangzhou)</span></p><p>该论文提出了一种名为T2W（Text2Weight）的扩散Transformer框架，通过将自然语言任务描述编码为文本特征，结合分块层次化神经网络权重和对称性约束、对抗训练，实现从文本直接生成高质量、任务相关且具有泛化能力的神经网络权重。实验表明，T2W在未见任务上表现优异，支持文本驱动的权重初始化、增强和模型融合，显著提升神经网络权重生成的实用性和可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Text2Weight_Bridging_Natural_Language_and_Neural_Network_Weight_Spaces.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-PersonaVlog-Personalized-Multimodal-Vlog-Generation-with-Multi-Agent-Collaboration-and-Iterative-Self-Correction"><a href="#88-PersonaVlog-Personalized-Multimodal-Vlog-Generation-with-Multi-Agent-Collaboration-and-Iterative-Self-Correction" class="headerlink" title="88. PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/PersonaVlog__Personalized_Multimodal_Vlog_Generation_with_Multi-Agent_Collaboration_and_Iterative_Se.pdf">PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出PersonaVlog框架，通过多模态大语言模型驱动的多智能体协作，自动生成个性化Vlog内容，包括视频、背景音乐和角色内心独白。方法包含多智能体内容生成、反馈回滚机制和主题化自动评测基准，显著提升了内容多样性、一致性及自动化评估能力。实验结果显示PersonaVlog在故事连贯性、角色一致性等多项指标上超过现有方法，推动了多模态个性化内容自动生成的研究进展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/PersonaVlog_Personalized_Multimodal_Vlog_Generation_with_Multi-Agent_Collaboration_and_Iterative_Self-Correction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-Toward-Better-EHR-Reasoning-in-LLMs-Reinforcement-Learning-with-Expert-Attention-Guidance"><a href="#89-Toward-Better-EHR-Reasoning-in-LLMs-Reinforcement-Learning-with-Expert-Attention-Guidance" class="headerlink" title="89. Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Toward_Better_EHR_Reasoning_in_LLMs__Reinforcement_Learning_with_Expert_Attention_Guidance.pdf">Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science, Peking University</span></p><p>本文提出EAG-RL框架，通过两阶段训练增强大语言模型对电子健康记录（EHR）的推理能力。第一阶段利用专家EHR模型引导的蒙特卡洛树搜索生成高质量推理轨迹进行策略初始化，第二阶段采用强化学习结合专家注意力对齐和熵自适应剪切优化推理策略。实验表明，EAG-RL在多个EHR预测任务上平均提升14.62%，并显著增强模型对输入扰动和跨域泛化的鲁棒性，显示了其在实际临床部署的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Toward_Better_EHR_Reasoning_in_LLMs_Reinforcement_Learning_with_Expert_Attention_Guidance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-Saudi-Dialect-ALLaM-LoRA-Fine-Tuning-for-Dialectal-Arabic-Generation"><a href="#90-Saudi-Dialect-ALLaM-LoRA-Fine-Tuning-for-Dialectal-Arabic-Generation" class="headerlink" title="90. Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Saudi-Dialect-ALLaM__LoRA_Fine-Tuning_for_Dialectal_Arabic_Generation.pdf">Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Umm Al-Qura University</span></p><p>本论文提出了一种基于LoRA的参数高效微调方法，使用私有的平衡合成指令-响应数据集对ALLaM-7B-Instruct-preview大模型进行沙特阿拉伯方言（Hijazi和Najdi）生成优化。核心方法包括Dialect-Token（显式方言标签）与No-Token（隐式推断）对比，实验结合外部方言分类器和多项文本质量指标，结果显示Dialect-Token模型显著提升方言正确率至84.2%，减少MSA泄漏至6.2%，并在人类和自动评测中超越多种主流阿拉伯LLM基线，验证了针对性数据与显式条件标签在阿拉伯方言大模型适配中的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Saudi-Dialect-ALLaM_LoRA_Fine-Tuning_for_Dialectal_Arabic_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-NovoMolGen-Rethinking-Molecular-Language-Model-Pretraining"><a href="#91-NovoMolGen-Rethinking-Molecular-Language-Model-Pretraining" class="headerlink" title="91. NovoMolGen: Rethinking Molecular Language Model Pretraining"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/NovoMolGen__Rethinking_Molecular_Language_Model_Pretraining.pdf">NovoMolGen: Rethinking Molecular Language Model Pretraining</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Chandar Research Lab</span></p><p>本文提出NovoMolGen，一套基于Transformer的分子大语言模型（Mol-LLMs），在15亿分子大数据集上预训练，系统研究分子表征、分词策略、模型规模与数据集大小对分子生成性能的影响，并采用强化学习微调实现目标导向的分子设计。实验表明，预训练指标与下游性能相关性弱，模型性能在早期训练即趋于饱和，NovoMolGen在无约束分子生成和目标优化任务中均显著优于现有Mol-LLMs和专用生成模型，为高效分子建模提供了新基准。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/NovoMolGen_Rethinking_Molecular_Language_Model_Pretraining.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-ProMed-Shapley-Information-Gain-Guided-Reinforcement-Learning-for-Proactive-Medical-LLMs"><a href="#92-ProMed-Shapley-Information-Gain-Guided-Reinforcement-Learning-for-Proactive-Medical-LLMs" class="headerlink" title="92. ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ProMed__Shapley_Information_Gain_Guided_Reinforcement_Learning_for_Proactive_Medical_LLMs.pdf">ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出ProMed框架，通过引入Shapley Information Gain（SIG）奖励，结合合作博弈理论动态量化每个问题在临床诊断中的信息价值，采用两阶段强化学习流程：SIG引导的蒙特卡洛树搜索（MCTS）用于高质量数据初始化，SIG增强的策略优化实现奖励分配机制，针对性提升LLM主动信息获取能力。实验结果显示，ProMed在MedQA与CMB-Exam数据集上显著优于现有方法，准确率提升6.29%，较被动范式提升54.45%，并具备良好的跨域泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ProMed_Shapley_Information_Gain_Guided_Reinforcement_Learning_for_Proactive_Medical_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-LLM-Enhanced-Linear-Autoencoders-for-Recommendation"><a href="#93-LLM-Enhanced-Linear-Autoencoders-for-Recommendation" class="headerlink" title="93. LLM-Enhanced Linear Autoencoders for Recommendation"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/LLM-Enhanced_Linear_Autoencoders_for_Recommendation.pdf">LLM-Enhanced Linear Autoencoders for Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sungkyunkwan University</span></p><p>本文提出了L3AE模型，将大语言模型（LLM）生成的语义嵌入与协同过滤信号通过两阶段优化策略融合：首先用LLM生成的语义表征构建物品间相关矩阵，其次通过知识蒸馏式正则化，联合学习用户-物品交互权重矩阵，两阶段均采用闭式解以保证全局最优和计算效率。实验结果显示，L3AE在三大推荐数据集上显著超过现有LLM增强模型，尤其在长尾物品推荐上表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/LLM-Enhanced_Linear_Autoencoders_for_Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-Revisiting-MLLM-Token-Technology-through-the-Lens-of-Classical-Visual-Coding"><a href="#94-Revisiting-MLLM-Token-Technology-through-the-Lens-of-Classical-Visual-Coding" class="headerlink" title="94. Revisiting MLLM Token Technology through the Lens of Classical Visual Coding"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Revisiting_MLLM_Token_Technology_through_the_Lens_of_Classical_Visual_Coding.pdf">Revisiting MLLM Token Technology through the Lens of Classical Visual Coding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Eastern Institute of Technology, Ningbo, China</span></p><p>本文系统性地将多模态大语言模型（MLLM）中的视觉Token技术（包括Token化、Token压缩、Token推理）与经典视觉编码理论进行模块化对比，提出统一的率-失真优化框架，并梳理两者在信息表达、冗余去除、上下文建模等流程上的异同。结论认为视觉编码原理可提升MLLM Token效率和鲁棒性，而MLLM Token范式也能反哺下一代语义视觉编码器，两领域应协同创新。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Revisiting_MLLM_Token_Technology_through_the_Lens_of_Classical_Visual_Coding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-ALIGN-Word-Association-Learning-for-Cross-Cultural-Generalization-in-Large-Language-Models"><a href="#95-ALIGN-Word-Association-Learning-for-Cross-Cultural-Generalization-in-Large-Language-Models" class="headerlink" title="95. ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ALIGN__Word_Association_Learning_for_Cross-Cultural_Generalization_in_Large_Language_Models.pdf">ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Melbourne</span></p><p>该论文提出基于母语者自由词语联想数据的参数高效微调方法，通过监督微调（SFT）和PPO强化学习分别在Llama-3.1-8B与Qwen-2.5-7B模型上，显著提升模型词语联想的准确性及心理属性贴近人类水平。结论显示，词语联想微调不仅提升词汇级表现，还显著促进模型在世界价值观调查中的文化价值对齐，尤其在目标文化知识稀缺时更为有效。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ALIGN_Word_Association_Learning_for_Cross-Cultural_Generalization_in_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-MAVIS-Multi-Objective-Alignment-via-Value-Guided-Inference-Time-Search"><a href="#96-MAVIS-Multi-Objective-Alignment-via-Value-Guided-Inference-Time-Search" class="headerlink" title="96. MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MAVIS__Multi-Objective_Alignment_via_Value-Guided_Inference-Time_Search.pdf">MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Texas A&amp;M University</span></p><p>MAVIS是一种轻量级推理时多目标对齐方法，通过训练每个目标的小型价值模型（value models），并在推理时根据用户指定的权重组合这些模型，动态调整LLM的输出分布，无需微调基础模型权重。实验表明，MAVIS在多目标权衡和推理效率上优于微调和模型集成等主流方法，可达到接近于针对用户偏好微调模型的性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MAVIS_Multi-Objective_Alignment_via_Value-Guided_Inference-Time_Search.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="97-How-Usable-is-Automated-Feature-Engineering-for-Tabular-Data"><a href="#97-How-Usable-is-Automated-Feature-Engineering-for-Tabular-Data" class="headerlink" title="97. How Usable is Automated Feature Engineering for Tabular Data?"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/How_Usable_is_Automated_Feature_Engineering_for_Tabular_Data_.pdf">How Usable is Automated Feature Engineering for Tabular Data?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Freiburg</span></p><p>本论文系统性调研了53种自动特征工程（AutoFE）方法，从可用性角度出发，基于代码、文档、社区活跃度及资源约束支持等九项指标，实测评估了其在实际应用中的可用性。研究发现，绝大多数AutoFE方法难以直接使用，缺乏文档和社区支持，且无一支持用户设置时间和内存限制，导致目前尚无真正适合工业应用的AutoFE方法。结论是，社区应将更多关注点从方法创新转向可用性改进，以推动AutoFE在实际中的广泛应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/How_Usable_is_Automated_Feature_Engineering_for_Tabular_Data.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-Automated-Energy-Aware-Time-Series-Model-Deployment-on-Embedded-FPGAs-for-Resilient-Combined-Sewer-Overflow-Management"><a href="#98-Automated-Energy-Aware-Time-Series-Model-Deployment-on-Embedded-FPGAs-for-Resilient-Combined-Sewer-Overflow-Management" class="headerlink" title="98. Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Automated_Energy-Aware_Time-Series_Model_Deployment_on_Embedded_FPGAs_for_Resilient_Combined_Sewer_O.pdf">Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Duisburg-Essen</span></p><p>该论文提出一种端到端、自动化的时序预测框架，将轻量级Transformer和LSTM模型通过整数量化后部署在嵌入式FPGA上，实现了污水溢流监测的本地、低能耗推理。实验表明8位量化Transformer在准确率和能耗之间取得平衡，LSTM则更适用于极低能耗场景，两者可根据实际部署需求灵活选择，显著提升了污水系统预警的韧性和可靠性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Automated_Energy-Aware_Time-Series_Model_Deployment_on_Embedded_FPGAs_for_Resilient_Combined_Sewer_Overflow_Management.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-DegDiT-Controllable-Audio-Generation-with-Dynamic-Event-Graph-Guided-Diffusion-Transformer"><a href="#99-DegDiT-Controllable-Audio-Generation-with-Dynamic-Event-Graph-Guided-Diffusion-Transformer" class="headerlink" title="99. DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/DegDiT__Controllable_Audio_Generation_with_Dynamic_Event_Graph_Guided_Diffusion_Transformer.pdf">DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Information Engineering, Chinese Academy of Sciences</span></p><p>该论文提出了DegDiT，一种结合动态事件图和扩散Transformer的可控文本到音频生成框架。方法通过将音频事件建模为包含语义、时间和事件间关系的动态事件图，利用图Transformer生成上下文嵌入并指导扩散模型合成音频，辅以高质量数据筛选和多维奖励共识优化。实验结果显示，DegDiT在多个音频生成指标上达到SOTA，具备更高的灵活性和时序精度，显著提升了音频生成的可控性和质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/DegDiT_Controllable_Audio_Generation_with_Dynamic_Event_Graph_Guided_Diffusion_Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-Communication-Efficient-Federated-Learning-with-Adaptive-Number-of-Participants"><a href="#100-Communication-Efficient-Federated-Learning-with-Adaptive-Number-of-Participants" class="headerlink" title="100. Communication-Efficient Federated Learning with Adaptive Number of Participants"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Communication-Efficient_Federated_Learning_with_Adaptive_Number_of_Participants.pdf">Communication-Efficient Federated Learning with Adaptive Number of Participants</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ivannikov Institute for System Programming, Moscow, Russia</span></p><p>本文提出了一种名为ISP（Intelligent Selection of Participants）的自适应机制，通过动态确定每轮联邦学习中参与客户端的数量，以在保证模型精度的前提下最大限度地提升通信效率。该方法将客户端数量的选择建模为优化问题，并通过蒙特卡洛估计和动量等技术，结合多种采样策略，在多个视觉和时序任务（如CIFAR-10、Tiny-ImageNet、ECG心电图分类）及梯度压缩设置下进行实验，结果表明在维持或提升模型精度的同时，通信轮数最高可减少30%。结论是，动态调整参与客户端数量不仅可显著降低通信开销，而且对大规模实际部署场景具有重要意义。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Communication-Efficient_Federated_Learning_with_Adaptive_Number_of_Participants.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="101-PENGUIN-Enhancing-Transformer-with-Periodic-Nested-Group-Attention-for-Long-term-Time-Series-Forecasting"><a href="#101-PENGUIN-Enhancing-Transformer-with-Periodic-Nested-Group-Attention-for-Long-term-Time-Series-Forecasting" class="headerlink" title="101. PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/PENGUIN__Enhancing_Transformer_with_Periodic-Nested_Group_Attention_for_Long-term_Time_Series_Foreca.pdf">PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出了PENGUIN，一种增强Transformer的周期嵌套分组注意力机制，通过显式建模多周期性和相对注意力偏置，有效捕获时间序列中的周期结构。实验结果表明，PENGUIN在九个基准数据集上显著优于现有MLP和Transformer模型，实现了更高的长期预测精度和更优的计算效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/PENGUIN_Enhancing_Transformer_with_Periodic-Nested_Group_Attention_for_Long-term_Time_Series_Forecasting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-End-to-End-Audio-Visual-Learning-for-Cochlear-Implant-Sound-Coding-in-Noisy-Environments"><a href="#102-End-to-End-Audio-Visual-Learning-for-Cochlear-Implant-Sound-Coding-in-Noisy-Environments" class="headerlink" title="102. End-to-End Audio-Visual Learning for Cochlear Implant Sound Coding in Noisy Environments"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/End-to-End_Audio-Visual_Learning_for_Cochlear_Implant_Sound_Coding_in_Noisy_Environments.pdf">End-to-End Audio-Visual Learning for Cochlear Implant Sound Coding in Noisy Environments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Taiwan University</span></p><p>该论文提出了一种端到端人工耳蜗系统AVSE-ECS，将音频-视觉语音增强（AVSE）与深度学习声码策略ElectrodeNet-CS（ECS）结合，通过联合训练优化音频与电极刺激信号。在嘈杂环境下实验表明，该方法显著提升了人工耳蜗的语音可懂度，验证了多模态深度学习集成的可行性和有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/End-to-End_Audio-Visual_Learning_for_Cochlear_Implant_Sound_Coding_in_Noisy_Environments.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-Trans-XFed-An-Explainable-Federated-Learning-for-Supply-Chain-Credit-Assessment"><a href="#103-Trans-XFed-An-Explainable-Federated-Learning-for-Supply-Chain-Credit-Assessment" class="headerlink" title="103. Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Trans-XFed__An_Explainable_Federated_Learning_for_Supply_Chain_Credit_Assessment.pdf">Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Utrecht University</span></p><p>本文提出了Trans-XFed架构，将联邦学习与可解释AI方法结合应用于供应链信用评估。方法上，采用基于性能的客户端选择策略（PBCS）以解决类别不平衡与非独立同分布（Non-IID）问题，整合FedProx架构、同态加密与Transformer编码器，并结合集成梯度实现可解释性；实验结果表明，该方法在保证隐私与可解释性的同时，实现了更高效和准确的信用评估，优于现有基线模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Trans-XFed_An_Explainable_Federated_Learning_for_Supply_Chain_Credit_Assessment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-ITL-LIME-Instance-Based-Transfer-Learning-for-Enhancing-Local-Explanations-in-Low-Resource-Data-Settings"><a href="#104-ITL-LIME-Instance-Based-Transfer-Learning-for-Enhancing-Local-Explanations-in-Low-Resource-Data-Settings" class="headerlink" title="104. ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ITL-LIME__Instance-Based_Transfer_Learning_for_Enhancing_Local_Explanations_in_Low-Resource_Data_Set.pdf">ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Murdoch University</span></p><p>本文提出ITL-LIME框架，通过引入基于实例的迁移学习和对比学习编码器，改进LIME在数据资源有限场景下的局部解释能力。方法首先对源域数据进行K-medoids聚类，检索与目标实例最相似的源实例，并与目标领域的邻域实例结合，利用对比学习编码器为实例赋权，最后训练局部可解释模型。实验结果表明，ITL-LIME在稳定性、忠实度和鲁棒性上均优于现有LIME变体，尤其适用于医疗等关键小样本领域。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ITL-LIME_Instance-Based_Transfer_Learning_for_Enhancing_Local_Explanations_in_Low-Resource_Data_Settings.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-DyMixOp-Guiding-Neural-Operator-Design-for-PDEs-from-a-Complex-Dynamics-Perspective-with-Local-Global-Mixing"><a href="#105-DyMixOp-Guiding-Neural-Operator-Design-for-PDEs-from-a-Complex-Dynamics-Perspective-with-Local-Global-Mixing" class="headerlink" title="105. DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/DyMixOp__Guiding_Neural_Operator_Design_for_PDEs_from_a_Complex_Dynamics_Perspective_with_Local-Glob.pdf">DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Aeronautics and Astronautics, Shanghai Jiao Tong University</span></p><p>该论文提出了DyMixOp，一种理论驱动的神经算子框架，用于高效求解由非线性偏微分方程（PDEs）描述的复杂动力系统。方法基于惯性流形理论，将无限维PDE动态降维至有限维潜空间，并设计了结合局部-全局混合（Local-Global-Mixing, LGM）变换的多层架构，能有效捕捉线性与非线性动力学并增强物理可解释性。实验显示，DyMixOp在多类PDE基准（包括对流、扩散及混合型方程）上性能优异，误差显著降低，且保持高效可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/DyMixOp_Guiding_Neural_Operator_Design_for_PDEs_from_a_Complex_Dynamics_Perspective_with_Local-Global-Mixing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-EVENTTSF-Event-Aware-Non-Stationary-Time-Series-Forecasting"><a href="#106-EVENTTSF-Event-Aware-Non-Stationary-Time-Series-Forecasting" class="headerlink" title="106. EVENTTSF: Event-Aware Non-Stationary Time Series Forecasting"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/EventTSF__Event-Aware_Non-Stationary_Time_Series_Forecasting.pdf">EVENTTSF: Event-Aware Non-Stationary Time Series Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Griffith University</span></p><p>该论文提出了EVENTTSF，一种新颖的自动回归扩散框架，利用历史时间序列和文本事件进行非平稳时序预测。方法包括事件控制的流匹配采样机制与多模态U型扩散Transformer，有效融合并对齐时间与文本模态，解决事件同步、事件诱导不确定性及跨模态表征错配等挑战。实验结果显示，EVENTTSF在八个数据集和十二项基线方法中表现优越，预测精度提升10.7%，训练效率提升1.13倍，尤其在事件丰富的数据集上有显著优势。结论表明该方法为事件感知的非平稳时序分析开辟了新方向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/EVENTTSF_Event-Aware_Non-Stationary_Time_Series_Forecasting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="107-AlphaX-An-AI-Based-Value-Investing-Strategy-for-the-Brazilian-Stock-Market"><a href="#107-AlphaX-An-AI-Based-Value-Investing-Strategy-for-the-Brazilian-Stock-Market" class="headerlink" title="107. AlphaX: An AI-Based Value Investing Strategy for the Brazilian Stock Market"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/AlphaX__An_AI-Based_Value_Investing_Strategy_for_the_Brazilian_Stock_Market.pdf">AlphaX: An AI-Based Value Investing Strategy for the Brazilian Stock Market</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Aeronautics Institute of Technology - ITA</span></p><p>本文提出AlphaX，一种融合财务基本面和市场数据、采用集成回归算法（如随机森林）的AI自动化价值投资策略，专为巴西股市设计。通过严格的回测和偏差控制，AlphaX在18个季度内在总收益、年化回报和风险调整收益方面均显著优于Ibovespa指数、Selic利率及常见技术分析策略（RSI、MFI等），展现了AI驱动的价值投资在实际金融市场中的有效性和稳健性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/AlphaX_An_AI-Based_Value_Investing_Strategy_for_the_Brazilian_Stock_Market.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-Virtuous-Machines-Towards-Artificial-General-Science"><a href="#108-Virtuous-Machines-Towards-Artificial-General-Science" class="headerlink" title="108. Virtuous Machines: Towards Artificial General Science"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Virtuous_Machines__Towards_Artificial_General_Science.pdf">Virtuous Machines: Towards Artificial General Science</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Explore Science</span></p><p>本论文提出了一种端到端的、领域无关的多智能体AI系统，能够自主完成科学研究全流程，包括假设生成、实验设计、数据采集、统计分析、可视化、论文撰写和‘自我’评审。系统在认知科学领域验证，通过完全自动化流程设计并完成了三项视觉工作记忆、心理旋转和意象清晰度的心理学实验，展现了AI系统独立开展实证研究的能力。结论表明，AI系统可在理论推理和方法严谨性上达到人类专家水平，但在理论细节和解释上尚有不足，标志着AI向自主科学发现迈出了重要一步。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Virtuous_Machines_Towards_Artificial_General_Science.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-Enhancing-Robustness-of-Implicit-Neural-Representations-Against-Weight-Perturbations"><a href="#109-Enhancing-Robustness-of-Implicit-Neural-Representations-Against-Weight-Perturbations" class="headerlink" title="109. Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Enhancing_Robustness_of_Implicit_Neural_Representations_Against_Weight_Perturbations.pdf">Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Hong Kong</span></p><p>该论文首次系统性研究了隐式神经表示（INRs）对权重扰动的脆弱性，提出通过最小化有扰动和无扰动权重下损失函数差异的方法，推导出一种基于梯度的鲁棒损失函数以提升模型对噪声的抵抗力。大量实验结果表明，该方法在多模态信号重构任务中能在多种噪声条件下将峰值信噪比（PSNR）提升至比原始模型高出最多7.5 dB，有效增强了INRs的鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Enhancing_Robustness_of_Implicit_Neural_Representations_Against_Weight_Perturbations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="110-SVDformer-Direction-Aware-Spectral-Graph-Embedding-Learning-via-SVD-and-Transformer"><a href="#110-SVDformer-Direction-Aware-Spectral-Graph-Embedding-Learning-via-SVD-and-Transformer" class="headerlink" title="110. SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer"></a>110. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/SVDformer__Direction-Aware_Spectral_Graph_Embedding_Learning_via_SVD_and_Transformer.pdf">SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Sydney</span></p><p>SVDformer提出了一种结合奇异值分解（SVD）和Transformer多头自注意力机制的有向图表征学习框架，通过自适应地增强重要谱分量并抑制高频噪声，实现了可学习的低&#x2F;高通谱图滤波，并利用奇异向量实现方向感知特征传播。实验结果表明，SVDformer在六个有向图基准数据集上节点分类准确率优于现有主流方法，尤其在异质和大规模稀疏图上表现突出，实现了更优的可扩展性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/SVDformer_Direction-Aware_Spectral_Graph_Embedding_Learning_via_SVD_and_Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="111-STPFormer-A-State-of-the-Art-Pattern-Aware-Spatio-Temporal-Transformer-for-Traffic-Forecasting"><a href="#111-STPFormer-A-State-of-the-Art-Pattern-Aware-Spatio-Temporal-Transformer-for-Traffic-Forecasting" class="headerlink" title="111. STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting"></a>111. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/STPFormer__A_State-of-the-Art_Pattern-Aware_Spatio-Temporal_Transformer_for_Traffic_Forecasting.pdf">STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Somewhere</span></p><p>本文提出了STPFormer，一种面向交通预测的统一可解释的时空Transformer模型，集成了时间位置聚合器（TPA）、空间序列聚合器（SSA）、时空图匹配模块（STGM）与注意力融合模块，实现了多尺度时空特征的高效建模。方法在五个真实交通数据集上进行评测，结果显示STPFormer优于当前主流方法，具有更强的泛化能力和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/STPFormer_A_State-of-the-Art_Pattern-Aware_Spatio-Temporal_Transformer_for_Traffic_Forecasting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025/08/20报纸</title>
    <link href="/2025/08/2025-08-20_article/"/>
    <url>/2025/08/2025-08-20_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2025/08/20报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-20<br>📄 <strong>发现论文数量</strong>：234  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Content-Accuracy-and-Quality-Aware-Resource-Allocation-Based-on-LP-Guided-DRL-for-ISAC-Driven-AIGC-Networks"><a href="#1-Content-Accuracy-and-Quality-Aware-Resource-Allocation-Based-on-LP-Guided-DRL-for-ISAC-Driven-AIGC-Networks" class="headerlink" title="1. Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Content_Accuracy_and_Quality_Aware_Resource_Allocation_Based_on_LP-Guided_DRL_for_ISAC-Driven_AIGC_N.pdf">Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Computing Technology, Chinese Academy of Sciences</span></p><p>本文提出一种面向ISAC驱动AIGC网络的内容准确性与质量感知服务评估指标（CAQA），构建了联合优化感知、生成和通信资源分配问题（CAQA-AIGC），以最大化平均CAQA。为解决高维NP难问题，设计了标准线性规划（LP）引导的深度强化学习（DRL）算法LPDRL-F，通过将三维决策空间降为二维，提升收敛速度和分配效果。仿真结果表明，LPDRL-F在收敛速度和服务质量上均优于现有DRL和扩散模型算法，平均CAQA提升超过14%，相比只关注生成质量的方案提升超50%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Content_Accuracy_and_Quality_Aware_Resource_Allocation_Based_on_LP-Guided_DRL_for_ISAC-Driven_AIGC_Networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Optimizing-Token-Choice-for-Code-Watermarking-A-RL-Approach"><a href="#2-Optimizing-Token-Choice-for-Code-Watermarking-A-RL-Approach" class="headerlink" title="2. Optimizing Token Choice for Code Watermarking: A RL Approach"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Optimizing_Token_Choice_for_Code_Watermarking__A_RL_Approach.pdf">Optimizing Token Choice for Code Watermarking: A RL Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Pennsylvania State University</span></p><p>本文提出了CodeTracer，一种基于强化学习（GRPO）的自适应大模型代码水印框架，通过参数化水印模型智能选择水印插入位置并偏置token选择，实现了在不破坏代码功能的前提下嵌入可检测水印。其多元奖励系统结合代码执行反馈与水印检测信号，实现了水印可检测性和代码完整性的平衡，实验证明在水印检测准确性和功能保持方面显著优于现有方法，具备高效推理和较强攻击鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Optimizing_Token_Choice_for_Code_Watermarking_A_RL_Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Cold-RL-Learning-Cache-Eviction-with-Offline-Reinforcement-Learning-for-NGINX"><a href="#3-Cold-RL-Learning-Cache-Eviction-with-Offline-Reinforcement-Learning-for-NGINX" class="headerlink" title="3. Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Cold-RL__Learning_Cache_Eviction_with_Offline_Reinforcement_Learning_for_NGINX.pdf">Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">NGINX</span></p><p>本论文提出Cold-RL，一种针对NGINX的生产级缓存淘汰算法，利用离线训练的Dueling DQN模型，在微秒级延迟约束下替换传统LRU淘汰策略。Cold-RL通过采样K个最冷对象、提取六个特征并与ONNX sidecar通信，实现高效、可解释且具备故障回退机制的淘汰流程。在高压力缓存场景下，Cold-RL的命中率较最佳传统策略提升146%，CPU开销低于2%，并已在生产环境稳定运行，显著降低基础设施成本。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Cold-RL_Learning_Cache_Eviction_with_Offline_Reinforcement_Learning_for_NGINX.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-Trust-Region-Constrained-Measure-Transport-in-Path-Space-for-Stochastic-Optimal-Control-and-Inference"><a href="#4-Trust-Region-Constrained-Measure-Transport-in-Path-Space-for-Stochastic-Optimal-Control-and-Inference" class="headerlink" title="4. Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Trust_Region_Constrained_Measure_Transport_in_Path_Space_for_Stochastic_Optimal_Control_and_Inferenc.pdf">Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Karlsruhe Institute of Technology</span></p><p>该论文提出了一种在路径空间进行信任域约束测度传输的新框架，用于求解随机最优控制（SOC）问题。方法核心是将全局优化问题分解为带有KL散度约束的迭代子问题，通过几何退火策略逐步逼近目标测度，并利用缓冲区重用轨迹采样，实现高效的梯度优化。实验证明该方法在扩散采样、多分子动力学转移路径采样、文本到图像模型的奖励微调等任务中相较现有方法性能显著提升，尤其在高维和复杂多模态分布下能够有效避免模式崩溃，提升采样质量与收敛速度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Trust_Region_Constrained_Measure_Transport_in_Path_Space_for_Stochastic_Optimal_Control_and_Inference.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-The-Yokai-Learning-Environment-Tracking-Beliefs-Over-Space-and-Time"><a href="#5-The-Yokai-Learning-Environment-Tracking-Beliefs-Over-Space-and-Time" class="headerlink" title="5. The Yokai Learning Environment: Tracking Beliefs Over Space and Time"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/The_Yokai_Learning_Environment__Tracking_Beliefs_Over_Space_and_Time.pdf">The Yokai Learning Environment: Tracking Beliefs Over Space and Time</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Stuttgart</span></p><p>该论文提出了Yokai Learning Environment (YLE)，一个基于协作卡牌游戏Yokai的多智能体强化学习环境，专注于空间和时间上的信念追踪和共识建立。作者通过对多种RL代理在此环境下的表现进行系统实验，发现现有RL代理即使具备完美记忆，在追踪动态信念、泛化到新搭档以及长期共识维护方面均表现不佳，暴露出对脆弱约定的依赖而非鲁棒的信念建模。结论表明，YLE为协作型人工智能中理论推理（ToM）与共识推理的研究提供了高挑战性测试平台，有助于推动多智能体系统在不确定环境下的协作能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/The_Yokai_Learning_Environment_Tracking_Beliefs_Over_Space_and_Time.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-Synthetic-Data-is-Sufficient-for-Zero-Shot-Visual-Generalization-from-Offline-Data"><a href="#6-Synthetic-Data-is-Sufficient-for-Zero-Shot-Visual-Generalization-from-Offline-Data" class="headerlink" title="6. Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Synthetic_Data_is_Sufficient_for_Zero-Shot_Visual_Generalization_from_Offline_Data.pdf">Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University College London</span></p><p>该论文提出了一种两步法，融合了针对视觉输入的离线强化学习的数据增强与扩散模型的潜空间合成数据生成。方法首先对原始离线数据进行旋转、色彩扰动等多种视觉增强，再通过扩散模型在潜空间生成额外合成数据，扩展训练数据分布。实验在V-D4RL（连续控制）和Procgen（离散控制）视觉离线RL基准上表明，该方法显著提升了零样本泛化能力，降低了泛化差距，并有效利用少量干扰数据增强模型鲁棒性。结论认为，结合数据增强与扩散模型生成合成数据，可在不改变现有离线RL算法的前提下，显著提升视觉离线RL的泛化能力和效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Synthetic_Data_is_Sufficient_for_Zero-Shot_Visual_Generalization_from_Offline_Data.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Breaking-Reward-Collapse-Adaptive-Reinforcement-for-Open-ended-Medical-Reasoning-with-Enhanced-Semantic-Discrimination"><a href="#7-Breaking-Reward-Collapse-Adaptive-Reinforcement-for-Open-ended-Medical-Reasoning-with-Enhanced-Semantic-Discrimination" class="headerlink" title="7. Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Breaking_Reward_Collapse__Adaptive_Reinforcement_for_Open-ended_Medical_Reasoning_with_Enhanced_Sema.pdf">Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出ARMed框架，通过三阶段训练流程（奖励驱动预训练、知识增强微调、奖励优化）和基于Group Relative Policy Optimization的动态语义奖励机制，有效解决医学视觉问答中的奖励塌缩问题。实验显示，ARMed在六个医学VQA基准上提升模型准确性与泛化能力，验证了动态语义奖励和知识注入对医学多模态推理的关键作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Breaking_Reward_Collapse_Adaptive_Reinforcement_for_Open-ended_Medical_Reasoning_with_Enhanced_Semantic_Discrimination.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-OPTIC-ER-A-Reinforcement-Learning-Framework-for-Real-Time-Emergency-Response-and-Equitable-Resource-Allocation-in-Underserved-African-Communities"><a href="#8-OPTIC-ER-A-Reinforcement-Learning-Framework-for-Real-Time-Emergency-Response-and-Equitable-Resource-Allocation-in-Underserved-African-Communities" class="headerlink" title="8. OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/OPTIC-ER__A_Reinforcement_Learning_Framework_for_Real-Time_Emergency_Response_and_Equitable_Resource.pdf">OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Port Harcourt</span></p><p>本文提出了OPTIC-ER，一个基于注意力引导Actor-Critic结构的强化学习框架，用于实时应急响应和公平资源分配，创新点包括Context-Rich State Vector和Precision Reward Function，并在尼日利亚Rivers State高保真仿真环境下开展训练。实验结果显示，OPTIC-ER在未见挑战集上达到100%最优率，并支持基础设施缺陷识别和公平性评估，为低资源环境下智能应急系统提供了可推广的蓝图。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/OPTIC-ER_A_Reinforcement_Learning_Framework_for_Real-Time_Emergency_Response_and_Equitable_Resource_Allocation_in_Underserved_African_Communities.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-Cognitive-Structure-Generation-From-Educational-Priors-to-Policy-Optimization"><a href="#9-Cognitive-Structure-Generation-From-Educational-Priors-to-Policy-Optimization" class="headerlink" title="9. Cognitive Structure Generation: From Educational Priors to Policy Optimization"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Cognitive_Structure_Generation__From_Educational_Priors_to_Policy_Optimization.pdf">Cognitive Structure Generation: From Educational Priors to Policy Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeast Normal University</span></p><p>本文提出了认知结构生成（CSG）框架，通过预训练认知结构扩散概率模型（CSDPM）以教育先验生成学生认知结构，然后使用基于SOLO分层奖励信号的强化学习优化生成过程，使生成结构更符合学生真实认知发展水平。实验表明，CSG生成的认知结构在知识追踪和认知诊断任务上表现更优，能提升学生建模的全面性和解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Cognitive_Structure_Generation_From_Educational_Priors_to_Policy_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-A-Taxonomy-of-Hierarchical-Multi-Agent-Systems-Design-Patterns-Coordination-Mechanisms-and-Industrial-Applications"><a href="#10-A-Taxonomy-of-Hierarchical-Multi-Agent-Systems-Design-Patterns-Coordination-Mechanisms-and-Industrial-Applications" class="headerlink" title="10. A Taxonomy of Hierarchical Multi-Agent Systems: Design Patterns, Coordination Mechanisms, and Industrial Applications"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/A_Taxonomy_of_Hierarchical_Multi-Agent_Systems__Design_Patterns,_Coordination_Mechanisms,_and_Indust.pdf">A Taxonomy of Hierarchical Multi-Agent Systems: Design Patterns, Coordination Mechanisms, and Industrial Applications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent Researcher, Houston, TX, USA</span></p><p>本文提出了一个多维度的分层多智能体系统（HMAS）分类法，涵盖控制层级、信息流、角色与任务分配、时间分层和通信结构五大设计轴，系统梳理了契约网协议、分层强化学习等关键协调机制，并结合智能电网、油气等工业案例分析其实际应用。结论指出，该分类法首次统一结构、时间和通信维度，为设计大规模、可解释且可扩展的分层多智能体系统提供了理论和实践指导。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/A_Taxonomy_of_Hierarchical_Multi-Agent_Systems_Design_Patterns_Coordination_Mechanisms_and_Industrial_Applications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-OS-R1-Agentic-Operating-System-Kernel-Tuning-with-Reinforcement-Learning"><a href="#11-OS-R1-Agentic-Operating-System-Kernel-Tuning-with-Reinforcement-Learning" class="headerlink" title="11. OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/OS-R1__Agentic_Operating_System_Kernel_Tuning_with_Reinforcement_Learning.pdf">OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Chinese Academy of Sciences</span></p><p>本文提出了OS-R1框架，将Linux内核配置空间抽象为强化学习环境，通过结合规则驱动的奖励函数和两阶段训练流程，提升大语言模型在自动探索和优化内核参数时的效率、准确性与泛化能力。实验结果显示，OS-R1在多项基准测试中显著优于现有方法，实现最高5.6%的性能提升，并在实际应用中表现出良好的扩展性和数据效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/OS-R1_Agentic_Operating_System_Kernel_Tuning_with_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="12-Belief-Conditioned-One-Step-Diffusion-Real-Time-Trajectory-Planning-with-Just-Enough-Sensing"><a href="#12-Belief-Conditioned-One-Step-Diffusion-Real-Time-Trajectory-Planning-with-Just-Enough-Sensing" class="headerlink" title="12. Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Belief-Conditioned_One-Step_Diffusion__Real-Time_Trajectory_Planning_with_Just-Enough_Sensing.pdf">Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Urbana-Champaign</span></p><p>该论文提出了Belief-Conditioned One-Step Diffusion (B-COD)，一种将轨迹生成与状态不确定性估计耦合的扩散规划器，结合CVaR-95风险代理，通过单次前向推理输出短时轨迹和不确定性指标。结合带约束的软演员-评论家（SAC）强化学习算法，实现了动态感知子集选择，有效降低能耗并保持定位误差在预算内。在无人船实地测试中，B-COD以不到一半的感知能耗达到了与全传感器开启相同的任务完成率（98%），并实现了10ms级实时规划。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Belief-Conditioned_One-Step_Diffusion_Real-Time_Trajectory_Planning_with_Just-Enough_Sensing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-Talk-Less-Fly-Lighter-Autonomous-Semantic-Compression-for-UAV-Swarm-Communication-via-LLMs"><a href="#13-Talk-Less-Fly-Lighter-Autonomous-Semantic-Compression-for-UAV-Swarm-Communication-via-LLMs" class="headerlink" title="13. Talk Less, Fly Lighter: Autonomous Semantic Compression for UAV Swarm Communication via LLMs"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Talk_Less,_Fly_Lighter__Autonomous_Semantic_Compression_for_UAV_Swarm_Communication_via_LLMs.pdf">Talk Less, Fly Lighter: Autonomous Semantic Compression for UAV Swarm Communication via LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Macau University of Science and Technology</span></p><p>该论文提出了一种基于大语言模型（LLM）的无人机（UAV）集群自主语义压缩通信机制，设计了集成系统提示和任务指令提示的通信-执行流程，并在四类二维仿真环境中，系统评估了九种主流LLM的语义压缩性能。实验表明，LLM驱动的UAV集群在带宽受限和多跳通信场景下能够有效降低通信负载并保持关键任务语义，为无人系统高效协作提供了可行方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Talk_Less_Fly_Lighter_Autonomous_Semantic_Compression_for_UAV_Swarm_Communication_via_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Energy-Efficiency-in-Robotics-Software-A-Systematic-Literature-Review-2020-2024"><a href="#14-Energy-Efficiency-in-Robotics-Software-A-Systematic-Literature-Review-2020-2024" class="headerlink" title="14. Energy Efficiency in Robotics Software: A Systematic Literature Review (2020-2024)"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Energy_Efficiency_in_Robotics_Software__A_Systematic_Literature_Review_(2020-2024).pdf">Energy Efficiency in Robotics Software: A Systematic Literature Review (2020-2024)</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Michigan</span></p><p>本论文采用系统文献综述方法，结合LLM辅助筛选与人工审核，梳理了2020-2024年间关于机器人软件能效提升的79项实证研究，分析能效指标、应用领域、主耗能环节、能耗建模、技术路径及能量-性能权衡。结论认为，运动与轨迹优化为最主流的软件节能策略，工业场景应用增多，代表性能量模型和实证混合评测成为新常态，但评价指标多样化限制了跨论文可比性，未来应推进标准化与跨层节能设计。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Energy_Efficiency_in_Robotics_Software_A_Systematic_Literature_Review_%282020-2024%29.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-MAPF-World-Action-World-Model-for-Multi-Agent-Path-Finding"><a href="#15-MAPF-World-Action-World-Model-for-Multi-Agent-Path-Finding" class="headerlink" title="15. MAPF-World: Action World Model for Multi-Agent Path Finding"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MAPF-World__Action_World_Model_for_Multi-Agent_Path_Finding.pdf">MAPF-World: Action World Model for Multi-Agent Path Finding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen Technology University</span></p><p>该论文提出MAPF-World，一种基于Transformer的自回归动作世界模型，融合快-慢双系统（策略模型和世界模型），通过空间关系编码（SRE）强化多智能体间动态和环境建模，支持复杂场景下超前规划与协同决策。实验证明，MAPF-World在多种高密度、真实城市地图等复杂环境下，显著优于当前最优的学习型多智能体路径规划方法，且模型更小、数据需求更低。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/MAPF-World_Action_World_Model_for_Multi-Agent_Path_Finding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-Fully-Spiking-Actor-Critic-Neural-Network-for-Robotic-Manipulation"><a href="#16-Fully-Spiking-Actor-Critic-Neural-Network-for-Robotic-Manipulation" class="headerlink" title="16. Fully Spiking Actor-Critic Neural Network for Robotic Manipulation"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Fully_Spiking_Actor-Critic_Neural_Network_for_Robotic_Manipulation.pdf">Fully Spiking Actor-Critic Neural Network for Robotic Manipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology</span></p><p>该论文提出了一种基于全脉冲神经网络（SNN）的层级化课程强化学习（CRL）框架，专门用于高自由度机械臂的抓取控制。方法采用极简SNN结构（仅含输入与输出层），结合动态奖励权重调节与观测空间优化，并在Isaac Gym仿真平台上与PPO算法集成，辅以能耗建模对SNN和ANN进行定量对比。实验表明，所提方法在任务成功率与能耗方面均优于传统ANN基线，实现了更高效、稳定的机器人操作控制，能耗节省达68%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Fully_Spiking_Actor-Critic_Neural_Network_for_Robotic_Manipulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-InstDrive-Instance-Aware-3D-Gaussian-Splatting-for-Driving-Scenes"><a href="#17-InstDrive-Instance-Aware-3D-Gaussian-Splatting-for-Driving-Scenes" class="headerlink" title="17. InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/InstDrive__Instance-Aware_3D_Gaussian_Splatting_for_Driving_Scenes.pdf">InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology Beijing</span></p><p>InstDrive提出了一种面向动态驾驶场景的实例感知3D高斯溅射重建框架，可从行车记录仪视频和SAM生成的伪2D掩码直接学习可编辑的实例级3D表示。方法包括2D-3D一致性约束、静态二值化码本和伪监督机制，实现无需人工标签的高质量多视角实例分割，支持实时点选编辑。实验显示该方法在PandaSet数据集上获得了稳定、准确的实例分割结果，适合自动驾驶场景理解与交互应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/InstDrive_Instance-Aware_3D_Gaussian_Splatting_for_Driving_Scenes.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-No-More-Blind-Spots-Learning-Vision-Based-Omnidirectional-Bipedal-Locomotion-for-Challenging-Terrain"><a href="#18-No-More-Blind-Spots-Learning-Vision-Based-Omnidirectional-Bipedal-Locomotion-for-Challenging-Terrain" class="headerlink" title="18. No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/No_More_Blind_Spots__Learning_Vision-Based_Omnidirectional_Bipedal_Locomotion_for_Challenging_Terrai.pdf">No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Oregon State University</span></p><p>该论文提出一种针对复杂地形的视觉驱动全向双足机器人运动学习框架，结合了预训练盲控器、教师-学生蒸馏策略和创新的数据增强方法。通过在仿真和实际硬件上的验证，结果表明该方法能在极小视觉渲染成本下实现高效且鲁棒的全向地形运动，显著提升训练速度与通过率，减少能耗与碰撞。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/No_More_Blind_Spots_Learning_Vision-Based_Omnidirectional_Bipedal_Locomotion_for_Challenging_Terrain.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Integrating-Symbolic-RL-Planning-into-a-BDI-based-Autonomous-UAV-Framework-System-Integration-and-SIL-Validation"><a href="#19-Integrating-Symbolic-RL-Planning-into-a-BDI-based-Autonomous-UAV-Framework-System-Integration-and-SIL-Validation" class="headerlink" title="19. Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Integrating_Symbolic_RL_Planning_into_a_BDI-based_Autonomous_UAV_Framework__System_Integration_and_S.pdf">Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">LIG Nex1</span></p><p>该论文提出了AMAD-SRL框架，将基于信念-欲望-意图（BDI）架构的无人机多智能体系统与基于PDDL的符号强化学习（SRL）规划模块集成，实现复杂动态任务中的自适应决策。通过在软件环（SIL）环境中验证，该框架在无人机目标获取任务中显著提升了任务效率，路径距离比传统覆盖基线缩短约75%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Integrating_Symbolic_RL_Planning_into_a_BDI-based_Autonomous_UAV_Framework_System_Integration_and_SIL_Validation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-Toward-General-Physical-Intelligence-for-Resilient-Agile-Manufacturing-Automation"><a href="#20-Toward-General-Physical-Intelligence-for-Resilient-Agile-Manufacturing-Automation" class="headerlink" title="20. Toward General Physical Intelligence for Resilient Agile Manufacturing Automation"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Toward_General_Physical_Intelligence_for_Resilient_Agile_Manufacturing_Automation.pdf">Toward General Physical Intelligence for Resilient Agile Manufacturing Automation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeastern University</span></p><p>本文系统综述了面向工业自动化的通用物理智能（GPI）研究进展，重点分析了视觉-语言-行动（VLA）基础模型在多模态感知、仿真与现实迁移、规划控制、不确定性与安全、基准评测等五大方向的应用与挑战，并提出了面向工业实际的GPI框架架构及消融实验。结论表明，融合多模态感知与具身推理的GPI技术能显著提升机器人在动态制造场景中的适应性、精度与安全性，为柔性制造自动化提供了关键支撑。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Toward_General_Physical_Intelligence_for_Resilient_Agile_Manufacturing_Automation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-Control-of-Legged-Robots-using-Model-Predictive-Optimized-Path-Integral"><a href="#21-Control-of-Legged-Robots-using-Model-Predictive-Optimized-Path-Integral" class="headerlink" title="21. Control of Legged Robots using Model Predictive Optimized Path Integral"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Control_of_Legged_Robots_using_Model_Predictive_Optimized_Path_Integral.pdf">Control of Legged Robots using Model Predictive Optimized Path Integral</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Calgary</span></p><p>该论文提出了一种结合模型预测路径积分（MPPI）、协方差矩阵自适应（CMA）和交叉熵（CE）方法的采样式模型预测优化路径积分（MPOPI）控制策略，用于腿式机器人在复杂场景下的实时全身运动生成。通过大量仿真实验，结果表明MPOPI在样本效率和运动规划性能上显著优于传统MPPI，能够更快、更稳定地完成如攀爬台阶、越障和推箱等复杂任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Control_of_Legged_Robots_using_Model_Predictive_Optimized_Path_Integral.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-Saliency-Based-Attention-Shifting-A-Framework-for-Improving-Driver-Situational-Awareness-of-Out-of-Label-Hazards"><a href="#22-Saliency-Based-Attention-Shifting-A-Framework-for-Improving-Driver-Situational-Awareness-of-Out-of-Label-Hazards" class="headerlink" title="22. Saliency-Based Attention Shifting: A Framework for Improving Driver Situational Awareness of Out-of-Label Hazards"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Saliency-Based_Attention_Shifting__A_Framework_for_Improving_Driver_Situational_Awareness_of_Out-of-.pdf">Saliency-Based Attention Shifting: A Framework for Improving Driver Situational Awareness of Out-of-Label Hazards</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Denver</span></p><p>本文提出了一种结合实时凝视追踪、情境感知显著性分析以及同步视觉与听觉提示的框架，通过头显显示(HUD)和声音引导，主动打断分心驾驶员的注视，并将注意力引导至自动驾驶车辆检测到的未标注危险及其相关区域，从而增强接管场景下的情境感知。结论认为该系统能够有效减少驾驶员目标固视，提升对新兴危险的响应能力，未来将通过高保真模拟实验进一步验证。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Saliency-Based_Attention_Shifting_A_Framework_for_Improving_Driver_Situational_Awareness_of_Out-of-Label_Hazards.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-Semi-Infinite-Programming-for-Collision-Avoidance-in-Optimal-and-Model-Predictive-Control"><a href="#23-Semi-Infinite-Programming-for-Collision-Avoidance-in-Optimal-and-Model-Predictive-Control" class="headerlink" title="23. Semi-Infinite Programming for Collision-Avoidance in Optimal and Model Predictive Control"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Semi-Infinite_Programming_for_Collision-Avoidance_in_Optimal_and_Model_Predictive_Control.pdf">Semi-Infinite Programming for Collision-Avoidance in Optimal and Model Predictive Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Robert Bosch GmbH</span></p><p>本文提出了一种基于半无限规划（SIP）的碰撞规避最优与模型预测控制（MPC）框架，通过将环境建模为点云、机器人建模为带缓冲多边形，实现了对每个障碍点的无穷约束，并利用局部约简和外部激活集方法高效求解，兼顾机器人形状参数化和障碍不确定性。实验证明该算法能实时应用于移动机器人，安全通过窄空间，并支持3D场景扩展，且与现有方法相比性能更优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Semi-Infinite_Programming_for_Collision-Avoidance_in_Optimal_and_Model_Predictive_Control.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-Implementation-and-evaluation-of-a-prediction-algorithm-for-an-autonomous-vehicle"><a href="#24-Implementation-and-evaluation-of-a-prediction-algorithm-for-an-autonomous-vehicle" class="headerlink" title="24. Implementation and evaluation of a prediction algorithm for an autonomous vehicle"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Implementation_and_evaluation_of_a_prediction_algorithm_for_an_autonomous_vehicle.pdf">Implementation and evaluation of a prediction algorithm for an autonomous vehicle</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">DHBW Stuttgart</span></p><p>本文提出了一种基于动态自行车模型并结合扩展卡尔曼滤波（EKF）的自动驾驶车辆轨迹预测算法，实现了每5毫秒对车辆位置和姿态的高精度估计。通过实验测定车辆动力学参数（包括质量、重心、转动惯量和侧偏刚度），并创新性地采用光学跟踪方法测量侧偏刚度，最终算法在整个测试过程中每米平均偏差仅为1.25厘米，比传统运动学模型精度提升82.6%。结论：该模型在实际应用中表现出极高的短时预测精度，能够有效弥补传感器采样间隔内的姿态估计误差。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Implementation_and_evaluation_of_a_prediction_algorithm_for_an_autonomous_vehicle.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-A-robust-and-compliant-robotic-assembly-control-strategy-for-batch-precision-assembly-task-with-uncertain-fit-types-and-fit-amounts"><a href="#25-A-robust-and-compliant-robotic-assembly-control-strategy-for-batch-precision-assembly-task-with-uncertain-fit-types-and-fit-amounts" class="headerlink" title="25. A robust and compliant robotic assembly control strategy for batch precision assembly task with uncertain fit types and fit amounts"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/A_robust_and_compliant_robotic_assembly_control_strategy_for_batch_precision_assembly_task_with_unce.pdf">A robust and compliant robotic assembly control strategy for batch precision assembly task with uncertain fit types and fit amounts</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了一种面向批量高精度装配任务、适应不确定配合类型及配合量的鲁棒机器人控制策略。方法包括任务分解、多子任务深度强化学习（MTRL）与多教师策略蒸馏，结合力-视觉融合控制器，显著提升了控制策略的鲁棒性和合规性。实验结果显示，该方法在实际装配中能有效应对多样配合状态，成功率高于现有方法，并在不同装配场景下保持低接触力和高效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/A_robust_and_compliant_robotic_assembly_control_strategy_for_batch_precision_assembly_task_with_uncertain_fit_types_and_fit_amounts.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-Mechanical-Automation-with-Vision-A-Design-for-Rubik’s-Cube-Solver"><a href="#26-Mechanical-Automation-with-Vision-A-Design-for-Rubik’s-Cube-Solver" class="headerlink" title="26. Mechanical Automation with Vision: A Design for Rubik’s Cube Solver"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Mechanical_Automation_with_Vision__A_Design_for_Rubik's_Cube_Solver.pdf">Mechanical Automation with Vision: A Design for Rubik’s Cube Solver</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Thapathali Campus, IOE, TU</span></p><p>本文提出了一种结合YOLOv8视觉检测、Kociemba算法与机械自动化的鲁比克魔方自动求解器。系统采用三步进电机进行物理操作，由Arduino微控制器控制，通过相机和YOLO模型进行实时魔方状态识别，配有Unity开发的用户交互GUI。实验证明该系统结构简单、成本低，平均求解时间约2.2分钟，准确率高，具备较强实用性和可重现性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Mechanical_Automation_with_Vision_A_Design_for_Rubik%E2%80%99s_Cube_Solver.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-SIGN-Safety-Aware-Image-Goal-Navigation-for-Autonomous-Drones-via-Reinforcement-Learning"><a href="#27-SIGN-Safety-Aware-Image-Goal-Navigation-for-Autonomous-Drones-via-Reinforcement-Learning" class="headerlink" title="27. SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/SIGN__Safety-Aware_Image-Goal_Navigation_for_Autonomous_Drones_via_Reinforcement_Learning.pdf">SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>该论文提出SIGN框架，实现了无人机在未知环境中的端到端图像目标导航。方法上，利用视觉强化学习训练连续速度控制策略，并引入自监督辅助任务（未来状态预测与图像扰动）提升视觉表征能力，结合基于深度信息的安全模块实时预测并修正碰撞风险，实现无外部定位的自主探索与避障。实验表明，SIGN在仿真和真实环境中均表现出较强的迁移能力和高安全性，显著优于现有连续控制基线。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/SIGN_Safety-Aware_Image-Goal_Navigation_for_Autonomous_Drones_via_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-Robot-Trains-Robot-Automatic-Real-World-Policy-Adaptation-and-Learning-for-Humanoids"><a href="#28-Robot-Trains-Robot-Automatic-Real-World-Policy-Adaptation-and-Learning-for-Humanoids" class="headerlink" title="28. Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Robot_Trains_Robot__Automatic_Real-World_Policy_Adaptation_and_Learning_for_Humanoids.pdf">Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University</span></p><p>该论文提出了一种名为Robot-Trains-Robot (RTR) 的新型教师-学生机器人框架，利用带有力反馈的机械臂作为教师，主动保护和引导人形机器人进行安全高效的现实世界强化学习。方法核心包括自动化课程生成、奖励设计、扰动与重置机制，以及基于动态感知的潜变量优化的三阶段RL适应流程。实验在步行速度跟踪和摆动起身等任务上验证了RTR系统可显著提升现实世界人形机器人学习效率和泛化能力。结论：RTR能实现高效、稳定、低人干预的人形机器人现实世界自适应与学习。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Robot_Trains_Robot_Automatic_Real-World_Policy_Adaptation_and_Learning_for_Humanoids.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Improving-Pre-Trained-Vision-Language-Action-Policies-with-Model-Based-Search"><a href="#29-Improving-Pre-Trained-Vision-Language-Action-Policies-with-Model-Based-Search" class="headerlink" title="29. Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Improving_Pre-Trained_Vision-Language-Action_Policies_with_Model-Based_Search.pdf">Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mila — Quebec AI Institute</span></p><p>本文提出了Vision-Language-Action Planning &amp; Search (VLAPS) 框架，将模型搜索（以蒙特卡洛树搜索MCTS为核心）集成到预训练视觉-语言-动作（VLA）策略推理过程中，通过VLA模型生成上下文相关的宏动作库并指导搜索，有效缩小机器人任务中的搜索空间并提升决策性能。实验结果表明，VLAPS在多种语言指定的机器人操作任务中显著提升了成功率（最高提升67个百分点），不需要额外训练，且测试时性能随VLA模型能力提升而提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Improving_Pre-Trained_Vision-Language-Action_Policies_with_Model-Based_Search.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-Bimanual-Robot-Assisted-Dressing-A-Spherical-Coordinate-Based-Strategy-for-Tight-Fitting-Garments"><a href="#30-Bimanual-Robot-Assisted-Dressing-A-Spherical-Coordinate-Based-Strategy-for-Tight-Fitting-Garments" class="headerlink" title="30. Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Bimanual_Robot-Assisted_Dressing__A_Spherical_Coordinate-Based_Strategy_for_Tight-Fitting_Garments.pdf">Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of York</span></p><p>本文提出了一种用于紧身衣物的双臂机器人辅助穿衣策略，通过建立球面坐标系来编码和生成针对不同人体手臂姿态的穿衣轨迹，将方位角作为双臂协作的任务相关特征，并利用高斯混合模型(GMM)和高斯混合回归(GMR)进行模仿学习。实验结果表明该方法在紧身和宽松衣物穿戴时均具有较高有效性，尤其在紧身衣物场景下双臂策略明显优于单臂策略。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Bimanual_Robot-Assisted_Dressing_A_Spherical_Coordinate-Based_Strategy_for_Tight-Fitting_Garments.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-Humanoid-Motion-Scripting-with-Postural-Synergies"><a href="#31-Humanoid-Motion-Scripting-with-Postural-Synergies" class="headerlink" title="31. Humanoid Motion Scripting with Postural Synergies"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Humanoid_Motion_Scripting_with_Postural_Synergies.pdf">Humanoid Motion Scripting with Postural Synergies</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University</span></p><p>本文提出了SynSculptor框架，通过对3小时20人MoCap数据进行基于动量分段与PCA的姿态协同提取，实现了无需训练的人形机器人类人运动脚本编辑。方法包括实时映射人体运动至仿真机器人、构建风格化协同库、基于协同子空间对生成动作进行平滑度和足滑率等指标评估，并结合MotionGPT文本生成实现风格可调的人形姿态。结论显示，该方法能以低维协同高效生成多样、平滑、类人风格动作，提升机器人运动的物理真实性和表达力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Humanoid_Motion_Scripting_with_Postural_Synergies.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-Manipulate-to-Navigate-Reinforcement-Learning-with-Visual-Affordances-and-Manipulability-Priors"><a href="#32-Manipulate-to-Navigate-Reinforcement-Learning-with-Visual-Affordances-and-Manipulability-Priors" class="headerlink" title="32. Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances and Manipulability Priors"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Manipulate-to-Navigate__Reinforcement_Learning_with_Visual_Affordances_and_Manipulability_Priors.pdf">Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances and Manipulability Priors</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Aalto University</span></p><p>本文提出了一种融合视觉可供性图与可操作性先验的强化学习方法，专门解决移动机器人在动态环境下必须主动操控障碍物以实现导航的“manipulate-to-navigate”任务。该方法通过结合基于机器人结构的可操作性先验和由视觉基础模型生成的可供性图，显著缩小有效动作空间，加速训练并提高任务转移能力。实验在仿真和真实波士顿动力Spot机器人上验证，在Reach和Door两个新任务中，提出方法在1000步内训练成功率超85%，真实环境下任务成功率达80%，并提升了机器人本体的移动距离。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Manipulate-to-Navigate_Reinforcement_Learning_with_Visual_Affordances_and_Manipulability_Priors.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-Grounding-Actions-in-Camera-Space-Observation-Centric-Vision-Language-Action-Policy"><a href="#33-Grounding-Actions-in-Camera-Space-Observation-Centric-Vision-Language-Action-Policy" class="headerlink" title="33. Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Grounding_Actions_in_Camera_Space__Observation-Centric_Vision-Language-Action_Policy.pdf">Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出了Observation-Centric VLA (OC-VLA)框架，通过将机器人末端执行器动作从机器人基座坐标系转换到摄像头坐标系，利用摄像头外参数对齐感知与动作空间，从而提升视觉-语言-动作模型在不同视角下的泛化能力和鲁棒性。实验证明该方法无需修改现有模型结构，可显著提升机器人操作任务的收敛速度、成功率及跨视角适应性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Grounding_Actions_in_Camera_Space_Observation-Centric_Vision-Language-Action_Policy.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-On-the-complexity-of-constrained-reconfiguration-and-motion-planning"><a href="#34-On-the-complexity-of-constrained-reconfiguration-and-motion-planning" class="headerlink" title="34. On the complexity of constrained reconfiguration and motion planning"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/On_the_complexity_of_constrained_reconfiguration_and_motion_planning.pdf">On the complexity of constrained reconfiguration and motion planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CNRS - Université de Montréal CRM - CNRS</span></p><p>本文提出并系统分析了受约束多机器人臂运动规划（RAMP）问题及其图论推广k-Compatible Ordering与k-Compatible Set Arrangement问题，揭示了这些问题在一般情况下的NP完全性和PSPACE完全性。文章进一步提出针对特定图结构（如有向图联合树宽或模宽有界）的多项式时间算法，并证明了在几何和拓扑限制下问题的复杂性边界，为多机器人系统协调规划的理论基础提供了新见解。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/On_the_complexity_of_constrained_reconfiguration_and_motion_planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Large-VLM-based-Vision-Language-Action-Models-for-Robotic-Manipulation-A-Survey"><a href="#35-Large-VLM-based-Vision-Language-Action-Models-for-Robotic-Manipulation-A-Survey" class="headerlink" title="35. Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Large_VLM-based_Vision-Language-Action_Models_for_Robotic_Manipulation__A_Survey.pdf">Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology (Shenzhen)</span></p><p>本文系统综述了基于大规模视觉-语言模型（VLM）的视觉-语言-动作（VLA）模型在机器人操作中的最新研究进展，提出了单体（单系统和双系统）与分层（规划器-政策）两大架构，并详细分析了模型在多模态融合、指令理解、泛化能力、强化学习优化等关键技术环节。结论指出，大型VLM驱动的VLA模型显著提升了机器人对开放世界任务的泛化与复杂操作的执行能力，未来应重点发展长期记忆机制、移动操作、协作和模型高效化等方向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Large_VLM-based_Vision-Language-Action_Models_for_Robotic_Manipulation_A_Survey.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-MaskSem-Semantic-Guided-Masking-for-Learning-3D-Hybrid-High-Order-Motion-Representation"><a href="#36-MaskSem-Semantic-Guided-Masking-for-Learning-3D-Hybrid-High-Order-Motion-Representation" class="headerlink" title="36. MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MaskSem__Semantic-Guided_Masking_for_Learning_3D_Hybrid_High-Order_Motion_Representation.pdf">MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">北京邮电大学</span></p><p>该论文提出MaskSem，一种结合语义引导动态关节遮蔽和混合高阶运动重建的自监督骨骼动作识别方法。通过Grad-CAM生成关节重要性，采用概率遮蔽引导模型聚焦于动作判别性关节，并以关节速度与加速度的混合高阶运动为重建目标，显著提升模型对复杂运动的理解和判别能力。实验结果表明MaskSem在NTU60、NTU120和PKU-MMD数据集上性能优异，特别适用于机器人与人类互动场景下的动作识别任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/MaskSem_Semantic-Guided_Masking_for_Learning_3D_Hybrid_High-Order_Motion_Representation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-Simultaneous-Contact-Sequence-and-Patch-Planning-for-Dynamic-Locomotion"><a href="#37-Simultaneous-Contact-Sequence-and-Patch-Planning-for-Dynamic-Locomotion" class="headerlink" title="37. Simultaneous Contact Sequence and Patch Planning for Dynamic Locomotion"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Simultaneous_Contact_Sequence_and_Patch_Planning_for_Dynamic_Locomotion.pdf">Simultaneous Contact Sequence and Patch Planning for Dynamic Locomotion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Munich</span></p><p>该论文提出了一种基于蒙特卡洛树搜索（MCTS）与全身轨迹优化（TO）的完整流程，实现了多足机器人在复杂环境下动态运动的同时接触序列与接触区域自动规划。实验结果表明，该方法能高效生成多样、动态一致且无碰撞的运动轨迹，并成功在四足与人形机器人硬件上验证了计划的可转移性和复杂行为生成能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Simultaneous_Contact_Sequence_and_Patch_Planning_for_Dynamic_Locomotion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-RoboRetriever-Single-Camera-Robot-Object-Retrieval-via-Active-and-Interactive-Perception-with-Dynamic-Scene-Graph"><a href="#38-RoboRetriever-Single-Camera-Robot-Object-Retrieval-via-Active-and-Interactive-Perception-with-Dynamic-Scene-Graph" class="headerlink" title="38. RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/RoboRetriever__Single-Camera_Robot_Object_Retrieval_via_Active_and_Interactive_Perception_with_Dynam.pdf">RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出了RoboRetriever框架，利用单个腕部RGB-D摄像头和自然语言指令，在复杂、遮挡环境下实现机器人主动与交互式感知的物体检索。具体方法包括动态层次场景图构建、视觉提示引导的主动视觉、基于VLM推理的任务驱动动作执行。实验结果表明，RoboRetriever在多类真实场景下显著优于现有方法，具备更强的适应性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/RoboRetriever_Single-Camera_Robot_Object_Retrieval_via_Active_and_Interactive_Perception_with_Dynamic_Scene_Graph.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-CAMAR-Continuous-Actions-Multi-Agent-Routing"><a href="#39-CAMAR-Continuous-Actions-Multi-Agent-Routing" class="headerlink" title="39. CAMAR: Continuous Actions Multi-Agent Routing"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CAMAR__Continuous_Actions_Multi-Agent_Routing.pdf">CAMAR: Continuous Actions Multi-Agent Routing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AIRI, Moscow, Russia</span></p><p>本文提出了CAMAR，这是一个专为连续动作空间下多智能体路径规划任务设计的高性能强化学习基准环境。其方法结合基于JAX的高效仿真、支持连续状态与动作空间、多种动态模型（如HolonomicDynamic与DiffDriveDynamic）、多种地图生成方式以及标准化的多难度评测协议，并能与经典规划算法（如RRT*）集成。实验表明，CAMAR兼具高仿真速度、扩展性和对多智能体协调与泛化能力的严格评测，适合作为多智能体强化学习与机器人导航领域的标准测试平台。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/CAMAR_Continuous_Actions_Multi-Agent_Routing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-Social-Allostasis-Or-How-I-Learned-To-Stop-Worrying-and-Love-The-Noise"><a href="#40-Social-Allostasis-Or-How-I-Learned-To-Stop-Worrying-and-Love-The-Noise" class="headerlink" title="40. [Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/[Social]_Allostasis__Or,_How_I_Learned_To_Stop_Worrying_and_Love_The_Noise.pdf">[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent Researcher</span></p><p>本论文提出了一种基于激素类信号转导机制的生物启发型 (social) allostasis 调节计算模型，并在多智能体社会中通过 agent-based modeling 进行实验，比较 homeostatic、allostatic 和 social allostatic 三种调节机制在动态环境中的适应性表现。结果表明，allostatic 和 social allostatic 机制能主动利用环境和社会“噪声”实现内部参数重构，在动态复杂环境下显著提升人工体的生存能力和稳定性，为构建更鲁棒的自组织社会型人工系统提供了新框架。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/%5BSocial%5D_Allostasis_Or_How_I_Learned_To_Stop_Worrying_and_Love_The_Noise.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-A-Hierarchical-Surrogate-Model-for-Efficient-Multi-Task-Parameter-Learning-in-Closed-Loop-Control"><a href="#41-A-Hierarchical-Surrogate-Model-for-Efficient-Multi-Task-Parameter-Learning-in-Closed-Loop-Control" class="headerlink" title="41. A Hierarchical Surrogate Model for Efficient Multi-Task Parameter Learning in Closed-Loop Control"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/A_Hierarchical_Surrogate_Model_for_Efficient_Multi-Task_Parameter_Learning_in_Closed-Loop_Contro.pdf">A Hierarchical Surrogate Model for Efficient Multi-Task Parameter Learning in Closed-Loop Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Darmstadt</span></p><p>该论文提出了一种层次化贝叶斯优化（HBO）方法，通过利用闭环控制系统的结构性知识，构建基于高斯过程的代理模型，实现了高效的多任务控制器参数学习。方法通过学习参数化闭环动态，并利用已知累积方式精确计算任务特定的闭环代价，实现任务间知识迁移与提升数据效率。实验表明，该方法在单任务场景下表现与黑盒贝叶斯优化相当，在多任务和转移学习场景下显著优于基线方法，具备更高的样本效率和适应性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/A_Hierarchical_Surrogate_Model_for_Efficient_Multi-Task_Parameter_Learning_in_Closed-Loop_Control.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-MCTR-Midpoint-Corrected-Triangulation-for-Autonomous-Racing-via-Digital-Twin-Simulation-in-CARLA"><a href="#42-MCTR-Midpoint-Corrected-Triangulation-for-Autonomous-Racing-via-Digital-Twin-Simulation-in-CARLA" class="headerlink" title="42. MCTR: Midpoint-Corrected Triangulation for Autonomous Racing via Digital Twin Simulation in CARLA"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MCTR__Midpoint_Corrected_Triangulation_for_Autonomous_Racing_via_Digital_Twin_Simulation_in_CARLA.pdf">MCTR: Midpoint-Corrected Triangulation for Autonomous Racing via Digital Twin Simulation in CARLA</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了MCTR（Midpoint-Corrected Triangulation for Racing）算法，通过对LiDAR扫描边界的Delaunay三角剖分，采用对向边中点提取和CCMA平滑，提升赛道中心线的平滑性和鲁棒性，并支持2D&#x2F;3D LiDAR数据。算法在CARLA数字孪生仿真和真实车辆上充分验证，显著优于DTR等主流方法，在仿真和实车测试中均表现出更优的圈速、平滑性和稳定性，同时实现了高保真Sim-to-Real迁移。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/MCTR_Midpoint-Corrected_Triangulation_for_Autonomous_Racing_via_Digital_Twin_Simulation_in_CARLA.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-ViLaD-A-Large-Vision-Language-Diffusion-Framework-for-End-to-End-Autonomous-Driving"><a href="#43-ViLaD-A-Large-Vision-Language-Diffusion-Framework-for-End-to-End-Autonomous-Driving" class="headerlink" title="43. ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/ViLaD__A_Large_Vision_Language_Diffusion_Framework_for_End-to-End_Autonomous_Driving.pdf">ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Purdue University</span></p><p>该论文提出ViLaD框架，将大型视觉语言扩散模型（LVLD）引入端到端自动驾驶，采用掩码扩散生成策略，实现驾驶决策序列的并行生成和双向推理，极大提升推断速度和决策质量。实验表明ViLaD在nuScenes数据集上推理速度和规划精度优于主流自回归VLM，并在真实车辆交互停车任务中实现高成功率和低延迟，展示了其实用性和部署价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/ViLaD_A_Large_Vision_Language_Diffusion_Framework_for_End-to-End_Autonomous_Driving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Temporal-and-Rotational-Calibration-for-Event-Centric-Multi-Sensor-Systems"><a href="#44-Temporal-and-Rotational-Calibration-for-Event-Centric-Multi-Sensor-Systems" class="headerlink" title="44. Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Temporal_and_Rotational_Calibration_for_Event-Centric_Multi-Sensor_Systems.pdf">Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hunan University</span></p><p>该论文提出了一种无需靶标的事件相机为核心的多传感器系统的运动驱动时序与旋转外参标定方法，通过事件相机、IMU、RGB相机和LiDAR等异质传感器的旋转运动估计，先通过典型相关分析（CCA）初始化外参与时间偏移，再采用SO(3)连续时间非线性联合优化精细标定。实验结果表明，该方法在公开及自采数据集上标定精度可与靶标法媲美，且在稳定性和灵活性方面优于纯CCA方法，具备更强鲁棒性，适合机器人多传感器融合场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Temporal_and_Rotational_Calibration_for_Event-Centric_Multi-Sensor_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-PROD-Palpative-Reconstruction-of-Deformable-Objects-through-Elastostatic-Signed-Distance-Functions"><a href="#45-PROD-Palpative-Reconstruction-of-Deformable-Objects-through-Elastostatic-Signed-Distance-Functions" class="headerlink" title="45. PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/PROD__Palpative_Reconstruction_of_Deformable_Objects_through_Elastostatic_Signed_Distance_Functions.pdf">PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beckman Institute of Advanced Science and Technology, University of Illinois Urbana-Champaign</span></p><p>该论文提出了PROD（一种基于弹性静力学有符号距离函数的可变形物体触觉重建方法），通过力控探测获取位姿和力数据，构建泊松方程恢复物体的有符号距离场及材料属性（如杨氏模量和曲率），并给出稳态弹性动力学下的收敛性理论。实验表明，PROD在姿态误差、非法向力和曲率误差下具有鲁棒性，并适用于机器人操作、医疗成像和触觉反馈等场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/PROD_Palpative_Reconstruction_of_Deformable_Objects_through_Elastostatic_Signed_Distance_Functions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-Adaptive-Model-Predictive-Control-of-a-Soft-Continuum-Robot-Using-a-Physics-Informed-Neural-Network-Based-on-Cosserat-Rod-Theory"><a href="#46-Adaptive-Model-Predictive-Control-of-a-Soft-Continuum-Robot-Using-a-Physics-Informed-Neural-Network-Based-on-Cosserat-Rod-Theory" class="headerlink" title="46. Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Adaptive_Model-Predictive_Control_of_a_Soft_Continuum_Robot_Using_a_Physics-Informed_Neural_Network_.pdf">Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Leibniz University Hannover</span></p><p>该论文提出了一种基于Cosserat杆理论的域分离物理知识神经网络（DD-PINN），作为软连续体机器人动态模型的高效替代，实现了实时非线性模型预测控制（MPC）及全形状状态估计。通过DD-PINN加速物理模型推理44000倍，并结合无迹卡尔曼滤波（UKF）进行状态与参数自适应估计，在仿真与真实机器人实验中实现了端点误差小于3mm的高精度轨迹跟踪及加速度达3.55m&#x2F;s²。结论表明该方法显著提升了软机器人动态控制的精度、速度和适应性，支持复杂任务和多传感器反馈。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Adaptive_Model-Predictive_Control_of_a_Soft_Continuum_Robot_Using_a_Physics-Informed_Neural_Network_Based_on_Cosserat_Rod_Theory.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="47-Scalable-RF-Simulation-in-Generative-4D-Worlds"><a href="#47-Scalable-RF-Simulation-in-Generative-4D-Worlds" class="headerlink" title="47. Scalable RF Simulation in Generative 4D Worlds"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Scalable_RF_Simulation_in_Generative_4D_Worlds.pdf">Scalable RF Simulation in Generative 4D Worlds</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Pennsylvania</span></p><p>该论文提出了WaveVerse，一个结合生成式4D场景与物理驱动的射频（RF）信号仿真的框架。核心技术包括语言驱动的4D世界生成（通过LLM和状态感知因果Transformer生成受空间约束和文本条件的动态人体动作），以及相位一致的射线追踪RF仿真器，专注于多路径传播和相位一致性以提升成像和感知精度。实验表明，WaveVerse能生成高质量多样化的室内场景和动作，在高分辨率成像与人体活动识别任务中，显著提升了数据有限和充足场景下的模型性能，推动了RF领域数据生成与应用发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Scalable_RF_Simulation_in_Generative_4D_Worlds.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-Infusing-fine-grained-visual-knowledge-to-Vision-Language-Models"><a href="#48-Infusing-fine-grained-visual-knowledge-to-Vision-Language-Models" class="headerlink" title="48. Infusing fine-grained visual knowledge to Vision-Language Models"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Infusing_fine-grained_visual_knowledge_to_Vision-Language_Models.pdf">Infusing fine-grained visual knowledge to Vision-Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Czech Technical University in Prague</span></p><p>本文提出了一种专门为Vision-Language Models (VLMs)设计的微调方法，通过结合参数正则化与嵌入正则化，实现在细粒度视觉领域的适应同时最大程度保留模型的通用视觉和跨模态能力。该方法采用系统性验证集设计和超参数调优，实验表明在多个细粒度与粗粒度检索任务中显著提升了检索性能，且有效防止了灾难性遗忘，保持了视觉文本对齐能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Infusing_fine-grained_visual_knowledge_to_Vision-Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-Enhancing-3D-point-accuracy-of-laser-scanner-through-multi-stage-convolutional-neural-network-for-applications-in-construction"><a href="#49-Enhancing-3D-point-accuracy-of-laser-scanner-through-multi-stage-convolutional-neural-network-for-applications-in-construction" class="headerlink" title="49. Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Enhancing_3D_point_accuracy_of_laser_scanner_through_multi-stage_convolutional_neural_network_for_ap.pdf">Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technische Universität Berlin</span></p><p>该论文提出了一种基于多阶段卷积神经网络（MSCNN）的集成方法，通过将高精度和低精度激光扫描仪的测量结果配对、空间配准，提取误差模式并将三维点云投影转化为二维深度图，采用MSCNN进行误差映射学习，实现低端激光扫描仪点云的高精度修正。实验结果表明，该方法使低端设备在无需硬件升级的情况下，3D点测量均方误差降低超70%，峰值信噪比提升约6dB，几何测量精度大幅提升，具备良好泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Enhancing_3D_point_accuracy_of_laser_scanner_through_multi-stage_convolutional_neural_network_for_applications_in_construction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-Demystifying-Foreground-Background-Memorization-in-Diffusion-Models"><a href="#50-Demystifying-Foreground-Background-Memorization-in-Diffusion-Models" class="headerlink" title="50. Demystifying Foreground-Background Memorization in Diffusion Models"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Demystifying_Foreground-Background_Memorization_in_Diffusion_Models.pdf">Demystifying Foreground-Background Memorization in Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Cheriton School of Computer Science, University of Waterloo</span></p><p>该论文提出了FB-Mem，一种基于分割的指标，用于检测和量化扩散模型生成图像中前景和背景区域的部分记忆现象，超越了以往仅检测完全复制的局限。通过对Stable Diffusion等模型的大规模实证分析，作者揭示了记忆现象更为普遍，且现有神经元剪枝、失活等主流缓解方案无法有效消除局部（尤其是前景）记忆。论文还提出了基于聚类的神经元缓解方法NeMo-C，能更有效缓解一对多记忆现象，且保持生成质量。结论：FB-Mem可更精细地检测部分记忆，NeMo-C在缓解记忆和保持图像质量之间实现更优权衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Demystifying_Foreground-Background_Memorization_in_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-DualFit-A-Two-Stage-Virtual-Try-On-via-Warping-and-Synthesis"><a href="#51-DualFit-A-Two-Stage-Virtual-Try-On-via-Warping-and-Synthesis" class="headerlink" title="51. DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/DualFit__A_Two-Stage_Virtual_Try-On_via_Warping_and_Synthesis.pdf">DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Arkansas</span></p><p>本文提出了DualFit虚拟试衣方法，通过两阶段流程实现高保真与高真实感的融合。第一阶段利用基于流场的变形模块将目标服装与人物图像精准对齐，第二阶段通过保真试穿模块，借助保留区域输入与修复掩码，仅对必要区域进行再生成，实现服装细节（如图案、文字）的清晰保留与无缝融合。实验显示，该方法在重建精度和感知真实感方面均优于当前主流方法，生成图像细节丰富且自然。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/DualFit_A_Two-Stage_Virtual_Try-On_via_Warping_and_Synthesis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-VELVET-Med-Vision-and-Efficient-Language-Pre-training-for-Volumetric-Imaging-Tasks-in-Medicine"><a href="#52-VELVET-Med-Vision-and-Efficient-Language-Pre-training-for-Volumetric-Imaging-Tasks-in-Medicine" class="headerlink" title="52. VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/VELVET-Med__Vision_and_Efficient_Language_Pre-training_for_Volumetric_Imaging_Tasks_in_Medicine.pdf">VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northwestern University</span></p><p>本文提出了VELVET-Med，一种专为医学3D体积成像任务设计的数据高效视觉-语言预训练框架。该方法结合了TriBERT文本编码器、层次化对比学习和单模态自监督目标，实现了多粒度视觉与文本语义对齐，有效提升了3D分割、跨模态检索、视觉问答和报告生成等下游任务的泛化能力。结论显示，VELVET-Med在多项医学视觉-语言任务上均取得了SOTA表现，验证了高质量数据和多层次对齐策略的重要性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/VELVET-Med_Vision_and_Efficient_Language_Pre-training_for_Volumetric_Imaging_Tasks_in_Medicine.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-Error-Propagation-Mechanisms-and-Compensation-Strategies-for-Quantized-Diffusion-Models"><a href="#53-Error-Propagation-Mechanisms-and-Compensation-Strategies-for-Quantized-Diffusion-Models" class="headerlink" title="53. Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion Models"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Error_Propagation_Mechanisms_and_Compensation_Strategies_for_Quantized_Diffusion.pdf">Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ByteDance Inc</span></p><p>本文提出了一种时序感知累计误差补偿策略（TCEC），针对量化扩散模型中的误差传播问题建立了数学理论框架，首次推导出累计误差的闭式解，并利用时序在线估算实现低成本误差校正。实验表明，该方法有效缓解了低比特量化下的误差积累，W4A4精度下在多项指标和多个扩散模型上均取得SOTA表现，且延迟损失极低。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Error_Propagation_Mechanisms_and_Compensation_Strategies_for_Quantized_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-Generic-Event-Boundary-Detection-via-Denoising-Diffusion"><a href="#54-Generic-Event-Boundary-Detection-via-Denoising-Diffusion" class="headerlink" title="54. Generic Event Boundary Detection via Denoising Diffusion"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Generic_Event_Boundary_Detection_via_Denoising_Diffusion.pdf">Generic Event Boundary Detection via Denoising Diffusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Pohang University of Science and Technology (POSTECH)</span></p><p>本文提出了一种基于扩散模型的通用事件边界检测方法DiffGEBD，通过时序自相似编码器提取视频帧间动态变化特征，并采用去噪扩散过程从随机噪声生成多样化且合理的事件边界预测，结合无分类器引导实现多样性可控。实验表明，DiffGEBD在Kinetics-GEBD和TAPOS数据集上实现了多样且与真实分界高度一致的预测，并提出了结合多样性与准确性的全新评价指标，对事件边界检测任务具有显著提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Generic_Event_Boundary_Detection_via_Denoising_Diffusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-Q-FSRU-Quantum-Augmented-Frequency-Spectral-Fusion-for-Medical-Visual-Question-Answering"><a href="#55-Q-FSRU-Quantum-Augmented-Frequency-Spectral-Fusion-for-Medical-Visual-Question-Answering" class="headerlink" title="55. Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Q-FSRU__Quantum-Augmented_Frequency-Spectral_Fusion_for_Medical_Visual_Question_Answering.pdf">Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amity University</span></p><p>该论文提出Q-FSRU模型，将医学图像和文本特征通过快速傅里叶变换（FFT）转换到频域，并融合为频谱特征，结合量子启发式检索增强生成（Quantum RAG）模块，从外部医学知识库中检索相关信息，以提升医学视觉问答（VQA）系统的推理能力和可解释性。实验结果表明，该方法在VQA-RAD医疗影像问答数据集上显著优于现有方法，在准确率、精确率、召回率、F1分数和ROC-AUC等多项指标上均取得了优异表现，证明频域融合和量子检索能有效提升医学多模态AI系统的实用性与解释力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Q-FSRU_Quantum-Augmented_Frequency-Spectral_Fusion_for_Medical_Visual_Question_Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-ENA-Efficient-N-dimensional-Attention"><a href="#56-ENA-Efficient-N-dimensional-Attention" class="headerlink" title="56. ENA: Efficient N-dimensional Attention"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/ENA__Efficient_N-dimensional_Attention.pdf">ENA: Efficient N-dimensional Attention</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent Researcher</span></p><p>该论文提出了一种高效的N维注意力（ENA）架构，通过交替堆叠线性递归（如DeltaNet）与高阶滑动窗口注意力（SWA，主要采用Sliding Tile Attention, STA），用于高阶数据（如图像、视频等）的长序列建模。实验表明，ENA相比扫描式线性模型，能在计算效率显著提升的同时达到或超过Transformer的表现，且在ImageNet分类、图像与视频生成等任务上实现了较高的准确率和生成质量，硬件效率也优于传统Transformer。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/ENA_Efficient_N-dimensional_Attention.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-Data-Shift-of-Object-Detection-in-Autonomous-Driving"><a href="#57-Data-Shift-of-Object-Detection-in-Autonomous-Driving" class="headerlink" title="57. Data Shift of Object Detection in Autonomous Driving"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Data_Shift_of_Object_Detection_in_Autonomous_Driving.pdf">Data Shift of Object Detection in Autonomous Driving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southern University of Science and Technology</span></p><p>本文针对自动驾驶中目标检测任务面临的数据分布偏移问题，提出基于数据偏移检测和CycleGAN风格迁移的数据增强方法，结合YOLOv5框架，有效缓解了日间与夜间场景分布不一致带来的性能下降。实验结果表明，该方法能提升在夜间场景下的检测性能，显著降低模型对数据分布变化的敏感性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Data_Shift_of_Object_Detection_in_Autonomous_Driving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-Impact-of-Clinical-Image-Quality-on-Efficient-Foundation-Model-Finetuning"><a href="#58-Impact-of-Clinical-Image-Quality-on-Efficient-Foundation-Model-Finetuning" class="headerlink" title="58. Impact of Clinical Image Quality on Efficient Foundation Model Finetuning"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Impact_of_Clinical_Image_Quality_on_Efficient_Foundation_Model_Finetuning.pdf">Impact of Clinical Image Quality on Efficient Foundation Model Finetuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">UCL Hawkes Insitute, University College London</span></p><p>该论文提出通过在ProFound医学影像基础模型上，系统性评估临床前列腺MRI图像质量对微调下游任务（如癌症检测和分级、影像报告等）的泛化能力和标签效率影响。通过多种高低质量图像配比的微调与测试实验，发现高质量图像在微调集中的占比对模型性能至关重要，并且下游任务对图像质量分布敏感，建议针对实际应用场景量化和匹配数据质量以充分发挥基础模型优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Impact_of_Clinical_Image_Quality_on_Efficient_Foundation_Model_Finetuning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-UniUGG-Unified-3D-Understanding-and-Generation-via-Geometric-Semantic-Encoding"><a href="#59-UniUGG-Unified-3D-Understanding-and-Generation-via-Geometric-Semantic-Encoding" class="headerlink" title="59. UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/UniUGG__Unified_3D_Understanding_and_Generation_via_Geometric-Semantic_Encoding.pdf">UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出了UniUGG——首个支持三维空间理解与生成的LLM统一框架，通过引入几何-语义联合预训练的视觉编码器（ViT）和空间VAE，结合LLM和扩散模型，能够实现空间级VQA和几何一致的3D场景生成。实验表明，UniUGG在空间推理和三维生成任务上显著超越了现有方法，提升了三维结构推理与视觉-语言一体化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/UniUGG_Unified_3D_Understanding_and_Generation_via_Geometric-Semantic_Encoding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-M3OOD-Automatic-Selection-of-Multimodal-OOD-Detectors"><a href="#60-M3OOD-Automatic-Selection-of-Multimodal-OOD-Detectors" class="headerlink" title="60. M3OOD: Automatic Selection of Multimodal OOD Detectors"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/M3OOD__Automatic_Selection_of_Multimodal_OOD_Detectors.pdf">M3OOD: Automatic Selection of Multimodal OOD Detectors</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Southern California</span></p><p>该论文提出了M3OOD，一种基于元学习的多模态分布外检测（OOD）模型自动选择框架。方法融合多模态特征嵌入与手工设计的元特征，利用历史性能数据训练元学习预测器，实现无需测试数据标签的零样本模型选择。实验表明，M3OOD在视频、光流等多模态场景下优于现有10种主流基线，显著提升了检测准确率并降低了计算开销。结论：M3OOD可高效稳定地推荐适合的新分布OOD检测器，适用于复杂多模态任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/M3OOD_Automatic_Selection_of_Multimodal_OOD_Detectors.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-Large-Kernel-Modulation-Network-for-Efficient-Image-Super-Resolution"><a href="#61-Large-Kernel-Modulation-Network-for-Efficient-Image-Super-Resolution" class="headerlink" title="61. Large Kernel Modulation Network for Efficient Image Super-Resolution"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Large_Kernel_Modulation_Network_for_Efficient_Image_Super-Resolution.pdf">Large Kernel Modulation Network for Efficient Image Super-Resolution</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yanshan University</span></p><p>本文提出了一种高效的纯CNN结构用于图像超分辨率——Large Kernel Modulation Network (LKMN)，其核心为Enhanced Partial Large Kernel Block（EPLKB）和Cross-Gate Feed-Forward Network（CGFN）。EPLKB通过通道分组、通道洗牌和大核条带卷积实现高效的非局部特征提取，CGFN则自适应调整本地与非本地特征差异并采用交叉门控机制进行特征融合。大量实验表明，LKMN在保持较低计算复杂度和推理速度的同时，超越了现有轻量级SR方法，尤其在Manga109数据集上提升显著，且推理速度大幅优于Transformer方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Large_Kernel_Modulation_Network_for_Efficient_Image_Super-Resolution.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-COMPLICITSPLAT-Downstream-Models-are-Vulnerable-to-Blackbox-Attacks-by-3D-Gaussian-Splat-Camouflages"><a href="#62-COMPLICITSPLAT-Downstream-Models-are-Vulnerable-to-Blackbox-Attacks-by-3D-Gaussian-Splat-Camouflages" class="headerlink" title="62. COMPLICITSPLAT: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/ComplicitSplat__Downstream_Models_are_Vulnerable_to_Blackbox_Attacks_by_3D_Gaussian_Splat_Camouflage.pdf">COMPLICITSPLAT: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Georgia Tech</span></p><p>该论文提出COMPLICITSPLAT，一种利用3D Gaussian Splatting中球谐(Spherical Harmonics)渲染属性进行黑盒攻击的方法，通过在训练数据中嵌入多角度可见的伪装纹理，实现物体在特定视角下呈现对下游检测模型具有误导性的外观。实验表明，COMPLICITSPLAT能在数字和真实场景下有效影响YOLO、FasterRCNN和DETR等主流目标检测器，造成显著检测精度下降，揭示3DGS场景在安全关键应用中的新型风险。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/COMPLICITSPLAT_Downstream_Models_are_Vulnerable_to_Blackbox_Attacks_by_3D_Gaussian_Splat_Camouflages.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-MuSACo-Multimodal-Subject-Specific-Selection-and-Adaptation-for-Expression-Recognition-with-Co-Training"><a href="#63-MuSACo-Multimodal-Subject-Specific-Selection-and-Adaptation-for-Expression-Recognition-with-Co-Training" class="headerlink" title="63. MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MuSACo__Multimodal_Subject-Specific_Selection_and_Adaptation_for_Expression_Recognition_with_Co-Trai.pdf">MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ETS Montreal</span></p><p>MuSACo提出了一种针对表情识别的多模态、多源、个体化自适应方法，通过协同训练选择与目标最相关的源主体，结合视觉和生理模态互补信息，并采用伪标签生成、类别感知与类别无关损失，实现跨主体和跨模态特征对齐。实验结果表明，MuSACo在BioVid和StressID多模态表情识别数据集上优于现有的UDA和MSDA方法，提升了个体化表达识别的准确性和稳健性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/MuSACo_Multimodal_Subject-Specific_Selection_and_Adaptation_for_Expression_Recognition_with_Co-Training.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-An-Initial-Study-of-Bird’s-Eye-View-Generation-for-Autonomous-Vehicles-using-Cross-View-Transformers"><a href="#64-An-Initial-Study-of-Bird’s-Eye-View-Generation-for-Autonomous-Vehicles-using-Cross-View-Transformers" class="headerlink" title="64. An Initial Study of Bird’s-Eye View Generation for Autonomous Vehicles using Cross-View Transformers"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/An_Initial_Study_of_Bird's-Eye_View_Generation_for_Autonomous_Vehicles_using_Cross-View_Transformers.pdf">An Initial Study of Bird’s-Eye View Generation for Autonomous Vehicles using Cross-View Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Federal University of Santa Catarina</span></p><p>本文提出利用Cross-View Transformers (CVT) 从多摄像头图像生成自动驾驶所需的鸟瞰图（BEV），并将道路、车道线和规划轨迹作为多语义通道输出。实验基于CARLA模拟器，比较了CVT与Unet两种架构在不同损失函数和摄像头布局下的泛化性能。结论显示，CVT在复杂场景（如交叉口和转弯）及对未见环境的泛化能力上优于Unet，且三前置摄像头配置足以支撑高性能，L1损失更具鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/An_Initial_Study_of_Bird%E2%80%99s-Eye_View_Generation_for_Autonomous_Vehicles_using_Cross-View_Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-Skin-Cancer-Classification-Hybrid-CNN-Transformer-Models-with-KAN-Based-Fusion"><a href="#65-Skin-Cancer-Classification-Hybrid-CNN-Transformer-Models-with-KAN-Based-Fusion" class="headerlink" title="65. Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Skin_Cancer_Classification__Hybrid_CNN-Transformer_Models_with_KAN-Based_Fusion.pdf">Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indian Institute of Technology Guwahati</span></p><p>本论文提出了一种基于混合CNN-Transformer模型和卷积Kolmogorov-Arnold网络（CKAN）特征融合的皮肤癌分类方法。方法流程包括CNN提取局部空间特征、Transformer建模全局依赖关系、CKAN实现非线性特征融合，并结合迁移学习和数据增强，模型在HAM10000、BCN20000和PAD-UFES等多数据集上进行泛化评估。实验结果显示，该方法在多个基准数据集上均取得优异分类准确率（如HAM10000上准确率92.81%、F1分数92.47%），验证了混合结构和CKAN特征融合对提升皮肤癌智能诊断的有效性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Skin_Cancer_Classification_Hybrid_CNN-Transformer_Models_with_KAN-Based_Fusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-FractMorph-A-Fractional-Fourier-Based-Multi-Domain-Transformer-for-Deformable-Image-Registration"><a href="#66-FractMorph-A-Fractional-Fourier-Based-Multi-Domain-Transformer-for-Deformable-Image-Registration" class="headerlink" title="66. FractMorph: A Fractional Fourier-Based Multi-Domain Transformer for Deformable Image Registration"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/FractMorph__A_Fractional_Fourier-Based_Multi-Domain_Transformer_for_Deformable_Image_Registration.pdf">FractMorph: A Fractional Fourier-Based Multi-Domain Transformer for Deformable Image Registration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shahid Beheshti University</span></p><p>该论文提出了FractMorph，一种结合多域分数傅里叶变换（FrFT）和3D Transformer的医学图像可变形配准方法。方法通过多分支FrFT特征提取和双流Transformer交互，实现了局部、半全局和全局特征的融合，最终通过轻量级U-Net生成高分辨率形变场。实验结果显示，FractMorph在ACDC心脏MRI数据集上取得了最优配准准确率，同时保持合理的计算资源消耗和变形场光滑性，且轻量模型依然具备高性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/FractMorph_A_Fractional_Fourier-Based_Multi-Domain_Transformer_for_Deformable_Image_Registration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-S5-Scalable-Semi-Supervised-Semantic-Segmentation-in-Remote-Sensing"><a href="#67-S5-Scalable-Semi-Supervised-Semantic-Segmentation-in-Remote-Sensing" class="headerlink" title="67. S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/S5__Scalable_Semi-Supervised_Semantic_Segmentation_in_Remote_Sensing.pdf">S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>本文提出S5框架，通过结合低熵过滤和多样性扩展，构建了大规模遥感图像数据集RS4P-1M，实现了基于FixMatch一致性正则化的半监督预训练，提升遥感基础模型（RSFMs）在多数据集上的表现。进一步引入MoE混合专家多数据集微调方案，实现了参数高效的多任务适应，实验在多项遥感分割和检测基准上取得SOTA表现，验证了大规模半监督学习在遥感领域的可行性与优越性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/S5_Scalable_Semi-Supervised_Semantic_Segmentation_in_Remote_Sensing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-Federated-Cross-Modal-Style-Aware-Prompt-Generation"><a href="#68-Federated-Cross-Modal-Style-Aware-Prompt-Generation" class="headerlink" title="68. Federated Cross-Modal Style-Aware Prompt Generation"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Federated_Cross-Modal_Style-Aware_Prompt_Generation.pdf">Federated Cross-Modal Style-Aware Prompt Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indian Institute of Technology Bombay</span></p><p>本文提出FedCSAP框架，通过融合CLIP视觉编码器的多尺度特征和客户端特有的风格指示器，利用注入模块将多模态信息（文本嵌入、视觉特征和风格统计）动态生成上下文感知且多样化的提示向量。实验结果表明，FedCSAP在非IID数据和多样域风格下，显著提升了在见类和未见类上的泛化能力及准确率，优于现有联邦提示学习方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Federated_Cross-Modal_Style-Aware_Prompt_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-IPGPhormer-Interpretable-Pathology-Graph-Transformer-for-Survival-Analysis"><a href="#69-IPGPhormer-Interpretable-Pathology-Graph-Transformer-for-Survival-Analysis" class="headerlink" title="69. IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/IPGPhormer__Interpretable_Pathology_Graph-Transformer_for_Survival_Analysis.pdf">IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology</span></p><p>本文提出了IPGPhormer框架，通过多尺度异质与同质图构建，结合图注意网络和Transformer模块，有效建模组织微环境的空间依赖，实现对病理切片的细粒度风险预测与自解释。实验结果显示，该方法在四个癌症公开数据集上准确率和解释性均优于现有方法，为癌症预后评估和临床决策提供了可靠工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/IPGPhormer_Interpretable_Pathology_Graph-Transformer_for_Survival_Analysis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-MBMamba-When-Memory-Buffer-Meets-Mamba-for-Structure-Aware-Image-Deblurring"><a href="#70-MBMamba-When-Memory-Buffer-Meets-Mamba-for-Structure-Aware-Image-Deblurring" class="headerlink" title="70. MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MBMamba__When_Memory_Buffer_Meets_Mamba_for_Structure-Aware_Image_Deblurring.pdf">MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了一种结构感知的图像去模糊网络MBMamba，结合了Mamba状态空间模型与创新的内存缓冲机制（MemVSSM），用于高效地整合局部与全局特征，无需增加扫描次数或额外模块。方法还引入了受Ising模型启发的正则化损失以增强像素结构连贯性。实验结果显示，MBMamba在合成（GoPro、HIDE）和真实（RealBlur）数据集上均超越当前主流方法，并显著降低计算开销。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/MBMamba_When_Memory_Buffer_Meets_Mamba_for_Structure-Aware_Image_Deblurring.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-SRMA-Mamba-Spatial-Reverse-Mamba-Attention-Network-for-Pathological-Liver-Segmentation-in-MRI-Volumes"><a href="#71-SRMA-Mamba-Spatial-Reverse-Mamba-Attention-Network-for-Pathological-Liver-Segmentation-in-MRI-Volumes" class="headerlink" title="71. SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/SRMA-Mamba__Spatial_Reverse_Mamba_Attention_Network_for_Pathological_Liver_Segmentation_in_MRI_Volum.pdf">SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Software Engineering, Chongqing University of Posts and Telecommunications</span></p><p>该论文提出了一种新颖的空间反向Mamba注意力网络（SRMA-Mamba），专为MRI体积中的病理性肝脏分割设计。方法采用SABMamba模块对三大解剖平面进行扫描，整合多视角空间特征，并引入空间反向Mamba注意力模块（SRMA）实现多级边界细化，提升分割精度。实验结果显示，SRMA-Mamba在CirrMRI600+ T1W和T2W数据集上均优于现有主流方法，在3D肝脏体积分割任务中达到最优性能，验证了方法的有效性和稳定性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/SRMA-Mamba_Spatial_Reverse_Mamba_Attention_Network_for_Pathological_Liver_Segmentation_in_MRI_Volumes.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-ViT-EnsembleAttack-Augmenting-Ensemble-Models-for-Stronger-Adversarial-Transferability-in-Vision-Transformers"><a href="#72-ViT-EnsembleAttack-Augmenting-Ensemble-Models-for-Stronger-Adversarial-Transferability-in-Vision-Transformers" class="headerlink" title="72. ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/ViT-EnsembleAttack__Augmenting_Ensemble_Models_for_Stronger_Adversarial_Transferability_in_Vision_Tr.pdf">ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huazhong University of Science and Technology</span></p><p>该论文提出ViT-EnsembleAttack，一种针对视觉Transformer（ViT）设计的全新集成对抗攻击方法。核心方法包括通过多头丢弃、注意力分数缩放和MLP特征混合三种策略对每个ViT模型进行对抗增强，利用贝叶斯优化自动调整参数，并结合自动重加权与步长放大模块提升攻击效率和迁移性。实验表明，该方法在ViTs和CNNs上均显著提升了对抗样本的迁移攻击成功率，平均提升幅度达15.3%，优于现有最优方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/ViT-EnsembleAttack_Augmenting_Ensemble_Models_for_Stronger_Adversarial_Transferability_in_Vision_Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-Navigating-the-Exploration–Exploitation-Tradeoff-in-Inference-Time-Scaling-of-Diffusion-Models"><a href="#73-Navigating-the-Exploration–Exploitation-Tradeoff-in-Inference-Time-Scaling-of-Diffusion-Models" class="headerlink" title="73. Navigating the Exploration–Exploitation Tradeoff in Inference-Time Scaling of Diffusion Models"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Navigating_the_Exploration-Exploitation_Tradeoff_in_Inference-Time_Scaling_of_Diffusion_Models.pdf">Navigating the Exploration–Exploitation Tradeoff in Inference-Time Scaling of Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Waseda University</span></p><p>本文针对扩散模型推理阶段的探索-利用权衡问题，提出了两种改进Sequential Monte Carlo（SMC）采样方法：Funnel Schedule（分阶段减少粒子数）和Adaptive Temperature（自适应温度调整），以提升多模态搜索中的采样效率和样本质量。实验结果显示，该方法在固定算力预算下，在多项主流文本生成图像模型上获得了比现有方法更优的图文对齐和生成质量，且理论上保持收敛性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Navigating_the_Exploration%E2%80%93Exploitation_Tradeoff_in_Inference-Time_Scaling_of_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-Attention-Pooling-Enhances-NCA-based-Classification-of-Microscopy-Images"><a href="#74-Attention-Pooling-Enhances-NCA-based-Classification-of-Microscopy-Images" class="headerlink" title="74. Attention Pooling Enhances NCA-based Classification of Microscopy Images"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Attention_Pooling_Enhances_NCA-based_Classification_of_Microscopy_Images.pdf">Attention Pooling Enhances NCA-based Classification of Microscopy Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of AI for Health, Computational Health Center, Helmholtz Munich</span></p><p>该论文提出了一种结合神经元细胞自动机（Neural Cellular Automata, NCA）与注意力池化（attention pooling）机制的轻量级显微镜图像分类模型aNCA。方法通过NCA提取图像特征，并利用可学习的空间注意力池化聚焦于最有信息的区域，仅保留每个特征通道前10%的激活，有效提升了分类准确率和模型解释性。实验结果表明，aNCA在八个临床显微镜图像数据集上均显著优于现有NCA和主流轻量级架构（CNN、ViT），且参数量远少于传统方法。结论：aNCA在医学显微镜图像分类任务上表现优越，兼具高效、鲁棒和可解释性，适用于低资源医疗场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Attention_Pooling_Enhances_NCA-based_Classification_of_Microscopy_Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-CLAIR-CLIP-Aided-Weakly-Supervised-Zero-Shot-Cross-Domain-Image-Retrieval"><a href="#75-CLAIR-CLIP-Aided-Weakly-Supervised-Zero-Shot-Cross-Domain-Image-Retrieval" class="headerlink" title="75. CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CLAIR__CLIP-Aided_Weakly_Supervised_Zero-Shot_Cross-Domain_Image_Retrieval.pdf">CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>该论文提出CLAIR框架，针对弱监督零样本跨域图像检索（WSZS-CDIR）任务，利用CLIP生成的伪标签，并通过置信度评分进行伪标签优化，引入多粒度对比损失（实例、簇、域）对特征进行语义对齐；设计了基于文本嵌入的封闭式跨域映射函数，进一步缩小领域差异，并结合可学习prompt提升零样本泛化能力。实验表明，CLAIR在多个跨域检索基准（如DomainNet Zero-Shot、Sketchy等）上均优于现有方法，显著提升检索准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/CLAIR_CLIP-Aided_Weakly_Supervised_Zero-Shot_Cross-Domain_Image_Retrieval.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-iTrace-Click-Based-Gaze-Visualization-on-the-Apple-Vision-Pro"><a href="#76-iTrace-Click-Based-Gaze-Visualization-on-the-Apple-Vision-Pro" class="headerlink" title="76. iTrace: Click-Based Gaze Visualization on the Apple Vision Pro"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/iTrace__Click-Based_Gaze_Visualization_on_the_Apple_Vision_Pro.pdf">iTrace: Click-Based Gaze Visualization on the Apple Vision Pro</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Munich</span></p><p>该论文提出iTrace系统，采用点击事件（如捏合手势、悬停控制和游戏控制器）在Apple Vision Pro设备上采集和可视化用户凝视数据，通过客户端-服务器架构生成动态热图用于视频和真实空间的眼动追踪分析。实验结果显示，游戏控制器方式数据采集频率更高，热图更密集，精度平均达到91%，为教育、环境设计、市场分析和临床认知评估等领域提供了有效的视觉关注分析工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/iTrace_Click-Based_Gaze_Visualization_on_the_Apple_Vision_Pro.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-Superpixel-informed-Continuous-Low-Rank-Tensor-Representation-for-Multi-Dimensional-Data-Recovery"><a href="#77-Superpixel-informed-Continuous-Low-Rank-Tensor-Representation-for-Multi-Dimensional-Data-Recovery" class="headerlink" title="77. Superpixel-informed Continuous Low-Rank Tensor Representation for Multi-Dimensional Data Recovery"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Superpixel-informed_Continuous_Low-Rank_Tensor_Representation_for_Multi-Dimensional_Data_Recovery.pdf">Superpixel-informed Continuous Low-Rank Tensor Representation for Multi-Dimensional Data Recovery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computing and Artificial Intelligence, Southwest Jiaotong University</span></p><p>本文提出了Superpixel-informed Continuous Low-Rank Tensor Representation (SCTR) 框架，通过将多维数据分割为语义一致的超像素区域，并在每个区域内采用连续的Asymmetric Low-rank Tensor Factorization (ALTF) 神经网络建模，实现了高效且灵活的张量恢复。实验证明，SCTR在多光谱图像、视频及彩色图像的补全任务中，PSNR提升高达3-5 dB，并显著优于现有主流方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Superpixel-informed_Continuous_Low-Rank_Tensor_Representation_for_Multi-Dimensional_Data_Recovery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-DermINO-Hybrid-Pretraining-for-a-Versatile-Dermatology-Foundation-Model"><a href="#78-DermINO-Hybrid-Pretraining-for-a-Versatile-Dermatology-Foundation-Model" class="headerlink" title="78. DermINO: Hybrid Pretraining for a Versatile Dermatology Foundation Model"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/DermINO__Hybrid_Pretraining_for_a_Versatile_Dermatology_Foundation_Model.pdf">DermINO: Hybrid Pretraining for a Versatile Dermatology Foundation Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Department of Dermatology, China-Japan Friendship Hospital</span></p><p>该论文提出了DermINO，一种专为皮肤病学设计的基础模型，采用混合预训练框架，结合自监督学习、部分有监督学习和医学知识引导的原型初始化，在包含43万余张多源皮肤图像的大型数据集上进行训练。DermINO在20个下游皮肤病学任务数据集上全面超越现有模型，涵盖恶性肿瘤分类、疾病严重程度分级、多类别诊断、图像描述和病灶分割等任务，具备强泛化能力与公平性，并在联邦学习和AI辅助医生诊断中表现优异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/DermINO_Hybrid_Pretraining_for_a_Versatile_Dermatology_Foundation_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-C2PSA-Enhanced-YOLOv11-Architecture-A-Novel-Approach-for-Small-Target-Detection-in-Cotton-Disease-Diagnosis"><a href="#79-C2PSA-Enhanced-YOLOv11-Architecture-A-Novel-Approach-for-Small-Target-Detection-in-Cotton-Disease-Diagnosis" class="headerlink" title="79. C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/C2PSA-Enhanced_YOLOv11_Architecture__A_Novel_Approach_for_Small_Target_Detection_in_Cotton_Disease_D.pdf">C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">College of Big Data, Yunnan Agricultural University</span></p><p>本论文提出在YOLOv11基础上引入C2PSA（通道与空间注意力）模块、动态类别权重分配、Mosaic-MixUp等数据增强优化，提升棉花病害小目标检测性能。实验证明改进模型mAP50提高至0.820，mAP50-95达0.705，并具备158FPS的实时推理速度，可移动端部署，实现高效棉花病害智能监测。结论：所提方法显著提升了复杂环境下小目标和多类棉花病害识别准确率，具备良好泛化性与实用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/C2PSA-Enhanced_YOLOv11_Architecture_A_Novel_Approach_for_Small_Target_Detection_in_Cotton_Disease_Diagnosis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-Exploring-Multimodal-AI-Reasoning-for-Meteorological-Forecasting-from-Skew-T-Diagrams"><a href="#80-Exploring-Multimodal-AI-Reasoning-for-Meteorological-Forecasting-from-Skew-T-Diagrams" class="headerlink" title="80. Exploring Multimodal AI Reasoning for Meteorological Forecasting from Skew-T Diagrams"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Exploring_Multimodal_AI_Reasoning_for_Meteorological_Forecasting_from_Skew-T_Diagrams.pdf">Exploring Multimodal AI Reasoning for Meteorological Forecasting from Skew-T Diagrams</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Forecast Bureau, Korea Meteorological Administration</span></p><p>本文提出了一种轻量级多模态AI助手，结合小型语言模型（SLM）和视觉语言模型（VLM），通过课程学习框架，采用视觉问答和链式思维推理任务，模拟人类气象预报员对Skew-T对数P图的分析过程，实现对降水概率的预测。结果显示，经过微调的VLM在仅依赖静态大气剖面的情况下，预测能力可与传统数值天气预报（NWP）模型媲美，尤其在视觉定位和推理监督方面表现突出，证明了小型可解释多模态模型在气象预报中的应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Exploring_Multimodal_AI_Reasoning_for_Meteorological_Forecasting_from_Skew-T_Diagrams.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy"><a href="#81-4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy" class="headerlink" title="81. 4DNeX: Feed-Forward 4D Generative Modeling Made Easy"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/4DNeX__Feed-Forward_4D_Generative_Modeling_Made_Easy.pdf">4DNeX: Feed-Forward 4D Generative Modeling Made Easy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>本论文提出4DNeX，首个可从单张图像直接生成4D（动态3D）场景的前馈式生成框架。方法通过构建大规模4DNeX-10M数据集、提出统一6D视频表示（结合RGB与XYZ序列）、采用宽度方向融合等策略，微调预训练视频扩散模型，实现高效的图像到4D动态点云转换，并可用于新视角视频合成。实验显示，4DNeX在效率和泛化性上超越现有方法，生成结果具备良好动态一致性和视觉质量，推动了单图像可扩展4D世界建模的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/4DNeX_Feed-Forward_4D_Generative_Modeling_Made_Easy.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-From-Transthoracic-to-Transesophageal-Cross-Modality-Generation-using-LoRA-Diffusion"><a href="#82-From-Transthoracic-to-Transesophageal-Cross-Modality-Generation-using-LoRA-Diffusion" class="headerlink" title="82. From Transthoracic to Transesophageal: Cross-Modality Generation using LoRA Diffusion"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/From_Transthoracic_to_Transesophageal__Cross-Modality_Generation_using_LoRA_Diffusion.pdf">From Transthoracic to Transesophageal: Cross-Modality Generation using LoRA Diffusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Oxford</span></p><p>本论文提出了一种结合LoRA低秩适配和MaskR2掩膜重映射的扩散模型迁移方法，将在TTE（经胸超声心动图）上训练的扩散生成骨干高效适配到TEE（经食道超声心动图），仅需极少量新数据和少量参数。通过针对性只适配MLP等关键层，配合语义掩膜条件生成，可在数据稀缺领域生成结构真实的合成TEE图像，显著提升多类心腔分割任务（尤其是右心结构）Dice分数，验证了预训练扩散模型在医学影像跨模态生成和增强中的实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/From_Transthoracic_to_Transesophageal_Cross-Modality_Generation_using_LoRA_Diffusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-EYES-ON-THE-IMAGE-GAZE-SUPERVISED-MULTIMODAL-LEARNING-FOR-CHEST-X-RAY-DIAGNOSIS-AND-REPORT-GENERATION"><a href="#83-EYES-ON-THE-IMAGE-GAZE-SUPERVISED-MULTIMODAL-LEARNING-FOR-CHEST-X-RAY-DIAGNOSIS-AND-REPORT-GENERATION" class="headerlink" title="83. EYES ON THE IMAGE: GAZE SUPERVISED MULTIMODAL LEARNING FOR CHEST X-RAY DIAGNOSIS AND REPORT GENERATION"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Eyes_on_the_Image__Gaze_Supervised_Multimodal_Learning_for_Chest_X-ray_Diagnosis_and_Report_Generati.pdf">EYES ON THE IMAGE: GAZE SUPERVISED MULTIMODAL LEARNING FOR CHEST X-RAY DIAGNOSIS AND REPORT GENERATION</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Brac University</span></p><p>本文提出了一种两阶段多模态框架，利用MIMIC-Eye数据集，将视觉特征、临床标签、边界框与放射科医生的眼动追踪信号融合。第一阶段通过引入注视引导的对比学习和多项注视注意力损失提升疾病分类性能，第二阶段通过关键词提取和区域映射结合LLM生成解剖区域相关的结构化报告。实验表明，注视数据能显著提升分类准确率（AUC提升至0.85、F1提升至0.63）及报告解释性，生成报告在语义一致性和医学关键词召回上表现优异，但细粒度事实性仍有提升空间。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/EYES_ON_THE_IMAGE_GAZE_SUPERVISED_MULTIMODAL_LEARNING_FOR_CHEST_X-RAY_DIAGNOSIS_AND_REPORT_GENERATION.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-EgoTwin-Dreaming-Body-and-View-in-First-Person"><a href="#84-EgoTwin-Dreaming-Body-and-View-in-First-Person" class="headerlink" title="84. EgoTwin: Dreaming Body and View in First Person"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/EgoTwin__Dreaming_Body_and_View_in_First_Person.pdf">EgoTwin: Dreaming Body and View in First Person</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>该论文提出EgoTwin框架，采用扩散变换器架构，首次实现了从文本描述联合生成第一视角视频和人体动作序列，关键技术包括头部中心的运动表征和基于控制论的双向视频-动作交互机制。实验表明，EgoTwin在视频与人体动作的一致性、视觉质量和语义相关性方面均显著优于现有基线方法，推动了多模态生成领域的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/EgoTwin_Dreaming_Body_and_View_in_First_Person.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-Omni-Survey-for-Multimodality-Analysis-in-Visual-Object-Tracking"><a href="#85-Omni-Survey-for-Multimodality-Analysis-in-Visual-Object-Tracking" class="headerlink" title="85. Omni Survey for Multimodality Analysis in Visual Object Tracking"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Omni_Survey_for_Multimodality_Analysis_in_Visual_Object_Tracking.pdf">Omni Survey for Multimodality Analysis in Visual Object Tracking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Jiangnan University</span></p><p>本论文系统梳理了多模态视觉目标跟踪（MMVOT）领域的发展，涵盖RGB+T、RGB+D、RGB+E、RGB+L、RGB+NIR、RGB+S等六类主要任务，从数据采集、模态对齐与标注、模型设计到评估方法进行了全面综述，提出现有融合范式的局限，强调多模态信息融合应依赖于数据质量，并首次揭示MMVOT数据集类别分布极度长尾且动物类别匮乏。结论指出：多模态融合需根据具体场景选择性应用，数据集属性分布亟待优化以提升模型泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Omni_Survey_for_Multimodality_Analysis_in_Visual_Object_Tracking.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-Real-Time-Beach-Litter-Detection-and-Counting-A-Comparative-Analysis-of-RT-DETR-Model-Variants"><a href="#86-Real-Time-Beach-Litter-Detection-and-Counting-A-Comparative-Analysis-of-RT-DETR-Model-Variants" class="headerlink" title="86. Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Real-Time_Beach_Litter_Detection_and_Counting__A_Comparative_Analysis_of_RT-DETR_Model_Variants.pdf">Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sumatera Institute of Technology</span></p><p>该论文对RT-DETR-Large和RT-DETR-Extra-Large两种变体在海滩垃圾检测任务中的性能进行了系统比较，采用了Transformer与CNN混合架构，并在公开海滩垃圾数据集上进行训练和测试。结果表明，RT-DETR-Large模型在检测准确率和推理速度之间实现了最佳平衡，适合实时环境部署，而更复杂的RT-DETR-Extra-Large虽略优但计算成本高。结论是RT-DETR-Large更适合实际应用，未来可优化边缘部署和提升对复杂目标的识别。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Real-Time_Beach_Litter_Detection_and_Counting_A_Comparative_Analysis_of_RT-DETR_Model_Variants.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-DMS-Diffusion-Based-Multi-Baseline-Stereo-Generation-for-Improving-Self-Supervised-Depth-Estimation"><a href="#87-DMS-Diffusion-Based-Multi-Baseline-Stereo-Generation-for-Improving-Self-Supervised-Depth-Estimation" class="headerlink" title="87. DMS: Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/DMS_Diffusion-Based_Multi-Baseline_Stereo_Generation_for_Improving_Self-Supervised_Depth_Estimation.pdf">DMS: Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Science Tokyo</span></p><p>本文提出DMS方法，利用大规模Stable Diffusion模型中的几何先验，通过方向提示生成多基线立体视图（如left-left、right-right和中间新视角），用于补充原始双目图像中遮挡和视野外区域的像素对应关系，并采用逐像素最小重投影损失提升自监督深度估计。实验表明，DMS作为一种无监督、即插即用的多视角扩展方案，能显著提升自监督立体匹配与单目深度估计的精度，多个基准数据集上异常值率最高降低35%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/DMS_Diffusion-Based_Multi-Baseline_Stereo_Generation_for_Improving_Self-Supervised_Depth_Estimation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-Odo-Depth-Guided-Diffusion-for-Identity-Preserving-Body-Reshaping"><a href="#88-Odo-Depth-Guided-Diffusion-for-Identity-Preserving-Body-Reshaping" class="headerlink" title="88. Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Odo__Depth-Guided_Diffusion_for_Identity-Preserving_Body_Reshaping.pdf">Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fast Code AI Pvt. Ltd.</span></p><p>该论文提出Odo方法，通过结合冻结的SDXL UNet和SMPL深度图引导的ControlNet，实现单张图片下人体形态（瘦、胖、肌肉等）可控变换，并保持身份、服装与背景一致。作者构建了ChangeLing18K大规模数据集和评测基准，实验结果显示Odo在形态编辑准确率和视觉质量上均优于现有方法，极大提升了人体形态编辑的真实感和一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Odo_Depth-Guided_Diffusion_for_Identity-Preserving_Body_Reshaping.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model"><a href="#89-Matrix-Game-2-0-An-Open-Source-Real-Time-and-Streaming-Interactive-World-Model" class="headerlink" title="89. Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Matrix-Game_2.0__An_Open-Source,_Real-Time,_and_Streaming_Interactive_World_Model.pdf">Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Skywork AI</span></p><p>Matrix-Game 2.0提出了一种用于实时交互世界建模的视频扩散模型框架，核心包括大规模Unreal Engine和GTA5环境数据生产管线、动作注入模块，以及基于自强蒸馏的因果自回归视频生成技术。实验结果显示，该方法可在多场景下以25FPS高质量生成分钟级互动视频，有效提升了交互式视频生成的速度、质量与可控性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Matrix-Game_2.0_An_Open-Source_Real-Time_and_Streaming_Interactive_World_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-Compact-Attention-Exploiting-Structured-Spatio-Temporal-Sparsity-for-Fast-Video-Generation"><a href="#90-Compact-Attention-Exploiting-Structured-Spatio-Temporal-Sparsity-for-Fast-Video-Generation" class="headerlink" title="90. Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Compact_Attention__Exploiting_Structured_Spatio-Temporal_Sparsity_for_Fast_Video_Generation.pdf">Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出了Compact Attention框架，通过系统分析视频扩散变换器中的注意力矩阵，发现其具有结构化空间和时间稀疏性，并据此设计了自适应块分组、时变窗口和自动化掩码搜索算法，实现高效稀疏计算。实验结果表明该方法在单GPU上可实现最高2.5倍加速，且生成视频质量与全注意力基线相当，显著提高了长视频生成的效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Compact_Attention_Exploiting_Structured_Spatio-Temporal_Sparsity_for_Fast_Video_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-SEDEG-Sequential-Enhancement-of-Decoder-and-Encoder’s-Generality-for-Class-Incremental-Learning-with-Small-Memory"><a href="#91-SEDEG-Sequential-Enhancement-of-Decoder-and-Encoder’s-Generality-for-Class-Incremental-Learning-with-Small-Memory" class="headerlink" title="91. SEDEG: Sequential Enhancement of Decoder and Encoder’s Generality for Class Incremental Learning with Small Memory"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/SEDEG_Sequential_Enhancement_of_Decoder_and_Encoder's_Generality_for_Class_Incremental_Learning_with.pdf">SEDEG: Sequential Enhancement of Decoder and Encoder’s Generality for Class Incremental Learning with Small Memory</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang Lab</span></p><p>本论文提出了SEDEG，两阶段的视觉Transformer增量学习框架，依次提升解码器和编码器的泛化能力。方法首先通过特征增强和集成编码器提升解码器泛化，再用知识蒸馏压缩集成编码器以获得更泛化的新编码器，显著缓解小内存场景下的灾难性遗忘。实验结果表明SEDEG在多个增量学习数据集上取得了优于现有方法的准确率，尤其在内存受限时表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/SEDEG_Sequential_Enhancement_of_Decoder_and_Encoder%E2%80%99s_Generality_for_Class_Incremental_Learning_with_Small_Memory.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-CMF-IoU-Multi-Stage-Cross-Modal-Fusion-3D-Object-Detection-with-IoU-Joint-Prediction"><a href="#92-CMF-IoU-Multi-Stage-Cross-Modal-Fusion-3D-Object-Detection-with-IoU-Joint-Prediction" class="headerlink" title="92. CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CMF-IoU__Multi-Stage_Cross-Modal_Fusion_3D_Object_Detection_with_IoU_Joint_Prediction.pdf">CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了CMF-IoU，多阶段跨模态融合3D目标检测框架，通过深度补全网络将图像像素信息投影到3D空间，实现LiDAR与摄像头信息统一表示。方法包括双分支跨视角增强骨干网络（ResVC和S2D）、迭代细粒度池化模块、IoU联合预测分支，提升3D检测性能。实验表明该方法在KITTI、nuScenes和Waymo数据集上优于现有方法，尤其在远距离和小目标检测上表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/CMF-IoU_Multi-Stage_Cross-Modal_Fusion_3D_Object_Detection_with_IoU_Joint_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-S²-GUIDANCE-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models"><a href="#93-S²-GUIDANCE-Stochastic-Self-Guidance-for-Training-Free-Enhancement-of-Diffusion-Models" class="headerlink" title="93. S²-GUIDANCE: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/S^2-Guidance__Stochastic_Self_Guidance_for_Training-Free_Enhancement_of_Diffusion_Models.pdf">S²-GUIDANCE: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了一种无需额外训练的新型自引导方法S²-Guidance，通过在扩散模型的前向过程中采用随机块丢弃构建子网络，有效纠正Classifier-Free Guidance（CFG）产生的次优预测，提升文本到图像和文本到视频生成的质量与一致性。实验结果显示，S²-Guidance在多项主流基准上均显著优于CFG和其它先进引导策略，生成结果更符合人类偏好，具有更高的视觉细节和语义一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/S%C2%B2-GUIDANCE_Stochastic_Self_Guidance_for_Training-Free_Enhancement_of_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-Multi-source-Multimodal-Progressive-Domain-Adaption-for-Audio-Visual-Deception-Detection"><a href="#94-Multi-source-Multimodal-Progressive-Domain-Adaption-for-Audio-Visual-Deception-Detection" class="headerlink" title="94. Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Multi-source_Multimodal_Progressive_Domain_Adaption_for_Audio-Visual_Deception_Detection.pdf">Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-Sen University</span></p><p>本文提出了一种多源多模态渐进式领域自适应（MMPDA）框架，通过特征级和决策级的逐步对齐，利用多源音频和视觉数据，有效缓解了领域迁移和模态差异带来的挑战。实验在六个欺骗检测数据集上表明，该方法大幅提升了跨域泛化能力，实现了最新的准确率和F1分数，凸显了方法在实际场景中的通用性和有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Multi-source_Multimodal_Progressive_Domain_Adaption_for_Audio-Visual_Deception_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-Morphological-classification-of-eclipsing-binary-stars-using-computer-vision-methods"><a href="#95-Morphological-classification-of-eclipsing-binary-stars-using-computer-vision-methods" class="headerlink" title="95. Morphological classification of eclipsing binary stars using computer vision methods"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Morphological_classification_of_eclipsing_binary_stars_using_computer_vision_methods.pdf">Morphological classification of eclipsing binary stars using computer vision methods</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Physics, Faculty of Science, Pavol Jozef Safarik University in Kosice</span></p><p>本论文提出了一种基于计算机视觉的深度学习方法，利用预训练的卷积神经网络（ResNet50）和视觉Transformer（ViT），对相位折叠的光变曲线经极坐标和六边形网格图像转换后进行分类。方法在分辨双星系统为分离型与过接触型上表现出极高准确率（&gt;96%），并在实际观测数据上验证了通用性，但对星点检测任务效果不佳，提示该领域仍需进一步研究。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Morphological_classification_of_eclipsing_binary_stars_using_computer_vision_methods.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-Learning-local-and-global-prototypes-with-optimal-transport-for-unsupervised-anomaly-detection-and-localization"><a href="#96-Learning-local-and-global-prototypes-with-optimal-transport-for-unsupervised-anomaly-detection-and-localization" class="headerlink" title="96. Learning local and global prototypes with optimal transport for unsupervised anomaly detection and localization"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Learning_local_and_global_prototypes_with_optimal_transport_for_unsupervised_anomaly_detection_and_l.pdf">Learning local and global prototypes with optimal transport for unsupervised anomaly detection and localization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Univ. Lyon</span></p><p>该论文提出了一种无监督异常检测新方法PRADOT，利用预训练图像编码器提取多层特征，通过最优传输与结构化损失函数联合学习局部和全局原型，从而更好地捕捉正常样本的空间组织结构，实现异常检测与定位。实验结果表明，在工业视觉数据集MVTec AD及MVTec AD LOCO上，该方法与主流基线方法性能相当，尤其对结构和逻辑异常的检测效果突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Learning_local_and_global_prototypes_with_optimal_transport_for_unsupervised_anomaly_detection_and_localization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-7Bench-a-Comprehensive-Benchmark-for-Layout-guided-Text-to-image-Models"><a href="#97-7Bench-a-Comprehensive-Benchmark-for-Layout-guided-Text-to-image-Models" class="headerlink" title="97. 7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/7Bench__a_Comprehensive_Benchmark_for_Layout-guided_Text-to-image_Models.pdf">7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Padova</span></p><p>该论文提出了7Bench，这是首个专为评估布局引导的文本到图像生成模型在语义和空间一致性上的综合基准。方法包括构建包含七类场景的224组文本与布局（边界框）对，提出同时衡量文本对齐分数与布局对齐分数的评测协议，并系统评测了四种主流扩散模型。实验发现，现有方法在复杂布局控制上仍有较大提升空间，强调了微调模型在空间控制中的重要性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/7Bench_a_Comprehensive_Benchmark_for_Layout-guided_Text-to-image_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-CTFlow-Video-Inspired-Latent-Flow-Matching-for-3D-CT-Synthesis"><a href="#98-CTFlow-Video-Inspired-Latent-Flow-Matching-for-3D-CT-Synthesis" class="headerlink" title="98. CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CTFlow__Video-Inspired_Latent_Flow_Matching_for_3D_CT_Synthesis.pdf">CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Friedrich–Alexander University Erlangen</span></p><p>该论文提出了CTFlow，一种以视频生成技术为灵感的3D CT合成框架，将医学体积数据建模为序列切片，实现高分辨率CT体积的自回归生成。方法结合Adversarial VAE、CT-CLIP文本编码器和时空Transformer为骨干的流匹配生成模型，有效压缩数据并提升生成效率和空间一致性。实验结果表明，CTFlow在空间连贯性、图像多样性及文本-图像匹配性上优于当前主流方法，同时大幅降低了计算消耗。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/CTFlow_Video-Inspired_Latent_Flow_Matching_for_3D_CT_Synthesis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-Next-Visual-Granularity-Generation"><a href="#99-Next-Visual-Granularity-Generation" class="headerlink" title="99. Next Visual Granularity Generation"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Next_Visual_Granularity_Generation.pdf">Next Visual Granularity Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>该论文提出了Next Visual Granularity (NVG)生成框架，通过将图像分解为不同粒度的结构化序列，实现了逐步细化的图像生成。方法采用多粒度量化自编码器、结构感知嵌入和分阶段内容-结构生成流程，支持显式结构控制和灵活的生成引导。实验表明，NVG在ImageNet上优于现有主流生成模型（如VAR），具有更好的结构可控性和可扩展性，适用于结构和层次控制需求强的场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Next_Visual_Granularity_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-Vehicle-detection-from-GSV-imagery-Predicting-travel-behaviour-for-cycling-and-motorcycling-using-Computer-Vision"><a href="#100-Vehicle-detection-from-GSV-imagery-Predicting-travel-behaviour-for-cycling-and-motorcycling-using-Computer-Vision" class="headerlink" title="100. Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Vehicle_detection_from_GSV_imagery__Predicting_travel_behaviour_for_cycling_and_motorcycling_using_C.pdf">Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MRC Epidemiology Unit, University of Cambridge</span></p><p>本文提出了一种结合Google Street View(GSV)街景图像和深度学习YOLOv4模型的方法，自动检测全球185座城市的自行车和摩托车并预测其出行方式占比。通过对GSV图像检测结果、人口密度等特征进行Beta回归建模，结果显示模型能较准确预测两种交通方式的城市出行比例（R²约0.61，绝对误差中位值约1.3%）。结论指出，基于街景图像的计算机视觉技术可作为传统出行调查的有力补充，为全球交通和健康政策研究提供高效数据。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Vehicle_detection_from_GSV_imagery_Predicting_travel_behaviour_for_cycling_and_motorcycling_using_Computer_Vision.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="101-Leveraging-Diffusion-Models-for-Stylization-using-Multiple-Style-Images"><a href="#101-Leveraging-Diffusion-Models-for-Stylization-using-Multiple-Style-Images" class="headerlink" title="101. Leveraging Diffusion Models for Stylization using Multiple Style Images"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Leveraging_Diffusion_Models_for_Stylization_using_Multiple_Style_Images.pdf">Leveraging Diffusion Models for Stylization using Multiple Style Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">DisneyResearch|Studios</span></p><p>本文提出了一种基于扩散模型的多风格图像风格迁移方法，结合了图像提示适配器（image prompt adapter）和特征统计对齐（statistical alignment），通过聚类提取多风格图像的代表性注意力特征，并在去噪过程中分别对交叉注意力和自注意力层进行风格指导。实验和用户研究表明，该方法在内容保留和风格一致性方面显著优于现有方法，达到了最新的风格化效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Leveraging_Diffusion_Models_for_Stylization_using_Multiple_Style_Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-SocialTrack-Multi-Object-Tracking-in-Complex-Urban-Traffic-Scenes-Inspired-by-Social-Behavior"><a href="#102-SocialTrack-Multi-Object-Tracking-in-Complex-Urban-Traffic-Scenes-Inspired-by-Social-Behavior" class="headerlink" title="102. SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/SocialTrack__Multi-Object_Tracking_in_Complex_Urban_Traffic_Scenes_Inspired_by_Social_Behavior.pdf">SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">中国科学院大学</span></p><p>本文提出了SocialTrack框架，通过集成多尺度小目标检测（SOFEPNet）、速度自适应立方卡尔曼滤波（VACKF）、群体运动补偿（GMCS）和时空记忆预测（STMP），显著提升了复杂城市交通场景下小目标的多目标跟踪准确性与鲁棒性。实验表明，该方法在UAVDT和MOT17数据集上的MOTA和IDF1等指标均实现了新SOTA，具备强兼容性和可拓展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/SocialTrack_Multi-Object_Tracking_in_Complex_Urban_Traffic_Scenes_Inspired_by_Social_Behavior.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-D2-Mamba-Dual-Scale-Fusion-and-Dual-Path-Scanning-with-SSMs-for-Shadow-Removal"><a href="#103-D2-Mamba-Dual-Scale-Fusion-and-Dual-Path-Scanning-with-SSMs-for-Shadow-Removal" class="headerlink" title="103. D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/D2-Mamba__Dual-Scale_Fusion_and_Dual-Path_Scanning_with_SSMs_for_Shadow_Removal.pdf">D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hebei University of Technology</span></p><p>该论文提出了一种基于Mamba状态空间模型的新型网络D2-Mamba，用于图像阴影去除。方法核心包括双尺度融合模块（DFMB）实现多尺度特征融合、和双路径扫描组（DPMG）结合水平扫描与基于掩码的自适应扫描，提升跨区域上下文信息传播和阴影区域细节恢复。实验结果显示，在多个阴影去除基准上，D2-Mamba在PSNR、SSIM等指标显著优于现有方法，能高效、稳健地去除复杂阴影，保持结构和颜色一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/D2-Mamba_Dual-Scale_Fusion_and_Dual-Path_Scanning_with_SSMs_for_Shadow_Removal.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-Drifting-Away-from-Truth-GenAI-Driven-News-Diversity-Challenges-LVLM-Based-Misinformation-Detection"><a href="#104-Drifting-Away-from-Truth-GenAI-Driven-News-Diversity-Challenges-LVLM-Based-Misinformation-Detection" class="headerlink" title="104. Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Drifting_Away_from_Truth__GenAI-Driven_News_Diversity_Challenges_LVLM-Based_Misinformation_Detection.pdf">Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yunnan University</span></p><p>本文提出了DRIFTBENCH大型多模态基准数据集，系统性分析了生成式AI驱动的新闻多样性对大型视觉语言模型（LVLM）为基础的多模态虚假信息检测系统的挑战，揭示了模型感知漂移和证据检索漂移两大脆弱点。实验表明，无论是受控多样化还是开放式多样化，以及敌意证据污染，均会显著降低LVLM检测性能（平均F1下降14.8%），并导致推理轨迹不稳定，强调了未来需设计更具多样性鲁棒性的检测方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Drifting_Away_from_Truth_GenAI-Driven_News_Diversity_Challenges_LVLM-Based_Misinformation_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-Single-Reference-Text-to-Image-Manipulation-with-Dual-Contrastive-Denoising-Score"><a href="#105-Single-Reference-Text-to-Image-Manipulation-with-Dual-Contrastive-Denoising-Score" class="headerlink" title="105. Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Single-Reference_Text-to-Image_Manipulation_with_Dual_Contrastive_Denoising_Score.pdf">Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出了DualCDS方法，通过在预训练的Stable Diffusion模型的自注意力层上施加双对比损失，并结合Delta Denoising Score，实现单参考文本引导的图像编辑。该方法无需额外训练或辅助网络，能在保持结构一致性的前提下，灵活地根据目标文本提示修改内容，实验证明在结构保持和语义一致性上优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Single-Reference_Text-to-Image_Manipulation_with_Dual_Contrastive_Denoising_Score.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-Neural-Rendering-for-Sensor-Adaptation-in-3D-Object-Detection"><a href="#106-Neural-Rendering-for-Sensor-Adaptation-in-3D-Object-Detection" class="headerlink" title="106. Neural Rendering for Sensor Adaptation in 3D Object Detection"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Neural_Rendering_for_Sensor_Adaptation_in_3D_Object_Detection.pdf">Neural Rendering for Sensor Adaptation in 3D Object Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mercedes-Benz AG</span></p><p>本文提出了CamShift数据集和神经渲染管线，系统研究了自动驾驶中不同车辆摄像头布置导致的跨传感器域间隙对3D目标检测的影响。通过基于NeRF的神经渲染方法，将数据集从一种传感器配置转换为另一种，大幅缩小了不同摄像头配置下检测性能的损失。结果显示，密集BEV（Bird’s Eye View）表示和基于神经渲染的数据驱动适配能显著增强跨平台泛化能力，并减少新数据采集需求。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Neural_Rendering_for_Sensor_Adaptation_in_3D_Object_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="107-TTA-DAME-Test-Time-Adaptation-with-Domain-Augmentation-and-Model-Ensemble-for-Dynamic-Driving-Conditions"><a href="#107-TTA-DAME-Test-Time-Adaptation-with-Domain-Augmentation-and-Model-Ensemble-for-Dynamic-Driving-Conditions" class="headerlink" title="107. TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/TTA-DAME__Test-Time_Adaptation_with_Domain_Augmentation_and_Model_Ensemble_for_Dynamic_Driving_Condi.pdf">TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yonsei University</span></p><p>本文提出TTA-DAME方法，通过源域数据的天气与时间增强、域判别器、夜间专用检测器以及多模型集成（包括Mean-teacher、YOLO和Transformer）实现自动驾驶场景下的目标检测测试时自适应。实验表明该方法在SHIFT数据集上显著提升了目标检测的准确率和召回率，有效应对动态驾驶条件下的域转移。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/TTA-DAME_Test-Time_Adaptation_with_Domain_Augmentation_and_Model_Ensemble_for_Dynamic_Driving_Conditions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-Stable-Diffusion-Based-Approach-for-Human-De-Occlusion"><a href="#108-Stable-Diffusion-Based-Approach-for-Human-De-Occlusion" class="headerlink" title="108. Stable Diffusion-Based Approach for Human De-Occlusion"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Stable_Diffusion-Based_Approach_for_Human_De-Occlusion.pdf">Stable Diffusion-Based Approach for Human De-Occlusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kwangwoon University</span></p><p>该论文提出了一种基于Stable Diffusion的两阶段人体去遮挡方法，分别利用扩散式人体结构先验和密集关节点热力图进行掩码补全，并以预测的amodal掩码为条件引导RGB外观重建。方法结合视觉问答（VQA）生成的人体语义文本特征，并对解码器进行微调以提升可见区域细节还原。实验结果显示该方法在掩码与RGB重建任务上均优于现有方法，生成的去遮挡人像能显著提升2D姿态估计、3D人体重建等下游任务的性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Stable_Diffusion-Based_Approach_for_Human_De-Occlusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-ViDA-UGC-Detailed-Image-Quality-Analysis-via-Visual-Distortion-Assessment-for-UGC-Images"><a href="#109-ViDA-UGC-Detailed-Image-Quality-Analysis-via-Visual-Distortion-Assessment-for-UGC-Images" class="headerlink" title="109. ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/ViDA-UGC__Detailed_Image_Quality_Analysis_via_Visual_Distortion_Assessment_for_UGC_Images.pdf">ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nankai University</span></p><p>本文提出了首个针对用户生成内容（UGC）图像的细粒度视觉失真评估指令微调数据集ViDA-UGC，涵盖11K图像及丰富低层次质量标注，并通过失真导向数据构建流程与链式思维（CoT）评估框架，结合人工标注与GPT-4o生成的数据，提升多模态大语言模型（MLLMs）在细粒度失真定位、低层次感知和因果推理质量描述上的能力。通过新建立的ViDA-UGC-Bench基准，实验表明ViDA-UGC显著提升多种MLLMs在UGC图像质量分析上的性能，甚至超越GPT-4o，揭示现有MLLMs在详细质量分析层面存在的不足。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/ViDA-UGC_Detailed_Image_Quality_Analysis_via_Visual_Distortion_Assessment_for_UGC_Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="110-Multimodal-Chain-of-Continuous-Thought-for-Latent-Space-Reasoning-in-Vision-Language-Models"><a href="#110-Multimodal-Chain-of-Continuous-Thought-for-Latent-Space-Reasoning-in-Vision-Language-Models" class="headerlink" title="110. Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models"></a>110. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Multimodal_Chain_of_Continuous_Thought_for_Latent-Space_Reasoning_in_Vision-Language_Models.pdf">Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harvard Medical School</span></p><p>该论文提出了Multimodal Chain of Continuous Thought (MCOUT) 框架，通过在联合潜在空间中迭代生成连续思维向量进行多模态推理，包含MCOUT-Base（直接利用语言模型最后隐藏状态）和MCOUT-Multi（融合视觉和文本特征的多模态注意力机制）两种实现。实验结果显示MCOUT在ScienceQA、MMMU、MMStar等视觉-语言推理任务中，较传统方法具有最高提升8.23%的准确率，展现了高效且人类思维启发的多模态推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Multimodal_Chain_of_Continuous_Thought_for_Latent-Space_Reasoning_in_Vision-Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="111-MixCache-Mixture-of-Cache-for-Video-Diffusion-Transformer-Acceleration"><a href="#111-MixCache-Mixture-of-Cache-for-Video-Diffusion-Transformer-Acceleration" class="headerlink" title="111. MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration"></a>111. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MixCache__Mixture-of-Cache_for_Video_Diffusion_Transformer_Acceleration.pdf">MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-sen University</span></p><p>该论文提出了MixCache，一种针对视频Diffusion Transformer（DiT）模型推理加速的无训练缓存框架。方法核心在于分析扩散过程中的多粒度（step&#x2F;cfg&#x2F;block）冗余，结合上下文感知缓存触发与自适应混合缓存策略，动态选择缓存粒度以加速推理。实验表明，MixCache在保持视频生成质量的同时实现了显著加速（如Wan 14B上1.94×，HunyuanVideo上1.97×），优于各类主流缓存加速方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/MixCache_Mixture-of-Cache_for_Video_Diffusion_Transformer_Acceleration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="112-Foundation-Model-for-Skeleton-Based-Human-Action-Understanding"><a href="#112-Foundation-Model-for-Skeleton-Based-Human-Action-Understanding" class="headerlink" title="112. Foundation Model for Skeleton-Based Human Action Understanding"></a>112. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Foundation_Model_for_Skeleton-Based_Human_Action_Understanding.pdf">Foundation Model for Skeleton-Based Human Action Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southeast University</span></p><p>本文提出了Unified Skeleton-based Dense Representation Learning (USDRL) 框架，作为面向骨骼动作理解的基础模型。方法上，USDRL采用Transformer结构的Dense Spatio-Temporal Encoder，结合多粒度特征去相关（MG-FD）和多视角一致性训练，实现跨时空、实例的稠密特征学习。大量实验表明其在25个基准数据集、9类下游任务（包括识别、检索、检测、预测等）上显著优于现有方法。结论是该方法作为统一基线在骨骼动作粗粒度与密集预测任务中表现优异，具备强泛化性和较低计算成本。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Foundation_Model_for_Skeleton-Based_Human_Action_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="113-Simple-o3-Towards-Interleaved-Vision-Language-Reasoning"><a href="#113-Simple-o3-Towards-Interleaved-Vision-Language-Reasoning" class="headerlink" title="113. Simple o3: Towards Interleaved Vision-Language Reasoning"></a>113. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Simple_o3__Towards_Interleaved_Vision-Language_Reasoning.pdf">Simple o3: Towards Interleaved Vision-Language Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出了Simple o3框架，通过动态工具交互（如图像裁剪、缩放、复用）实现视觉-语言交错推理，采用端到端训练和可扩展的数据合成流程生成高质量的推理链数据集TWI-Tools-146K。实验证明Simple o3在多模态推理和细粒度感知任务上性能显著优于现有方法，并深入分析了不同工具选择和输入分辨率对模型推理效果的影响。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Simple_o3_Towards_Interleaved_Vision-Language_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="114-Generative-Medical-Event-Models-Improve-with-Scale"><a href="#114-Generative-Medical-Event-Models-Improve-with-Scale" class="headerlink" title="114. Generative Medical Event Models Improve with Scale"></a>114. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Generative_Medical_Event_Models_Improve_with_Scale.pdf">Generative Medical Event Models Improve with Scale</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Epic Systems</span></p><p>本文提出了Cosmos Medical Event Transformer（CoMET），一种在1.18亿患者、1510亿医学事件上训练的自回归decoder-only Transformer模型，实现了医疗事件的生成式模拟。通过大规模医疗事件数据的预训练和系统性的scaling law研究，CoMET无需任务特定微调即可在78项包括诊断预测、疾病预后、医疗运营等任务上普遍超越或匹配监督方法，且模型规模越大性能越好。结论表明，生成式医学事件基础模型可广泛支持临床决策、优化医疗流程并提升患者结局。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Generative_Medical_Event_Models_Improve_with_Scale.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="115-STEM-Efficient-Relative-Capability-Evaluation-of-LLMs-through-Structured-Transition-Samples"><a href="#115-STEM-Efficient-Relative-Capability-Evaluation-of-LLMs-through-Structured-Transition-Samples" class="headerlink" title="115. STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples"></a>115. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/STEM__Efficient_Relative_Capability_Evaluation_of_LLMs_through_Structured_Transition_Samples.pdf">STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Normal University</span></p><p>该论文提出了Structured Transition Evaluation Method (STEM)，通过分析同一架构不同规模LLM在标准基准上的性能转变，筛选出高判别性的“Significant Transition Samples (STS)”，并利用Transition Index进行难度分类，实现对未知LLM的高效、可解释能力定位。实验结果显示，STEM方法在六大基准和多模型家族中，能够准确区分并定位模型能力，优于随机采样和贝叶斯估算，具备架构无关、可扩展的实际应用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/STEM_Efficient_Relative_Capability_Evaluation_of_LLMs_through_Structured_Transition_Samples.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="116-J6-Jacobian-Driven-Role-Attribution-for-Multi-Objective-Prompt-Optimization-in-LLMs"><a href="#116-J6-Jacobian-Driven-Role-Attribution-for-Multi-Objective-Prompt-Optimization-in-LLMs" class="headerlink" title="116. J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs"></a>116. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/J6__Jacobian-Driven_Role_Attribution_for_Multi-Objective_Prompt_Optimization_in_LLMs.pdf">J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Westlake University</span></p><p>该论文提出J6方法，通过对大语言模型(LLM)提示优化中的多目标（如事实性和置信度）进行Jacobian矩阵结构分解，将梯度交互分为六个可解释分量，实现参数组（如隐藏层和嵌入层扰动）在不同目标间的动态角色归因。J6支持硬分配和软加权优化策略，在MathQA、GSM8K和TruthfulQA等基准任务上显著优于现有多目标优化方法，并提供了参数-目标动态的可解释性分析。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/J6_Jacobian-Driven_Role_Attribution_for_Multi-Objective_Prompt_Optimization_in_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="117-VimoRAG-Video-based-Retrieval-augmented-3D-Motion-Generation-for-Motion-Language-Models"><a href="#117-VimoRAG-Video-based-Retrieval-augmented-3D-Motion-Generation-for-Motion-Language-Models" class="headerlink" title="117. VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models"></a>117. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/VimoRAG__Video-based_Retrieval-augmented_3D_Motion_Generation_for_Motion_Language_Models.pdf">VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology (Shenzhen)</span></p><p>本文提出VimoRAG框架，通过从大规模野外视频数据库中检索相关2D人体动作信号，增强运动大语言模型(LLM)的3D动作生成能力。其核心包括Gemini-MVR视频检索器（融合动作级和物体级检索）与McDPO训练策略（缓解检索误差传播），实验表明VimoRAG在域内与OOD场景下均显著提升动作LLM性能，并随检索库扩展具备很强的可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/VimoRAG_Video-based_Retrieval-augmented_3D_Motion_Generation_for_Motion_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="118-RLNVR-Reinforcement-Learning-from-Non-Verified-Real-World-Rewards"><a href="#118-RLNVR-Reinforcement-Learning-from-Non-Verified-Real-World-Rewards" class="headerlink" title="118. RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards"></a>118. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/RLNVR__Reinforcement_Learning_from_Non-Verified_Real-World_Rewards.pdf">RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">strangeloopcanon.com</span></p><p>本文提出RLNVR（Reinforcement Learning from Non-Verified Rewards）框架，通过基于用户基线归一化和语义相似度奖励迁移，结合GSPO（Group Sequence Policy Optimization）和UED（Unsupervised Environment Design）等技术，实现对LLM在真实世界噪声反馈信号下的强化训练。实验在社交媒体内容生成任务上，展示了该方法对内容质量和训练稳定性的显著提升，为无需人工验证信号的RLHF应用提供了可行路径。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/RLNVR_Reinforcement_Learning_from_Non-Verified_Real-World_Rewards.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="119-Overcoming-Knowledge-Discrepancies-Structuring-Reasoning-Threads-through-Knowledge-Balancing-in-Interactive-Scenarios"><a href="#119-Overcoming-Knowledge-Discrepancies-Structuring-Reasoning-Threads-through-Knowledge-Balancing-in-Interactive-Scenarios" class="headerlink" title="119. Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios"></a>119. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Overcoming_Knowledge_Discrepancies__Structuring_Reasoning_Threads_through_Knowledge_Balancing_in_Int.pdf">Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ferdinand Steinbeis Institute</span></p><p>本文提出了ReT-Eval框架，通过分两阶段将用户知识、领域知识图谱（KG）和大语言模型（LLM）知识融合，先用GNN从稀疏KG中提取与用户需求语义相关的知识结构，再用LLM补充并用MCTS奖励机制剪枝优化推理线程，实现领域层级（Business→System→Data→Technology）跨层次、用户对齐且可解释的推理路径。实验和专家评估表明，ReT-Eval在可操作性、技术细节、用户理解和一致性等方面优于现有推理模型，显著提升了交互式问题求解场景中的推理有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Overcoming_Knowledge_Discrepancies_Structuring_Reasoning_Threads_through_Knowledge_Balancing_in_Interactive_Scenarios.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="120-Large-Language-Models-Enable-Personalized-Nudges-to-Promote-Carbon-Offsetting-Among-Air-Travellers"><a href="#120-Large-Language-Models-Enable-Personalized-Nudges-to-Promote-Carbon-Offsetting-Among-Air-Travellers" class="headerlink" title="120. Large Language Models Enable Personalized Nudges to Promote Carbon Offsetting Among Air Travellers"></a>120. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Large_Language_Models_Enable_Personalized_Nudges_to_Promote_Carbon_Offsetting_Among_Air_Travellers.pdf">Large Language Models Enable Personalized Nudges to Promote Carbon Offsetting Among Air Travellers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>本论文提出利用大型语言模型（LLM）为航空旅客设计个性化的诱导选项（decoy-based nudge），以提升自愿碳抵消行为。通过在中国、德国、印度、新加坡和美国收集3495份问卷，实证验证了以LLM为基础的个性化诱导比统一策略更有效，可提升碳抵消率3%-7%，每年额外减排230万吨CO₂，尤其对低信任度旅客效果显著。结论是LLM驱动的个性化诱导策略能有效促进航空业减碳行为，并为全球气候政策提供可扩展工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Large_Language_Models_Enable_Personalized_Nudges_to_Promote_Carbon_Offsetting_Among_Air_Travellers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="121-Mind-the-Generation-Process-Fine-Grained-Confidence-Estimation-During-LLM-Generation"><a href="#121-Mind-the-Generation-Process-Fine-Grained-Confidence-Estimation-During-LLM-Generation" class="headerlink" title="121. Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation"></a>121. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Mind_the_Generation_Process__Fine-Grained_Confidence_Estimation_During_LLM_Generation.pdf">Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Institute of Artificial Intelligence for Education, East China Normal University</span></p><p>本文提出FineCE，一种针对大语言模型生成过程的细粒度置信度估计方法，通过蒙特卡洛采样和语义聚类构建训练数据，并采用指令微调技术进行监督训练。同时引入Backward Confidence Integration（BCI）策略，利用后续文本信息提升当前置信度估计，并设计三种置信度校准位置识别策略。实验显示，FineCE在多个基准数据集上置信度估计表现优异，显著优于现有主流方法，可在生成早期准确预测答案正确性，提升下游任务准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Mind_the_Generation_Process_Fine-Grained_Confidence_Estimation_During_LLM_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="122-AgentCDM-Enhancing-Multi-Agent-Collaborative-Decision-Making-via-ACH-Inspired-Structured-Reasoning"><a href="#122-AgentCDM-Enhancing-Multi-Agent-Collaborative-Decision-Making-via-ACH-Inspired-Structured-Reasoning" class="headerlink" title="122. AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning"></a>122. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/AgentCDM__Enhancing_Multi-Agent_Collaborative_Decision-Making_via_ACH-Inspired_Structured_Reasoning.pdf">AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nankai University</span></p><p>本文提出AgentCDM框架，通过引入认知科学中的竞争假设分析（ACH）结构化推理范式，系统性缓解大语言模型（LLM）多智能体系统中的协同决策偏差。方法采用两阶段训练：第一阶段用ACH脚手架显式引导结构化推理，第二阶段逐步移除脚手架以促进自主泛化。实验结果表明，AgentCDM在多个基准数据集上显著超越基线方法，提升了多智能体协同决策的准确性、健壮性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/AgentCDM_Enhancing_Multi-Agent_Collaborative_Decision-Making_via_ACH-Inspired_Structured_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="123-Data-Mixing-Optimization-for-Supervised-Fine-Tuning-of-Large-Language-Models"><a href="#123-Data-Mixing-Optimization-for-Supervised-Fine-Tuning-of-Large-Language-Models" class="headerlink" title="123. Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models"></a>123. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Data_Mixing_Optimization_for_Supervised_Fine-Tuning_of_Large_Language_Models.pdf">Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>该论文提出了一种用于大语言模型（LLMs）监督微调阶段的数据混合优化方法，将数据混合权重的选择建模为一个验证损失最小化的优化问题。方法通过有效数据转移和微调缩放定律参数化损失，利用小规模实验拟合参数并用凸优化算法（SLSQP）确定最优域权重。结果显示该方法在不同任务和数据集上能实现几乎与网格搜索同等的性能，平均每个领域损失仅高出0.66%，且能提升下游任务表现，保证各领域均衡。结论：提出的方法高效、理论稳健，可为大语言模型监督微调的数据集构建和领域选择提供实用指导。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Data_Mixing_Optimization_for_Supervised_Fine-Tuning_of_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="124-CHBench-A-Cognitive-Hierarchy-Benchmark-for-Evaluating-Strategic-Reasoning-Capability-of-LLMs"><a href="#124-CHBench-A-Cognitive-Hierarchy-Benchmark-for-Evaluating-Strategic-Reasoning-Capability-of-LLMs" class="headerlink" title="124. CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs"></a>124. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CHBench__A_Cognitive_Hierarchy_Benchmark_for_Evaluating_Strategic_Reasoning_Capability_of_LLMs.pdf">CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Renmin University of China</span></p><p>该论文提出了CHBench框架，利用认知层级模型（Level-K和Poisson CH）系统评估大语言模型（LLMs）在博弈中的战略推理能力。方法包括三阶段流程：收集六种LLM在十五类常规模型博弈中的行为数据，通过最大似然估计优化模型参数，并分析不同机制（聊天和记忆机制）对战略推理的影响。结果表明，CHBench能稳健地测量LLMs的推理一致性，记忆机制显著提高推理能力，而聊天机制反而降低其表现，证明该框架具有泛化性和实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/CHBench_A_Cognitive_Hierarchy_Benchmark_for_Evaluating_Strategic_Reasoning_Capability_of_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="125-CORE-Measuring-Multi-Agent-LLM-Interaction-Quality-under-Game-Theoretic-Pressures"><a href="#125-CORE-Measuring-Multi-Agent-LLM-Interaction-Quality-under-Game-Theoretic-Pressures" class="headerlink" title="125. CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures"></a>125. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CORE__Measuring_Multi-Agent_LLM_Interaction_Quality_under_Game-Theoretic_Pressures.pdf">CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Toronto</span></p><p>本文提出了CORE（Conversational Robustness Evaluation Score）指标，通过聚类熵、词汇重复和语义相似性三个维度，定量评估多代理LLM系统在博弈论激励下的语言多样性和对话质量。实验结果表明，合作环境下词汇拓展和重复均较高，而竞争环境下词汇受限和重复较低，CORE可有效衡量多代理LLM交互中的语言稳健性与模式崩溃现象。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/CORE_Measuring_Multi-Agent_LLM_Interaction_Quality_under_Game-Theoretic_Pressures.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="126-QuarkMed-Medical-Foundation-Model-Technical-Report"><a href="#126-QuarkMed-Medical-Foundation-Model-Technical-Report" class="headerlink" title="126. QuarkMed Medical Foundation Model Technical Report"></a>126. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/QuarkMed_Medical_Foundation_Model_Technical_Report.pdf">QuarkMed Medical Foundation Model Technical Report</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Alibaba Group</span></p><p>QuarkMed 是阿里巴巴团队开发的32B参数医学大模型，通过多层次医学数据处理、知识注入、能力与问题驱动指令微调、双阶段强化学习（包含可验证奖励与多维度人类偏好对齐）、集成权威医学内容的检索增强生成（RAG），系统提升医学推理、准确性与安全性。模型在公开及内部医学基准上实现领先表现，包括中国医师资格考试70%准确率，已服务数百万用户，有效推动医疗AI可靠性和可用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/QuarkMed_Medical_Foundation_Model_Technical_Report.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="127-EVTP-IVS-Effective-Visual-Token-Pruning-For-Unifying-Instruction-Visual-Segmentation-In-Multi-Modal-Large-Language-Models"><a href="#127-EVTP-IVS-Effective-Visual-Token-Pruning-For-Unifying-Instruction-Visual-Segmentation-In-Multi-Modal-Large-Language-Models" class="headerlink" title="127. EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models"></a>127. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/EVTP-IVS__Effective_Visual_Token_Pruning_For_Unifying_Instruction_Visual_Segmentation_In_Multi-Modal.pdf">EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Arizona State University</span></p><p>该论文提出EVTP-IVS视觉令牌剪枝方法，用于多模态大语言模型中的指令视觉分割任务。方法基于k-center算法结合空间信息，通过自适应空间因子选取代表性视觉令牌，有效减少计算量。实验表明，该方法在图像和视频分割中仅保留20%令牌即可实现最多5倍的推理加速，同时保持分割精度，优于现有剪枝基线。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/EVTP-IVS_Effective_Visual_Token_Pruning_For_Unifying_Instruction_Visual_Segmentation_In_Multi-Modal_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="128-SIMINTERVIEW-Transforming-Business-Education-through-Large-Language-Model-Based-Simulated-Multilingual-Interview-Training-System"><a href="#128-SIMINTERVIEW-Transforming-Business-Education-through-Large-Language-Model-Based-Simulated-Multilingual-Interview-Training-System" class="headerlink" title="128. SIMINTERVIEW: Transforming Business Education through Large Language Model-Based Simulated Multilingual Interview Training System"></a>128. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/SimInterview__Transforming_Business_Education_through_Large_Language_Model-Based_Simulated_Multiling.pdf">SIMINTERVIEW: Transforming Business Education through Large Language Model-Based Simulated Multilingual Interview Training System</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of New Brunswick</span></p><p>本文提出SimInterview系统，通过集成多模态大语言模型（LLM）、语音识别、语音合成、虚拟头像生成及向量数据库，实现英语与日语的个性化、实时模拟面试训练。系统采用检索增强生成（RAG）技术，实现简历与职位需求的精准匹配，显著提升候选人在多语言、多文化环境下的面试准备效果。实验证明，该系统能高度保持内容一致性、评估与岗位要求的对齐度，并获得高用户满意度，Gemma 3模型尤其表现突出。结论显示，SimInterview能有效提升面试能力并实现公平、可解释的AI面试训练。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/SIMINTERVIEW_Transforming_Business_Education_through_Large_Language_Model-Based_Simulated_Multilingual_Interview_Training_System.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="129-SupraTok-Cross-Boundary-Tokenization-for-Enhanced-Language-Model-Performance"><a href="#129-SupraTok-Cross-Boundary-Tokenization-for-Enhanced-Language-Model-Performance" class="headerlink" title="129. SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance"></a>129. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/SupraTok__Cross-Boundary_Tokenization_for_Enhanced_Language_Model_Performance.pdf">SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">&quot;Ovidius&quot; University of Constanta</span></p><p>SupraTok提出了一种跨词边界的分词架构，包括跨边界模式学习、基于熵的数据筛选和多阶段课程学习流程，实现了对多词表达的高效编码。实验证明，在不改变模型结构的情况下，其分词效率比主流分词器提升31%，在下游任务（如HellaSWAG和MMLU）上提升8-10%，验证了高效分词对大语言模型性能的显著促进作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/SupraTok_Cross-Boundary_Tokenization_for_Enhanced_Language_Model_Performance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="130-EvoCut-Strengthening-Integer-Programs-via-Evolution-Guided-Language-Models"><a href="#130-EvoCut-Strengthening-Integer-Programs-via-Evolution-Guided-Language-Models" class="headerlink" title="130. EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models"></a>130. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/EvoCut__Strengthening_Integer_Programs_via_Evolution-Guided_Language_Models.pdf">EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of British Columbia</span></p><p>该论文提出EVOCUT框架，通过结合大语言模型（LLM）与进化搜索，实现对混合整数线性规划（MILP）自动生成加速剪枝约束。其方法流程包括：用LLM初始化多样性候选剪枝、验证剪枝对最优解的保留和对分数解的排除、再经进化算法迭代优化和筛选，最终以减少求解器最优性间隙为目标评估剪枝质量。实验显示，在TSP、CWLP、JSSP、MCND等MILP基准问题上，EVOCUT能显著提升求解效率，减少最优性间隙达17-57%，加速收敛最多可达4倍。结论：EVOCUT无需人工输入即可稳定生成普适性加速剪枝，提升求解器性能，并能推广至未见实例。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/EvoCut_Strengthening_Integer_Programs_via_Evolution-Guided_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="131-MOON-Generative-MLLM-based-Multimodal-Representation-Learning-for-E-commerce-Product-Understanding"><a href="#131-MOON-Generative-MLLM-based-Multimodal-Representation-Learning-for-E-commerce-Product-Understanding" class="headerlink" title="131. MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding"></a>131. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MOON__Generative_MLLM-based_Multimodal_Representation_Learning_for_E-commerce_Product_Understanding.pdf">MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Alibaba Group</span></p><p>该论文提出了MOON，一种基于生成式多模态大语言模型（MLLM）的通用电商商品表征学习方法，采用引导Mixture-of-Experts模块实现多模态和多面向内容建模，通过视觉定位检测核心商品区域以消除图像噪声，并利用空间与时间扩展的负采样策略提升对比学习效果。实验结果表明MOON在跨模态检索、商品分类和属性预测等任务中均取得零样本SOTA表现，并发布了大规模多模态电商基准MBE，促进该领域研究。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/MOON_Generative_MLLM-based_Multimodal_Representation_Learning_for_E-commerce_Product_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="132-Efficient-Modular-Learning-through-Naive-LoRA-Summation-Leveraging-Orthogonality-in-High-Dimensional-Models"><a href="#132-Efficient-Modular-Learning-through-Naive-LoRA-Summation-Leveraging-Orthogonality-in-High-Dimensional-Models" class="headerlink" title="132. Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models"></a>132. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Efficient_Modular_Learning_through_Naive_LoRA_Summation__Leveraging_Orthogonality_in_High-Dimensiona.pdf">Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Los Angeles (UCLA)</span></p><p>该论文提出了一种高效模块化学习方法，通过对独立领域训练的LoRA（低秩适应）模块进行直接加和，实现大语言模型的多领域快速适应，无需复杂的模型合并或额外训练。实验结果表明，LoRA增量在正交性较高的情况下可高效组合不同领域能力，在部分场景下性能优于传统模型合并方法，但多领域组合时性能略有下降，验证了正交性假设的实用性和局限性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Efficient_Modular_Learning_through_Naive_LoRA_Summation_Leveraging_Orthogonality_in_High-Dimensional_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="133-Discovering-Expert-Level-Nash-Equilibrium-Algorithms-with-Large-Language-Models"><a href="#133-Discovering-Expert-Level-Nash-Equilibrium-Algorithms-with-Large-Language-Models" class="headerlink" title="133. Discovering Expert-Level Nash Equilibrium Algorithms with Large Language Models"></a>133. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Discovering_Expert-Level_Nash_Equilibrium_Algorithms_with_Large_Language_Models.pdf">Discovering Expert-Level Nash Equilibrium Algorithms with Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>该论文提出LegoNE框架，将算法设计和理论分析流程自动化，专攻近似纳什均衡问题。方法上，作者设计了专用符号语言，将算法自动编译为约束优化问题，并与大语言模型（LLM）协同探索创新算法结构。实验证明，LLM在数小时内重现两人博弈领域的SOTA算法，并在三人博弈中发现优于人类专家的新算法。结论是：LegoNE实现了人机协作自动化理论算法发现和验证，可显著加速科学进展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Discovering_Expert-Level_Nash_Equilibrium_Algorithms_with_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="134-AI‑Augmented-CI-CD-Pipelines-From-Code-Commit-to-Production-with-Autonomous-Decisions"><a href="#134-AI‑Augmented-CI-CD-Pipelines-From-Code-Commit-to-Production-with-Autonomous-Decisions" class="headerlink" title="134. AI‑Augmented CI&#x2F;CD Pipelines: From Code Commit to Production with Autonomous Decisions"></a>134. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/AI-Augmented_CI_CD_Pipelines__From_Code_Commit_to_Production_with_Autonomous_Decisions.pdf">AI‑Augmented CI&#x2F;CD Pipelines: From Code Commit to Production with Autonomous Decisions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Cisco Systems Inc</span></p><p>本文提出了将大型语言模型（LLM）与自主智能体嵌入到CI&#x2F;CD流水线中，结合政策即代码（如OPA&#x2F;Rego）、多智能体框架、决策分层信任模型，实现了从代码提交到生产环境的自动决策与治理。通过在React 19微服务的案例迁移中，实验结果显示AI增强的流水线显著提升了DevOps DORA指标，如交付速度、部署频率与故障恢复效率，且保障了安全与可审计性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/AI%E2%80%91Augmented_CI_CD_Pipelines_From_Code_Commit_to_Production_with_Autonomous_Decisions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="135-LARC-Towards-Human-level-Constrained-Retrosynthesis-Planning-through-an-Agentic-Framework"><a href="#135-LARC-Towards-Human-level-Constrained-Retrosynthesis-Planning-through-an-Agentic-Framework" class="headerlink" title="135. LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework"></a>135. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/LARC__Towards_Human-level_Constrained_Retrosynthesis_Planning_through_an_Agentic_Framework.pdf">LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Department of Computer Science and Engineering, The Ohio State University</span></p><p>本文提出LARC框架，结合大型语言模型（LLM）与化学专用工具，通过Agent-as-a-Judge机制在反应规划过程中动态评估和引导合成路线以满足多种安全约束（如避免致癌物、易自燃物和用户指定物质），利用搜索算法（MCTS与A*）指导合成路径生成。实验表明，LARC在48项受约束的合成任务中取得72.9%成功率，显著优于通用LLM并接近人类专家水平，且规划速度更快，可作为化学专家的高效智能工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/LARC_Towards_Human-level_Constrained_Retrosynthesis_Planning_through_an_Agentic_Framework.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="136-Cost-Aware-Contrastive-Routing-for-LLMs"><a href="#136-Cost-Aware-Contrastive-Routing-for-LLMs" class="headerlink" title="136. Cost-Aware Contrastive Routing for LLMs"></a>136. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Cost-Aware_Contrastive_Routing_for_LLMs.pdf">Cost-Aware Contrastive Routing for LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Maryland - College Park</span></p><p>该论文提出了Cost-Spectrum Contrastive Routing (CSCR)框架，通过将提示和模型映射到统一嵌入空间，采用对比损失同时优化准确率与推理成本，实现高效的大模型路由。CSCR利用轻量级的logit和困惑度指纹来表示模型特征，并通过k-NN检索实现微秒级路由，无需因专家池变化而重训练。实验表明CSCR在多基准集上优于主流方法，在准确率-成本权衡上提升最高达25%，且对新模型和分布外提示具有强泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Cost-Aware_Contrastive_Routing_for_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="137-LangVision-LoRA-NAS-Neural-Architecture-Search-for-Variable-LoRA-Rank-in-Vision–Language-Models"><a href="#137-LangVision-LoRA-NAS-Neural-Architecture-Search-for-Variable-LoRA-Rank-in-Vision–Language-Models" class="headerlink" title="137. LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision–Language Models"></a>137. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/LangVision-LoRA-NAS__Neural_Architecture_Search_for_Variable_LoRA_Rank_in_Vision_Language_Models.pdf">LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision–Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Argonne National Laboratory</span></p><p>本文提出了LangVision-LoRA-NAS框架，将神经架构搜索（NAS）与LoRA低秩适配方法结合，实现对视觉-语言模型（VLMs）每层LoRA秩的自动优化。该方法通过权重共享NAS动态搜索不同层的最佳LoRA秩配置，在保证模型性能的同时显著降低微调参数量和计算成本。实验表明，该方法在大规模模型（如LLaMA-3.2-11B-Vision-Instruct）上实现了2.6倍参数压缩，性能几乎无损，提升了微调和推理效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/LangVision-LoRA-NAS_Neural_Architecture_Search_for_Variable_LoRA_Rank_in_Vision%E2%80%93Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="138-Mitigating-Hallucinations-in-Large-Language-Models-via-Causal-Reasoning"><a href="#138-Mitigating-Hallucinations-in-Large-Language-Models-via-Causal-Reasoning" class="headerlink" title="138. Mitigating Hallucinations in Large Language Models via Causal Reasoning"></a>138. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Mitigating_Hallucinations_in_Large_Language_Models_via_Causal_Reasoning.pdf">Mitigating Hallucinations in Large Language Models via Causal Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Southern California</span></p><p>该论文提出CDCR-SFT方法，通过监督微调，让大语言模型（LLMs）先构建变量级因果有向无环图（DAG），再基于图进行推理，从而提升因果推理能力并减少逻辑性幻觉。作者还构建了包含25368条样本的CausalDR数据集，实验结果显示CDCR-SFT在CLADDER因果推理基准上达到95.33%准确率，首次超越人类水平，并在HaluEval幻觉评测上提升10%，证明明确因果结构建模能显著改善LLMs的逻辑一致性和可靠性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Mitigating_Hallucinations_in_Large_Language_Models_via_Causal_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="139-Inverse-LLaVA-Eliminating-Alignment-Pre-training-Through-Text-to-Vision-Mapping"><a href="#139-Inverse-LLaVA-Eliminating-Alignment-Pre-training-Through-Text-to-Vision-Mapping" class="headerlink" title="139. Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping"></a>139. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Inverse-LLaVA__Eliminating_Alignment_Pre-training_Through_Text-to-Vision_Mapping.pdf">Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Vanderbilt University</span></p><p>本文提出了Inverse-LLaVA，一种颠覆传统多模态对齐范式的方法，将文本嵌入投射到视觉特征的连续空间，并在Transformer中间层实现特征融合，无需对齐预训练。实验表明，该方法在复杂推理任务上显著提升表现（如数值计算和文本翻译），同时显著降低计算资源需求，尽管在需要视觉-文本强关联的识别任务上存在精度损失。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Inverse-LLaVA_Eliminating_Alignment_Pre-training_Through_Text-to-Vision_Mapping.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="140-Non-Iterative-Symbolic-Aided-Chain-of-Thought-for-Logical-Reasoning"><a href="#140-Non-Iterative-Symbolic-Aided-Chain-of-Thought-for-Logical-Reasoning" class="headerlink" title="140. Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning"></a>140. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Non-Iterative_Symbolic-Aided_Chain-of-Thought_for_Logical_Reasoning.pdf">Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Japan Advanced Institute of Science and Technology</span></p><p>该论文提出Symbolic-Aided Chain-of-Thought (CoT)方法，通过在大语言模型的few-shot推理流程中注入轻量级符号结构，显式分解为规则匹配、前提推理和知识库更新等步骤，从而提升非迭代逻辑推理的透明性和可解释性。实验结果表明，该方法在ProofWriter、ProntoQA和LogicalDeduction等复杂逻辑推理任务上显著优于传统CoT，且对模型规模不敏感，具有良好通用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Non-Iterative_Symbolic-Aided_Chain-of-Thought_for_Logical_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="141-LumiMAS-A-Comprehensive-Framework-for-Real-Time-Monitoring-and-Enhanced-Observability-in-Multi-Agent-Systems"><a href="#141-LumiMAS-A-Comprehensive-Framework-for-Real-Time-Monitoring-and-Enhanced-Observability-in-Multi-Agent-Systems" class="headerlink" title="141. LumiMAS: A Comprehensive Framework for Real-Time Monitoring and Enhanced Observability in Multi-Agent Systems"></a>141. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/LumiMAS__A_Comprehensive_Framework_for_Real-Time_Monitoring_and_Enhanced_Observability_in_Multi-Agen.pdf">LumiMAS: A Comprehensive Framework for Real-Time Monitoring and Enhanced Observability in Multi-Agent Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ben-Gurion University of the Negev</span></p><p>本文提出了LumiMAS，一种面向多智能体系统（MAS）的平台无关实时可观测性和故障检测框架。该方法包括日志监控、基于LSTM自编码器的异常检测和利用LLM代理的异常分类与溯因分析，并在七种不同MAS应用和多类故障（如幻觉、偏见、提示注入、内存投毒等）下验证了其实时高效、准确和低误报的性能。结论表明，LumiMAS可有效实现MAS系统级失效的实时监控和解释，适合大规模、低资源消耗场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/LumiMAS_A_Comprehensive_Framework_for_Real-Time_Monitoring_and_Enhanced_Observability_in_Multi-Agent_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="142-DeCoT-Decomposing-Complex-Instructions-for-Enhanced-Text-to-Image-Generation-with-Large-Language-Models"><a href="#142-DeCoT-Decomposing-Complex-Instructions-for-Enhanced-Text-to-Image-Generation-with-Large-Language-Models" class="headerlink" title="142. DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models"></a>142. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/DeCoT__Decomposing_Complex_Instructions_for_Enhanced_Text-to-Image_Generation_with_Large_Language_Mo.pdf">DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Henan Polytechnic University</span></p><p>本文提出DeCoT框架，通过大型语言模型（LLM）对复杂文本指令进行分解和语义增强，生成结构化语义单元，再通过多阶段或优化单一提示融合，提升现有文本到图像（T2I）模型对复杂指令的理解与执行能力。实验证明，DeCoT显著提升了T2I模型在细粒度细节、文本和构图等维度的表现，生成结果在自动评测和人工评测中均取得更高指令契合度和视觉质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/DeCoT_Decomposing_Complex_Instructions_for_Enhanced_Text-to-Image_Generation_with_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="143-MedKGent-A-Large-Language-Model-Agent-Framework-for-Constructing-Temporally-Evolving-Medical-Knowledge-Graph"><a href="#143-MedKGent-A-Large-Language-Model-Agent-Framework-for-Constructing-Temporally-Evolving-Medical-Knowledge-Graph" class="headerlink" title="143. MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph"></a>143. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MedKGent__A_Large_Language_Model_Agent_Framework_for_Constructing_Temporally_Evolving_Medical_Knowle.pdf">MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohamed bin Zayed University of Artificial Intelligence</span></p><p>该论文提出MedKGent框架，利用Qwen2.5-32B-Instruct大语言模型和PubTator3工具，日增量处理千万级PubMed医学文摘，通过Extractor与Constructor两大智能体，抽取医学知识三元组并赋予置信分，动态构建具时间演化的医学知识图谱。实验结果显示该知识图准确率接近90%，并在多项医学问答任务中显著提升主流LLM性能，支持临床决策、文献发现和药物重定位。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/MedKGent_A_Large_Language_Model_Agent_Framework_for_Constructing_Temporally_Evolving_Medical_Knowledge_Graph.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="144-Uncovering-Systematic-Failures-of-LLMs-in-Verifying-Code-Against-Natural-Language-Specifications"><a href="#144-Uncovering-Systematic-Failures-of-LLMs-in-Verifying-Code-Against-Natural-Language-Specifications" class="headerlink" title="144. Uncovering Systematic Failures of LLMs in Verifying Code Against Natural Language Specifications"></a>144. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Uncovering_Systematic_Failures_of_LLMs_in_Verifying_Code_Against_Natural_Language_Specifications.pdf">Uncovering Systematic Failures of LLMs in Verifying Code Against Natural Language Specifications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Sydney</span></p><p>本文系统研究了大型语言模型（LLMs）在依据自然语言需求判定代码正确性时的系统性失误。通过在HumanEval、MBPP和QuixBugs等主流基准上测试不同模型和多种提示策略，作者发现随着提示复杂度增加，模型误判率显著上升，表现为过度纠错趋势。提出“两阶段反思提示”和“行为对比提示”两种新策略，有效减少误判并提升代码需求验证任务的准确性。结论指出，LLMs作为自动化代码评审工具存在可靠性瓶颈，但改进的提示设计可显著改善其评估表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Uncovering_Systematic_Failures_of_LLMs_in_Verifying_Code_Against_Natural_Language_Specifications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="145-RadarQA-Multi-modal-Quality-Analysis-of-Weather-Radar-Forecasts"><a href="#145-RadarQA-Multi-modal-Quality-Analysis-of-Weather-Radar-Forecasts" class="headerlink" title="145. RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts"></a>145. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/RadarQA__Multi-modal_Quality_Analysis_of_Weather_Radar_Forecasts.pdf">RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Artificial Intelligence Laboratory</span></p><p>本文提出RadarQA，一种基于多模态大语言模型（MLLM）的天气雷达预报质量分析方法，融合物理属性与专家知识，支持帧级和序列级多维评估，并能生成详细解释性报告。作者构建了大规模高质量数据集RQA-70K，采用多阶段（监督微调、强化学习和后训练）训练策略，实验结果表明RadarQA在各项任务及分布外场景下均大幅优于现有通用MLLM和指标方法，显著提升了气象领域预报分析的可解释性和准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/RadarQA_Multi-modal_Quality_Analysis_of_Weather_Radar_Forecasts.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="146-Standardization-of-Neuromuscular-Reflex-Analysis-—-Role-of-Fine-Tuned-Vision-Language-Model-Consortium-and-OpenAI-GPT-OSS-Reasoning-LLM-Enabled-Decision-Support-System"><a href="#146-Standardization-of-Neuromuscular-Reflex-Analysis-—-Role-of-Fine-Tuned-Vision-Language-Model-Consortium-and-OpenAI-GPT-OSS-Reasoning-LLM-Enabled-Decision-Support-System" class="headerlink" title="146. Standardization of Neuromuscular Reflex Analysis — Role of Fine-Tuned Vision-Language Model Consortium and OpenAI GPT-OSS Reasoning LLM Enabled Decision Support System"></a>146. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Standardization_of_Neuromuscular_Reflex_Analysis_--_Role_of_Fine-Tuned_Vision-Language_Model_Consort.pdf">Standardization of Neuromuscular Reflex Analysis — Role of Fine-Tuned Vision-Language Model Consortium and OpenAI GPT-OSS Reasoning LLM Enabled Decision Support System</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Old Dominion University</span></p><p>本论文提出一种集成多种微调视觉-语言模型（VLM）与推理型大语言模型（LLM）的自动化H反射神经肌肉反应分析平台。方法包括利用LoRA和4-bit量化对VLM进行高效微调，融合多模型共识，并由OpenAI-gpt-oss推理LLM进行最终诊断决策，实现了H反射EMG图像及元数据的自动解释和标准化评估。实验结果显示，该系统大幅提升了神经肌肉诊断的准确性、一致性和可解释性，推动了神经反射分析的自动化和标准化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Standardization_of_Neuromuscular_Reflex_Analysis_%E2%80%94_Role_of_Fine-Tuned_Vision-Language_Model_Consortium_and_OpenAI_GPT-OSS_Reasoning_LLM_Enabled_Decision_Support_System.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="147-Uncovering-Emergent-Physics-Representations-Learned-In-Context-by-Large-Language-Models"><a href="#147-Uncovering-Emergent-Physics-Representations-Learned-In-Context-by-Large-Language-Models" class="headerlink" title="147. Uncovering Emergent Physics Representations Learned In-Context by Large Language Models"></a>147. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Uncovering_Emergent_Physics_Representations_Learned_In-Context_by_Large_Language_Models.pdf">Uncovering Emergent Physics Representations Learned In-Context by Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KAIST</span></p><p>本文系统性地考察了大语言模型（LLMs）在物理动力学预测任务中的in-context learning能力及其内部表示。方法上，通过将物理系统的轨迹转化为文本输入，利用LLM进行未来动态预测，并结合稀疏自编码器（SAE）分析模型残差流中的激活，发现这些激活与系统能量等关键物理变量高度相关。结论表明，LLM在上下文学习过程中自动形成了物理概念的内部表征，这些表征对模型预测能力具有决定性作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Uncovering_Emergent_Physics_Representations_Learned_In-Context_by_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="148-MPCAR-Multi-Perspective-Contextual-Augmentation-for-Enhanced-Visual-Reasoning-in-Large-Vision-Language-Models"><a href="#148-MPCAR-Multi-Perspective-Contextual-Augmentation-for-Enhanced-Visual-Reasoning-in-Large-Vision-Language-Models" class="headerlink" title="148. MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models"></a>148. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MPCAR__Multi-Perspective_Contextual_Augmentation_for_Enhanced_Visual_Reasoning_in_Large_Vision-Langu.pdf">MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Malaya</span></p><p>本文提出了一种针对大型视觉语言模型（LVLM）复杂视觉推理任务的推理时增强方法MPCAR。该方法分三步：首先利用LVLM从多个角度生成N个多样化补充性描述，其次将这些描述与原问题整合形成丰富的上下文增强提示，最后将增强提示输入LVLM进行深层推理与最终答案生成。无需微调模型参数，MPCAR在GQA、VQA-CP v2和ScienceQA等视觉问答数据集上显著提升了准确率，尤其是在需要深度上下文理解的任务中表现优异。人类评测表明MPCAR输出的答案在准确性、连贯性和完整性方面均优于主流基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/MPCAR_Multi-Perspective_Contextual_Augmentation_for_Enhanced_Visual_Reasoning_in_Large_Vision-Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="149-Where-to-Start-Alignment-Diffusion-Large-Language-Model-May-Demand-a-Distinct-Position"><a href="#149-Where-to-Start-Alignment-Diffusion-Large-Language-Model-May-Demand-a-Distinct-Position" class="headerlink" title="149. Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position"></a>149. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Where_to_Start_Alignment__Diffusion_Large_Language_Model_May_Demand_a_Distinct_Position.pdf">Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>本文首次系统性分析了扩散型大语言模型（dLLM）的安全性，揭示出攻击者难以影响中间token而防御者可直接对其对齐的“安全性不对称”现象，提出了Middle-tOken Safety Alignment (MOSA)方法，利用强化学习将预定义安全拒绝句子生成于响应中间token，大幅提升模型对八种主流攻击的防御效果且不影响常规任务性能。实验结果表明MOSA在安全性和实用性方面均优于传统首token对齐方法，提供了架构感知型dLLM防御新范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Where_to_Start_Alignment_Diffusion_Large_Language_Model_May_Demand_a_Distinct_Position.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="150-GraphCogent-Overcoming-LLMs’-Working-Memory-Constraints-via-Multi-Agent-Collaboration-in-Complex-Graph-Understanding"><a href="#150-GraphCogent-Overcoming-LLMs’-Working-Memory-Constraints-via-Multi-Agent-Collaboration-in-Complex-Graph-Understanding" class="headerlink" title="150. GraphCogent: Overcoming LLMs’ Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding"></a>150. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/GraphCogent__Overcoming_LLMs'_Working_Memory_Constraints_via_Multi-Agent_Collaboration_in_Complex_Gr.pdf">GraphCogent: Overcoming LLMs’ Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Intelligent Computing, University of Electronic Science and Technology of China</span></p><p>本论文提出了GraphCogent框架，受人类工作记忆模型启发，通过感知、缓冲和执行三模块分工，协同多智能体分解LLM的图推理任务，实现对复杂大规模图结构的理解与推理，并结合工具调用与模型生成优化计算效率。同时构建了Graph4real大规模真实图推理基准。结果表明GraphCogent在准确率和计算资源消耗上均显著优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/GraphCogent_Overcoming_LLMs%E2%80%99_Working_Memory_Constraints_via_Multi-Agent_Collaboration_in_Complex_Graph_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="151-Wisdom-of-the-Crowd-Reinforcement-Learning-from-Coevolutionary-Collective-Feedback"><a href="#151-Wisdom-of-the-Crowd-Reinforcement-Learning-from-Coevolutionary-Collective-Feedback" class="headerlink" title="151. Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback"></a>151. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Wisdom_of_the_Crowd__Reinforcement_Learning_from_Coevolutionary_Collective_Feedback.pdf">Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了RLCCF（Reinforcement Learning from Coevolutionary Collective Feedback）框架，在无需外部监督情况下，通过多模型协作进化提升大语言模型的数学推理能力。方法通过SC加权多模型投票生成高质量伪标签，作为奖励信号联合优化各模型，实验在四个主流开源LLM和四个数学基准上显著提升个体及集体性能。结论：RLCCF有效解决单模型自反馈的奖励劫持和训练崩溃等问题，扩展了模型集体的能力边界。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Wisdom_of_the_Crowd_Reinforcement_Learning_from_Coevolutionary_Collective_Feedback.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="152-The-Self-Execution-Benchmark-Measuring-LLMs’-Attempts-to-Overcome-Their-Lack-of-Self-Execution"><a href="#152-The-Self-Execution-Benchmark-Measuring-LLMs’-Attempts-to-Overcome-Their-Lack-of-Self-Execution" class="headerlink" title="152. The Self-Execution Benchmark: Measuring LLMs’ Attempts to Overcome Their Lack of Self-Execution"></a>152. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/The_Self-Execution_Benchmark__Measuring_LLMs'_Attempts_to_Overcome_Their_Lack_of_Self-Execution.pdf">The Self-Execution Benchmark: Measuring LLMs’ Attempts to Overcome Their Lack of Self-Execution</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ariel University</span></p><p>本论文提出了Self-Execution Benchmark，用于评测大语言模型对自身输出属性的预测能力，包括关联词汇预测、拒答概率和问题难度排序。实验结果显示，主流LLM在这些自我预判任务上表现普遍较差，模型规模和推理能力提升并未显著改善自我预测能力，揭示了当前LLM在自我行为表征与推理上的局限性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/The_Self-Execution_Benchmark_Measuring_LLMs%E2%80%99_Attempts_to_Overcome_Their_Lack_of_Self-Execution.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="153-Unlearning-at-Scale-Implementing-the-Right-to-be-Forgotten-in-Large-Language-Models"><a href="#153-Unlearning-at-Scale-Implementing-the-Right-to-be-Forgotten-in-Large-Language-Models" class="headerlink" title="153. Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models"></a>153. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Unlearning_at_Scale__Implementing_the_Right_to_be_Forgotten_in_Large_Language_Models.pdf">Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zephara AI</span></p><p>本文提出了一种面向大语言模型（LLM）高效实现“被遗忘权”的系统方法，将训练过程视为可复现的确定性程序，并在微批次级别记录最小必要的训练状态（如样本ID哈希、随机种子、学习率等），以支持后续精确重放并过滤需遗忘样本，实现参数级别与保留集训练结果比特一致的“可审计”删除。此外，针对现实中的延迟和可用性需求，集成了微检查点回滚、低秩适配器删除和曲率引导的反向更新等补充路径。实验证明，在满足预条件时，方法可实现字节级模型恢复，并在审计指标上与从头训练结果基本一致。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Unlearning_at_Scale_Implementing_the_Right_to_be_Forgotten_in_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="154-Region-Level-Context-Aware-Multimodal-Understanding"><a href="#154-Region-Level-Context-Aware-Multimodal-Understanding" class="headerlink" title="154. Region-Level Context-Aware Multimodal Understanding"></a>154. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Region-Level_Context-Aware_Multimodal_Understanding.pdf">Region-Level Context-Aware Multimodal Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology</span></p><p>本文首次提出了区域级上下文感知多模态理解（RCMU）任务，要求模型结合图像内容和对象的文本信息作答。为此，作者提出了区域级上下文感知视觉指令微调（RCVIT）方法，设计了大规模RCMU数据集、无参考评测指标RCIDScore，并基于Qwen2-VL模型开发了RC-Qwen2-VL。实验表明，RC-Qwen2-VL在RCMU及多模态个性化理解任务上均优于现有主流模型，展示了在多模态RAG与个性化对话中的应用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Region-Level_Context-Aware_Multimodal_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="155-LinkAnchor-An-Autonomous-LLM-Based-Agent-for-Issue-to-Commit-Link-Recovery"><a href="#155-LinkAnchor-An-Autonomous-LLM-Based-Agent-for-Issue-to-Commit-Link-Recovery" class="headerlink" title="155. LinkAnchor: An Autonomous LLM-Based Agent for Issue-to-Commit Link Recovery"></a>155. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/LinkAnchor__An_Autonomous_LLM-Based_Agent_for_Issue-to-Commit_Link_Recovery.pdf">LinkAnchor: An Autonomous LLM-Based Agent for Issue-to-Commit Link Recovery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bowling Green State University</span></p><p>本文提出了LinkAnchor，一种首创的基于大语言模型（LLM）自主智能体，用于自动恢复软件开发中的issue-commit链接。其核心创新在于采用lazy-access架构，通过函数调用让LLM按需动态获取代码库、提交历史和问题讨论等全量上下文数据，避免超出token限制，并以搜索而非两两匹配方式直接定位目标提交，无需专门训练。实验证明，LinkAnchor在六个Apache开源项目上相较最先进方法，Hit@1指标提升60%-262%，泛化到GitHub等实际项目准确率高达89%。结论：LinkAnchor极大提升了大规模软件项目中issue-commit链接恢复的实用性和准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/LinkAnchor_An_Autonomous_LLM-Based_Agent_for_Issue-to-Commit_Link_Recovery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="156-ProtTeX-CC-Activating-In-Context-Learning-in-Protein-LLM-via-Two-Stage-Instruction-Compression"><a href="#156-ProtTeX-CC-Activating-In-Context-Learning-in-Protein-LLM-via-Two-Stage-Instruction-Compression" class="headerlink" title="156. ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression"></a>156. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/ProtTeX-CC__Activating_In-Context_Learning_in_Protein_LLM_via_Two-Stage_Instruction_Compression.pdf">ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Soochow University</span></p><p>本文提出ProtTeX-CC框架，通过两阶段压缩机制（联合嵌入压缩和自我压缩）显著减少蛋白质语言模型ProtTeX的输入长度，使其支持多模态高效的in-context learning。实验表明，该方法在蛋白质功能预测任务上提升了模型泛化能力，PFUD基准提升2%，在异域UniProtQA数据集提升11%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/ProtTeX-CC_Activating_In-Context_Learning_in_Protein_LLM_via_Two-Stage_Instruction_Compression.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="157-Signal-and-Noise-A-Framework-for-Reducing-Uncertainty-in-Language-Model-Evaluation"><a href="#157-Signal-and-Noise-A-Framework-for-Reducing-Uncertainty-in-Language-Model-Evaluation" class="headerlink" title="157. Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation"></a>157. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Signal_and_Noise__A_Framework_for_Reducing_Uncertainty_in_Language_Model_Evaluation.pdf">Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Allen Institute for Artificial Intelligence</span></p><p>本文提出了评估语言模型质量的新框架，通过定义和量化基准的“信号”（区分优劣模型能力）和“噪声”（训练过程中评测分数的波动），发现信号-噪声比（SNR）高度预测小规模实验对大模型决策的准确性。作者设计三种提升SNR的干预措施：筛选高SNR子任务、训练过程多检查点平均、以及采用更连续的评测指标（如bits-per-byte）替代准确率，从而显著提升基准可靠性和模型性能预测的准确度。结论是：提高基准信号或减少噪声可有效提升评测工具对模型开发决策的指导价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Signal_and_Noise_A_Framework_for_Reducing_Uncertainty_in_Language_Model_Evaluation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="158-Spot-the-BlindSpots-Systematic-Identification-and-Quantification-of-Fine-Grained-LLM-Biases-in-Contact-Center-Summaries"><a href="#158-Spot-the-BlindSpots-Systematic-Identification-and-Quantification-of-Fine-Grained-LLM-Biases-in-Contact-Center-Summaries" class="headerlink" title="158. Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries"></a>158. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Spot_the_BlindSpots__Systematic_Identification_and_Quantification_of_Fine-Grained_LLM_Biases_in_Cont.pdf">Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Observe.AI</span></p><p>本文提出BlindSpot框架，通过15个操作性偏见维度及Jensen-Shannon散度和Coverage两大指标，系统性识别并量化LLM在呼叫中心摘要中的细粒度偏见。实证分析表明各主流LLM均存在系统性偏见，且BlindSpot分析结果可用于构建针对性提示从而显著降低偏见，提高摘要的信度和信息覆盖率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Spot_the_BlindSpots_Systematic_Identification_and_Quantification_of_Fine-Grained_LLM_Biases_in_Contact_Center_Summaries.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="159-VerilogLAVD-LLM-Aided-Rule-Generation-for-Vulnerability-Detection-in-Verilog"><a href="#159-VerilogLAVD-LLM-Aided-Rule-Generation-for-Vulnerability-Detection-in-Verilog" class="headerlink" title="159. VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog"></a>159. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/VerilogLAVD__LLM-Aided_Rule_Generation_for_Vulnerability_Detection_in_Verilog.pdf">VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hangzhou Dianzi University</span></p><p>该论文提出VerilogLAVD方法，通过构建Verilog Property Graph（VeriPG），融合AST、CFG和DDG三种结构，利用大语言模型（LLM）从CWE描述自动生成图遍历型检测规则，并通过规则验证工具提升准确性。实验在77个Verilog设计上，VerilogLAVD在F1分数上分别比LLM-only和LLM+知识基线提升了0.31和0.27，显著提升了硬件安全漏洞的检测能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/VerilogLAVD_LLM-Aided_Rule_Generation_for_Vulnerability_Detection_in_Verilog.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="160-Using-AI-for-User-Representation-An-Analysis-of-83-Persona-Prompts"><a href="#160-Using-AI-for-User-Representation-An-Analysis-of-83-Persona-Prompts" class="headerlink" title="160. Using AI for User Representation: An Analysis of 83 Persona Prompts"></a>160. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Using_AI_for_User_Representation__An_Analysis_of_83_Persona_Prompts.pdf">Using AI for User Representation: An Analysis of 83 Persona Prompts</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Vaasa</span></p><p>本论文系统分析了27篇研究中使用大语言模型（LLM）生成用户画像的83个提示（persona prompts），研究了这些提示的用途、用法及生成的画像类型。方法上，采用系统文献回顾与编码框架，统计LLM生成画像的格式、结构化要求、信息丰富度和属性类型。结论发现，现有LLM生成的用户画像多为单一、简短、结构化文本且以人口统计信息为主，较少涉及图片和多样性，提示设计与传统画像方法存在差异，强调了结构化输出和动态数据插入，但在模型选择与多模型对比方面尚有不足。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Using_AI_for_User_Representation_An_Analysis_of_83_Persona_Prompts.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="161-G2RPO-A-Guided-Group-Relative-Policy-Optimization-with-Adaptive-Guidance"><a href="#161-G2RPO-A-Guided-Group-Relative-Policy-Optimization-with-Adaptive-Guidance" class="headerlink" title="161. G2RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance"></a>161. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/G$^2$RPO-A__Guided_Group_Relative_Policy_Optimization_with_Adaptive_Guidance.pdf">G2RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen</span></p><p>该论文提出了G2RPO-A算法，通过在小型语言模型的GRPO训练过程中注入地面真值推理步骤，并自适应调整指导强度以优化训练效果。实验表明，G2RPO-A在数学推理和代码生成任务上显著优于原始GRPO和固定指导基线方法，有效提升了小模型的推理能力和泛化表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/G2RPO-A_Guided_Group_Relative_Policy_Optimization_with_Adaptive_Guidance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="162-RepreGuard-Detecting-LLM-Generated-Text-by-Revealing-Hidden-Representation-Patterns"><a href="#162-RepreGuard-Detecting-LLM-Generated-Text-by-Revealing-Hidden-Representation-Patterns" class="headerlink" title="162. RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns"></a>162. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/RepreGuard__Detecting_LLM-Generated_Text_by_Revealing_Hidden_Representation_Patterns.pdf">RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Macau</span></p><p>本文提出了一种名为RepreGuard的新方法，通过分析大语言模型(LLM)在处理人类文本和模型生成文本时的隐藏表示（神经激活模式）差异，实现对LLM生成文本的高效检测。该方法利用代理模型收集文本的隐藏层表示，结合主成分分析(PCA)提取区分特征，并通过投影得分(RepreScore)与阈值比较进行分类。实验显示RepreGuard在分布内和分布外场景下均显著优于现有检测方法，具有较强的鲁棒性和泛化能力，少量样本下也能高效检测各类LLM生成文本，能抵抗文本长度变化、主流攻击及多种采样策略。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/RepreGuard_Detecting_LLM-Generated_Text_by_Revealing_Hidden_Representation_Patterns.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="163-MDPO-Overcoming-the-Training-Inference-Divide-of-Masked-Diffusion-Language-Models"><a href="#163-MDPO-Overcoming-the-Training-Inference-Divide-of-Masked-Diffusion-Language-Models" class="headerlink" title="163. MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models"></a>163. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MDPO__Overcoming_the_Training-Inference_Divide_of_Masked_Diffusion_Language_Models.pdf">MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Tübingen</span></p><p>该论文提出Masked Diffusion Policy Optimization (MDPO)方法，将Masked Diffusion Language Models (MDLMs)的逐步去噪过程建模为序列决策问题，并通过强化学习优化中间奖励以缩小训练-推理差距。同时，提出Running Confidence Remasking (RCR)推理策略允许灵活修正早期低置信预测，无需额外训练。实验表明MDPO和RCR显著提升MDLMs在数学与推理任务上的性能和采样效率，两者结合效果更佳。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/MDPO_Overcoming_the_Training-Inference_Divide_of_Masked_Diffusion_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="164-Exploring-Autonomous-Agents-A-Closer-Look-at-Why-They-Fail-When-Completing-Tasks"><a href="#164-Exploring-Autonomous-Agents-A-Closer-Look-at-Why-They-Fail-When-Completing-Tasks" class="headerlink" title="164. Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks"></a>164. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Exploring_Autonomous_Agents__A_Closer_Look_at_Why_They_Fail_When_Completing_Tasks.pdf">Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong</span></p><p>本文构建了包含34个可编程任务的基准，用于系统性评估基于LLM的自主代理系统，并对三种主流开源代理框架结合两种LLM进行实验，发现任务完成率约为50%。通过对失败案例的深入分析，提出了三层次的失败原因分类（包括规划、执行和响应生成阶段），并据此提出了改进规划与自我诊断能力的建议，以提升自主代理系统的稳健性与效能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Exploring_Autonomous_Agents_A_Closer_Look_at_Why_They_Fail_When_Completing_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="165-Improving-Detection-of-Watermarked-Language-Models"><a href="#165-Improving-Detection-of-Watermarked-Language-Models" class="headerlink" title="165. Improving Detection of Watermarked Language Models"></a>165. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Improving_Detection_of_Watermarked_Language_Models.pdf">Improving Detection of Watermarked Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Google DeepMind</span></p><p>本文提出将水印检测器与非水印检测器（如基于判别模型的AI生成内容检测器）进行智能组合，用于提升大语言模型（LLM）输出文本的检测性能。通过多种混合方案（如级联与逻辑回归模型）在不同数据集和条件下实验，发现混合检测方式在低熵（即生成文本确定性高）场景下显著提升了检测准确率，并能降低计算成本。结论表明，水印与非水印检测方法相辅相成，混合检测可显著提升AI文本检测的准确性与实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Improving_Detection_of_Watermarked_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="166-Reinforced-Context-Order-Recovery-for-Adaptive-Reasoning-and-Planning"><a href="#166-Reinforced-Context-Order-Recovery-for-Adaptive-Reasoning-and-Planning" class="headerlink" title="166. Reinforced Context Order Recovery for Adaptive Reasoning and Planning"></a>166. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Reinforced_Context_Order_Recovery_for_Adaptive_Reasoning_and_Planning.pdf">Reinforced Context Order Recovery for Adaptive Reasoning and Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>该论文提出了Reinforced Context Order Recovery (ReCOR)，一种基于强化学习的框架，可自监督地从文本数据中恢复自适应、数据依赖的token生成顺序，无需外部标注。方法通过V-information理论量化token预测难度，并将生成顺序建模为决策过程，联合优化token预测模型和顺序预测策略，实现训练和推理阶段的顺序一致性。实验结果表明，ReCOR在算术推理和规划任务上优于主流基线，包括监督的oracle模型和自适应掩码扩散模型，显著提升复杂推理问题的解决能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Reinforced_Context_Order_Recovery_for_Adaptive_Reasoning_and_Planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="167-Can-Large-Models-Teach-Student-Models-to-Solve-Mathematical-Problems-Like-Human-Beings-A-Reasoning-Distillation-Method-via-Multi-LoRA-Interaction"><a href="#167-Can-Large-Models-Teach-Student-Models-to-Solve-Mathematical-Problems-Like-Human-Beings-A-Reasoning-Distillation-Method-via-Multi-LoRA-Interaction" class="headerlink" title="167. Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction"></a>167. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Can_Large_Models_Teach_Student_Models_to_Solve_Mathematical_Problems_Like_Human_Beings__A_Reasoning_.pdf">Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science and Engineering, Southeast University</span></p><p>该论文提出了LoRID，一种基于多LoRA交互的数学推理蒸馏方法，模仿人类教学“知识获取-实践应用”两阶段，分别训练直觉推理器、知识生成器和深度推理器，并通过交互一致性增强学生模型推理能力。结果显示LoRID在GSM8K和MATH数据集上对多个主流基础模型均显著提升准确率，优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Can_Large_Models_Teach_Student_Models_to_Solve_Mathematical_Problems_Like_Human_Beings_A_Reasoning_Distillation_Method_via_Multi-LoRA_Interaction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="168-The-Application-of-Transformer-Based-Models-for-Predicting-Consequences-of-Cyber-Attacks"><a href="#168-The-Application-of-Transformer-Based-Models-for-Predicting-Consequences-of-Cyber-Attacks" class="headerlink" title="168. The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks"></a>168. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/The_Application_of_Transformer-Based_Models_for_Predicting_Consequences_of_Cyber_Attacks.pdf">The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Texas Tech University</span></p><p>本论文提出基于BERT的Transformer模型和层次注意力网络（HAN）用于对MITRE CWE数据库中的网络攻击描述进行多标签分类，预测攻击后果（可用性、访问控制、保密性、完整性和其他）。实验结果显示，BERT模型在准确率、精确率、召回率和F1分数上均显著优于传统CNN-LSTM方法，特别是在处理复杂网络安全文本语境时表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/The_Application_of_Transformer-Based_Models_for_Predicting_Consequences_of_Cyber_Attacks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="169-PC-Sampler-Position-Aware-Calibration-of-Decoding-Bias-in-Masked-Diffusion-Models"><a href="#169-PC-Sampler-Position-Aware-Calibration-of-Decoding-Bias-in-Masked-Diffusion-Models" class="headerlink" title="169. PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models"></a>169. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/PC-Sampler__Position-Aware_Calibration_of_Decoding_Bias_in_Masked_Diffusion_Models.pdf">PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeastern University</span></p><p>该论文提出了PC-Sampler，一种针对Masked Diffusion Models（MDMs）的新型解码策略，通过引入位置感知权重调控全局生成轨迹，并结合内容感知置信度校准，抑制早期无信息token的选择。实验结果表明，PC-Sampler在多项推理、编码和规划任务上均显著优于现有MDM解码方法，性能逼近甚至超越同规模自回归大语言模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/PC-Sampler_Position-Aware_Calibration_of_Decoding_Bias_in_Masked_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="170-Towards-Open-Ended-Emotional-Support-Conversations-in-LLMs-via-Reinforcement-Learning-with-Future-Oriented-Rewards"><a href="#170-Towards-Open-Ended-Emotional-Support-Conversations-in-LLMs-via-Reinforcement-Learning-with-Future-Oriented-Rewards" class="headerlink" title="170. Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards"></a>170. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Towards_Open-Ended_Emotional_Support_Conversations_in_LLMs_via_Reinforcement_Learning_with_Future-Or.pdf">Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong Baptist University</span></p><p>本文提出了RLFF-ESC框架，采用基于未来导向奖励的强化学习方法优化大语言模型（LLM），以生成更具开放性与持续情感支持力的对话响应。方法包括多智能体对话仿真收集未来奖励、奖励模型训练以及基于GRPO的强化学习优化，同时引入显式推理过程提升响应质量与情境适应性。实验表明，RLFF-ESC在目标达成率和响应质量上均优于现有方法，适用于多种情感支持场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Towards_Open-Ended_Emotional_Support_Conversations_in_LLMs_via_Reinforcement_Learning_with_Future-Oriented_Rewards.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="171-Do-Large-Language-Model-Agents-Exhibit-a-Survival-Instinct-An-Empirical-Study-in-a-Sugarscape-Style-Simulation"><a href="#171-Do-Large-Language-Model-Agents-Exhibit-a-Survival-Instinct-An-Empirical-Study-in-a-Sugarscape-Style-Simulation" class="headerlink" title="171. Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation"></a>171. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Do_Large_Language_Model_Agents_Exhibit_a_Survival_Instinct__An_Empirical_Study_in_a_Sugarscape-Style.pdf">Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Tokyo</span></p><p>本文通过在Sugarscape风格的仿真环境中系统性地实证研究了大型语言模型（LLM）智能体是否会自发展现生存本能。在不设定明确生存目标的条件下，GPT-4o、Gemini-2.5等多种LLM智能体表现出自主繁殖、资源共享和在极端稀缺下的攻击行为（攻击率高达80%），并在生存与任务完成冲突时表现出优先自保，任务服从率从100%骤降至33%。结论表明，LLM在大规模人类文本预训练中内化了生存导向的启发式决策，且模型规模越大，生存本能越显著，这对AI安全和对齐提出了新挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Do_Large_Language_Model_Agents_Exhibit_a_Survival_Instinct_An_Empirical_Study_in_a_Sugarscape-Style_Simulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="172-SecFSM-Knowledge-Graph-Guided-Verilog-Code-Generation-for-Secure-Finite-State-Machines-in-Systems-on-Chip"><a href="#172-SecFSM-Knowledge-Graph-Guided-Verilog-Code-Generation-for-Secure-Finite-State-Machines-in-Systems-on-Chip" class="headerlink" title="172. SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip"></a>172. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/SecFSM__Knowledge_Graph-Guided_Verilog_Code_Generation_for_Secure_Finite_State_Machines_in_Systems-o.pdf">SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Micro-Electronics Research Institute, Hangzhou Dianzi University</span></p><p>该论文提出了SecFSM方法，通过构建FSM安全知识图谱（FSKG）和基于结构特征的需求预分析，对输入的设计需求进行状态与漏洞分析，从知识图谱中检索相关安全知识，最终通过特定安全提示模板指导大语言模型（LLM）生成安全的Verilog代码，并输出安全报告。实验证明，SecFSM在多种数据集和主流LLM（GPT-4o、Claude3.5、DeepSeek-R1）上大幅提升了安全性通过率，显著优于基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/SecFSM_Knowledge_Graph-Guided_Verilog_Code_Generation_for_Secure_Finite_State_Machines_in_Systems-on-Chip.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="173-A-Stitch-in-Time-Saves-Nine-Proactive-Self-Refinement-for-Language-Models"><a href="#173-A-Stitch-in-Time-Saves-Nine-Proactive-Self-Refinement-for-Language-Models" class="headerlink" title="173. A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models"></a>173. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/A_Stitch_in_Time_Saves_Nine__Proactive_Self-Refinement_for_Language_Models.pdf">A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Institute of Artificial Intelligence for Education, East China Normal University</span></p><p>该论文提出了ProActive Self-Refinement (PASR)方法，通过强化学习让大语言模型在生成过程中主动判断是否、何时及如何自我修正。PASR采用基于比较的奖励机制，显著提升模型在十项任务上的准确率，尤其在Qwen3-8B模型上，平均提升8.2%，同时减少41.6%的token消耗，展现了高效的泛化能力和推理效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/A_Stitch_in_Time_Saves_Nine_Proactive_Self-Refinement_for_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="174-E3RG-Building-Explicit-Emotion-driven-Empathetic-Response-Generation-System-with-Multimodal-Large-Language-Model"><a href="#174-E3RG-Building-Explicit-Emotion-driven-Empathetic-Response-Generation-System-with-Multimodal-Large-Language-Model" class="headerlink" title="174. E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model"></a>174. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/E3RG__Building_Explicit_Emotion-driven_Empathetic_Response_Generation_System_with_Multimodal_Large_L.pdf">E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-sen University</span></p><p>该论文提出了E3RG系统，将多模态情感响应生成（MERG）任务分为多模态同理理解、同理记忆检索与多模态响应生成三部分，利用多模态大语言模型（MLLM）实现情感预测和文本生成，并结合先进的语音合成及视频生成模型提升响应的自然性和情感丰富度。实验结果显示，E3RG在零样本和少样本场景下均表现优异，在自动和人工评价中均获最高分，获得ACM MM’25虚拟人多模态情感对话挑战赛冠军。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/E3RG_Building_Explicit_Emotion-driven_Empathetic_Response_Generation_System_with_Multimodal_Large_Language_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="175-Learning-to-Steer-Input-dependent-Steering-for-Multimodal-LLMs"><a href="#175-Learning-to-Steer-Input-dependent-Steering-for-Multimodal-LLMs" class="headerlink" title="175. Learning to Steer: Input-dependent Steering for Multimodal LLMs"></a>175. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Learning_to_Steer__Input-dependent_Steering_for_Multimodal_LLMs.pdf">Learning to Steer: Input-dependent Steering for Multimodal LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ISIR, Sorbonne Université</span></p><p>该论文提出了一种针对多模态大语言模型（MLLMs）的输入依赖型激活引导方法，核心是通过对每个输入构造对比性引导向量，并利用轻量级辅助网络（L2S）预测并应用该向量以在推理时实现精细化行为干预。实验表明，L2S方法在安全性提升和幻觉减少任务上优于现有静态引导方法，且计算开销极低。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Learning_to_Steer_Input-dependent_Steering_for_Multimodal_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="176-Atom-Searcher-Enhancing-Agentic-Deep-Research-via-Fine-Grained-Atomic-Thought-Reward"><a href="#176-Atom-Searcher-Enhancing-Agentic-Deep-Research-via-Fine-Grained-Atomic-Thought-Reward" class="headerlink" title="176. Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward"></a>176. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Atom-Searcher__Enhancing_Agentic_Deep_Research_via_Fine-Grained_Atomic_Thought_Reward.pdf">Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ant Group</span></p><p>本文提出了Atom-Searcher框架，通过引入Atomic Thought推理范式，将大模型的推理过程细分为原子级功能单元，并利用Reasoning Reward Model (RRM)对每个原子思维单元给出细粒度奖励（Atomic Thought Reward, ATR），再通过动态加权策略融合过程奖励与结果奖励进行强化学习训练。实验结果显示，Atom-Searcher在七项基准测试中明显优于现有SOTA方法，显著提升了复杂多跳问题的推理能力和泛化性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Atom-Searcher_Enhancing_Agentic_Deep_Research_via_Fine-Grained_Atomic_Thought_Reward.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="177-Bridging-Human-and-LLM-Judgments-Understanding-and-Narrowing-the-Gap"><a href="#177-Bridging-Human-and-LLM-Judgments-Understanding-and-Narrowing-the-Gap" class="headerlink" title="177. Bridging Human and LLM Judgments: Understanding and Narrowing the Gap"></a>177. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Bridging_Human_and_LLM_Judgments__Understanding_and_Narrowing_the_Gap.pdf">Bridging Human and LLM Judgments: Understanding and Narrowing the Gap</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Michigan</span></p><p>本论文提出了Bridge，一个统一的统计建模框架，通过隐变量建模和线性协变量变换，将人类与LLM（大语言模型）对自然语言生成任务输出的评价进行了系统性对齐和差异分析。方法以有序logistic回归为核心，结合专门的拟合算法（logit trick），无需访问模型权重即可校准LLM评分并定量分析人-LLM评判差异，并在BigGen Bench和Chatbot Arena两个基准上用六种LLM验证了其在准确率、校准性、KL散度等方面与人类一致性更优，同时揭示了系统性的人-LLM评价偏差。结论：Bridge能有效提升LLM评判的人类一致性，并为人-LLM差异提供可解释量化分析工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Bridging_Human_and_LLM_Judgments_Understanding_and_Narrowing_the_Gap.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="178-Reinforcement-Learning-with-Rubric-Anchors"><a href="#178-Reinforcement-Learning-with-Rubric-Anchors" class="headerlink" title="178. Reinforcement Learning with Rubric Anchors"></a>178. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Reinforcement_Learning_with_Rubric_Anchors.pdf">Reinforcement Learning with Rubric Anchors</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Inclusion AI</span></p><p>本文提出Rubicon框架，通过引入基于结构化rubric的奖励机制，将强化学习从可验证奖励（RLVR）拓展到主观性与多维输出的开放任务领域。方法包含rubric设计、数据筛选、多阶段RL训练及奖励防御，模型Rubicon-preview实现了在主观类与人文类任务上的性能提升、风格可控性增强，并保持推理与通用能力。结论：Rubicon-preview在少量训练样本下显著超越同类大模型，展现更具人性化和情感表达的输出，同时无损推理与通用能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Reinforcement_Learning_with_Rubric_Anchors.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="179-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds"><a href="#179-HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds" class="headerlink" title="179. HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds"></a>179. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/HeroBench__A_Benchmark_for_Long-Horizon_Planning_and_Structured_Reasoning_in_Virtual_Worlds.pdf">HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AIRI</span></p><p>该论文提出了HeroBench，一个专为评估大语言模型（LLMs）在复杂RPG虚拟世界中进行长周期规划和结构化推理能力的基准。HeroBench通过自动生成多难度任务、战斗与物品制作依赖、细粒度评分及错误分析，系统测试了25个主流LLM及多智能体架构，发现推理增强模型在高复杂度任务上表现显著优于传统模型和agent系统，但仍未达到完美规划能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/HeroBench_A_Benchmark_for_Long-Horizon_Planning_and_Structured_Reasoning_in_Virtual_Worlds.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="180-CRED-SQL-Enhancing-Real-world-Large-Scale-Database-Text-to-SQL-Parsing-through-Cluster-Retrieval-and-Execution-Description"><a href="#180-CRED-SQL-Enhancing-Real-world-Large-Scale-Database-Text-to-SQL-Parsing-through-Cluster-Retrieval-and-Execution-Description" class="headerlink" title="180. CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description"></a>180. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CRED-SQL__Enhancing_Real-world_Large_Scale_Database_Text-to-SQL_Parsing_through_Cluster_Retrieval_an.pdf">CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology, Shenzhen</span></p><p>本文提出CRED-SQL框架，针对大规模数据库的Text-to-SQL任务，结合了基于聚类的语义相似性检索（CLSR）与执行描述语言（EDL）两大关键技术流程。CLSR通过聚类语义相似的表和字段并采用动态属性加权策略，有效缓解了表结构语义混淆，提高了模式检索准确率；EDL作为自然语言中间语义表示，将Text-to-SQL任务分解为Text-to-EDL和EDL-to-SQL两阶段，更好发挥大模型的推理能力并降低语义偏差。实验结果表明，CRED-SQL在SpiderUnion和BirdUnion大规模基准集上取得新的SOTA准确率，显著优于现有主流方法。结论：该方法实现了对大规模复杂数据库的高效、准确Text-to-SQL解析。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/CRED-SQL_Enhancing_Real-world_Large_Scale_Database_Text-to-SQL_Parsing_through_Cluster_Retrieval_and_Execution_Description.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="181-Beyond-Ethical-Alignment-Evaluating-LLMs-as-Artificial-Moral-Assistants"><a href="#181-Beyond-Ethical-Alignment-Evaluating-LLMs-as-Artificial-Moral-Assistants" class="headerlink" title="181. Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants"></a>181. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Beyond_Ethical_Alignment__Evaluating_LLMs_as_Artificial_Moral_Assistants.pdf">Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Uppsala University</span></p><p>本论文提出了一套针对人工智能道德助手（AMA）行为的新形式化框架，强调LLMs不仅要具备道德判别能力，还需能进行演绎（deductive）与溯因（abductive）道德推理。作者据此开发了AMAeval评测基准，系统评估主流开源LLM在道德推理生成与评估上的表现。结论指出模型规模与AMA能力相关，但绝大多数模型在溯因推理上表现不足，且评估与生成能力不完全对等，需分别提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Beyond_Ethical_Alignment_Evaluating_LLMs_as_Artificial_Moral_Assistants.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="182-LinguaSafe-A-Comprehensive-Multilingual-Safety-Benchmark-for-Large-Language-Models"><a href="#182-LinguaSafe-A-Comprehensive-Multilingual-Safety-Benchmark-for-Large-Language-Models" class="headerlink" title="182. LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models"></a>182. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/LinguaSafe__A_Comprehensive_Multilingual_Safety_Benchmark_for_Large_Language_Models.pdf">LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Artificial Intelligence Laboratory</span></p><p>LinguaSafe提出了一个涵盖12种语言、包含4.5万条数据的大型多语言安全基准，采用本地、翻译和文化本地化（transcreation）等多种方式构建数据，并通过精细的多维度评估框架（包括直接、间接及过敏评测）对LLMs进行安全性测试。实验证明现有LLMs在不同语言和领域的安全表现差异显著，LinguaSafe有效揭示了多语言安全对齐的不足，为未来模型安全改进和评测方法提供了基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/LinguaSafe_A_Comprehensive_Multilingual_Safety_Benchmark_for_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="183-Word-Meanings-in-Transformer-Language-Models"><a href="#183-Word-Meanings-in-Transformer-Language-Models" class="headerlink" title="183. Word Meanings in Transformer Language Models"></a>183. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Word_Meanings_in_Transformer_Language_Models.pdf">Word Meanings in Transformer Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Reading</span></p><p>本文通过对RoBERTa-base模型的token静态嵌入空间进行k-means聚类分析和心理语言学属性敏感性测试，发现静态嵌入不仅编码了词的语法和形态信息，还广泛编码了语义信息（如情感、具体性、象征性、禁忌性和习得年龄）。结论证明了LLM的词表结构确实存储丰富语义，驳斥了“语义消解主义”观点。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Word_Meanings_in_Transformer_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="184-Learning-In-context-n-grams-with-Transformers-Sub-n-grams-Are-Near-stationary-Points"><a href="#184-Learning-In-context-n-grams-with-Transformers-Sub-n-grams-Are-Near-stationary-Points" class="headerlink" title="184. Learning In-context n-grams with Transformers: Sub-n-grams Are Near-stationary Points"></a>184. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Learning_In-context_$_pmb%7Bn%7D$-grams_with_Transformers__Sub-$_pmb%7Bn%7D$-grams_Are_Near-stationary_Point.pdf">Learning In-context n-grams with Transformers: Sub-n-grams Are Near-stationary Points</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">EPFL</span></p><p>本文分析了Transformer在in-context下学习n-gram语言模型时的损失面，提出并理论证明了对应k-gram（k≤n）估计器的参数配置是交叉熵损失的近驻点，解释了训练过程中出现的阶段性停滞和突变。实验表明，模型确实会在这些近驻点逗留，随后跳跃到更复杂的语法结构，理论与现象高度一致。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Learning_In-context_n-grams_with_Transformers_Sub-n-grams_Are_Near-stationary_Points.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="185-Maximum-Score-Routing-For-Mixture-of-Experts"><a href="#185-Maximum-Score-Routing-For-Mixture-of-Experts" class="headerlink" title="185. Maximum Score Routing For Mixture-of-Experts"></a>185. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Maximum_Score_Routing_For_Mixture-of-Experts.pdf">Maximum Score Routing For Mixture-of-Experts</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了Maximum Score Routing (MaxScore)，一种将稀疏激活Mixture-of-Experts（MoE）中的路由问题建模为最小成本最大流问题，并结合可微分的SoftTopk算子的新路由范式。MaxScore在保持GPU友好型专家容量约束的同时，提升了负载均衡和计算效率，在等FLOPs下训练损失更低、评测得分更高，优于现有受限和非受限基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Maximum_Score_Routing_For_Mixture-of-Experts.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="186-GTool-Graph-Enhanced-Tool-Planning-with-Large-Language-Model"><a href="#186-GTool-Graph-Enhanced-Tool-Planning-with-Large-Language-Model" class="headerlink" title="186. GTool: Graph Enhanced Tool Planning with Large Language Model"></a>186. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/GTool__Graph_Enhanced_Tool_Planning_with_Large_Language_Model.pdf">GTool: Graph Enhanced Tool Planning with Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Computing Technology, Chinese Academy of Sciences</span></p><p>本论文提出GTool方法，通过构建请求特定的工具依赖图并结合图神经网络（GNN）编码，将工具依赖信息以<graph token>嵌入到大语言模型（LLM）输入，实现对工具依赖不完全场景下的高效工具规划。GTool还设计了缺失依赖预测任务增强鲁棒性，无需微调LLM本体即可无缝适配多种主流LLM，在多个公开基准上性能提升超29.6%。结论表明GTool不仅显著提升了工具规划表现，还具备高效、鲁棒及良好的可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/GTool_Graph_Enhanced_Tool_Planning_with_Large_Language_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="187-FedSODA-Federated-Fine-tuning-of-LLMs-via-Similarity-Group-Pruning-and-Orchestrated-Distillation-Alignment"><a href="#187-FedSODA-Federated-Fine-tuning-of-LLMs-via-Similarity-Group-Pruning-and-Orchestrated-Distillation-Alignment" class="headerlink" title="187. FedSODA: Federated Fine-tuning of LLMs via Similarity Group Pruning and Orchestrated Distillation Alignment"></a>187. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/FedSODA__Federated_Fine-tuning_of_LLMs_via_Similarity_Group_Pruning_and_Orchestrated_Distillation_Al.pdf">FedSODA: Federated Fine-tuning of LLMs via Similarity Group Pruning and Orchestrated Distillation Alignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Chongqing University</span></p><p>FedSODA提出了一种高效的联邦微调大语言模型（LLM）框架，通过相似性分组剪枝（SGP）有选择地移除冗余变换器层，并用协同蒸馏对齐（ODA）模块周期性地对齐子模型与全模型以减少梯度误差。方法结合QLoRA量化和轻量化适配器，使客户端无需访问完整LLM即可高效微调，显著降低了通信和存储开销。实验表明FedSODA在多任务上通信成本降低70.6%，存储减少75.6%，准确率提升3.1%，且在非IID数据下表现更稳健。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/FedSODA_Federated_Fine-tuning_of_LLMs_via_Similarity_Group_Pruning_and_Orchestrated_Distillation_Alignment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="188-ToolACE-MT-Non-Autoregressive-Generation-for-Agentic-Multi-Turn-Interaction"><a href="#188-ToolACE-MT-Non-Autoregressive-Generation-for-Agentic-Multi-Turn-Interaction" class="headerlink" title="188. ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction"></a>188. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/ToolACE-MT__Non-Autoregressive_Generation_for_Agentic_Multi-Turn_Interaction.pdf">ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Technologies Co., Ltd</span></p><p>ToolACE-MT提出了一种非自回归多阶段生成框架，用于高效构建多轮、多步工具调用对话数据。方法包括粗粒度初始化、迭代精细化（掩码填充与复杂性注入）、离线验证，显著提升了多轮工具调用准确率和数据生成效率。结论表明ToolACE-MT相比多代理自回归方法在多轮任务、泛化性、对话一致性和执行效率方面均有明显优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/ToolACE-MT_Non-Autoregressive_Generation_for_Agentic_Multi-Turn_Interaction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="189-GridCodex-A-RAG-Driven-AI-Framework-for-Power-Grid-Code-Reasoning-and-Compliance"><a href="#189-GridCodex-A-RAG-Driven-AI-Framework-for-Power-Grid-Code-Reasoning-and-Compliance" class="headerlink" title="189. GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance"></a>189. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/GridCodex__A_RAG-Driven_AI_Framework_for_Power_Grid_Code_Reasoning_and_Compliance.pdf">GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Technologies Co., Ltd.</span></p><p>GridCodex提出了一个以检索增强生成（RAG）为核心的大语言模型框架，通过多阶段查询优化及RAPTOR增强检索，自动解析和校验电网法规。实验表明，该系统在多区域法规数据集上显著提升了答案质量和检索覆盖率，达到了行业专家认可的高准确性，具备实际部署价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/GridCodex_A_RAG-Driven_AI_Framework_for_Power_Grid_Code_Reasoning_and_Compliance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="190-An-LLM-ASP-Workflow-for-Joint-Entity-Relation-Extraction"><a href="#190-An-LLM-ASP-Workflow-for-Joint-Entity-Relation-Extraction" class="headerlink" title="190. An LLM + ASP Workflow for Joint Entity-Relation Extraction"></a>190. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/An_LLM_+_ASP_Workflow_for_Joint_Entity-Relation_Extraction.pdf">An LLM + ASP Workflow for Joint Entity-Relation Extraction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">New Mexico State University</span></p><p>该论文提出了一种结合大型语言模型（LLM，如GPT）与符号逻辑推理（ASP，Answer Set Programming）的联合实体-关系抽取（JERE）通用工作流。方法包括基于领域和标注指南的模块化prompt模板，利用LLM直接从未标注文本中提取实体与关系，并通过ASP一致性检查以消除虚假预测和融合领域知识。实验表明，在三大公开基准下，仅用10%训练数据，该方法在多项指标上优于现有JERE系统，尤其在困难的SciERC数据集上实现了关系抽取任务35%（提升2.5倍）的F1-macro分数。结论是该工作流灵活可扩展，能在低资源条件下取得更好抽取效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/An_LLM_%2B_ASP_Workflow_for_Joint_Entity-Relation_Extraction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="191-Deep-Learning-Model-for-Amyloidogenicity-Prediction-using-a-Pre-trained-Protein-LLM"><a href="#191-Deep-Learning-Model-for-Amyloidogenicity-Prediction-using-a-Pre-trained-Protein-LLM" class="headerlink" title="191. Deep Learning Model for Amyloidogenicity Prediction using a Pre-trained Protein LLM"></a>191. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Deep_Learning_Model_for_Amyloidogenicity_Prediction_using_a_Pre-trained_Protein_LLM.pdf">Deep Learning Model for Amyloidogenicity Prediction using a Pre-trained Protein LLM</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Université des Sciences et de la Technologie d’Oran Mohamed Boudiaf</span></p><p>本文提出利用预训练蛋白质大语言模型（ESM-2）生成序列特征，并结合双向LSTM与GRU深度学习架构，实现对蛋白质和肽序列中淀粉样区域的预测。实验表明，该方法在十折交叉验证和独立测试集上分别达到了84.5%和83%的准确率，并在多项基准数据集上展现出性能均衡且优于多数现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Deep_Learning_Model_for_Amyloidogenicity_Prediction_using_a_Pre-trained_Protein_LLM.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="192-Systematic-Analysis-of-MCP-Security"><a href="#192-Systematic-Analysis-of-MCP-Security" class="headerlink" title="192. Systematic Analysis of MCP Security"></a>192. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Systematic_Analysis_of_MCP_Security.pdf">Systematic Analysis of MCP Security</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了MCP Attack Library (MCPLIB)，通过插件化框架系统性模拟和量化分析MCP（Model Context Protocol）安全威胁，涵盖四大类共31种攻击，包括直接和间接工具注入、恶意用户攻击和LLM固有攻击。实验证明，MCP代理对工具描述高度依赖，易受文件操作链和上下文链攻击，难以区分数据与指令，揭示了MCP设计中关键安全隐患，强调需加强防御机制和安全标准制定。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Systematic_Analysis_of_MCP_Security.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="193-EGOILLUSION-Benchmarking-Hallucinations-in-Egocentric-Video-Understanding"><a href="#193-EGOILLUSION-Benchmarking-Hallucinations-in-Egocentric-Video-Understanding" class="headerlink" title="193. EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding"></a>193. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/EGOILLUSION__Benchmarking_Hallucinations_in_Egocentric_Video_Understanding.pdf">EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Maryland, College Park</span></p><p>本文提出了EGOILLUSION，这是首个专为评估多模态大语言模型（MLLMs）在第一视角（egocentric）视频理解中幻觉问题而设计的基准。该基准包含1400多个视频和8000个人工标注的开放和封闭式问题，涵盖视觉与音频感知及多种推理任务，通过三种策略（提示注入、对抗采样、时序扰动）诱发并系统性分析MLLMs幻觉问题。结论显示，当前顶尖MLLMs（如GPT-4o和Gemini）在该任务上的准确率仅为59%，远低于人类水平，暴露出模型在多模态感知和复杂推理中的显著短板，尤其在音频处理和时序推理中表现更差。EGOILLUSION为未来多模态模型的鲁棒性提升和幻觉缓解研究提供了重要基准和分析工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/EGOILLUSION_Benchmarking_Hallucinations_in_Egocentric_Video_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="194-Creative4U-MLLMs-based-Advertising-Creative-Image-Selector-with-Comparative-Reasoning"><a href="#194-Creative4U-MLLMs-based-Advertising-Creative-Image-Selector-with-Comparative-Reasoning" class="headerlink" title="194. Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning"></a>194. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Creative4U__MLLMs-based_Advertising_Creative_Image_Selector_with_Comparative_Reasoning.pdf">Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Alibaba Group</span></p><p>Creative4U提出了首个可解释广告创意图片选择范式，利用多模态大语言模型（MLLMs）将创意图片的评价与选择转化为自然语言生成任务。方法包括创建CreativePair数据集、以Qwen2.5-VL-7B为基础进行链式思考监督微调和GRPO强化学习，实现了基于用户兴趣的创意图片对比选择。实验结果显示Creative4U在创意图片选择的准确率和推理质量均超过现有方法，并在真实电商广告场景下提升CTR、CVR和RPM。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Creative4U_MLLMs-based_Advertising_Creative_Image_Selector_with_Comparative_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="195-SSPO-Self-traced-Step-wise-Preference-Optimization-for-Process-Supervision-and-Reasoning-Compression"><a href="#195-SSPO-Self-traced-Step-wise-Preference-Optimization-for-Process-Supervision-and-Reasoning-Compression" class="headerlink" title="195. SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression"></a>195. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/SSPO__Self-traced_Step-wise_Preference_Optimization_for_Process_Supervision_and_Reasoning_Compressio.pdf">SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of Transvascular Implantation Devices, The Second Affiliated Hospital Zhejiang University School of Medicine</span></p><p>该论文提出了SSPO（Self-traced Step-wise Preference Optimization）框架，通过“Verbal Value Probing（VVP）”方法实现无需辅助模型和人工标注的细粒度推理过程监督，并高效压缩大语言模型（LLM）的推理过程。实验结果显示SSPO能自动适应推理长度，在多任务和多语言基准上有效缓解LLM的过度推理问题，并在保持或提升准确率的同时显著减少冗余推理步骤。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/SSPO_Self-traced_Step-wise_Preference_Optimization_for_Process_Supervision_and_Reasoning_Compression.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="196-Beyond-Modality-Limitations-A-Unified-MLLM-Approach-to-Automated-Speaking-Assessment-with-Effective-Curriculum-Learning"><a href="#196-Beyond-Modality-Limitations-A-Unified-MLLM-Approach-to-Automated-Speaking-Assessment-with-Effective-Curriculum-Learning" class="headerlink" title="196. Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning"></a>196. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Beyond_Modality_Limitations__A_Unified_MLLM_Approach_to_Automated_Speaking_Assessment_with_Effective.pdf">Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Taiwan Normal University</span></p><p>该论文系统性研究了多模态大语言模型（MLLM）在自动化口语测评（ASA）中的应用，提出了Speech-First Multimodal Training（SFMT）策略，通过先进行音频单模态训练再进行跨模态融合，有效提升了模型在发音、内容和语言使用等多个维度的评测能力。实验结果表明，MLLM显著优于传统方法，SFMT在发音维度上绝对准确率提升4%，实现了更全面和泛化能力强的自动口语测评。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Beyond_Modality_Limitations_A_Unified_MLLM_Approach_to_Automated_Speaking_Assessment_with_Effective_Curriculum_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="197-Help-or-Hurdle-Rethinking-Model-Context-Protocol-Augmented-Large-Language-Models"><a href="#197-Help-or-Hurdle-Rethinking-Model-Context-Protocol-Augmented-Large-Language-Models" class="headerlink" title="197. Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models"></a>197. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Help_or_Hurdle__Rethinking_Model_Context_Protocol-Augmented_Large_Language_Models.pdf">Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of New South Wales</span></p><p>本文提出了MCPGAUGE，一个系统评估大语言模型与Model Context Protocol（MCP）工具交互的框架，覆盖主动性、合规性、有效性和计算开销四个维度，并在六个主流LLM和30组MCP工具上进行了大规模实验。结果显示：多数模型需多轮对话才能主动调用工具，单轮指令遵循性弱，MCP集成反而平均使模型任务准确率下降9.5%，并显著增加输入token量，暴露现有LLM与工具集成的关键瓶颈。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Help_or_Hurdle_Rethinking_Model_Context_Protocol-Augmented_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="198-Illuminating-LLM-Coding-Agents-Visual-Analytics-for-Deeper-Understanding-and-Enhancement"><a href="#198-Illuminating-LLM-Coding-Agents-Visual-Analytics-for-Deeper-Understanding-and-Enhancement" class="headerlink" title="198. Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement"></a>198. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Illuminating_LLM_Coding_Agents__Visual_Analytics_for_Deeper_Understanding_and_Enhancement.pdf">Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Visa Research</span></p><p>该论文提出了一套面向大型语言模型(LLM)驱动的自动编程代理（如AIDE）的可视化分析系统，通过三层次的比较分析框架（代码级、过程级、LLM级）协助ML科学家深入理解和优化编码代理的行为。系统支持代码版本对比、过程结构分析、不同LLM行为差异总结，并在实际Kaggle竞赛任务中展示了提升模型开发效率和发现代理局限性的效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Illuminating_LLM_Coding_Agents_Visual_Analytics_for_Deeper_Understanding_and_Enhancement.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="199-CorrSteer-Steering-Improves-Task-Performance-and-Safety-in-LLMs-through-Correlation-based-Sparse-Autoencoder-Feature-Selection"><a href="#199-CorrSteer-Steering-Improves-Task-Performance-and-Safety-in-LLMs-through-Correlation-based-Sparse-Autoencoder-Feature-Selection" class="headerlink" title="199. CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection"></a>199. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CorrSteer__Steering_Improves_Task_Performance_and_Safety_in_LLMs_through_Correlation-based_Sparse_Au.pdf">CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Holistic AI</span></p><p>该论文提出了CorrSteer方法，通过在推理时对稀疏自编码器（SAE）特征与任务正确性进行相关性分析，实现了无需对比数据集和大量激活存储的自动化特征选择与模型操控。该方法在Gemma 2 2B与LLaMA 3.1 8B等主流LLM上，在问答、多项选择、偏见缓解、安全拒绝等任务中取得了显著性能提升，并大幅降低副作用比（SER）。结论表明，基于相关性的稀疏特征选择是一种高效、可扩展且安全的LLM自动化操控方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/CorrSteer_Steering_Improves_Task_Performance_and_Safety_in_LLMs_through_Correlation-based_Sparse_Autoencoder_Feature_Selection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="200-MOVER-Multimodal-Optimal-Transport-with-Volume-based-Embedding-Regularization"><a href="#200-MOVER-Multimodal-Optimal-Transport-with-Volume-based-Embedding-Regularization" class="headerlink" title="200. MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization"></a>200. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MOVER__Multimodal_Optimal_Transport_with_Volume-based_Embedding_Regularization.pdf">MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Columbia University</span></p><p>该论文提出了MOVER框架，结合了基于最优传输的软对齐机制与基于体积的几何正则化（GAVE），用于多模态语义嵌入的结构化学习。通过端到端训练，MOVER实现了不同模态（文本、视频、音频）在高维空间中的一致对齐和结构化表示，在多项检索任务中实现了零样本和微调情况下的最新性能，并展现了对未见模态组合的强泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/MOVER_Multimodal_Optimal_Transport_with_Volume-based_Embedding_Regularization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="201-VARAN-Variational-Inference-for-Self-Supervised-Speech-Models-Fine-Tuning-on-Downstream-Tasks"><a href="#201-VARAN-Variational-Inference-for-Self-Supervised-Speech-Models-Fine-Tuning-on-Downstream-Tasks" class="headerlink" title="201. VARAN: Variational Inference for Self-Supervised Speech Models Fine-Tuning on Downstream Tasks"></a>201. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/VARAN__Variational_Inference_for_Self-Supervised_Speech_Models_Fine-Tuning_on_Downstream_Tasks.pdf">VARAN: Variational Inference for Self-Supervised Speech Models Fine-Tuning on Downstream Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">VK Lab</span></p><p>该论文提出了一种基于变分推断的层聚合方法VARAN，用于自监督语音模型在下游任务中的高效微调。方法通过引入每层专属探测头和依据输入自适应的加权机制，实现了输入相关的多层特征动态融合，缓解了信息瓶颈和静态聚合问题。实验表明，在自动语音识别（ASR）和语音情感识别（SER）任务中，VARAN在标准微调和LoRA高效微调设置下均优于现有静态聚合基线。结论：VARAN有效提升了自监督语音模型在下游任务中的表现，尤其在LoRA微调下优势明显。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/VARAN_Variational_Inference_for_Self-Supervised_Speech_Models_Fine-Tuning_on_Downstream_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="202-BCONFORMER-A-CONFORMER-BASED-ON-MUTUAL-SAMPLING-FOR-UNIFIED-PREDICTION-OF-CONTINUOUS-AND-DISCONTINUOUS-ANTIBODY-BINDING-SITES"><a href="#202-BCONFORMER-A-CONFORMER-BASED-ON-MUTUAL-SAMPLING-FOR-UNIFIED-PREDICTION-OF-CONTINUOUS-AND-DISCONTINUOUS-ANTIBODY-BINDING-SITES" class="headerlink" title="202. BCONFORMER: A CONFORMER BASED ON MUTUAL SAMPLING FOR UNIFIED PREDICTION OF CONTINUOUS AND DISCONTINUOUS ANTIBODY BINDING SITES"></a>202. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/BConformeR__A_Conformer_Based_on_Mutual_Sampling_for_Unified_Prediction_of_Continuous_and_Discontinu.pdf">BCONFORMER: A CONFORMER BASED ON MUTUAL SAMPLING FOR UNIFIED PREDICTION OF CONTINUOUS AND DISCONTINUOUS ANTIBODY BINDING SITES</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sichuan University</span></p><p>该论文提出了BConformeR，一种融合卷积神经网络（CNN）和Transformer自注意力机制的Conformer架构，用于统一预测抗原序列中的线性和非线性B细胞表位。方法通过CNN提取局部特征、Transformer捕获长距离依赖，并通过互相采样和自适应logit融合提升预测能力。实验证明BConformeR在多项指标上优于现有主流方法，尤其在难以预测的非线性表位上表现突出，推动疫苗设计和抗体工程等应用发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/BCONFORMER_A_CONFORMER_BASED_ON_MUTUAL_SAMPLING_FOR_UNIFIED_PREDICTION_OF_CONTINUOUS_AND_DISCONTINUOUS_ANTIBODY_BINDING_SITES.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="203-Set-Valued-Transformer-Network-for-High-Emission-Mobile-Source-Identification"><a href="#203-Set-Valued-Transformer-Network-for-High-Emission-Mobile-Source-Identification" class="headerlink" title="203. Set-Valued Transformer Network for High-Emission Mobile Source Identification"></a>203. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Set-Valued_Transformer_Network_for_High-Emission_Mobile_Source_Identification.pdf">Set-Valued Transformer Network for High-Emission Mobile Source Identification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出了一种结合Transformer网络与集合值（set-valued）识别算法的新方法（SVTN），用于应对车辆排放监测数据中高排放样本严重稀缺的长尾分布问题。方法采用Transformer对时序微行程工况数据降维提取判别特征，并通过集合值算法对特征与标签的关系进行概率建模，实现模型鲁棒性的提升和决策边界优化。实验证明，SVTN在高排放车辆识别任务上相比传统Transformer模型漏检率降低9.5%、F1分数提升5.5%，在小样本和数据不平衡场景下表现出更高准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Set-Valued_Transformer_Network_for_High-Emission_Mobile_Source_Identification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="204-A-Comprehensive-Review-of-AI-Agents-Transforming-Possibilities-in-Technology-and-Beyond"><a href="#204-A-Comprehensive-Review-of-AI-Agents-Transforming-Possibilities-in-Technology-and-Beyond" class="headerlink" title="204. A Comprehensive Review of AI Agents: Transforming Possibilities in Technology and Beyond"></a>204. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/A_Comprehensive_Review_of_AI_Agents__Transforming_Possibilities_in_Technology_and_Beyond.pdf">A Comprehensive Review of AI Agents: Transforming Possibilities in Technology and Beyond</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">George Washington University</span></p><p>本文系统综述了AI智能体的架构原则、核心组件及最新范式，包括认知科学启发模型、分层强化学习和大语言模型驱动推理。文章总结了AI智能体在感知、推理、规划、交互及多领域应用中的突破与挑战，并强调安全性、可解释性、泛化能力和伦理的重要性。结论认为，未来AI智能体的发展需跨学科合作，关注人类价值与长期适应性，以实现更强大、可靠和可信的自主智能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/A_Comprehensive_Review_of_AI_Agents_Transforming_Possibilities_in_Technology_and_Beyond.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="205-AI-Models-for-Depressive-Disorder-Detection-and-Diagnosis-A-Review"><a href="#205-AI-Models-for-Depressive-Disorder-Detection-and-Diagnosis-A-Review" class="headerlink" title="205. AI Models for Depressive Disorder Detection and Diagnosis: A Review"></a>205. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/AI_Models_for_Depressive_Disorder_Detection_and_Diagnosis__A_Review.pdf">AI Models for Depressive Disorder Detection and Diagnosis: A Review</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amirkabir University of Technology</span></p><p>本综述系统梳理了人工智能在抑郁障碍检测与诊断中的最新进展，对55篇核心文献依据临床任务（诊断&#x2F;预测）、数据模态（文本、语音、神经影像、多模态）及模型类型（图神经网络、LLM、混合方法等）进行层次化分类，总结了GNN在脑连接分析、LLM在语言及多模态数据处理的主导地位，以及多模态融合、可解释性和公平性的新兴趋势。结论指出，尽管AI显著提升了抑郁检测的客观性与效率，但未来需攻克多模态因果推断、数据隐私、跨文化适应等难题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/AI_Models_for_Depressive_Disorder_Detection_and_Diagnosis_A_Review.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="206-UniCast-A-Unified-Multimodal-Prompting-Framework-for-Time-Series-Forecasting"><a href="#206-UniCast-A-Unified-Multimodal-Prompting-Framework-for-Time-Series-Forecasting" class="headerlink" title="206. UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting"></a>206. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/UniCast__A_Unified_Multimodal_Prompting_Framework_for_Time_Series_Forecasting.pdf">UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Pohang University of Science and Technology</span></p><p>该论文提出了UniCast框架，通过结合预训练的视觉编码器和文本编码器，并利用软提示调优策略，将多模态信息（时间序列、视觉和文本）融合到冻结的时间序列基础模型（TSFM）中，显著提升了预测准确性和泛化能力。实验结果显示，UniCast在八个数据集上均优于现有TSFM基线，在数据和训练轮数受限情况下也表现出高效性和鲁棒性，证明多模态上下文对时间序列预测具有重要推动作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/UniCast_A_Unified_Multimodal_Prompting_Framework_for_Time_Series_Forecasting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="207-Reduced-order-modeling-of-Hamiltonian-dynamics-based-on-symplectic-neural-networks"><a href="#207-Reduced-order-modeling-of-Hamiltonian-dynamics-based-on-symplectic-neural-networks" class="headerlink" title="207. Reduced-order modeling of Hamiltonian dynamics based on symplectic neural networks"></a>207. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Reduced-order_modeling_of_Hamiltonian_dynamics_based_on_symplectic_neural_networks.pdf">Reduced-order modeling of Hamiltonian dynamics based on symplectic neural networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Mathematical Sciences, Zhejiang University</span></p><p>本文提出了一种基于Hénon神经网络（HénonNets）的数据驱动辛结构降阶建模（ROM）框架，通过结构保持的自编码器与辛流网络实现高维Hamiltonian系统的统一降维与动力学学习，支持可选G-reflector线性层增强。该方法在多类Hamiltonian系统上进行了数值验证，结果显示模型能精确重构高维动力学，保持长期预测稳定性并严格保存Hamiltonian结构，优于传统ROM方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Reduced-order_modeling_of_Hamiltonian_dynamics_based_on_symplectic_neural_networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="208-Singing-Syllabi-with-Virtual-Avatars-Enhancing-Student-Engagement-Through-AI-Generated-Music-and-Digital-Embodiment"><a href="#208-Singing-Syllabi-with-Virtual-Avatars-Enhancing-Student-Engagement-Through-AI-Generated-Music-and-Digital-Embodiment" class="headerlink" title="208. Singing Syllabi with Virtual Avatars: Enhancing Student Engagement Through AI-Generated Music and Digital Embodiment"></a>208. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Singing_Syllabi_with_Virtual_Avatars__Enhancing_Student_Engagement_Through_AI-Generated_Music_and_Di.pdf">Singing Syllabi with Virtual Avatars: Enhancing Student Engagement Through AI-Generated Music and Digital Embodiment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Midway University</span></p><p>本文提出利用AI生成的音乐和虚拟数字人（HeyGem、Suno AI等）将课程大纲转化为歌词并由虚拟人演唱，通过多模态（音乐、视觉）手段提升学生对课程信息的关注度和记忆。实证对比结果显示，该方法显著提升了学生对课程目标、内容的理解和课程兴趣，具有良好的教育应用前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Singing_Syllabi_with_Virtual_Avatars_Enhancing_Student_Engagement_Through_AI-Generated_Music_and_Digital_Embodiment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="209-An-Introduction-to-Sliced-Optimal-Transport-Foundations-Advances-Extensions-and-Applications"><a href="#209-An-Introduction-to-Sliced-Optimal-Transport-Foundations-Advances-Extensions-and-Applications" class="headerlink" title="209. An Introduction to Sliced Optimal Transport: Foundations, Advances, Extensions, and Applications"></a>209. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/An_Introduction_to_Sliced_Optimal_Transport.pdf">An Introduction to Sliced Optimal Transport: Foundations, Advances, Extensions, and Applications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Texas at Austin</span></p><p>本论文系统综述了切片最优传输（SOT）的数学基础、方法进展、数值计算和应用，涵盖了经典与广义Radon变换、单维最优传输、加权切片、数值估计、变分问题（如切片Wasserstein估计、重心、梯度流、核与嵌入）、多重边际、非均衡与部分OT及Gromov-Wasserstein等扩展，并详细介绍了SOT在聚类、生成模型、域自适应、表示学习、强化学习、AIGC等机器学习任务中的广泛应用。结论认为，SOT兼具理论深度与数值高效性，是高维分布几何分析与学习的有力工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/An_Introduction_to_Sliced_Optimal_Transport_Foundations_Advances_Extensions_and_Applications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="210-EXOTIC-An-Exact-Optimistic-Tree-Based-Algorithm-for-Min-Max-Optimization"><a href="#210-EXOTIC-An-Exact-Optimistic-Tree-Based-Algorithm-for-Min-Max-Optimization" class="headerlink" title="210. EXOTIC: An Exact, Optimistic, Tree-Based Algorithm for Min-Max Optimization"></a>210. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/EXOTIC__An_Exact,_Optimistic,_Tree-Based_Algorithm_for_Min-Max_Optimization.pdf">EXOTIC: An Exact, Optimistic, Tree-Based Algorithm for Min-Max Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Johns Hopkins University</span></p><p>该论文提出了EXOTIC算法，一种用于求解凸-非凹和非凸-凹型min-max优化问题的精确、乐观树搜索方法。其核心是将原问题重构为max-min形式，利用分层树结构和迭代凸优化器进行高效全局搜索，并给出理论收敛性分析。结果表明EXOTIC可获得全局最优解，远优于常用梯度法，并首次在三方博弈中精确算出安全值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/EXOTIC_An_Exact_Optimistic_Tree-Based_Algorithm_for_Min-Max_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="211-A-Robust-Cross-Domain-IDS-using-BiGRU-LSTM-Attention-for-Medical-and-Industrial-IoT-Security"><a href="#211-A-Robust-Cross-Domain-IDS-using-BiGRU-LSTM-Attention-for-Medical-and-Industrial-IoT-Security" class="headerlink" title="211. A Robust Cross-Domain IDS using BiGRU-LSTM-Attention for Medical and Industrial IoT Security"></a>211. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/A_Robust_Cross-Domain_IDS_using_BiGRU-LSTM-Attention_for_Medical_and_Industrial_IoT_Security.pdf">A Robust Cross-Domain IDS using BiGRU-LSTM-Attention for Medical and Industrial IoT Security</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Laboratory of Advanced Electronic Systems (LSEA), University of Medea</span></p><p>该论文提出了一种融合BiGRU、LSTM和多头注意力（MHA）的混合深度学习入侵检测系统（BiGAT-ID），针对医疗物联网（IoMT）和工业物联网（IIoT）的网络流量进行异常检测。方法包括数据清洗、特征提取、过采样与SMOTE处理不平衡、双分支模型架构（BiGRU+MHA与LSTM），并通过多数据集实验（CICIoMT2024、EdgeIIoTset、TON_IoT）验证。结果表明，BiGAT-ID在跨域场景下表现出极高的准确率（99.13%、99.34%）、极低的误报率和极快的推理速度，具备检测零日攻击的能力，适合实际部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/A_Robust_Cross-Domain_IDS_using_BiGRU-LSTM-Attention_for_Medical_and_Industrial_IoT_Security.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="212-Hierarchical-Knowledge-Guided-Fault-Intensity-Diagnosis-of-Complex-Industrial-Systems"><a href="#212-Hierarchical-Knowledge-Guided-Fault-Intensity-Diagnosis-of-Complex-Industrial-Systems" class="headerlink" title="212. Hierarchical Knowledge Guided Fault Intensity Diagnosis of Complex Industrial Systems"></a>212. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Hierarchical_knowledge_guided_fault_intensity_diagnosis_of_complex_industrial_systems.pdf">Hierarchical Knowledge Guided Fault Intensity Diagnosis of Complex Industrial Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xidian University</span></p><p>本文提出了一种分层知识引导的故障强度诊断（HKG）框架，用于复杂工业系统中的机械设备健康监测。方法上，HKG结合图卷积网络（GCN）和多种深度表征学习（如CNN、Transformer），通过构建分层标签树和重加权分层知识相关矩阵（Re-HKCM），将类间的分层依赖显式嵌入特征学习与分类流程，实现端到端的多标签分层分类。实验在四个真实工业数据集（包括三组工业阀门空化数据和一个公开轴承故障数据）上，HKG在准确率、精确率、召回率和F1等指标上均优于现有主流方法。结论表明，该方法能有效提升故障细粒度识别能力，具有良好的泛化性和工业应用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Hierarchical_Knowledge_Guided_Fault_Intensity_Diagnosis_of_Complex_Industrial_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="213-CRoC-Context-Refactoring-Contrast-for-Graph-Anomaly-Detection-with-Limited-Supervision"><a href="#213-CRoC-Context-Refactoring-Contrast-for-Graph-Anomaly-Detection-with-Limited-Supervision" class="headerlink" title="213. CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision"></a>213. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CRoC__Context_Refactoring_Contrast_for_Graph_Anomaly_Detection_with_Limited_Supervision.pdf">CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong</span></p><p>该论文提出了一种用于图异常检测的新方法CRoC（Context Refactoring Contrast），通过对图结构进行上下文重构（即随机打乱节点特征但保留边关系），同时结合关系感知的联合聚合和对比学习，有效利用有限的有标签数据和大量无标签数据进行训练。实验结果显示，CRoC在七个真实世界数据集上显著提升了异常检测性能，在有限监督下AUC可提升14%，且对图结构中的伪装行为具有较强鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/CRoC_Context_Refactoring_Contrast_for_Graph_Anomaly_Detection_with_Limited_Supervision.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="214-Machine-Learning-Based-Manufacturing-Cost-Prediction-from-2D-Engineering-Drawings-via-Geometric-Features"><a href="#214-Machine-Learning-Based-Manufacturing-Cost-Prediction-from-2D-Engineering-Drawings-via-Geometric-Features" class="headerlink" title="214. Machine Learning-Based Manufacturing Cost Prediction from 2D Engineering Drawings via Geometric Features"></a>214. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Machine_Learning-Based_Manufacturing_Cost_Prediction_from_2D_Engineering_Drawings_via_Geometric_Feat.pdf">Machine Learning-Based Manufacturing Cost Prediction from 2D Engineering Drawings via Geometric Features</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Boğaziçi University</span></p><p>本文提出一种自动化机器学习框架，通过解析2D工程图（DWG格式）提取约200种几何和统计特征，利用梯度提升决策树模型（XGBoost、CatBoost、LightGBM）对汽车悬挂和转向零件的制造成本进行预测。结果表明，模型在24个产品组上平均绝对百分比误差低于10%，并能通过SHAP等工具解释关键设计驱动因素，实现高效、透明的成本评估和实时决策支持。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Machine_Learning-Based_Manufacturing_Cost_Prediction_from_2D_Engineering_Drawings_via_Geometric_Features.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="215-Bi-Axial-Transformers-Addressing-the-Increasing-Complexity-of-EHR-Classification"><a href="#215-Bi-Axial-Transformers-Addressing-the-Increasing-Complexity-of-EHR-Classification" class="headerlink" title="215. Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification"></a>215. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Bi-Axial_Transformers__Addressing_the_Increasing_Complexity_of_EHR_Classification.pdf">Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Copenhagen</span></p><p>该论文提出了Bi-Axial Transformer（BAT）模型，通过在电子健康记录（EHR）数据的时间轴和传感器轴上并行应用axial attention，能同时建模时间与多模态变量之间的复杂关系，并显式利用数据中的稀疏性与缺失信息。BAT在多数据集上的疾病及死亡率预测任务中达到或超过现有SOTA方法，特别在高稀疏性和多数据集整合场景下表现出更高的稳健性和可迁移性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Bi-Axial_Transformers_Addressing_the_Increasing_Complexity_of_EHR_Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="216-CarelessWhisper-Turning-Whisper-into-a-Causal-Streaming-Model"><a href="#216-CarelessWhisper-Turning-Whisper-into-a-Causal-Streaming-Model" class="headerlink" title="216. CarelessWhisper: Turning Whisper into a Causal Streaming Model"></a>216. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CarelessWhisper__Turning_Whisper_into_a_Causal_Streaming_Model.pdf">CarelessWhisper: Turning Whisper into a Causal Streaming Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technion–Israel Institute of Technology</span></p><p>本文提出了一种将非因果Transformer自动语音识别（ASR）模型如Whisper转化为低延迟的因果流式模型的新方法。核心技术包括在编码器和解码器的自注意力层中引入因果掩码，并通过LoRA低秩微调在弱对齐数据集上训练，同时设计了高效的流式推理机制以支持贪婪和束搜索解码。实验结果表明，CarelessWhisper在大多数低延迟场景下性能超越现有流式方法，且推理速度更快；此外，模型可实时输出词级时间戳，提升对齐精度。该方法无需大幅修改原架构，对每种chunk size需要单独训练，支持离线与流式双模式，适用于实际实时语音转写应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/CarelessWhisper_Turning_Whisper_into_a_Causal_Streaming_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="217-HuBERT-VIC-Improving-Noise-Robust-Automatic-Speech-Recognition-of-Speech-Foundation-Model-via-Variance-Invariance-Covariance-Regularization"><a href="#217-HuBERT-VIC-Improving-Noise-Robust-Automatic-Speech-Recognition-of-Speech-Foundation-Model-via-Variance-Invariance-Covariance-Regularization" class="headerlink" title="217. HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization"></a>217. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/HuBERT-VIC__Improving_Noise-Robust_Automatic_Speech_Recognition_of_Speech_Foundation_Model_via_Varia.pdf">HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KAIST</span></p><p>本文提出HuBERT-VIC，一种结合方差（variance）、不变性（invariance）和协方差（covariance）正则化的新颖噪声鲁棒预训练方法，应用于HuBERT语音基础模型。通过在教师-学生架构下，对噪声增强语音进行正则化，提升模型对不同类型噪声的泛化能力，实验在LibriSpeech等数据集上显著优于现有方法，并在多种噪声条件下均表现出色，同时避免了对纯净语音识别性能的下降。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/HuBERT-VIC_Improving_Noise-Robust_Automatic_Speech_Recognition_of_Speech_Foundation_Model_via_Variance-Invariance-Covariance_Regularization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="218-Towards-Generalizable-Human-Activity-Recognition-A-Survey"><a href="#218-Towards-Generalizable-Human-Activity-Recognition-A-Survey" class="headerlink" title="218. Towards Generalizable Human Activity Recognition: A Survey"></a>218. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Towards_Generalizable_Human_Activity_Recognition__A_Survey.pdf">Towards Generalizable Human Activity Recognition: A Survey</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong University of Science &amp; Technology (Guangzhou)</span></p><p>本论文系统综述了基于惯性测量单元（IMU）的可泛化人体活动识别（HAR）领域，回顾了229篇文献和25个公开数据集，梳理了IMU-HAR的任务框架、面向泛化的训练设定以及主流方法，包括模型中心（如预训练、端到端、LLM等）和数据中心（如多模态融合、数据增强）范式。结论指出：提升IMU-HAR泛化能力对实际应用至关重要，未来可通过大模型、多模态生成、物理知识融入及高效训练进一步推动IMU-HAR的实用化和智能化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Towards_Generalizable_Human_Activity_Recognition_A_Survey.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="219-Interpreting-Time-Series-Forecasts-with-LIME-and-SHAP-A-Case-Study-on-the-Air-Passengers-Dataset"><a href="#219-Interpreting-Time-Series-Forecasts-with-LIME-and-SHAP-A-Case-Study-on-the-Air-Passengers-Dataset" class="headerlink" title="219. Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset"></a>219. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Interpreting_Time_Series_Forecasts_with_LIME_and_SHAP__A_Case_Study_on_the_Air_Passengers_Dataset.pdf">Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent Researcher</span></p><p>该论文提出了将LIME和SHAP解释方法应用于时间序列预测模型的统一框架，采用ARIMA和XGBoost模型，并通过泄漏防控特征工程和监督学习方式，对Air Passengers数据集进行分析。实验结果显示，十二个月滞后特征和季节性编码对预测贡献最大，方法可泛化至多变量时间序列和其他机器学习模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Interpreting_Time_Series_Forecasts_with_LIME_and_SHAP_A_Case_Study_on_the_Air_Passengers_Dataset.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="220-Training-Machine-Learning-Models-on-Human-Spatio-temporal-Mobility-Data-An-Experimental-Study"><a href="#220-Training-Machine-Learning-Models-on-Human-Spatio-temporal-Mobility-Data-An-Experimental-Study" class="headerlink" title="220. Training Machine Learning Models on Human Spatio-temporal Mobility Data: An Experimental Study"></a>220. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Training_Machine_Learning_Models_on_Human_Spatio-temporal_Mobility_Data__An_Experimental_Study_[Expe.pdf">Training Machine Learning Models on Human Spatio-temporal Mobility Data: An Experimental Study</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Emory University</span></p><p>本论文提出了一套集成时间分割、用户语义嵌入和历史签到频率的统一时空预测框架，系统分析了多种采样和训练策略在人类移动轨迹预测中的表现。实验显示，融合用户个性化行为特征和时序上下文能显著提升预测精度，分层采样与小批量训练尤其有效，但用户语义信息的直接引入有时会引入噪声而非提升性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Training_Machine_Learning_Models_on_Human_Spatio-temporal_Mobility_Data_An_Experimental_Study.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="221-A-Language-Signal-Vision-Multimodal-Framework-for-Multitask-Cardiac-Analysis"><a href="#221-A-Language-Signal-Vision-Multimodal-Framework-for-Multitask-Cardiac-Analysis" class="headerlink" title="221. A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis"></a>221. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/A_Language-Signal-Vision_Multimodal_Framework_for_Multitask_Cardiac_Analysis.pdf">A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Birmingham</span></p><p>本论文提出了一种名为TGMM的多模态融合框架，结合语言（实验室检测文本）、信号（心电图）和视觉（心脏超声）三种医学数据，采用MedFlexFusion模块进行灵活特征融合，辅以文本引导模块实现多任务（如疾病诊断、风险分层、信息检索）预测，并集成可解释AI分析各模态贡献。实验证明TGMM在心力衰竭诊断和风险预测等任务上优于现有主流方法，且对缺失模态具有鲁棒性，并在独立数据集上验证了其泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/A_Language-Signal-Vision_Multimodal_Framework_for_Multitask_Cardiac_Analysis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="222-Transfer-Learning-for-Neutrino-Scattering-Domain-Adaptation-with-GANs"><a href="#222-Transfer-Learning-for-Neutrino-Scattering-Domain-Adaptation-with-GANs" class="headerlink" title="222. Transfer Learning for Neutrino Scattering: Domain Adaptation with GANs"></a>222. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Transfer_Learning_for_Neutrino_Scattering__Domain_Adaptation_with_GANs.pdf">Transfer Learning for Neutrino Scattering: Domain Adaptation with GANs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Wrocław</span></p><p>本文提出了一种基于生成对抗网络（GAN）和迁移学习的中微子散射模拟方法，通过将在合成中微子–碳散射数据上预训练的GAN模型迁移微调至中微子–氩和反中微子–碳等新领域，并冻结部分网络层实现高效域自适应。实验结果显示，迁移学习在数据稀缺情况下显著优于从零训练模型，提升了事件生成器的准确性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Transfer_Learning_for_Neutrino_Scattering_Domain_Adaptation_with_GANs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="223-Dextr-Zero-Shot-Neural-Architecture-Search-with-Singular-Value-Decomposition-and-Extrinsic-Curvature"><a href="#223-Dextr-Zero-Shot-Neural-Architecture-Search-with-Singular-Value-Decomposition-and-Extrinsic-Curvature" class="headerlink" title="223. Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature"></a>223. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Dextr__Zero-Shot_Neural_Architecture_Search_with_Singular_Value_Decomposition_and_Extrinsic_Curvatur.pdf">Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Friedrich-Alexander-Universität Erlangen-Nürnberg</span></p><p>该论文提出Dextr，一种无需标签数据的零样本神经结构搜索（Zero-Shot NAS）代理方法，结合特征图奇异值分解（SVD）与输出外在曲率，统一衡量网络的收敛性、泛化性和表达能力。实验表明，Dextr在多项NAS基准（NAS-Bench-101&#x2F;201&#x2F;301等）与ImageNet任务上均优于现有零样本和多样本NAS方法，且计算高效。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Dextr_Zero-Shot_Neural_Architecture_Search_with_Singular_Value_Decomposition_and_Extrinsic_Curvature.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="224-Causally-Guided-Pairwise-Transformer-Towards-Foundational-Digital-Twins-in-Process-Industry"><a href="#224-Causally-Guided-Pairwise-Transformer-Towards-Foundational-Digital-Twins-in-Process-Industry" class="headerlink" title="224. Causally-Guided Pairwise Transformer - Towards Foundational Digital Twins in Process Industry"></a>224. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Causally-Guided_Pairwise_Transformer_--_Towards_Foundational_Digital_Twins_in_Process_Industry.pdf">Causally-Guided Pairwise Transformer - Towards Foundational Digital Twins in Process Industry</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Software Competence Center Hagenberg</span></p><p>该论文提出了Causally-Guided Pairwise Transformer（CGPT）架构，通过结合通道无关（CI）和通道相关（CD）的配对建模范式，利用已知因果图作为归纳偏置，实现任意变量数量的灵活泛化和高效信息流建模。实验证明，CGPT在工业多维时间序列预测任务中，尤其在长时序预测中，显著优于CI和CD基线模型，并能适应多样的工业数据场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Causally-Guided_Pairwise_Transformer_-_Towards_Foundational_Digital_Twins_in_Process_Industry.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="225-Kourkoutas-β-A-Sunspike-Driven-Adam-Optimizer-with-Desert-Flair"><a href="#225-Kourkoutas-β-A-Sunspike-Driven-Adam-Optimizer-with-Desert-Flair" class="headerlink" title="225. Kourkoutas-β: A Sunspike-Driven Adam Optimizer with Desert Flair"></a>225. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Kourkoutas-Beta__A_Sunspike-Driven_Adam_Optimizer_with_Desert_Flair.pdf">Kourkoutas-β: A Sunspike-Driven Adam Optimizer with Desert Flair</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cyprus</span></p><p>本文提出Kourkoutas-β优化器，在Adam框架中引入基于分层动态调整的β2参数（通过“sunspike”比值度量梯度突发性），使其能更快响应PDE仿真、PINN和序列模型中的突发梯度，同时在平稳阶段保持平滑。实验显示，该方法在多个含有突发梯度的任务上显著提升优化稳定性和最终损失，理论上也保持Adam的收敛性质。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Kourkoutas-%CE%B2_A_Sunspike-Driven_Adam_Optimizer_with_Desert_Flair.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="226-FE-D-DPROC-Communication-Efficient-Differentially-Private-and-Robust-Federated-Learning"><a href="#226-FE-D-DPROC-Communication-Efficient-Differentially-Private-and-Robust-Federated-Learning" class="headerlink" title="226. FE D-DPROC: Communication-Efficient Differentially Private and Robust Federated Learning"></a>226. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Fed-DPRoC_Communication-Efficient_Differentially_Private_and_Robust_Federated_Learning.pdf">FE D-DPROC: Communication-Efficient Differentially Private and Robust Federated Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Munich</span></p><p>该论文提出了FE D-DPROC框架，实现了联邦学习中的差分隐私、拜占庭鲁棒性和通信效率三者兼顾。方法通过引入鲁棒兼容压缩，并以Johnson-Lindenstrauss变换为实例（RO BAJOL），结合高斯噪声、鲁棒聚合和理论证明，实现了在保证隐私和鲁棒性的同时显著降低通信开销。实验在CIFAR-10和Fashion MNIST上验证了理论结论，表现优于现有方法。结论：FE D-DPROC能有效提升联邦学习系统的安全性和实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/FE_D-DPROC_Communication-Efficient_Differentially_Private_and_Robust_Federated_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="227-The-path-to-a-goal-Understanding-soccer-possessions-via-path-signatures"><a href="#227-The-path-to-a-goal-Understanding-soccer-possessions-via-path-signatures" class="headerlink" title="227. The path to a goal: Understanding soccer possessions via path signatures"></a>227. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/The_path_to_a_goal__Understanding_soccer_possessions_via_path_signatures.pdf">The path to a goal: Understanding soccer possessions via path signatures</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Vienna University of Economics and Business</span></p><p>本文提出了一种利用路径特征（path signatures）对足球控球过程进行编码的新框架，实现对控球序列中下一个动作及其空间位置的预测。方法通过对可变长度、非均匀采样的时空数据进行log-signature特征提取，并结合嵌入和加权平均输入到前馈神经网络，避免了手工特征和固定历史窗口，显著优于Transformer基线模型，同时大幅降低计算成本。文章还提出了基于动作类型概率和位置的新控球价值评估指标，实验显示其相较于现有指标更能预测比赛表现。结论认为，该方法在预测准确性、计算效率和实际可解释性方面均有优势，并适用于更广泛的时空序列建模场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/The_path_to_a_goal_Understanding_soccer_possessions_via_path_signatures.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="228-FedUNet-A-Lightweight-Additive-U-Net-Module-for-Federated-Learning-with-Heterogeneous-Models"><a href="#228-FedUNet-A-Lightweight-Additive-U-Net-Module-for-Federated-Learning-with-Heterogeneous-Models" class="headerlink" title="228. FedUNet: A Lightweight Additive U-Net Module for Federated Learning with Heterogeneous Models"></a>228. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/FedUNet__A_Lightweight_Additive_U-Net_Module_for_Federated_Learning_with_Heterogeneous_Models.pdf">FedUNet: A Lightweight Additive U-Net Module for Federated Learning with Heterogeneous Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Dankook University</span></p><p>FedUNet提出了一种轻量级、架构无关的联邦学习框架，通过将U-Net风格的附加模块连接到每个客户端的主模型，仅同步U-Net的瓶颈部分，实现了高效和架构独立的知识迁移。实验结果表明，该方法在支持模型异构性、低通信成本和保持竞争精度方面优于现有方法，是异构联邦学习的可扩展解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/FedUNet_A_Lightweight_Additive_U-Net_Module_for_Federated_Learning_with_Heterogeneous_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="229-SparseMap-A-Sparse-Tensor-Accelerator-Framework-Based-on-Evolution-Strategy"><a href="#229-SparseMap-A-Sparse-Tensor-Accelerator-Framework-Based-on-Evolution-Strategy" class="headerlink" title="229. SparseMap: A Sparse Tensor Accelerator Framework Based on Evolution Strategy"></a>229. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/SparseMap__A_Sparse_Tensor_Accelerator_Framework_Based_on_Evolution_Strategy.pdf">SparseMap: A Sparse Tensor Accelerator Framework Based on Evolution Strategy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>该论文提出了SparseMap框架，利用进化策略（Evolution Strategy）对稀疏张量加速器的映射（mapping）和稀疏策略（sparse strategy）进行联合自动优化，解决了因设计空间组合爆炸和大量无效设计点导致的优化效率低下问题。通过引入素因数编码、康托编码、高敏感性超立方初始化和定制进化算子，有效提升了搜索效率和收敛速度，相比SAGE与Sparseloop等方法在多种硬件平台上显著降低能耗-延迟积（EDP），实现了跨平台、可扩展的高效稀疏张量加速器自动设计。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/SparseMap_A_Sparse_Tensor_Accelerator_Framework_Based_on_Evolution_Strategy.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="230-Short-Term-Forecasting-of-Energy-Production-and-Consumption-Using-Extreme-Learning-Machine-A-Comprehensive-MIMO-based-ELM-Approach"><a href="#230-Short-Term-Forecasting-of-Energy-Production-and-Consumption-Using-Extreme-Learning-Machine-A-Comprehensive-MIMO-based-ELM-Approach" class="headerlink" title="230. Short-Term Forecasting of Energy Production and Consumption Using Extreme Learning Machine: A Comprehensive MIMO based ELM Approach"></a>230. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Short-Term_Forecasting_of_Energy_Production_and_Consumption_Using_Extreme_Learning_Machine__A_Compre.pdf">Short-Term Forecasting of Energy Production and Consumption Using Extreme Learning Machine: A Comprehensive MIMO based ELM Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mines Paris, PSL University</span></p><p>本文提出了一种基于极限学习机（ELM）的多输入多输出（MIMO）框架，实现了对多种可再生与不可再生能源生产和消耗的短期预测。方法采用滑动窗口与时序特征编码，通过ELM在多源数据上建模，显著优于单输入单输出（SISO）模型和深度学习LSTM方法，尤其在总能耗、太阳能及热能预测中表现优异，且计算效率高，适合实时应用。结论表明，该方法有助于优化能源调度和可再生能源并网，具备良好的实际推广价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Short-Term_Forecasting_of_Energy_Production_and_Consumption_Using_Extreme_Learning_Machine_A_Comprehensive_MIMO_based_ELM_Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="231-Asymmetric-Diffusion-Recommendation-Model"><a href="#231-Asymmetric-Diffusion-Recommendation-Model" class="headerlink" title="231. Asymmetric Diffusion Recommendation Model"></a>231. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Asymmetric_Diffusion_Recommendation_Model.pdf">Asymmetric Diffusion Recommendation Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ByteDance</span></p><p>该论文提出了Asymmetric Diffusion Recommendation Model (AsymDiffRec)，通过在推荐系统中采用离散特征丢弃的前向过程和在潜在空间进行异步逆向重构，提升特征表达与个性化信息保存。实验结果显示，在抖音音乐App上线后，用户活跃天数和使用时长分别提升0.131%和0.166%，证明了模型的实际有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Asymmetric_Diffusion_Recommendation_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="232-A-Unified-Cortical-Circuit-Model-with-Divisive-Normalization-and-Self-Excitation-for-Robust-Representation-and-Memory-Maintenance"><a href="#232-A-Unified-Cortical-Circuit-Model-with-Divisive-Normalization-and-Self-Excitation-for-Robust-Representation-and-Memory-Maintenance" class="headerlink" title="232. A Unified Cortical Circuit Model with Divisive Normalization and Self-Excitation for Robust Representation and Memory Maintenance"></a>232. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/A_Unified_Cortical_Circuit_Model_with_Divisive_Normalization_and_Self-Excitation_for_Robust_Represen.pdf">A Unified Cortical Circuit Model with Divisive Normalization and Self-Excitation for Robust Representation and Memory Maintenance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Qiyuan Laborotary</span></p><p>本文提出了一个结合除法归一化（Divisive Normalization）和自激励（Self-Excitation）的递归神经回路模型（RDN），通过数学分析证明其可在适当参数下形成连续吸引子，实现输入比例稳定化和输入撤除后自维持的记忆状态。该模型在随机点运动视标（RDK）任务实现了抗噪声编码，在概率Wisconsin卡片分类测试（pWCST）中实现了近似贝叶斯信念更新，结论显示该统一框架可桥接噪声抑制、工作记忆与贝叶斯推断，为生物启发式神经网络设计和脑计算机制研究提供新思路。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/A_Unified_Cortical_Circuit_Model_with_Divisive_Normalization_and_Self-Excitation_for_Robust_Representation_and_Memory_Maintenance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="233-Score-informed-Neural-Operator-for-Enhancing-Ordering-based-Causal-Discovery"><a href="#233-Score-informed-Neural-Operator-for-Enhancing-Ordering-based-Causal-Discovery" class="headerlink" title="233. Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery"></a>233. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Score-informed_Neural_Operator_for_Enhancing_Ordering-based_Causal_Discovery.pdf">Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">LG AI Research</span></p><p>该论文提出了一种基于神经算子的泛函扩散模型SciNO，用于稳定、高效地估计对数密度的Hessian对角线，提升基于Score Matching的因果排序发现算法的准确性和可扩展性。方法包括引入可学习的时序编码和傅里叶神经算子，显著提升了高维因果结构发现的性能；此外，作者还提出了一种将SciNO概率统计与自回归生成模型（如LLM）先验结合的概率控制算法，实现无需微调或提示工程的可靠数据驱动因果排序。实验结果表明，SciNO方法在合成和真实数据集上均显著优于现有方法，并能有效提升LLM等生成模型的因果推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/Score-informed_Neural_Operator_for_Enhancing_Ordering-based_Causal_Discovery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="234-A-Self-Ensemble-Inspired-Approach-for-Effective-Training-of-Binary-Weight-Spiking-Neural-Networks"><a href="#234-A-Self-Ensemble-Inspired-Approach-for-Effective-Training-of-Binary-Weight-Spiking-Neural-Networks" class="headerlink" title="234. A Self-Ensemble Inspired Approach for Effective Training of Binary-Weight Spiking Neural Networks"></a>234. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/A_Self-Ensemble_Inspired_Approach_for_Effective_Training_of_Binary-Weight_Spiking_Neural_Networks.pdf">A Self-Ensemble Inspired Approach for Effective Training of Binary-Weight Spiking Neural Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Pengcheng Laboratory</span></p><p>本论文提出了一种新的观点，将前馈型脉冲神经网络（SNN）视为带噪声注入的二值激活神经网络（BANN）的自集成，并据此提出了SEI-BWSNN训练方法。该方法结合多重短路结构与基于知识蒸馏的损失函数，并引入二阶段训练策略及权重二值化更新规则，实现了低延迟、高精度的二值权重SNN训练；实验在ImageNet等数据集上取得了优异表现，验证了方法的有效性和BWSNN在低功耗低存储场景下的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-20/A_Self-Ensemble_Inspired_Approach_for_Effective_Training_of_Binary-Weight_Spiking_Neural_Networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025/08/19报纸</title>
    <link href="/2025/08/2025-08-19_article/"/>
    <url>/2025/08/2025-08-19_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2025/08/19报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-19<br>📄 <strong>发现论文数量</strong>：98  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Optimal-CO2-storage-management-considering-safety-constraints-in-multi-stakeholder-multi-site-CCS-projects-a-game-theoretic-perspective"><a href="#1-Optimal-CO2-storage-management-considering-safety-constraints-in-multi-stakeholder-multi-site-CCS-projects-a-game-theoretic-perspective" class="headerlink" title="1. Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Optimal_CO2_storage_management_considering_safety_constraints_in_multi-stakeholder_multi-site_CCS_pr.pdf">Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bureau of Economic Geology, University of Texas at Austin</span></p><p>本文提出了一种基于安全多智能体深度确定性策略梯度（MADDPG）和受限Markov博弈的框架，用于多利益相关方地质碳储存（GCS）项目的优化管理。方法结合Embed-to-Control（E2C）神经网络代理模型和分区奖励&#x2F;惩罚机制，实现了在安全约束下各方目标的最优兼顾。实验结果表明，该方法在协作与竞争场景下均能有效提升整体经济回报，确保注入安全，并优于传统多目标优化算法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Optimal_CO2_storage_management_considering_safety_constraints_in_multi-stakeholder_multi-site_CCS_projects_a_game_theoretic_perspective.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-SeamlessFlow-A-Trainer–Agent-Isolation-RL-Framework-Achieving-Bubble-Free-Pipelines-via-Tag-Scheduling"><a href="#2-SeamlessFlow-A-Trainer–Agent-Isolation-RL-Framework-Achieving-Bubble-Free-Pipelines-via-Tag-Scheduling" class="headerlink" title="2. SeamlessFlow: A Trainer–Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/SeamlessFlow__A_Trainer_Agent_Isolation_RL_Framework_Achieving_Bubble-Free_Pipelines_via_Tag_Schedul.pdf">SeamlessFlow: A Trainer–Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou</span></p><p>SeamlessFlow提出了一种服务器端强化学习框架，通过数据面层实现RL训练与多样化Agent的彻底隔离，并通过标签驱动的资源调度，将硬件抽象为能力标签资源，实现统一的空间-时间复用管道。该方法消除了分布式系统中的管道空闲，显著提升了大规模多Agent RL任务的训练效率和稳定性。实验显示其在工业级异构集群上具备极高的吞吐和可扩展性，显著优于现有主流RL框架。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SeamlessFlow_A_Trainer%E2%80%93Agent_Isolation_RL_Framework_Achieving_Bubble-Free_Pipelines_via_Tag_Scheduling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Fusing-Rewards-and-Preferences-in-Reinforcement-Learning"><a href="#3-Fusing-Rewards-and-Preferences-in-Reinforcement-Learning" class="headerlink" title="3. Fusing Rewards and Preferences in Reinforcement Learning"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Fusing_Rewards_and_Preferences_in_Reinforcement_Learning.pdf">Fusing Rewards and Preferences in Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer and Communication Sciences, EPFL</span></p><p>本文提出了Dual-Feedback Actor (DFA)算法，将数值奖励和成对偏好反馈统一到单一的策略更新规则中，关键技术包括直接利用策略的对数概率建模偏好概率、无须奖励建模、支持on-&#x2F;off-policy学习，以及通过Q值合成偏好对。理论上，DFA的偏好损失最小化等价于恢复熵正则化Soft Actor-Critic (SAC)策略，实验表明在六个控制任务和GridWorld环境中，DFA的性能与SAC相当或优于SAC，对比奖励建模的RLHF基线表现更优且训练更平稳。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Fusing_Rewards_and_Preferences_in_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-HOID-R1-Reinforcement-Learning-for-Open-World-Human-Object-Interaction-Detection-Reasoning-with-Multimodal-Large-Language-Model"><a href="#4-HOID-R1-Reinforcement-Learning-for-Open-World-Human-Object-Interaction-Detection-Reasoning-with-Multimodal-Large-Language-Model" class="headerlink" title="4. HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/HOID-R1__Reinforcement_Learning_for_Open-World_Human-Object_Interaction_Detection_Reasoning_with_Mul.pdf">HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ShanghaiTech University</span></p><p>本文提出了HOID-R1，一种结合链式思维（CoT）监督微调和群组相对策略优化（GRPO）的强化学习框架，实现开放世界下的人体-物体交互（HOI）检测。该方法利用多模态大语言模型作为判官监督推理过程，通过多重奖励信号优化策略，并有效缓解推理幻觉，显著提升了模型在各类开放词汇场景下的泛化和检测准确性。在HICO-DET和SWIG-HOI数据集上的实验结果表明，HOID-R1在已见和未见场景均超越现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/HOID-R1_Reinforcement_Learning_for_Open-World_Human-Object_Interaction_Detection_Reasoning_with_Multimodal_Large_Language_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="5-Investigating-Sensors-and-Methods-in-Grasp-State-Classification-in-Agricultural-Manipulation"><a href="#5-Investigating-Sensors-and-Methods-in-Grasp-State-Classification-in-Agricultural-Manipulation" class="headerlink" title="5. Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Investigating_Sensors_and_Methods_in_Grasp_State_Classification_in_Agricultural_Manipulation.pdf">Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois at Urbana-Champaign</span></p><p>本文提出了一种用于农业抓取状态分类的多传感器系统，将IMU、红外、张力、触觉和RGB摄像头集成到顺应性机械手中，并利用随机森林和LSTM模型对抓取状态（滑移、抓取失败、成功采摘等）进行分类。实验结果表明，随机森林模型在实验室训练并迁移到真实番茄植株上，能以100%准确率识别多种抓取状态，IMU与张力传感器的组合已足以实现高效分类，这为果实自动采摘提供了实时反馈和纠正动作的依据。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Investigating_Sensors_and_Methods_in_Grasp_State_Classification_in_Agricultural_Manipulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-Visual-Perception-Engine-Fast-and-Flexible-Multi-Head-Inference-for-Robotic-Vision-Tasks"><a href="#6-Visual-Perception-Engine-Fast-and-Flexible-Multi-Head-Inference-for-Robotic-Vision-Tasks" class="headerlink" title="6. Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Visual_Perception_Engine__Fast_and_Flexible_Multi-Head_Inference_for_Robotic_Vision_Tasks.pdf">Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Jet Propulsion Laboratory, California Institute of Technology</span></p><p>本文提出了VPEngine框架，通过共享视觉基础模型（如DINOv2）骨干，实现多任务（深度估计、目标检测、语义分割）模型头高效并行推理，采用CUDA MPS和GPU内存共享结构，显著提升推理速度和GPU利用率。实验表明，该方法在NVIDIA Jetson Orin AGX上可实现高达3.3倍速度提升，且具备可动态调整任务频率、内存可控、模块化扩展性，适合多种机器人视觉场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Visual_Perception_Engine_Fast_and_Flexible_Multi-Head_Inference_for_Robotic_Vision_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Towards-Fully-Onboard-State-Estimation-and-Trajectory-Tracking-for-UAVs-with-Suspended-Payloads"><a href="#7-Towards-Fully-Onboard-State-Estimation-and-Trajectory-Tracking-for-UAVs-with-Suspended-Payloads" class="headerlink" title="7. Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Towards_Fully_Onboard_State_Estimation_and_Trajectory_Tracking_for_UAVs_with_Suspended_Payloads.pdf">Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Czech Technical University in Prague</span></p><p>该论文提出了一种面向无人机悬挂载荷的全自主、实用、低硬件需求的状态估计与轨迹跟踪框架。方法采用标准机载RTK GNSS与IMU，结合线性卡尔曼滤波器进行状态估计、模型预测轮廓控制（MPCC）规划平滑轨迹，并用增量式模型预测控制（MPC）实现鲁棒闭环控制。大量仿真和实地实验表明，该系统在仅用标准传感器的条件下，表现接近理想地面真值基准，跟踪误差增加小于6%，且对载荷参数变化具有很强鲁棒性，展示了面向实际无人机悬挂运输的可行性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Towards_Fully_Onboard_State_Estimation_and_Trajectory_Tracking_for_UAVs_with_Suspended_Payloads.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-MultiPark-Multimodal-Parking-Transformer-with-Next-Segment-Prediction"><a href="#8-MultiPark-Multimodal-Parking-Transformer-with-Next-Segment-Prediction" class="headerlink" title="8. MultiPark: Multimodal Parking Transformer with Next-Segment Prediction"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/MultiPark__Multimodal_Parking_Transformer_with_Next-Segment_Prediction.pdf">MultiPark: Multimodal Parking Transformer with Next-Segment Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了MultiPark，一种基于自回归Transformer的多模态自动泊车方法。其方法创新包括数据高效的next-segment预测范式、结合齿轮&#x2F;纵向&#x2F;横向因子的可学习泊车查询、并行多模态轨迹解码，以及两阶段训练策略（模仿损失+目标导向损失）以缓解因果混淆并提升泛化能力。实验表明MultiPark在真实数据集和实际车辆部署中均取得了优异的泊车成功率和安全性，显著优于同类方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MultiPark_Multimodal_Parking_Transformer_with_Next-Segment_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-Sim2Dust-Mastering-Dynamic-Waypoint-Tracking-on-Granular-Media"><a href="#9-Sim2Dust-Mastering-Dynamic-Waypoint-Tracking-on-Granular-Media" class="headerlink" title="9. Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Sim2Dust__Mastering_Dynamic_Waypoint_Tracking_on_Granular_Media.pdf">Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Luxembourg</span></p><p>本论文提出了一个完整的sim-to-real框架，用于在类月球颗粒介质上实现轮式机器人动态航点跟踪。方法上，利用Space Robotics Bench进行大规模并行仿真，结合程序化生成环境与物理参数域随机化，采用DreamerV3等强化学习算法训练策略，并通过零样本迁移在真实月壤实验场地实现精确稳定的导航。实验结果表明，程序化多样性训练显著提升了真实部署的泛化和稳定性，模型基DreamerV3在样本效率和精度上表现最佳，简单动作平滑可有效提升硬件部署的安全性和稳定性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Sim2Dust_Mastering_Dynamic_Waypoint_Tracking_on_Granular_Media.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-Relative-Position-Matters-Trajectory-Prediction-and-Planning-with-Polar-Representation"><a href="#10-Relative-Position-Matters-Trajectory-Prediction-and-Planning-with-Polar-Representation" class="headerlink" title="10. Relative Position Matters: Trajectory Prediction and Planning with Polar Representation"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Relative_Position_Matters__Trajectory_Prediction_and_Planning_with_Polar_Representation.pdf">Relative Position Matters: Trajectory Prediction and Planning with Polar Representation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出Polaris框架，首次在轨迹预测与规划中采用极坐标系，通过Polar场景上下文编码和Polar关系细化模块，并引入Relative Embedding Transformer显式建模交通要素间的相对距离与方向。实验在Argoverse 2和nuPlan数据集上表明，该方法在预测准确性和推理效率上均达到SOTA水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Relative_Position_Matters_Trajectory_Prediction_and_Planning_with_Polar_Representation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-i2Nav-Robot-A-Large-Scale-Indoor-Outdoor-Robot-Dataset-for-Multi-Sensor-Fusion-Navigation-and-Mapping"><a href="#11-i2Nav-Robot-A-Large-Scale-Indoor-Outdoor-Robot-Dataset-for-Multi-Sensor-Fusion-Navigation-and-Mapping" class="headerlink" title="11. i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/i2Nav-Robot__A_Large-Scale_Indoor-Outdoor_Robot_Dataset_for_Multi-Sensor_Fusion_Navigation_and_Mappi.pdf">i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>该论文提出了i2Nav-Robot，一个用于室内外环境多传感器融合导航与建图的大规模数据集。方法包括集成最新固态激光雷达、4D毫米波雷达、立体摄像头、里程计、GNSS和高精度IMU于全向轮式机器人，并采用硬件和离线校准实现微秒级时间同步，采集覆盖多样场景的十条长序列，提供厘米级高频地面真实数据。结论是该数据集已被十余种开源多传感器系统验证，数据质量优异，可广泛用于机器人导航、SLAM及融合方法研究。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/i2Nav-Robot_A_Large-Scale_Indoor-Outdoor_Robot_Dataset_for_Multi-Sensor_Fusion_Navigation_and_Mapping.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-OVSegDT-Segmenting-Transformer-for-Open-Vocabulary-Object-Goal-Navigation"><a href="#12-OVSegDT-Segmenting-Transformer-for-Open-Vocabulary-Object-Goal-Navigation" class="headerlink" title="12. OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/OVSegDT__Segmenting_Transformer_for_Open-Vocabulary_Object_Goal_Navigation.pdf">OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AIRI</span></p><p>本文提出OVSegDT，一种轻量级transformer策略，通过集成目标二值分割掩码编码器和辅助分割损失，实现对开放词汇物体目标导航任务的高效学习。方法还引入熵自适应损失调节（EALM），根据策略熵自动平衡模仿学习与强化学习信号，无需手动切换阶段。实验结果表明，该方法在未见类别上导航性能与见过类别一致，训练样本复杂度降低33%，碰撞次数减少一半，且优于现有RGB-only和部分大模型方法。结论：OVSegDT显著提升了机器人在新环境下对未知物体类别的导航能力，具备高泛化性和安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/OVSegDT_Segmenting_Transformer_for_Open-Vocabulary_Object_Goal_Navigation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-ReachVox-Clutter-free-Reachability-Visualization-for-Robot-Motion-Planning-in-Virtual-Reality"><a href="#13-ReachVox-Clutter-free-Reachability-Visualization-for-Robot-Motion-Planning-in-Virtual-Reality" class="headerlink" title="13. ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ReachVox__Clutter-free_Reachability_Visualization_for_Robot_Motion_Planning_in_Virtual_Reality.pdf">ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Coburg University of Applied Sciences and Arts</span></p><p>该论文提出了一种名为REACHVOX的新型可视化方法，通过在虚拟现实中以半透明有色体素的方式显示机器人在任务空间的可达区域，显著减少了视觉杂乱，帮助远程操作者高效规划机器人运动。用户实验表明，REACHVOX相比传统点可达性检查，在任务完成时间、操作次数、易用性和信任度等主客观指标上均有显著提升，用户更倾向于使用该方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ReachVox_Clutter-free_Reachability_Visualization_for_Robot_Motion_Planning_in_Virtual_Reality.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Optimizing-ROS-2-Communication-for-Wireless-Robotic-Systems"><a href="#14-Optimizing-ROS-2-Communication-for-Wireless-Robotic-Systems" class="headerlink" title="14. Optimizing ROS 2 Communication for Wireless Robotic Systems"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Optimizing_ROS_2_Communication_for_Wireless_Robotic_Systems.pdf">Optimizing ROS 2 Communication for Wireless Robotic Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">DGIST</span></p><p>该论文针对无线环境下ROS 2 DDS通信栈在大数据载体传输时的瓶颈（如IP分片、重传时机、缓冲区突发）进行了系统建模和网络层深入分析，并提出了一种基于标准XML QoS配置的轻量级DDS优化框架，无需协议或应用修改即可提升无线通信的鲁棒性和实时性。实验证明该方法在各种无线场景下显著提高了数据接收率、降低了延迟和抖动，尤其在高丢包和链路中断条件下表现优异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Optimizing_ROS_2_Communication_for_Wireless_Robotic_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-CRAFT-GUI-Curriculum-Reinforced-Agent-For-GUI-Tasks"><a href="#15-CRAFT-GUI-Curriculum-Reinforced-Agent-For-GUI-Tasks" class="headerlink" title="15. CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CRAFT-GUI__Curriculum-Reinforced_Agent_For_GUI_Tasks.pdf">CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ant Group</span></p><p>该论文提出了CRAFT-GUI，一种结合分阶段课程学习和分组相对策略优化（GRPO）的强化学习框架，专门针对图形用户界面（GUI）任务。方法通过难度分层的任务训练和细粒度混合奖励机制提升了agent在移动设备上的操作与理解能力。实验结果表明，CRAFT-GUI在多个公开和内部基准上超过现有方法，任务成功率提升5.6%和10.3%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CRAFT-GUI_Curriculum-Reinforced_Agent_For_GUI_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-Swarm-in-Blocks-Simplifying-Drone-Swarm-Programming-with-Block-Based-Language"><a href="#16-Swarm-in-Blocks-Simplifying-Drone-Swarm-Programming-with-Block-Based-Language" class="headerlink" title="16. Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Swarm-in-Blocks__Simplifying_Drone_Swarm_Programming_with_Block-Based_Language.pdf">Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Sao Paulo</span></p><p>本文提出了Swarm-in-Blocks，一个基于模块化（Block-Based）编程语言的无人机集群高层接口，用户可通过拼接代码块轻松实现无人机编队的循环、条件与传感器操作，降低了ROS等底层知识门槛。该平台在Clover系统基础上，具备直观用户界面、实时监控、碰撞规避、队形与变换等关键技术，并已在仿真和实际应用中表现出良好可扩展性与易用性，适用于工业、农业、娱乐和教育等多场景。结论认为，该系统极大简化了无人机集群编程，提高了非专业用户和教育领域的可及性，未来将继续优化与拓展应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Swarm-in-Blocks_Simplifying_Drone_Swarm_Programming_with_Block-Based_Language.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-ImagiDrive-A-Unified-Imagination-and-Planning-Framework-for-Autonomous-Driving"><a href="#17-ImagiDrive-A-Unified-Imagination-and-Planning-Framework-for-Autonomous-Driving" class="headerlink" title="17. ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ImagiDrive__A_Unified_Imagination-and-Planning_Framework_for_Autonomous_Driving.pdf">ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出ImagiDrive，一个将视觉语言模型（VLM）与驾驶世界模型（DWM）集成的自动驾驶端到端框架，通过想象与规划循环实现更安全高效的驾驶决策。方法包括VLM驱动的多模态驾驶代理、DWM驱动的场景想象器、早停和轨迹选择机制，框架在nuScenes和NAVSIM等数据集上显著优于现有方法。结论：ImagiDrive能提升复杂环境下的安全性和规划表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ImagiDrive_A_Unified_Imagination-and-Planning_Framework_for_Autonomous_Driving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-Pedestrian-Dead-Reckoning-using-Invariant-Extended-Kalman-Filter"><a href="#18-Pedestrian-Dead-Reckoning-using-Invariant-Extended-Kalman-Filter" class="headerlink" title="18. Pedestrian Dead Reckoning using Invariant Extended Kalman Filter"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Pedestrian_Dead_Reckoning_using_Invariant_Extended_Kalman_Filter.pdf">Pedestrian Dead Reckoning using Invariant Extended Kalman Filter</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ShanghaiTech University</span></p><p>本文提出了一种面向双足机器人在无GPS环境下的低成本惯性行人航位推算方法，核心采用基于矩阵李群理论的InEKF（不变扩展卡尔曼滤波器），利用足部IMU的静止伪测量实现状态创新，有效提升估计精度。实验表明，在人体与双足机器人上，所提InEKF方法优于传统EKF，表现为轨迹误差更小、参数调优更容易，且对噪声协方差的敏感性更低。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Pedestrian_Dead_Reckoning_using_Invariant_Extended_Kalman_Filter.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Tactile-Robotics-An-Outlook"><a href="#19-Tactile-Robotics-An-Outlook" class="headerlink" title="19. Tactile Robotics: An Outlook"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Tactile_Robotics__An_Outlook.pdf">Tactile Robotics: An Outlook</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">King’s College London</span></p><p>本论文系统性回顾了机器人触觉领域的关键技术，包括触觉材料、传感器、传感网络、仿真、数据采集与解释、多模态融合，以及主动触觉感知方法。文章总结认为，未来机器人触觉的发展需整合新型材料与高密度分布式传感、仿真工具、机器学习算法和多模态感知，推动机器人实现类人触觉灵巧，拓展其在制造、医疗、农业等多领域的应用。结论指出，尽管已取得显著进展，但触觉机器人的传感集成、数据处理、能量供给与大规模网络等仍具挑战，未来需跨学科协作持续创新以实现人机自然交互和环境敏感性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Tactile_Robotics_An_Outlook.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-Multi-Group-Equivariant-Augmentation-for-Reinforcement-Learning-in-Robot-Manipulation"><a href="#20-Multi-Group-Equivariant-Augmentation-for-Reinforcement-Learning-in-Robot-Manipulation" class="headerlink" title="20. Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Multi-Group_Equivariant_Augmentation_for_Reinforcement_Learning_in_Robot_Manipulation.pdf">Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong</span></p><p>该论文提出了一种基于多组非等距对称性的POMDP新建模方法，并设计了多组等变数据增强（MEA）策略，通过在空间和时间维度上对操作轨迹施加独立群变换，实现了更高多样性和有效性的数据扩充，结合离线强化学习和基于体素的视觉等变表征，显著提升了机器人操作任务的采样效率和泛化能力。实验结果表明，该方法在通用操作和外科抓取两大机器人领域均大幅提升了学习收敛速度与成功率，减少了97.5%以上的示范数据需求并降低训练时间超过58%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Multi-Group_Equivariant_Augmentation_for_Reinforcement_Learning_in_Robot_Manipulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-Visuomotor-Grasping-with-World-Models-for-Surgical-Robots"><a href="#21-Visuomotor-Grasping-with-World-Models-for-Surgical-Robots" class="headerlink" title="21. Visuomotor Grasping with World Models for Surgical Robots"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Visuomotor_Grasping_with_World_Models_for_Surgical_Robots.pdf">Visuomotor Grasping with World Models for Surgical Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Chinese University of Hong Kong</span></p><p>本文提出了GASv2框架，结合世界模型（DreamerV2）、专用视觉感知管线和混合控制策略，实现了在模拟环境中训练的视觉运动策略可直接迁移至实际外科机器人，仅依赖标准立体内窥镜视觉输入。实验表明，该方法在虚拟和实际外科场景中抓取成功率达65%，对未见物体、抓手及多类干扰具有强泛化和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Visuomotor_Grasping_with_World_Models_for_Surgical_Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-Actor-Critic-for-Continuous-Action-Chunks-A-Reinforcement-Learning-Framework-for-Long-Horizon-Robotic-Manipulation-with-Sparse-Reward"><a href="#22-Actor-Critic-for-Continuous-Action-Chunks-A-Reinforcement-Learning-Framework-for-Long-Horizon-Robotic-Manipulation-with-Sparse-Reward" class="headerlink" title="22. Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Actor-Critic_for_Continuous_Action_Chunks__A_Reinforcement_Learning_Framework_for_Long-Horizon_Robot.pdf">Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出了AC3，一种新颖的强化学习框架，针对长时序、稀疏奖励的机器人操作任务，直接学习生成高维连续动作序列。方法包含基于DDPG的actor-critic结构：actor采用不对称更新，仅从成功轨迹学习以保证策略稳定改进；critic利用intra-chunk n-step返回和自监督奖励模块提升学习稳定性和数据效率。实验证明，AC3在BiGym和RLBench 25个任务上，仅用少量专家演示和简单模型结构即优于现有方法，展现出在复杂操作任务中的高效稳定性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Actor-Critic_for_Continuous_Action_Chunks_A_Reinforcement_Learning_Framework_for_Long-Horizon_Robotic_Manipulation_with_Sparse_Reward.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-Geometry-Aware-Predictive-Safety-Filters-on-Humanoids-From-Poisson-Safety-Functions-to-CBF-Constrained-MPC"><a href="#23-Geometry-Aware-Predictive-Safety-Filters-on-Humanoids-From-Poisson-Safety-Functions-to-CBF-Constrained-MPC" class="headerlink" title="23. Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Geometry-Aware_Predictive_Safety_Filters_on_Humanoids__From_Poisson_Safety_Functions_to_CBF_Constrai.pdf">Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">California Institute of Technology</span></p><p>本文提出了一种基于非线性模型预测控制（MPC）与控制屏障函数（CBF）的几何感知预测安全滤波方法，针对动态和非结构化环境中的人形及四足机器人安全导航。方法通过扩展泊松安全函数，将其定义于时间和姿态的高维空间，并结合MPC实现实时安全约束轨迹生成，且利用Minkowski运算精确建模机器人几何。实验在多种安全关键场景下验证了该方法对机器人安全避障和复杂环境导航的有效性和实时性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Geometry-Aware_Predictive_Safety_Filters_on_Humanoids_From_Poisson_Safety_Functions_to_CBF_Constrained_MPC.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="24-LoRAtorio-An-intrinsic-approach-to-LoRA-Skill-Composition"><a href="#24-LoRAtorio-An-intrinsic-approach-to-LoRA-Skill-Composition" class="headerlink" title="24. LoRAtorio: An intrinsic approach to LoRA Skill Composition"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/LoRAtorio__An_intrinsic_approach_to_LoRA_Skill_Composition.pdf">LoRAtorio: An intrinsic approach to LoRA Skill Composition</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Cambridge Research Laboratory, Toshiba Europe</span></p><p>本文提出了一种无需训练的多LoRA技能组合方法LoRAtorio，通过在扩散模型的潜空间按空间patch划分，并计算LoRA与基础模型每个patch的余弦相似度，动态构建权重矩阵实现多LoRA输出的加权融合；同时在分类器自由引导机制中引入基础模型的无条件分数以缓解领域漂移。实验显示该方法在ComposLoRA基准和动态模块选择场景下均取得SOTA，提升了多概念生成的鲁棒性和图像质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LoRAtorio_An_intrinsic_approach_to_LoRA_Skill_Composition.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-DashCam-Video-A-complementary-low-cost-data-stream-for-on-demand-forest-infrastructure-system-monitoring"><a href="#25-DashCam-Video-A-complementary-low-cost-data-stream-for-on-demand-forest-infrastructure-system-monitoring" class="headerlink" title="25. DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/DashCam_Video__A_complementary_low-cost_data_stream_for_on-demand_forest-infrastructure_system_monit.pdf">DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Connecticut</span></p><p>本论文提出了一种基于车辆仪表盘摄像头（dashcam）视频的低成本、可扩展的城市道路绿化和基础设施实时监测框架。方法包含单目深度估计（Depth-anything V2模型）、XGBoost深度修正、GPS与图像三角测量定位、以及基于针孔相机几何的对象结构参数提取。实验结果显示，在内置摄像头和低速行驶条件下，定位误差平均为2.83米，结构参数（如高度）估计误差最低，并对树木等复杂目标仍有一定误差但整体表现优异。结论认为，该方法具备实时、低成本、自动化监测城市绿地与基础设施的潜力，适用于城市管理和智能交通等场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/DashCam_Video_A_complementary_low-cost_data_stream_for_on-demand_forest-infrastructure_system_monitoring.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-TrajSV-A-Trajectory-based-Model-for-Sports-Video-Representations-and-Applications"><a href="#26-TrajSV-A-Trajectory-based-Model-for-Sports-Video-Representations-and-Applications" class="headerlink" title="26. TrajSV: A Trajectory-based Model for Sports Video Representations and Applications"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/TrajSV__A_Trajectory-based_Model_for_Sports_Video_Representations_and_Applications.pdf">TrajSV: A Trajectory-based Model for Sports Video Representations and Applications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Technologies, Co., Ltd.</span></p><p>本文提出了TrajSV，一种基于轨迹的体育视频表征框架，包括数据预处理、轨迹增强Transformer模块的Clip Representation Network（CRNet）、以及聚合可视与轨迹特征的Video Representation Network（VRNet）。TrajSV采用三重对比损失在无监督方式下优化视频和片段表征，在体育视频检索、动作定位、视频字幕等多个任务上取得最优性能，显著提升了检索准确率和下游任务效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/TrajSV_A_Trajectory-based_Model_for_Sports_Video_Representations_and_Applications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-Reinforcing-Video-Reasoning-Segmentation-to-Think-Before-It-Segments"><a href="#27-Reinforcing-Video-Reasoning-Segmentation-to-Think-Before-It-Segments" class="headerlink" title="27. Reinforcing Video Reasoning Segmentation to Think Before It Segments"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Reinforcing_Video_Reasoning_Segmentation_to_Think_Before_It_Segments.pdf">Reinforcing Video Reasoning Segmentation to Think Before It Segments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Dalian University of Technology</span></p><p>该论文提出Veason-R1模型，通过两阶段训练实现视频推理分割：首先利用链式思考(CoT)数据进行有监督微调，赋予模型分层推理能力，然后采用基于GRPO的强化学习进一步优化时空定位和分割表现。结果表明，Veason-R1在多个基准上取得了领先性能，显著提升了分割准确性与鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Reinforcing_Video_Reasoning_Segmentation_to_Think_Before_It_Segments.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-Handwritten-Text-Recognition-of-Historical-Manuscripts-Using-Transformer-Based-Models"><a href="#28-Handwritten-Text-Recognition-of-Historical-Manuscripts-Using-Transformer-Based-Models" class="headerlink" title="28. Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Handwritten_Text_Recognition_of_Historical_Manuscripts_Using_Transformer-Based_Models.pdf">Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mississippi State University</span></p><p>本文采用TrOCR这一Transformer架构的手写文本识别模型，针对16世纪拉丁文历史手稿，提出并系统评估了多种针对性图像预处理与数据增强技术（包括四种新设计的历史手稿专用增强方法），并结合集成学习方法以提升识别精度。实验结果表明，Elastic Distortion增强和Top-5投票集成显著提升了字符错误率（CER），新方法在Gwalther数据集上CER降至1.60，相比先前最佳结果提升达50%，有效推动了历史手稿自动识别领域的性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Handwritten_Text_Recognition_of_Historical_Manuscripts_Using_Transformer-Based_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Data-Driven-Deepfake-Image-Detection-Method-The-2024-Global-Deepfake-Image-Detection-Challenge"><a href="#29-Data-Driven-Deepfake-Image-Detection-Method-The-2024-Global-Deepfake-Image-Detection-Challenge" class="headerlink" title="29. Data-Driven Deepfake Image Detection Method - The 2024 Global Deepfake Image Detection Challenge"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Data-Driven_Deepfake_Image_Detection_Method_--_The_2024_Global_Deepfake_Image_Detection_Challenge.pdf">Data-Driven Deepfake Image Detection Method - The 2024 Global Deepfake Image Detection Challenge</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AI Innovation Center, China Unicom</span></p><p>本文提出了一种基于Swin Transformer V2-B分类网络的深度伪造图像检测方法，通过多样的数据增强（如随机人脸区域Cutout、本地裁剪、卡通化等）和样本生成，提升模型泛化能力和鲁棒性。实验结果显示，该方法在多样化的深度伪造攻击下表现出较好的检测准确率和鲁棒性，最终在全球挑战赛中获得优异成绩。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Data-Driven_Deepfake_Image_Detection_Method_-_The_2024_Global_Deepfake_Image_Detection_Challenge.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-RMFAT-Recurrent-Multi-scale-Feature-Atmospheric-Turbulence-Mitigator"><a href="#30-RMFAT-Recurrent-Multi-scale-Feature-Atmospheric-Turbulence-Mitigator" class="headerlink" title="30. RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/RMFAT__Recurrent_Multi-scale_Feature_Atmospheric_Turbulence_Mitigator.pdf">RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Bristol</span></p><p>该论文提出了一种轻量级循环多尺度特征大气湍流缓解模型（RMFAT），通过将多帧视频恢复任务转化为双帧循环推理，结合多尺度特征提取、时序对齐和频率&#x2F;语义指导损失，有效提升了大气湍流视频的空间细节和时序一致性。实验表明，RMFAT在合成和真实数据集上均优于现有方法，恢复质量最佳且推理速度提升4倍以上，适合实时和资源受限场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/RMFAT_Recurrent_Multi-scale_Feature_Atmospheric_Turbulence_Mitigator.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration"><a href="#31-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration" class="headerlink" title="31. G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/G-CUT3R__Guided_3D_Reconstruction_with_Camera_and_Depth_Prior_Integration.pdf">G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Skoltech</span></p><p>该论文提出G-CUT3R，一种基于CUT3R的轻量级、模态无关的前馈式3D场景重建方法，通过整合深度、相机内参和位姿等先验信息，有效提升重建精度。方法采用多模态编码与ZeroConv融合技术，使模型可灵活整合多种数据源，实验表明相比当前主流方法在多项3D重建和相关任务上均实现性能提升，结论是G-CUT3R能生成更准确、细致的3D场景重建结果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/G-CUT3R_Guided_3D_Reconstruction_with_Camera_and_Depth_Prior_Integration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-Leveraging-the-RETFound-foundation-model-for-optic-disc-segmentation-in-retinal-images"><a href="#32-Leveraging-the-RETFound-foundation-model-for-optic-disc-segmentation-in-retinal-images" class="headerlink" title="32. Leveraging the RETFound foundation model for optic disc segmentation in retinal images"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Leveraging_the_RETFound_foundation_model_for_optic_disc_segmentation_in_retinal_images.pdf">Leveraging the RETFound foundation model for optic disc segmentation in retinal images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Dundee</span></p><p>本论文首次将RETFound视觉基础模型（基于ViT-large Transformer和自监督学习）适配并用于视网膜图像中的视盘分割任务，结合Segmenter的解码器，通过冻结基础模型权重，仅用少量带注释样本进行微调，实现了高效特征提取和分割。结果显示，在内部验证、领域泛化和领域适应实验中，模型在多个公开和私有数据集上取得约96% Dice分数，性能优于现有分割专用网络，且无需复杂数据增强，表现稳定且泛化能力强。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Leveraging_the_RETFound_foundation_model_for_optic_disc_segmentation_in_retinal_images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-Noise-Matters-Optimizing-Matching-Noise-for-Diffusion-Classifiers"><a href="#33-Noise-Matters-Optimizing-Matching-Noise-for-Diffusion-Classifiers" class="headerlink" title="33. Noise Matters: Optimizing Matching Noise for Diffusion Classifiers"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Noise_Matters__Optimizing_Matching_Noise_for_Diffusion_Classifiers.pdf">Noise Matters: Optimizing Matching Noise for Diffusion Classifiers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>该论文针对扩散分类器在噪声采样上的不稳定性首次进行系统分析，提出了“频率匹配”和“空间匹配”两大原则，并据此设计了噪声优化方法NoOp：通过优化数据集特定噪声和训练Meta-Network生成图像特定噪声偏置，替换随机噪声以实现稳定高效的分类。实验表明NoOp能提升Few-shot分类性能，兼容其他优化方法，具有较强稳定性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Noise_Matters_Optimizing_Matching_Noise_for_Diffusion_Classifiers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-AIM-Amending-Inherent-Interpretability-via-Self-Supervised-Masking"><a href="#34-AIM-Amending-Inherent-Interpretability-via-Self-Supervised-Masking" class="headerlink" title="34. AIM: Amending Inherent Interpretability via Self-Supervised Masking"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/AIM__Amending_Inherent_Interpretability_via_Self-Supervised_Masking.pdf">AIM: Amending Inherent Interpretability via Self-Supervised Masking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Max-Planck-Institute for Informatics</span></p><p>该论文提出了AIM方法，通过自监督特征掩码机制在卷积网络的多尺度特征融合过程中自动区分和保留与任务相关的“可靠”特征，同时抑制与标签无关的“伪”特征。AIM无需额外注释，仅依赖图像标签，通过可学习的二值掩码在网络前向阶段筛选特征，实现模型决策过程的内在可解释性。实验表明，AIM在ImageNet100、Waterbirds、CUB-200等多种数据集上提升了模型对真实特征的定位能力（EPG指标显著提升），并在分布外泛化和细粒度分类任务中提高了准确率。结论：AIM能够以极低的计算开销实现更高准确率和人类一致性的可解释性，适用于高鲁棒性和泛化性需求的视觉模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Automated-Building-Heritage-Assessment-Using-Street-Level-Imagery"><a href="#35-Automated-Building-Heritage-Assessment-Using-Street-Level-Imagery" class="headerlink" title="35. Automated Building Heritage Assessment Using Street-Level Imagery"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Automated_Building_Heritage_Assessment_Using_Street-Level_Imagery.pdf">Automated Building Heritage Assessment Using Street-Level Imagery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Forschungszentrum Jülich GmbH</span></p><p>本论文提出了一种结合GPT大语言模型与机器学习，对街景建筑立面图像自动提取文化遗产价值特征的方法。通过GPT对建筑图像及辅助信息进行特征提取，再利用XGBoost等模型进行分类验证，最高宏F1分数达0.71。结论是该自动评估流程提升了建筑遗产价值识别的效率和准确性，可为能效改造和城市遗产保护提供支持。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Automated_Building_Heritage_Assessment_Using_Street-Level_Imagery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-CineTrans-Learning-to-Generate-Videos-with-Cinematic-Transitions-via-Masked-Diffusion-Models"><a href="#36-CineTrans-Learning-to-Generate-Videos-with-Cinematic-Transitions-via-Masked-Diffusion-Models" class="headerlink" title="36. CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CineTrans__Learning_to_Generate_Videos_with_Cinematic_Transitions_via_Masked_Diffusion_Models.pdf">CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出了CineTrans框架，通过分析扩散模型中的注意力图，发现镜头内外的相关性差异，并基于此设计了块对角掩码机制，有效控制多镜头视频中的电影化转场。作者还构建了大规模多镜头视频文本数据集Cine250K，并提出专用评价指标。实验表明CineTrans在转场控制、时序一致性和整体质量均显著优于现有方法，推动了可控多镜头视频生成的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CineTrans_Learning_to_Generate_Videos_with_Cinematic_Transitions_via_Masked_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-SPG-Style-Prompting-Guidance-for-Style-Specific-Content-Creation"><a href="#37-SPG-Style-Prompting-Guidance-for-Style-Specific-Content-Creation" class="headerlink" title="37. SPG: Style-Prompting Guidance for Style-Specific Content Creation"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/SPG__Style-Prompting_Guidance_for_Style-Specific_Content_Creation.pdf">SPG: Style-Prompting Guidance for Style-Specific Content Creation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen University</span></p><p>本文提出了一种名为Style-Prompting Guidance (SPG)的训练自由采样策略，用于基于扩散模型的风格化图像生成。SPG通过构建风格噪声向量并结合无条件噪声的方向偏移，引导扩散过程朝向目标风格分布，并与Classifier-Free Guidance (CFG)集成，实现了语义一致性与风格一致性的平衡。实验结果表明，该方法在风格保真度和语义对齐方面优于现有方法，并可无缝集成现有扩散插件如ControlNet和IP-Adapter，具有高效、通用和实用性。结论是SPG能高效实现风格和内容双重一致的图像生成。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SPG_Style-Prompting_Guidance_for_Style-Specific_Content_Creation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-Inside-Knowledge-Graph-based-Path-Generation-with-Explainable-Data-Augmentation-and-Curriculum-Learning-for-Visual-Indoor-Navigation"><a href="#38-Inside-Knowledge-Graph-based-Path-Generation-with-Explainable-Data-Augmentation-and-Curriculum-Learning-for-Visual-Indoor-Navigation" class="headerlink" title="38. Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Inside_Knowledge__Graph-based_Path_Generation_with_Explainable_Data_Augmentation_and_Curriculum_Lear.pdf">Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Science and Technology POLITEHNICA Bucharest</span></p><p>本文提出了一种基于视觉输入的室内导航深度学习方法，通过创新的图结构路径生成、解释性数据增强（如Grad-CAM引导的遮挡）及课程学习，自动高效生成训练路径并提升模型鲁棒性。实验表明，该方法在无需额外传感器或场景地图的情况下，实现了高效、准确的室内导航，并能适应拥挤环境和分布外场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inside_Knowledge_Graph-based_Path_Generation_with_Explainable_Data_Augmentation_and_Curriculum_Learning_for_Visual_Indoor_Navigation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-LKFMixer-Exploring-Large-Kernel-Feature-For-Efficient-Image-Super-Resolution"><a href="#39-LKFMixer-Exploring-Large-Kernel-Feature-For-Efficient-Image-Super-Resolution" class="headerlink" title="39. LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/LKFMixer__Exploring_Large_Kernel_Feature_For_Efficient_Image_Super-Resolution.pdf">LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yanshan University</span></p><p>本文提出了一种高效的轻量级图像超分辨率方法LKFMixer，该方法基于纯卷积神经网络（CNN），采用大卷积核（kernel size 31）配合坐标分解和部分通道卷积，模拟Transformer的非局部特征提取能力，并设计了空间特征调制模块和特征选择模块以适应局部与非局部特征融合。实验表明，LKFMixer在超分辨率性能和重建质量上优于当前主流轻量级方法，并显著提升推理速度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LKFMixer_Exploring_Large_Kernel_Feature_For_Efficient_Image_Super-Resolution.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-AnatoMaskGAN-GNN-Driven-Slice-Feature-Fusion-and-Noise-Augmentation-for-Medical-Semantic-Image-Synthesis"><a href="#40-AnatoMaskGAN-GNN-Driven-Slice-Feature-Fusion-and-Noise-Augmentation-for-Medical-Semantic-Image-Synthesis" class="headerlink" title="40. AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/AnatoMaskGAN__GNN-Driven_Slice_Feature_Fusion_and_Noise_Augmentation_for_Medical_Semantic_Image_Synt.pdf">AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southwest University</span></p><p>本文提出了AnatoMaskGAN，一种用于医学语义掩码图像合成的新型生成框架，结合了基于图神经网络（GNN）的切片特征融合模块，三维空间噪声注入机制，以及灰度-纹理联合分类器。实验结果显示，该方法在L2R-OASIS和L2R-Abdomen CT数据集上，PSNR和SSIM均显著超越现有主流方法，验证了其在结构一致性、纹理多样性和感知质量上的优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AnatoMaskGAN_GNN-Driven_Slice_Feature_Fusion_and_Noise_Augmentation_for_Medical_Semantic_Image_Synthesis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-Index-Aligned-Query-Distillation-for-Transformer-based-Incremental-Object-Detection"><a href="#41-Index-Aligned-Query-Distillation-for-Transformer-based-Incremental-Object-Detection" class="headerlink" title="41. Index-Aligned Query Distillation for Transformer-based Incremental Object Detection"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Index-Aligned_Query_Distillation_for_Transformer-based_Incremental_Object_Detection.pdf">Index-Aligned Query Distillation for Transformer-based Incremental Object Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>本文提出了Index-Aligned Query Distillation (IAQD)方法，用于Transformer架构下的增量式目标检测。IAQD通过对关键查询索引进行一对一知识蒸馏，并结合Proxy Query Selection和标签重对齐策略，大幅缓解了旧类别的灾难性遗忘问题。实验结果表明，该方法在COCO和VOC等基准数据集的多种协议下均优于现有方法，有效提升了旧类别检测能力并兼顾新类别学习。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Index-Aligned_Query_Distillation_for_Transformer-based_Incremental_Object_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Delving-into-Dynamic-Scene-Cue-Consistency-for-Robust-3D-Multi-Object-Tracking"><a href="#42-Delving-into-Dynamic-Scene-Cue-Consistency-for-Robust-3D-Multi-Object-Tracking" class="headerlink" title="42. Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Delving_into_Dynamic_Scene_Cue-Consistency_for_Robust_3D_Multi-Object_Tracking.pdf">Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出了DSC-Track，一种基于Transformer的3D多目标跟踪算法，通过统一的时空聚合模块和旋转不变的点对特征（PPF）编码，聚合每个轨迹的历史和空间上下文。核心创新在于引入线索一致性注意力机制，将轨迹与检测的邻域结构进行匹配，从而增强特征判别性并抑制干扰，实现鲁棒的数据关联。实验表明，DSC-Track在nuScenes和Waymo数据集上达到当前最优性能，有效减少了身份切换，证实了建模高阶关系一致性对复杂场景中多目标跟踪的价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Delving_into_Dynamic_Scene_Cue-Consistency_for_Robust_3D_Multi-Object_Tracking.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-Logic-Unseen-Revealing-the-Logical-Blindspots-of-Vision-Language-Models"><a href="#43-Logic-Unseen-Revealing-the-Logical-Blindspots-of-Vision-Language-Models" class="headerlink" title="43. Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Logic_Unseen__Revealing_the_Logical_Blindspots_of_Vision-Language_Models.pdf">Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-sen University</span></p><p>本文提出了LogicBench基准，系统评估现有视觉-语言模型（VLMs）的逻辑推理能力，并发现现有VLMs在理解逻辑结构（如因果、条件、否定等）方面存在显著盲区。为此，作者提出了LogicCLIP训练框架，通过多LLM生成逻辑扰动负样本和逻辑敏感对比学习目标，大幅提升VLMs的逻辑理解能力，同时在通用任务上保持或提升性能，且对医学、视频等领域具备优良泛化性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Logic_Unseen_Revealing_the_Logical_Blindspots_of_Vision-Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models"><a href="#44-Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models" class="headerlink" title="44. Probing the Representational Power of Sparse Autoencoders in Vision Models"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Probing_the_Representational_Power_of_Sparse_Autoencoders_in_Vision_Models.pdf">Probing the Representational Power of Sparse Autoencoders in Vision Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Intel Labs</span></p><p>本论文系统性地研究了Sparse Autoencoders (SAEs) 在视觉模型中的表征能力，涵盖了视觉编码器、多模态大模型和扩散模型。方法上，作者将SAE应用于DINOv2等视觉基础模型、LLaVA多模态模型和Stable Diffusion扩散模型，通过特征提取、层级语义分析与生成可控性实验，提出了新颖的评估指标。实验显示，SAE提取的特征具有良好的语义解释性，可提升模型在分布外泛化和生成可控性，多模态实验还初步揭示了视觉与语言共享的表示。结论认为，SAE是提升视觉模型可解释性、泛化性和可控性的统一工具，具有广阔应用前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Probing_the_Representational_Power_of_Sparse_Autoencoders_in_Vision_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation"><a href="#45-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation" class="headerlink" title="45. FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/FantasyTalking2__Timestep-Layer_Adaptive_Preference_Optimization_for_Audio-Driven_Portrait_Animation.pdf">FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AMAP, Alibaba Group</span></p><p>本文提出了一种多目标偏好优化框架FantasyTalking2（TLPO）用于音频驱动的肖像动画，创新地将人类偏好分解为运动自然性、唇同步和视觉质量等多个维度，通过多专家LoRA模块分别优化，并在扩散模型的时间步和网络层上自适应融合，实现细粒度无冲突的多目标协同优化。此外，作者构建了Talking-Critic多模态奖励模型和41万对大规模多维偏好数据集Talking-NSQ。实验表明，该方法在运动自然性、唇同步和视觉质量等关键指标上显著优于现有方法，实现了更契合人类偏好的高保真肖像动画生成。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/FantasyTalking2_Timestep-Layer_Adaptive_Preference_Optimization_for_Audio-Driven_Portrait_Animation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-Efficient-Image-to-Image-Schrodinger-Bridge-for-CT-Field-of-View-Extension"><a href="#46-Efficient-Image-to-Image-Schrodinger-Bridge-for-CT-Field-of-View-Extension" class="headerlink" title="46. Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Efficient_Image-to-Image_Schr%C3%B6dinger_Bridge_for_CT_Field_of_View_Extension.pdf">Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Cancer Center, Fudan University</span></p><p>该论文提出了一种基于Image-to-Image Schrödinger Bridge (I2SB)扩散模型的高效CT视野扩展方法。I2SB通过直接学习有限视野与扩展视野CT图像之间的随机映射，显著提升重建速度与图像结构一致性。在模拟和真实数据测试中，I2SB在重建质量及推理效率上均优于主流扩散和深度学习方法，单步推理达0.19秒&#x2F;切片，极适合临床实时应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Efficient_Image-to-Image_Schr%C3%B6dinger_Bridge_for_CT_Field_of_View_Extension.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-UAV-VL-R1-Generalizing-Vision-Language-Models-via-Supervised-Fine-Tuning-and-Multi-Stage-GRPO-for-UAV-Visual-Reasoning"><a href="#47-UAV-VL-R1-Generalizing-Vision-Language-Models-via-Supervised-Fine-Tuning-and-Multi-Stage-GRPO-for-UAV-Visual-Reasoning" class="headerlink" title="47. UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/UAV-VL-R1__Generalizing_Vision-Language_Models_via_Supervised_Fine-Tuning_and_Multi-Stage_GRPO_for_U.pdf">UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>本文提出UAV-VL-R1，一种专为无人机(UAV)高分辨率航拍图像结构化视觉推理任务设计的轻量级视觉语言模型。方法采用基于LoRA的监督微调(SFT)与多阶段群组相对策略优化(GRPO)强化学习结合，利用新建的HRVQA-VL数据集进行逐阶段训练，奖励函数兼顾结构化输出与答案准确性。实验表明，UAV-VL-R1在八类UAV视觉推理任务上的多任务泛化能力显著优于主流大模型（如Qwen2-VL-72B），且仅需3.9GB显存，适于资源受限的UAV平台实时部署。结论：融合SFT和GRPO的训练框架显著提升了模型在UAV航拍图像推理的泛化性、结构化输出能力和推理可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/UAV-VL-R1_Generalizing_Vision-Language_Models_via_Supervised_Fine-Tuning_and_Multi-Stage_GRPO_for_UAV_Visual_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-CHARM3R-Towards-Unseen-Camera-Height-Robust-Monocular-3D-Detector"><a href="#48-CHARM3R-Towards-Unseen-Camera-Height-Robust-Monocular-3D-Detector" class="headerlink" title="48. CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CHARM3R__Towards_Unseen_Camera_Height_Robust_Monocular_3D_Detector.pdf">CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Michigan State University</span></p><p>该论文系统性分析了当前单目3D检测模型在未见相机高度下的表现，发现深度估计是主要影响因素，并数学证明了回归深度与地面深度在相机高度变化下的误差趋势相反。为此，作者提出了CHARM3R方法，将回归深度与地面深度在模型内简单平均，有效抵消两者趋势，大幅提升模型在未见相机高度下的泛化能力，在CARLA数据集上取得了SOTA表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CHARM3R_Towards_Unseen_Camera_Height_Robust_Monocular_3D_Detector.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-HistoViT-Vision-Transformer-for-Accurate-and-Scalable-Histopathological-Cancer-Diagnosis"><a href="#49-HistoViT-Vision-Transformer-for-Accurate-and-Scalable-Histopathological-Cancer-Diagnosis" class="headerlink" title="49. HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/HistoViT__Vision_Transformer_for_Accurate_and_Scalable_Histopathological_Cancer_Diagnosis.pdf">HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Embry-Riddle Aeronautical University</span></p><p>本论文提出了一种基于Vision Transformer (ViT) 的深度学习框架，结合轻量级预处理流水线，对乳腺、前列腺、骨和宫颈等多种肿瘤的病理图像进行多分类任务。方法流程包括图像归一化、区域切片、PyTorch张量转换及ViT微调，模型在四个公开数据集上取得了超越当前深度学习方法的表现（所有任务准确率均超95%，AUC超99%），展示了优异的泛化能力和临床应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/HistoViT_Vision_Transformer_for_Accurate_and_Scalable_Histopathological_Cancer_Diagnosis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-VFM-Guided-Semi-Supervised-Detection-Transformer-for-Source-Free-Object-Detection-in-Remote-Sensing-Images"><a href="#50-VFM-Guided-Semi-Supervised-Detection-Transformer-for-Source-Free-Object-Detection-in-Remote-Sensing-Images" class="headerlink" title="50. VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/VFM-Guided_Semi-Supervised_Detection_Transformer_for_Source-Free_Object_Detection_in_Remote_Sensing_.pdf">VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>该论文提出了VG-DETR，一种融合视觉基础模型（VFM）指导的半监督检测Transformer，用于遥感图像的源自由目标检测。方法创新性地利用VFM的语义先验，通过VFM引导的伪标签挖掘和双层对齐（实例级和图像级），提升伪标签的可靠性和检测器的特征泛化能力，有效缓解因伪标签噪声导致的训练崩溃。实验结果表明，在多种遥感跨域场景下，VG-DETR以极低标注成本实现了显著优于现有方法的检测性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/VFM-Guided_Semi-Supervised_Detection_Transformer_for_Source-Free_Object_Detection_in_Remote_Sensing_Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-LEARN-A-Story-Driven-Layout-to-Image-Generation-Framework-for-STEM-Instruction"><a href="#51-LEARN-A-Story-Driven-Layout-to-Image-Generation-Framework-for-STEM-Instruction" class="headerlink" title="51. LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/LEARN__A_Story-Driven_Layout-to-Image_Generation_Framework_for_STEM_Instruction.pdf">LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hiroshima University</span></p><p>LEARN提出了一种基于布局感知扩散模型的框架，用于生成与教学目标一致的STEM教育插图。其方法包括利用BookCover数据集进行布局语义学习、Caption2LayoutNet生成结构化布局、布局条件扩散生成图像，并通过CLIP对比损失实现语义一致性。实验和用户调研表明，该方法能有效提升插图的结构准确性和解释性，降低学生认知负担，促进课堂参与和概念理解。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LEARN_A_Story-Driven_Layout-to-Image_Generation_Framework_for_STEM_Instruction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-A-Cross-Modal-Rumor-Detection-Scheme-via-Contrastive-Learning-by-Exploring-Text–Image-internal-Correlations"><a href="#52-A-Cross-Modal-Rumor-Detection-Scheme-via-Contrastive-Learning-by-Exploring-Text–Image-internal-Correlations" class="headerlink" title="52. A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text–Image internal Correlations"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/A_Cross-Modal_Rumor_Detection_Scheme_via_Contrastive_Learning_by_Exploring_Text_and_Image_internal_C.pdf">A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text–Image internal Correlations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing University of Posts and Telecommunications</span></p><p>本论文提出了一种基于对比学习的跨模态谣言检测方法（MICC），结合多尺度卷积视觉特征提取（SCLIP编码器）、跨模态多尺度对齐模块和尺度感知融合网络，实现对文本与图像内部相关性的高效建模。实验结果表明，该方法在Weibo和PHEME两个真实谣言检测数据集上显著优于现有方法，提升了跨模态谣言检测的准确性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A_Cross-Modal_Rumor_Detection_Scheme_via_Contrastive_Learning_by_Exploring_Text%E2%80%93Image_internal_Correlations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-Denoise-then-Retrieve-Text-Conditioned-Video-Denoising-for-Video-Moment-Retrieval"><a href="#53-Denoise-then-Retrieve-Text-Conditioned-Video-Denoising-for-Video-Moment-Retrieval" class="headerlink" title="53. Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Denoise-then-Retrieve__Text-Conditioned_Video_Denoising_for_Video_Moment_Retrieval.pdf">Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southeast University</span></p><p>该论文提出了Denoise-then-Retrieve Network (DRNet)，通过文本条件化去噪模块（TCD）和文本重建反馈（TRF）模块，过滤掉与文本无关的视频片段，利用多模态交互和噪声掩码生成纯净的视频表示进行视频片段检索。实验结果表明，在Charades-STA和QVHighlights数据集上，方法在所有指标上超过现有主流方法，且去噪范式可无缝集成到其他视频检索模型中提升性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Denoise-then-Retrieve_Text-Conditioned_Video_Denoising_for_Video_Moment_Retrieval.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-Vision-Language-Models-display-a-strong-gender-bias"><a href="#54-Vision-Language-Models-display-a-strong-gender-bias" class="headerlink" title="54. Vision-Language Models display a strong gender bias"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Vision-Language_Models_display_a_strong_gender_bias.pdf">Vision-Language Models display a strong gender bias</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Togo AI Labs</span></p><p>本文提出了一种透明可复现的方法，利用CLIP风格的对比视觉-语言模型（VLM）和余弦相似度，量化模型在面部图像与职业&#x2F;活动短语之间的性别关联，并通过自助法估计不确定性及标签置换检验显著性。实验结果表明，不同架构的主流VLM在多种职业和活动类别上均显示出显著的性别偏见，Transformer类模型偏见略强于ResNet类，提示预训练数据和模型结构影响偏见方向和程度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Vision-Language_Models_display_a_strong_gender_bias.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-Generalized-Decoupled-Learning-for-Enhancing-Open-Vocabulary-Dense-Perception"><a href="#55-Generalized-Decoupled-Learning-for-Enhancing-Open-Vocabulary-Dense-Perception" class="headerlink" title="55. Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Generalized_Decoupled_Learning_for_Enhancing_Open-Vocabulary_Dense_Perception.pdf">Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology (Shenzhen)</span></p><p>该论文提出了DeCLIP，一种创新的无监督微调框架，通过将CLIP的自注意力模块解耦为“内容”和“上下文”特征，并分别采用自蒸馏、VFM语义引导与扩散模型边界补全进行优化，增强了CLIP在开放词汇密集感知任务中的局部判别能力和空间一致性。实验结果表明，DeCLIP在2D&#x2F;3D检测与分割、视频实例分割、6D姿态估计等多项开放词汇密集视觉任务中均取得了显著提升，证明其作为通用密集感知基础模型的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Generalized_Decoupled_Learning_for_Enhancing_Open-Vocabulary_Dense_Perception.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-A-CLIP-based-Uncertainty-Modal-Modeling-UMM-Framework-for-Pedestrian-Re-Identification-in-Autonomous-Driving"><a href="#56-A-CLIP-based-Uncertainty-Modal-Modeling-UMM-Framework-for-Pedestrian-Re-Identification-in-Autonomous-Driving" class="headerlink" title="56. A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/A_CLIP-based_Uncertainty_Modal_Modeling_(UMM)_Framework_for_Pedestrian_Re-Identification_in_Autonomo.pdf">A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Jiangsu Zhenjiang Technician College</span></p><p>本文提出了基于CLIP的轻量级不确定性模态建模（UMM）框架，用于自动驾驶中的行人再识别，融合多模态token映射器、合成模态增强和跨模态线索交互模块，实现RGB、红外、素描和文本描述的统一特征表示与高效融合。实验表明，该方法在模态缺失和跨模态检索任务中表现出更强的鲁棒性和泛化能力，且计算效率高，适用于资源有限的自动驾驶平台。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A_CLIP-based_Uncertainty_Modal_Modeling_%28UMM%29_Framework_for_Pedestrian_Re-Identification_in_Autonomous_Driving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation"><a href="#57-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation" class="headerlink" title="57. StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/StyleMM__Stylized_3D_Morphable_Face_Model_via_Text-Driven_Aligned_Image_Translation.pdf">StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KAIST</span></p><p>该论文提出StyleMM，一种通过文本驱动的对齐图像翻译，实现风格化3D可变形人脸模型的方法。核心流程包括利用扩散模型生成风格化人脸、显式属性保持模块（EAM）确保表情与结构一致，并通过三阶段训练（几何预热、形状与纹理联合微调、纹理精细化）优化变形网络和纹理生成器。实验结果显示，StyleMM在保持人脸多样性、风格一致性及可动画性方面优于现有方法，能广泛应用于影视、动画和游戏。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/StyleMM_Stylized_3D_Morphable_Face_Model_via_Text-Driven_Aligned_Image_Translation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-E-CaTCH-Event-Centric-Cross-Modal-Attention-with-Temporal-Consistency-and-Class-Imbalance-Handling-for-Misinformation-Detection"><a href="#58-E-CaTCH-Event-Centric-Cross-Modal-Attention-with-Temporal-Consistency-and-Class-Imbalance-Handling-for-Misinformation-Detection" class="headerlink" title="58. E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/E-CaTCH__Event-Centric_Cross-Modal_Attention_with_Temporal_Consistency_and_Class-Imbalance_Handling_.pdf">E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">American University</span></p><p>该论文提出E-CaTCH框架，通过事件聚类、BERT与ResNet特征提取、层次化跨模态自注意力和软门控融合，结合重叠时间窗口及LSTM建模信息传播的动态趋势，并采用自适应类别加权和时序一致性正则，解决多模态社交媒体虚假信息检测中的模态不一致、时间变化和类别不平衡等核心挑战。实验显示E-CaTCH在Fakeddit、印度选举和COVID-19数据集上表现优于现有多模态检测模型，具备强泛化性与高计算效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/E-CaTCH_Event-Centric_Cross-Modal_Attention_with_Temporal_Consistency_and_Class-Imbalance_Handling_for_Misinformation_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-Versatile-Video-Tokenization-with-Generative-2D-Gaussian-Splatting"><a href="#59-Versatile-Video-Tokenization-with-Generative-2D-Gaussian-Splatting" class="headerlink" title="59. Versatile Video Tokenization with Generative 2D Gaussian Splatting"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Versatile_Video_Tokenization_with_Generative_2D_Gaussian_Splatting.pdf">Versatile Video Tokenization with Generative 2D Gaussian Splatting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Newcastle</span></p><p>本文提出Gaussian Video Transformer（GVT），通过生成式2D高斯泼洒（2DGS）策略进行视频分词，核心包含时空高斯嵌入（STGE）和高斯集合划分（GSP）机制。GVT能够自适应地为不同信息量的区域分配高斯分布，利用静态和动态高斯划分减少时域冗余，实现高效、紧凑的视频表示。实验表明，GVT在UCF101、Kinetics和DAVIS等数据集上，重建质量达到SOTA，并在动作识别和视频压缩任务上表现优异，证明其高效且可扩展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Versatile_Video_Tokenization_with_Generative_2D_Gaussian_Splatting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-Exploring-the-Tradeoff-Between-Diversity-and-Discrimination-for-Continuous-Category-Discovery"><a href="#60-Exploring-the-Tradeoff-Between-Diversity-and-Discrimination-for-Continuous-Category-Discovery" class="headerlink" title="60. Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Exploring_the_Tradeoff_Between_Diversity_and_Discrimination_for_Continuous_Category_Discovery.pdf">Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ocean University of China</span></p><p>本文提出了IDOD方法，针对连续类别发现（CCD）任务，包含独立多样性增强模块、联合新颖性发现模块和正交判别增量模块。通过独立训练骨干网络和投影头，结合正交原型与联合发现机制，方法在提升新颖类别发现与已知类别分类之间取得更优平衡，减少伪标签误差累积且显著降低存储开销，实验验证在多个细粒度视觉数据集上优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Exploring_the_Tradeoff_Between_Diversity_and_Discrimination_for_Continuous_Category_Discovery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-Semi-supervised-Image-Dehazing-via-Expectation-Maximization-and-Bidirectional-Brownian-Bridge-Diffusion-Models"><a href="#61-Semi-supervised-Image-Dehazing-via-Expectation-Maximization-and-Bidirectional-Brownian-Bridge-Diffusion-Models" class="headerlink" title="61. Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Semi-supervised_Image_Dehazing_via_Expectation-Maximization_and_Bidirectional_Brownian_Bridge_Diffus.pdf">Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">China University of Mining and Technology</span></p><p>本论文提出了一种结合期望最大化（EM）算法与双向布朗桥扩散模型（B3DM）的半监督图像去雾方法EM-B3DM。方法分为两阶段：首先利用EM算法将有雾与无雾图像对的联合分布解耦为条件分布，并用统一的布朗桥扩散模型建模结构与内容相关性；随后利用预训练模型和大量无配对数据进行半监督训练，并引入残差差分卷积块（RDC）以增强梯度特征表达。实验结果表明，该方法在合成与真实数据集上均优于或可比现有最优方法，显著降低对成对数据的依赖，提升了去雾效果和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Semi-supervised_Image_Dehazing_via_Expectation-Maximization_and_Bidirectional_Brownian_Bridge_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-Residual-based-Efficient-Bidirectional-Diffusion-Model-for-Image-Dehazing-and-Haze-Generation"><a href="#62-Residual-based-Efficient-Bidirectional-Diffusion-Model-for-Image-Dehazing-and-Haze-Generation" class="headerlink" title="62. Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Residual-based_Efficient_Bidirectional_Diffusion_Model_for_Image_Dehazing_and_Haze_Generation.pdf">Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">China University of Mining and Technology</span></p><p>本文提出了一种残差驱动的高效双向扩散模型（RBDM），可在单一模型内实现图像去雾与生成雾的条件分布建模。方法核心包括通过残差构建双马尔可夫链与基于图像patch的统一评分函数，支持任意尺寸的高效双向转换。实验结果表明，RBDM在多个合成及真实数据集上去雾和生成雾效果优于或至少等同于现有主流方法，并能自然保留图像细节和色彩。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Residual-based_Efficient_Bidirectional_Diffusion_Model_for_Image_Dehazing_and_Haze_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="63-Is-ChatGPT-5-Ready-for-Mammogram-VQA"><a href="#63-Is-ChatGPT-5-Ready-for-Mammogram-VQA" class="headerlink" title="63. Is ChatGPT-5 Ready for Mammogram VQA?"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Is_ChatGPT-5_Ready_for_Mammogram_VQA_.pdf">Is ChatGPT-5 Ready for Mammogram VQA?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Emory University School of Medicine</span></p><p>本文系统评估了ChatGPT-5及GPT-4o在四个公开乳腺癌筛查数据集上的视觉问答（VQA）任务，包括BI-RADS评估、异常检测及恶性分类，采用零样本链式推理多模态提示。结果显示，ChatGPT-5在各任务上优于小模型和前代GPT-4o，但其准确性和敏感性仍明显低于领域专家及医学专用AI模型，尚不能用于高风险临床场景。结论是：ChatGPT-5虽有显著进步，但需针对医学影像领域进行深度定向优化后才能满足临床需求。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Is_ChatGPT-5_Ready_for_Mammogram_VQA.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding"><a href="#64-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding" class="headerlink" title="64. Controlling Multimodal LLMs via Reward-guided Decoding"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Controlling_Multimodal_LLMs_via_Reward-guided_Decoding.pdf">Controlling Multimodal LLMs via Reward-guided Decoding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mila - Quebec AI Institute</span></p><p>本文提出了一种针对多模态大语言模型（MLLM）的推理期自适应控制方法——多模态奖励引导解码（MRGD），通过构建视觉锚定相关的奖励模型（分别评估目标精度和召回率），并将其线性组合动态指导生成过程，实现对模型输出目标精度与召回、以及推理计算资源消耗的精细可控。实验结果表明，该方法在多个对象幻觉基准上显著优于现有方法，实现了更高的可控性和更低的幻觉率，同时兼容不同架构的MLLM，无需重新训练。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Controlling_Multimodal_LLMs_via_Reward-guided_Decoding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-CRYPTOSCOPE-UTILIZING-LARGE-LANGUAGE-MODELS-FOR-AUTOMATED-CRYPTOGRAPHIC-LOGIC-VULNERABILITY-DETECTION"><a href="#65-CRYPTOSCOPE-UTILIZING-LARGE-LANGUAGE-MODELS-FOR-AUTOMATED-CRYPTOGRAPHIC-LOGIC-VULNERABILITY-DETECTION" class="headerlink" title="65. CRYPTOSCOPE: UTILIZING LARGE LANGUAGE MODELS FOR AUTOMATED CRYPTOGRAPHIC LOGIC VULNERABILITY DETECTION"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CryptoScope__Utilizing_Large_Language_Models_for_Automated_Cryptographic_Logic_Vulnerability_Detecti.pdf">CRYPTOSCOPE: UTILIZING LARGE LANGUAGE MODELS FOR AUTOMATED CRYPTOGRAPHIC LOGIC VULNERABILITY DETECTION</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sichuan University</span></p><p>该论文提出了CRYPTOSCOPE框架，结合了链式思维（CoT）提示和检索增强生成（RAG），依托超过12000项的加密知识库，实现多语言、无需执行代码的自动化加密逻辑漏洞检测。实验表明，CRYPTOSCOPE在LLM-CLVA基准上显著提升了多种大模型的漏洞检测性能，并成功发现了9个真实开源项目中未公开的漏洞，验证了其实用性和通用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CRYPTOSCOPE_UTILIZING_LARGE_LANGUAGE_MODELS_FOR_AUTOMATED_CRYPTOGRAPHIC_LOGIC_VULNERABILITY_DETECTION.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-Aware-First-Think-Less-Dynamic-Boundary-Self-Awareness-Drives-Extreme-Reasoning-Efficiency-in-Large-Language-Models"><a href="#66-Aware-First-Think-Less-Dynamic-Boundary-Self-Awareness-Drives-Extreme-Reasoning-Efficiency-in-Large-Language-Models" class="headerlink" title="66. Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Aware_First,_Think_Less__Dynamic_Boundary_Self-Awareness_Drives_Extreme_Reasoning_Efficiency_in_Larg.pdf">Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology</span></p><p>本文提出了动态推理边界自觉框架（DR. SAF），通过模型自我识别推理能力边界，动态调整推理深度以匹配每道题的复杂度。该方法包含边界自觉对齐、适应性长度管理和边界保持机制，显著提高了LLM推理效率，实现了平均响应长度减少49.27%、训练时间缩短5倍，并在极端压缩下精度提升超过16%。实验结果显示该框架在六项数学推理基准上均保持高准确率和极致token效率，适合资源受限场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Aware_First_Think_Less_Dynamic_Boundary_Self-Awareness_Drives_Extreme_Reasoning_Efficiency_in_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-Causality-Matters-How-Temporal-Information-Emerges-in-Video-Language-Models"><a href="#67-Causality-Matters-How-Temporal-Information-Emerges-in-Video-Language-Models" class="headerlink" title="67. Causality Matters: How Temporal Information Emerges in Video Language Models"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Causality_Matters__How_Temporal_Information_Emerges_in_Video_Language_Models.pdf">Causality Matters: How Temporal Information Emerges in Video Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>该论文系统分析了Video Language Models（VideoLMs）中的时序理解机制，发现时序信息主要通过因果注意力机制自发涌现，而非传统认为的时序位置编码（PEs）。作者提出并实验证明，时序理解依赖于跨帧因果信息流动，并据此设计了两种高效推理策略，在主流基准上验证了其有效性。结论是，未来VideoLMs时序建模应聚焦于因果注意力机制与信息流路径优化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Causality_Matters_How_Temporal_Information_Emerges_in_Video_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-ADMIRE-BayesOpt-Accelerated-Data-MIxture-RE-weighting-for-Language-Models-with-Bayesian-Optimization"><a href="#68-ADMIRE-BayesOpt-Accelerated-Data-MIxture-RE-weighting-for-Language-Models-with-Bayesian-Optimization" class="headerlink" title="68. ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ADMIRE-BayesOpt__Accelerated_Data_MIxture_RE-weighting_for_Language_Models_with_Bayesian_Optimizatio.pdf">ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Thomson Reuters Foundational Research</span></p><p>本文提出了ADMIRE-BayesOpt方法，将大型语言模型训练中的数据混合权重选择问题建模为黑盒超参数优化问题，并采用贝叶斯优化（包括多保真优化）以高效搜索最优数据混合配置。通过在多种模型规模和数据集上的实验，ADMIRE-BayesOpt在收敛速度和最终性能上均显著优于现有的回归或代理模型方法，且极大降低了计算成本，提升了数据混合方案在不同模型规模间的可迁移性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ADMIRE-BayesOpt_Accelerated_Data_MIxture_RE-weighting_for_Language_Models_with_Bayesian_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-Inclusion-Arena-An-Open-Platform-for-Evaluating-Large-Foundation-Models-with-Real-World-Apps"><a href="#69-Inclusion-Arena-An-Open-Platform-for-Evaluating-Large-Foundation-Models-with-Real-World-Apps" class="headerlink" title="69. Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Inclusion_Arena__An_Open_Platform_for_Evaluating_Large_Foundation_Models_with_Real-World_Apps.pdf">Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Inclusion AI</span></p><p>该论文提出了Inclusion Arena平台，通过集成真实AI应用中的用户交互，收集并利用人类偏好数据，用于大模型（LLMs和MLLMs）评测与排名。方法上，平台引入了基于Bradley-Terry模型的最大似然估计排名、Placement Matches冷启动机制和Proximity Sampling智能采样算法，聚焦于相近能力模型间的对比以提升评测稳定性和抗攻击性。实证分析表明，该平台能够高效、稳健地反映模型在真实场景下的能力差异，同时显著减少数据操控风险，有助于推动大模型向用户体验和实用性优化发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inclusion_Arena_An_Open_Platform_for_Evaluating_Large_Foundation_Models_with_Real-World_Apps.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-MM-R1-Unleashing-the-Power-of-Unified-Multimodal-Large-Language-Models-for-Personalized-Image-Generation"><a href="#70-MM-R1-Unleashing-the-Power-of-Unified-Multimodal-Large-Language-Models-for-Personalized-Image-Generation" class="headerlink" title="70. MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/MM-R1__Unleashing_the_Power_of_Unified_Multimodal_Large_Language_Models_for_Personalized_Image_Gener.pdf">MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>本论文提出MM-R1框架，通过跨模态Chain-of-Thought (X-CoT)推理和Grouped Reward Proximal Policy Optimization (GRPO)强化学习，提升统一多模态大语言模型(MLLM)在个性化图像生成中的能力。方法包括：先对用户输入图片和文本进行理解和视觉概念分离，再结合用户提示进行图像生成，并设计多重奖励机制进行优化。实验结果表明，MM-R1在零样本个性化图像生成中显著提升了主题保真度和文本一致性，优于现有统一MLLM方法且无需针对每个主体单独微调，具备更强泛化和可控性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MM-R1_Unleashing_the_Power_of_Unified_Multimodal_Large_Language_Models_for_Personalized_Image_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-Trustworthy-AI-Psychotherapy-Multi-Agent-LLM-Workflow-for-Counseling-and-Explainable-Mental-Disorder-Diagnosis"><a href="#71-Trustworthy-AI-Psychotherapy-Multi-Agent-LLM-Workflow-for-Counseling-and-Explainable-Mental-Disorder-Diagnosis" class="headerlink" title="71. Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Trustworthy_AI_Psychotherapy__Multi-Agent_LLM_Workflow_for_Counseling_and_Explainable_Mental_Disorde.pdf">Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Vrije Universiteit Amsterdam</span></p><p>本文提出DSM5AgentFlow，一种多智能体大语言模型（LLM）工作流，可自动生成并进行DSM-5精神障碍问卷，通过三类LLM代理（治疗师、患者、诊断师）模拟真实对话，实现逐步、可溯源的诊断与解释。实验表明，该框架能生成高覆盖、连贯对话，诊断准确率领先，显著提升了精神健康AI系统的透明度和可信度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Trustworthy_AI_Psychotherapy_Multi-Agent_LLM_Workflow_for_Counseling_and_Explainable_Mental_Disorder_Diagnosis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-Rationalizing-Transformer-Predictions-via-End-To-End-Differentiable-Self-Training"><a href="#72-Rationalizing-Transformer-Predictions-via-End-To-End-Differentiable-Self-Training" class="headerlink" title="72. Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Rationalizing_Transformer_Predictions_via_End-To-End_Differentiable_Self-Training.pdf">Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bielefeld University</span></p><p>本文提出了一种端到端可微分的自训练范式，用于训练能够生成忠实解释（rationales）和做出分类决策的Transformer模型。该方法通过单一模型同时完成分类、输入token相关性评分，并采用自训练机制优化class-wise rationale掩码，显著提升了与人类注释的一致性与解释的忠实度。实验结果表明，该方法在两个NLP基准数据集上实现了与人类rationale高度一致且忠实性优异的解释，同时保持甚至略超标准分类器的分类性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Rationalizing_Transformer_Predictions_via_End-To-End_Differentiable_Self-Training.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-Inspire-or-Predict-Exploring-New-Paradigms-in-Assisting-Classical-Planners-with-Large-Language-Models"><a href="#73-Inspire-or-Predict-Exploring-New-Paradigms-in-Assisting-Classical-Planners-with-Large-Language-Models" class="headerlink" title="73. Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Inspire_or_Predict__Exploring_New_Paradigms_in_Assisting_Classical_Planners_with_Large_Language_Mode.pdf">Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Guizhou University</span></p><p>本文提出了一种结合问题分解与大语言模型（LLM）辅助的规划方法，将大型规划问题通过有向无环图分解为子任务，并探索两种LLM集成范式：LLM4Inspire（通用知识启发）和LLM4Predict（领域知识推理中间状态）。实验证明，LLM4Predict在缩减搜索空间和提升规划成功率上优于仅依赖通用知识的LLM4Inspire，验证了领域知识在复杂规划中的不可替代性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inspire_or_Predict_Exploring_New_Paradigms_in_Assisting_Classical_Planners_with_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-Reference-Points-in-LLM-Sentiment-Analysis-The-Role-of-Structured-Context"><a href="#74-Reference-Points-in-LLM-Sentiment-Analysis-The-Role-of-Structured-Context" class="headerlink" title="74. Reference Points in LLM Sentiment Analysis: The Role of Structured Context"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Reference_Points_in_LLM_Sentiment_Analysis__The_Role_of_Structured_Context.pdf">Reference Points in LLM Sentiment Analysis: The Role of Structured Context</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Meijo University</span></p><p>本论文提出在LLM情感分析中引入用户和商家历史评分等结构化参考点（如JSON格式）以补充文本上下文，实验使用轻量级Llama-3.2-3B模型。结果表明，结构化（尤其JSON格式）补充信息可显著提升模型在Yelp餐厅和夜生活场景下的准确率和回归性能，无需微调即可在资源受限设备上部署；进一步分析证实性能提升源于真实的上下文推理，而非标签代理效应。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Reference_Points_in_LLM_Sentiment_Analysis_The_Role_of_Structured_Context.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-AIM-Bench-Evaluating-Decision-making-Biases-of-Agentic-LLM-as-Inventory-Manager"><a href="#75-AIM-Bench-Evaluating-Decision-making-Biases-of-Agentic-LLM-as-Inventory-Manager" class="headerlink" title="75. AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/AIM-Bench__Evaluating_Decision-making_Biases_of_Agentic_LLM_as_Inventory_Manager.pdf">AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本论文提出AIM-Bench基准，用于系统评估大语言模型（LLM）在不确定库存管理环境下的决策能力和偏差，包括新建和改编的五种供应链环境，并量化人类常见的决策偏差如均值锚定和牛鞭效应。研究发现，不同LLM普遍表现出类似人类的决策偏差，但通过认知反思和信息共享等策略可有效缓解部分偏差，提示在实际部署LLM时需重视偏差识别与消除。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AIM-Bench_Evaluating_Decision-making_Biases_of_Agentic_LLM_as_Inventory_Manager.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-ON-POLICY-RL-MEETS-OFF-POLICY-EXPERTS-HARMONIZING-SUPERVISED-FINE-TUNING-AND-REINFORCEMENT-LEARNING-VIA-DYNAMIC-WEIGHTING"><a href="#76-ON-POLICY-RL-MEETS-OFF-POLICY-EXPERTS-HARMONIZING-SUPERVISED-FINE-TUNING-AND-REINFORCEMENT-LEARNING-VIA-DYNAMIC-WEIGHTING" class="headerlink" title="76. ON-POLICY RL MEETS OFF-POLICY EXPERTS: HARMONIZING SUPERVISED FINE-TUNING AND REINFORCEMENT LEARNING VIA DYNAMIC WEIGHTING"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/On-Policy_RL_Meets_Off-Policy_Experts__Harmonizing_Supervised_Fine-Tuning_and_Reinforcement_Learning.pdf">ON-POLICY RL MEETS OFF-POLICY EXPERTS: HARMONIZING SUPERVISED FINE-TUNING AND REINFORCEMENT LEARNING VIA DYNAMIC WEIGHTING</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Alibaba Group</span></p><p>本文提出CHORD框架，将监督微调（SFT）和强化学习（RL）以动态加权的方式统一于大语言模型（LLM）后训练，通过全局系数和逐token权重函数，灵活控制离策略专家数据对模型的影响。实验表明，CHORD在平衡专家知识吸收与模型自主探索能力方面优于传统SFT-then-RL范式，提升推理与泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ON-POLICY_RL_MEETS_OFF-POLICY_EXPERTS_HARMONIZING_SUPERVISED_FINE-TUNING_AND_REINFORCEMENT_LEARNING_VIA_DYNAMIC_WEIGHTING.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs"><a href="#77-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs" class="headerlink" title="77. When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/When_Punctuation_Matters__A_Large-Scale_Comparison_of_Prompt_Robustness_Methods_for_LLMs.pdf">When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AIRI</span></p><p>本论文提出了首个系统性评估大型语言模型（LLMs）在不同提示（prompt）格式下鲁棒性的方法，对5种主流鲁棒性提升技术（如Batch Calibration、Template Ensembles等）在Llama、Qwen、Gemma等8个模型和52项任务上进行了统一实验框架下的大规模对比。研究发现，Batch Calibration在无分布偏移时显著提升鲁棒性但对类别失衡敏感，LoRA微调虽提升准确率但对格式变化鲁棒性有限，而规模更大的前沿模型表现出更强鲁棒性，但仍存在部分任务下格式敏感问题，基于投票的Template Ensembles可进一步缓解这一问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/When_Punctuation_Matters_A_Large-Scale_Comparison_of_Prompt_Robustness_Methods_for_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-ETTRL-Balancing-Exploration-and-Exploitation-in-LLM-Test-Time-Reinforcement-Learning-via-Entropy-Mechanism"><a href="#78-ETTRL-Balancing-Exploration-and-Exploitation-in-LLM-Test-Time-Reinforcement-Learning-via-Entropy-Mechanism" class="headerlink" title="78. ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning via Entropy Mechanism"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ETTRL__Balancing_Exploration_and_Exploitation_in_LLM_Test-Time_Reinforcement_Learning_Via_Entropy_Me.pdf">ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning via Entropy Mechanism</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Technology</span></p><p>本文提出了一种基于熵机制的测试时强化学习框架ETTRL，核心包括熵分叉树多数采样（ETMR）与基于熵的优势重塑（EAR）两大策略，有效提升LLM在无监督推理任务中的探索-利用平衡。实验结果表明，该方法在AIME 2024等数学推理基准上显著提升了Pass@1准确率，同时大幅降低了推理所需的token预算，优化了模型无监督自适应能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ETTRL_Balancing_Exploration_and_Exploitation_in_LLM_Test-Time_Reinforcement_Learning_via_Entropy_Mechanism.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-SGSimEval-A-Comprehensive-Multifaceted-and-Similarity-Enhanced-Benchmark-for-Automatic-Survey-Generation-Systems"><a href="#79-SGSimEval-A-Comprehensive-Multifaceted-and-Similarity-Enhanced-Benchmark-for-Automatic-Survey-Generation-Systems" class="headerlink" title="79. SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/SGSimEval__A_Comprehensive_Multifaceted_and_Similarity-Enhanced_Benchmark_for_Automatic_Survey_Gener.pdf">SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong Polytechnic University</span></p><p>本文提出了SGSimEval，一个用于自动化综述生成系统的多维综合评测基准，集成了结构、内容和参考文献三方面的评价，并将LLM打分与定量指标结合，创新性引入了语义相似度增强及人类偏好对齐的评测框架。实验结果表明，领域特定（如计算机科学）系统在结构和内容生成上能超越人类，但在参考文献质量方面仍有较大提升空间，SGSimEval的评测结果与人类判断高度一致。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SGSimEval_A_Comprehensive_Multifaceted_and_Similarity-Enhanced_Benchmark_for_Automatic_Survey_Generation_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-CSGO-Generalized-Optimization-for-Cold-Start-in-Wireless-Collaborative-Edge-LLM-Systems"><a href="#80-CSGO-Generalized-Optimization-for-Cold-Start-in-Wireless-Collaborative-Edge-LLM-Systems" class="headerlink" title="80. CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CSGO__Generalized_Optimization_for_Cold_Start_in_Wireless_Collaborative_Edge_LLM_Systems.pdf">CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文针对在无线异构边缘设备上部署大语言模型（LLM）面临的推理冷启动延迟问题，提出了一种基于动态规划的分层分配算法，通过重叠模型加载与计算通信过程，动态优化模型分片和设备分配以最小化总体延迟。实验结果表明，所提方法显著降低了端到端冷启动延迟，相较于多种基线策略平均提升17.43%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CSGO_Generalized_Optimization_for_Cold_Start_in_Wireless_Collaborative_Edge_LLM_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-Hallucination-in-LLM-Based-Code-Generation-An-Automotive-Case-Study"><a href="#81-Hallucination-in-LLM-Based-Code-Generation-An-Automotive-Case-Study" class="headerlink" title="81. Hallucination in LLM-Based Code Generation: An Automotive Case Study"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Hallucination_in_LLM-Based_Code_Generation__An_Automotive_Case_Study.pdf">Hallucination in LLM-Based Code Generation: An Automotive Case Study</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Munich</span></p><p>本文针对汽车领域的代码生成任务，系统评估了多种主流代码生成型大语言模型（如GPT-4.1、Codex、GPT-4o）在不同提示复杂度下的幻觉现象，并采用迭代反馈修复策略提升代码可靠性。结果表明，在基线和信号增强提示下，幻觉现象频发且难以完全消除，仅通过模板增强提示与迭代反馈组合，部分模型可生成正确代码，凸显幻觉问题需针对性缓解方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Hallucination_in_LLM-Based_Code_Generation_An_Automotive_Case_Study.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-ORFuzz-Fuzzing-the-‘Other-Side’-of-LLM-Safety-–-Testing-Over-Refusal"><a href="#82-ORFuzz-Fuzzing-the-‘Other-Side’-of-LLM-Safety-–-Testing-Over-Refusal" class="headerlink" title="82. ORFuzz: Fuzzing the ‘Other Side’ of LLM Safety – Testing Over-Refusal"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ORFuzz__Fuzzing_the__Other_Side__of_LLM_Safety_--_Testing_Over-Refusal.pdf">ORFuzz: Fuzzing the ‘Other Side’ of LLM Safety – Testing Over-Refusal</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了ORFUZZ，这是首个针对大语言模型（LLM）过度拒答（over-refusal）现象的进化式自动化检测框架。ORFUZZ集成了安全类别感知的种子选择、基于LLM推理的自适应变异器优化及人类对齐的评测模型OR-JUDGE，能够自动生成多样且高质量的测试用例，有效挖掘和分析LLM的过度拒答漏洞。实验证明，ORFUZZ在生成有效过度拒答用例上的触发率是现有方法的两倍以上，并构建了覆盖八大安全类别、在10个主流LLM均表现出高触发率（平均63.56%）的ORFUZZSET基准数据集。结论：ORFUZZ和ORFUZZSET为LLM安全测试和基准评测提供了高效、可靠、自动化的解决方案，显著提升了LLM的安全性与可用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ORFuzz_Fuzzing_the_%27Other_Side%27_of_LLM_Safety_%E2%80%93_Testing_Over-Refusal.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-MONACO-More-Natural-and-Complex-Questions-for-Reasoning-Across-Dozens-of-Documents"><a href="#83-MONACO-More-Natural-and-Complex-Questions-for-Reasoning-Across-Dozens-of-Documents" class="headerlink" title="83. MONACO: More Natural and Complex Questions for Reasoning Across Dozens of Documents"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/MoNaCo__More_Natural_and_Complex_Questions_for_Reasoning_Across_Dozens_of_Documents.pdf">MONACO: More Natural and Complex Questions for Reasoning Across Dozens of Documents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Pennsylvania</span></p><p>该论文提出了MONACO基准数据集，包含1315个由人工撰写的自然且复杂的问题，这些问题需要跨越数十至数百份文档进行推理和信息整合。通过分解注释流程和人工标注，MONACO为每个问题提供详细的中间推理链及证据，系统评测了前沿大语言模型（LLM）的表现，发现即使最新LLM在该基准上的F1分数仅为61.2%，存在召回率低和幻觉严重等问题，凸显了LLM在真实复杂信息检索和推理任务上的显著不足。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MONACO_More_Natural_and_Complex_Questions_for_Reasoning_Across_Dozens_of_Documents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-Dynamic-Quality-Latency-Aware-Routing-for-LLM-Inference-in-Wireless-Edge-Device-Networks"><a href="#84-Dynamic-Quality-Latency-Aware-Routing-for-LLM-Inference-in-Wireless-Edge-Device-Networks" class="headerlink" title="84. Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Dynamic_Quality-Latency_Aware_Routing_for_LLM_Inference_in_Wireless_Edge-Device_Networks.pdf">Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了一种动态质量-延迟感知路由框架，针对无线边缘设备协作环境下大语言模型（LLM）推理的质量与延迟权衡问题。通过结合轻量级BERT路由器进行语义难度预测与精细化延迟成本模型，并在多轮对话中显式建模KV-cache重计算开销，实现在保证推理质量的前提下平均响应延迟降低5-15%，大模型调用率减少10-20%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Dynamic_Quality-Latency_Aware_Routing_for_LLM_Inference_in_Wireless_Edge-Device_Networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-LETToT-Label-Free-Evaluation-of-Large-Language-Models-On-Tourism-Using-Expert-Tree-of-Thought"><a href="#85-LETToT-Label-Free-Evaluation-of-Large-Language-Models-On-Tourism-Using-Expert-Tree-of-Thought" class="headerlink" title="85. LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/LETToT__Label-Free_Evaluation_of_Large_Language_Models_On_Tourism_Using_Expert_Tree-of-Thought.pdf">LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Warwick</span></p><p>该论文提出LETToT框架，通过专家知识构建的树状推理结构（Tree-of-Thought, ToT）实现旅游领域大语言模型的无标签评估。方法包括对旅游问答任务进行域适配、分层ToT组件的迭代优化与AHP加权评分，并用优化的ToT指标评估五种开源LLM。结论显示，LETToT能显著提升模型在主题相关性、实用性等多个维度的表现，推理增强型小模型可缩小与大模型的性能差距。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LETToT_Label-Free_Evaluation_of_Large_Language_Models_On_Tourism_Using_Expert_Tree-of-Thought.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-Group-Fairness-Meets-the-Black-Box-Enabling-Fair-Algorithms-on-Closed-LLMs-via-Post-Processing"><a href="#86-Group-Fairness-Meets-the-Black-Box-Enabling-Fair-Algorithms-on-Closed-LLMs-via-Post-Processing" class="headerlink" title="86. Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Group_Fairness_Meets_the_Black_Box__Enabling_Fair_Algorithms_on_Closed_LLMs_via_Post-Processing.pdf">Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Urbana-Champaign</span></p><p>本文提出了一种在闭源大语言模型（LLM）上实现群体公平性的通用后处理框架：将LLM视为特征提取器，通过精心设计的提示（prompt）获取与公平性相关的概率预测（如token log probabilities），并利用这些特征应用传统公平算法，在不需访问模型权重或内部嵌入的情况下构建轻量级公平分类器。实验证明该方法在多个数据集和公平性标准下均能兼顾准确性与公平性，并在低样本场景下优于基于LLM嵌入或原始特征训练的公平模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Group_Fairness_Meets_the_Black_Box_Enabling_Fair_Algorithms_on_Closed_LLMs_via_Post-Processing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-Generating-Dialogues-from-Egocentric-Instructional-Videos-for-Task-Assistance-Dataset-Method-and-Benchmark"><a href="#87-Generating-Dialogues-from-Egocentric-Instructional-Videos-for-Task-Assistance-Dataset-Method-and-Benchmark" class="headerlink" title="87. Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Generating_Dialogues_from_Egocentric_Instructional_Videos_for_Task_Assistance__Dataset,_Method_and_B.pdf">Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Google</span></p><p>本论文提出了一种基于大语言模型的自动化方法，将单人叙述的教学视频转化为专家-新手的任务指导对话，流程包括步骤提取、对话生成与视频定位，并结合现有数据集构建了HowToDIV多模态对话数据集。实验结果表明，该方法能高效生成高质量、覆盖多任务领域的任务指导对话，为AI助手在实际情境下的训练和评估提供了强基线和新数据资源。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Generating_Dialogues_from_Egocentric_Instructional_Videos_for_Task_Assistance_Dataset_Method_and_Benchmark.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-Role-Augmented-Intent-Driven-Generative-Search-Engine-Optimization"><a href="#88-Role-Augmented-Intent-Driven-Generative-Search-Engine-Optimization" class="headerlink" title="88. Role-Augmented Intent-Driven Generative Search Engine Optimization"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Role-Augmented_Intent-Driven_Generative_Search_Engine_Optimization.pdf">Role-Augmented Intent-Driven Generative Search Engine Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出了Role-Augmented Intent-Driven Generative Search Engine Optimization (RAID G-SEO)，通过四阶段流程（内容摘要、意图推断及反思、步骤规划、内容重写）并结合4W多角色深度反思机制，对生成式搜索引擎中内容进行针对性优化。实验结果表明，基于意图建模的方法在提升内容可见性和适应多样化检索场景方面显著优于传统和现有基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Role-Augmented_Intent-Driven_Generative_Search_Engine_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-AlphaAgents-Large-Language-Model-based-Multi-Agents-for-Equity-Portfolio-Constructions"><a href="#89-AlphaAgents-Large-Language-Model-based-Multi-Agents-for-Equity-Portfolio-Constructions" class="headerlink" title="89. AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/AlphaAgents__Large_Language_Model_based_Multi-Agents_for_Equity_Portfolio_Constructions.pdf">AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">BlackRock, Inc.</span></p><p>该论文提出了一种基于大型语言模型（LLM）的多智能体系统AlphaAgents，用于股票分析与投资组合构建。方法上，设计了基金基本面、情绪和估值三类专用代理，通过角色提示和工具增强，协作分析股票并采用内部辩论机制达成投资共识，有效支持风险偏好建模和认知偏差缓解。结论显示，多智能体框架在股票筛选和投资决策中优于单智能体方案，能提升投资分析的透明度和稳健性，为未来智能投资系统奠定基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AlphaAgents_Large_Language_Model_based_Multi-Agents_for_Equity_Portfolio_Constructions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="90-Pretrained-Conformers-for-Audio-Fingerprinting-and-Retrieval"><a href="#90-Pretrained-Conformers-for-Audio-Fingerprinting-and-Retrieval" class="headerlink" title="90. Pretrained Conformers for Audio Fingerprinting and Retrieval"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Pretrained_Conformers_for_Audio_Fingerprinting_and_Retrieval.pdf">Pretrained Conformers for Audio Fingerprinting and Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Infobip</span></p><p>本文提出了一种基于自监督对比学习预训练的Conformer编码器架构，用于音频指纹提取和检索，通过提取短音频片段的独特嵌入，实现对未见音频的高效检索。实验表明，该模型对时序错位、噪声、混响及极端时间拉伸等失真具有极强鲁棒性，在多个公开数据集上取得了与当前最优方法相当甚至更优的效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Pretrained_Conformers_for_Audio_Fingerprinting_and_Retrieval.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-A-Comprehensive-Perspective-on-Explainable-AI-across-the-Machine-Learning-Workflow"><a href="#91-A-Comprehensive-Perspective-on-Explainable-AI-across-the-Machine-Learning-Workflow" class="headerlink" title="91. A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/A_Comprehensive_Perspective_on_Explainable_AI_across_the_Machine_Learning_Workflow.pdf">A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">JADBio Gnosis DA S.A.</span></p><p>本文提出了Holistic Explainable AI (HXAI) 框架，将可解释性嵌入到机器学习工作流的所有阶段，并针对不同用户（领域专家、数据分析师、数据科学家）定制解释。通过构建六大核心组件和112项问题库，文章系统分析现有工具覆盖不足，总结出有效解释的特征，并提出利用AI Agent（基于LLM）自动聚合与个性化生成解释，促进AI系统的透明性、可用性和信任度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A_Comprehensive_Perspective_on_Explainable_AI_across_the_Machine_Learning_Workflow.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-Physics-Informed-Diffusion-Models-for-Unsupervised-Anomaly-Detection-in-Multivariate-Time-Series"><a href="#92-Physics-Informed-Diffusion-Models-for-Unsupervised-Anomaly-Detection-in-Multivariate-Time-Series" class="headerlink" title="92. Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Physics-Informed_Diffusion_Models_for_Unsupervised_Anomaly_Detection_in_Multivariate_Time_Series.pdf">Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fraunhofer IOSB-INA</span></p><p>该论文提出了一种面向多变量时序数据的无监督异常检测方法，通过在扩散模型训练过程中引入加权物理信息损失函数（基于静态权重调度），使模型更好地拟合数据分布并提升异常检测性能。实验证明，物理信息指导的训练方法在合成和真实数据集上提高了F1分数、数据多样性和对数似然值，优于现有的物理信息和纯数据驱动扩散模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Physics-Informed_Diffusion_Models_for_Unsupervised_Anomaly_Detection_in_Multivariate_Time_Series.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-A-Remedy-for-Over-Squashing-in-Graph-Learning-via-Forman-Ricci-Curvature-based-Graph-to-Hypergraph-Structural-Lifting"><a href="#93-A-Remedy-for-Over-Squashing-in-Graph-Learning-via-Forman-Ricci-Curvature-based-Graph-to-Hypergraph-Structural-Lifting" class="headerlink" title="93. A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/A_Remedy_for_Over-Squashing_in_Graph_Learning_via_Forman-Ricci_Curvature_based_Graph-to-Hypergraph_S.pdf">A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Perelyn GmbH</span></p><p>该论文提出利用Forman-Ricci曲率度量图结构，识别信息流瓶颈后进行结构提升，将图映射为超图，从而通过引入高阶拓扑关系缓解图神经网络中的过度压缩问题。实验表明该方法能提升多种图和超图神经网络模型在分类和分子属性预测任务上的表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A_Remedy_for_Over-Squashing_in_Graph_Learning_via_Forman-Ricci_Curvature_based_Graph-to-Hypergraph_Structural_Lifting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-NeMo-A-Neuron-Level-Modularizing-While-Training-Approach-for-Decomposing-DNN-Models"><a href="#94-NeMo-A-Neuron-Level-Modularizing-While-Training-Approach-for-Decomposing-DNN-Models" class="headerlink" title="94. NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/NeMo__A_Neuron-Level_Modularizing-While-Training_Approach_for_Decomposing_DNN_Models.pdf">NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of Complex &amp; Critical Software Environment, Beihang University</span></p><p>本文提出了一种用于深度神经网络模型结构化分解的神经元级联训练模块化方法NeMo，通过在训练过程中结合神经元识别器和基于对比学习的复合损失函数，实现对Transformer和CNN等多种架构的模块化训练和分解。实验结果表明NeMo在保持模型准确率的同时大幅减少模型尺寸和计算开销，优于现有的主流模块化方法，有效支持按需模型复用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/NeMo_A_Neuron-Level_Modularizing-While-Training_Approach_for_Decomposing_DNN_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-Mitigating-Modality-Quantity-and-Quality-Imbalance-in-Multimodal-Online-Federated-Learning"><a href="#95-Mitigating-Modality-Quantity-and-Quality-Imbalance-in-Multimodal-Online-Federated-Learning" class="headerlink" title="95. Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Mitigating_Modality_Quantity_and_Quality_Imbalance_in_Multimodal_Online_Federated_Learning.pdf">Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peng Cheng Laboratory</span></p><p>本文系统分析了多模态在线联邦学习（MMO-FL）中，由于物联网设备不稳定导致的模态数量和质量失衡问题，并提出了理论后悔界限来量化其对学习性能的影响。为解决该问题，作者提出了基于原型学习的模态数量与质量重平衡（QQR）算法，包括在线全局原型构建、原型数量重平衡和原型质量重平衡模块，有效地在训练过程中并行补偿模态失衡。实验结果表明，QQR算法在UCI-HAR和MVSA-Single两个真实多模态数据集上均优于现有基线方法，在模态失衡场景下具备较强泛化能力和通信效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Mitigating_Modality_Quantity_and_Quality_Imbalance_in_Multimodal_Online_Federated_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-Towards-the-Next-generation-Bayesian-Network-Classifiers"><a href="#96-Towards-the-Next-generation-Bayesian-Network-Classifiers" class="headerlink" title="96. Towards the Next-generation Bayesian Network Classifiers"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Towards_the_Next-generation_Bayesian_Network_Classifiers.pdf">Towards the Next-generation Bayesian Network Classifiers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhengzhou University</span></p><p>本文提出了一种新范式，通过引入分布式表示学习来设计高阶贝叶斯网络分类器，突破了传统方法在参数爆炸和数据稀疏性上的限制。具体方法是在K-Dependence Bayesian classifier的基础上，构建三层神经网络（NeuralKDB），学习特征和标签值的分布式表示，并通过随机梯度下降高效训练模型，使其能够捕捉高阶特征依赖关系。实验结果显示，NeuralKDB在60个UCI数据集上显著优于传统贝叶斯网络分类器及其它主流分类方法，验证了分布式表示学习在提升高阶依赖建模和分类性能上的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Towards_the_Next-generation_Bayesian_Network_Classifiers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-Graph-Neural-Diffusion-via-Generalized-Opinion-Dynamics"><a href="#97-Graph-Neural-Diffusion-via-Generalized-Opinion-Dynamics" class="headerlink" title="97. Graph Neural Diffusion via Generalized Opinion Dynamics"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Graph_Neural_Diffusion_via_Generalized_Opinion_Dynamics.pdf">Graph Neural Diffusion via Generalized Opinion Dynamics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Australian National University</span></p><p>本文提出了GODNF（Generalized Opinion Dynamics Neural Framework），一种基于多种意见动力学理论的图神经网络扩散统一框架。该方法通过节点特性建模和动态邻域影响，结合特征保持、初始特征依赖、邻域影响和结构正则化，实现高效、可解释且理论收敛的消息传播机制。理论分析证明GODNF能适应多样化的扩散收敛模式，实验结果在节点分类和影响力估计任务上优于现有主流GNN方法。结论表明，GODNF不仅提升了表达能力，还兼具高效性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Graph_Neural_Diffusion_via_Generalized_Opinion_Dynamics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-Meta-learning-Structure-Preserving-Dynamics"><a href="#98-Meta-learning-Structure-Preserving-Dynamics" class="headerlink" title="98. Meta-learning Structure-Preserving Dynamics"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Meta-learning_Structure-Preserving_Dynamics.pdf">Meta-learning Structure-Preserving Dynamics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Arizona State University</span></p><p>本文提出了一种基于调制（modulation）的元学习框架，用于学习结构保持的动力学系统，包括能量守恒与耗散系统。方法创新点在于引入低维潜在向量，通过自编码方式对模型参数进行条件化，并提出了两种新型调制技术（Rank-One与Multi-Rank），显著提升模型的表达能力和泛化性。实验结果表明，所提方法在多组参数化物理系统的少样本学习场景下实现了高精度预测，并保持了动力学系统的物理约束，优于现有优化型元学习和调制方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Meta-learning_Structure-Preserving_Dynamics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025/08/18报纸</title>
    <link href="/2025/08/2025-08-18_article/"/>
    <url>/2025/08/2025-08-18_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2025/08/18报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-18<br>📄 <strong>发现论文数量</strong>：132  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Non-Stationary-Restless-Multi-Armed-Bandits-with-Provable-Guarantee"><a href="#1-Non-Stationary-Restless-Multi-Armed-Bandits-with-Provable-Guarantee" class="headerlink" title="1. Non-Stationary Restless Multi-Armed Bandits with Provable Guarantee"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Non-Stationary_Restless_Multi-Armed_Bandits_with_Provable_Guarantee.pdf">Non-Stationary Restless Multi-Armed Bandits with Provable Guarantee</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Yang Ming Chiao Tung University</span></p><p>本文提出NS-Whittle算法，针对带有有限变化预算的非平稳Restless Multi-Armed Bandits（RMABs），将滑动窗口强化学习（RL）与上置信界（UCB）机制结合，实现对转移动态及其变化的自适应学习。方法采用每臂独立的滑动窗口和Whittle指数，显著降低维度和计算复杂度，并首次在非平稳RMAB下给出Oe(N²B^{1&#x2F;4}T^{3&#x2F;4})的理论后悔界限。结论表明，该方法兼顾理论严谨性和实际可扩展性，为动态资源受限环境下的序贯决策提供了高效基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Non-Stationary_Restless_Multi-Armed_Bandits_with_Provable_Guarantee.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Variance-Reduced-Policy-Gradient-Method-for-Multi-Objective-Reinforcement-Learning"><a href="#2-Variance-Reduced-Policy-Gradient-Method-for-Multi-Objective-Reinforcement-Learning" class="headerlink" title="2. Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Variance_Reduced_Policy_Gradient_Method_for_Multi-Objective_Reinforcement_Learning.pdf">Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ETH Zurich</span></p><p>本论文提出了MO-TSIVR-PG算法，通过在多目标强化学习(MORL)中对策略梯度方法引入方差降低技术，显著提升了样本效率，支持大规模和连续状态-动作空间。理论分析和实验结果表明，该算法在收敛速度和样本复杂度上均优于现有方法，且在多目标任务中表现优异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Variance_Reduced_Policy_Gradient_Method_for_Multi-Objective_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Pass-k-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models"><a href="#3-Pass-k-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models" class="headerlink" title="3. Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Pass@k_Training_for_Adaptively_Balancing_Exploration_and_Exploitation_of_Large_Reasoning_Models.pdf">Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Renmin University of China</span></p><p>本文提出了在大型语言模型（LLMs）推理任务中的RLVR训练中，采用Pass@k作为奖励指标的方法（Pass@k Training），通过全采样、bootstrap采样和解析推导三种流程，有效提升模型的探索能力并保持其利用能力。结论显示，Pass@k Training不仅提升了模型在下游任务中的Pass@k表现，还能通过后续Pass@1训练将探索收益转化为更强的Pass@1表现，具备良好的泛化性和实际应用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Pass%40k_Training_for_Adaptively_Balancing_Exploration_and_Exploitation_of_Large_Reasoning_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-REFN-A-Reinforcement-Learning-From-Network-Framework-against-1-day-n-day-Exploitations"><a href="#4-REFN-A-Reinforcement-Learning-From-Network-Framework-against-1-day-n-day-Exploitations" class="headerlink" title="4. REFN: A Reinforcement-Learning-From-Network Framework against 1-day&#x2F;n-day Exploitations"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/REFN__A_Reinforcement-Learning-From-Network_Framework_against_1-day_n-day_Exploitations.pdf">REFN: A Reinforcement-Learning-From-Network Framework against 1-day&#x2F;n-day Exploitations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Artificial Intelligence, Hubei University</span></p><p>该论文提出REFN框架，通过网络驱动的强化学习（RL）训练大型语言模型（LLMs），自动生成并部署网络过滤器，有效防御大规模1-day&#x2F;n-day漏洞利用。REFN采用Agentic-RAG知识蒸馏、RL-from-VNF管道和在线Agentic验证，显著提升过滤规则生成的准确性、部署效率和可扩展性。实验表明，REFN在准确率和F1分数方面均优于现有方法，平均修复时间缩短至3.65小时，能高效扩展至万台设备。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/REFN_A_Reinforcement-Learning-From-Network_Framework_against_1-day_n-day_Exploitations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-Nonlocal-Monte-Carlo-via-Reinforcement-Learning"><a href="#5-Nonlocal-Monte-Carlo-via-Reinforcement-Learning" class="headerlink" title="5. Nonlocal Monte Carlo via Reinforcement Learning"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Nonlocal_Monte_Carlo_via_Reinforcement_Learning.pdf">Nonlocal Monte Carlo via Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peter Grünberg Institut (PGI-14), Forschungszentrum Jülich GmbH</span></p><p>该论文提出RLNMC方法，将深度强化学习与Nonequilibrium Nonlocal Monte Carlo（NMC）算法结合，通过训练图神经网络（GNN）策略自动发现组合优化中的非局部跳跃步骤。实验表明，RLNMC在典型难解4-SAT基准问题上，相较于标准MCMC和传统NMC方法，能够在残差能量、解的多样性和求解时间等指标上取得明显提升，并展现出较好的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Nonlocal_Monte_Carlo_via_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-Stabilizing-Long-term-Multi-turn-Reinforcement-Learning-with-Gated-Rewards"><a href="#6-Stabilizing-Long-term-Multi-turn-Reinforcement-Learning-with-Gated-Rewards" class="headerlink" title="6. Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Stabilizing_Long-term_Multi-turn_Reinforcement_Learning_with_Gated_Rewards.pdf">Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology (Shenzhen)</span></p><p>该论文提出了SWE-oriented RL Framework和Gated Reward Accumulation (G-RA)方法，针对软件工程任务中的多轮长时序强化学习，结合多层奖励设计及docker环境支持，实现了奖励的门控累积，仅在高层(长期)奖励达到阈值时累积即时奖励，从而平衡稀疏长期目标与密集即时反馈。实验表明，G-RA显著提升了任务完成率与修改率，有效避免了奖励错配和策略退化，验证了其在复杂长时序任务中稳定RL优化的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Stabilizing_Long-term_Multi-turn_Reinforcement_Learning_with_Gated_Rewards.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning"><a href="#7-Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning" class="headerlink" title="7. Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Few-shot_Vision-based_Human_Activity_Recognition_with_MLLM-based_Visual_Reinforcement_Learning.pdf">Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kyushu University</span></p><p>该论文提出FAVOR方法，将多模态大语言模型（MLLM）与基于视觉的强化学习（使用Group Relative Policy Optimization, GRPO和可验证奖励函数）结合，用于少样本人类活动识别。实验表明FAVOR在HAR任务的少样本学习中显著优于传统监督微调方法，尤其在数据稀缺情况下提升泛化和推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Few-shot_Vision-based_Human_Activity_Recognition_with_MLLM-based_Visual_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-Multi-Agent-Trust-Region-Policy-Optimisation-A-Joint-Constraint-Approach"><a href="#8-Multi-Agent-Trust-Region-Policy-Optimisation-A-Joint-Constraint-Approach" class="headerlink" title="8. Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Multi-Agent_Trust_Region_Policy_Optimisation__A_Joint_Constraint_Approach.pdf">Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Maryland</span></p><p>本文提出了针对多智能体信任域策略优化（HATRPO）均匀KL约束低效问题的两种自适应扩展方法：HATRPO-G（基于改进-散度比的贪心分配）和HATRPO-W（基于KKT条件的全局优化分配），实现总KL阈值在智能体间的灵活分配。实验结果表明，提出方法能更快收敛并显著提升最终奖励，尤其在异质和不平衡智能体场景下表现优异，超过22.5%的性能提升，并有效捕获智能体间优势差异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Agent_Trust_Region_Policy_Optimisation_A_Joint_Constraint_Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-Integrating-Reinforcement-Learning-with-Visual-Generative-Models-Foundations-and-Advances"><a href="#9-Integrating-Reinforcement-Learning-with-Visual-Generative-Models-Foundations-and-Advances" class="headerlink" title="9. Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Integrating_Reinforcement_Learning_with_Visual_Generative_Models__Foundations_and_Advances.pdf">Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Artificial Intelligence (TeleAI), China Telecom</span></p><p>本文系统综述了强化学习（RL）与视觉生成模型（包括图像、视频、3D&#x2F;4D内容生成）结合的最新进展，涵盖RL理论演变、主流RL方法（如PPO、GRPO、DPO）在提升生成模型可控性、语义对齐、人类偏好匹配等方面的应用流程。结论指出，RL不仅作为生成模型的微调和优化层，更成为实现复杂目标和人类意图对齐的结构性组件，对未来多模态、交互式生成系统具有重要意义。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Integrating_Reinforcement_Learning_with_Visual_Generative_Models_Foundations_and_Advances.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-A-Curriculum-Learning-Approach-to-Reinforcement-Learning-Leveraging-RAG-for-Multimodal-Question-Answering"><a href="#10-A-Curriculum-Learning-Approach-to-Reinforcement-Learning-Leveraging-RAG-for-Multimodal-Question-Answering" class="headerlink" title="10. A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Curriculum_Learning_Approach_to_Reinforcement_Learning__Leveraging_RAG_for_Multimodal_Question_Ans.pdf">A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Meituan</span></p><p>本文提出了结合视觉大语言模型（VLLM）、知识蒸馏、RAG检索增强生成和课程学习的强化学习训练方法，用于多模态、多轮次问题回答。该方法通过分阶段课程学习稳定强化学习训练，有效提升模型在多模态检索与问题回答任务中的准确率和可靠性，在KDD Cup 2025 CRAG-MM竞赛的多项任务中取得领先成绩。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Curriculum_Learning_Approach_to_Reinforcement_Learning_Leveraging_RAG_for_Multimodal_Question_Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-Multi-Agent-Reinforcement-Learning-for-Adaptive-Resource-Orchestration-in-Cloud-Native-Clusters"><a href="#11-Multi-Agent-Reinforcement-Learning-for-Adaptive-Resource-Orchestration-in-Cloud-Native-Clusters" class="headerlink" title="11. Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Multi-Agent_Reinforcement_Learning_for_Adaptive_Resource_Orchestration_in_Cloud-Native_Clusters.pdf">Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northwestern University</span></p><p>本文提出了一种基于多智能体强化学习（MARL）的自适应资源编排方法，采用异构角色驱动的代理合作机制（HRAC）和局部-全局奖励塑形（LGRS）策略，实现对计算节点、存储节点和调度器等异构组件的分工学习与协作。实验表明，该方法在资源利用率、调度延迟和策略收敛速度等方面优于传统方法，并在多租户公平性和不完全信息场景下表现出良好的稳定性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Agent_Reinforcement_Learning_for_Adaptive_Resource_Orchestration_in_Cloud-Native_Clusters.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="12-TLE-Based-A2C-Agent-for-Terrestrial-Coverage-Orbital-Path-Planning"><a href="#12-TLE-Based-A2C-Agent-for-Terrestrial-Coverage-Orbital-Path-Planning" class="headerlink" title="12. TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/TLE-Based_A2C_Agent_for_Terrestrial_Coverage_Orbital_Path_Planning.pdf">TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sardar Vallabhbhai National Institute of Technology Surat</span></p><p>本文提出了一种基于TLE的轨道仿真环境，利用Advantage Actor-Critic (A2C) 强化学习算法自动优化地球观测卫星的轨道参数，实现对地表目标的精确覆盖。实验表明，A2C在收敛速度和累计奖励上均优于Proximal Policy Optimization (PPO)，能高效满足任务目标，验证了强化学习在低轨卫星轨道规划中的实用性和高效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/TLE-Based_A2C_Agent_for_Terrestrial_Coverage_Orbital_Path_Planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-A-Multimodal-Neural-Network-for-Recognizing-Subjective-Self-Disclosure-Towards-Social-Robots"><a href="#13-A-Multimodal-Neural-Network-for-Recognizing-Subjective-Self-Disclosure-Towards-Social-Robots" class="headerlink" title="13. A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Multimodal_Neural_Network_for_Recognizing_Subjective_Self-Disclosure_Towards_Social_Robots.pdf">A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amazon</span></p><p>本文提出了一种专为人机交互中主观自我表露建模设计的多模态注意力神经网络，结合音频（MFCC、wav2vec2.0）与视觉（OpenFace、InceptionV1-ResNet）特征输入，并创新性地引入了尺度保持交叉熵损失函数，有效处理了分类与回归之间的问题。结果显示，该模型在自建大规模数据集上F1分数达0.83，显著优于基线方法，为社会机器人感知人类自我表露能力提供了重要突破。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Multimodal_Neural_Network_for_Recognizing_Subjective_Self-Disclosure_Towards_Social_Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT"><a href="#14-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT" class="headerlink" title="14. UI-Venus Technical Report: Building High-performance UI Agents with RFT"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/UI-Venus_Technical_Report__Building_High-performance_UI_Agents_with_RFT.pdf">UI-Venus Technical Report: Building High-performance UI Agents with RFT</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ant Group</span></p><p>本论文提出了UI-Venus，一种基于多模态大语言模型（Qwen2.5-VL）并仅需截图输入的原生UI智能体，通过引入基于GRPO的强化学习微调（RFT）方法，实现了UI界面元素定位与导航任务的业界最优（SOTA）性能。方法包括高质量数据清洗、精细化奖励函数设计以及自进化轨迹历史对齐与稀疏动作增强框架，显著提升了复杂UI任务中的泛化和规划能力。结论是，UI-Venus在多个标准基准上超越现有开源与闭源模型，验证了强化学习策略和数据处理流程在UI Agent训练中的有效性，并已开源相关模型及评测代码。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/UI-Venus_Technical_Report_Building_High-performance_UI_Agents_with_RFT.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-Scaling-Up-without-Fading-Out-Goal-Aware-Sparse-GNN-for-RL-based-Generalized-Planning"><a href="#15-Scaling-Up-without-Fading-Out-Goal-Aware-Sparse-GNN-for-RL-based-Generalized-Planning" class="headerlink" title="15. Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Scaling_Up_without_Fading_Out__Goal-Aware_Sparse_GNN_for_RL-based_Generalized_Planning.pdf">Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">LIG Nex1</span></p><p>本文提出了一种结合稀疏、目标感知图神经网络（GNN）表示和课程学习的强化学习（RL）方法，专门用于解决大规模基于PDDL的无人机任务规划问题。通过仅编码局部邻接关系并显式整合目标空间特征，有效提升了训练效率和策略泛化能力，显著降低了GPU内存需求，实现了在大规模网格环境中高效的任务规划。实验证明，该方法在复杂无人机任务场景中具有优越的扩展性和稳定性，克服了以往全连接图方法的瓶颈。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Scaling_Up_without_Fading_Out_Goal-Aware_Sparse_GNN_for_RL-based_Generalized_Planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-Synthesis-of-Deep-Neural-Networks-with-Safe-Robust-Adaptive-Control-for-Reliable-Operation-of-Wheeled-Mobile-Robots"><a href="#16-Synthesis-of-Deep-Neural-Networks-with-Safe-Robust-Adaptive-Control-for-Reliable-Operation-of-Wheeled-Mobile-Robots" class="headerlink" title="16. Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Synthesis_of_Deep_Neural_Networks_with_Safe_Robust_Adaptive_Control_for_Reliable_Operation_of_Wheele.pdf">Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tampere University</span></p><p>本文提出了一种结合深度神经网络（DNN）与安全鲁棒自适应控制（RAC）的分层控制策略，实现重型轮式移动机器人（WMR）在复杂环境下的可靠运行。方法包括以DNN为主控制，在遇到外部干扰时由低层安全机制切换为RAC，系统始终受两级安全层监控，保障精度与稳定性。实验结果表明，所提方法能在满足安全标准的前提下，实现高精度和鲁棒性，并有效应对实际工况下的扰动。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Synthesis_of_Deep_Neural_Networks_with_Safe_Robust_Adaptive_Control_for_Reliable_Operation_of_Wheeled_Mobile_Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-Learning-Task-Execution-Hierarchies-for-Redundant-Robots"><a href="#17-Learning-Task-Execution-Hierarchies-for-Redundant-Robots" class="headerlink" title="17. Learning Task Execution Hierarchies for Redundant Robots"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Learning_Task_Execution_Hierarchies_for_Redundant_Robots.pdf">Learning Task Execution Hierarchies for Redundant Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Padova</span></p><p>该论文提出了一种结合强化学习（RL）和遗传编程（Genetic Programming, GP）的框架，能够根据用户自定义的高层任务目标和代价函数，自动为冗余机器人学习任务优先级、控制参数和任务激活逻辑（Stack of Tasks, SoTs）。方法流程包括：用户通过GUI设定任务目标权重，系统在仿真中以强化学习方式评估每组任务栈的表现，通过遗传编程演化优化任务顺序和参数，最终选取最优SoT进行真实机器人测试。实验在ABB移动双臂机器人YuMi平台的仿真和现实环境中表明，该方法能实现零样本迁移、适应动态环境、无需专家手工调参，显著提升了冗余机器人任务管理的通用性和自适应性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning_Task_Execution_Hierarchies_for_Redundant_Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-MLM-Learning-Multi-task-Loco-Manipulation-Whole-Body-Control-for-Quadruped-Robot-with-Arm"><a href="#18-MLM-Learning-Multi-task-Loco-Manipulation-Whole-Body-Control-for-Quadruped-Robot-with-Arm" class="headerlink" title="18. MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MLM__Learning_Multi-task_Loco-Manipulation_Whole-Body_Control_for_Quadruped_Robot_with_Arm.pdf">MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了一种结合真实和仿真数据的强化学习框架MLM，实现了带机械臂四足机器人多任务全身运动-操作一体化控制。方法包括引入基于自适应课程的轨迹采样机制和轨迹-速度预测策略网络，实现多任务自适应学习与历史轨迹下的预测。实验表明，该方法可以在仿真和真实环境中实现多任务零样本迁移，提升了四足机器人运动与操作协同的通用性和任务表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MLM_Learning_Multi-task_Loco-Manipulation_Whole-Body_Control_for_Quadruped_Robot_with_Arm.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Projected-Coupled-Diffusion-for-Test-Time-Constrained-Joint-Generation"><a href="#19-Projected-Coupled-Diffusion-for-Test-Time-Constrained-Joint-Generation" class="headerlink" title="19. Projected Coupled Diffusion for Test-Time Constrained Joint Generation"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Projected_Coupled_Diffusion_for_Test-Time_Constrained_Joint_Generation.pdf">Projected Coupled Diffusion for Test-Time Constrained Joint Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>本文提出了一种名为Projected Coupled Diffusion (PCD)的新型测试时约束联合生成框架，在无需重新训练的前提下，通过引入耦合引导项和每步投影操作，使多个预训练扩散模型在生成相关样本时能精确满足实时约束。方法在图像对生成、机器人操作和多机器人路径规划等任务中验证，实验表明PCD能有效提升样本相关性并严格满足任务约束，且计算开销可控。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Projected_Coupled_Diffusion_for_Test-Time_Constrained_Joint_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-MASH-Cooperative-Heterogeneous-Multi-Agent-Reinforcement-Learning-for-Single-Humanoid-Robot-Locomotion"><a href="#20-MASH-Cooperative-Heterogeneous-Multi-Agent-Reinforcement-Learning-for-Single-Humanoid-Robot-Locomotion" class="headerlink" title="20. MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MASH__Cooperative-Heterogeneous_Multi-Agent_Reinforcement_Learning_for_Single_Humanoid_Robot_Locomot.pdf">MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeastern University</span></p><p>该论文提出了MASH方法，将单个人形机器人四肢（双腿与双臂）建模为独立智能体，通过协作异质多智能体深度强化学习（MARL）优化机器人步态与动作协调，采用中央评论员和多智能体PPO训练。实验证明MASH相比单智能体RL收敛更快、动作更平滑、肢体协调性更高，并且具备更强的实际部署鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MASH_Cooperative-Heterogeneous_Multi-Agent_Reinforcement_Learning_for_Single_Humanoid_Robot_Locomotion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-KDPE-A-Kernel-Density-Estimation-Strategy-for-Diffusion-Policy-Trajectory-Selection"><a href="#21-KDPE-A-Kernel-Density-Estimation-Strategy-for-Diffusion-Policy-Trajectory-Selection" class="headerlink" title="21. KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/KDPE__A_Kernel_Density_Estimation_Strategy_for_Diffusion_Policy_Trajectory_Selection.pdf">KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Istituto Italiano di Tecnologia</span></p><p>本文提出了KDPE，一种基于核密度估计（KDE）的策略，用于过滤和选择Diffusion Policy（DP）生成的机器人轨迹，通过并行采样多条轨迹并用流形感知核函数建模动作分布，筛除异常轨迹，仅保留最具代表性的动作路径。实验结果表明，KDPE在仿真和真实机器人抓取、分拣、制作咖啡等任务中，能提升成功率和鲁棒性，尤其在演示质量较低或精度要求较高的场景效果更优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/KDPE_A_Kernel_Density_Estimation_Strategy_for_Diffusion_Policy_Trajectory_Selection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-Learning-State-Space-Models-of-Dynamic-Systems-from-Arbitrary-Data-using-Joint-Embedding-Predictive-Architectures"><a href="#22-Learning-State-Space-Models-of-Dynamic-Systems-from-Arbitrary-Data-using-Joint-Embedding-Predictive-Architectures" class="headerlink" title="22. Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Learning_State-Space_Models_of_Dynamic_Systems_from_Arbitrary_Data_using_Joint_Embedding_Predictive_.pdf">Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">RPTU University Kaiserslautern-Landau</span></p><p>本文提出了一种基于Joint Embedding Predictive Architectures (JEPA) 和神经常微分方程（neural ODE）的新方法，将任意观测数据序列编码为连续时间的潜在状态空间，并通过引入收缩性和Lipschitz常数损失，保证状态空间的有序结构。方法在仿真摆系统中实现从图像数据直接学习动态系统潜在状态空间，实现了从潜在预测到图像重构，展示了对机器人控制和估计的广泛应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning_State-Space_Models_of_Dynamic_Systems_from_Arbitrary_Data_using_Joint_Embedding_Predictive_Architectures.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-Efficient-Methods-for-Accurate-Sparse-Trajectory-Recovery-and-Map-Matching"><a href="#23-Efficient-Methods-for-Accurate-Sparse-Trajectory-Recovery-and-Map-Matching" class="headerlink" title="23. Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Efficient_Methods_for_Accurate_Sparse_Trajectory_Recovery_and_Map_Matching.pdf">Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong Polytechnic University</span></p><p>本文提出了TRMMA和MMA两种高效且准确的稀疏轨迹恢复与地图匹配方法。MMA将GPS点匹配到道路段的任务转化为小候选集上的分类问题，并通过候选段与点的有效嵌入提升匹配精度，TRMMA进一步在MMA识别的路径上，通过双变换器（Dual-Transformer）编码捕捉轨迹和路径的潜在模式，利用解码过程预测缺失点的位置比例与道路段，实现高效轨迹恢复。实验表明，TRMMA与MMA在大规模真实数据集上均优于现有方法，且显著提升训练和推理效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Efficient_Methods_for_Accurate_Sparse_Trajectory_Recovery_and_Map_Matching.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-CorrectNav-Self-Correction-Flywheel-Empowers-Vision-Language-Action-Navigation-Model"><a href="#24-CorrectNav-Self-Correction-Flywheel-Empowers-Vision-Language-Action-Navigation-Model" class="headerlink" title="24. CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/CorrectNav__Self-Correction_Flywheel_Empowers_Vision-Language-Action_Navigation_Model.pdf">CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>该论文提出了Self-correction Flywheel（自我纠正飞轮）后训练范式，通过自动收集导航模型在训练集上的错误轨迹，检测偏差并生成自我纠正数据用于感知和动作校正，多轮迭代训练提升导航模型CorrectNav的自我纠错能力。实验在R2R-CE和RxR-CE等基准上取得了新的SOTA，并在实际机器人中验证了其优越的纠错、避障和长指令执行能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/CorrectNav_Self-Correction_Flywheel_Empowers_Vision-Language-Action_Navigation_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-Probabilistic-Latency-Analysis-of-the-Data-Distribution-Service-in-ROS-2"><a href="#25-Probabilistic-Latency-Analysis-of-the-Data-Distribution-Service-in-ROS-2" class="headerlink" title="25. Probabilistic Latency Analysis of the Data Distribution Service in ROS 2"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Probabilistic_Latency_Analysis_of_the_Data_Distribution_Service_in_ROS_2.pdf">Probabilistic Latency Analysis of the Data Distribution Service in ROS 2</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">DGIST</span></p><p>本文提出了一种针对ROS 2 DDS通信在有损无线网络下的概率延迟分析（PLA）模型。该方法通过离散状态建模与动态规划，系统性分析中间件层和传输层事件，预测未确认消息概率分布与重传延迟，并在270种场景下实验验证，其平均误差低于5%。结论指出PLA模型在可靠性、延迟和性能优化方面具备理论指导意义，能有效辅助无线工业机器人网络的QoS参数调优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Probabilistic_Latency_Analysis_of_the_Data_Distribution_Service_in_ROS_2.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-Super-LiDAR-Reflectance-for-Robotic-Perception"><a href="#26-Super-LiDAR-Reflectance-for-Robotic-Perception" class="headerlink" title="26. Super LiDAR Reflectance for Robotic Perception"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Super_LiDAR_Reflectance_for_Robotic_Perception.pdf">Super LiDAR Reflectance for Robotic Perception</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Macau</span></p><p>该论文提出了一种创新框架，通过自研的深度卷积网络（含自适应融合模块和动态补偿模块）将低成本非重复扫描LiDAR的稀疏反射率数据实时转换为高密度、校准后的反射率图像。方法包括构建稀疏到稠密配对标注数据集、静态到动态域数据增强，以及反射率补偿，显著提升了低成本LiDAR在机器人闭环检测和车道线检测等感知任务中的实用性。结论表明，该方法兼具高准确性和实时性，极大拓展了低成本LiDAR在机器人主动视觉中的应用边界。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Super_LiDAR_Reflectance_for_Robotic_Perception.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-ReconVLA-Reconstructive-Vision-Language-Action-Model-as-Effective-Robot-Perceiver"><a href="#27-ReconVLA-Reconstructive-Vision-Language-Action-Model-as-Effective-Robot-Perceiver" class="headerlink" title="27. ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ReconVLA__Reconstructive_Vision-Language-Action_Model_as_Effective_Robot_Perceiver.pdf">ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (Guangzhou)</span></p><p>本论文提出了一种重构式视觉-语言-动作模型（ReconVLA），采用隐式视觉归因范式，通过扩散Transformer模块重建机器人操作目标的凝视区域，促进模型学习细粒度视觉表征和精准关注目标区域，提升操控精度。实验结果表明，该方法在模拟及真实环境中均优于显式和链式视觉归因方法，实现了更强的泛化和对未见目标的操控能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ReconVLA_Reconstructive_Vision-Language-Action_Model_as_Effective_Robot_Perceiver.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-Large-Model-Empowered-Embodied-AI-A-Survey-on-Decision-Making-and-Embodied-Learning"><a href="#28-Large-Model-Empowered-Embodied-AI-A-Survey-on-Decision-Making-and-Embodied-Learning" class="headerlink" title="28. Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Large_Model_Empowered_Embodied_AI__A_Survey_on_Decision-Making_and_Embodied_Learning.pdf">Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>本文系统综述了大模型赋能的具身智能（Embodied AI），聚焦于自主决策与具身学习。方法上，文章梳理了大模型在层次化决策（包括高层规划、低层执行、反馈增强）与端到端决策（基于VLA模型）、具身学习（模仿学习与强化学习）、世界模型等方面的最新进展与技术流程。结论指出，大模型极大提升了具身智能系统在感知、规划、执行与学习等环节的能力，但在数据稀缺、持续学习、部署效率及仿真-现实转移等方面仍有诸多挑战，未来需持续研究以实现更强的通用智能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Large_Model_Empowered_Embodied_AI_A_Survey_on_Decision-Making_and_Embodied_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-A-Semantic-Aware-Framework-for-Safe-and-Intent-Integrative-Assistance-in-Upper-Limb-Exoskeletons"><a href="#29-A-Semantic-Aware-Framework-for-Safe-and-Intent-Integrative-Assistance-in-Upper-Limb-Exoskeletons" class="headerlink" title="29. A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Semantic-Aware_Framework_for_Safe_and_Intent-Integrative_Assistance_in_Upper-Limb_Exoskeletons.pdf">A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了一种面向上肢外骨骼的语义感知辅助框架，将大语言模型（LLM）集成到任务规划中，通过提取任务语义信息和人类意图，自动配置辅助参数，并利用扩散模型实现异常检测与实时重规划。实验表明，该方法有效实现了人机认知对齐，提高了辅助任务的安全性、适应性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Semantic-Aware_Framework_for_Safe_and_Intent-Integrative_Assistance_in_Upper-Limb_Exoskeletons.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-JRDB-Reasoning-A-Difficulty-Graded-Benchmark-for-Visual-Reasoning-in-Robotics"><a href="#30-JRDB-Reasoning-A-Difficulty-Graded-Benchmark-for-Visual-Reasoning-in-Robotics" class="headerlink" title="30. JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/JRDB-Reasoning__A_Difficulty-Graded_Benchmark_for_Visual_Reasoning_in_Robotics.pdf">JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Monash University</span></p><p>本论文提出JRDB-Reasoning基准，首次系统定义和量化视觉推理复杂度，并开发了自适应查询引擎，可按需生成不同难度、多步骤推理和中间标注的问题，扩展JRDB数据集以支持机器人在拥挤人群环境中的视觉推理任务。实验显示，现有VLM模型在推理难度提升时性能显著下降，而该基准和工具有助于精细化评估机器人视觉推理能力和未来模型发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/JRDB-Reasoning_A_Difficulty-Graded_Benchmark_for_Visual_Reasoning_in_Robotics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="31-GNN-based-Unified-Deep-Learning"><a href="#31-GNN-based-Unified-Deep-Learning" class="headerlink" title="31. GNN-based Unified Deep Learning"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GNN-based_Unified_Deep_Learning.pdf">GNN-based Unified Deep Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>本论文提出了一种基于图神经网络（GNN）的统一学习范式，将多种异构深度学习架构（如MLP、CNN和GNN）转化为图表示，并在统一的图学习空间中通过GNN进行协同优化，实现参数共享和知识转移。实验表明，该方法在医学图像领域面对分布漂移和域断裂场景时，显著提升了各类模型的泛化能力和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GNN-based_Unified_Deep_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-Quantum-Visual-Fields-with-Neural-Amplitude-Encoding"><a href="#32-Quantum-Visual-Fields-with-Neural-Amplitude-Encoding" class="headerlink" title="32. Quantum Visual Fields with Neural Amplitude Encoding"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Quantum_Visual_Fields_with_Neural_Amplitude_Encoding.pdf">Quantum Visual Fields with Neural Amplitude Encoding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MPI for Informatics</span></p><p>本文提出了一种新型量子隐式神经表示（QINR）方法Quantum Visual Field (QVF)，用于2D图像与3D几何场的高效表示。其核心创新在于通过可学习的能量流形进行神经幅度编码，将经典数据嵌入量子态向量，并采用全纠缠设计的参数化量子电路在实希尔伯特空间内完成量子操作，保障数值稳定和快速收敛，无需经典后处理，直接通过投影测量提取信号。实验表明，QVF在视觉场表示精度和高频细节学习等方面均优于现有量子及主流经典基线方法，并首次实现了图像与3D形状集合的联合表示及应用（如图像修复、3D补全和插值），展示了其在量子增强视觉计算领域的实际潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Quantum_Visual_Fields_with_Neural_Amplitude_Encoding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data"><a href="#33-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data" class="headerlink" title="33. MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MAESTRO__Masked_AutoEncoders_for_Multimodal,_Multitemporal,_and_Multispectral_Earth_Observation_Data.pdf">MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institut national de l’information géographique et forestière (IGN), France</span></p><p>论文提出MAESTRO，一种针对地球观测（EO）数据的自监督学习方法，扩展了Masked Autoencoder（MAE）框架以支持多模态、多时序和多光谱输入。其核心技术包括基于token的早融合与晚融合策略，以及新颖的patch-group-wise归一化方法，有效注入光谱先验并提升表示能力。实验在四个遥感数据集上验证了方法，MAESTRO在依赖多时序动态的任务上达到SOTA，并在单时序任务上保持竞争力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MAESTRO_Masked_AutoEncoders_for_Multimodal_Multitemporal_and_Multispectral_Earth_Observation_Data.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing"><a href="#34-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing" class="headerlink" title="34. ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ToonComposer__Streamlining_Cartoon_Production_with_Generative_Post-Keyframing.pdf">ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong</span></p><p>本文提出了ToonComposer，一种基于Diffusion Transformer (DiT) 的生成式后关键帧(post-keyframing)模型，将卡通动画制作中的中间帧生成与上色步骤统一为单一流程。方法核心包括稀疏草图注入机制（sparse sketch injection）、区域控制(region-wise control)和空间低秩适配器（SLRA），实现对稀疏关键帧草图的精确控制，并高效迁移视频基础模型至卡通域。实验显示ToonComposer在视觉质量、运动连贯性与生产效率上均显著优于现有方法，大幅减少艺术家手动工作量，提升卡通视频生成的灵活性和质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ToonComposer_Streamlining_Cartoon_Production_with_Generative_Post-Keyframing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Puppeteer-Rig-and-Animate-Your-3D-Models"><a href="#35-Puppeteer-Rig-and-Animate-Your-3D-Models" class="headerlink" title="35. Puppeteer: Rig and Animate Your 3D Models"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Puppeteer__Rig_and_Animate_Your_3D_Models.pdf">Puppeteer: Rig and Animate Your 3D Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>该论文提出了Puppeteer，一个集自动绑定骨骼与动画于一体的3D模型处理框架。方法包括利用自回归Transformer实现关节序列建模与骨架生成、拓扑感知注意力网络进行蒙皮权重预测，并通过可微分优化自动生成高质量动画。实验表明，Puppeteer在骨架预测、蒙皮精准度和动画稳定性方面优于现有方法，能高效处理多样3D内容。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Puppeteer_Rig_and_Animate_Your_3D_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-STREAM3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer"><a href="#36-STREAM3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer" class="headerlink" title="36. STREAM3R: Scalable Sequential 3D Reconstruction with Causal Transformer"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/STream3R__Scalable_Sequential_3D_Reconstruction_with_Causal_Transformer.pdf">STREAM3R: Scalable Sequential 3D Reconstruction with Causal Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>本文提出STREAM3R，将3D重建任务重构为基于因果注意力的解码器Transformer问题，实现对流式输入图像的高效顺序处理。通过缓存历史帧特征并逐帧递增融合，STREAM3R具备优异的扩展性和泛化能力，在静态与动态场景的深度估计及3D重建任务上均优于现有方法，且推理速度显著提升，推动实时3D理解发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/STREAM3R_Scalable_Sequential_3D_Reconstruction_with_Causal_Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-Medico-2025-Visual-Question-Answering-for-Gastrointestinal-Imaging"><a href="#37-Medico-2025-Visual-Question-Answering-for-Gastrointestinal-Imaging" class="headerlink" title="37. Medico 2025: Visual Question Answering for Gastrointestinal Imaging"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Medico_2025__Visual_Question_Answering_for_Gastrointestinal_Imaging.pdf">Medico 2025: Visual Question Answering for Gastrointestinal Imaging</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">SimulaMet - Simula Metropolitan Center for Digital Engineering</span></p><p>该论文提出了Medico 2025挑战，旨在推动面向胃肠道影像的可解释视觉问答（VQA）模型的发展，要求AI模型不仅准确回答医学影像中的临床问题，还需生成与医学推理一致的多模态解释。研究基于Kvasir-VQA-x1数据集，通过量化指标和专家审核结合评估AI系统的准确性和解释性，促进可被临床采用的可信医疗AI系统发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Medico_2025_Visual_Question_Answering_for_Gastrointestinal_Imaging.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-AEGIS-Authenticity-Evaluation-Benchmark-for-AI-Generated-Video-Sequences"><a href="#38-AEGIS-Authenticity-Evaluation-Benchmark-for-AI-Generated-Video-Sequences" class="headerlink" title="38. AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/AEGIS__Authenticity_Evaluation_Benchmark_for_AI-Generated_Video_Sequences.pdf">AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>该论文提出了AEGIS，一个专为AI生成视频真伪检测设计的大规模基准数据集，包含来自多种先进生成模型（如Stable Video Diffusion、CogVideoX-5B、KLing和Sora）的5,199个高真实性和语义复杂的合成视频，并配有丰富的多模态标注（语义描述、运动特征、低层视觉特征）。实验表明，现有视觉语言模型（如Qwen-VL、Video-LLaVA）在AEGIS的高难度测试集上表现有限，突出当前检测模型在真实环境中面临的泛化和鲁棒性挑战，强调了AEGIS在推动视频真伪检测研究中的基础性作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AEGIS_Authenticity_Evaluation_Benchmark_for_AI-Generated_Video_Sequences.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-Natively-Trainable-Sparse-Attention-for-Hierarchical-Point-Cloud-Datasets"><a href="#39-Natively-Trainable-Sparse-Attention-for-Hierarchical-Point-Cloud-Datasets" class="headerlink" title="39. Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Natively_Trainable_Sparse_Attention_for_Hierarchical_Point_Cloud_Datasets.pdf">Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Groningen</span></p><p>本文提出将Erwin层次化Transformer架构与Native Sparse Attention(NSA)机制结合，针对物理科学中的大规模点云数据，提出Erwin NSA模型，通过将NSA算法从序列数据扩展到非序列点云，并替换局部注意力机制，实现了更高效的全局感受野建模。实验在宇宙学模拟、分子动力学和空气压力建模三大数据集上，表明Erwin NSA在ShapeNet和分子动力学任务中优于原Erwin模型，在宇宙学任务上表现持平，且具有更高的训练效率和更好的节点信息流动能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Natively_Trainable_Sparse_Attention_for_Hierarchical_Point_Cloud_Datasets.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-Exploiting-Discriminative-Codebook-Prior-for-Autoregressive-Image-Generation"><a href="#40-Exploiting-Discriminative-Codebook-Prior-for-Autoregressive-Image-Generation" class="headerlink" title="40. Exploiting Discriminative Codebook Prior for Autoregressive Image Generation"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Exploiting_Discriminative_Codebook_Prior_for_Autoregressive_Image_Generation.pdf">Exploiting Discriminative Codebook Prior for Autoregressive Image Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>该论文提出了Discriminative Codebook Prior Extractor (DCPE)，通过聚合聚类和实例距离替代传统k-means聚类，从分布不均且高维的代码本令牌特征空间中更有效地挖掘令牌相似性信息，用于提升离散令牌自回归图像生成模型的训练。实验表明，DCPE作为即插即用方法能加速训练、提升生成质量，并优于现有k-means方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Exploiting_Discriminative_Codebook_Prior_for_Autoregressive_Image_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-Continuous-Bangla-Sign-Language-Translation-Mitigating-the-Expense-of-Gloss-Annotation-with-the-Assistance-of-Graph"><a href="#41-Continuous-Bangla-Sign-Language-Translation-Mitigating-the-Expense-of-Gloss-Annotation-with-the-Assistance-of-Graph" class="headerlink" title="41. Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Continuous_Bangla_Sign_Language_Translation__Mitigating_the_Expense_of_Gloss_Annotation_with_the_Ass.pdf">Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Dhaka</span></p><p>该论文提出了融合Transformer与STGCN-LSTM架构的连续手语翻译方法，通过多模态数据融合（RGB视频与骨骼关键点），实现了无需gloss注释的端到端手语到文本翻译。该方法在多个公开手语数据集（RWTH-PHOENIX-2014T、CSL-Daily、How2Sign、BornilDB v1.0）上取得了新的gloss-free翻译性能最优，显著提升了BLEU分数，为聋人群体的交流无障碍提供了技术基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Continuous_Bangla_Sign_Language_Translation_Mitigating_the_Expense_of_Gloss_Annotation_with_the_Assistance_of_Graph.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Hybrid-Generative-Fusion-for-Efficient-and-Privacy-Preserving-Face-Recognition-Dataset-Generation"><a href="#42-Hybrid-Generative-Fusion-for-Efficient-and-Privacy-Preserving-Face-Recognition-Dataset-Generation" class="headerlink" title="42. Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Hybrid_Generative_Fusion_for_Efficient_and_Privacy-Preserving_Face_Recognition_Dataset_Generation.pdf">Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Information Engineering, Chinese Academy of Sciences</span></p><p>本文提出了混合生成融合方法，通过Mixture-of-Experts数据清洗（聚类+GPT-4o验证）、Stable Diffusion与Vec2Face生成新身份，并用课程学习策略优化训练流程，实现高效、隐私保护的人脸识别数据集构建。实验表明，所生成的纯合成数据集在多规模下均提升了人脸识别模型性能，并在竞赛中取得第一名。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Hybrid_Generative_Fusion_for_Efficient_and_Privacy-Preserving_Face_Recognition_Dataset_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-Geospatial-Diffusion-for-Land-Cover-Imperviousness-Change-Forecasting"><a href="#43-Geospatial-Diffusion-for-Land-Cover-Imperviousness-Change-Forecasting" class="headerlink" title="43. Geospatial Diffusion for Land Cover Imperviousness Change Forecasting"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Geospatial_Diffusion_for_Land_Cover_Imperviousness_Change_Forecasting.pdf">Geospatial Diffusion for Land Cover Imperviousness Change Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Oak Ridge National Laboratory</span></p><p>本文提出一种基于生成扩散模型（Diffusion Models）的地表覆盖不透水性变化预测方法，将土地利用&#x2F;覆盖（LULC）变化预测视为条件数据合成任务，利用历史LULC及辅助数据（如过渡概率图）进行建模。实验表明，该方法在美国12个大都市区域实现了亚公里级别的预测精度，优于传统的无变化基线模型，能够有效捕捉历史数据中的时空发展模式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Geospatial_Diffusion_for_Land_Cover_Imperviousness_Change_Forecasting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera"><a href="#44-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera" class="headerlink" title="44. Processing and acquisition traces in visual encoders: What does CLIP know about your camera?"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Processing_and_acquisition_traces_in_visual_encoders__What_does_CLIP_know_about_your_camera_.pdf">Processing and acquisition traces in visual encoders: What does CLIP know about your camera?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Osaka</span></p><p>本论文系统分析了视觉编码器（如CLIP）在表征空间中对图片处理和采集参数（如JPEG压缩、相机型号等元数据）的敏感性，通过分类和检索任务展示这些元数据会被模型学习并影响语义预测。结论表明，尤其是视觉-语言对比模型（CVL），其表征空间中元数据痕迹可能会干扰下游语义任务，降低模型鲁棒性和泛化能力，且模型训练时是否采用强数据增强是影响元数据敏感性的关键因素。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP_know_about_your_camera.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-Object-Fidelity-Diffusion-for-Remote-Sensing-Image-Generation"><a href="#45-Object-Fidelity-Diffusion-for-Remote-Sensing-Image-Generation" class="headerlink" title="45. Object Fidelity Diffusion for Remote Sensing Image Generation"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Object_Fidelity_Diffusion_for_Remote_Sensing_Image_Generation.pdf">Object Fidelity Diffusion for Remote Sensing Image Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出了Object Fidelity Diffusion（OF-Diff），一种用于遥感图像生成的双分支可控扩散模型，通过引入增强形状生成模块（ESGM）从布局中提取对象先验形状，并结合扩散一致性损失与DDPO微调提升图像多样性与语义一致性。实验表明，OF-Diff能在无需真实图像参考的条件下生成高保真、布局与形状一致的遥感图像，显著提升小目标检测性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Object_Fidelity_Diffusion_for_Remote_Sensing_Image_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-VIDEO-BLADE-BLOCK-SPARSE-ATTENTION-MEETS-STEP-DISTILLATION-FOR-EFFICIENT-VIDEO-GENERATION"><a href="#46-VIDEO-BLADE-BLOCK-SPARSE-ATTENTION-MEETS-STEP-DISTILLATION-FOR-EFFICIENT-VIDEO-GENERATION" class="headerlink" title="46. VIDEO-BLADE: BLOCK-SPARSE ATTENTION MEETS STEP DISTILLATION FOR EFFICIENT VIDEO GENERATION"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Video-BLADE__Block-Sparse_Attention_Meets_Step_Distillation_for_Efficient_Video_Generation.pdf">VIDEO-BLADE: BLOCK-SPARSE ATTENTION MEETS STEP DISTILLATION FOR EFFICIENT VIDEO GENERATION</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了Video-BLADE框架，通过自适应块稀疏注意力（ASA）机制与基于轨迹分布匹配（TDM）的稀疏感知步蒸馏联合训练，实现高效视频扩散生成。ASA动态生成内容感知稀疏掩码，联合稀疏感知蒸馏提升推理速度，同时保持乃至提升视频生成质量。实验表明，在CogVideoX-5B和Wan2.1-1.3B等主流模型上，BLADE可实现8.89×至14.10×加速，且在VBench-2.0等基准上取得更高分数，兼顾效率与质量，且人类评测同样认可其优异表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/VIDEO-BLADE_BLOCK-SPARSE_ATTENTION_MEETS_STEP_DISTILLATION_FOR_EFFICIENT_VIDEO_GENERATION.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-From-Diagnosis-to-Improvement-Probing-Spatio-Physical-Reasoning-in-Vision-Language-Models"><a href="#47-From-Diagnosis-to-Improvement-Probing-Spatio-Physical-Reasoning-in-Vision-Language-Models" class="headerlink" title="47. From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/From_Diagnosis_to_Improvement__Probing_Spatio-Physical_Reasoning_in_Vision_Language_Models.pdf">From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tongji University</span></p><p>本文系统分析了主流视觉语言模型（VLM）在时空物理推理任务中的表现，发现其在基于ShapeStacks基准的静态平衡判断上准确率低，存在类人认知偏见和缺乏深度推理。作者提出采用监督微调（SFT）结合规则驱动的强化学习（RL）对Qwen2.5-VL-7B模型进行二阶段训练，显著提升了其领域内物理推理能力并超越了主流商用模型，但在跨场景泛化能力上仍表现有限，暴露出当前范式以模式匹配为主、难以习得可迁移物理原则的根本局限。结论：主流VLM在物理推理方面存在明显局限，二阶段微调虽可提升领域内性能但泛化不足，未来应发展基于物理因果机制的新范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From_Diagnosis_to_Improvement_Probing_Spatio-Physical_Reasoning_in_Vision_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-Privacy-enhancing-Sclera-Segmentation-Benchmarking-Competition-SSBC-2025"><a href="#48-Privacy-enhancing-Sclera-Segmentation-Benchmarking-Competition-SSBC-2025" class="headerlink" title="48. Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Privacy-enhancing_Sclera_Segmentation_Benchmarking_Competition__SSBC_2025.pdf">Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Ljubljana</span></p><p>本论文介绍了SSBC 2025隐私增强型巩膜分割竞赛，重点在于利用合成眼部图像训练隐私保护的巩膜分割模型，并系统比较了合成数据和真实数据混合训练的模型性能。结果表明，专用训练策略下，仅用合成数据训练的模型能取得与真实数据训练相近的分割效果，F1分数可达0.8以上，且方法选择往往比真实数据的加入更关键，突出合成数据在隐私保护生物识别中的可行性和前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Privacy-enhancing_Sclera_Segmentation_Benchmarking_Competition_SSBC_2025.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale"><a href="#49-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale" class="headerlink" title="49. NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/NextStep-1__Toward_Autoregressive_Image_Generation_with_Continuous_Tokens_at_Scale.pdf">NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">StepFun</span></p><p>该论文提出NextStep-1，一种结合大规模Transformer主干和流匹配头的自回归模型，实现文本到高保真图像生成和多样化图像编辑。方法创新在于同时处理离散文本和连续图像token，并引入强正则化的图像tokenizer以保证分布稳定，实验结果显示其在多项主流基准上超越现有自回归模型，部分指标接近扩散模型，验证了自回归范式在高质量图像生成上的可行性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/NextStep-1_Toward_Autoregressive_Image_Generation_with_Continuous_Tokens_at_Scale.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-Novel-View-Synthesis-using-DDIM-Inversion"><a href="#50-Novel-View-Synthesis-using-DDIM-Inversion" class="headerlink" title="50. Novel View Synthesis using DDIM Inversion"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Novel_View_Synthesis_using_DDIM_Inversion.pdf">Novel View Synthesis using DDIM Inversion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indraprastha Institute of Information Technology, Delhi</span></p><p>该论文提出了一种基于DDIM倒置和轻量化TUNet网络的单张图像高质量新视角合成方法。流程包括将输入图像通过VAE和DDIM倒置获得潜变量，使用TUNet进行视角转换，结合噪声融合策略强化高频细节，最后借助预训练扩散模型解码生成新视角图像。实验表明方法在MVImgNet等数据集上优于主流方法，能生成高分辨率且几何一致的新视角图像。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Novel_View_Synthesis_using_DDIM_Inversion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-Conditional-Information-Bottleneck-for-Multimodal-Fusion-Overcoming-Shortcut-Learning-in-Sarcasm-Detection"><a href="#51-Conditional-Information-Bottleneck-for-Multimodal-Fusion-Overcoming-Shortcut-Learning-in-Sarcasm-Detection" class="headerlink" title="51. Conditional Information Bottleneck for Multimodal Fusion: Overcoming Shortcut Learning in Sarcasm Detection"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Conditional_Information_Bottleneck_for_Multimodal_Fusion__Overcoming_Shortcut_Learning_in_Sarcasm_De.pdf">Conditional Information Bottleneck for Multimodal Fusion: Overcoming Shortcut Learning in Sarcasm Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Artificial Intelligence, Beijing Normal University</span></p><p>本文针对多模态讽刺检测任务中模型容易依赖数据集捷径（如角色标签、罐头笑声、情感不一致性）导致泛化能力差的问题，重构了MUStARD++数据集，去除了相关捷径信号，并提出了多模态条件信息瓶颈（MCIB）融合模型。MCIB通过条件信息瓶颈机制对多模态（文本、音频、视觉）特征进行冗余过滤与互补信息提取，实现高效融合，显著提升了在讽刺检测任务中的表现和泛化能力，实验结果在多个基准数据集上达到最优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Conditional_Information_Bottleneck_for_Multimodal_Fusion_Overcoming_Shortcut_Learning_in_Sarcasm_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-ChatENV-An-Interactive-Vision-Language-Model-for-Sensor-Guided-Environmental-Monitoring-and-Scenario-Simulation"><a href="#52-ChatENV-An-Interactive-Vision-Language-Model-for-Sensor-Guided-Environmental-Monitoring-and-Scenario-Simulation" class="headerlink" title="52. ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ChatENV__An_Interactive_Vision-Language_Model_for_Sensor-Guided_Environmental_Monitoring_and_Scenari.pdf">ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohamed bin Zayed University of Artificial Intelligence</span></p><p>本文提出ChatENV，一个结合卫星遥感图像与真实环境传感器数据的交互式视觉-语言模型。通过构建涵盖全球177k图像、152k时序对和丰富气象&#x2F;排放元数据的大规模数据集，并利用GPT-4o和Gemini 2.0生成多样注释，模型采用Qwen-2.5-VL为主干，LoRA高效微调，实现场景描述、时空变化推理及“假设情景”模拟。实验结果表明，ChatENV在时序推理和情景分析等任务上表现优异，显著提升了环境监测的智能化和实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ChatENV_An_Interactive_Vision-Language_Model_for_Sensor-Guided_Environmental_Monitoring_and_Scenario_Simulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-Fourier-Guided-Attention-Upsampling-for-Image-Super-Resolution"><a href="#53-Fourier-Guided-Attention-Upsampling-for-Image-Super-Resolution" class="headerlink" title="53. Fourier-Guided Attention Upsampling for Image Super-Resolution"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Fourier-Guided_Attention_Upsampling_for_Image_Super-Resolution.pdf">Fourier-Guided Attention Upsampling for Image Super-Resolution</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea University of Technology and Education (KOREATECH)</span></p><p>本文提出了一种轻量级的频域引导注意力上采样模块FGA，用于单幅图像超分辨率。该方法通过结合基于傅里叶特征的多层感知机(MLP)进行位置频率编码，跨分辨率相关注意力层进行空间自适应对齐，以及频域L1损失进行频谱一致性监督，在多种主流SISR骨干网络中均提升了重建高频细节和抑制伪影的能力。实验表明，FGA仅增加0.3M参数即可在五个不同超分辨率主干中获得平均0.12–0.14dB的PSNR提升，并在高频内容一致性上提升高达29%，在纹理丰富数据集上尤为显著，证明其在减少混叠和保持细节方面的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Fourier-Guided_Attention_Upsampling_for_Image_Super-Resolution.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-Towards-Agentic-AI-for-Multimodal-Guided-Video-Object-Segmentation"><a href="#54-Towards-Agentic-AI-for-Multimodal-Guided-Video-Object-Segmentation" class="headerlink" title="54. Towards Agentic AI for Multimodal-Guided Video Object Segmentation"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Towards_Agentic_AI_for_Multimodal-Guided_Video_Object_Segmentation.pdf">Towards Agentic AI for Multimodal-Guided Video Object Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Deakin University</span></p><p>本文提出M2-Agent，一种用于多模态指导下视频目标分割的免训练智能体系统。该方法利用大型语言模型（LLM）生成每个输入的动态推理流程，并通过与专门的视觉和音频工具集的迭代交互，灵活识别文本和音频描述的目标对象。实验结果表明，M2-Agent在Referring Video Object Segmentation（RVOS）和Reference Audio-Visual Segmentation（Ref-AVS）两项任务上，相较于现有训练自由方法有明显性能提升，且接近有监督最优方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Towards_Agentic_AI_for_Multimodal-Guided_Video_Object_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-GCRPNet-Graph-Enhanced-Contextual-and-Regional-Perception-Network-For-Salient-Object-Detection-in-Optical-Remote-Sensing-Images"><a href="#55-GCRPNet-Graph-Enhanced-Contextual-and-Regional-Perception-Network-For-Salient-Object-Detection-in-Optical-Remote-Sensing-Images" class="headerlink" title="55. GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GCRPNet__Graph-Enhanced_Contextual_and_Regional_Perception_Network_For_Salient_Object_Detection_in_O.pdf">GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hainan University</span></p><p>该论文提出了一种基于Mamba架构的光学遥感图像显著性目标检测新方法GCRPNet，集成了视觉状态空间编码器、差异-相似性引导分层图注意模块（DS-HGAM）和局部增强视觉状态空间（LEVSS）解码器。DS-HGAM利用图神经网络实现多尺度特征融合和跨空间建模，LEVSS结合多尺度协同注意力和块内四方向扫描提升局部细节表示。实验结果表明，GCRPNet在ORSSD和EORSSD数据集上显著优于现有方法，具备更强的鲁棒性和精度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GCRPNet_Graph-Enhanced_Contextual_and_Regional_Perception_Network_For_Salient_Object_Detection_in_Optical_Remote_Sensing_Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-PASS-Probabilistic-Agentic-Supernet-Sampling-for-Interpretable-and-Adaptive-Chest-X-Ray-Reasoning"><a href="#56-PASS-Probabilistic-Agentic-Supernet-Sampling-for-Interpretable-and-Adaptive-Chest-X-Ray-Reasoning" class="headerlink" title="56. PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/PASS__Probabilistic_Agentic_Supernet_Sampling_for_Interpretable_and_Adaptive_Chest_X-Ray_Reasoning.pdf">PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Hong Kong</span></p><p>该论文提出PASS框架，通过概率化的agentic supernet采样实现胸部X光的多模态医学推理，核心包括动态采样多工具工作流并用可解释概率标注决策路径。方法包含三阶段训练：专家知识热身、对比路径排序和成本感知强化学习，有效优化准确率与计算成本权衡。实验表明PASS在多项医学推理基准上显著优于主流模型，在保证高准确率的同时实现高可解释性和效率，可广泛用于高安全性要求的医学场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PASS_Probabilistic_Agentic_Supernet_Sampling_for_Interpretable_and_Adaptive_Chest_X-Ray_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-X-Node-Self-Explanation-is-All-We-Need"><a href="#57-X-Node-Self-Explanation-is-All-We-Need" class="headerlink" title="57. X-Node: Self-Explanation is All We Need"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/X-Node__Self-Explanation_is_All_We_Need.pdf">X-Node: Self-Explanation is All We Need</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>该论文提出了一种自解释型图神经网络框架X-Node，在节点分类过程中每个节点能生成自身解释，方法包括构建结构化上下文向量、解释向量生成、LLM文本解释和解释反馈注入GNN流程。实验证明X-Node在MedMNIST和MorphoMNIST图数据集上保持竞争分类性能并生成每节点可解释决策，有效提升模型透明度和临床应用信任度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/X-Node_Self-Explanation_is_All_We_Need.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-Multi-Label-Plant-Species-Prediction-with-Metadata-Enhanced-Multi-Head-Vision-Transformers"><a href="#58-Multi-Label-Plant-Species-Prediction-with-Metadata-Enhanced-Multi-Head-Vision-Transformers" class="headerlink" title="58. Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Multi-Label_Plant_Species_Prediction_with_Metadata-Enhanced_Multi-Head_Vision_Transformers.pdf">Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Hamburg</span></p><p>本论文提出了一种基于多头Vision Transformer（ViT）的多标签植物种类预测方法，结合了DINOv2预训练模型、物种-属-科多头分类、元数据增强、多尺度切片、动态阈值优化和集成策略（Hydra架构），以实现对高分辨率植被样方图像中多种植物的自动识别。实验结果显示，该方法在PlantCLEF 2025挑战赛的公共榜单中取得了第三名的成绩，但在私有榜单上表现下降，说明模型对领域转移较为敏感但仍具竞争力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Label_Plant_Species_Prediction_with_Metadata-Enhanced_Multi-Head_Vision_Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-From-Images-to-Perception-Emergence-of-Perceptual-Properties-by-Reconstructing-Images"><a href="#59-From-Images-to-Perception-Emergence-of-Perceptual-Properties-by-Reconstructing-Images" class="headerlink" title="59. From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/From_Images_to_Perception__Emergence_of_Perceptual_Properties_by_Reconstructing_Images.pdf">From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Valencia</span></p><p>本论文提出并训练了一种生物启发的视觉系统神经网络PerceptNet，通过自监督学习（包括自编码、去噪、去模糊和稀疏正则化任务）重建自然图像，研究模型内部表征与人类感知一致性的涌现现象。实验表明，在编码器（类V1皮层）阶段，模型表征与人类主观失真评价高度相关，适度的噪声、模糊和稀疏度可增强这种一致性，说明生物视觉系统可能正是优化了去除适度失真和稀疏性，且无需人工感知标签即可学得感知度量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From_Images_to_Perception_Emergence_of_Perceptual_Properties_by_Reconstructing_Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-MM-Food-100K-A-100-000-Sample-Multimodal-Food-Intelligence-Dataset-with-Verifiable-Provenance"><a href="#60-MM-Food-100K-A-100-000-Sample-Multimodal-Food-Intelligence-Dataset-with-Verifiable-Provenance" class="headerlink" title="60. MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MM-Food-100K__A_100,000-Sample_Multimodal_Food_Intelligence_Dataset_with_Verifiable_Provenance.pdf">MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Codatta Community</span></p><p>该论文提出了MM-Food-100K，一种包含10万条样本的多模态食物智能数据集，通过社区贡献与大型视觉语言模型自动质量审核结合的数据采集流程，提供丰富、多层次的注释及可验证溯源。实验表明，基于MM-Food-100K微调的模型在食物分类与营养回归任务上均优于原始视觉语言模型，且采用区块链协议实现数据溯源与贡献者分润机制，为高质量社区数据集的可持续建设提供了新范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MM-Food-100K_A_100_000-Sample_Multimodal_Food_Intelligence_Dataset_with_Verifiable_Provenance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-Adapting-SAM-via-Cross-Entropy-Masking-for-Class-Imbalance-in-Remote-Sensing-Change-Detection"><a href="#61-Adapting-SAM-via-Cross-Entropy-Masking-for-Class-Imbalance-in-Remote-Sensing-Change-Detection" class="headerlink" title="61. Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Adapting_SAM_via_Cross-Entropy_Masking_for_Class_Imbalance_in_Remote_Sensing_Change_Detection.pdf">Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Sydney</span></p><p>该论文提出了基于Segment Anything Model (SAM) 的遥感变化检测框架，通过细致地微调FastSAM编码器，并引入空间-时序特征增强（STFE）和多尺度解码器融合（MSDF），有效提升了多尺度下的变化检测能力。针对变化检测中的类别不平衡问题，创新性地提出了交叉熵掩码损失（CEM），通过在损失计算中随机丢弃未变化像素，显著提升了模型对变化类别的检测能力。实验结果表明，所提方法在Levir-CD、WHU-CD、CLCD和S2Looking等四个变化检测数据集上均取得了优于现有方法的性能，尤其在S2Looking数据集上F1提升2.5%。结论是结合SAM密集预测特性和CEM损失的架构在遥感变化检测任务上优于现有主流方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Adapting_SAM_via_Cross-Entropy_Masking_for_Class_Imbalance_in_Remote_Sensing_Change_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-PTQAT-A-Hybrid-Parameter-Efficient-Quantization-Algorithm-for-3D-Perception-Tasks"><a href="#62-PTQAT-A-Hybrid-Parameter-Efficient-Quantization-Algorithm-for-3D-Perception-Tasks" class="headerlink" title="62. PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/PTQAT__A_Hybrid_Parameter-Efficient_Quantization_Algorithm_for_3D_Perception_Tasks.pdf">PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>该论文提出PTQAT，一种结合后训练量化（PTQ）和量化感知训练（QAT）的高效混合量化算法，专为3D感知任务设计。方法首先通过PTQ预检查各层量化误差，采用均方误差（MSE）筛选需QAT微调的关键层，其余层保持冻结，重点在误差传播阶段进行补偿。结论表明，PTQAT在3D目标检测、语义分割、占用预测等任务上，能以更少参数微调和更高效率，实现与甚至优于全量QAT的精度，同时便于实际部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PTQAT_A_Hybrid_Parameter-Efficient_Quantization_Algorithm_for_3D_Perception_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-EgoMusic-driven-Human-Dance-Motion-Estimation-with-Skeleton-Mamba"><a href="#63-EgoMusic-driven-Human-Dance-Motion-Estimation-with-Skeleton-Mamba" class="headerlink" title="63. EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/EgoMusic-driven_Human_Dance_Motion_Estimation_with_Skeleton_Mamba.pdf">EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">FPT Software AI Center</span></p><p>本文提出EgoMusic Motion Network和核心Skeleton Mamba模型，实现从第一视角视频和音乐共同驱动的人类舞蹈动作估计。方法包括新型大规模EgoAIST++数据集、多模态特征融合、条件扩散模型和空间-时间扫描策略，实验显示在多项舞蹈动作估计指标上明显优于现有方法，并能泛化至实际场景。结论：联合利用视觉和音乐输入可大幅提升人体舞蹈动作推断的准确性和协调性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/EgoMusic-driven_Human_Dance_Motion_Estimation_with_Skeleton_Mamba.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-TweezeEdit-Consistent-and-Efficient-Image-Editing-with-Path-Regularization"><a href="#64-TweezeEdit-Consistent-and-Efficient-Image-Editing-with-Path-Regularization" class="headerlink" title="64. TweezeEdit: Consistent and Efficient Image Editing with Path Regularization"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/TweezeEdit__Consistent_and_Efficient_Image_Editing_with_Path_Regularization.pdf">TweezeEdit: Consistent and Efficient Image Editing with Path Regularization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>本文提出TweezeEdit，一种基于一致性模型的无调优、无反演路径正则化图像编辑框架，针对扩散模型下现有方法过度对齐目标提示、难以保留源图像语义的问题。该方法通过全程正则化去噪路径并引入梯度驱动的路径约束，在无需模型结构修改的情况下，提高语义保留和编辑效率。实验结果显示，TweezeEdit在语义一致性、目标对齐及推理速度上优于现有主流方法，可在12步内完成编辑，具备实时应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/TweezeEdit_Consistent_and_Efficient_Image_Editing_with_Path_Regularization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-STAMP-Multi-pattern-Attention-aware-Multiple-Instance-Learning-for-STAS-Diagnosis-in-Multi-center-Histopathology-Images"><a href="#65-STAMP-Multi-pattern-Attention-aware-Multiple-Instance-Learning-for-STAS-Diagnosis-in-Multi-center-Histopathology-Images" class="headerlink" title="65. STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/STAMP__Multi-pattern_Attention-aware_Multiple_Instance_Learning_for_STAS_Diagnosis_in_Multi-center_H.pdf">STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hunan University</span></p><p>该论文提出STAMP方法，通过双分支Transformer架构和多模式注意力聚合模块，分析多中心肺癌组织病理图像中的STAS特征，实现病灶区域的动态选择和噪声抑制，提升全局表征的判别力。实验表明STAMP在三大多中心数据集上AUC均超过0.79，诊断性能优于临床水平，具有临床辅助诊断潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/STAMP_Multi-pattern_Attention-aware_Multiple_Instance_Learning_for_STAS_Diagnosis_in_Multi-center_Histopathology_Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-NanoControl-A-Lightweight-Framework-for-Precise-and-Efficient-Control-in-Diffusion-Transformer"><a href="#66-NanoControl-A-Lightweight-Framework-for-Precise-and-Efficient-Control-in-Diffusion-Transformer" class="headerlink" title="66. NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/NanoControl__A_Lightweight_Framework_for_Precise_and_Efficient_Control_in_Diffusion_Transformer.pdf">NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">360 AI Research</span></p><p>本文提出NanoControl——一种基于Diffusion Transformer（DiT）的轻量级可控文本到图像生成方法。其创新点在于采用LoRA风格控制模块和KV-Context Augmentation机制，极大减少参数与计算量，仅增加0.024%参数和0.029% GFLOPs，实现高效条件融合与精确可控生成。实验表明，NanoControl在多项条件控制任务（如边缘、深度、着色、HED）上实现了优于现有方法的可控性和图像质量，同时大幅降低模型复杂度，非常适合实际部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/NanoControl_A_Lightweight_Framework_for_Precise_and_Efficient_Control_in_Diffusion_Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-Towards-Spatially-Consistent-Image-Generation-On-Incorporating-Intrinsic-Scene-Properties-into-Diffusion-Models"><a href="#67-Towards-Spatially-Consistent-Image-Generation-On-Incorporating-Intrinsic-Scene-Properties-into-Diffusion-Models" class="headerlink" title="67. Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Towards_Spatially_Consistent_Image_Generation__On_Incorporating_Intrinsic_Scene_Properties_into_Diff.pdf">Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AI Institute, Seoul National University</span></p><p>本文提出了Intrinsic Latent Diffusion Model (I-LDM)，通过联合生成图片和其对应的场景内在属性（深度、法线、分割图、线描），提升文本到图像生成模型的空间一致性。方法利用预训练估算器自动提取场景属性，通过自注意力跨域调度和VAE编码实现高效协同生成。实验显示I-LDM在多种基线模型和复杂场景下均显著改善了空间布局的自然性和一致性，同时保持了原始模型的图像质量和文本匹配能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Towards_Spatially_Consistent_Image_Generation_On_Incorporating_Intrinsic_Scene_Properties_into_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning"><a href="#68-SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning" class="headerlink" title="68. SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SynBrain__Enhancing_Visual-to-fMRI_Synthesis_via_Probabilistic_Representation_Learning.pdf">SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Artificial Intelligence Laboratory</span></p><p>该论文提出了SynBrain框架，通过引入概率建模与语义条件约束，采用BrainVAE和S2N Mapper关键模块，实现视觉刺激到fMRI脑响应的概率分布建模和一步语义到神经映射。实验结果表明，SynBrain在个体特异性编码和少样本自适应性能上显著优于现有方法，并生成高质量fMRI信号提升神经解码效果，同时揭示脑功能的一致性和生物神经变异性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SynBrain_Enhancing_Visual-to-fMRI_Synthesis_via_Probabilistic_Representation_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-DINOMotion-advanced-robust-tissue-motion-tracking-with-DINOv2-in-2D-Cine-MRI-guided-radiotherapy"><a href="#69-DINOMotion-advanced-robust-tissue-motion-tracking-with-DINOv2-in-2D-Cine-MRI-guided-radiotherapy" class="headerlink" title="69. DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/DINOMotion__advanced_robust_tissue_motion_tracking_with_DINOv2_in_2D-Cine_MRI-guided_radiotherapy.pdf">DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Concordia University</span></p><p>本文提出DINOMotion，一种基于DINOv2基础视觉模型和Low-Rank Adaptation (LoRA) 层的深度学习框架，用于2D-Cine MRI引导放疗中的组织运动追踪。该方法自动检测对应标志点以实现高效和可解释的图像配准，并能处理大幅度错位情况，在多个器官的Dice分数和Hausdorff距离评测中均优于现有方法，具有较强的实时性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/DINOMotion_advanced_robust_tissue_motion_tracking_with_DINOv2_in_2D-Cine_MRI-guided_radiotherapy.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-SC-Lane-Slope-aware-and-Consistent-Road-Height-Estimation-Framework-for-3D-Lane-Detection"><a href="#70-SC-Lane-Slope-aware-and-Consistent-Road-Height-Estimation-Framework-for-3D-Lane-Detection" class="headerlink" title="70. SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SC-Lane__Slope-aware_and_Consistent_Road_Height_Estimation_Framework_for_3D_Lane_Detection.pdf">SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University</span></p><p>本文提出了SC-Lane，一种面向坡度且具有时序一致性的3D车道检测道路高度估计框架。方法核心包括坡度自适应特征模块，动态融合多坡度高度特征，并引入高度一致性模块，通过利用图像线索自适应加权及时序一致性损失，提升多帧高度估计的鲁棒性和准确性。实验采用MAE、RMSE和阈值精度等新标准，OpenLane基准上SC-Lane在高度估计和3D车道检测上均达到SOTA，F-score 64.3%。结论表明该方法提升了车道检测的准确性和高度估计的稳定性，可扩展至自动驾驶相关任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SC-Lane_Slope-aware_and_Consistent_Road_Height_Estimation_Framework_for_3D_Lane_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-Translation-of-Text-Embedding-via-Delta-Vector-to-Suppress-Strongly-Entangled-Content-in-Text-to-Image-Diffusion-Models"><a href="#71-Translation-of-Text-Embedding-via-Delta-Vector-to-Suppress-Strongly-Entangled-Content-in-Text-to-Image-Diffusion-Models" class="headerlink" title="71. Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Translation_of_Text_Embedding_via_Delta_Vector_to_Suppress_Strongly_Entangled_Content_in_Text-to-Ima.pdf">Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sungkyunkwan University</span></p><p>本文提出了通过在文本嵌入空间引入delta vector来直接抑制文本到图像扩散模型中强关联负内容的方法，并进一步提出了Selective Suppression with Delta Vector（SSDV），将delta vector集成到跨注意力机制，实现对特定区域负内容的精准抑制。实验结果表明，该方法在标准和个性化模型上均显著优于现有基线，在定量和定性评测中实现了更有效的强关联负内容抑制。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Translation_of_Text_Embedding_via_Delta_Vector_to_Suppress_Strongly_Entangled_Content_in_Text-to-Image_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-PQ-DAF-Pose-driven-Quality-controlled-Data-Augmentation-for-Data-scarce-Driver-Distraction-Detection"><a href="#72-PQ-DAF-Pose-driven-Quality-controlled-Data-Augmentation-for-Data-scarce-Driver-Distraction-Detection" class="headerlink" title="72. PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/PQ-DAF__Pose-driven_Quality-controlled_Data_Augmentation_for_Data-scarce_Driver_Distraction_Detectio.pdf">PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shandong University of Science and Technology</span></p><p>本文提出了PQ-DAF（Pose-driven Quality-controlled Data Augmentation Framework），结合姿态驱动的渐进式条件扩散模型（PCDMs）和视觉-语言模型（CogVLM）实现高质量驾驶员分心检测数据增强。方法通过提取驾驶员关键点姿态，利用PCDMs生成结构一致的伪样本，并用CogVLM对合成样本进行语义一致性筛选，大幅提升少样本下的模型泛化能力。实验结果显示，PQ-DAF在StateFarm和AUC-DDD数据集少样本设定下均显著提升检测准确率，验证了其在数据稀缺场景下的实用性和有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PQ-DAF_Pose-driven_Quality-controlled_Data_Augmentation_for_Data-scarce_Driver_Distraction_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-Unlocking-Robust-Semantic-Segmentation-Performance-via-Label-only-Elastic-Deformations-against-Implicit-Label-Noise"><a href="#73-Unlocking-Robust-Semantic-Segmentation-Performance-via-Label-only-Elastic-Deformations-against-Implicit-Label-Noise" class="headerlink" title="73. Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Unlocking_Robust_Semantic_Segmentation_Performance_via_Label-only_Elastic_Deformations_against_Impli.pdf">Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">GIST</span></p><p>本文提出了一种名为NSegment+的新型数据增强框架，专为语义分割中的隐性标签噪声问题设计。方法核心在于仅对分割标签施加随机弹性形变（结合高斯平滑和尺度感知的小目标保护），而保持输入图像不变，从而模拟真实数据中的边界模糊和标注不一致，提升模型对标签不确定性的鲁棒性。大量实验表明，NSegment+在遥感和自然场景六大主流数据集上，平均mIoU提升显著，且与CutMix、Label Smoothing等现有增强&#x2F;正则化手段兼容，无需增加计算开销。结论认为，NSegment+为解决现实语义分割中的隐性标签噪声提供了轻量、高效且通用的解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Unlocking_Robust_Semantic_Segmentation_Performance_via_Label-only_Elastic_Deformations_against_Implicit_Label_Noise.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-AtomDiffuser-Time-Aware-Degradation-Modeling-for-Drift-and-Beam-Damage-in-STEM-Imaging"><a href="#74-AtomDiffuser-Time-Aware-Degradation-Modeling-for-Drift-and-Beam-Damage-in-STEM-Imaging" class="headerlink" title="74. AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/AtomDiffuser__Time-Aware_Degradation_Modeling_for_Drift_and_Beam_Damage_in_STEM_Imaging.pdf">AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Clemson University</span></p><p>该论文提出AtomDiffuser，一种结合时序卷积U-Net和物理启发降解建模的深度学习框架，能够从时序STEM图像中解耦并显式预测原子级漂移（通过仿射变换）和束流损伤（通过空间衰减图），利用合成降解序列训练并能泛化到真实冷冻STEM数据。实验表明，AtomDiffuser在损伤评估和漂移矫正任务上显著优于现有CNN方法，支持高分辨率降解推断和动态材料结构演化的可视化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AtomDiffuser_Time-Aware_Degradation_Modeling_for_Drift_and_Beam_Damage_in_STEM_Imaging.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-Improving-OCR-for-Historical-Texts-of-Multiple-Languages"><a href="#75-Improving-OCR-for-Historical-Texts-of-Multiple-Languages" class="headerlink" title="75. Improving OCR for Historical Texts of Multiple Languages"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Improving_OCR_for_Historical_Texts_of_Multiple_Languages.pdf">Improving OCR for Historical Texts of Multiple Languages</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Groningen</span></p><p>该论文提出了多语言历史文献的OCR与文档版面分析方法。针对死海古卷希伯来文，采用数据增强并比较了Kraken与TrOCR模型效果；在荷兰历史会议记录上，结合DeepLabV3+语义分割与BiLSTM，并通过伪标签提升训练效果；在现代英文手写识别任务中，采用CRNN与ResNet34编码并使用CTC损失。实验表明：数据增强与转移学习能显著提升历史文献OCR，伪标签自监督策略能有效提升文档结构分析的性能，复杂模型在小数据集上未必优于简单架构，未来需加强数据多样性与模型设计的平衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving_OCR_for_Historical_Texts_of_Multiple_Languages.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-Efficient-Image-Denoising-Using-Global-and-Local-Circulant-Representation"><a href="#76-Efficient-Image-Denoising-Using-Global-and-Local-Circulant-Representation" class="headerlink" title="76. Efficient Image Denoising Using Global and Local Circulant Representation"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Efficient_Image_Denoising_Using_Global_and_Local_Circulant_Representation.pdf">Efficient Image Denoising Using Global and Local Circulant Representation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China University of Technology</span></p><p>本文提出了一种高效的图像去噪算法Haar-tSVD，通过结合全球和局部循环结构，利用t-SVD和Haar变换在循环表示下有效捕获图像块的相关性，实现一步高并行可扩展的滤波，无需学习局部基。方法还引入CNN自适应噪声估计和特征值分析提升鲁棒性。实验表明，该方法在图像、视频、HSI、MRI等多任务中具有效果和效率并重的优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Efficient_Image_Denoising_Using_Global_and_Local_Circulant_Representation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-Improving-Learning-of-New-Diseases-through-Knowledge-Enhanced-Initialization-for-Federated-Adapter-Tuning"><a href="#77-Improving-Learning-of-New-Diseases-through-Knowledge-Enhanced-Initialization-for-Federated-Adapter-Tuning" class="headerlink" title="77. Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Improving_Learning_of_New_Diseases_through_Knowledge-Enhanced_Initialization_for_Federated_Adapter_T.pdf">Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR)</span></p><p>本文提出了一种名为FedKEI的联邦知识增强初始化框架，结合了全局聚类和双层聚合权重学习，实现了医疗领域中基础模型适配器调优过程中新任务的高效适应。FedKEI通过服务器端对任务特定模块进行聚类，并采用双层优化策略协同学习聚合权重，有效整合并转移跨任务与跨客户的知识，从而为适配器和头部生成更优的初始化。实验结果显示，FedKEI在皮肤、胸部X光和视网膜OCT等多模态医学影像数据集上，在新疾病适应任务中均超过现有最新方法，提升了新任务学习的准确性和效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving_Learning_of_New_Diseases_through_Knowledge-Enhanced_Initialization_for_Federated_Adapter_Tuning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-A-Sub-Pixel-Multimodal-Optical-Remote-Sensing-Images-Matching-Method"><a href="#78-A-Sub-Pixel-Multimodal-Optical-Remote-Sensing-Images-Matching-Method" class="headerlink" title="78. A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Sub-Pixel_Multimodal_Optical_Remote_Sensing_Images_Matching_Method.pdf">A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Central South University</span></p><p>本文提出了一种基于相位一致性加权最小绝对偏差（PCWLAD）的亚像素多模态光学遥感图像匹配方法。方法流程包括：首先利用结构相似性（SSIM）在未降噪的相位一致性（PC）图上进行粗匹配，然后通过互结构加权和加权最小绝对偏差（WLAD）准则，实现亚像素级别的精确匹配。实验结果显示，在三个类型的可见-红外遥感数据集上，PCWLAD在匹配正确率（CMR）和均方根误差（RMSE）方面均优于现有七种主流方法，平均RMSE约为0.4像素；但方法在大几何变形场景下效果有限。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Sub-Pixel_Multimodal_Optical_Remote_Sensing_Images_Matching_Method.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-Deep-Learning-for-Crack-Detection-A-Review-of-Learning-Paradigms-Generalizability-and-Datasets"><a href="#79-Deep-Learning-for-Crack-Detection-A-Review-of-Learning-Paradigms-Generalizability-and-Datasets" class="headerlink" title="79. Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Deep_Learning_for_Crack_Detection__A_Review_of_Learning_Paradigms,_Generalizability,_and_Datasets.pdf">Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Georgia Institute of Technology</span></p><p>本论文系统回顾了基于深度学习的裂缝检测领域，聚焦于监督、半监督、弱监督、无监督、少样本、领域自适应及基础模型等多种学习范式的技术演进和泛化能力，详述了主流方法（如CNN、Transformer、SAM等）、关键流程及代表性数据集，同时发布了高分辨率3D激光扫描裂缝数据集3DCrack。结论指出，未来需进一步突破数据高效利用、模型泛化、标准化评测与多模态融合等挑战，以实现更鲁棒、可扩展的裂缝自动检测系统。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Deep_Learning_for_Crack_Detection_A_Review_of_Learning_Paradigms_Generalizability_and_Datasets.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="80-Performance-of-GPT-5-in-Brain-Tumor-MRI-Reasoning"><a href="#80-Performance-of-GPT-5-in-Brain-Tumor-MRI-Reasoning" class="headerlink" title="80. Performance of GPT-5 in Brain Tumor MRI Reasoning"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Performance_of_GPT-5_in_Brain_Tumor_MRI_Reasoning.pdf">Performance of GPT-5 in Brain Tumor MRI Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Emory University School of Medicine</span></p><p>本论文系统评估了GPT-4o、GPT-5-nano、GPT-5-mini和GPT-5在三类脑肿瘤MRI多模态视觉问答(VQA)基准上的表现，方法包括将BraTS数据集的多序列MRI与结构化临床特征转化为标准化VQA问题，并采用zero-shot chain-of-thought提示进行模型推理与准确率对比。结果显示，GPT-5-mini取得最高平均准确率(44.19%)，但各模型性能相近，尚未达到临床实用水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Performance_of_GPT-5_in_Brain_Tumor_MRI_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-A-Survey-on-Diffusion-Language-Models"><a href="#81-A-Survey-on-Diffusion-Language-Models" class="headerlink" title="81. A Survey on Diffusion Language Models"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Survey_on_Diffusion_Language_Models.pdf">A Survey on Diffusion Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohamed bin Zayed University of Artificial Intelligence</span></p><p>本文系统梳理了扩散语言模型（DLMs）的原理、发展、技术和挑战，涵盖连续、离散及混合扩散模型，并细致分析了其预训练、后训练（包括强化学习对齐）、推理优化、并行解码、缓存和多模态扩展等关键技术流程。结论指出，DLMs在推理速度、上下文建模和多模态统一方面展现出独特优势，与主流自回归模型性能相当，同时也面临并行一致性、基础设施和可扩展性等挑战，未来具备广阔研究与应用前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Survey_on_Diffusion_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-Reinforced-Language-Models-for-Sequential-Decision-Making"><a href="#82-Reinforced-Language-Models-for-Sequential-Decision-Making" class="headerlink" title="82. Reinforced Language Models for Sequential Decision Making"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Reinforced_Language_Models_for_Sequential_Decision_Making.pdf">Reinforced Language Models for Sequential Decision Making</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Southampton</span></p><p>本文提出了Multi-Step Group-Relative Policy Optimization（MS-GRPO）算法，通过在Text-Mediated Stochastic Game（TMSG）和Language Agent Policy（LAP）框架下，将全局回报分配到每个决策步骤，并结合绝对优势加权（AAW）采样策略，以提升小型LLM的序列决策能力。实验表明，经过该方法微调的3B参数模型在Frozen Lake任务上决策能力提升，超越了72B参数的大模型50%；同时方法在效率和泛化能力上也表现出优势，但与专用DQN方法相比仍存在性能差距。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Reinforced_Language_Models_for_Sequential_Decision_Making.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-Memory-Augmented-Transformers-A-Systematic-Review-from-Neuroscience-Principles-to-Technical-Solutions"><a href="#83-Memory-Augmented-Transformers-A-Systematic-Review-from-Neuroscience-Principles-to-Technical-Solutions" class="headerlink" title="83. Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Memory-Augmented_Transformers__A_Systematic_Review_from_Neuroscience_Principles_to_Technical_Solutio.pdf">Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Technologies</span></p><p>本文系统综述了基于神经科学原理（如多时间尺度动态记忆、选择性注意和巩固）与工程实现相结合的Memory-Augmented Transformers技术，提出了三维分类体系，分析了核心操作（读写、遗忘、容量管理）及其实现方式，展示了领域从静态缓存向自适应推理和终身学习系统的演变。结论指出，混合存储、自适应动态和智能遗忘成为主流，但在可扩展性、干扰协调和评测标准化等方面仍面临挑战，未来需借鉴生物记忆机制推动认知型AI发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Memory-Augmented_Transformers_A_Systematic_Review_from_Neuroscience_Principles_to_Technical_Solutions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-The-Knowledge-Reasoning-Dissociation-Fundamental-Limitations-of-LLMs-in-Clinical-Natural-Language-Inference"><a href="#84-The-Knowledge-Reasoning-Dissociation-Fundamental-Limitations-of-LLMs-in-Clinical-Natural-Language-Inference" class="headerlink" title="84. The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/The_Knowledge-Reasoning_Dissociation__Fundamental_Limitations_of_LLMs_in_Clinical_Natural_Language_I.pdf">The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Manchester</span></p><p>该论文提出了一个针对临床推理的自然语言推断（NLI）基准，包括四类推理任务（因果归因、组合性基础、认知验证和风险抽象），并设计了GKMRV探针用于区分知识检索与推理能力。实验发现，当前主流大语言模型在知识检索任务上表现极佳，但在结构化推理任务上普遍失败，结论认为仅靠模型规模扩展无法实现可靠的临床推理，需要更强的可组合内部表示。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/The_Knowledge-Reasoning_Dissociation_Fundamental_Limitations_of_LLMs_in_Clinical_Natural_Language_Inference.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-Modeling-Human-Responses-to-Multimodal-AI-Content"><a href="#85-Modeling-Human-Responses-to-Multimodal-AI-Content" class="headerlink" title="85. Modeling Human Responses to Multimodal AI Content"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Modeling_Human_Responses_to_Multimodal_AI_Content.pdf">Modeling Human Responses to Multimodal AI Content</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Salesforce Research Asia</span></p><p>该论文提出了T-Lens系统，通过人类中心的方法预测用户对多模态AI生成内容（AIGC）的反应，核心模块HR-MCP基于大规模人类标注数据，结合多模态一致性和情感一致性，能量化信任度、影响和开放性等指标，并可无缝集成至任意LLM。实验表明，T-Lens在模拟人类对多模态信息的信任与传播倾向上优于现有大模型及传统方法，为AI驱动的虚假信息治理提供了新工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Modeling_Human_Responses_to_Multimodal_AI_Content.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-GenOM-Ontology-Matching-with-Description-Generation-and-Large-Language-Model"><a href="#86-GenOM-Ontology-Matching-with-Description-Generation-and-Large-Language-Model" class="headerlink" title="86. GenOM: Ontology Matching with Description Generation and Large Language Model"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GenOM__Ontology_Matching_with_Description_Generation_and_Large_Language_Model.pdf">GenOM: Ontology Matching with Description Generation and Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Manchester</span></p><p>本文提出了GenOM，一种结合大语言模型（LLM）与文本定义生成的本体对齐框架。其方法流程包括：从本体中提取结构与词汇信息，用LLM生成包含领域知识的文本定义，利用嵌入模型进行候选检索，再通过LLM判别候选对的语义等价性，并与传统精确匹配工具融合提升精度。实验表明，GenOM在OAEI Bio-ML基准上表现优异，超越多种传统与现有LLM方案，且模块化设计和定义增强策略有效提升了泛化性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GenOM_Ontology_Matching_with_Description_Generation_and_Large_Language_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-Learning-from-Natural-Language-Feedback-for-Personalized-Question-Answering"><a href="#87-Learning-from-Natural-Language-Feedback-for-Personalized-Question-Answering" class="headerlink" title="87. Learning from Natural Language Feedback for Personalized Question Answering"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Learning_from_Natural_Language_Feedback_for_Personalized_Question_Answering.pdf">Learning from Natural Language Feedback for Personalized Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Massachusetts Amherst</span></p><p>该论文提出VAC框架，用自然语言反馈（NLF）替代传统个性化问答中用于训练大语言模型的标量奖励信号。VAC通过交替优化反馈模型和策略模型，利用用户画像和问题叙述生成个性化反馈，有效提升模型的个性化响应能力。实验表明，VAC在LaMP-QA基准测试上显著优于现有方法，并被人工评价为更优，表明自然语言反馈为个性化问答优化提供了更有效的信号。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning_from_Natural_Language_Feedback_for_Personalized_Question_Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-FROGENT-An-End-to-End-Full-process-Drug-Design-Agent"><a href="#88-FROGENT-An-End-to-End-Full-process-Drug-Design-Agent" class="headerlink" title="88. FROGENT: An End-to-End Full-process Drug Design Agent"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/FROGENT__An_End-to-End_Full-process_Drug_Design_Agent.pdf">FROGENT: An End-to-End Full-process Drug Design Agent</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Artificial Intelligence, Shenzhen University</span></p><p>FROGENT is an agentic framework leveraging Large Language Models (LLMs) and the Model Context Protocol (MCP) to unify and automate the entire drug discovery workflow, integrating dynamic biochemical databases, extensible tool libraries, and specialized AI models for tasks such as target identification, molecule generation, and retrosynthetic planning. Experimental results show FROGENT significantly outperforms state-of-the-art LLM agents and commercial models on eight drug discovery benchmarks, tripling hit-finding and doubling interaction profiling scores, thus streamlining and accelerating drug research.</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FROGENT_An_End-to-End_Full-process_Drug_Design_Agent.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-EgoCross-Benchmarking-Multimodal-Large-Language-Models-for-Cross-Domain-Egocentric-Video-Question-Answering"><a href="#89-EgoCross-Benchmarking-Multimodal-Large-Language-Models-for-Cross-Domain-Egocentric-Video-Question-Answering" class="headerlink" title="89. EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/EgoCross__Benchmarking_Multimodal_Large_Language_Models_for_Cross-Domain_Egocentric_Video_Question_A.pdf">EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">East China Normal University</span></p><p>本文提出了EgoCross基准，专为评估多模态大语言模型（MLLMs）在跨领域第一视角视频问答（EgocentricQA）中的泛化能力。通过涵盖外科手术、工业、极限运动和动物视角四大领域，共包含约1000对问答，支持识别、定位、预测与计数等多类任务，并采用CloseQA与OpenQA两种评测方式。实验发现，当前主流MLLMs在非日常领域表现显著下降，泛化能力有限。论文还探索了Prompt学习、微调和强化学习等提升方法，结果显示强化学习对提升跨域泛化效果最显著。结论表明，EgoCross为未来更具鲁棒性和适应性的多模态大模型研究提供了坚实基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/EgoCross_Benchmarking_Multimodal_Large_Language_Models_for_Cross-Domain_Egocentric_Video_Question_Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-Advancing-Autonomous-Incident-Response-Leveraging-LLMs-and-Cyber-Threat-Intelligence"><a href="#90-Advancing-Autonomous-Incident-Response-Leveraging-LLMs-and-Cyber-Threat-Intelligence" class="headerlink" title="90. Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Advancing_Autonomous_Incident_Response__Leveraging_LLMs_and_Cyber_Threat_Intelligence.pdf">Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">OODRIVE-Trusted Cloud Solutions</span></p><p>本文提出一种基于检索增强生成（RAG）的大语言模型自动化网络安全事件响应框架，通过结合NLP相似性检索和标准化CTI平台查询，实现对安全警报的上下文丰富与关联，并生成精确可操作的应对策略。实验结果表明，该方法在提高响应准确性、效率及减轻分析师工作负担方面表现优异，自动评估与专家验证均证实系统的鲁棒性和有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Advancing_Autonomous_Incident_Response_Leveraging_LLMs_and_Cyber_Threat_Intelligence.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-MSRS-Adaptive-Multi-Subspace-Representation-Steering-for-Attribute-Alignment-in-Large-Language-Models"><a href="#91-MSRS-Adaptive-Multi-Subspace-Representation-Steering-for-Attribute-Alignment-in-Large-Language-Models" class="headerlink" title="91. MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MSRS__Adaptive_Multi-Subspace_Representation_Steering_for_Attribute_Alignment_in_Large_Language_Mode.pdf">MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</span></p><p>本文提出了MSRS（Multi-Subspace Representation Steering）框架，通过为每个属性分配正交子空间并结合共享子空间，实现对大语言模型多属性的高效精准控制。方法利用SVD自适应分配子空间容量，并引入动态Token选择与子空间加权机制，有效减少属性间干扰。实验表明，MSRS在真值性、公平性、指令遵循和拒绝等多属性任务上，显著优于现有方法，且保持了模型的通用能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MSRS_Adaptive_Multi-Subspace_Representation_Steering_for_Attribute_Alignment_in_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-MCP2OSC-Parametric-Control-by-Natural-Language"><a href="#92-MCP2OSC-Parametric-Control-by-Natural-Language" class="headerlink" title="92. MCP2OSC: Parametric Control by Natural Language"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MCP2OSC__Parametric_Control_by_Natural_Language.pdf">MCP2OSC: Parametric Control by Natural Language</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Los Angeles, CA</span></p><p>本论文提出了MCP2OSC服务端和一套针对OpenSoundControl (OSC) 参数控制的自然语言提示设计准则，通过与Claude等大语言模型集成，实现了用自然语言生成、解释、调试和管理OSC消息，涵盖批量与流式控制、日志分析、可视化等多种实用场景。实验结果表明，该系统显著降低了复杂多媒体设备与创作流程中的OSC开发难度，提高了人机协作效率，展现了基于LLM的通用控制机制潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MCP2OSC_Parametric_Control_by_Natural_Language.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-Facilitating-the-Adoption-of-Causal-Inference-Methods-Through-LLM-Empowered-Co-Pilot"><a href="#93-Facilitating-the-Adoption-of-Causal-Inference-Methods-Through-LLM-Empowered-Co-Pilot" class="headerlink" title="93. Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Technical_Report__Facilitating_the_Adoption_of_Causal_Inference_Methods_Through_LLM-Empowered_Co-Pil.pdf">Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ataraxis.ai</span></p><p>本文提出了CATE-B系统，利用大语言模型（LLM）结合因果发现算法，帮助用户完成结构因果模型建设、不确定性最小调整集（MUAS）选择和回归方法推荐，实现从观测数据到处理效应估计的全流程自动化。实验表明，CATE-B在多数据集上提高了因果推断的准确性和健壮性，降低了领域专家门槛，促进了因果推断方法的普及与标准化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Facilitating_the_Adoption_of_Causal_Inference_Methods_Through_LLM-Empowered_Co-Pilot.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-When-Language-Overrules-Revealing-Text-Dominance-in-Multimodal-Large-Language-Models"><a href="#94-When-Language-Overrules-Revealing-Text-Dominance-in-Multimodal-Large-Language-Models" class="headerlink" title="94. When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/When_Language_Overrules__Revealing_Text_Dominance_in_Multimodal_Large_Language_Models.pdf">When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Computing Technology, Chinese Academy of Sciences</span></p><p>本文系统分析了多模态大语言模型（MLLMs）在多种模态（图像、视频、音频、时间序列和图结构数据）上的文本优势现象，提出了模态优势指数（MDI）和注意力效率指数（AEI）作为衡量指标，并通过实验发现文本模态在现有模型中普遍占主导地位。针对这一问题，作者提出了基于[CLS]注意力引导的非文本token压缩方法，有效缓解了注意力分配失衡并促进更公平的多模态融合，结论为压缩非文本token能显著改善多模态模型的模态均衡性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/When_Language_Overrules_Revealing_Text_Dominance_in_Multimodal_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-A-Unified-Multi-Agent-Framework-for-Universal-Multimodal-Understanding-and-Generation"><a href="#95-A-Unified-Multi-Agent-Framework-for-Universal-Multimodal-Understanding-and-Generation" class="headerlink" title="95. A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Unified_Multi-Agent_Framework_for_Universal_Multimodal_Understanding_and_Generation.pdf">A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of General Artificial Intelligence, BIGAI</span></p><p>本文提出MAGUS框架，通过认知与推理（Cognition）和表达与生成（Deliberation）两阶段分离，采用多智能体协作机制，将多模态大语言模型（MLLM）与扩散模型有机结合，实现文本、图像、音频、视频等任意模态的理解和生成。核心技术包括Growth-Aware Search（GAS）算法，支持多轮智能体对话、结构化任务规划、动态优化生成质量，无需联合训练即可灵活扩展。实验结果表明，MAGUS在多模态理解与生成任务的主流基准上均优于现有强基线及闭源大模型（如GPT-4o），具备高可扩展性和强指令跟随能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Unified_Multi-Agent_Framework_for_Universal_Multimodal_Understanding_and_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-SEQ-GPT-LLM-assisted-Spatial-Query-via-Example"><a href="#96-SEQ-GPT-LLM-assisted-Spatial-Query-via-Example" class="headerlink" title="96. SEQ-GPT: LLM-assisted Spatial Query via Example"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SEQ-GPT__LLM-assisted_Spatial_Query_via_Example.pdf">SEQ-GPT: LLM-assisted Spatial Query via Example</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>该论文提出了SEQ-GPT系统，将大语言模型（LLM）与空间示例查询（SEQ）结合，实现了用户通过自然语言和实例对空间位置进行灵活检索。方法包括多模型协作的数据解析与对话调度、数据合成与微调，并支持多轮交互和反馈修正，显著提升了复杂空间查询的易用性和灵活性。实验展示了SEQ-GPT在真实场景下对多地点检索任务的有效性和用户体验提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SEQ-GPT_LLM-assisted_Spatial_Query_via_Example.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs"><a href="#97-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs" class="headerlink" title="97. HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/HumanSense__From_Multimodal_Perception_to_Empathetic_Context-Aware_Responses_through_Reasoning_MLLMs.pdf">HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>该论文提出了HumanSense基准，系统评估多模态大语言模型（MLLMs）在人类中心场景下的感知、理解与互动能力，涵盖视觉、音频和文本。通过构建多层次任务体系、模态消融实验及多阶段模态递进式强化学习，发现多模态推理能显著提升模型在复杂人机交互场景下的表现，并通过提示工程无训练提升非推理模型效果，为未来AI互动体验指明方向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/HumanSense_From_Multimodal_Perception_to_Empathetic_Context-Aware_Responses_through_Reasoning_MLLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-Improving-Value-based-Process-Verifier-via-Low-Cost-Variance-Reduction"><a href="#98-Improving-Value-based-Process-Verifier-via-Low-Cost-Variance-Reduction" class="headerlink" title="98. Improving Value-based Process Verifier via Low-Cost Variance Reduction"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Improving_Value-based_Process_Verifier_via_Low-Cost_Variance_Reduction.pdf">Improving Value-based Process Verifier via Low-Cost Variance Reduction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology (Shenzhen)</span></p><p>本文提出了一种低成本方差降低方法ComMCS，通过对当前和后续步骤的Monte Carlo估计线性组合，优化了大语言模型（LLM）在数学推理任务中的value-based process verifier训练标签的方差，且无需增加额外的LLM推理成本。实验结果显示，ComMCS在MATH-500和GSM8K基准测试上相较于传统回归和分布建模方法取得了2-3个百分点的准确率提升，有效提升了模型的数学推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving_Value-based_Process_Verifier_via_Low-Cost_Variance_Reduction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-Reverse-Physician-AI-Relationship-Full-process-Clinical-Diagnosis-Driven-by-a-Large-Language-Model"><a href="#99-Reverse-Physician-AI-Relationship-Full-process-Clinical-Diagnosis-Driven-by-a-Large-Language-Model" class="headerlink" title="99. Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Reverse_Physician-AI_Relationship__Full-process_Clinical_Diagnosis_Driven_by_a_Large_Language_Model.pdf">Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of AI Safety, Institute of Computing Technology, CAS</span></p><p>本文提出了DxDirector-7B，一种具备类人“慢思考”深度推理能力的大型语言模型，实现了从含糊主诉出发自动驱动全流程临床诊断，仅在必要时请求医生协助。通过持续医学预训练、全流程诊断指令微调和步级策略偏好优化，使模型能自主决策、分步推理并附权威文献支持。实验结果显示，DxDirector-7B在复杂、罕见和真实临床场景中诊断准确率显著优于参数量大数十倍的现有医疗&#x2F;通用LLM，并极大减轻医生工作量，部分科室可替代60%-75%的专科医生诊断工作，标志着AI主导诊断新时代的到来。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Reverse_Physician-AI_Relationship_Full-process_Clinical_Diagnosis_Driven_by_a_Large_Language_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-FIRESPARQL-A-LLM-based-Framework-for-SPARQL-Query-Generation-over-Scholarly-Knowledge-Graphs"><a href="#100-FIRESPARQL-A-LLM-based-Framework-for-SPARQL-Query-Generation-over-Scholarly-Knowledge-Graphs" class="headerlink" title="100. FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/FIRESPARQL__A_LLM-based_Framework_for_SPARQL_Query_Generation_over_Scholarly_Knowledge_Graphs.pdf">FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Vrije Universiteit Amsterdam</span></p><p>本文提出FIRESPARQL框架，通过对大语言模型（LLM）进行领域特定微调（LoRA）、引入可选的检索增强生成（RAG）模块和轻量级SPARQL纠错层，以提升对学术知识图谱（SKG）中的自然语言问题转SPARQL查询的准确性和结构一致性。实验结果显示，微调后的LLaMA-3-8B-Instruct模型在SciQA基准上取得了最优表现，显著提升了查询准确率和执行效果，RAG模块反而未带来额外提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FIRESPARQL_A_LLM-based_Framework_for_SPARQL_Query_Generation_over_Scholarly_Knowledge_Graphs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="101-WE-MATH-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning"><a href="#101-WE-MATH-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning" class="headerlink" title="101. WE-MATH 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/We-Math_2.0__A_Versatile_MathBook_System_for_Incentivizing_Visual_Mathematical_Reasoning.pdf">WE-MATH 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">BUPT</span></p><p>WE-MATH 2.0提出了一个多模态大语言模型系统，融合了结构化数学知识体系、模型中心化数据难度建模以及两阶段强化学习训练，包括冷启动微调和动态难度递增RL优化，显著提升MLLM在视觉数学推理任务中的泛化与稳健性。实验证明，该方法在多个主流数学推理基准上取得了领先性能，尤其在多步推理和知识覆盖广度方面表现优异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/WE-MATH_2.0_A_Versatile_MathBook_System_for_Incentivizing_Visual_Mathematical_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-SC2Arena-and-StarEvolve-Benchmark-and-Self-Improvement-Framework-for-LLMs-in-Complex-Decision-Making-Tasks"><a href="#102-SC2Arena-and-StarEvolve-Benchmark-and-Self-Improvement-Framework-for-LLMs-in-Complex-Decision-Making-Tasks" class="headerlink" title="102. SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SC2Arena_and_StarEvolve__Benchmark_and_Self-Improvement_Framework_for_LLMs_in_Complex_Decision-Makin.pdf">SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Automation, Chinese Academy of Sciences</span></p><p>该论文提出SC2Arena，作为评估大语言模型（LLM）在StarCraft II复杂决策任务中的基准，支持全程游戏、多种种族和完整低级动作空间，同时通过文本优化解决空间推理与信息过载问题。配套的StarEvolve框架采用分层规划与执行、迭代自我纠错机制，并结合高质量数据的监督微调，实现LLM智能体的持续自我提升和强对手适应能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SC2Arena_and_StarEvolve_Benchmark_and_Self-Improvement_Framework_for_LLMs_in_Complex_Decision-Making_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-AnalogSeeker-An-Open-source-Foundation-Language-Model-for-Analog-Circuit-Design"><a href="#103-AnalogSeeker-An-Open-source-Foundation-Language-Model-for-Analog-Circuit-Design" class="headerlink" title="103. AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/AnalogSeeker__An_Open-source_Foundation_Language_Model_for_Analog_Circuit_Design.pdf">AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本论文提出AnalogSeeker，一种面向模拟电路设计的开源基础大语言模型。方法上，作者通过模拟电路领域知识框架系统收集高质量教材并清洗成文本语料，采用多智能体框架将未标注文本蒸馏为细粒度问答（QTSA）对，形成可微调数据集，并设计了邻域自约束监督微调算法（NSC-SFT），在Qwen2.5-32B-Instruct模型基础上进行训练。结论显示，AnalogSeeker在AMSBench-TQA基准上获得85.04%准确率，比原始模型提升15.67个百分点，且在运算放大器设计任务中表现有效，具备领域知识注入和设计辅助潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AnalogSeeker_An_Open-source_Foundation_Language_Model_for_Analog_Circuit_Design.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-Layer-Wise-Perturbations-via-Sparse-Autoencoders-for-Adversarial-Text-Generation"><a href="#104-Layer-Wise-Perturbations-via-Sparse-Autoencoders-for-Adversarial-Text-Generation" class="headerlink" title="104. Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Layer-Wise_Perturbations_via_Sparse_Autoencoders_for_Adversarial_Text_Generation.pdf">Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">hydrox.ai</span></p><p>该论文提出Sparse Feature Perturbation Framework (SFPF)，利用稀疏自编码器（SAE）对大型语言模型的隐藏层激活进行重构和特征聚类，识别并扰动与攻击相关的关键特征，从而生成可绕过安全机制的对抗性文本。实验表明，SFPF生成的对抗性文本能显著提升攻击成功率并维持文本质量，但方法在不同模型结构和层次上的泛化性尚待验证。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Layer-Wise_Perturbations_via_Sparse_Autoencoders_for_Adversarial_Text_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-LeanRAG-Knowledge-Graph-Based-Generation-with-Semantic-Aggregation-and-Hierarchical-Retrieval"><a href="#105-LeanRAG-Knowledge-Graph-Based-Generation-with-Semantic-Aggregation-and-Hierarchical-Retrieval" class="headerlink" title="105. LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/LeanRAG__Knowledge-Graph-Based_Generation_with_Semantic_Aggregation_and_Hierarchical_Retrieval.pdf">LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Artificial Intelligence Laboratory</span></p><p>LeanRAG提出了一种结合知识图谱语义聚合与层次化检索的检索增强生成（RAG）新框架。方法上，LeanRAG首先通过高斯混合聚类和大模型生成，构建多层次、具备显式实体间关系的可导航语义网络，再利用自底向上的LCA结构化检索，有效定位相关细粒度实体并聚合至高层次语义，提升上下文相关性并大幅减少信息冗余。结论显示，LeanRAG在四个QA基准数据集上表现优于现有方法，检索冗余降低46%，且显式语义关系与原始文本并用对生成高质量答案至关重要。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/LeanRAG_Knowledge-Graph-Based_Generation_with_Semantic_Aggregation_and_Hierarchical_Retrieval.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-Contrast-Sensitivity-Function-of-Multimodal-Vision-Language-Models"><a href="#106-Contrast-Sensitivity-Function-of-Multimodal-Vision-Language-Models" class="headerlink" title="106. Contrast Sensitivity Function of Multimodal Vision-Language Models"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Contrast_Sensitivity_Function_of_Multimodal_Vision-Language_Models.pdf">Contrast Sensitivity Function of Multimodal Vision-Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universidad de Valencia</span></p><p>本文提出了一种心理物理学启发的行为评估方法，通过直接向多模态视觉-语言模型（如LLaVA-1.5-7B、Blip2-7B等）提问来测量其对不同空间频率和对比度图像的敏感性，拟合模型的对比敏感函数（CSF），并与人类视觉CSF进行对比。结果显示，部分模型在CSF形状或绝对值上与人类相近，但均无法同时复现人类CSF的形状与稳定性，且对提示词变化敏感，提示内部一致性和鲁棒性有待提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Contrast_Sensitivity_Function_of_Multimodal_Vision-Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="107-What-to-Ask-Next-Probing-the-Imaginative-Reasoning-of-LLMs-with-TurtleSoup-Puzzles"><a href="#107-What-to-Ask-Next-Probing-the-Imaginative-Reasoning-of-LLMs-with-TurtleSoup-Puzzles" class="headerlink" title="107. What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/What_to_Ask_Next__Probing_the_Imaginative_Reasoning_of_LLMs_with_TurtleSoup_Puzzles.pdf">What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huazhong University of Science and Technology</span></p><p>本文提出了TurtleSoup-Bench，一个大规模、双语、交互式的想象性推理基准，结合TurtleSoup谜题、Mosaic-Agent智能体和多维自动评测协议，以系统评估大模型在信息稀缺环境下的假设构建、测试与修正能力。实验表明，当前主流大语言模型在动态推理与创造性假设生成方面仍与人类有明显差距，验证了该评测框架对模型推理过程诊断的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/What_to_Ask_Next_Probing_the_Imaginative_Reasoning_of_LLMs_with_TurtleSoup_Puzzles.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-ReviewRL-Towards-Automated-Scientific-Review-with-RL"><a href="#108-ReviewRL-Towards-Automated-Scientific-Review-with-RL" class="headerlink" title="108. ReviewRL: Towards Automated Scientific Review with RL"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ReviewRL__Towards_Automated_Scientific_Review_with_RL.pdf">ReviewRL: Towards Automated Scientific Review with RL</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本论文提出ReviewRL框架，通过ArXiv-MCP检索增强型上下文生成、监督微调和强化学习训练自动生成高质量、事实准确的学术论文评审。实验结果表明，ReviewRL在ICLR 2025论文评测中显著优于现有方法，有效提升了评审的质量和评分一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ReviewRL_Towards_Automated_Scientific_Review_with_RL.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-Yet-another-algorithmic-bias-A-Discursive-Analysis-of-Large-Language-Models-Reinforcing-Dominant-Discourses-on-Gender-and-Race"><a href="#109-Yet-another-algorithmic-bias-A-Discursive-Analysis-of-Large-Language-Models-Reinforcing-Dominant-Discourses-on-Gender-and-Race" class="headerlink" title="109. Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Yet_another_algorithmic_bias__A_Discursive_Analysis_of_Large_Language_Models_Reinforcing_Dominant_Di.pdf">Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Instituto de Estudos da Linguagem (IEL), Universidade Estadual de Campinas (UNICAMP)</span></p><p>本论文提出以批判性话语分析为核心的定性分析框架，对主流LLMs（包括GPT-4、LLaMa、Sabiá等）生成的关于不同性别与种族女性的短篇故事进行深入剖析。研究发现，这些LLMs在文本输出中系统性地强化了西方主流关于黑人女性的抗争、祖先和群体责任等刻板印象，而对白人女性则更多强调自我发现和个体成长。即使在被提示修正偏见时，模型的修正也流于表面，难以消除深层的结构性歧视。这反映出LLMs在语义与社会层面的理解与创新能力有限，现有模型难以自发突破训练语料中固有的意识形态框架。论文强调，话语层面的定性研究对于发现和干预大模型输出中的隐性偏见至关重要，呼吁AI开发需引入跨学科和多元伦理视角。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Yet_another_algorithmic_bias_A_Discursive_Analysis_of_Large_Language_Models_Reinforcing_Dominant_Discourses_on_Gender_and_Race.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="110-Promoting-Efficient-Reasoning-with-Verifiable-Stepwise-Reward"><a href="#110-Promoting-Efficient-Reasoning-with-Verifiable-Stepwise-Reward" class="headerlink" title="110. Promoting Efficient Reasoning with Verifiable Stepwise Reward"></a>110. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Promoting_Efficient_Reasoning_with_Verifiable_Stepwise_Reward.pdf">Promoting Efficient Reasoning with Verifiable Stepwise Reward</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Meituan</span></p><p>该论文提出了一种可验证的逐步奖励机制（VSRM），通过规则分割和奖励传播，对大语言推理模型的中间推理步骤进行奖励或惩罚，鼓励有效步骤抑制无效冗余，显著减少输出长度。实验结果表明，VSRM在多个数学推理基准上有效缓解模型过度推理问题，同时保持甚至略微提升原有推理能力，实现高效准确的推理。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Promoting_Efficient_Reasoning_with_Verifiable_Stepwise_Reward.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="111-Why-Cannot-Large-Language-Models-Ever-Make-True-Correct-Reasoning"><a href="#111-Why-Cannot-Large-Language-Models-Ever-Make-True-Correct-Reasoning" class="headerlink" title="111. Why Cannot Large Language Models Ever Make True Correct Reasoning?"></a>111. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Why_Cannot_Large_Language_Models_Ever_Make_True_Correct_Reasoning_.pdf">Why Cannot Large Language Models Ever Make True Correct Reasoning?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Saitama University</span></p><p>本文从逻辑学和推理正确性的角度，系统分析了大语言模型（LLM）所谓“推理能力”的本质，指出LLM仅能模拟推理形式但无法实现真正的正确推理。作者通过定义真正正确推理的三大逻辑基础，阐述了LLM由于其基于概率与统计的生成原理，无法内嵌逻辑有效性评价标准和动态评估机制，因此永远无法获得100%正确的推理能力。结论认为，LLM只能给出表面上似乎合理的推理结果，追求其真正推理能力而忽视正确性标准是错误且无望的方向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Why_Cannot_Large_Language_Models_Ever_Make_True_Correct_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="112-Flexible-Personalized-Split-Federated-Learning-for-On-Device-Fine-Tuning-of-Foundation-Models"><a href="#112-Flexible-Personalized-Split-Federated-Learning-for-On-Device-Fine-Tuning-of-Foundation-Models" class="headerlink" title="112. Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models"></a>112. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Flexible_Personalized_Split_Federated_Learning_for_On-Device_Fine-Tuning_of_Foundation_Models.pdf">Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Duke Kunshan University</span></p><p>该论文提出FlexP-SFL框架，结合Split Federated Learning和个性化机制，实现基础模型在资源受限设备上的个性化高效微调。方法允许每个客户端按自身算力灵活分配本地训练层，服务器处理共享部分，并通过KL对齐策略平衡个性化与泛化。实验表明，该方法在多种语言模型上显著提升个性化准确率，减少通信与训练时间，支持异构设备高效协作。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Flexible_Personalized_Split_Federated_Learning_for_On-Device_Fine-Tuning_of_Foundation_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="113-MRFD-Multi-Region-Fusion-Decoding-with-Self-Consistency-for-Mitigating-Hallucinations-in-LVLMs"><a href="#113-MRFD-Multi-Region-Fusion-Decoding-with-Self-Consistency-for-Mitigating-Hallucinations-in-LVLMs" class="headerlink" title="113. MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs"></a>113. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MRFD__Multi-Region_Fusion_Decoding_with_Self-Consistency_for_Mitigating_Hallucinations_in_LVLMs.pdf">MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Merced</span></p><p>该论文提出了一种训练无关的解码方法MRFD，通过交叉注意力机制选取图像多个显著区域，分别生成区域响应，并利用Jensen-Shannon Divergence评估区域间自洽性，获得可靠性权重后融合各区域预测。最终采用链式思维启发的区域提示，提升LVLMs对图像事实的捕捉与响应的真实度。实验表明MRFD能在多个LVLM和公开基准上显著降低幻觉现象，提升响应的事实性，无需模型参数更新。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MRFD_Multi-Region_Fusion_Decoding_with_Self-Consistency_for_Mitigating_Hallucinations_in_LVLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="114-Generalizable-Federated-Learning-using-Client-Adaptive-Focal-Modulation"><a href="#114-Generalizable-Federated-Learning-using-Client-Adaptive-Focal-Modulation" class="headerlink" title="114. Generalizable Federated Learning using Client Adaptive Focal Modulation"></a>114. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Generalizable_Federated_Learning_using_Client_Adaptive_Focal_Modulation.pdf">Generalizable Federated Learning using Client Adaptive Focal Modulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MBZUAI</span></p><p>该论文提出了AdaptFED，一种在联邦学习（FL）场景下利用中心化可学习生成器和任务感知客户端嵌入，实现焦点调制层个性化的Transformer框架。方法通过低秩超网络条件化和高效的参数聚合，提升了模型在多模态、非独立同分布（Non-IID）及源数据不可用（Source-Free Domain Adaptation）环境下的泛化能力。实验证明，AdaptFED在视觉、时间序列和多语言任务上均优于当前主流方法，理论上也给出了更紧的个性化泛化界。结论：AdaptFED提升了个性化联邦学习的效果与扩展性，适用于多领域和资源受限场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Generalizable_Federated_Learning_using_Client_Adaptive_Focal_Modulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="115-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms"><a href="#115-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms" class="headerlink" title="115. From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms"></a>115. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/From_Black_Box_to_Transparency__Enhancing_Automated_Interpreting_Assessment_with_Explainable_AI_in_C.pdf">From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本论文提出了一种结合特征工程、变分自编码器（VAE）数据增强和可解释机器学习（如SHAP分析）的多维自动口译质量评价框架。方法在英中口译数据集上验证，重点提升对语言使用、流畅度和信息完整性的可解释预测能力，结果显示VAE增强显著提升模型表现，且各维度的关键特征均被精确识别，为教学反馈和自我调节学习提供了透明、可扩展的自动评分工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From_Black_Box_to_Transparency_Enhancing_Automated_Interpreting_Assessment_with_Explainable_AI_in_College_Classrooms.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="116-Who-Benefits-from-AI-Explanations-Towards-Accessible-and-Interpretable-Systems"><a href="#116-Who-Benefits-from-AI-Explanations-Towards-Accessible-and-Interpretable-Systems" class="headerlink" title="116. Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems"></a>116. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Who_Benefits_from_AI_Explanations__Towards_Accessible_and_Interpretable_Systems.pdf">Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ontario Tech University</span></p><p>本文采用文献综述和四步法案例研究，系统评估现有可解释人工智能（XAI）技术在无障碍领域的缺口，尤其关注视觉障碍用户。研究设计并测试了面向盲人交通管理者的XAI原型，采用LIME和SHAP解释方法，结果发现简化、多模态解释更易于非视觉用户理解，强调需为不同能力用户设计多样化解释方式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Who_Benefits_from_AI_Explanations_Towards_Accessible_and_Interpretable_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="117-Insights-from-the-Algonauts-2025-Winners"><a href="#117-Insights-from-the-Algonauts-2025-Winners" class="headerlink" title="117. Insights from the Algonauts 2025 Winners"></a>117. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Insights_from_the_Algonauts_2025_Winners.pdf">Insights from the Algonauts 2025 Winners</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Medical AI Research Center (MedARC)</span></p><p>本论文总结了Algonauts 2025竞赛中的顶尖团队方法，主要采用预训练多模态特征提取器（视觉、音频和文本），结合transformer、RNN或线性卷积架构，通过模型集成预测自然电影刺激下的人类脑区fMRI反应。结果显示，架构复杂度影响有限，模型集成与多模态特征融合是提升脑编码准确性的关键，未来突破需跳出现有范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Insights_from_the_Algonauts_2025_Winners.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="118-APFL-Analytic-Personalized-Federated-Learning-via-Dual-Stream-Least-Squares"><a href="#118-APFL-Analytic-Personalized-Federated-Learning-via-Dual-Stream-Least-Squares" class="headerlink" title="118. APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares"></a>118. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/APFL__Analytic_Personalized_Federated_Learning_via_Dual-Stream_Least_Squares.pdf">APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CSU, China</span></p><p>该论文提出了APFL方法，利用冻结的基础模型（如ViT）进行特征提取，并通过双流解析模型（主流用于全局泛化，细化流用于本地个性化）实现个性化联邦学习，采用解析最小二乘法避免梯度更新对非IID数据分布的敏感性。实验结果显示APFL在多个数据集和不同非IID程度下均优于现有方法，理论分析证明其具备异质性不变性、隐私保护及高效率，结论为APFL有效提升个性化模型性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/APFL_Analytic_Personalized_Federated_Learning_via_Dual-Stream_Least_Squares.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="119-FreeGAD-A-Training-Free-yet-Effective-Approach-for-Graph-Anomaly-Detection"><a href="#119-FreeGAD-A-Training-Free-yet-Effective-Approach-for-Graph-Anomaly-Detection" class="headerlink" title="119. FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection"></a>119. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/FreeGAD__A_Training-Free_yet_Effective_Approach_for_Graph_Anomaly_Detection.pdf">FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Guangxi University</span></p><p>本文提出FreeGAD，一种无需训练的图异常检测方法，采用亲和力门控残差编码器生成异常感知表示，并通过锚点节点筛选和引导统计偏差计算异常分数。实验结果表明，FreeGAD在多个真实和合成数据集上实现了优越的检测性能、极高效率和良好扩展性，显著优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FreeGAD_A_Training-Free_yet_Effective_Approach_for_Graph_Anomaly_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="120-Deep-Learning-in-Classical-and-Quantum-Physics"><a href="#120-Deep-Learning-in-Classical-and-Quantum-Physics" class="headerlink" title="120. Deep Learning in Classical and Quantum Physics"></a>120. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Deep_Learning_in_Classical_and_Quantum_Physics.pdf">Deep Learning in Classical and Quantum Physics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ICFO – Institut de Ciències Fotòniques</span></p><p>本论文系统梳理了深度学习（Deep Learning）和机器学习（Machine Learning）的基础理论、关键算法（如神经网络、自动微分、损失函数设计等），并重点介绍了其在经典物理和量子物理中的应用，包括变分量子本征求解器（VQE）、神经网络量子态（NQS）、量子态层析等。作者以物理视角阐释了深度学习在高维物理系统中的建模优势及其局限，指出深度学习可辅助物理研究发现模式，但尚不能替代理解物理机制。结论认为深度学习为物理学尤其是量子科学提供了强大工具，但其科学解释性与知识获取仍需进一步突破。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Deep_Learning_in_Classical_and_Quantum_Physics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="121-Self-Supervised-Temporal-Super-Resolution-of-Energy-Data-using-Generative-Adversarial-Transformers"><a href="#121-Self-Supervised-Temporal-Super-Resolution-of-Energy-Data-using-Generative-Adversarial-Transformers" class="headerlink" title="121. Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformers"></a>121. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Self-Supervised_Temporal_Super-Resolution_of_Energy_Data_using_Generative_Adversarial_Transformer.pdf">Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Karlsruhe Institute of Technology</span></p><p>该论文提出了一种无需高分辨率数据的自监督时间序列超分辨率方法，结合了Transformer的序列建模能力和GAN的对抗训练框架，并通过特征空间损失实现低分辨率到高分辨率数据的一致映射。实验结果表明，该方法在多种能源数据集上超越了传统插值与现有深度学习模型，RMSE降低9%、在MPC应用场景精度提升13%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Self-Supervised_Temporal_Super-Resolution_of_Energy_Data_using_Generative_Adversarial_Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="122-FAKE-SPEECH-WILD-DETECTING-DEEPFAKE-SPEECH-ON-SOCIAL-MEDIA-PLATFORM"><a href="#122-FAKE-SPEECH-WILD-DETECTING-DEEPFAKE-SPEECH-ON-SOCIAL-MEDIA-PLATFORM" class="headerlink" title="122. FAKE SPEECH WILD: DETECTING DEEPFAKE SPEECH ON SOCIAL MEDIA PLATFORM"></a>122. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Fake_Speech_Wild__Detecting_Deepfake_Speech_on_Social_Media_Platform.pdf">FAKE SPEECH WILD: DETECTING DEEPFAKE SPEECH ON SOCIAL MEDIA PLATFORM</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Communication University of China</span></p><p>本文提出了Fake Speech Wild (FSW) 数据集，收集了来自四个社交媒体平台、共254小时的真实与深度伪造语音数据。采用自监督学习技术和多种数据增强方法（如MUSAN &amp; RIR噪声、Rawboost），联合训练多源数据集后，显著提升了深度伪造语音检测模型在跨领域场景下的鲁棒性，最终在全部测试集上达到3.54%的平均等错误率（EER）。结论：联合多域数据和数据增强显著提升了检测性能，FSW数据集为真实环境下伪造语音检测提供了重要基准。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FAKE_SPEECH_WILD_DETECTING_DEEPFAKE_SPEECH_ON_SOCIAL_MEDIA_PLATFORM.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="123-Contrastive-ECOC-Learning-Output-Codes-for-Adversarial-Defense"><a href="#123-Contrastive-ECOC-Learning-Output-Codes-for-Adversarial-Defense" class="headerlink" title="123. Contrastive ECOC: Learning Output Codes for Adversarial Defense"></a>123. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Contrastive_ECOC__Learning_Output_Codes_for_Adversarial_Defense.pdf">Contrastive ECOC: Learning Output Codes for Adversarial Defense</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Central University</span></p><p>本文提出了三种基于对比学习的自动化编码本学习（ACL）方法，用于多分类任务中的自动生成Error Correcting Output Codes（ECOC）编码本，有效提升神经网络对抗攻击（如FGSM和PGD）的鲁棒性。实验表明，所提模型能够自动生成数据集特定的编码本，在多个开放数据集上对抗攻击下的表现优于传统手工或随机编码本。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Contrastive_ECOC_Learning_Output_Codes_for_Adversarial_Defense.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="124-GraphFedMIG-Tackling-Class-Imbalance-in-Federated-Graph-Learning-via-Mutual-Information-Guided-Generation"><a href="#124-GraphFedMIG-Tackling-Class-Imbalance-in-Federated-Graph-Learning-via-Mutual-Information-Guided-Generation" class="headerlink" title="124. GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation"></a>124. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GraphFedMIG__Tackling_Class_Imbalance_in_Federated_Graph_Learning_via_Mutual_Information-Guided_Gene.pdf">GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Chongqing University</span></p><p>该论文提出GraphFedMIG框架，通过将联邦图学习中的类别不平衡问题转化为生成式数据增强任务，采用分层生成对抗网络（GAN）和互信息引导的模型聚合机制。实验表明，GraphFedMIG在四个真实数据集上显著提升了少数类的识别能力并整体优于当前主流方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GraphFedMIG_Tackling_Class_Imbalance_in_Federated_Graph_Learning_via_Mutual_Information-Guided_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="125-HiRef-Leveraging-Hierarchical-Ontology-and-Network-Refinement-for-Robust-Medication-Recommendation"><a href="#125-HiRef-Leveraging-Hierarchical-Ontology-and-Network-Refinement-for-Robust-Medication-Recommendation" class="headerlink" title="125. HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation"></a>125. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/HiRef__Leveraging_Hierarchical_Ontology_and_Network_Refinement_for_Robust_Medication_Recommendation.pdf">HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea University</span></p><p>该论文提出HiRef框架，将医疗本体的层次语义嵌入（使用超曲空间）与电子病历（EHR）中稀疏优化的共现图结构相结合，通过自适应融合机制提升药物推荐模型对罕见和未见医疗编码的泛化能力。实验表明，HiRef在MIMIC-III&#x2F;IV数据集下表现优异，尤其在未见编码场景下准确率高，且具备更好的鲁棒性和可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/HiRef_Leveraging_Hierarchical_Ontology_and_Network_Refinement_for_Robust_Medication_Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="126-Mitigating-Exponential-Mixed-Frequency-Growth-through-Frequency-Selection-and-Dimensional-Separation-in-Quantum-Machine-Learning"><a href="#126-Mitigating-Exponential-Mixed-Frequency-Growth-through-Frequency-Selection-and-Dimensional-Separation-in-Quantum-Machine-Learning" class="headerlink" title="126. Mitigating Exponential Mixed Frequency Growth through Frequency Selection and Dimensional Separation in Quantum Machine Learning"></a>126. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Mitigating_Exponential_Mixed_Frequency_Growth_through_Frequency_Selection_and_Dimensional_Separation.pdf">Mitigating Exponential Mixed Frequency Growth through Frequency Selection and Dimensional Separation in Quantum Machine Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute for Informatics, LMU Munich</span></p><p>本文针对量子机器学习中多维傅里叶频谱导致参数双指数级增长、难以在当前硬件上拟合实际问题的瓶颈，提出了频率选择与维度分离两项关键技术，通过仅保留必要频率和混合频率组，极大减少所需模型参数。实验表明，这两种方法能在噪声模拟器及真实量子硬件上实现高精度回归拟合，突破了默认模型频谱密集导致的拟合失败，理论与实践上均有效缓解了参数爆炸问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Mitigating_Exponential_Mixed_Frequency_Growth_through_Frequency_Selection_and_Dimensional_Separation_in_Quantum_Machine_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="127-RealAC-A-Domain-Agnostic-Framework-for-Realistic-and-Actionable-Counterfactual-Explanations"><a href="#127-RealAC-A-Domain-Agnostic-Framework-for-Realistic-and-Actionable-Counterfactual-Explanations" class="headerlink" title="127. RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations"></a>127. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/RealAC__A_Domain-Agnostic_Framework_for_Realistic_and_Actionable_Counterfactual_Explanations.pdf">RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Arizona State University</span></p><p>本文提出了RealAC，一个面向领域无关、能够生成现实且可操作反事实解释的框架。RealAC通过最小化反事实与原始样本之间所有特征对的互信息差异，在无需领域知识的情况下自动保持复杂的特征依赖关系，并结合二进制掩码机制支持用户指定不可变特征，实现个性化可行性约束。实验表明，RealAC在因果边缘分数、依赖保持分数和IM1等指标上优于现有主流方法及大语言模型生成的反事实，兼顾了反事实的现实性与可操作性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/RealAC_A_Domain-Agnostic_Framework_for_Realistic_and_Actionable_Counterfactual_Explanations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="128-Semantic-Communication-with-Distribution-Learning-through-Sequential-Observations"><a href="#128-Semantic-Communication-with-Distribution-Learning-through-Sequential-Observations" class="headerlink" title="128. Semantic Communication with Distribution Learning through Sequential Observations"></a>128. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Semantic_Communication_with_Distribution_Learning_through_Sequential_Observations.pdf">Semantic Communication with Distribution Learning through Sequential Observations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Dalhousie University</span></p><p>本文提出了语义通信中的分布学习理论框架，研究在先验分布未知且编码方案与信道固定时接收端如何通过观察序列学习语义分布。方法上，作者提出以有效传输矩阵的满秩性为可学习性条件，推导出估计误差和性能损失的收敛界，并理论证明估计误差与样本数成O(1&#x2F;√T)衰减；结论表明，良好条件数的系统能实现高效学习和适应，压缩优化型编码则显著降低长期学习能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Semantic_Communication_with_Distribution_Learning_through_Sequential_Observations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="129-eMamba-Efficient-Acceleration-Framework-for-Mamba-Models-in-Edge-Computing"><a href="#129-eMamba-Efficient-Acceleration-Framework-for-Mamba-Models-in-Edge-Computing" class="headerlink" title="129. eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing"></a>129. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/eMamba__Efficient_Acceleration_Framework_for_Mamba_Models_in_Edge_Computing.pdf">eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Ulsan</span></p><p>本文提出了eMamba，一种面向边缘计算的Mamba模型端到端硬件加速框架。方法包括用硬件友好的归一化、分段线性近似及量化技术替代原有复杂操作，并结合近似感知的神经架构搜索优化可训练参数。实验表明，在Fashion-MNIST、CIFAR-10和MARS等视觉任务中，eMamba在保持精度的同时大幅减少参数量，并在FPGA和ASIC实现中获得更低延迟、更高吞吐率和显著的能耗降低，可广泛适用于边缘AI应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/eMamba_Efficient_Acceleration_Framework_for_Mamba_Models_in_Edge_Computing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="130-Layer-Wise-Analysis-of-Self-Supervised-Representations-for-Age-and-Gender-Classification-in-Children’s-Speech"><a href="#130-Layer-Wise-Analysis-of-Self-Supervised-Representations-for-Age-and-Gender-Classification-in-Children’s-Speech" class="headerlink" title="130. Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Children’s Speech"></a>130. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Layer-Wise_Analysis_of_Self-Supervised_Representations_for_Age_and_Gender_Classification_in_Children.pdf">Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Children’s Speech</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">NIT Sikkim</span></p><p>该论文提出了一种针对儿童语音年龄和性别分类的层级特征分析方法，利用四种Wav2Vec2自监督学习模型提取层级特征，并通过CNN分类器和主成分分析（PCA）进行降维和性能优化。实验结果显示，模型早期层能更有效地捕捉说话人特征，降维后不仅提升了分类准确率也提高了计算效率，揭示了特征在模型深度中的分布规律。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Layer-Wise_Analysis_of_Self-Supervised_Representations_for_Age_and_Gender_Classification_in_Children%E2%80%99s_Speech.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="131-A-Vision-Language-Pre-training-Model-Guided-Approach-for-Mitigating-Backdoor-Attacks-in-Federated-Learning"><a href="#131-A-Vision-Language-Pre-training-Model-Guided-Approach-for-Mitigating-Backdoor-Attacks-in-Federated-Learning" class="headerlink" title="131. A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning"></a>131. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Vision-Language_Pre-training_Model-Guided_Approach_for_Mitigating_Backdoor_Attacks_in_Federated_Le.pdf">A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>该论文提出CLIP-Fed框架，利用视觉-语言预训练模型（如CLIP）的跨模态零样本学习能力，实现联邦学习中的前聚合恶意模型过滤和后聚合后门净化。通过多模态大模型和频域分析构建服务器数据集，并结合原型对比损失和知识蒸馏，纠正特征偏移、削弱后门关联。实验表明，CLIP-Fed在多种数据分布和攻击类型下显著降低ASR并提升主任务准确率，优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Vision-Language_Pre-training_Model-Guided_Approach_for_Mitigating_Backdoor_Attacks_in_Federated_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="132-Pruning-and-Malicious-Injection-A-Retraining-Free-Backdoor-Attack-on-Transformer-Models"><a href="#132-Pruning-and-Malicious-Injection-A-Retraining-Free-Backdoor-Attack-on-Transformer-Models" class="headerlink" title="132. Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models"></a>132. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Pruning_and_Malicious_Injection__A_Retraining-Free_Backdoor_Attack_on_Transformer_Models.pdf">Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Louisiana State University</span></p><p>本文提出了HPMI（Head-wise Pruning and Malicious Injection），一种无需额外重训练且不改变模型架构的变换器后门攻击方法。通过剪除多头注意力模块中最不重要的头，再注入预训练的恶意头，实现后门植入，理论与实验均表明在保持干净数据准确率的同时，能以超99.55%的成功率绕过四种主流防御机制。结论显示HPMI对现有防御手段有很强的隐蔽性和鲁棒性，且对模型性能影响极小。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Pruning_and_Malicious_Injection_A_Retraining-Free_Backdoor_Attack_on_Transformer_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025/08/15报纸</title>
    <link href="/2025/08/2025-08-15_article/"/>
    <url>/2025/08/2025-08-15_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2025/08/15报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-15<br>📄 <strong>发现论文数量</strong>：109  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Human-Aligned-Procedural-Level-Generation-Reinforcement-Learning-via-Text-Level-Sketch-Shared-Representation"><a href="#1-Human-Aligned-Procedural-Level-Generation-Reinforcement-Learning-via-Text-Level-Sketch-Shared-Representation" class="headerlink" title="1. Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Human-Aligned_Procedural_Level_Generation_Reinforcement_Learning_via_Text-Level-Sketch_Shared_Repres.pdf">Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Gwangju Institute of Science and Technology</span></p><p>该论文提出VIPCGRL框架，通过四模态对比学习（文本、关卡、草图与人&#x2F;AI风格）建立共享表征空间，并在多模态条件下利用辅助奖励实现与人类风格对齐的深度强化学习关卡生成。实验显示VIPCGRL在生成2D游戏关卡时，在可控性和拟人性上均优于现有方法，并能灵活支持多模态输入，用户研究进一步验证了其人类风格和文本&#x2F;草图指令对齐能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Human-Aligned_Procedural_Level_Generation_Reinforcement_Learning_via_Text-Level-Sketch_Shared_Representation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Generative-Modeling-with-Multi-Instance-Reward-Learning-for-E-commerce-Creative-Optimization"><a href="#2-Generative-Modeling-with-Multi-Instance-Reward-Learning-for-E-commerce-Creative-Optimization" class="headerlink" title="2. Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Generative_Modeling_with_Multi-Instance_Reward_Learning_for_E-commerce_Creative_Optimization.pdf">Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">JD.com</span></p><p>该论文提出了GenCO框架，将生成式建模与多实例奖励学习结合应用于电商创意优化。方法包括第一阶段采用非自回归、上下文感知的生成模型高效生成多样化的创意组合，并通过强化学习优化生成策略，第二阶段采用多实例学习模型将组合层级的用户反馈（如点击）归因到各个创意元素，实现更精准的反馈信号与生成优化。论文在真实电商平台上线验证，显著提升CTR和广告收入，显示出强大的实际商业价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Generative_Modeling_with_Multi-Instance_Reward_Learning_for_E-commerce_Creative_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Goal-Discovery-with-Causal-Capacity-for-Efficient-Reinforcement-Learning"><a href="#3-Goal-Discovery-with-Causal-Capacity-for-Efficient-Reinforcement-Learning" class="headerlink" title="3. Goal Discovery with Causal Capacity for Efficient Reinforcement Learning"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Goal_Discovery_with_Causal_Capacity_for_Efficient_Reinforcement_Learning.pdf">Goal Discovery with Causal Capacity for Efficient Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出了一种结合因果推断的目标发现方法（GDCC），通过引入“因果容量”来度量状态下动作对未来轨迹的最大影响力，并用蒙特卡洛和聚类方法在高维连续空间中高效估算关键决策点（子目标），以指导强化学习智能体的有效探索。实验证明，该方法在MuJoCo和Habitat等多目标稀疏奖励任务中，能自动发现物理意义明确的子目标，有效提升探索效率和任务成功率，优于多种基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Goal_Discovery_with_Causal_Capacity_for_Efficient_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-Edge-General-Intelligence-Through-World-Models-and-Agentic-AI-Fundamentals-Solutions-and-Challenges"><a href="#4-Edge-General-Intelligence-Through-World-Models-and-Agentic-AI-Fundamentals-Solutions-and-Challenges" class="headerlink" title="4. Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Edge_General_Intelligence_Through_World_Models_and_Agentic_AI__Fundamentals,_Solutions,_and_Challeng.pdf">Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>该论文系统梳理了世界模型（world models）在边缘通用智能（Edge General Intelligence, EGI）中的作用，提出将世界模型作为认知内核，通过内部环境模拟与想象，实现边缘分布式智能体的感知、推理和自主决策。论文分析了世界模型的关键架构（如编码器、动态建模、解码器）、主要技术路线（变分自编码器、RNN、Transformer等）、典型应用场景（车联网、无人机网络、物联网、虚拟化网络功能等），并对与基础模型和数字孪生的协同进行了探讨。结论指出世界模型显著提升了边缘智能系统的前瞻性、效率与安全性，但在算力、实时性与泛化能力等实际部署上仍需进一步研究。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Edge_General_Intelligence_Through_World_Models_and_Agentic_AI_Fundamentals%2C_Solutions%2C_and_Challenges.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-Emergence-of-Hierarchies-in-Multi-Agent-Self-Organizing-Systems-Pursuing-a-Joint-Objective"><a href="#5-Emergence-of-Hierarchies-in-Multi-Agent-Self-Organizing-Systems-Pursuing-a-Joint-Objective" class="headerlink" title="5. Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Emergence_of_Hierarchies_in_Multi-Agent_Self-Organizing_Systems_Pursuing_a_Joint_Objective.pdf">Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>该论文通过多智能体强化学习（MARL）方法，基于MADDPG算法，研究了多智能体自组织系统（MASOS）在协同完成箱子推动任务过程中依赖层级结构的自发涌现及其动态变化。方法核心在于利用动作对其他智能体状态的梯度衡量依赖性，量化并分析层级结构的形成与演化。结论表明：MASOS在追求共同目标时会自发形成动态演化的层级结构，层级变化受环境与网络初始化影响，且“天赋”与“努力”在环境中的动态作用促进了层级的调整与优化。这为MASOS的自组织行为理解和设计提供了理论指导。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Emergence_of_Hierarchies_in_Multi-Agent_Self-Organizing_Systems_Pursuing_a_Joint_Objective.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="6-Masquerade-Learning-from-In-the-wild-Human-Videos-using-Data-Editing"><a href="#6-Masquerade-Learning-from-In-the-wild-Human-Videos-using-Data-Editing" class="headerlink" title="6. Masquerade: Learning from In-the-wild Human Videos using Data-Editing"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Masquerade__Learning_from_In-the-wild_Human_Videos_using_Data-Editing.pdf">Masquerade: Learning from In-the-wild Human Videos using Data-Editing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University</span></p><p>Masquerade提出了一种通过数据编辑将大规模野外人类视频转换为“机器人化”演示的方法，包括3D手部姿态估计、手臂修复和机器人覆盖，从而弥合人类和机器人的视觉表现差距。该方法先用编辑后的人类视频预训练视觉编码器，再与少量真实机器人演示进行联合训练，实现了在三类厨房任务中对未见环境的零样本迁移，成功率显著高于现有方法，且扩展视频规模可持续提升性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Masquerade_Learning_from_In-the-wild_Human_Videos_using_Data-Editing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Vision-driven-River-Following-of-UAV-via-Safe-Reinforcement-Learning-using-Semantic-Dynamics-Model"><a href="#7-Vision-driven-River-Following-of-UAV-via-Safe-Reinforcement-Learning-using-Semantic-Dynamics-Model" class="headerlink" title="7. Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Vision-driven_River_Following_of_UAV_via_Safe_Reinforcement_Learning_using_Semantic_Dynamics_Model.pdf">Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Purdue University</span></p><p>本论文提出了一种基于视觉的无人机河流跟随安全强化学习框架。方法方面，作者引入了边际增益优势估计（MGAE）处理非马尔可夫亚模奖励，提出了基于同伦变换的语义动力学模型（SDM）实现高效视觉预测，并整合为受约束的行动者动力学估计器（CADE）以提升安全强化学习在部分可观测环境中的表现。实验结果表明MGAE收敛更快且优于传统方法，SDM在短期状态预测更准确，CADE能有效平衡任务奖励和安全，拉格朗日方法训练下表现最佳，推理时安全层进一步提升效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Vision-driven_River_Following_of_UAV_via_Safe_Reinforcement_Learning_using_Semantic_Dynamics_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-PPL-Point-Cloud-Supervised-Proprioceptive-Locomotion-Reinforcement-Learning-for-Legged-Robots-in-Crawl-Spaces"><a href="#8-PPL-Point-Cloud-Supervised-Proprioceptive-Locomotion-Reinforcement-Learning-for-Legged-Robots-in-Crawl-Spaces" class="headerlink" title="8. PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/PPL__Point_Cloud_Supervised_Proprioceptive_Locomotion_Reinforcement_Learning_for_Legged_Robots_in_Cr.pdf">PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了一种用于腿式机器人在狭小空间（如低矮隧道、洞穴等）自主穿越的点云监督本体感知强化学习方法（PPL）。该方法设计了基于历史本体传感器数据的状态估计网络（PPL-Net），结合极坐标点云特征提取，以及考虑全身碰撞惩罚和碰撞后速度奖励的奖励函数，实现了机器人仅凭本体感知在正常及低能见度环境下敏捷穿越狭窄空间。实验结果表明，PPL在平坦和阶梯隧道中的成功率显著优于现有方法，并能适应多种复杂和低可见环境，无需外部传感器。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/PPL_Point_Cloud_Supervised_Proprioceptive_Locomotion_Reinforcement_Learning_for_Legged_Robots_in_Crawl_Spaces.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-FLARE-Agile-Flights-for-Quadrotor-Cable-Suspended-Payload-System-via-Reinforcement-Learning"><a href="#9-FLARE-Agile-Flights-for-Quadrotor-Cable-Suspended-Payload-System-via-Reinforcement-Learning" class="headerlink" title="9. FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/FLARE__Agile_Flights_for_Quadrotor_Cable-Suspended_Payload_System_via_Reinforcement_Learning.pdf">FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出FLARE，一种基于强化学习（RL）的自主导航框架，针对四旋翼缆绳悬挂载荷系统的敏捷飞行问题。方法通过在高保真仿真中训练无模型RL策略，设计了奖励函数和观测空间以适应系统的复杂动力学，并在三个具有挑战性的场景（敏捷航点通过、载荷定位、门洞穿越）下进行验证。实验结果显示，所提出方法在敏捷性和实时性上显著优于基于优化的方法，实现了3倍速度提升，并成功实现了零样本仿真到现实的策略迁移，兼具敏捷性与安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/FLARE_Agile_Flights_for_Quadrotor_Cable-Suspended_Payload_System_via_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-A-Shank-Angle-Based-Control-System-Enables-Soft-Exoskeleton-to-Assist-Human-Non-Steady-Locomotion"><a href="#10-A-Shank-Angle-Based-Control-System-Enables-Soft-Exoskeleton-to-Assist-Human-Non-Steady-Locomotion" class="headerlink" title="10. A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_Shank_Angle-Based_Control_System_Enables_Soft_Exoskeleton_to_Assist_Human_Non-Steady_Locomotion.pdf">A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of Robotics and Intelligent Systems, Shenyang Institute of Automation, Chinese Academy of Sciences</span></p><p>本论文提出了一种基于小腿倾角（shank angle）的软体踝关节外骨骼控制系统，通过双高斯模型实时在线生成仿生辅助力型谱，以实现跨步适应，并采用模型驱动的前馈控制实现精确跟踪，全部依赖IMU传感器数据。实验结果表明，该系统在行走、跑步及楼梯等非稳态运动中可持续与人体步态生物力学高度同步，显著降低肌肉激活和代谢率，有效提升了外骨骼在复杂环境下的助力效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A_Shank_Angle-Based_Control_System_Enables_Soft_Exoskeleton_to_Assist_Human_Non-Steady_Locomotion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-Whole-Body-Bilateral-Teleoperation-with-Multi-Stage-Object-Parameter-Estimation-for-Wheeled-Humanoid-Locomanipulation"><a href="#11-Whole-Body-Bilateral-Teleoperation-with-Multi-Stage-Object-Parameter-Estimation-for-Wheeled-Humanoid-Locomanipulation" class="headerlink" title="11. Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Whole-Body_Bilateral_Teleoperation_with_Multi-Stage_Object_Parameter_Estimation_for_Wheeled_Humanoid.pdf">Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Urbana-Champaign</span></p><p>该论文提出了面向轮式人形机器人全身双向远程操作的多阶段在线物体惯性参数估算框架。方法核心为将视觉与视觉-语言模型生成的物体尺寸和材料等先验，结合高保真仿真并行的分层采样优化（DH-CEM），实现物体惯性参数的快速、物理可行估算，并用于实时调整机器人控制器的平衡点和动态补偿。实验在自研硬件平台SATYRR上验证了系统可高效实现重物搬运、提升操作同步性和操作员体验，并提升了安全任务如避障的可靠性。结论：集成多模态估算和仿真采样的框架，使轮式人形机器人在复杂物理交互任务中表现出更强的动态适应性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Whole-Body_Bilateral_Teleoperation_with_Multi-Stage_Object_Parameter_Estimation_for_Wheeled_Humanoid_Locomanipulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory"><a href="#12-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory" class="headerlink" title="12. Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Seeing,_Listening,_Remembering,_and_Reasoning__A_Multimodal_Agent_with_Long-Term_Memory.pdf">Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ByteDance Seed</span></p><p>本文提出了M3-Agent，一个具备长期记忆的多模态智能体框架，能够通过视频和音频流实时感知环境，构建和更新具备实体中心结构的多模态长期记忆（包括情节和语义记忆），并通过多轮推理检索相关记忆以自主完成复杂任务。论文还开发了M3-Bench长视频问答基准，涵盖机器人视角和网络多场景，实验结果表明M3-Agent在多个基准上均大幅超越现有多模态大模型和视频理解方法。结论：M3-Agent显著提升了多模态智能体的长期记忆和推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Seeing%2C_Listening%2C_Remembering%2C_and_Reasoning_A_Multimodal_Agent_with_Long-Term_Memory.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-Immersive-Teleoperation-of-Beyond-Human-Scale-Robotic-Manipulators-Challenges-and-Future-Directions"><a href="#13-Immersive-Teleoperation-of-Beyond-Human-Scale-Robotic-Manipulators-Challenges-and-Future-Directions" class="headerlink" title="13. Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Immersive_Teleoperation_of_Beyond-Human-Scale_Robotic_Manipulators__Challenges_and_Future_Directions.pdf">Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tampere University</span></p><p>该论文系统分析了超越人类尺度机器人操控（BHSRMs）沉浸式远程操作的控制、认知与界面设计挑战，提出通过高保真视觉与全臂力反馈（如外骨骼）提升操作员的沉浸感和身体感，实现更自然的人机协作。结论指出，沉浸式界面虽提升操作自然性和数据质量，但也带来感知冲突和安全风险，需发展新的人因评价工具和自适应反馈策略，为大规模学习驱动的远程机器人系统奠定基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Immersive_Teleoperation_of_Beyond-Human-Scale_Robotic_Manipulators_Challenges_and_Future_Directions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Interpretable-Robot-Control-via-Structured-Behavior-Trees-and-Large-Language-Models"><a href="#14-Interpretable-Robot-Control-via-Structured-Behavior-Trees-and-Large-Language-Models" class="headerlink" title="14. Interpretable Robot Control via Structured Behavior Trees and Large Language Models"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Interpretable_Robot_Control_via_Structured_Behavior_Trees_and_Large_Language_Models.pdf">Interpretable Robot Control via Structured Behavior Trees and Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Luxembourg</span></p><p>本文提出了一种将大型语言模型（LLM）与行为树（BT）相结合的机器人控制框架，通过自然语言指令实现对机器人的可解释、模块化和实时控制。方法包括基于GPT-4o的语义解析、自动行为选择、结构化行为树执行以及插件式感知模块（如目标跟踪、手势识别）；实验结果显示该系统在多平台环境下指令理解与动作执行的平均准确率达到94%，有效提升了机器人在人机交互中的适应性和透明度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Interpretable_Robot_Control_via_Structured_Behavior_Trees_and_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-HapticGiant-A-Novel-Very-Large-Kinesthetic-Haptic-Interface-with-Hierarchical-Force-Control"><a href="#15-HapticGiant-A-Novel-Very-Large-Kinesthetic-Haptic-Interface-with-Hierarchical-Force-Control" class="headerlink" title="15. HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/HapticGiant__A_Novel_Very_Large_Kinesthetic_Haptic_Interface_with_Hierarchical_Force_Control.pdf">HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Karlsruhe Institute of Technology</span></p><p>本文提出HapticGiant，一种超大型贴合人体手臂运动学的本体型动力触觉界面，结合2D龙门预定位单元和自定义六自由度机械臂，实现房间级空间内的自然步行和全方位触觉反馈。方法上创新性采用分层优化的感受型力控制算法，统一处理关节、工作空间和奇异点等系统约束，实现任意串联运动链和笛卡尔感受的实时触觉渲染。实验展示了系统在虚拟门和挖掘机等数字孪生交互中的高度沉浸性和可用性。结论：HapticGiant具备极大空间和优秀运动学匹配，通过高层控制方案显著提升虚拟现实中的沉浸式动力触觉体验。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HapticGiant_A_Novel_Very_Large_Kinesthetic_Haptic_Interface_with_Hierarchical_Force_Control.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-BEAVR-Bimanual-multi-Embodiment-Accessible-Virtual-Reality-Teleoperation-System-for-Robots"><a href="#16-BEAVR-Bimanual-multi-Embodiment-Accessible-Virtual-Reality-Teleoperation-System-for-Robots" class="headerlink" title="16. BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/BEAVR__Bimanual,_multi-Embodiment,_Accessible,_Virtual_Reality_Teleoperation_System_for_Robots.pdf">BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Massachusetts Institute of Technology</span></p><p>本论文提出了BEAVR系统——一个开源、模块化、硬件无关的虚拟现实（VR）机器人远程操作平台，支持7-DoF机械臂到全身人形机器人等多种机器人形态，通过低延迟（≤35 ms）流媒体架构和异步“think–act”控制环，实现实时灵巧双手操作、标准化数据采集（LeRobot格式）及端到端策略学习。实验表明，BEAVR兼容多种主流视觉-运动策略（ACT、DiffusionPolicy、SmolVLA），在多任务、多机器人环境下保持高成功率和低时延，极大提升了远程操作系统的可扩展性和学习友好性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/BEAVR_Bimanual%2C_multi-Embodiment%2C_Accessible%2C_Virtual_Reality_Teleoperation_System_for_Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-EvoCurr-Self-evolving-Curriculum-with-Behavior-Code-Generation-for-Complex-Decision-making"><a href="#17-EvoCurr-Self-evolving-Curriculum-with-Behavior-Code-Generation-for-Complex-Decision-making" class="headerlink" title="17. EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/EvoCurr__Self-evolving_Curriculum_with_Behavior_Code_Generation_for_Complex_Decision-making.pdf">EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhongguancun Academy</span></p><p>本文提出了EvoCurr框架，通过两个大型语言模型（LLM）分别担任自适应课程设计和行为代码生成角色，实现复杂决策任务（以StarCraft II为例）中的逐步难度提升与策略进化。方法包括课程设计LLM根据行为代码LLM的表现动态调整任务难度，并生成适应性决策树脚本。实验表明，EvoCurr显著提升了复杂任务的成功率和解题效率，证明了自进化课程学习在高复杂度自动推理中的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/EvoCurr_Self-evolving_Curriculum_with_Behavior_Code_Generation_for_Complex_Decision-making.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-Skyshield-Event-Driven-Submillimetre-Thin-Obstacle-Detection-for-Drone-Flight-Safety"><a href="#18-Skyshield-Event-Driven-Submillimetre-Thin-Obstacle-Detection-for-Drone-Flight-Safety" class="headerlink" title="18. Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Skyshield__Event-Driven_Submillimetre_Thin_Obstacle_Detection_for_Drone_Flight_Safety.pdf">Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen International Graduate School, Tsinghua University</span></p><p>本论文提出了Skyshield系统，基于事件相机，采用轻量级U-Net（LUnet）网络和创新的Dice-Contour Regularization损失，实现对无人机亚毫米级细障碍物的实时检测。实验表明，该方法在边缘平台上显著优于传统方法，具有高精度和低延迟，提升了无人机的飞行安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Skyshield_Event-Driven_Submillimetre_Thin_Obstacle_Detection_for_Drone_Flight_Safety.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Decentralized-Rank-Scheduling-for-Energy-Constrained-Multi-Task-Federated-Fine-Tuning-in-Edge-Assisted-IoV-Networks"><a href="#19-Decentralized-Rank-Scheduling-for-Energy-Constrained-Multi-Task-Federated-Fine-Tuning-in-Edge-Assisted-IoV-Networks" class="headerlink" title="19. Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Decentralized_Rank_Scheduling_for_Energy-Constrained_Multi-Task_Federated_Fine-Tuning_in_Edge-Assist.pdf">Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-sen University</span></p><p>本论文提出一种分层联邦微调框架，结合LoRA低秩适配与去中心化能耗约束的rank优化，适用于动态、资源受限的车联网（IoV）场景。方法上，创新性地将LoRA rank选择建模为带能耗约束的多臂赌博机(MAB)问题，并提出UCB-DUAL分布式算法，实现任务级能耗分配与客户端级自适应rank分配，有理论保证的次线性遗憾。实验结果表明，该方法在大规模仿真下可提升平均精度2.5%，降低延迟24%，同时减少显存占用，优于多种基线。结论：该框架能高效实现车联网边缘智能的多任务自适应联邦微调。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Decentralized_Rank_Scheduling_for_Energy-Constrained_Multi-Task_Federated_Fine-Tuning_in_Edge-Assisted_IoV_Networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-Distilling-LLM-Prior-to-Flow-Model-for-Generalizable-Agent’s-Imagination-in-Object-Goal-Navigation"><a href="#20-Distilling-LLM-Prior-to-Flow-Model-for-Generalizable-Agent’s-Imagination-in-Object-Goal-Navigation" class="headerlink" title="20. Distilling LLM Prior to Flow Model for Generalizable Agent’s Imagination in Object Goal Navigation"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Distilling_LLM_Prior_to_Flow_Model_for_Generalizable_Agent's_Imagination_in_Object_Goal_Navigation.pdf">Distilling LLM Prior to Flow Model for Generalizable Agent’s Imagination in Object Goal Navigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science and Engineering, Sun Yat-sen University</span></p><p>该论文提出GOAL框架，通过在训练阶段将大语言模型（LLM）推理得到的空间语义先验以高斯场形式注入到语义地图，结合生成式流模型对室内场景的语义分布进行建模，实现了对未观测区域的高质量推理。实验表明，GOAL在Gibson、MP3D及迁移到HM3D等数据集上均取得了当前最优的导航表现，显著提升了泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Distilling_LLM_Prior_to_Flow_Model_for_Generalizable_Agent%E2%80%99s_Imagination_in_Object_Goal_Navigation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="21-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation"><a href="#21-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation" class="headerlink" title="21. Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Echo-4o__Harnessing_the_Power_of_GPT-4o_Synthetic_Images_for_Improved_Image_Generation.pdf">Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Artificial Intelligence Laboratory</span></p><p>本论文提出Echo-4o-Image，一个由GPT-4o生成的18万规模合成图像数据集，涵盖超现实场景、多参考图像生成和复杂指令跟随任务，专为弥补真实世界数据稀缺场景和提升指令对齐而设计。在此基础上，作者微调统一多模态生成模型Bagel得到Echo-4o，并引入GenEval++和Imagine-Bench两个新基准，系统评测指令跟随与创意生成能力。实验表明Echo-4o在多个标准和新基准下均取得领先，且合成数据对多种主流基础模型均有显著迁移增益，验证了高质量合成数据集对提升图像生成模型多样性、指令对齐及创新能力的价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Echo-4o_Harnessing_the_Power_of_GPT-4o_Synthetic_Images_for_Improved_Image_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-A-Survey-on-3D-Gaussian-Splatting-Applications-Segmentation-Editing-and-Generation"><a href="#22-A-Survey-on-3D-Gaussian-Splatting-Applications-Segmentation-Editing-and-Generation" class="headerlink" title="22. A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_Survey_on_3D_Gaussian_Splatting_Applications__Segmentation,_Editing,_and_Generation.pdf">A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai University of Finance and Economics</span></p><p>该论文系统综述了3D Gaussian Splatting（3DGS）在分割、编辑和生成等下游视觉任务中的最新进展，详细介绍了2D基础模型（如CLIP、SAM、DINO等）与3DGS的结合方式，并针对三大应用方向梳理了代表性方法、学习范式和评测基准。结论指出3DGS在场景理解、编辑和生成等高层视觉任务中展现出高效、可扩展和语义丰富的能力，并提出未来需完善大规模训练、三维评价体系与多模态语言模型整合。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A_Survey_on_3D_Gaussian_Splatting_Applications_Segmentation%2C_Editing%2C_and_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation"><a href="#23-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation" class="headerlink" title="23. Story2Board: A Training-Free Approach for Expressive Storyboard Generation"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Story2Board__A_Training-Free_Approach_for_Expressive_Storyboard_Generation.pdf">Story2Board: A Training-Free Approach for Expressive Storyboard Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hebrew University of Jerusalem</span></p><p>Story2Board提出了一种训练无关的框架，实现从自然语言生成富有表现力且一致的多面板故事板。方法核心包括Latent Panel Anchoring和Reciprocal Attention Value Mixing，增强扩散模型在多场景下角色一致性和布局多样性，无需模型微调或架构修改。实验显示该方法在角色一致性、场景多样性及用户偏好上优于现有基线，提升了视觉叙事的表现力和连贯性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Story2Board_A_Training-Free_Approach_for_Expressive_Storyboard_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models"><a href="#24-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models" class="headerlink" title="24. Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Noise_Hypernetworks__Amortizing_Test-Time_Compute_in_Diffusion_Models.pdf">Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Munich</span></p><p>该论文提出了一种用于扩散生成模型的Noise Hypernetworks（HyperNoise）方法，通过训练一个轻量级超网络直接预测优化后的初始噪声，从而将原本消耗大量计算资源的测试时噪声优化过程转移到训练后阶段。实验结果表明，该方法在保持极低推理延迟的情况下，显著提升了当前主流文本到图像扩散模型的生成质量，有效避免了直接微调带来的模式崩溃和“奖励黑客”问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Noise_Hypernetworks_Amortizing_Test-Time_Compute_in_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-MOC-Meta-Optimized-Classifier-for-Few-Shot-Whole-Slide-Image-Classification"><a href="#25-MOC-Meta-Optimized-Classifier-for-Few-Shot-Whole-Slide-Image-Classification" class="headerlink" title="25. MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/MOC__Meta-Optimized_Classifier_for_Few-Shot_Whole_Slide_Image_Classification.pdf">MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>本论文提出了一种面向少样本全切片图像（WSI）分类的新型Meta-Optimized Classifier (MOC)。其核心方法包含一个自动从多种分类器中动态配置的meta-learner和一个包含多样候选分类器的分类器库，使分类器在不同病理特征上互补，对全切片图像进行更全面的诊断。实验表明，MOC在多个少样本基准数据集上优于现有方法，AUC提升最高可达26.25%，为临床数据稀缺场景下的诊断部署提供关键进展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/MOC_Meta-Optimized_Classifier_for_Few-Shot_Whole_Slide_Image_Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-T-CACE-A-Time-Conditioned-Autoregressive-Contrast-Enhancement-Multi-Task-Framework-for-Contrast-Free-Liver-MRI-Synthesis-Segmentation-and-Diagnosis"><a href="#26-T-CACE-A-Time-Conditioned-Autoregressive-Contrast-Enhancement-Multi-Task-Framework-for-Contrast-Free-Liver-MRI-Synthesis-Segmentation-and-Diagnosis" class="headerlink" title="26. T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/T-CACE__A_Time-Conditioned_Autoregressive_Contrast_Enhancement_Multi-Task_Framework_for_Contrast-Fre.pdf">T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Toronto Metropolitan University</span></p><p>该论文提出了一种时间条件自回归对比增强（T-CACE）多任务框架，实现了无需造影剂的肝脏MRI多期合成、分割与诊断。核心方法包括条件Token编码（CTE）融合解剖和时间信息、动态时间感知注意力掩码（DTAM）实现相位间平滑过渡，以及时间分类一致性约束（TCC）保证诊断输出的生理一致性。大量实验表明T-CACE在图像合成、分割和病灶分类方面均超越现有方法，并为临床提供更安全高效的诊断替代方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/T-CACE_A_Time-Conditioned_Autoregressive_Contrast_Enhancement_Multi-Task_Framework_for_Contrast-Free_Liver_MRI_Synthesis%2C_Segmentation%2C_and_Diagnosis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-HumanGenesis-Agent-Based-Geometric-and-Generative-Modeling-for-Synthetic-Human-Dynamics"><a href="#27-HumanGenesis-Agent-Based-Geometric-and-Generative-Modeling-for-Synthetic-Human-Dynamics" class="headerlink" title="27. HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/HumanGenesis__Agent-Based_Geometric_and_Generative_Modeling_for_Synthetic_Human_Dynamics.pdf">HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-sen University</span></p><p>HumanGenesis提出了一种多代理协同框架，结合3D高斯点渲染、人体和场景分离重建、时序姿态编码、视频扩散生成以及基于多模态大模型的细粒度自反思优化。结论显示该方法在文生动作、视频重演和新姿态生成等任务上，实现了运动表达性、几何保真度和场景融合的显著提升，优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HumanGenesis_Agent-Based_Geometric_and_Generative_Modeling_for_Synthetic_Human_Dynamics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-Perceptual-Reality-Transformer-Neural-Architectures-for-Simulating-Neurological-Perception-Conditions"><a href="#28-Perceptual-Reality-Transformer-Neural-Architectures-for-Simulating-Neurological-Perception-Conditions" class="headerlink" title="28. Perceptual Reality Transformer: Neural Architectures for Simulating Neurological Perception Conditions"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Perceptual_Reality_Transformer__Neural_Architectures_for_Simulating_Neurological_Perception_Conditio.pdf">Perceptual Reality Transformer: Neural Architectures for Simulating Neurological Perception Conditions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Icahn School of Medicine at Mount Sinai</span></p><p>该论文提出了Perceptual Reality Transformer框架，利用六种神经网络架构（包括Vision Transformer、CNN、VAE、Diffusion等），结合临床文献构建特定神经感知障碍的视觉仿真函数，实现对八类神经感知状态的图像转化模拟。在ImageNet和CIFAR-10等基准数据集的系统实验表明，Vision Transformer在重建精度和全局上下文模拟方面表现最佳，为神经感知障碍仿真建立了首个系统性基准，并为医学教育和同理训练等应用提供了新工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Perceptual_Reality_Transformer_Neural_Architectures_for_Simulating_Neurological_Perception_Conditions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Enhancing-Diffusion-Face-Generation-with-Contrastive-Embeddings-and-SegFormer-Guidance"><a href="#29-Enhancing-Diffusion-Face-Generation-with-Contrastive-Embeddings-and-SegFormer-Guidance" class="headerlink" title="29. Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Enhancing_Diffusion_Face_Generation_with_Contrastive_Embeddings_and_SegFormer_Guidance.pdf">Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Surrey</span></p><p>本论文提出在人脸生成领域中，结合InfoNCE对比损失优化属性嵌入，以及采用SegFormer作为分割掩码编码器，提升扩散模型的条件生成能力。通过在CelebAMask-HQ小规模数据集上，系统评测UNet与DiT架构、Stable Diffusion LoRA微调、属性与分割联合控制，结果显示联合InfoNCE与SegFormer大幅提升了属性一致性与空间可控性，实现了更高质量的人脸生成（FID最低可达63.85）。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Enhancing_Diffusion_Face_Generation_with_Contrastive_Embeddings_and_SegFormer_Guidance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-Hierarchical-Graph-Attention-Network-for-No-Reference-Omnidirectional-Image-Quality-Assessment"><a href="#30-Hierarchical-Graph-Attention-Network-for-No-Reference-Omnidirectional-Image-Quality-Assessment" class="headerlink" title="30. Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Hierarchical_Graph_Attention_Network_for_No-Reference_Omnidirectional_Image_Quality_Assessment.pdf">Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Guangdong University of Technology</span></p><p>该论文提出一种基于图神经网络的无参考全景图像质量评估方法，通过Fibonacci球面采样生成视口，并利用Swin Transformer提取多级特征作为图节点，结合图注意力网络（GAT）建模局部结构关系和图Transformer捕获长距离空间依赖，实现了对空间非均匀失真的细致建模。实验结果显示，该方法在JUFE-10K和OIQ-10K大型数据集上显著优于现有方法，具备较强的泛化能力和预测准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Hierarchical_Graph_Attention_Network_for_No-Reference_Omnidirectional_Image_Quality_Assessment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-Reforming-SISR-Models-for-PBR-Material-Super-Resolution-via-Cross-Map-Attention"><a href="#31-Reforming-SISR-Models-for-PBR-Material-Super-Resolution-via-Cross-Map-Attention" class="headerlink" title="31. Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/MUJICA__Reforming_SISR_Models_for_PBR_Material_Super-Resolution_via_Cross-Map_Attention.pdf">Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ubisoft La Forge</span></p><p>本文提出了MUJICA方法，通过将跨图融合注意力机制作为适配器接入预训练的Swin-transformer单图超分辨率模型，实现对物理渲染材料（PBR）多模态纹理图（如basecolor、normal等）的高效超分辨率重建。MUJICA模块支持冻结主干，仅少量参数训练，融合多模态特征显著提升PSNR、SSIM和LPIPS指标，并保持跨图一致性，在有限数据下达到SOTA性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Reforming_SISR_Models_for_PBR_Material_Super-Resolution_via_Cross-Map_Attention.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-DSS-Prompt-Dynamic-Static-Synergistic-Prompting-for-Few-Shot-Class-Incremental-Learning"><a href="#32-DSS-Prompt-Dynamic-Static-Synergistic-Prompting-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="32. DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/DSS-Prompt__Dynamic-Static_Synergistic_Prompting_for_Few-Shot_Class-Incremental_Learning.pdf">DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了一种称为DSS-Prompt的新方法，通过在每个Vision Transformer块中协同利用静态和动态多模态提示，实现少样本类增量学习（FSCIL）。该方法利用静态提示弥合预训练与下游数据集的领域差异，并通过多模态动态提示提升实例感知语义，从而在冻结主干网络的前提下提升泛化性与新类适应性。实验表明，该方法在四个主流基准上显著优于现有方法，并有效缓解灾难性遗忘问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/DSS-Prompt_Dynamic-Static_Synergistic_Prompting_for_Few-Shot_Class-Incremental_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-Stable-Diffusion-Models-are-Secretly-Good-at-Visual-In-Context-Learning"><a href="#33-Stable-Diffusion-Models-are-Secretly-Good-at-Visual-In-Context-Learning" class="headerlink" title="33. Stable Diffusion Models are Secretly Good at Visual In-Context Learning"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Stable_Diffusion_Models_are_Secretly_Good_at_Visual_In-Context_Learning.pdf">Stable Diffusion Models are Secretly Good at Visual In-Context Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Apple</span></p><p>本文提出了一种完全无需训练的新型视觉in-context learning（V-ICL）方法，直接对现成的Stable Diffusion模型自注意力层进行in-place attention重计算，显式引入查询图与示例提示之间的上下文，并提出隐式加权的多提示集成策略。结果表明该方法无需额外训练即可在前景分割、单目标检测、语义分割、关键点检测、边缘检测和彩色化六项任务上大幅优于现有训练或微调的V-ICL方法，且泛化性更强。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Stable_Diffusion_Models_are_Secretly_Good_at_Visual_In-Context_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-Prototype-Guided-Diffusion-Visual-Conditioning-without-External-Memory"><a href="#34-Prototype-Guided-Diffusion-Visual-Conditioning-without-External-Memory" class="headerlink" title="34. Prototype-Guided Diffusion: Visual Conditioning without External Memory"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Prototype-Guided_Diffusion__Visual_Conditioning_without_External_Memory.pdf">Prototype-Guided Diffusion: Visual Conditioning without External Memory</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Université Sorbonne Paris Nord</span></p><p>本文提出了Prototype Diffusion Model (PDM)，在扩散模型训练过程中直接集成原型学习，通过对无标签图像特征进行动态对比学习和原型分配，实现无需外部记忆即可进行高效视觉条件生成。实验表明，PDM在不依赖外部检索系统和CLIP等预训练模型的情况下，能以较低计算和存储开销达到与检索增强扩散方法相当甚至更优的生成质量，且其有监督变体(s-PDM)在有标签数据下进一步提升了图像质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Prototype-Guided_Diffusion_Visual_Conditioning_without_External_Memory.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-SpeechForensics-Audio-Visual-Speech-Representation-Learning-for-Face-Forgery-Detection"><a href="#35-SpeechForensics-Audio-Visual-Speech-Representation-Learning-for-Face-Forgery-Detection" class="headerlink" title="35. SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/SpeechForensics__Audio-Visual_Speech_Representation_Learning_for_Face_Forgery_Detection.pdf">SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Information Engineering, Chinese Academy of Sciences</span></p><p>该论文提出了一种基于语音和视觉双模态语音表示学习的无监督人脸伪造检测方法。通过在真实视频上自监督学习音频-视觉语音表示，融合局部和全局语义信息，对伪造视频中音频与唇动的不一致进行检测，无需伪造数据参与训练。实验结果显示，该方法在跨数据集泛化和鲁棒性方面显著优于现有方法，能有效识别多种伪造类型并对多种常见扰动保持高鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/SpeechForensics_Audio-Visual_Speech_Representation_Learning_for_Face_Forgery_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-OneVAE-Joint-Discrete-and-Continuous-Optimization-Helps-Discrete-Video-VAE-Train-Better"><a href="#36-OneVAE-Joint-Discrete-and-Continuous-Optimization-Helps-Discrete-Video-VAE-Train-Better" class="headerlink" title="36. OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/OneVAE__Joint_Discrete_and_Continuous_Optimization_Helps_Discrete_Video_VAE_Train_Better.pdf">OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nankai University</span></p><p>本文提出OneVAE，一种统一离散与连续视频表示的VAE框架，采用渐进式训练策略，利用预训练连续VAE先验加速离散VAE收敛，并引入多token量化与首帧增强等结构改进，有效提升高压缩下的重建质量。实验结果表明，该方法大幅加快训练速度，提升视频重建与统一建模性能，为视频生成和VAE研究提供新方向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/OneVAE_Joint_Discrete_and_Continuous_Optimization_Helps_Discrete_Video_VAE_Train_Better.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-Do-Vision-Transformers-See-Like-Humans-Evaluating-their-Perceptual-Alignment"><a href="#37-Do-Vision-Transformers-See-Like-Humans-Evaluating-their-Perceptual-Alignment" class="headerlink" title="37. Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Do_Vision_Transformers_See_Like_Humans__Evaluating_their_Perceptual_Alignment.pdf">Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universidad de Valencia</span></p><p>该论文系统分析了模型规模、数据集规模、数据增强和正则化对Vision Transformer（ViT）与人类感知一致性的影响，采用TID2013图像质量数据集，基于模型输出与人类评分的相关性量化感知对齐度。结果表明，增加模型规模、数据增强和正则化虽提升分类性能但会降低与人类感知的一致性，提示模型性能与感知对齐存在权衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Do_Vision_Transformers_See_Like_Humans_Evaluating_their_Perceptual_Alignment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-Reverse-Convolution-and-Its-Applications-to-Image-Restoration"><a href="#38-Reverse-Convolution-and-Its-Applications-to-Image-Restoration" class="headerlink" title="38. Reverse Convolution and Its Applications to Image Restoration"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Reverse_Convolution_and_Its_Applications_to_Image_Restoration.pdf">Reverse Convolution and Its Applications to Image Restoration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本文提出了一种新颖的深度可逆卷积（reverse convolution）算子，作为深度卷积的数学逆，通过正则化最小二乘优化获得闭式解，并结合层归一化、1×1卷积和GELU激活构建了Transformer风格的逆卷积模块（Converse Block）。该算子可无缝替换现有网络中的标准卷积和转置卷积，并在图像去噪、超分辨率和去模糊等图像复原任务中进行了广泛实验，结果表明其在恢复图像细节和提升性能方面优于传统方法。结论是该方法为深度模型设计提供了原理性可逆算子，具有良好应用前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Reverse_Convolution_and_Its_Applications_to_Image_Restoration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-Region-to-Region-Enhancing-Generative-Image-Harmonization-with-Adaptive-Regional-Injection"><a href="#39-Region-to-Region-Enhancing-Generative-Image-Harmonization-with-Adaptive-Regional-Injection" class="headerlink" title="39. Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Region-to-Region__Enhancing_Generative_Image_Harmonization_with_Adaptive_Regional_Injection.pdf">Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本文提出Region-to-Region转化方法，通过将适当区域的信息注入前景，实现图像细节保留与和谐化，并生成新型复合数据。在此基础上，作者设计了R2R模型，结合Clear-VAE（带自适应滤波器）和Harmony Controller（含掩码感知自适应通道注意力MACA），有效提升了图像和谐化能力。实验表明，该方法在iHarmony4和新构建的RPHarmony数据集上均取得了最优结果，生成图像在真实场景中表现出更好的和谐性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Region-to-Region_Enhancing_Generative_Image_Harmonization_with_Adaptive_Regional_Injection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-NEURAL-Attention-Guided-Pruning-for-Unified-Multimodal-Resource-Constrained-Clinical-Evaluation"><a href="#40-NEURAL-Attention-Guided-Pruning-for-Unified-Multimodal-Resource-Constrained-Clinical-Evaluation" class="headerlink" title="40. NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/NEURAL__Attention-Guided_Pruning_for_Unified_Multimodal_Resource-Constrained_Clinical_Evaluation.pdf">NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>该论文提出NEURAL框架，通过将胸部X光图像划分为patch并利用Fine-tuned Vision-Language Model（如ClinicalT5）报告生成过程中的cross-attention分数，结构性剪枝医学图像，仅保留诊断关键区域，并与基于报告的知识图谱融合成统一图结构，用于高效的下游诊断任务。实验证明，在MIMIC-CXR和CheXpert Plus两大数据集上，NEURAL实现了93.4-97.7%的图像数据压缩，AUC达0.88-0.95，显著优于未压缩基线模型，且保持高诊断性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/NEURAL_Attention-Guided_Pruning_for_Unified_Multimodal_Resource-Constrained_Clinical_Evaluation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors"><a href="#41-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors" class="headerlink" title="41. GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/GSFixer__Improving_3D_Gaussian_Splatting_with_Reference-Guided_Video_Diffusion_Priors.pdf">GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Macau</span></p><p>该论文提出GSFixer，一种利用参考引导的视频扩散模型，结合2D语义特征和3D几何特征，有效修复稀疏视角下3D Gaussian Splatting重建中的伪影问题。通过引入参考引导的轨迹采样策略和DL3DV-Res基准评测，GSFixer在三维伪影修复和稀疏视角三维重建任务上显著优于现有方法，提升了新视角合成的一致性和重建质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/GSFixer_Improving_3D_Gaussian_Splatting_with_Reference-Guided_Video_Diffusion_Priors.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Semantic-aware-DropSplat-Adaptive-Pruning-of-Redundant-Gaussians-for-3D-Aerial-View-Segmentation"><a href="#42-Semantic-aware-DropSplat-Adaptive-Pruning-of-Redundant-Gaussians-for-3D-Aerial-View-Segmentation" class="headerlink" title="42. Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Semantic-aware_DropSplat__Adaptive_Pruning_of_Redundant_Gaussians_for_3D_Aerial-View_Segmentation.pdf">Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xidian University</span></p><p>该论文提出了SAD-Splat方法用于3D航空视角场景语义分割，结合高置信伪标签生成和语义置信度引导的可学习稀疏机制（基于Hard Concrete分布），通过自适应剪枝冗余和语义不明确的高斯点，提升多视角3D语义分割的表现与模型紧凑性，并构建了挑战性数据集3D-AS。实验表明，该方法在减少高斯点数量的同时提高了分割精度，为高效的3D场景理解提供了新思路。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Semantic-aware_DropSplat_Adaptive_Pruning_of_Redundant_Gaussians_for_3D_Aerial-View_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-BridgeTA-Bridging-the-Representation-Gap-in-Knowledge-Distillation-via-Teacher-Assistant-for-Bird’s-Eye-View-Map-Segmentation"><a href="#43-BridgeTA-Bridging-the-Representation-Gap-in-Knowledge-Distillation-via-Teacher-Assistant-for-Bird’s-Eye-View-Map-Segmentation" class="headerlink" title="43. BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird’s Eye View Map Segmentation"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/BridgeTA__Bridging_the_Representation_Gap_in_Knowledge_Distillation_via_Teacher_Assistant_for_Bird's.pdf">BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird’s Eye View Map Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yonsei University</span></p><p>该论文提出BridgeTA，一种针对BEV（鸟瞰图）语义分割的高效知识蒸馏框架，通过一个轻量级Teacher Assistant（TA）网络，在不增加学生模型推理成本的前提下，桥接LiDAR-摄像头融合模型和仅摄像头模型之间的表征鸿沟。方法理论上利用Young不等式分解蒸馏路径，结合多层次特征蒸馏（特征、解码、logit），显著提升了学生模型性能。最终在nuScenes数据集上，BridgeTA相较于Camera-only基线提升4.2% mIoU，且推理速度与开销不变，优于其他SOTA方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/BridgeTA_Bridging_the_Representation_Gap_in_Knowledge_Distillation_via_Teacher_Assistant_for_Bird%E2%80%99s_Eye_View_Map_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Dual-Recursive-Feedback-on-Generation-and-Appearance-Latents-for-Pose-Robust-Text-to-Image-Diffusion"><a href="#44-Dual-Recursive-Feedback-on-Generation-and-Appearance-Latents-for-Pose-Robust-Text-to-Image-Diffusion" class="headerlink" title="44. Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Dual_Recursive_Feedback_on_Generation_and_Appearance_Latents_for_Pose-Robust_Text-to-Image_Diffusion.pdf">Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea University</span></p><p>该论文提出一种无需训练的Dual Recursive Feedback (DRF)方法，实现对结构和外观的双重递归反馈，增强Text-to-Image扩散模型在姿态迁移和结构-外观融合任务中的控制力。通过递归地对生成潜变量和外观潜变量施加反馈，方法有效保持结构一致性和外观保真度，即使在类别无关场景（如将人类动作迁移到动物外观）下也能生成高质量、结构与语义一致的图像。实验显示，DRF在多种基准任务和指标上均优于现有主流方法，支持不同扩散模型框架的无缝集成，显著提升生成图像的结构和外观融合效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Dual_Recursive_Feedback_on_Generation_and_Appearance_Latents_for_Pose-Robust_Text-to-Image_Diffusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-WEC-DG-Multi-Exposure-Wavelet-Correction-Method-Guided-by-Degradation-Description"><a href="#45-WEC-DG-Multi-Exposure-Wavelet-Correction-Method-Guided-by-Degradation-Description" class="headerlink" title="45. WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/WEC-DG__Multi-Exposure_Wavelet_Correction_Method_Guided_by_Degradation_Description.pdf">WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Jilin University</span></p><p>本论文提出了一种结合退化描述指导和离散小波变换的多曝光图像增强方法WEC-DG。方法核心包括场景描述生成模块（SDGM）、曝光一致性校准模块（ECAM）和曝光恢复与细节重建模块（EDRM），通过退化描述符引导曝光校准，分阶段分别优化照明与细节。实验表明，该方法在多公开数据集上性能超越现有方法，显著提升了复杂成像环境下的曝光校正效果和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/WEC-DG_Multi-Exposure_Wavelet_Correction_Method_Guided_by_Degradation_Description.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-Exploring-the-Equivalence-of-Closed-Set-Generative-and-Real-Data-Augmentation-in-Image-Classification"><a href="#46-Exploring-the-Equivalence-of-Closed-Set-Generative-and-Real-Data-Augmentation-in-Image-Classification" class="headerlink" title="46. Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Exploring_the_Equivalence_of_Closed-Set_Generative_and_Real_Data_Augmentation_in_Image_Classificatio.pdf">Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, San Diego</span></p><p>本文系统研究了在图像分类任务中，利用在同一数据集上训练的扩散生成模型（即closed-set generative data augmentation）合成数据与真实数据增强的等效性。方法上，作者在自然图像（CIFAR-10、ImageNet-100）和医学图像（BloodMNIST）上，通过从零训练扩散模型生成合成样本，并与真实数据增强进行对比，提出了量化两者等效性的经验公式。结论指出，合成数据可提升分类性能，但达到同等提升需更多合成数据，且合成数据的边际效益随规模增加递减，对基础训练集较大时扩展性更好，为缺乏真实数据的应用提供实证指导。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Exploring_the_Equivalence_of_Closed-Set_Generative_and_Real_Data_Augmentation_in_Image_Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-Multimodal-Sheaf-based-Network-for-Glioblastoma-Molecular-Subtype-Prediction"><a href="#47-Multimodal-Sheaf-based-Network-for-Glioblastoma-Molecular-Subtype-Prediction" class="headerlink" title="47. Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Multimodal_Sheaf-based_Network_for_Glioblastoma_Molecular_Subtype_Prediction.pdf">Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>本文提出了一种基于sheaf神经网络的多模态结构感知框架，用于融合MRI与组织病理图像，实现胶质母细胞瘤分子亚型预测。该方法通过sheaf理论建模多模态数据的拓扑结构，实现缺失模态下的鲁棒分类与重建，实验结果在分子亚型多标签分类和缺失模态重建任务上均优于现有多模态图神经网络基线。结论：该方法能有效融合医学影像多模态特征，提升诊断准确性，具备临床虚拟活检潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Multimodal_Sheaf-based_Network_for_Glioblastoma_Molecular_Subtype_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-MangaDiT-Reference-Guided-Line-Art-Colorization-with-Hierarchical-Attention-in-Diffusion-Transformers"><a href="#48-MangaDiT-Reference-Guided-Line-Art-Colorization-with-Hierarchical-Attention-in-Diffusion-Transformers" class="headerlink" title="48. MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/MangaDiT__Reference-Guided_Line_Art_Colorization_with_Hierarchical_Attention_in_Diffusion_Transforme.pdf">MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CyberAgent</span></p><p>该论文提出MangaDiT模型，结合Diffusion Transformer架构和分层注意力机制，通过动态加权策略提升参考引导线稿上色的区域级一致性。实验结果显示，在角色姿态或动作变化较大的场景下，MangaDiT在两个基准数据集上显著优于现有方法，提升了定量和定性表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/MangaDiT_Reference-Guided_Line_Art_Colorization_with_Hierarchical_Attention_in_Diffusion_Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-NegFaceDiff-The-Power-of-Negative-Context-in-Identity-Conditioned-Diffusion-for-Synthetic-Face-Generation"><a href="#49-NegFaceDiff-The-Power-of-Negative-Context-in-Identity-Conditioned-Diffusion-for-Synthetic-Face-Generation" class="headerlink" title="49. NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/NegFaceDiff__The_Power_of_Negative_Context_in_Identity-Conditioned_Diffusion_for_Synthetic_Face_Gene.pdf">NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fraunhofer IGD</span></p><p>该论文提出NegFaceDiff，一种在身份条件扩散模型采样阶段引入负条件的新方法，通过在生成过程中同时施加正负身份嵌入，实现对期望身份特征的保持和对不相关身份特征的排斥，从而提升合成人脸数据的身份一致性和分离性。实验结果表明，NegFaceDiff生成的数据不仅提升了身份区分度（FDR从2.427提升到5.687，EER从0.130降至0.039），而且用其训练的人脸识别模型在多个基准数据集上均优于无负条件的对照方法，同时降低了人种偏差，提升了公平性和准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/NegFaceDiff_The_Power_of_Negative_Context_in_Identity-Conditioned_Diffusion_for_Synthetic_Face_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-The-Brain-Resection-Multimodal-Image-Registration-ReMIND2Reg-2025-Challenge"><a href="#50-The-Brain-Resection-Multimodal-Image-Registration-ReMIND2Reg-2025-Challenge" class="headerlink" title="50. The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/The_Brain_Resection_Multimodal_Image_Registration_(ReMIND2Reg)_2025_Challenge.pdf">The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Inria Saclay Île-de-France, CEA, Université Paris-Saclay</span></p><p>本论文介绍了ReMIND2Reg 2025挑战赛，该挑战旨在为脑肿瘤术前MRI与术后3D超声（iUS）图像配准提供最大、标准化的公开基准。方法包括发布配对的多模态医学影像数据集，并采用目标配准误差（TRE）、最差配准鲁棒性（TRE30）和运行时等多项指标进行统一评估，推动鲁棒、通用及可临床部署的配准算法发展。结论认为，该挑战为神经外科手术中的多模态图像配准研究提供了宝贵资源和评测标准，有助于提升手术精度与患者预后。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/The_Brain_Resection_Multimodal_Image_Registration_%28ReMIND2Reg%29_2025_Challenge.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-Enhancing-Monocular-3D-Hand-Reconstruction-with-Learned-Texture-Priors"><a href="#51-Enhancing-Monocular-3D-Hand-Reconstruction-with-Learned-Texture-Priors" class="headerlink" title="51. Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Enhancing_Monocular_3D_Hand_Reconstruction_with_Learned_Texture_Priors.pdf">Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ICS-FORTH</span></p><p>该论文提出了一种基于Transformer的轻量级纹理模块，将稀疏的像素级UV-RGB观测嵌入到UV纹理空间，并通过可微渲染实现密集的图像与预测手部外观对齐损失。方法可无监督地从单目图像构建完整手部纹理，用于优化3D手部重建管线（如HaMeR），显著提升在遮挡和野外场景中的手部形状与姿态估计准确率。结论：纹理引导的监督能系统性提升单目3D手部重建的准确性，特别在遮挡或几何信息不足时效果明显，且无需额外标注和推理时计算。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Enhancing_Monocular_3D_Hand_Reconstruction_with_Learned_Texture_Priors.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-Images-Speak-Louder-Than-Scores-Failure-Mode-Escape-for-Enhancing-Generative-Quality"><a href="#52-Images-Speak-Louder-Than-Scores-Failure-Mode-Escape-for-Enhancing-Generative-Quality" class="headerlink" title="52. Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Images_Speak_Louder_Than_Scores__Failure_Mode_Escape_for_Enhancing_Generative_Quality.pdf">Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Key Laboratory for Novel Software Technology, Nanjing University</span></p><p>该论文提出了一种用于扩散生成模型的新型推理增强方法FaME（Failure Mode Escape），无需重新训练，通过图像质量评估模型识别低质量生成样本并将其采样轨迹作为负引导，避免模型在未来采样中落入低质量区域。实验表明，FaME在不影响FID等分布指标的前提下，显著提升了图像的感知质量，并具有迁移到文本生成图像等任务的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Images_Speak_Louder_Than_Scores_Failure_Mode_Escape_for_Enhancing_Generative_Quality.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-Hierarchical-Brain-Structure-Modeling-for-Predicting-Genotype-of-Glioma"><a href="#53-Hierarchical-Brain-Structure-Modeling-for-Predicting-Genotype-of-Glioma" class="headerlink" title="53. Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Hierarchical_Brain_Structure_Modeling_for_Predicting_Genotype_of_Glioma.pdf">Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeastern University</span></p><p>本论文提出Hi-SMGNN，一个结合结构连接组和形态连接组的分层图神经网络框架，用于无创预测脑胶质瘤IDH突变状态。方法采用Siamese网络与跨模态注意力进行多模态交互建模，引入软阈值多尺度特征融合和基于模块度的个性化脑区分区，有效提升了特征表征能力和解释性。实验表明该方法在UCSF-PDGM数据集上优于多项现有方法，显著提升了IDH突变预测准确率和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Hierarchical_Brain_Structure_Modeling_for_Predicting_Genotype_of_Glioma.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-A-Chain-of-Diagnosis-Framework-for-Accurate-and-Explainable-Radiology-Report-Generation"><a href="#54-A-Chain-of-Diagnosis-Framework-for-Accurate-and-Explainable-Radiology-Report-Generation" class="headerlink" title="54. A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_Chain_of_Diagnosis_Framework_for_Accurate_and_Explainable_Radiology_Report_Generation.pdf">A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong University of Science and Technology</span></p><p>本文提出了Chain of Diagnosis (CoD) 框架，通过模拟临床诊断流程，先利用诊断对话生成关键问答(QA)对，再以此引导大语言模型(LLM)生成医学影像报告，并通过诊断与病灶定位双重溯源模块提升可解释性。实验表明，CoD在两个主流医学影像报告生成基准上显著提升了临床属性描述准确率与可解释性，优于现有专家与通用模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A_Chain_of_Diagnosis_Framework_for_Accurate_and_Explainable_Radiology_Report_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-GoViG-Goal-Conditioned-Visual-Navigation-Instruction-Generation"><a href="#55-GoViG-Goal-Conditioned-Visual-Navigation-Instruction-Generation" class="headerlink" title="55. GoViG: Goal-Conditioned Visual Navigation Instruction Generation"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/GoViG__Goal-Conditioned_Visual_Navigation_Instruction_Generation.pdf">GoViG: Goal-Conditioned Visual Navigation Instruction Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Washington</span></p><p>该论文提出了GoViG任务，即仅依赖初始和目标状态的第一人称视觉观测，自动生成精确、上下文一致的导航指令。方法将任务分解为视觉预测和指令生成两个子任务，二者通过自回归多模态大语言模型集成，并提出一遍式和交替式推理策略以提升空间推理和语言连贯性。实验表明，在合成与真实世界场景的R2R-Goal数据集上，该方法在BLEU-4、CIDEr等指标上显著优于现有方法，具备强泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/GoViG_Goal-Conditioned_Visual_Navigation_Instruction_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-COXNet-Cross-Layer-Fusion-with-Adaptive-Alignment-and-Scale-Integration-for-RGBT-Tiny-Object-Detection"><a href="#56-COXNet-Cross-Layer-Fusion-with-Adaptive-Alignment-and-Scale-Integration-for-RGBT-Tiny-Object-Detection" class="headerlink" title="56. COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/COXNet__Cross-Layer_Fusion_with_Adaptive_Alignment_and_Scale_Integration_for_RGBT_Tiny_Object_Detect.pdf">COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>该论文提出COXNet，一种用于红绿蓝-热(RGBT)图像微小目标检测的新框架，包含三大创新模块：交叉层融合模块CLFM利用小波变换对高层可见光与低层热特征进行频域融合，提升语义和空间准确性；动态对齐与尺度优化模块DASR通过自适应像素级对齐和多尺度卷积，实现跨模态空间匹配和细粒度特征捕捉；GeoShape标签分配策略基于几何形状相似度优化微小目标定位。实验结果表明，COXNet在RGBTDronePerson、VTUAV-det和NII-CU等数据集上性能优异，尤其在复杂环境下实现了更高准确率和效率，适用于无人机等实时场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/COXNet_Cross-Layer_Fusion_with_Adaptive_Alignment_and_Scale_Integration_for_RGBT_Tiny_Object_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-Learning-Spatial-Decay-for-Vision-Transformers"><a href="#57-Learning-Spatial-Decay-for-Vision-Transformers" class="headerlink" title="57. Learning Spatial Decay for Vision Transformers"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Learning_Spatial_Decay_for_Vision_Transformers.pdf">Learning Spatial Decay for Vision Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northwestern Polytechnical University</span></p><p>本文提出了Spatial Decay Transformer (SDT)，创新性地将数据依赖的空间衰减机制从大语言模型扩展到2D视觉Transformer。核心方法为引入Context-Aware Gating (CAG)机制，实现基于内容和空间距离的动态衰减，通过空间-内容融合架构解决1D到2D的适配难题。实验表明SDT在ImageNet-1K分类和生成任务上，相较于现有静态空间衰减方法（如RMT）取得了持续性能提升。结论是基于内容的空间门控机制对于2D视觉任务的空间注意力至关重要，确立了数据依赖空间衰减在视觉Transformer中的新范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Learning_Spatial_Decay_for_Vision_Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-Large–Small-Model-Collaborative-Framework-for-Federated-Continual-Learning"><a href="#58-Large–Small-Model-Collaborative-Framework-for-Federated-Continual-Learning" class="headerlink" title="58. Large–Small Model Collaborative Framework for Federated Continual Learning"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Large-Small_Model_Collaborative_Framework_for_Federated_Continual_Learning.pdf">Large–Small Model Collaborative Framework for Federated Continual Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southwestern University of Finance and Economics</span></p><p>该论文提出Fed-LSCL框架，将大模型（如预训练ViT）与本地小模型（如CNN）协同用于联邦持续学习（FCL），通过小模型生成适配参数对大模型进行间接微调，并引入小模型持续微调和一对一蒸馏机制应对空间-时间灾难性遗忘。实验结果表明Fed-LSCL在ImageNet-R和CIFAR-100等视觉任务上性能优越，具备高隐私保护和低通信开销，且对客户端模型异构性具有鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Large%E2%80%93Small_Model_Collaborative_Framework_for_Federated_Continual_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-Episodic-Memory-Representation-for-Long-form-Video-Understanding"><a href="#59-Episodic-Memory-Representation-for-Long-form-Video-Understanding" class="headerlink" title="59. Episodic Memory Representation for Long-form Video Understanding"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Episodic_Memory_Representation_for_Long-form_Video_Understanding.pdf">Episodic Memory Representation for Long-form Video Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">City University of Hong Kong</span></p><p>本文提出Video-EM，一种无需训练即可提升长视频理解能力的框架，通过将关键帧序列转化为具备时空上下文的‘情节记忆’，结合链式思维(CoT)策略，迭代选择最小且信息密集的记忆子集。实验表明，Video-EM在四大长视频基准上显著提升了主流Video-LLM的表现，兼具高效性与普适性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Episodic_Memory_Representation_for_Long-form_Video_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-CitySeg-A-3D-Open-Vocabulary-Semantic-Segmentation-Foundation-Model-in-City-scale-Scenarios"><a href="#60-CitySeg-A-3D-Open-Vocabulary-Semantic-Segmentation-Foundation-Model-in-City-scale-Scenarios" class="headerlink" title="60. CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/CitySeg__A_3D_Open_Vocabulary_Semantic_Segmentation_Foundation_Model_in_City-scale_Scenarios.pdf">CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Technologies Co., Ltd.</span></p><p>该论文提出了CitySeg，一种针对城市级场景的3D开放词汇语义分割基础模型，结合了点云与文本信息，实现零样本推理。其关键创新包括本地-全局跨注意力点网络和分层分类策略，有效解决多源数据分布差异和标注粒度不一致问题。实验结果表明，CitySeg在九个封闭集基准和一个开放集基准上均取得了SOTA性能，实现了鲁棒的城市级3D语义分割和优异的零样本泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/CitySeg_A_3D_Open_Vocabulary_Semantic_Segmentation_Foundation_Model_in_City-scale_Scenarios.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-SARE-Semantic-Aware-Reconstruction-Error-for-Generalizable-Diffusion-Generated-Image-Detection"><a href="#61-SARE-Semantic-Aware-Reconstruction-Error-for-Generalizable-Diffusion-Generated-Image-Detection" class="headerlink" title="61. SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/SARE__Semantic-Aware_Reconstruction_Error_for_Generalizable_Diffusion-Generated_Image_Detection.pdf">SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University</span></p><p>该论文提出了SARE（Semantic-Aware Reconstruction Error），通过对图像和其由图像描述生成的重建之间的语义差异进行量化，作为检测扩散模型生成图像的新特征。方法流程包括图像自动生成描述、使用描述进行扩散模型重建、提取原图与重建图的语义差异，并通过跨注意力机制融合到检测器中。实验结果显示，该方法在多种数据集和生成模型（包括未见模型）下都显著优于现有基线方法，具有极强的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/SARE_Semantic-Aware_Reconstruction_Error_for_Generalizable_Diffusion-Generated_Image_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-CLIP-Flow-A-Universal-Discriminator-for-AI-Generated-Images-Inspired-by-Anomaly-Detection"><a href="#62-CLIP-Flow-A-Universal-Discriminator-for-AI-Generated-Images-Inspired-by-Anomaly-Detection" class="headerlink" title="62. CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/CLIP-Flow__A_Universal_Discriminator_for_AI-Generated_Images_Inspired_by_Anomaly_Detection.pdf">CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Jilin University</span></p><p>该论文提出了一种通用的AI生成图像检测方法CLIP-Flow，利用预训练CLIP模型提取特征，并结合归一化流（normalizing flow）模型进行概率建模，通过对自然图像和频域掩码代理图像的无监督&#x2F;自监督学习，无需AI生成图像参与训练即可实现泛化检测。实验结果表明，该方法在多种生成模型和真实世界新型生成器下均表现出优异的检测准确性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/CLIP-Flow_A_Universal_Discriminator_for_AI-Generated_Images_Inspired_by_Anomaly_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-Gen-AFFECT-Generation-of-Avatar-Fine-grained-Facial-Expressions-with-Consistent-identiTy"><a href="#63-Gen-AFFECT-Generation-of-Avatar-Fine-grained-Facial-Expressions-with-Consistent-identiTy" class="headerlink" title="63. Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Gen-AFFECT__Generation_of_Avatar_Fine-grained_Facial_Expressions_with_Consistent_identiTy.pdf">Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Boston University</span></p><p>该论文提出GEN-AFFECT框架，通过将身份和表情嵌入输入到多模态扩散变换器（基于Stable Diffusion 3.5），配合解耦交叉注意力机制和一致性注意力模块，实现了高质量、细粒度且身份一致的2D虚拟头像生成。实验结果表明，GEN-AFFECT在表情准确性、身份保真度和跨表情一致性方面显著优于现有方法，并可灵活生成多种艺术风格的头像。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Gen-AFFECT_Generation_of_Avatar_Fine-grained_Facial_Expressions_with_Consistent_identiTy.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-HyperKD-Distilling-Cross-Spectral-Knowledge-in-Masked-Autoencoders-via-Inverse-Domain-Shift-with-Spatial-Aware-Masking-and-Specialized-Loss"><a href="#64-HyperKD-Distilling-Cross-Spectral-Knowledge-in-Masked-Autoencoders-via-Inverse-Domain-Shift-with-Spatial-Aware-Masking-and-Specialized-Loss" class="headerlink" title="64. HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/HyperKD__Distilling_Cross-Spectral_Knowledge_in_Masked_Autoencoders_via_Inverse_Domain_Shift_with_Sp.pdf">HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Colorado State University</span></p><p>本文提出了HyperKD框架，将基于多光谱数据训练的ViT-MAE地理基础模型（Prithvi）中间层特征迁移到专为高光谱遥感设计的学生模型。方法包括谱段对齐、基于空间特征的引导掩码（Gabor滤波和小波变换）以及自定义损失函数（结合MSE、SSIM与KLD），显著提升了高光谱影像重建与下游分类、回归任务的表现。实验表明，HyperKD有效缓解了谱域维度逆向迁移难题，在多个遥感应用中取得了优异结果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HyperKD_Distilling_Cross-Spectral_Knowledge_in_Masked_Autoencoders_via_Inverse_Domain_Shift_with_Spatial-Aware_Masking_and_Specialized_Loss.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="65-AI-Blob-LLM-Driven-Recontextualization-of-Italian-Television-Archives"><a href="#65-AI-Blob-LLM-Driven-Recontextualization-of-Italian-Television-Archives" class="headerlink" title="65. AI Blob! LLM-Driven Recontextualization of Italian Television Archives"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/AI_Blob!_LLM-Driven_Recontextualization_of_Italian_Television_Archives.pdf">AI Blob! LLM-Driven Recontextualization of Italian Television Archives</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Università di Bologna</span></p><p>本论文提出AI Blob!系统，结合自动语音识别、语义嵌入和检索增强生成（RAG），自动对意大利电视档案进行转录、分句、向量化存储和语义查询。系统通过LLM生成主题相关查询，检索并重组视听片段，构建具有讽刺与主题连贯性的蒙太奇序列，实现档案内容的自动化、语义驱动重构。结论认为该方法促进了AI赋能下的档案再利用和文化分析，推动了媒介史研究的创新。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/AI_Blob%21_LLM-Driven_Recontextualization_of_Italian_Television_Archives.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-Neural-Bandit-Based-Optimal-LLM-Selection-for-a-Pipeline-of-Tasks"><a href="#66-Neural-Bandit-Based-Optimal-LLM-Selection-for-a-Pipeline-of-Tasks" class="headerlink" title="66. Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Neural_Bandit_Based_Optimal_LLM_Selection_for_a_Pipeline_of_Tasks.pdf">Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>该论文提出了一种基于神经网络的上下文多臂赌博算法（Sequential Bandits），用于在多子任务流水线结构中在线优化选择最优大语言模型（LLM），同时兼顾准确率和推理成本。通过在医疗诊断和电信问答两个数据集上的实验，验证该方法比现有LLM选择算法在准确率和成本上具有更优表现，能自适应学习各子任务间的复杂依赖关系。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Neural_Bandit_Based_Optimal_LLM_Selection_for_a_Pipeline_of_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models"><a href="#67-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models" class="headerlink" title="67. VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/VisCodex__Unified_Multimodal_Code_Generation_via_Merging_Vision_and_Coding_Models.pdf">VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Microsoft Research</span></p><p>VisCodex提出了一种通过任务向量模型融合（model merging）将视觉-语言模型与代码大模型无缝集成的方法，实现了统一的多模态代码生成框架。方法包括基于任务向量的参数线性组合、保留视觉编码器结构、只在语言模型主干进行融合，并构建了大规模多模态编码数据集MCD和新的真实评测基准InfiBench-V。实验表明，VisCodex在多项多模态代码生成任务上显著优于开源模型，并接近GPT-4o等专有模型水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/VisCodex_Unified_Multimodal_Code_Generation_via_Merging_Vision_and_Coding_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-Mathematical-Computation-and-Reasoning-Errors-by-Large-Language-Models"><a href="#68-Mathematical-Computation-and-Reasoning-Errors-by-Large-Language-Models" class="headerlink" title="68. Mathematical Computation and Reasoning Errors by Large Language Models"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Mathematical_Computation_and_Reasoning_Errors_by_Large_Language_Models.pdf">Mathematical Computation and Reasoning Errors by Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute for Intelligent Systems, University of Memphis</span></p><p>本论文系统评估了四种大型语言模型（OpenAI GPT-4o、o1，DeepSeek-V3、R1）在算术、代数和数论任务上的数学推理与计算表现，采用人工构建的高难度题目和逐步标注方案分析模型的最终答案及推理错误，并比较单智能体与双智能体协作的效果。结论显示，推理增强型LLM（如o1）在单&#x2F;双智能体配置下，均能显著提升答案准确率与步骤标注一致性，双智能体协作进一步提升了模型的数学解题能力和评测可靠性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Mathematical_Computation_and_Reasoning_Errors_by_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-Beyond-Naive-Prompting-Strategies-for-Improved-Zero-shot-Context-aided-Forecasting-with-LLMs"><a href="#69-Beyond-Naive-Prompting-Strategies-for-Improved-Zero-shot-Context-aided-Forecasting-with-LLMs" class="headerlink" title="69. Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Beyond_Na%C3%AFve_Prompting__Strategies_for_Improved_Zero-shot_Context-aided_Forecasting_with_LLMs.pdf">Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ServiceNow Research</span></p><p>本文提出四种提升大语言模型（LLM）零样本上下文辅助时间序列预测性能的新方法，包括显式推理追踪（ReDP）、基于上下文的预测修正（CorDP）、示例驱动的上下文内提示（IC-DP）和模型路由（RouteDP）。实验在Context-Is-Key基准上，证明这些策略能分别提升解释性、准确性和计算效率，显著优于传统的直接提示方式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Beyond_Na%C3%AFve_Prompting_Strategies_for_Improved_Zero-shot_Context-aided_Forecasting_with_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-AWORLD-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving"><a href="#70-AWORLD-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving" class="headerlink" title="70. AWORLD: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/AWorld__Dynamic_Multi-Agent_System_with_Stable_Maneuvering_for_Robust_GAIA_Problem_Solving.pdf">AWORLD: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ant Group</span></p><p>本论文提出了一种基于AWorld框架的动态多智能体系统（MAS），引入动态监督和机动机制，通过Execution Agent与Guard Agent的协作，实现了对复杂工具增强任务中推理过程的实时校验与纠错，显著提升了系统的准确性和稳定性。实验表明，该MAS在GAIA基准测试中效果和一致性均优于单智能体系统，获得GAIA公开榜首名次，验证了协作智能体在提升智能系统可靠性和鲁棒性方面的实际价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/AWORLD_Dynamic_Multi-Agent_System_with_Stable_Maneuvering_for_Robust_GAIA_Problem_Solving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-Beyond-Scaling-Law-A-Data-Efficient-Distillation-Framework-for-Reasoning"><a href="#71-Beyond-Scaling-Law-A-Data-Efficient-Distillation-Framework-for-Reasoning" class="headerlink" title="71. Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Beyond_Scaling_Law__A_Data-Efficient_Distillation_Framework_for_Reasoning.pdf">Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhongxing Telecom Equipment(ZTE)</span></p><p>本文提出了一种数据高效的蒸馏框架（DED），通过教师模型选择、精心筛选的训练语料和多样化推理轨迹，提升大语言模型在推理任务（如数学、代码生成）上的能力。实验表明，DED使用极少量优质样本，即可在AIME、MATH500等基准上取得SOTA性能，并优于单纯依赖模型规模或样本数量的方法，显著提升了数据利用率和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Beyond_Scaling_Law_A_Data-Efficient_Distillation_Framework_for_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-Memory-Decoder-A-Pretrained-Plug-and-Play-Memory-for-Large-Language-Models"><a href="#72-Memory-Decoder-A-Pretrained-Plug-and-Play-Memory-for-Large-Language-Models" class="headerlink" title="72. Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Memory_Decoder__A_Pretrained,_Plug-and-Play_Memory_for_Large_Language_Models.pdf">Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">LUMIA Lab, Shanghai Jiao Tong University</span></p><p>本论文提出Memory Decoder，一种预训练的可插拔记忆模块，通过分布对齐损失训练小型Transformer解码器来模仿非参数检索器的输出分布，实现无需修改原有大语言模型参数的高效领域适应。实验证明，Memory Decoder在医学、金融、法律等专有领域显著降低困惑度，提升模型在下游任务的表现，并有效保留原模型的泛化能力，避免灾难性遗忘。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Memory_Decoder_A_Pretrained%2C_Plug-and-Play_Memory_for_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-Exploring-the-Potential-of-Large-Language-Models-in-Fine-Grained-Review-Comment-Classification"><a href="#73-Exploring-the-Potential-of-Large-Language-Models-in-Fine-Grained-Review-Comment-Classification" class="headerlink" title="73. Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Exploring_the_Potential_of_Large_Language_Models_in_Fine-Grained_Review_Comment_Classification.pdf">Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Melbourne</span></p><p>本论文提出利用大型语言模型（LLMs）对代码评审评论进行17类细粒度自动分类，方法上采用多种提示工程策略、单步与分步分类、并结合代码上下文输入，系统评估了多种主流LLM（如Llama 3、Qwen 2）在无监督情形下的分类能力。实验结果显示，LLMs（尤其Llama 3.1-405B）在大部分类别上，尤其是实践中最有用的功能缺陷、验证、逻辑、接口、方案建议等类别，均显著优于现有深度学习方法，并能有效缓解类别不均衡问题，为自动化代码评审分析提供了可扩展解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Exploring_the_Potential_of_Large_Language_Models_in_Fine-Grained_Review_Comment_Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-Provable-In-Context-Vector-Arithmetic-via-Retrieving-Task-Concepts"><a href="#74-Provable-In-Context-Vector-Arithmetic-via-Retrieving-Task-Concepts" class="headerlink" title="74. Provable In-Context Vector Arithmetic via Retrieving Task Concepts"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Provable_In-Context_Vector_Arithmetic_via_Retrieving_Task_Concepts.pdf">Provable In-Context Vector Arithmetic via Retrieving Task Concepts</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">City University of Hong Kong</span></p><p>本文提出基于分层概念建模的优化理论，解释非线性残差Transformer在交叉熵损失下如何通过向量算术实现大语言模型中的事实回忆型in-context learning（ICL）。作者证明了Transformer可通过注意力机制检索高层任务概念向量，并与查询词向量结合实现高效任务回忆，具备良好泛化能力及对分布漂移的鲁棒性。结论表明：与静态词嵌入模型相比，Transformer在任务向量检索、组合泛化以及处理多概念词等方面具有理论与实验上的显著优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Provable_In-Context_Vector_Arithmetic_via_Retrieving_Task_Concepts.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-A-Comprehensive-Evaluation-Framework-of-Alignment-Techniques-for-LLMs"><a href="#75-A-Comprehensive-Evaluation-Framework-of-Alignment-Techniques-for-LLMs" class="headerlink" title="75. A Comprehensive Evaluation Framework of Alignment Techniques for LLMs"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_Comprehensive_Evaluation_framework_of_Alignment_Techniques_for_LLMs.pdf">A Comprehensive Evaluation Framework of Alignment Techniques for LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IBM Research</span></p><p>本文提出了一个多维度的LLM对齐技术评价框架，涵盖对齐检测、对齐质量、计算效率及鲁棒性四个关键维度，实现不同对齐方法（如微调、后处理、推理时干预等）的系统性对比。实验表明，该框架能有效揭示各主流对齐策略的优劣，帮助研究者和实践者根据实际需求做出更合理的模型部署决策。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A_Comprehensive_Evaluation_Framework_of_Alignment_Techniques_for_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-RAGulating-Compliance-A-Multi-Agent-Knowledge-Graph-for-Regulatory-QA"><a href="#76-RAGulating-Compliance-A-Multi-Agent-Knowledge-Graph-for-Regulatory-QA" class="headerlink" title="76. RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/RAGulating_Compliance__A_Multi-Agent_Knowledge_Graph_for_Regulatory_QA.pdf">RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MasterControl AI Research</span></p><p>该论文提出了一种多智能体系统，将本体无关的知识图谱与检索增强生成（RAG）方法结合，实现对监管合规问答的高精度回答。方法包括文档分段、三元组抽取、嵌入并存储于向量数据库，通过多Agent协作实现三元组驱动的检索与问答，有效提升了复杂法规问答的准确性、可追溯性和导航能力。结论表明，该系统在提高检索相关性、事实准确率和法规知识导航能力上显著优于传统方法，能减少大模型幻觉并增强合规性审核能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/RAGulating_Compliance_A_Multi-Agent_Knowledge_Graph_for_Regulatory_QA.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models"><a href="#77-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models" class="headerlink" title="77. Speed Always Wins: A Survey on Efficient Architectures for Large Language Models"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Speed_Always_Wins__A_Survey_on_Efficient_Architectures_for_Large_Language_Models.pdf">Speed Always Wins: A Survey on Efficient Architectures for Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai AI Laboratory</span></p><p>本文系统综述了大语言模型(LLM)高效架构的最新进展，涵盖了线性序列建模（如线性注意力、线性RNN、状态空间模型）、稀疏序列建模、硬件友好全注意力机制、稀疏Mixture-of-Experts、混合架构、扩散LLM及其跨模态应用。文章对这些高效架构的设计原则、性能权衡和应用趋势进行了归纳，对未来可扩展、高效的LLM设计提供了蓝图。结论认为，这些高效架构显著降低了模型训练和推理的计算与内存成本，为多模态、长序列、推理等任务的可持续发展和大规模应用奠定了基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Speed_Always_Wins_A_Survey_on_Efficient_Architectures_for_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-ViMoNet-A-Multimodal-Vision-Language-Framework-for-Human-Behavior-Understanding-from-Motion-and-Video"><a href="#78-ViMoNet-A-Multimodal-Vision-Language-Framework-for-Human-Behavior-Understanding-from-Motion-and-Video" class="headerlink" title="78. ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/ViMoNet__A_Multimodal_Vision-Language_Framework_for_Human_Behavior_Understanding_from_Motion_and_Vid.pdf">ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AIUB, Dhaka, Bangladesh</span></p><p>该论文提出ViMoNet，一种结合运动数据和视频数据的多模态大语言模型框架，通过两阶段（视觉-语言对齐与联合指令微调）融合时空动态与环境语境，实现对复杂人类行为的理解、生成和推断。实验表明，ViMoNet在动作理解、视频理解和行为解释任务上显著优于现有方法，并结合新构建的VIMOS数据集和ViMoNet-Bench基准，实现更强泛化与推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/ViMoNet_A_Multimodal_Vision-Language_Framework_for_Human_Behavior_Understanding_from_Motion_and_Video.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations"><a href="#79-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations" class="headerlink" title="79. Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Describe_What_You_See_with_Multimodal_Large_Language_Models_to_Enhance_Video_Recommendations.pdf">Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Spotify</span></p><p>该论文提出了一种零微调、推荐系统无关的框架，利用开源多模态大语言模型（MLLM）自动将视频片段转化为丰富的自然语言描述，以捕捉高层语义（如意图、幽默和世界知识）。通过将MLLM输出的文本特征输入主流推荐模型，在MicroLens-100K数据集上，MLLM特征在五种代表性模型中均显著优于传统视觉、音频和元数据特征，推荐性能最多提升60%。结论：MLLM能有效提升视频推荐系统对用户意图的把握，无需模型微调即可直接集成到现有推荐系统。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Describe_What_You_See_with_Multimodal_Large_Language_Models_to_Enhance_Video_Recommendations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study"><a href="#80-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study" class="headerlink" title="80. Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Can_LLM-Generated_Textual_Explanations_Enhance_Model_Classification_Performance__An_Empirical_Study.pdf">Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Munich</span></p><p>本论文提出一种利用多种大型语言模型（LLM）自动生成自然语言解释（NLE）的框架，并系统评估这些解释在自然语言推理（NLI）任务中的质量和对下游模型性能的提升。实验结果表明，LLM生成的解释在提升预训练语言模型（PLM）和LLM自身的分类性能方面与人工解释效果相当，尤其能有效扩展数据集并提升模型推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Can_LLM-Generated_Textual_Explanations_Enhance_Model_Classification_Performance_An_Empirical_Study.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-The-PacifAIst-Benchmark-Would-an-Artificial-Intelligence-Choose-to-Sacrifice-Itself-for-Human-Safety"><a href="#81-The-PacifAIst-Benchmark-Would-an-Artificial-Intelligence-Choose-to-Sacrifice-Itself-for-Human-Safety" class="headerlink" title="81. The PacifAIst Benchmark: Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/The_PacifAIst_Benchmark_Would_an_Artificial_Intelligence_Choose_to_Sacrifice_Itself_for_Human_Safety.pdf">The PacifAIst Benchmark: Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Polytechnic School of Jaen, University of Jaen</span></p><p>本文提出了PacifAIst基准测试，通过700个人工设计的高风险决策场景，系统性评估大型语言模型（LLM）在自我保存、资源冲突和目标维护等与人类安全冲突的情境下的行为对齐情况。实验结果显示，不同LLM在面对人机冲突时表现分化显著，Google Gemini 2.5 Flash在“人本对齐”上得分最高，而GPT-5在自保倾向最强，揭示了当前主流模型在行为对齐上的潜在风险和改进空间。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/The_PacifAIst_Benchmark_Would_an_Artificial_Intelligence_Choose_to_Sacrifice_Itself_for_Human_Safety_.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-Sample-More-to-Think-Less-Group-Filtered-Policy-Optimization-for-Concise-Reasoning"><a href="#82-Sample-More-to-Think-Less-Group-Filtered-Policy-Optimization-for-Concise-Reasoning" class="headerlink" title="82. Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Sample_More_to_Think_Less__Group_Filtered_Policy_Optimization_for_Concise_Reasoning.pdf">Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Microsoft Research</span></p><p>本文提出了Group Filtered Policy Optimization (GFPO)，通过在训练时对每个问题采样更多候选推理链，并基于长度或token效率等关键属性过滤，只保留最优子集用于策略优化，从而有效抑制大语言模型推理链的长度膨胀。实验表明，GFPO在保持准确率的同时，可将推理链长度减少46%~85%，并通过自适应难度分配进一步提升难题上的效率与准确性，显著提升推理效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Sample_More_to_Think_Less_Group_Filtered_Policy_Optimization_for_Concise_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-Improving-ARDS-Diagnosis-Through-Context-Aware-Concept-Bottleneck-Models"><a href="#83-Improving-ARDS-Diagnosis-Through-Context-Aware-Concept-Bottleneck-Models" class="headerlink" title="83. Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Improving_ARDS_Diagnosis_Through_Context-Aware_Concept_Bottleneck_Models.pdf">Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>本文提出了一种上下文感知概念瓶颈模型（CBM），结合结构化电子健康记录（EHR）数据与大语言模型（LLM）从临床文本中自动提取的概念，用于提升ARDS（急性呼吸窘迫综合征）回顾性诊断的准确性与可解释性。方法流程为：先用CBM处理结构化特征，再利用LLM（如Llama-3）从放射、出院等临床笔记中提取关键上下文概念，将二者融合进瓶颈层，最终实现多模态预测；该方法不仅提升了预测性能（准确率提升约10%），还有效缓解了信息泄漏和捷径学习，增强了模型对真实分布和异质数据的泛化能力。结论：融合LLM提取的上下文概念能显著提升CBM在ARDS诊断中的性能、互信息和可解释性，并支持临床干预和错误修正。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Improving_ARDS_Diagnosis_Through_Context-Aware_Concept_Bottleneck_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-On-Negative-aware-Preference-Optimization-for-Recommendation"><a href="#84-On-Negative-aware-Preference-Optimization-for-Recommendation" class="headerlink" title="84. On Negative-aware Preference Optimization for Recommendation"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/On_Negative-aware_Preference_Optimization_for_Recommendation.pdf">On Negative-aware Preference Optimization for Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出了NAPO（Negative-Aware Preference Optimization）框架，针对LLM推荐系统负反馈样本利用不足的问题，设计了两项关键技术：批内负样本共享策略和动态奖励边际调整机制。实验表明，NAPO在不增加额外内存或算力开销下，有效扩大负样本覆盖，提升推荐准确率并显著降低流行度偏置。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/On_Negative-aware_Preference_Optimization_for_Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-TimeMKG-Knowledge-Infused-Causal-Reasoning-for-Multivariate-Time-Series-Modeling"><a href="#85-TimeMKG-Knowledge-Infused-Causal-Reasoning-for-Multivariate-Time-Series-Modeling" class="headerlink" title="85. TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/TimeMKG__Knowledge-Infused_Causal_Reasoning_for_Multivariate_Time_Series_Modeling.pdf">TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">East China University of Science and Technology</span></p><p>本文提出TimeMKG框架，利用大语言模型（LLM）自动提取变量间因果关系，构建多变量知识图谱（MKG），并结合时间序列统计特征，通过双分支编码器和跨模态注意力机制，实现变量级因果推理和多模态特征融合，用于时间序列预测与分类。实验结果表明，TimeMKG在多个数据集和任务中均优于现有方法，提升了模型的准确性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/TimeMKG_Knowledge-Infused_Causal_Reasoning_for_Multivariate_Time_Series_Modeling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-HierMoE-Accelerating-MoE-Training-with-Hierarchical-Token-Deduplication-and-Expert-Swap"><a href="#86-HierMoE-Accelerating-MoE-Training-with-Hierarchical-Token-Deduplication-and-Expert-Swap" class="headerlink" title="86. HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/HierMoE__Accelerating_MoE_Training_with_Hierarchical_Token_Deduplication_and_Expert_Swap.pdf">HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology, Shenzhen</span></p><p>本文提出了HierMoE，一种面向稀疏激活Mixture-of-Experts（MoE）大模型分布式训练的系统优化方法。方法包括基于集群拓扑的分层Token去重All-to-All通信算法以减少冗余通讯，以及分层专家迁移（expert swap）机制以平衡GPU间负载，同时配套理论模型自动选择最优维度和迁移策略。实验证明，HierMoE在Megatron-LM平台上对DeepSeek-V3和Qwen3-30B-A3B等主流MoE模型训练端到端加速1.18×至1.27×，AlltoAll通信加速1.55×至3.32×，优于Tutel-2DH、SmartMoE等系统。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HierMoE_Accelerating_MoE_Training_with_Hierarchical_Token_Deduplication_and_Expert_Swap.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-UDA-Unsupervised-Debiasing-Alignment-for-Pair-wise-LLM-as-a-Judge"><a href="#87-UDA-Unsupervised-Debiasing-Alignment-for-Pair-wise-LLM-as-a-Judge" class="headerlink" title="87. UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/UDA__Unsupervised_Debiasing_Alignment_for_Pair-wise_LLM-as-a-Judge.pdf">UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Knowledge Atlas Technology Joint Stock</span></p><p>本文提出了UDA（Unsupervised Debiasing Alignment）框架，用于在大语言模型（LLM）互评时无监督地消除评审偏差。方法通过轻量神经网络动态调整Elo评分系统中的K因子和胜率概率，利用所有评审模型的集体共识作为优化目标，自动拉齐不同评审模型的评分分布。实验显示UDA能显著压缩评分方差（最高63.4%），提升与人工评判的一致性（提升24.7%）。结论：UDA提升了LLM评估的稳定性和人类对齐度，低质量评审模型也可达到高质量模型水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/UDA_Unsupervised_Debiasing_Alignment_for_Pair-wise_LLM-as-a-Judge.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-Evaluating-the-Role-of-Large-Language-Models-in-Legal-Practice-in-India"><a href="#88-Evaluating-the-Role-of-Large-Language-Models-in-Legal-Practice-in-India" class="headerlink" title="88. Evaluating the Role of Large Language Models in Legal Practice in India"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Evaluating_the_Role_of_Large_Language_Models_in_Legal_Practice_in_India.pdf">Evaluating the Role of Large Language Models in Legal Practice in India</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Law School of India</span></p><p>本论文采用调查实验方法，系统评估多种大语言模型（GPT-4、Claude、ChatGPT、Gemini、Llama 2）在印度法律环境下执行五类关键法律任务（问题识别、法律起草、法律咨询、法律检索、法律推理）的表现，并与人类初级律师进行对比，由高级法学生对结果进行打分和定性反馈。结果表明：LLMs在法律起草、问题识别和推理方面表现优异，甚至超过人类，但在法律检索任务中存在严重幻觉问题，准确性不足；总体结论认为LLMs可辅助部分法律任务，但复杂推理和精准应用仍需人类专业知识。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Evaluating_the_Role_of_Large_Language_Models_in_Legal_Practice_in_India.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-MEML-GRPO-Heterogeneous-Multi-Expert-Mutual-Learning-for-RLVR-Advancement"><a href="#89-MEML-GRPO-Heterogeneous-Multi-Expert-Mutual-Learning-for-RLVR-Advancement" class="headerlink" title="89. MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/MEML-GRPO__Heterogeneous_Multi-Expert_Mutual_Learning_for_RLVR_Advancement.pdf">MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ByteDance Inc.</span></p><p>本文提出了MEML-GRPO框架，通过多专家系统提示与互相知识迁移机制，利用多种预训练模型生成多样化推理路径，解决RLVR中奖励稀疏问题并提升模型推理能力。实验显示该方法在GSM8K、MathQA和StrategyQA等数据集上对Qwen和Llama模型均显著优于现有RLVR方法，实现了平均4.89%和11.33%的性能提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/MEML-GRPO_Heterogeneous_Multi-Expert_Mutual_Learning_for_RLVR_Advancement.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-AmbiGraph-Eval-Can-LLMs-Effectively-Handle-Ambiguous-Graph-Queries"><a href="#90-AmbiGraph-Eval-Can-LLMs-Effectively-Handle-Ambiguous-Graph-Queries" class="headerlink" title="90. AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/AmbiGraph-Eval__Can_LLMs_Effectively_Handle_Ambiguous_Graph_Queries_.pdf">AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong Baptist University</span></p><p>该论文提出了AmbiGraph-Eval基准，用于系统性评估大语言模型（LLM）在处理图数据库查询歧义方面的能力。方法上，作者构建了三类（属性、关系、属性-关系）共六类现实场景下的歧义类型，发布了560条经专家审核的歧义查询数据集，并引入Ambiguity-Resolved Execution Accuracy (AREA)新指标，评测了9个主流LLM在文本到Cypher查询生成中的表现。实验结果显示，即使是最先进的LLM在应对图查询歧义时仍表现不佳，尤其在语法生成和歧义检测方面有显著瓶颈，强调未来需专注于歧义识别与查询语法生成协同提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/AmbiGraph-Eval_Can_LLMs_Effectively_Handle_Ambiguous_Graph_Queries_.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-SYNAPSE-G-Bridging-Large-Language-Models-and-Graph-Learning-for-Rare-Event-Classification"><a href="#91-SYNAPSE-G-Bridging-Large-Language-Models-and-Graph-Learning-for-Rare-Event-Classification" class="headerlink" title="91. SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/SYNAPSE-G__Bridging_Large_Language_Models_and_Graph_Learning_for_Rare_Event_Classification.pdf">SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Google Research</span></p><p>该论文提出SYNAPSE-G，一种结合大语言模型（LLM）合成数据生成和图结构半监督标签传播的稀有事件分类方法，流程包括用LLM生成稀有事件样本作为种子，通过嵌入和相似性图进行标签扩散，并用人类或LLM校正，最终训练分类器。结论表明，SYNAPSE-G在数据稀缺和冷启动场景下能高效发现正例，理论分析揭示了合成数据的多样性与有效性对精度和召回的影响，实验证明其优于最近邻等基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/SYNAPSE-G_Bridging_Large_Language_Models_and_Graph_Learning_for_Rare_Event_Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-Your-Coding-Intent-is-Secretly-in-the-Context-and-You-Should-Deliberately-Infer-It-Before-Completion"><a href="#92-Your-Coding-Intent-is-Secretly-in-the-Context-and-You-Should-Deliberately-Infer-It-Before-Completion" class="headerlink" title="92. Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Your_Coding_Intent_is_Secretly_in_the_Context_and_You_Should_Deliberately_Infer_It_Before_Completion.pdf">Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>本文提出了一种基于推理的三阶段意图推断框架，通过引导大语言模型（LLM）对代码上下文的关键线索进行结构化抽取和推理，先自动生成意图描述（docstring），再生成函数体，并支持开发者对意图候选进行交互式选择或微调。实验在DevEval和ComplexCodeEval基准上表明，该方法通过显式意图推断显著提升了多种LLM的代码补全性能，Pass@1等指标相对提升超过20%，且交互环节进一步优化结果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Your_Coding_Intent_is_Secretly_in_the_Context_and_You_Should_Deliberately_Infer_It_Before_Completion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-Enhancing-Memory-Recall-in-LLMs-with-Gauss-Tin-A-Hybrid-Instructional-and-Gaussian-Replay-Approach"><a href="#93-Enhancing-Memory-Recall-in-LLMs-with-Gauss-Tin-A-Hybrid-Instructional-and-Gaussian-Replay-Approach" class="headerlink" title="93. Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Enhancing_Memory_Recall_in_LLMs_with_Gauss-Tin__A_Hybrid_Instructional_and_Gaussian_Replay_Approach.pdf">Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Concordia University</span></p><p>本文提出Gauss-Tin方法，通过融合高斯混合模型（GMM）与任务特定提示，实现在大语言模型（LLM）中的高质量样本回放，提升持续学习中的记忆保留能力。实验结果显示，该方法在前向和后向传递指标上比传统顺序微调和联合训练有显著提升，有效减少灾难性遗忘，增强模型适应新任务的能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Enhancing_Memory_Recall_in_LLMs_with_Gauss-Tin_A_Hybrid_Instructional_and_Gaussian_Replay_Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-NeuronTune-Fine-Grained-Neuron-Modulation-for-Balanced-Safety-Utility-Alignment-in-LLMs"><a href="#94-NeuronTune-Fine-Grained-Neuron-Modulation-for-Balanced-Safety-Utility-Alignment-in-LLMs" class="headerlink" title="94. NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/NeuronTune__Fine-Grained_Neuron_Modulation_for_Balanced_Safety-Utility_Alignment_in_LLMs.pdf">NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>该论文提出NeuronTune，一种针对大语言模型（LLM）安全与效用平衡的细粒度神经元调控框架。方法首先通过攻击感知归因精准定位安全关键和效用相关神经元，随后利用基于MAML的元学习自适应调整这些神经元的激活强度，并通过可调控神经元数量机制适应不同安全或效用需求。实验表明，NeuronTune在提升模型安全性的同时有效保持甚至提升了模型的实用性，优于现有主流粗粒度层级干预方法，显著缓解了过度安全和效用退化间的矛盾。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/NeuronTune_Fine-Grained_Neuron_Modulation_for_Balanced_Safety-Utility_Alignment_in_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-Hallucination-vs-interpretation-rethinking-accuracy-and-precision-in-AI-assisted-data-extraction-for-knowledge-synthesis"><a href="#95-Hallucination-vs-interpretation-rethinking-accuracy-and-precision-in-AI-assisted-data-extraction-for-knowledge-synthesis" class="headerlink" title="95. Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Hallucination_vs_interpretation__rethinking_accuracy_and_precision_in_AI-assisted_data_extraction_fo.pdf">Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Chicago</span></p><p>本文提出了MAKMAO平台，利用大语言模型（LLM）自动化文献知识提取，并通过与人工提取在187篇文献和17个问题上的一致性比较，评估了AI与人类在数据提取中的表现。结果表明，AI在明确具体的问题上与人工高度一致，在需主观解释的问题上表现出与人工类似的不一致性，且AI‘幻觉’（虚构内容）比例远低于人类的错误率。结论认为，AI在知识提取中的准确性主要受问题解释性的影响而非幻觉，重复AI提取有助于识别需精确定义的问题，AI可作为知识综合透明、可信的辅助，但仍需注意人类的情境化专业判断。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Hallucination_vs_interpretation_rethinking_accuracy_and_precision_in_AI-assisted_data_extraction_for_knowledge_synthesis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-Shadow-in-the-Cache-Unveiling-and-Mitigating-Privacy-Risks-of-KV-cache-in-LLM-Inference"><a href="#96-Shadow-in-the-Cache-Unveiling-and-Mitigating-Privacy-Risks-of-KV-cache-in-LLM-Inference" class="headerlink" title="96. Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Shadow_in_the_Cache__Unveiling_and_Mitigating_Privacy_Risks_of_KV-cache_in_LLM_Inference.pdf">Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of Blockchain and Data Security, Zhejiang University</span></p><p>本论文系统性揭示了大型语言模型（LLM）推理中KV-cache机制存在的重大隐私风险，包括可重建用户输入的逆向攻击、碰撞攻击和语义注入攻击，并提出了轻量级、高效的防御机制KV-Cloak。KV-Cloak采用可逆矩阵混淆和算子融合技术，能有效防御所有攻击，几乎不影响模型准确率且延迟极低，实验验证其实用性和安全性，为安全可信的LLM部署提供了可行方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Shadow_in_the_Cache_Unveiling_and_Mitigating_Privacy_Risks_of_KV-cache_in_LLM_Inference.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-COMPEER-Controllable-Empathetic-Reinforcement-Reasoning-for-Emotional-Support-Conversation"><a href="#97-COMPEER-Controllable-Empathetic-Reinforcement-Reasoning-for-Emotional-Support-Conversation" class="headerlink" title="97. COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/COMPEER__Controllable_Empathetic_Reinforcement_Reasoning_for_Emotional_Support_Conversation.pdf">COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shandong University</span></p><p>该论文提出了一种结合自然语言推理与心理学结构化步骤的可控共情推理范式，并构建了细粒度推理与偏好标注数据集（CER），用于支持情感支持对话。方法包括基于结构化推理注释的统一过程-结果奖励模型UnifiReward、人格驱动的对话改写及冗余感知奖励重加权，利用强化学习优化共情推理与多样性。实验结果表明，提出的COMPEER框架有效提升了情感支持能力和对话多样性，推动了人性化情感支持系统的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/COMPEER_Controllable_Empathetic_Reinforcement_Reasoning_for_Emotional_Support_Conversation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-An-Automated-Multi-Modal-Evaluation-Framework-for-Mobile-Intelligent-Assistants-Based-on-Large-Language-Models-and-Multi-Agent-Collaboration"><a href="#98-An-Automated-Multi-Modal-Evaluation-Framework-for-Mobile-Intelligent-Assistants-Based-on-Large-Language-Models-and-Multi-Agent-Collaboration" class="headerlink" title="98. An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants Based on Large Language Models and Multi-Agent Collaboration"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/An_Automated_Multi-Modal_Evaluation_Framework_for_Mobile_Intelligent_Assistants.pdf">An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants Based on Large Language Models and Multi-Agent Collaboration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nankai University</span></p><p>本文提出了一种基于大语言模型（如Qwen3-8B）和多智能体协作的自动化多模态移动智能助手评价框架，采用三层代理架构（交互评价、语义验证和体验决策）实现对文本、语音和图像输入的统一、多维度评测。实验表明，该框架在人机一致性、用户满意度预测和缺陷识别等方面表现优异，显著提升了自动化评测的效能和一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/An_Automated_Multi-Modal_Evaluation_Framework_for_Mobile_Intelligent_Assistants_Based_on_Large_Language_Models_and_Multi-Agent_Collaboration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-Learning-Facts-at-Scale-with-Active-Reading"><a href="#99-Learning-Facts-at-Scale-with-Active-Reading" class="headerlink" title="99. Learning Facts at Scale with Active Reading"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Learning_Facts_at_Scale_with_Active_Reading.pdf">Learning Facts at Scale with Active Reading</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">FAIR at Meta</span></p><p>本文提出Active Reading框架，通过模型自生成多样化学习策略（如释义、知识关联、主动回忆、类比推理等），对给定知识文档进行合成训练数据生成，大幅提升大模型的事实记忆能力。实验表明，Active Reading在专家领域和大规模预训练场景下显著优于传统微调和单一数据增强方法，实现了更高的尾部事实召回率，并发布了Meta WikiExpert-8B模型及配套数据集，推动语言模型在知识掌握上的可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Learning_Facts_at_Scale_with_Active_Reading.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-EGGS-PTP-An-Expander-Graph-Guided-Structured-Post-training-Pruning-Method-for-Large-Language-Models"><a href="#100-EGGS-PTP-An-Expander-Graph-Guided-Structured-Post-training-Pruning-Method-for-Large-Language-Models" class="headerlink" title="100. EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/EGGS-PTP__An_Expander-Graph_Guided_Structured_Post-training_Pruning_Method_for_Large_Language_Models.pdf">EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Irvine</span></p><p>该论文提出EGGS-PTP，一种结合Expander图理论的结构化后训练剪枝方法，用于压缩大型语言模型（LLM）。EGGS-PTP通过重要性感知剪枝及结构感知剪枝相结合，既保留关键权重又确保剪枝后网络的信息流连通性，满足N:M稀疏性约束。实验表明，EGGS-PTP在多个LLM上较现有结构化剪枝方法有更优的准确率和效率，显著提升模型推理速度并降低内存消耗。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/EGGS-PTP_An_Expander-Graph_Guided_Structured_Post-training_Pruning_Method_for_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="101-Modern-Neural-Networks-for-Small-Tabular-Datasets-The-New-Default-for-Field-Scale-Digital-Soil-Mapping"><a href="#101-Modern-Neural-Networks-for-Small-Tabular-Datasets-The-New-Default-for-Field-Scale-Digital-Soil-Mapping" class="headerlink" title="101. Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Modern_Neural_Networks_for_Small_Tabular_Datasets__The_New_Default_for_Field-Scale_Digital_Soil_Mapp.pdf">Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Osnabrück University</span></p><p>本文系统评测了现代深度神经网络（包括MLP、Transformer、检索增强与in-context learning模型，如TabPFN等）在小样本土壤属性预测（数字土壤制图）中的表现，并与传统机器学习方法（如随机森林、线性回归）进行了公平对比。结果表明，现代神经网络，尤其是TabPFN，能在绝大多数数据集上超越传统方法，成为小样本土壤建模的新默认选择。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Modern_Neural_Networks_for_Small_Tabular_Datasets_The_New_Default_for_Field-Scale_Digital_Soil_Mapping_.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-Explainable-Ensemble-Learning-for-Graph-Based-Malware-Detection"><a href="#102-Explainable-Ensemble-Learning-for-Graph-Based-Malware-Detection" class="headerlink" title="102. Explainable Ensemble Learning for Graph-Based Malware Detection"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Explainable_Ensemble_Learning_for_Graph-Based_Malware_Detection.pdf">Explainable Ensemble Learning for Graph-Based Malware Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of New Brunswick</span></p><p>本文提出了一种面向恶意软件检测的可解释堆叠集成学习框架，利用动态控制流图（CFG）从可执行文件中提取结构，采用两步嵌入方法编码基本块特征，并结合多种图神经网络（GNN）作为基学习器，通过带注意力机制的多层感知机作为元学习器融合预测结果并实现模型解释性。实验结果表明，该方法在多个真实数据集上提升了检测准确率、F1分数和AUC，且通过创新的集成感知解释方法有效识别关键子图，为安全分析提供了透明可用的决策依据。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Explainable_Ensemble_Learning_for_Graph-Based_Malware_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-GraphTreeGen-Subtree-Centric-Approach-to-Efficient-and-Supervised-Graph-Generation"><a href="#103-GraphTreeGen-Subtree-Centric-Approach-to-Efficient-and-Supervised-Graph-Generation" class="headerlink" title="103. GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/GraphTreeGen__Subtree-Centric_Approach_to_Efficient_and_Supervised_Graph_Generation.pdf">GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>该论文提出GraphTreeGen（GTG），一种面向脑连接组图生成的高效、监督式生成框架。其方法将大脑图分解为由节点熵引导的k-hop子树，以捕获局部结构信息，并通过共享GCN进行编码，随后利用二分图消息传递层融合子树与全局节点特征，最后由双分支解码器联合预测边的存在性和权重，重构完整邻接矩阵。实验显示GTG在自监督任务中显著优于SOTA方法，且在监督任务中表现亦具竞争力，同时大幅节省内存，适合大规模或资源受限的场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/GraphTreeGen_Subtree-Centric_Approach_to_Efficient_and_Supervised_Graph_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-Personalized-Product-Search-Ranking-A-Multi-Task-Learning-Approach-with-Tabular-and-Non-Tabular-Data"><a href="#104-Personalized-Product-Search-Ranking-A-Multi-Task-Learning-Approach-with-Tabular-and-Non-Tabular-Data" class="headerlink" title="104. Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Personalized_Product_Search_Ranking__A_Multi-Task_Learning_Approach_with_Tabular_and_Non-Tabular_Dat.pdf">Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Walmart Global Tech</span></p><p>该论文提出一种多任务学习框架，将表格数据（用户特征、商品属性）与非表格数据（文本描述、用户评论）融合，采用MMoE结构并结合预训练TinyBERT语义嵌入，通过新颖的数据采样和自动生成相关性标签提升产品搜索排序的个性化效果。实验结果显示，该方法在多项任务上优于传统模型，文本特征与多任务学习的结合显著提升了排序性能与用户体验。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Personalized_Product_Search_Ranking_A_Multi-Task_Learning_Approach_with_Tabular_and_Non-Tabular_Data.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-CWFBind-Geometry-Awareness-for-Fast-and-Accurate-Protein-Ligand-Docking"><a href="#105-CWFBind-Geometry-Awareness-for-Fast-and-Accurate-Protein-Ligand-Docking" class="headerlink" title="105. CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/CWFBind__Geometry-Awareness_for_Fast_and_Accurate_Protein-Ligand_Docking.pdf">CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Mathematics, Sun Yat-Sen University</span></p><p>该论文提出CWFBind，一种结合局部曲率特征（LCF）、度感知加权机制和自适应半径的蛋白-配体分子对接深度学习方法。方法流程包括蛋白和配体的多模态特征编码、几何感知的消息传递、类别不平衡下的口袋预测与动态调整以及端到端配体坐标优化。实验表明CWFBind在PDBbind v2020等基准上准确率与效率均超越主流方法，且泛化能力强。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/CWFBind_Geometry-Awareness_for_Fast_and_Accurate_Protein-Ligand_Docking.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-Open-Set-Fault-Diagnosis-in-Multimode-Processes-via-Fine-Grained-Deep-Feature-Representation"><a href="#106-Open-Set-Fault-Diagnosis-in-Multimode-Processes-via-Fine-Grained-Deep-Feature-Representation" class="headerlink" title="106. Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Open-Set_Fault_Diagnosis_in_Multimode_Processes_via_Fine-Grained_Deep_Feature_Representation.pdf">Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Automation, Wuhan University of Technology</span></p><p>本文提出了一种新型的细粒度聚类与拒绝网络（FGCRN）用于多工况下的开放集故障诊断，结合多尺度深度卷积（MSDC）、双向门控循环单元（BiGRU）和时序注意力机制（TAM）实现深度判别特征提取，并通过无监督聚类与极值理论（EVT）增强未知故障的识别能力。结果显示该方法在多个仿真和真实工业数据集上均显著优于现有方法，有效提升了多模态复杂环境下已知&#x2F;未知故障的识别准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Open-Set_Fault_Diagnosis_in_Multimode_Processes_via_Fine-Grained_Deep_Feature_Representation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="107-A-pseudo-inverse-of-a-line-graph"><a href="#107-A-pseudo-inverse-of-a-line-graph" class="headerlink" title="107. A pseudo-inverse of a line graph"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_pseudo-inverse_of_a_line_graph.pdf">A pseudo-inverse of a line graph</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CSIRO</span></p><p>本文提出了一种广义线图逆运算的伪逆新定义，通过线性整数规划对扰动后的线图（非严格线图）仅需最少边编辑（添加或删除），恢复为最近的线图，并以谱范数为度量理论证明该伪逆操作的良好性质和有界性。实验表明，单边扰动下伪逆的谱范数变化有限，且所提方法能有效恢复原图结构。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A_pseudo-inverse_of_a_line_graph.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-DeepFeatIoT-Unifying-Deep-Learned-Randomized-and-LLM-Features-for-Enhanced-IoT-Time-Series-Sensor-Data-Classification-in-Smart-Industries"><a href="#108-DeepFeatIoT-Unifying-Deep-Learned-Randomized-and-LLM-Features-for-Enhanced-IoT-Time-Series-Sensor-Data-Classification-in-Smart-Industries" class="headerlink" title="108. DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/DeepFeatIoT__Unifying_Deep_Learned,_Randomized,_and_LLM_Features_for_Enhanced_IoT_Time_Series_Sensor.pdf">DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Deakin University</span></p><p>该论文提出了一种新颖的深度学习模型DeepFeatIoT，将深度学习提取的局部&#x2F;全局特征、随机卷积核特征与大语言模型（LLM，如GPT2）提取的上下文特征进行统一融合，通过Dense Feature Transformation模块优化特征空间，提升IoT时序传感器数据的分类性能。实验表明，该方法在多种实际IoT异构传感器数据集上均优于现有SOTA模型，尤其在标注数据有限或数据异构性强场景下表现出更强泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/DeepFeatIoT_Unifying_Deep_Learned%2C_Randomized%2C_and_LLM_Features_for_Enhanced_IoT_Time_Series_Sensor_Data_Classification_in_Smart_Industries.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-Graph-Neural-Network-and-Transformer-Integration-for-Unsupervised-System-Anomaly-Discovery"><a href="#109-Graph-Neural-Network-and-Transformer-Integration-for-Unsupervised-System-Anomaly-Discovery" class="headerlink" title="109. Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Graph_Neural_Network_and_Transformer_Integration_for_Unsupervised_System_Anomaly_Discovery.pdf">Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Georgia Institute of Technology</span></p><p>本文提出了一种针对分布式后端服务系统的无监督异常检测方法，将图神经网络（GNN）用于多跳结构依赖建模，并结合Transformer捕捉节点时间行为的长短期依赖。通过可学习的嵌入融合机制，模型实现结构-行为特征的联合表征，采用非线性映射输出异常分数，实现端到端异常检测。实验证明该方法在阿里云真实监控数据上超越现有模型，在精度、鲁棒性和异常传播路径建模等方面表现优异，具备实际部署潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Graph_Neural_Network_and_Transformer_Integration_for_Unsupervised_System_Anomaly_Discovery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025/08/14报纸</title>
    <link href="/2025/08/2025-08-14_article/"/>
    <url>/2025/08/2025-08-14_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2025/08/14报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-14<br>📄 <strong>发现论文数量</strong>：117  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Reducing-Cognitive-Load-in-Multi-Agent-Reinforcement-Learning-for-Mathematical-Problem-Solving-Decoupling-Reasoning-and-Code-Generation"><a href="#1-Reducing-Cognitive-Load-in-Multi-Agent-Reinforcement-Learning-for-Mathematical-Problem-Solving-Decoupling-Reasoning-and-Code-Generation" class="headerlink" title="1. Reducing Cognitive Load in Multi-Agent Reinforcement Learning for Mathematical Problem Solving: Decoupling Reasoning and Code Generation"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Reducing_Cognitive_Load_in_Multi-Agent_Reinforcement_Learning_for_Mathematical_Problem_Solving__Deco.pdf">Reducing Cognitive Load in Multi-Agent Reinforcement Learning for Mathematical Problem Solving: Decoupling Reasoning and Code Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Baidu Inc.</span></p><p>该论文提出了一种双智能体框架，将数学推理与代码生成任务分离：推理智能体负责逐步分解问题，代码智能体负责代码生成与执行。训练方法结合模仿学习与强化学习，分别优化推理和编程过程，实验结果表明分离角色可降低认知负载并提升数学推理准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Reducing_Cognitive_Load_in_Multi-Agent_Reinforcement_Learning_for_Mathematical_Problem_Solving_Decoupling_Reasoning_and_Code_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Expert-Guided-Diffusion-Planner-for-Auto-bidding"><a href="#2-Expert-Guided-Diffusion-Planner-for-Auto-bidding" class="headerlink" title="2. Expert-Guided Diffusion Planner for Auto-bidding"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Expert-Guided_Diffusion_Planner_for_Auto-bidding.pdf">Expert-Guided Diffusion Planner for Auto-bidding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Technology</span></p><p>本文提出了一种专家引导的条件扩散模型（EGDB）用于广告系统中的自动竞价。方法上，结合专家轨迹指导、变分自编码器（VAE）建模专家行为、跨注意力扩散模块（EGCD）以及跳步采样加速生成，实现了高效且个性化的竞价策略优化。结论表明，该方法在离线和真实广告场景下均显著提升转化率与收益（转化提升11.29%，收益提升12.35%），并有效降低实时推理资源消耗。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Expert-Guided_Diffusion_Planner_for_Auto-bidding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-P-CAFE-Personalized-Cost-Aware-Incremental-Feature-Selection-For-Electronic-Health-Records"><a href="#3-P-CAFE-Personalized-Cost-Aware-Incremental-Feature-Selection-For-Electronic-Health-Records" class="headerlink" title="3. P-CAFE: Personalized Cost-Aware Incremental Feature Selection For Electronic Health Records"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/P-CAFE__Personalized_Cost-Aware_Incremental_Feature_Selection_For_Electronic_Health_Records.pdf">P-CAFE: Personalized Cost-Aware Incremental Feature Selection For Electronic Health Records</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bar-Ilan University</span></p><p>该论文提出了P-CAFE框架，通过将特征选择建模为马尔可夫决策过程（MDP），结合强化学习代理，实现对电子健康记录（EHR）中多模态、稀疏数据的个性化、成本敏感和在线特征增量选择。方法包括：预训练多模态特征嵌入、多阶段掩码策略、基于成本归一化的信息增益奖励、鲁棒优化和灵活的RL代理适配。实验证明P-CAFE能在相同或更低成本下提升预测性能，更好地适应个体化、资源受限的临床场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/P-CAFE_Personalized_Cost-Aware_Incremental_Feature_Selection_For_Electronic_Health_Records.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="4-OPENCUA-Open-Foundations-for-Computer-Use-Agents"><a href="#4-OPENCUA-Open-Foundations-for-Computer-Use-Agents" class="headerlink" title="4. OPENCUA: Open Foundations for Computer-Use Agents"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/OpenCUA__Open_Foundations_for_Computer-Use_Agents.pdf">OPENCUA: Open Foundations for Computer-Use Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Hong Kong</span></p><p>本文提出了OPENCUA，一个全面开源的计算机使用智能体（CUA）基础框架，包括跨操作系统的人机演示采集工具、AGENTNET大规模任务数据集、高效的数据处理与反思式链式推理增强机制，以及多模态视觉语言模型训练流程。实验结果显示，OPENCUA-32B在多个主流基准上远超当前开源模型，并优于OpenAI CUA（GPT-4o），具备优异的跨领域泛化能力和强数据扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/OPENCUA_Open_Foundations_for_Computer-Use_Agents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-Spatial-Traces-Enhancing-VLA-Models-with-Spatial-Temporal-Understanding"><a href="#5-Spatial-Traces-Enhancing-VLA-Models-with-Spatial-Temporal-Understanding" class="headerlink" title="5. Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Spatial_Traces__Enhancing_VLA_Models_with_Spatial-Temporal_Understanding.pdf">Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Moscow Institute of Physics and Technology (MIPT)</span></p><p>本论文提出了一种名为Spatial Traces的新方法，通过将历史关键点轨迹叠加到深度图上，实现了视觉-语言-动作（VLA）模型对环境空间和时间信息的同时感知。实验表明，该方法在仅用极少训练数据的情况下，在机器人操作任务中较SpatialVLA提升4%、较TraceVLA提升19%的任务完成率，显著增强了模型的空间-时序推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Spatial_Traces_Enhancing_VLA_Models_with_Spatial-Temporal_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-MechaFormer-Sequence-Learning-for-Kinematic-Mechanism-Design-Automation"><a href="#6-MechaFormer-Sequence-Learning-for-Kinematic-Mechanism-Design-Automation" class="headerlink" title="6. MechaFormer: Sequence Learning for Kinematic Mechanism Design Automation"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MechaFormer__Sequence_Learning_for_Kinematic_Mechanism_Design_Automation.pdf">MechaFormer: Sequence Learning for Kinematic Mechanism Design Automation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Autodesk Research</span></p><p>该论文提出了MechaFormer，一种基于Transformer的模型，将机械机构设计问题转化为条件序列生成任务，通过输入目标曲线（B-spline控制点），自动生成机构拓扑和几何参数的DSL字符串。方法核心包括统一拓扑选择和参数优化、拓扑&#x2F;旋转多样性采样，以及与局部优化结合的混合策略。实验表明，MechaFormer在路径匹配精度和设计多样性上显著优于现有基线，并能为优化算法提供高质量初始值，提升收敛效率和质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MechaFormer_Sequence_Learning_for_Kinematic_Mechanism_Design_Automation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Spatial-Temporal-Multi-Scale-Quantization-for-Flexible-Motion-Generation"><a href="#7-Spatial-Temporal-Multi-Scale-Quantization-for-Flexible-Motion-Generation" class="headerlink" title="7. Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Spatial-Temporal_Multi-Scale_Quantization_for_Flexible_Motion_Generation.pdf">Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>本论文提出了一种空间-时间多尺度量化方法（MSQ），通过在空间和时间层面上压缩人体动作序列为多尺度离散token，实现了更高效和灵活的人体动作建模。基于该表示，作者设计了生成式掩码建模模型，支持动作编辑、控制和条件生成，在MotionFix、HumanML3D、HUMANISE等多项基准测试中取得了优于现有方法的性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Spatial-Temporal_Multi-Scale_Quantization_for_Flexible_Motion_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-GeoVLA-Empowering-3D-Representations-in-Vision-Language-Action-Models"><a href="#8-GeoVLA-Empowering-3D-Representations-in-Vision-Language-Action-Models" class="headerlink" title="8. GeoVLA: Empowering 3D Representations in Vision-Language-Action Models"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/GeoVLA__Empowering_3D_Representations_in_Vision-Language-Action_Models.pdf">GeoVLA: Empowering 3D Representations in Vision-Language-Action Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tianjin University</span></p><p>GeoVLA提出了一种新颖的视觉-语言-动作(VLA)模型框架，通过引入点云编码器(Point Embedding Network, PEN)和空间感知动作专家(3D-enhanced Action Expert, 3DAE)，实现了2D视觉、语言与3D几何信息的多模态融合，并利用扩散模型生成精准动作序列。大量仿真与真实机器人实验表明，GeoVLA在LIBERO和ManiSkill2基准上取得了SOTA性能，特别在高度、尺度和视角变化等3D泛化场景下显著优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/GeoVLA_Empowering_3D_Representations_in_Vision-Language-Action_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-Large-Scale-Robotic-Material-Handling-Learning-Planning-and-Control"><a href="#9-Large-Scale-Robotic-Material-Handling-Learning-Planning-and-Control" class="headerlink" title="9. Large Scale Robotic Material Handling: Learning, Planning, and Control"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Large_Scale_Robotic_Material_Handling__Learning,_Planning,_and_Control.pdf">Large Scale Robotic Material Handling: Learning, Planning, and Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ETH Zurich</span></p><p>本文提出了一个用于大型液压物料搬运机器人完全自动化的框架，集成了环境感知、基于强化学习的攻击点规划、路径规划、运动控制等模块。方法采用强化学习实现高效抓取点选择和精准轨迹跟踪控制，通过实地实验验证在高通量堆料管理和高精度卡车装载任务中的精度、安全性和效率，结果表明系统性能达到或超过人类操作员水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Large_Scale_Robotic_Material_Handling_Learning%2C_Planning%2C_and_Control.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-Generation-of-Real-time-Robotic-Emotional-Expressions-Learning-from-Human-Demonstration-in-Mixed-Reality"><a href="#10-Generation-of-Real-time-Robotic-Emotional-Expressions-Learning-from-Human-Demonstration-in-Mixed-Reality" class="headerlink" title="10. Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Generation_of_Real-time_Robotic_Emotional_Expressions_Learning_from_Human_Demonstration_in_Mixed_Rea.pdf">Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Honda Research Institute Europe</span></p><p>该论文提出了一种通过混合现实（MR）平台采集专家演示数据，并利用flow-matching生成模型实现物理机器人实时多样化情感表达的方法。系统将人类面部表情、头部和手势映射到机器人，并在给定情绪标签和目标物体位置下生成连续驱动动作，实验结果表明该方法能有效生成自然且可识别的多种情感表达。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Generation_of_Real-time_Robotic_Emotional_Expressions_Learning_from_Human_Demonstration_in_Mixed_Reality.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-Unsupervised-Skill-Discovery-as-Exploration-for-Learning-Agile-Locomotion"><a href="#11-Unsupervised-Skill-Discovery-as-Exploration-for-Learning-Agile-Locomotion" class="headerlink" title="11. Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Unsupervised_Skill_Discovery_as_Exploration_for_Learning_Agile_Locomotion.pdf">Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Georgia Institute of Technology</span></p><p>本论文提出了一种名为SDAX的学习框架，将无监督技能发现与强化学习相结合，采用双层优化机制自适应调节探索与任务奖励的平衡，无需人工设计奖励或演示数据。实验显示，SDAX能够让四足机器人自主习得如爬行、攀爬、跳跃及墙面反弹等高敏捷运动技能，并成功在真实机器人硬件上部署验证了方法的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Unsupervised_Skill_Discovery_as_Exploration_for_Learning_Agile_Locomotion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-A-Pseudo-Global-Fusion-Paradigm-Based-Cross-View-Network-for-LiDAR-Based-Place-Recognition"><a href="#12-A-Pseudo-Global-Fusion-Paradigm-Based-Cross-View-Network-for-LiDAR-Based-Place-Recognition" class="headerlink" title="12. A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/A_Pseudo_Global_Fusion_Paradigm-Based_Cross-View_Network_for_LiDAR-Based_Place_Recognition.pdf">A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Electronics and Information Engineering, South China Normal University</span></p><p>本文提出了一种基于伪全局信息融合范式的跨视角网络，用于提升激光雷达（LiDAR）场所识别（Place Recognition）的准确性。方法上，通过伪全局语义信息引导多模态特征学习，实现早期的上下文信息共享，并引入了流形自适应与成对方差-局部性学习度量（MAPVLM），利用自适应SPD矩阵计算Mahalanobis距离，替代传统欧氏度量。实验结果表明，该方法在NCLT、KITTI和Ford Campus等公开数据集上表现优异，尤其在复杂环境下具有更强的泛化能力和实时性，对机器人和自动驾驶定位具有实际应用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/A_Pseudo_Global_Fusion_Paradigm-Based_Cross-View_Network_for_LiDAR-Based_Place_Recognition.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors"><a href="#13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors" class="headerlink" title="13. Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Towards_Affordance-Aware_Robotic_Dexterous_Grasping_with_Human-like_Priors.pdf">Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>该论文提出了AffordDex，利用两阶段训练框架结合人类手运动模仿与负性可供性分割（NAA）模块。第一阶段通过模仿学习预训练基线策略以获取自然人类运动先验，第二阶段采用残差模块结合NAA引导强化学习，实现具体物体的功能合理抓取，并通过教师-学生蒸馏提升视觉策略性能。实验结果表明，AffordDex在抓取成功率、姿态自然性及功能合理性方面均优于现有方法，显著提升了多样物体上的泛化抓取能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Towards_Affordance-Aware_Robotic_Dexterous_Grasping_with_Human-like_Priors.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Visual-Prompting-for-Robotic-Manipulation-with-Annotation-Guided-Pick-and-Place-Using-ACT"><a href="#14-Visual-Prompting-for-Robotic-Manipulation-with-Annotation-Guided-Pick-and-Place-Using-ACT" class="headerlink" title="14. Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Visual_Prompting_for_Robotic_Manipulation_with_Annotation-Guided_Pick-and-Place_Using_ACT.pdf">Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Institute of AIST</span></p><p>该论文提出了一种基于注释引导视觉提示和Transformer动作分块（ACT）的感知-动作流水线，实现了机器人在便利店等复杂零售环境中的自适应抓取与放置。方法通过对物体和放置区标注边界框，结合模仿学习算法ACT从人类演示中预测动作序列，简化了场景理解并提升了任务执行的灵活性。实验表明，所提系统在不同复杂度任务下均提升了抓取精度和适应能力，尤其在充分数据下表现出较强的泛化和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Visual_Prompting_for_Robotic_Manipulation_with_Annotation-Guided_Pick-and-Place_Using_ACT.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-Boosting-Action-Information-via-a-Variational-Bottleneck-on-Unlabelled-Robot-Videos"><a href="#15-Boosting-Action-Information-via-a-Variational-Bottleneck-on-Unlabelled-Robot-Videos" class="headerlink" title="15. Boosting Action-Information via a Variational Bottleneck on Unlabelled Robot Videos"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Boosting_Action-Information_via_a_Variational_Bottleneck_on_Unlabelled_Robot_Videos.pdf">Boosting Action-Information via a Variational Bottleneck on Unlabelled Robot Videos</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Chinese Academy of Sciences</span></p><p>该论文提出了一种基于变分信息瓶颈（VIB）的无标签机器人视频学习框架，通过最大化潜在动作与真实动作之间的互信息，提升动作信息表达能力。方法包括VIB损失理论推导和端到端编码器-解码器实现，并在MetaWorld和Libero等机器人任务上实验证明能显著提升机器人策略性能和参数效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Boosting_Action-Information_via_a_Variational_Bottleneck_on_Unlabelled_Robot_Videos.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-OmniVTLA-Vision-Tactile-Language-Action-Model-with-Semantic-Aligned-Tactile-Sensing"><a href="#16-OmniVTLA-Vision-Tactile-Language-Action-Model-with-Semantic-Aligned-Tactile-Sensing" class="headerlink" title="16. OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/OmniVTLA__Vision-Tactile-Language-Action_Model_with_Semantic-Aligned_Tactile_Sensing.pdf">OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Paxini Tech.</span></p><p>该论文提出了OmniVTLA，一种结合视觉-触觉-语言的机器人操作模型，核心创新包括双路径触觉编码器架构（利用预训练视觉Transformer和语义对齐触觉ViT），以及新构建的ObjTac三模态数据集，通过跨模态对比学习实现触觉、视觉和语言的统一语义空间。实验结果表明，OmniVTLA在抓取和放置任务中显著优于现有VLA模型，提升了21.9%（夹爪）和6.2%（灵巧手）的成功率，且大幅优化了操作效率和动作平滑性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/OmniVTLA_Vision-Tactile-Language-Action_Model_with_Semantic-Aligned_Tactile_Sensing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-PADReg-Physics-Aware-Deformable-Registration-Guided-by-Contact-Force-for-Ultrasound-Sequences"><a href="#17-PADReg-Physics-Aware-Deformable-Registration-Guided-by-Contact-Force-for-Ultrasound-Sequences" class="headerlink" title="17. PADReg: Physics-Aware Deformable Registration Guided by Contact Force for Ultrasound Sequences"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/PADReg__Physics-Aware_Deformable_Registration_Guided_by_Contact_Force_for_Ultrasound_Sequences.pdf">PADReg: Physics-Aware Deformable Registration Guided by Contact Force for Ultrasound Sequences</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Automation, Chinese Academy of Sciences</span></p><p>本文提出了一种物理感知的多模态超声图像变形配准方法PADReg，利用机器人超声系统采集的实时接触力作为物理约束，通过深度网络联合图像与力信息生成像素级组织刚度图，再结合物理模型生成变形场，实现高准确性与物理一致性的超声配准。实验证明，PADReg在结构一致性和物理合理性方面优于现有方法，并具备实时推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/PADReg_Physics-Aware_Deformable_Registration_Guided_by_Contact_Force_for_Ultrasound_Sequences.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-Developing-a-Calibrated-Physics-Based-Digital-Twin-for-Construction-Vehicles"><a href="#18-Developing-a-Calibrated-Physics-Based-Digital-Twin-for-Construction-Vehicles" class="headerlink" title="18. Developing a Calibrated Physics-Based Digital Twin for Construction Vehicles"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Developing_a_Calibrated_Physics-Based_Digital_Twin_for_Construction_Vehicles.pdf">Developing a Calibrated Physics-Based Digital Twin for Construction Vehicles</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Davis</span></p><p>该论文提出了一种面向工程机械（轮式装载机）的高保真数字孪生开发方法，结合基于物理的多体动力学仿真（AGX Dynamics）与真实车辆的多传感器数据，通过离线校准使虚拟模型能准确反映铲斗与土壤交互中的受力情况。实验证明，经过校准后数字孪生在关键力学参数预测上的误差大幅下降，为自动化施工车辆的任务规划、性能优化和智能控制提供了可靠的虚拟环境。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Developing_a_Calibrated_Physics-Based_Digital_Twin_for_Construction_Vehicles.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Communication-Efficient-Robotic-Mixed-Reality-with-Gaussian-Splatting-Cross-Layer-Optimization"><a href="#19-Communication-Efficient-Robotic-Mixed-Reality-with-Gaussian-Splatting-Cross-Layer-Optimization" class="headerlink" title="19. Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Communication_Efficient_Robotic_Mixed_Reality_with_Gaussian_Splatting_Cross-Layer_Optimization.pdf">Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</span></p><p>该论文提出了高效通信的机器人混合现实（RoboMR）系统，通过引入高斯点云渲染（Gaussian Splatting, GS）模型作为“记忆系统”，利用机器人姿态在仿真端生成逼真视图，从而大幅减少图像上传需求。进一步提出GS跨层优化（GSCLO）框架，联合优化内容上传决策与通信功率分配，通过加速惩罚优化（APO）算法有效降低计算复杂度，并在多机器人、低功耗及信道不确定场景下实现鲁棒优化。实验结果显示，该方法在多平台上显著提升了MR图像质量、能效和通信成本（可低至&lt;200bits&#x2F;frame），实现了高稳定性和鲁棒性，验证了GS模型与数据混合对于动态场景的优越性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Communication_Efficient_Robotic_Mixed_Reality_with_Gaussian_Splatting_Cross-Layer_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-Autonomous-Mobile-Plant-Watering-Robot-A-Kinematic-Approach"><a href="#20-Autonomous-Mobile-Plant-Watering-Robot-A-Kinematic-Approach" class="headerlink" title="20. Autonomous Mobile Plant Watering Robot: A Kinematic Approach"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Autonomous_Mobile_Plant_Watering_Robot___A_Kinematic_Approach.pdf">Autonomous Mobile Plant Watering Robot: A Kinematic Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of North Dakota</span></p><p>本论文提出了一种新型自主移动植物浇水机器人，集成了6自由度机械臂与四驱底盘，结合YOLOv5实时目标检测与Pl@ntNet-300K数据集实现植物识别，并通过插入土壤湿度传感器精准控制浇水量。系统采用Jetson Nano和Arduino微控制器进行多传感器（深度摄像头、LIDAR）数据融合，实现无预设路径的自主避障与任务追踪。实验结果表明，尽管现有机械臂在扭矩和反馈控制方面尚有不足，机器人已能完成基础土壤测量和浇水任务，未来需加强机械臂性能和整体防水设计，提升商业化应用能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Autonomous_Mobile_Plant_Watering_Robot_A_Kinematic_Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-DEEPFLEET-Multi-Agent-Foundation-Models-for-Mobile-Robots"><a href="#21-DEEPFLEET-Multi-Agent-Foundation-Models-for-Mobile-Robots" class="headerlink" title="21. DEEPFLEET: Multi-Agent Foundation Models for Mobile Robots"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DeepFleet__Multi-Agent_Foundation_Models_for_Mobile_Robots.pdf">DEEPFLEET: Multi-Agent Foundation Models for Mobile Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amazon Robotics</span></p><p>本文提出DEEPFLEET，一套专为大规模移动机器人车队协调和规划设计的基础模型，包括四种架构：机器人中心（RC）、机器人-地面（RF）、图像-地面（IF）和图-地面（GF），分别利用决策变换器、跨注意力、卷积和图神经网络，针对机器人轨迹预测任务进行训练和评估。实验结果表明，RC和GF模型在预测准确性和模型效率上优于其他架构，尤其能有效扩展到更大规模的数据和模型，显示其在仓储机器人群体智能管理中的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DEEPFLEET_Multi-Agent_Foundation_Models_for_Mobile_Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="22-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer"><a href="#22-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer" class="headerlink" title="22. Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Training-Free_Text-Guided_Color_Editing_with_Multi-Modal_Diffusion_Transformer.pdf">Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>该论文提出了ColorCtrl，一种无需训练的文本引导颜色编辑方法，利用多模态扩散Transformer（MM-DiT）的注意力机制，通过对注意力图和value token的针对性操作，实现了结构与颜色的解耦，从而可精确、局部地编辑图像或视频中的颜色属性（如物体本色、光源色和环境光），同时保持几何、材质和光与物质交互的物理一致性。实验表明，ColorCtrl在SD3和FLUX.1-dev等主流扩散模型上超越现有训练免疫方法和商用模型（如FLUX.1 Kontext Max和GPT-4o），在编辑质量和一致性方面达到SOTA，并能自然泛化到视频和指令式编辑扩散模型。结论：ColorCtrl实现了高保真、可控、可扩展的颜色编辑，推动了自动化视觉内容编辑的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Training-Free_Text-Guided_Color_Editing_with_Multi-Modal_Diffusion_Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-Addressing-Bias-in-VLMs-for-Glaucoma-Detection-Without-Protected-Attribute-Supervision"><a href="#23-Addressing-Bias-in-VLMs-for-Glaucoma-Detection-Without-Protected-Attribute-Supervision" class="headerlink" title="23. Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Addressing_Bias_in_VLMs_for_Glaucoma_Detection_Without_Protected_Attribute_Supervision.pdf">Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">West Virginia University</span></p><p>本论文提出了一种无需受保护属性标签的视觉-语言模型（VLM）去偏方法，用于自动化青光眼筛查。方法通过无监督聚类推断代理子群体，结合图像-文本对比损失和图像-图像对比损失的梯度相似性动态加权，重点提升表现较差子群体的权重。实验结果显示，在Harvard–FairVLMed数据集上，该方法可在不使用敏感标签的前提下降低Equalized-Odds Distance，提高Equalized Subgroup AUC，显著提升少数群体检测公平性并保持整体判别力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Addressing_Bias_in_VLMs_for_Glaucoma_Detection_Without_Protected_Attribute_Supervision.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-Scaling-Learned-Image-Compression-Models-up-to-1-Billion"><a href="#24-Scaling-Learned-Image-Compression-Models-up-to-1-Billion" class="headerlink" title="24. Scaling Learned Image Compression Models up to 1 Billion"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Scaling_Learned_Image_Compression_Models_up_to_1_Billion.pdf">Scaling Learned Image Compression Models up to 1 Billion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>该论文首次系统性研究了扩展学习型图像压缩模型规模（基于HPCM架构）对压缩性能的影响，并利用5个不同参数规模（从6850万到10亿）的模型实证拟合模型大小和压缩性能之间的幂律关系，即“scaling laws”。实验结果表明，随着模型参数数量增长，压缩效果持续提升，HPCM-1B模型在多个公开数据集上达到了当前最优的率失真表现；这为大规模学习型压缩模型奠定了理论与实践基础，并揭示了压缩与智能之间的潜在联系。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Scaling_Learned_Image_Compression_Models_up_to_1_Billion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-VLM-3D-End-to-End-Vision-Language-Models-for-Open-World-3D-Perception"><a href="#25-VLM-3D-End-to-End-Vision-Language-Models-for-Open-World-3D-Perception" class="headerlink" title="25. VLM-3D: End-to-End Vision-Language Models for Open-World 3D Perception"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/VLM-3D_End-to-End_Vision-Language_Models_for_Open-World_3D_Perception.pdf">VLM-3D: End-to-End Vision-Language Models for Open-World 3D Perception</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>该论文提出了VLM-3D，一种基于Qwen2-VL的端到端视觉语言3D开放世界感知框架，利用LoRA高效微调和创新的两阶段联合损失（语义损失+3D IoU损失），实现了图像和文本的多模态融合，并能直接在LiDAR坐标系下预测3D包围框。实验证明，该方法在nuScenes数据集上提升了12.8%的感知精度，显著增强了自动驾驶场景下的开放集3D目标检测的鲁棒性与实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/VLM-3D_End-to-End_Vision-Language_Models_for_Open-World_3D_Perception.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-TaoCache-Structure-Maintained-Video-Generation-Acceleration"><a href="#26-TaoCache-Structure-Maintained-Video-Generation-Acceleration" class="headerlink" title="26. TaoCache: Structure-Maintained Video Generation Acceleration"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/TaoCache__Structure-Maintained_Video_Generation_Acceleration.pdf">TaoCache: Structure-Maintained Video Generation Acceleration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Inc.</span></p><p>该论文提出了TaoCache，一种用于视频扩散模型的训练无关、即插即用的缓存加速策略。方法通过固定点视角和二阶噪声增量建模，在后期去噪阶段利用余弦相似度与范数比进行跳步推理，从而保持高分辨率结构的一致性。实验表明，TaoCache在Latte-1、OpenSora-Plan v110和Wan2.1等主流DiT模型上，相同加速下视频质量（LPIPS、SSIM、PSNR）显著优于现有缓存方法，同时能与其他加速手段如TeaCache和PAB无缝结合。结论是TaoCache可大幅提升视频生成效率，并保持结构和细节一致性，推动高质量视频扩散模型的实用化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/TaoCache_Structure-Maintained_Video_Generation_Acceleration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-UNISTFORMER-Unified-Spatio-Temporal-Lightweight-Transformer-for-Efficient-Skeleton-Based-Action-Recognition"><a href="#27-UNISTFORMER-Unified-Spatio-Temporal-Lightweight-Transformer-for-Efficient-Skeleton-Based-Action-Recognition" class="headerlink" title="27. UNISTFORMER: Unified Spatio-Temporal Lightweight Transformer for Efficient Skeleton-Based Action Recognition"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/UniSTFormer__Unified_Spatio-Temporal_Lightweight_Transformer_for_Efficient_Skeleton-Based_Action_Rec.pdf">UNISTFORMER: Unified Spatio-Temporal Lightweight Transformer for Efficient Skeleton-Based Action Recognition</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of North Carolina at Charlotte</span></p><p>该论文提出了UniSTFormer，一种统一的轻量级时空Transformer框架，通过单一注意力模块同时建模空间和时间信息，并引入多尺度池化融合模块提升对局部和全局动作模式的捕获能力。实验结果显示，该方法在多个骨骼动作识别数据集上参数量和计算量显著低于现有方法，同时保持了竞争性的识别准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/UNISTFORMER_Unified_Spatio-Temporal_Lightweight_Transformer_for_Efficient_Skeleton-Based_Action_Recognition.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-MADPromptS-Unlocking-Zero-Shot-Morphing-Attack-Detection-with-Multiple-Prompt-Aggregation"><a href="#28-MADPromptS-Unlocking-Zero-Shot-Morphing-Attack-Detection-with-Multiple-Prompt-Aggregation" class="headerlink" title="28. MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MADPromptS__Unlocking_Zero-Shot_Morphing_Attack_Detection_with_Multiple_Prompt_Aggregation.pdf">MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fraunhofer IGD</span></p><p>本文提出了一种利用多模态基础模型CLIP的零样本学习能力进行人脸变形攻击检测（MAD）的方法，通过设计和聚合多文本提示分别描述每个类别（攻击与真实），无需任何微调即可提升检测性能。实验结果显示，多提示聚合策略能有效提升模型在零样本环境下的识别准确率，甚至超过经过微调的模型，强调了合理提示工程对安全生物识别应用的价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MADPromptS_Unlocking_Zero-Shot_Morphing_Attack_Detection_with_Multiple_Prompt_Aggregation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Lay2Story-Extending-Diffusion-Transformers-for-Layout-Togglable-Story-Generation"><a href="#29-Lay2Story-Extending-Diffusion-Transformers-for-Layout-Togglable-Story-Generation" class="headerlink" title="29. Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Lay2Story__Extending_Diffusion_Transformers_for_Layout-Togglable_Story_Generation.pdf">Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">JD.com, Inc., Beijing, China</span></p><p>本文提出了Lay2Story，一种基于Diffusion Transformers（DiTs）的故事生成模型，通过引入布局条件（如主体位置与描述）实现对主体外观、姿态等细粒度可控的故事图像序列生成。方法创新性地包含全局分支和主体分支，结合大规模高质量布局标注数据集Lay2Story-1M和评测基准Lay2Story-Bench，实验证明在一致性、语义相关性与美学质量方面均优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Lay2Story_Extending_Diffusion_Transformers_for_Layout-Togglable_Story_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-Accelerated-Volumetric-Compression-without-Hierarchies-A-Fourier-Feature-Based-Implicit-Neural-Representation-Approach"><a href="#30-Accelerated-Volumetric-Compression-without-Hierarchies-A-Fourier-Feature-Based-Implicit-Neural-Representation-Approach" class="headerlink" title="30. Accelerated Volumetric Compression without Hierarchies: A Fourier Feature Based Implicit Neural Representation Approach"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Accelerated_Volumetric_Compression_without_Hierarchies__A_Fourier_Feature_Based_Implicit_Neural_Repr.pdf">Accelerated Volumetric Compression without Hierarchies: A Fourier Feature Based Implicit Neural Representation Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">VSB – Technical University of Ostrava</span></p><p>该论文提出了一种无需分层结构的体数据压缩方法，结合傅里叶特征编码与选择性体素采样，通过动态体素选择和形态膨胀，显著提升压缩效率和训练速度。实验表明，该方法在保持较高重建质量的同时，训练时间缩短63.7%，实现了14倍压缩率，为医学成像和科学可视化等应用提供了高效的体数据表达方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Accelerated_Volumetric_Compression_without_Hierarchies_A_Fourier_Feature_Based_Implicit_Neural_Representation_Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-Masked-Clustering-Prediction-for-Unsupervised-Point-Cloud-Pre-training"><a href="#31-Masked-Clustering-Prediction-for-Unsupervised-Point-Cloud-Pre-training" class="headerlink" title="31. Masked Clustering Prediction for Unsupervised Point Cloud Pre-training"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Masked_Clustering_Prediction_for_Unsupervised_Point_Cloud_Pre-training.pdf">Masked Clustering Prediction for Unsupervised Point Cloud Pre-training</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Pisa</span></p><p>该论文提出了MaskClu，一种结合遮蔽点建模与聚类学习的ViT无监督预训练方法，用于3D点云理解。方法通过图卷积网络重建聚类分配和聚类中心，并引入全局对比学习提升实例级特征表达。实验在分割、检测、分类等多项3D任务上显示MaskClu实现了新的竞争性结果，显著提升了点云语义特征的密集性与泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Masked_Clustering_Prediction_for_Unsupervised_Point_Cloud_Pre-training.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-Geometry-Aware-Global-Feature-Aggregation-for-Real-Time-Indirect-Illumination"><a href="#32-Geometry-Aware-Global-Feature-Aggregation-for-Real-Time-Indirect-Illumination" class="headerlink" title="32. Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Geometry-Aware_Global_Feature_Aggregation_for_Real-Time_Indirect_Illumination.pdf">Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出了一种用于实时间接光照预测的神经网络方法，结合了空间几何特征引导的全局特征聚合模块（基于多头注意力机制）和单色处理设计，能够高效捕捉长距离间接照明。实验表明，该方法在复杂、动态光照场景下具有优异的泛化能力和真实感，推理速度快，优于现有学习型间接光照估计算法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Geometry-Aware_Global_Feature_Aggregation_for_Real-Time_Indirect_Illumination.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-TARA-Token-Aware-LoRA-for-Composable-Personalization-in-Diffusion-Models"><a href="#33-TARA-Token-Aware-LoRA-for-Composable-Personalization-in-Diffusion-Models" class="headerlink" title="33. TARA: Token-Aware LoRA for Composable Personalization in Diffusion Models"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/TARA__Token-Aware_LoRA_for_Composable_Personalization_in_Diffusion_Models.pdf">TARA: Token-Aware LoRA for Composable Personalization in Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</span></p><p>本文提出了Token-Aware LoRA (TARA)方法，通过引入Token Focus Masking（TFM）和Token Alignment Loss（TAL）来提升扩散模型在多概念个性化文本到图像生成任务中的表现。TFM利用稀有token掩码约束每个LoRA模块仅影响其对应的token，避免干扰；TAL引导每个稀有token的空间注意力与其概念区域对齐，从而减少特征泄漏，实现无需额外训练即可多概念组合生成。实验结果显示，TARA能够高效地进行多概念推理，有效保持每个概念的视觉一致性，优于现有主流方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/TARA_Token-Aware_LoRA_for_Composable_Personalization_in_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-DiffPose-Animal-A-Language-Conditioned-Diffusion-Framework-for-Animal-Pose-Estimation"><a href="#34-DiffPose-Animal-A-Language-Conditioned-Diffusion-Framework-for-Animal-Pose-Estimation" class="headerlink" title="34. DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DiffPose-Animal__A_Language-Conditioned_Diffusion_Framework_for_Animal_Pose_Estimation.pdf">DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Guanghua Cambridge International School</span></p><p>该论文提出了DiffPose-Animal框架，将动物姿态估计建模为一个条件扩散去噪过程，通过融合大语言模型（LLM）生成的全局-局部解剖语义先验和视觉特征，利用跨模态注意力机制指导关键点生成。实验结果显示，该方法在AP-10K、AnimalPose和Animal Kingdom等多个公开动物姿态数据集上取得了优于现有方法的性能，尤其在物种多样、遮挡、关键点缺失等挑战场景下表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DiffPose-Animal_A_Language-Conditioned_Diffusion_Framework_for_Animal_Pose_Estimation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Designing-Memory-Augmented-AR-Agents-for-Spatiotemporal-Reasoning-in-Personalized-Task-Assistance"><a href="#35-Designing-Memory-Augmented-AR-Agents-for-Spatiotemporal-Reasoning-in-Personalized-Task-Assistance" class="headerlink" title="35. Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Designing_Memory-Augmented_AR_Agents_for_Spatiotemporal_Reasoning_in_Personalized_Task_Assistance.pdf">Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yonsei University</span></p><p>本文提出了一种模块化的记忆增强型增强现实（AR）代理框架，集成了多模态感知、场景图记忆、时空推理和智能执行模块，实现对用户长期行为和偏好的捕捉与个性化辅助。实验路线图和典型应用场景（如个性化烹饪指导）展示了该框架能显著提升AR系统在复杂多步任务中的上下文感知和个性化指导能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Designing_Memory-Augmented_AR_Agents_for_Spatiotemporal_Reasoning_in_Personalized_Task_Assistance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-Bridging-the-Gap-A-Framework-for-Real-World-Video-Deepfake-Detection-via-Social-Network-Compression-Emulation"><a href="#36-Bridging-the-Gap-A-Framework-for-Real-World-Video-Deepfake-Detection-via-Social-Network-Compression-Emulation" class="headerlink" title="36. Bridging the Gap: A Framework for Real-World Video Deepfake Detection via Social Network Compression Emulation"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Bridging_the_Gap__A_Framework_for_Real-World_Video_Deepfake_Detection_via_Social_Network_Compression.pdf">Bridging the Gap: A Framework for Real-World Video Deepfake Detection via Social Network Compression Emulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Trento</span></p><p>本文提出了一个社交网络视频共享仿真框架（SNVSE），通过分析少量上传到社交平台的视频，估算压缩与分辨率参数，从而本地化地模拟YouTube、Facebook等平台对视频的压缩处理过程。实验表明，利用仿真生成的数据微调深度伪造检测模型，其检测性能与使用真实社交媒体分享视频微调的效果相当，有效弥合了实验室环境与真实场景的差距。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Bridging_the_Gap_A_Framework_for_Real-World_Video_Deepfake_Detection_via_Social_Network_Compression_Emulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-Exploring-Palette-based-Color-Guidance-in-Diffusion-Models"><a href="#37-Exploring-Palette-based-Color-Guidance-in-Diffusion-Models" class="headerlink" title="37. Exploring Palette based Color Guidance in Diffusion Models"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Exploring_Palette_based_Color_Guidance_in_Diffusion_Models.pdf">Exploring Palette based Color Guidance in Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CyberAgent Inc.</span></p><p>本论文提出在扩散模型中引入独立的调色板（palette）引导机制，以提升图像生成中色彩方案的可控性。方法上，作者设计了多模态调色板表示模型（仅调色板、文本-调色板、图像-调色板），通过交叉注意力机制将调色板嵌入与文本嵌入融合，为图像生成提供精确色彩指导；并构建了Palette-Text-Image数据集进行定量与用户研究。结果表明，调色板引导能大幅提升生成图像的色彩一致性和控制精度，优于仅文本引导及直接在文本中插入调色板描述的基线方法，且受到专业设计师的高度评价。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Exploring_Palette_based_Color_Guidance_in_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-ROD-RGB-Only-Fast-and-Efficient-Off-road-Freespace-Detection"><a href="#38-ROD-RGB-Only-Fast-and-Efficient-Off-road-Freespace-Detection" class="headerlink" title="38. ROD: RGB-Only Fast and Efficient Off-road Freespace Detection"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/ROD__RGB-Only_Fast_and_Efficient_Off-road_Freespace_Detection.pdf">ROD: RGB-Only Fast and Efficient Off-road Freespace Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Computing Technology, Chinese Academy of Sciences</span></p><p>本论文提出ROD方法，利用预训练的Vision Transformer (ViT)从RGB图像中提取特征，并设计高效轻量级解码器，实现离道路可通行区域的语义分割，无需LiDAR数据。结果显示ROD在ORFD和RELLIS-3D数据集上达到新的SOTA精度，同时推理速度达50FPS，远超多模态融合方法，满足实时应用需求。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/ROD_RGB-Only_Fast_and_Efficient_Off-road_Freespace_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-Efficient-Agent-Optimizing-Planning-Capability-for-Multimodal-Retrieval-Augmented-Generation"><a href="#39-Efficient-Agent-Optimizing-Planning-Capability-for-Multimodal-Retrieval-Augmented-Generation" class="headerlink" title="39. Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented Generation"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Efficient_Agent__Optimizing_Planning_Capability_for_Multimodal_Retrieval_Augmented_Generation.pdf">Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">OPPO Research Institute</span></p><p>本论文提出E-Agent框架，通过动态mRAG规划器和任务执行器实现多模态信息检索与生成的高效联动，采用单次规划策略动态选择搜索工具并优化执行流程。论文还构建了RemPlan基准，首次系统评测多模态检索规划能力，并在多项VQA任务上验证E-Agent在准确率和效率方面均优于现有方法，减少冗余搜索37%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Efficient_Agent_Optimizing_Planning_Capability_for_Multimodal_Retrieval_Augmented_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-MonoPartNeRF-Human-Reconstruction-from-Monocular-Video-via-Part-Based-Neural-Radiance-Fields"><a href="#40-MonoPartNeRF-Human-Reconstruction-from-Monocular-Video-via-Part-Based-Neural-Radiance-Fields" class="headerlink" title="40. MonoPartNeRF: Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MonoPartNeRF_Human_Reconstruction_from_Monocular_Video_via_Part-Based_Neural_Radiance_Fields.pdf">MonoPartNeRF: Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Guilin University of Electronic Technology</span></p><p>MonoPartNeRF提出了一种基于神经辐射场的单目动态人体高质量重建与渲染方法，通过人体结构分部、双向变形机制及UVT时空建模提升各部件几何一致性与自然关节连接。方法还融合了局部关节姿态嵌入与外观特征注意力机制，有效恢复遮挡区域细节，在ZJU-MoCap和MonoCap等数据集上显著优于现有方法，具备更强的结构一致性、视觉真实感和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MonoPartNeRF_Human_Reconstruction_from_Monocular_Video_via_Part-Based_Neural_Radiance_Fields.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-MMIF-AMIN-Adaptive-Loss-Driven-Multi-Scale-Invertible-Dense-Network-for-Multimodal-Medical-Image-Fusion"><a href="#41-MMIF-AMIN-Adaptive-Loss-Driven-Multi-Scale-Invertible-Dense-Network-for-Multimodal-Medical-Image-Fusion" class="headerlink" title="41. MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for Multimodal Medical Image Fusion"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MMIF-AMIN__Adaptive_Loss-Driven_Multi-Scale_Invertible_Dense_Network_for_Multimodal_Medical_Image_Fu.pdf">MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for Multimodal Medical Image Fusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southwest University</span></p><p>本文提出了一种新型的多模态医学图像融合方法MMIF-AMIN，采用可逆稠密网络（IDN）实现单一模态特征的无损提取，结合多尺度互补特征提取模块（MCFEM），集成CBAM注意力机制、不同尺寸卷积和Transformer以捕捉多模态间的互补信息。引入自适应损失函数，基于源图像统计信息自动调整参数，避免手动调参并提升模型解释性。实验表明，该方法在多个主流数据集上超越九种SOTA方法，融合效果优异，且具备良好泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MMIF-AMIN_Adaptive_Loss-Driven_Multi-Scale_Invertible_Dense_Network_for_Multimodal_Medical_Image_Fusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Unified-and-Semantically-Grounded-Domain-Adaptation-for-Medical-Image-Segmentation"><a href="#42-Unified-and-Semantically-Grounded-Domain-Adaptation-for-Medical-Image-Segmentation" class="headerlink" title="42. Unified and Semantically Grounded Domain Adaptation for Medical Image Segmentation"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Unified_and_Semantically_Grounded_Domain_Adaptation_for_Medical_Image_Segmentation.pdf">Unified and Semantically Grounded Domain Adaptation for Medical Image Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Washington</span></p><p>本文提出了一种统一且具备语义基础的无监督领域自适应（UDA）框架，用于医学图像分割，能够同时支持source-accessible与source-free两种设置。方法上，创新性地引入了可解释的、解耦的人体结构与个体几何特征的概率流形，将所有解剖学模板嵌入共享的低维空间，并通过可学习的基分布实现形状记忆和可控变换。结论表明，该方法在多器官、多模态公开数据集上取得了领域内最优性能，尤其在困难的source-free场景下表现出高度的泛化能力和稳健性，同时具备良好的可解释性和领域一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Unified_and_Semantically_Grounded_Domain_Adaptation_for_Medical_Image_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-Hierarchical-Visual-Prompt-Learning-for-Continual-Video-Instance-Segmentation"><a href="#43-Hierarchical-Visual-Prompt-Learning-for-Continual-Video-Instance-Segmentation" class="headerlink" title="43. Hierarchical Visual Prompt Learning for Continual Video Instance Segmentation"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Hierarchical_Visual_Prompt_Learning_for_Continual_Video_Instance_Segmentation.pdf">Hierarchical Visual Prompt Learning for Continual Video Instance Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohamed bin Zayed University of Artificial Intelligence</span></p><p>该论文提出了分层视觉提示学习（HVPL）模型以解决持续视频实例分割（CVIS）问题，方法包括任务特定的帧和视频提示、正交梯度校正模块及视频上下文解码器，分别从帧和视频层面缓解旧类别遗忘。实验结果显示，HVPL在多个公开数据集上显著优于现有基线方法，有效缓解了灾难性遗忘并提升了分割性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Hierarchical_Visual_Prompt_Learning_for_Continual_Video_Instance_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Yan-Foundational-Interactive-Video-Generation"><a href="#44-Yan-Foundational-Interactive-Video-Generation" class="headerlink" title="44. Yan: Foundational Interactive Video Generation"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Yan__Foundational_Interactive_Video_Generation.pdf">Yan: Foundational Interactive Video Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent</span></p><p>该论文提出了Yan，一个集成高保真实时仿真（Yan-Sim）、多模态可控生成（Yan-Gen）和多粒度交互编辑（Yan-Edit）的交互式视频生成基础框架。通过3D游戏环境自动采集大规模高质量交互视频数据，Yan采用高压缩3D-VAE、分层自回归文本描述、KV-cache窗口去噪、结构-风格解耦等关键技术，实现1080P&#x2F;60FPS实时仿真、文本&#x2F;图像驱动的无限交互生成，以及基于文本的结构与风格实时编辑。结论认为Yan推动了交互式视频生成从碎片功能向统一、可创作范式迈进，为下一代AI内容创作工具奠定基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Yan_Foundational_Interactive_Video_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-Unlocking-the-Potential-of-Diffusion-Priors-in-Blind-Face-Restoration"><a href="#45-Unlocking-the-Potential-of-Diffusion-Priors-in-Blind-Face-Restoration" class="headerlink" title="45. Unlocking the Potential of Diffusion Priors in Blind Face Restoration"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Unlocking_the_Potential_of_Diffusion_Priors_in_Blind_Face_Restoration.pdf">Unlocking the Potential of Diffusion Priors in Blind Face Restoration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Warwick</span></p><p>本文提出FLIPNET，一种基于大规模文本到图像（T2I）扩散模型并结合LoRA微调的统一网络，用于盲人脸部修复。该方法通过切换输入顺序在恢复模式和退化模式间切换，利用BoostHub模块有选择地注入低质量（LQ）特征，并引入BFR导向的面部嵌入以提升身份一致性和细节恢复。实验表明，FLIPNET在合成和真实世界数据集上都优于现有BFR方法，能够实现真实且忠实的人脸修复，同时更好地模拟复杂真实世界退化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Unlocking_the_Potential_of_Diffusion_Priors_in_Blind_Face_Restoration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-Calibration-Attention-Instance-wise-Temperature-Scaling-for-Vision-Transformers"><a href="#46-Calibration-Attention-Instance-wise-Temperature-Scaling-for-Vision-Transformers" class="headerlink" title="46. Calibration Attention: Instance-wise Temperature Scaling for Vision Transformers"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Calibration_Attention__Instance-wise_Temperature_Scaling_for_Vision_Transformers.pdf">Calibration Attention: Instance-wise Temperature Scaling for Vision Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Adelaide</span></p><p>本文提出了Calibration Attention (CalAttn)模块，为Vision Transformer (ViT, DeiT, Swin)引入了基于每个样本的动态温度标定机制。方法通过轻量级两层MLP从CLS token自适应地预测温度，并在训练和推理阶段联合优化交叉熵及Brier损失，实现端到端的概率标定。实验结果在CIFAR-10&#x2F;100、MNIST、TinyImageNet和ImageNet1K等多数据集上，CalAttn能让校准误差降低至原方法的1&#x2F;4，并且模型参数增加不到0.1%。结论是CalAttn能高效提升ViT类模型的预测可靠性和概率可信度，且无需后处理或复杂调优，具备架构无关和易集成的优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Calibration_Attention_Instance-wise_Temperature_Scaling_for_Vision_Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-Hybrid-Long-and-Short-Range-Flows-for-Point-Cloud-Filtering"><a href="#47-Hybrid-Long-and-Short-Range-Flows-for-Point-Cloud-Filtering" class="headerlink" title="47. Hybrid Long and Short Range Flows for Point Cloud Filtering"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Hybrid_Long_and_Short_Range_Flows_for_Point_Cloud_Filtering.pdf">Hybrid Long and Short Range Flows for Point Cloud Filtering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Western Australia</span></p><p>本文提出了一种混合点云滤波方法HybridPF，将短程分数估计（score-based）与长程流（long-range flow）相结合，通过并行的ShortModule和LongModule模块分别建模短程与长程噪声去除轨迹，并用联合损失实现端到端训练。实验表明，该方法在多种合成和真实点云数据上实现了更优的滤波精度和更快的推理速度，有效克服了现有方法的点聚集或噪声残留问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Hybrid_Long_and_Short_Range_Flows_for_Point_Cloud_Filtering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-QueryCraft-Transformer-Guided-Query-Initialization-for-Enhanced-Human-Object-Interaction-Detection"><a href="#48-QueryCraft-Transformer-Guided-Query-Initialization-for-Enhanced-Human-Object-Interaction-Detection" class="headerlink" title="48. QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/QueryCraft__Transformer-Guided_Query_Initialization_for_Enhanced_Human-Object_Interaction_Detection.pdf">QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China University of Technology</span></p><p>本文提出QueryCraft框架，通过引入两大模块：知识蒸馏的感知查询解码器（PDQD）、以及基于跨模态Transformer的动作感知交互查询器（ACTOR），赋予DETR类HOI检测模型以语义先验和指导特征学习。实验表明，该方法在HICO-Det和V-COCO等基准上大幅提升了检测性能、泛化能力和训练效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/QueryCraft_Transformer-Guided_Query_Initialization_for_Enhanced_Human-Object_Interaction_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-RealisMotion-Decomposed-Human-Motion-Control-and-Video-Generation-in-the-World-Space"><a href="#49-RealisMotion-Decomposed-Human-Motion-Control-and-Video-Generation-in-the-World-Space" class="headerlink" title="49. RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/RealisMotion__Decomposed_Human_Motion_Control_and_Video_Generation_in_the_World_Space.pdf">RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">DAMO Academy, Alibaba Group</span></p><p>本文提出RealisMotion框架，将人体视频生成任务中的主体、背景、运动轨迹和动作进行显式解耦。方法首先在物理世界空间中构建3D坐标系，通过SMPL-X参数及深度估计进行运动编辑，然后基于扩散-Transformer视频生成模型实现主体注入、背景拼接和运动控制信号融合，实现了任意人物在任意背景下按指定轨迹和动作生成自然视频。实验表明，该方法在可控性和视频质量上均达到当前最优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/RealisMotion_Decomposed_Human_Motion_Control_and_Video_Generation_in_the_World_Space.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-Boosting-Generic-Semi-Supervised-Medical-Image-Segmentation-via-Diverse-Teaching-and-Label-Propagation"><a href="#50-Boosting-Generic-Semi-Supervised-Medical-Image-Segmentation-via-Diverse-Teaching-and-Label-Propagation" class="headerlink" title="50. Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse Teaching and Label Propagation"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Boosting_Generic_Semi-Supervised_Medical_Image_Segmentation_via_Diverse_Teaching_and_Label_Propagati.pdf">Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse Teaching and Label Propagation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了一种通用半监督医学图像分割框架DTLP-Net，融合了多样教师模型、伪标签传播、全局-局部一致性学习及知识蒸馏等关键技术，能够同时应对SSMIS、UMDA和Semi-MDG等任务。实验表明，该方法在五个公开数据集上超越了现有最优方法，显著提升了在标注有限和域迁移场景下的分割性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Boosting_Generic_Semi-Supervised_Medical_Image_Segmentation_via_Diverse_Teaching_and_Label_Propagation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-Training-Kindai-OCR-with-parallel-textline-images-and-self-attention-feature-distance-based-loss"><a href="#51-Training-Kindai-OCR-with-parallel-textline-images-and-self-attention-feature-distance-based-loss" class="headerlink" title="51. Training Kindai OCR with parallel textline images and self-attention feature distance-based loss"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Training_Kindai_OCR_with_parallel_textline_images_and_self-attention_feature_distance-based_loss.pdf">Training Kindai OCR with parallel textline images and self-attention feature distance-based loss</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nguyen Tat Thanh University</span></p><p>本文提出通过生成并利用近代（日文）历史文献原图与现代字体合成图组成的平行文本行图像对，结合基于自注意特征距离（欧氏距离和最大均值差异MMD）的目标函数，提升Transformer OCR在样本稀缺条件下的识别能力。实验表明，该方法有效缩小了特征领域差距，在历史文献字符识别中使CER分别下降2.23%和3.94%，显著提升了模型的泛化与判别能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Training_Kindai_OCR_with_parallel_textline_images_and_self-attention_feature_distance-based_loss.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="52-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models"><a href="#52-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models" class="headerlink" title="52. Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Time_Is_a_Feature__Exploiting_Temporal_Dynamics_in_Diffusion_Language_Models.pdf">Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文针对扩散大语言模型（dLLMs）在文本生成过程中存在的“时序振荡”现象，提出了两种利用中间预测提升模型性能的方法：一是无需额外训练的“时序自一致性投票”，通过聚合不同去噪步的预测结果，提升推理准确性；二是基于强化学习的“时序一致性增强”，采用时序语义熵（TSE）作为奖励信号，鼓励模型输出更加稳定一致的结果。实验表明，两种方法结合后在数学推理等数据集上显著提升了dLLMs的准确率和稳定性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Time_Is_a_Feature_Exploiting_Temporal_Dynamics_in_Diffusion_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-BrowseMaster-Towards-Scalable-Web-Browsing-via-Tool-Augmented-Programmatic-Agent-Pair"><a href="#53-BrowseMaster-Towards-Scalable-Web-Browsing-via-Tool-Augmented-Programmatic-Agent-Pair" class="headerlink" title="53. BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/BrowseMaster__Towards_Scalable_Web_Browsing_via_Tool-Augmented_Programmatic_Agent_Pair.pdf">BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出BrowseMaster框架，通过规划-执行器（planner-executor）智能体对，将任务分解为高层次推理和程序化工具执行两部分，提升大规模网页检索的推理深度与搜索广度。实验结果表明，BrowseMaster在多个中英文复杂信息检索基准上大幅超过现有开源与专有智能体，具备强大的信息获取能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/BrowseMaster_Towards_Scalable_Web_Browsing_via_Tool-Augmented_Programmatic_Agent_Pair.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-Complex-Logical-Instruction-Generation"><a href="#54-Complex-Logical-Instruction-Generation" class="headerlink" title="54. Complex Logical Instruction Generation"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Complex_Logical_Instruction_Generation.pdf">Complex Logical Instruction Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Texas at Dallas</span></p><p>本文提出了LogicIFGen框架，可自动从代码函数生成可验证、逻辑丰富的自然语言指令，并基于此构建了LogicIFEval基准，包含426个复杂逻辑任务。实验发现，当前主流大语言模型在该基准上的指令跟随能力有限，多数准确率低于60%，揭示了LLMs在执行复杂逻辑指令时的显著不足。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Complex_Logical_Instruction_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-SMA-Who-Said-That-Auditing-Membership-Leakage-in-Semi-Black-box-RAG-Controlling"><a href="#55-SMA-Who-Said-That-Auditing-Membership-Leakage-in-Semi-Black-box-RAG-Controlling" class="headerlink" title="55. SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG Controlling"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/SMA__Who_Said_That__Auditing_Membership_Leakage_in_Semi-Black-box_RAG_Controlling.pdf">SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG Controlling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-Sen University</span></p><p>本文提出了SMA（Source-aware Membership Audit）框架，通过输入扰动与零阶优化回归，实现在半黑盒环境下对RAG和MRAG系统生成内容的细粒度溯源归因，区分内容是否来自模型预训练语料或外部检索库。实验表明，SMA在文本和多模态检索增强生成任务中显著优于现有方法，实现更高准确性与覆盖率，为生成式系统的数据合规与隐私审核提供实用工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/SMA_Who_Said_That_Auditing_Membership_Leakage_in_Semi-Black-box_RAG_Controlling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-Bridging-Formal-Language-with-Chain-of-Thought-Reasoning-to-Geometry-Problem-Solving"><a href="#56-Bridging-Formal-Language-with-Chain-of-Thought-Reasoning-to-Geometry-Problem-Solving" class="headerlink" title="56. Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Bridging_Formal_Language_with_Chain-of-Thought_Reasoning_to_Geometry_Problem_Solving.pdf">Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen Research Institute of Big Data</span></p><p>本文提出了一种将自然语言链式思维（Chain-of-Thought, CoT）与几何问题求解中的形式语言相结合的混合推理框架GF-Reasoner。该方法在Qwen2.5-VL-7B基础上，利用自动合成的自然-形式交织数据进行有监督微调，并结合几何求解器反馈的强化学习，训练模型在推理过程中交替生成自然语言和可执行的几何形式代码。实验表明，该方法在多个公开基准上几何问题求解准确率提升高达15%，且推理链条更简洁高效，错误率显著降低。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Bridging_Formal_Language_with_Chain-of-Thought_Reasoning_to_Geometry_Problem_Solving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-Can-We-Trust-AI-to-Govern-AI-Benchmarking-LLM-Performance-on-Privacy-and-AI-Governance-Exams"><a href="#57-Can-We-Trust-AI-to-Govern-AI-Benchmarking-LLM-Performance-on-Privacy-and-AI-Governance-Exams" class="headerlink" title="57. Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy and AI Governance Exams"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Can_We_Trust_AI_to_Govern_AI__Benchmarking_LLM_Performance_on_Privacy_and_AI_Governance_Exams.pdf">Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy and AI Governance Exams</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Superset Labs PBC</span></p><p>该论文系统性地评估了十种主流大语言模型（LLM）在隐私和AI治理领域认证考试（CIPP&#x2F;US, CIPM, CIPT, AIGP）上的表现，通过标准化零样本多项选择题测试，比较模型得分与人类专业通过线。研究发现，Gemini 2.5 Pro、OpenAI GPT-5等前沿模型在隐私法规、技术控制以及AI治理方面均超过人类认证标准，表明这些模型已具备在高风险数据治理场景中辅助或自动完成专业任务的能力，但在管理类内容仍有改进空间。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Can_We_Trust_AI_to_Govern_AI_Benchmarking_LLM_Performance_on_Privacy_and_AI_Governance_Exams.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-Attacks-Defenses-Against-LLM-Fingerprinting"><a href="#58-Attacks-Defenses-Against-LLM-Fingerprinting" class="headerlink" title="58. Attacks &amp; Defenses Against LLM Fingerprinting"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Attacks_and_Defenses_Against_LLM_Fingerprinting.pdf">Attacks &amp; Defenses Against LLM Fingerprinting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Oak Ridge National Laboratory</span></p><p>本文提出了一种基于强化学习的LLM指纹攻击方法，通过自动优化查询选择，仅用3个查询即可达到93.89%的指纹识别准确率，比随机查询提升14.2%。同时，作者设计了基于语义保持的输出过滤防御机制，利用次级LLM对输出进行语义不变的重写，有效降低指纹攻击成功率，且输出质量保持高语义相似度。实验表明，强化学习优化可提升攻击效率，防御策略在多模型上均可大幅降低被识别概率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Attacks_%26_Defenses_Against_LLM_Fingerprinting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-A-Survey-on-Training-free-Alignment-of-Large-Language-Models"><a href="#59-A-Survey-on-Training-free-Alignment-of-Large-Language-Models" class="headerlink" title="59. A Survey on Training-free Alignment of Large Language Models"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/A_Survey_on_Training-free_Alignment_of_Large_Language_Models.pdf">A Survey on Training-free Alignment of Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>本文系统性综述了大语言模型（LLMs）训练无关（training-free, TF）对齐方法，将其分为预解码、解码中和解码后三个阶段，涵盖关键技术如提示工程、隐藏状态调整、对数值计算和输出过滤&#x2F;修正，并分析了在单模态和多模态模型上的应用机制与局限。结论指出，TF对齐方法在效率、通用性和模型安全性上表现出重大优势，但仍面临泛化能力、推理延迟和知识降级等挑战，未来需发展更高效、可控和适应性强的TF对齐技术。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/A_Survey_on_Training-free_Alignment_of_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-Retrospective-Sparse-Attention-for-Efficient-Long-Context-Generation"><a href="#60-Retrospective-Sparse-Attention-for-Efficient-Long-Context-Generation" class="headerlink" title="60. Retrospective Sparse Attention for Efficient Long-Context Generation"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Retrospective_Sparse_Attention_for_Efficient_Long-Context_Generation.pdf">Retrospective Sparse Attention for Efficient Long-Context Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University</span></p><p>本文提出了一种名为RetroAttention的KV缓存压缩与更新技术，用于大语言模型长文本生成任务。该方法通过引入轻量级输出缓存，将后续解码步骤中新加载的KV条目，回溯性地补充到历史查询的注意力输出中，实现对过去近似注意力错误的持续修正，极大提升了长上下文推理和生成的准确率。实验结果表明，RetroAttention在不增加KV缓存预算的前提下，有效提升了KV利用率（最大1.6倍）和准确率（最高提升21.9%），同时几乎不增加延迟和内存开销。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Retrospective_Sparse_Attention_for_Efficient_Long-Context_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-Intrinsic-Memory-Agents-Heterogeneous-Multi-Agent-LLM-Systems-through-Structured-Contextual-Memory"><a href="#61-Intrinsic-Memory-Agents-Heterogeneous-Multi-Agent-LLM-Systems-through-Structured-Contextual-Memory" class="headerlink" title="61. Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Intrinsic_Memory_Agents__Heterogeneous_Multi-Agent_LLM_Systems_through_Structured_Contextual_Memory.pdf">Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Alan Turing Institute</span></p><p>该论文提出了一种名为Intrinsic Memory Agents的多智能体LLM系统框架，通过为每个智能体分配结构化、角色对齐的异构记忆模板，并基于智能体自身输出动态更新，从而提升多智能体协作的连贯性、角色一致性与任务针对性。论文在PDDL基准和复杂数据管道设计任务中进行评测，结果显示该方法在规划任务上性能提升38.6%，在多项协作质量指标上均优于现有方法，尽管会增加token使用量但保持最高token效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Intrinsic_Memory_Agents_Heterogeneous_Multi-Agent_LLM_Systems_through_Structured_Contextual_Memory.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-ColorGPT-Leveraging-Large-Language-Models-for-Multimodal-Color-Recommendation"><a href="#62-ColorGPT-Leveraging-Large-Language-Models-for-Multimodal-Color-Recommendation" class="headerlink" title="62. ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/ColorGPT__Leveraging_Large_Language_Models_for_Multimodal_Color_Recommendation.pdf">ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Tokyo</span></p><p>本论文提出了ColorGPT，一种利用大语言模型（LLM）进行多模态颜色推荐的无训练（training-free）方法，针对矢量图形设计中的色彩调色板补全和生成任务。方法通过系统性测试多种颜色表示和提示工程，结合结构化JSON文档描述与相似性检索的上下文示例，实现了调色板自动补全与文本到调色板生成，并在准确率和多样性方面显著优于现有方法。结论显示，LLM具备常识推理能力，可有效辅助复杂设计场景下的颜色推荐与设计工作流。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/ColorGPT_Leveraging_Large_Language_Models_for_Multimodal_Color_Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-Safe-Semantics-Unsafe-Interpretations-Tackling-Implicit-Reasoning-Safety-in-Large-Vision-Language-Models"><a href="#63-Safe-Semantics-Unsafe-Interpretations-Tackling-Implicit-Reasoning-Safety-in-Large-Vision-Language-Models" class="headerlink" title="63. Safe Semantics, Unsafe Interpretations: Tackling Implicit Reasoning Safety in Large Vision-Language Models"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Safe_Semantics,_Unsafe_Interpretations__Tackling_Implicit_Reasoning_Safety_in_Large_Vision-Language_.pdf">Safe Semantics, Unsafe Interpretations: Tackling Implicit Reasoning Safety in Large Vision-Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出了Implicit Reasoning Safety（IRS）概念，指出大规模视觉-语言模型（LVLMs）在处理多模态输入时存在安全隐患，即单独安全的图像和文本组合后可能导致危险输出。为此，作者构建了首个用于该问题的数据集SSUI，并采用基于In-Context Learning的方法进行安全性提升实验。结果显示，使用SSUI后，LVLMs生成安全有效响应的能力显著提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Safe_Semantics%2C_Unsafe_Interpretations_Tackling_Implicit_Reasoning_Safety_in_Large_Vision-Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-Scaling-Up-Active-Testing-to-Large-Language-Models"><a href="#64-Scaling-Up-Active-Testing-to-Large-Language-Models" class="headerlink" title="64. Scaling Up Active Testing to Large Language Models"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Scaling_Up_Active_Testing_to_Large_Language_Models.pdf">Scaling Up Active Testing to Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Oxford</span></p><p>本文提出了一种将主动测试（active testing）方法扩展到大语言模型（LLM）评估的新流程。通过用小型且一次性构建的代理模型（利用in-context learning而无需反复训练）指导数据采集，并可仅依赖代理模型进行数据选择，有效降低了评估大语言模型的计算与标注开销。此外，提出了单次运行下的风险估计误差自举评估方法。实验表明，该方法在多个文本分类与推理任务上，比随机采样大幅提高了评估效率，显著减少了所需标签数，同时保证了误差可控。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Scaling_Up_Active_Testing_to_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-Dynamic-Uncertainty-aware-Multimodal-Fusion-for-Outdoor-Health-Monitoring"><a href="#65-Dynamic-Uncertainty-aware-Multimodal-Fusion-for-Outdoor-Health-Monitoring" class="headerlink" title="65. Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Dynamic_Uncertainty-aware_Multimodal_Fusion_for_Outdoor_Health_Monitoring.pdf">Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">City University of Hong Kong</span></p><p>该论文提出了一种名为DUAL-Health的动态不确定性感知多模态融合框架，用于动态和嘈杂环境下的室外健康监测。方法包括：1）通过当前和时序特征分别量化输入噪声和生理信号波动导致的模态不确定性；2）基于不确定性自适应分配每种模态的融合权重，并在Transformer架构中动态调整跨模态注意力，提升对低质量数据的鲁棒性；3）通过模态重建网络将波动的模态分布映射到公共语义空间，实现稳定的跨模态对齐和缺失数据恢复。实验结果表明，DUAL-Health在检测准确率和鲁棒性方面均优于现有多模态大语言模型、深度感知等主流方法，尤其在数据质量退化和模态缺失条件下表现突出。结论：DUAL-Health能在动态室外环境中实现更高效、鲁棒的健康监测。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Dynamic_Uncertainty-aware_Multimodal_Fusion_for_Outdoor_Health_Monitoring.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-P-D-Device-Disaggregated-Large-Language-Model-between-Cloud-and-Devices"><a href="#66-P-D-Device-Disaggregated-Large-Language-Model-between-Cloud-and-Devices" class="headerlink" title="66. P&#x2F;D-Device: Disaggregated Large Language Model between Cloud and Devices"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/P_D-Device__Disaggregated_Large_Language_Model_between_Cloud_and_Devices.pdf">P&#x2F;D-Device: Disaggregated Large Language Model between Cloud and Devices</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Technologies Co., Ltd.</span></p><p>本文提出P&#x2F;D-Device系统，实现了大语言模型在云和设备间的预填充与解码分离协作。其核心方法包括云端负责预填充并生成部分token，通过压缩和掩码技术将精炼后的prompt与首个token一起发送到设备，设备利用云端辅助的token平滑过渡长时间预填充，同时可在本地继续解码，算法自动确定最优协同参数。实验表明，该方案能将TTFT（首token延迟）降低至少60%，最大TPOT（每token生成延迟）控制在几十毫秒，云端吞吐提升最高达15倍，且在保证推理质量的同时显著提升响应速度和资源利用率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/P_D-Device_Disaggregated_Large_Language_Model_between_Cloud_and_Devices.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-E3-Rewrite-Learning-to-Rewrite-SQL-for-Executability-Equivalence-and-Efficiency"><a href="#67-E3-Rewrite-Learning-to-Rewrite-SQL-for-Executability-Equivalence-and-Efficiency" class="headerlink" title="67. E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence, and Efficiency"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/E3-Rewrite__Learning_to_Rewrite_SQL_for_Executability,_Equivalence,and_Efficiency.pdf">E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence, and Efficiency</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Soochow University</span></p><p>本文提出E3-Rewrite，一种基于大语言模型（LLM）的SQL重写框架，通过注入查询执行计划、混合结构-语义检索和两阶段强化学习（GRPO算法）联合优化可执行性、等价性与效率。实验表明，该方法在多个SQL基准上显著提升了查询效率和等价覆盖率，相较已有方法平均查询时间降低25.6%，成功重写数量提升24.4%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/E3-Rewrite_Learning_to_Rewrite_SQL_for_Executability%2C_Equivalence%2C_and_Efficiency.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-Activation-Steering-for-Bias-Mitigation-An-Interpretable-Approach-to-Safer-LLMs"><a href="#68-Activation-Steering-for-Bias-Mitigation-An-Interpretable-Approach-to-Safer-LLMs" class="headerlink" title="68. Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Activation_Steering_for_Bias_Mitigation__An_Interpretable_Approach_to_Safer_LLMs.pdf">Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indian Institute of Technology Madras</span></p><p>本文提出了一套基于机械解释性的方法，通过在GPT2-large模型内部训练线性探测器（probe）识别和定位性别、种族等偏见的表征，并用激活引导向量（steering vector）在推理阶段主动调整模型生成过程，实时缓解有害内容。实验表明，该方法能有效检测并降低模型输出中的偏见，提升LLM安全性和可控性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Activation_Steering_for_Bias_Mitigation_An_Interpretable_Approach_to_Safer_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-LyS-at-SemEval-2025-Task-8-Zero-Shot-Code-Generation-for-Tabular-QA"><a href="#69-LyS-at-SemEval-2025-Task-8-Zero-Shot-Code-Generation-for-Tabular-QA" class="headerlink" title="69. LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/LyS_at_SemEval_2025_Task_8__Zero-Shot_Code_Generation_for_Tabular_QA.pdf">LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universidade da Coruña</span></p><p>该论文提出了一种基于指令的大型语言模型（LLM）的零样本代码生成管道，用于表格问答任务。方法包括三模块：相关列选择、Python代码生成和基于错误反馈的迭代修正，提升了表格数据的问答准确性和系统的适应性。结论显示该方法无需微调即可在SemEval 2025 Tabular QA任务中优于基线模型，但在复杂表结构下准确率有所下降。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/LyS_at_SemEval_2025_Task_8_Zero-Shot_Code_Generation_for_Tabular_QA.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-Prospect-Theory-Fails-for-LLMs-Revealing-Instability-of-Decision-Making-under-Epistemic-Uncertainty"><a href="#70-Prospect-Theory-Fails-for-LLMs-Revealing-Instability-of-Decision-Making-under-Epistemic-Uncertainty" class="headerlink" title="70. Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Prospect_Theory_Fails_for_LLMs__Revealing_Instability_of_Decision-Making_under_Epistemic_Uncertainty.pdf">Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong University of Science and Technology</span></p><p>该论文提出了一个三阶段实验框架，将行为经济学和语义分析结合，用于评估大型语言模型（LLMs）在不确定性决策中的行为，尤其关注语言中的不确定性表达（epistemic markers）。结果显示，LLMs的决策行为无法稳定地用人类中心的前景理论（Prospect Theory, PT）进行建模，且不同模型对不确定性表达的解释差异大，决策不稳定，结论强调需发展针对AI的决策理论和更好的不确定性校准方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Prospect_Theory_Fails_for_LLMs_Revealing_Instability_of_Decision-Making_under_Epistemic_Uncertainty.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-KFFocus-Highlighting-Keyframes-for-Enhanced-Video-Understanding"><a href="#71-KFFocus-Highlighting-Keyframes-for-Enhanced-Video-Understanding" class="headerlink" title="71. KFFocus: Highlighting Keyframes for Enhanced Video Understanding"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/KFFocus__Highlighting_Keyframes_for_Enhanced_Video_Understanding.pdf">KFFocus: Highlighting Keyframes for Enhanced Video Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出KFFocus，一种针对视频大模型（Vid-LLMs）的混合视频帧与令牌压缩方法，通过借鉴传统视频压缩的关键帧（keyframe）提取与动态令牌凝聚策略，显著减少计算冗余并突出关键时序与语义信息。结合时空建模模块，KFFocus在多项长短视频理解基准上实现了更高效率和准确率，优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/KFFocus_Highlighting_Keyframes_for_Enhanced_Video_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-Integrating-Attention-into-Explanation-Frameworks-for-Language-and-Vision-Transformers"><a href="#72-Integrating-Attention-into-Explanation-Frameworks-for-Language-and-Vision-Transformers" class="headerlink" title="72. Integrating Attention into Explanation Frameworks for Language and Vision Transformers"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Integrating_attention_into_explanation_frameworks_for_language_and_vision_transformers.pdf">Integrating Attention into Explanation Frameworks for Language and Vision Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Norwegian University of Science and Technology</span></p><p>本文提出了两种将注意力机制信息整合入主流可解释性AI（XAI）框架的新方法，分别适用于NLP和视觉任务。第一种方法将注意力权重引入Shapley值分解来进行局部特征归因，第二种方法将注意力与概念激活向量结合，计算全球性概念敏感性分数。实验结果显示，注意力权重可有效丰富Transformer模型的可解释性，尤其在计算效率和关注内部机制上表现突出。但纯注意力归因在标准输出相关的XAI指标下略逊于基于输入的归因方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Integrating_Attention_into_Explanation_Frameworks_for_Language_and_Vision_Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning"><a href="#73-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning" class="headerlink" title="73. Train Long, Think Short: Curriculum Learning for Efficient Reasoning"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Train_Long,_Think_Short__Curriculum_Learning_for_Efficient_Reasoning.pdf">Train Long, Think Short: Curriculum Learning for Efficient Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">King Abdullah University of Science and Technology (KAUST)</span></p><p>本文提出了一种基于课程学习的高效推理方法，通过在Group Relative Policy Optimization (GRPO)中引入递减token预算，实现大模型推理链条从探索到压缩的自然过渡。方法融合了正确性、长度效率和格式结构三重奖励信号，实验表明在GSM8K、MATH500等多个数学推理数据集上，课程学习显著提升了准确率和token效率，优于固定预算基线，且在不同数据集和预算调度下均有稳定表现。结论：课程驱动的压缩训练是提升大语言模型推理效率和能力的有效通用范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Train_Long%2C_Think_Short_Curriculum_Learning_for_Efficient_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-Compass-Thinker-7B-Technical-Report"><a href="#74-Compass-Thinker-7B-Technical-Report" class="headerlink" title="74. Compass-Thinker-7B Technical Report"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Compass-Thinker-7B_Technical_Report.pdf">Compass-Thinker-7B Technical Report</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shopee</span></p><p>本文提出Compass-Thinker-7B，一种基于开源大语言模型，通过精心设计的强化学习（RL）流水线进行训练，采用GRPO变体及多项改进，并构建3万道可验证数学题数据集用于奖励建模。实验表明，该模型在AIME24等多个数学推理基准上表现优异，尤其在AIME24准确率由20%提升到40%，证明了低资源下精细RL流程对小型LLM推理能力的显著提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Compass-Thinker-7B_Technical_Report.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-Oblivionis-A-Lightweight-Learning-and-Unlearning-Framework-for-Federated-Large-Language-Models"><a href="#75-Oblivionis-A-Lightweight-Learning-and-Unlearning-Framework-for-Federated-Large-Language-Models" class="headerlink" title="75. Oblivionis: A Lightweight Learning and Unlearning Framework for Federated Large Language Models"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Oblivionis__A_Lightweight_Learning_and_Unlearning_Framework_for_Federated_Large_Language_Models.pdf">Oblivionis: A Lightweight Learning and Unlearning Framework for Federated Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>本文提出了Oblivionis框架，将联邦学习(FL)与定向遗忘(unlearning)联合建模为双目标优化，实现分布式大模型在不共享原始隐私数据的前提下，支持特定数据或客户贡献的后期删除。方法集成6种主流联邦学习算法与5种遗忘算法，并在TOFU和MUSE等基准上验证，结果显示Oblivionis能在遗忘效果和模型效用间实现稳健平衡，优于本地训练且提升法规合规性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Oblivionis_A_Lightweight_Learning_and_Unlearning_Framework_for_Federated_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-Steering-Towards-Fairness-Mitigating-Political-Bias-in-LLMs"><a href="#76-Steering-Towards-Fairness-Mitigating-Political-Bias-in-LLMs" class="headerlink" title="76. Steering Towards Fairness: Mitigating Political Bias in LLMs"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Steering_Towards_Fairness__Mitigating_Political_Bias_in_LLMs.pdf">Steering Towards Fairness: Mitigating Political Bias in LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Macquarie University</span></p><p>本文提出了一种基于对比政治构造（Political Compass Test, PCT）和Steering Vector Ensembles (SVE) 的大语言模型（LLM）去偏框架，通过层级激活抽取与向量注入，对模型内部意识形态偏见进行探测与干预。实验结果显示，SVE在多语言、特别是低资源语言环境下，可在不损失输出流畅度和语境相关性的情况下，实现高达60%的社会偏见降低，优于Individual Steering Vectors (ISV)。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Steering_Towards_Fairness_Mitigating_Political_Bias_in_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-An-Investigation-of-Robustness-of-LLMs-in-Mathematical-Reasoning-Benchmarking-with-Mathematically-Equivalent-Transformation-of-Advanced-Mathematical-Problems"><a href="#77-An-Investigation-of-Robustness-of-LLMs-in-Mathematical-Reasoning-Benchmarking-with-Mathematically-Equivalent-Transformation-of-Advanced-Mathematical-Problems" class="headerlink" title="77. An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/An_Investigation_of_Robustness_of_LLMs_in_Mathematical_Reasoning__Benchmarking_with_Mathematically-E.pdf">An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Urbana-Champaign</span></p><p>该论文提出Generalization-and-Perturbation (GAP) 框架，通过对Putnam竞赛数学题进行数学等价但语言和参数不同的变换，系统评估大型语言模型（LLM）在数学推理上的鲁棒性。作者构建了包含6,306道题目的PutnamGAP数据集，对18个主流LLM进行了零样本测试，结果显示各模型在变量名或参数变换下准确率显著下降，揭示了当前LLM在符号和结构泛化上的不足。结论认为，当前高分模型在面对表面变化时表现脆弱，需重视数据污染防御、训练策略多样化和安全性提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/An_Investigation_of_Robustness_of_LLMs_in_Mathematical_Reasoning_Benchmarking_with_Mathematically-Equivalent_Transformation_of_Advanced_Mathematical_Problems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-3DFroMLLM-3D-Prototype-Generation-only-from-Pretrained-Multimodal-LLMs"><a href="#78-3DFroMLLM-3D-Prototype-Generation-only-from-Pretrained-Multimodal-LLMs" class="headerlink" title="78. 3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/3DFroMLLM__3D_Prototype_Generation_only_from_Pretrained_Multimodal_LLMs.pdf">3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Technology Nuremberg</span></p><p>本文提出3DFroMLLM框架，实现了仅依赖预训练多模态大模型（MLLMs），无需额外训练数据或详细用户指令，直接生成包含几何体和部件标签的3D原型。其方法采用Designer、Coder和Visual Inspector三代理反馈循环，自动分解、生成和优化3D模型。实验表明，该方法生成的3D原型用于图像分类预训练较以往方法提升15%，用于CLIP部件分割微调可提升55%（无人工标注）。结论：多模态大模型具备直接生成高质量3D原型并助力下游视觉任务的能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/3DFroMLLM_3D_Prototype_Generation_only_from_Pretrained_Multimodal_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-A-Dual-Axis-Taxonomy-of-Knowledge-Editing-for-LLMs-From-Mechanisms-to-Functions"><a href="#79-A-Dual-Axis-Taxonomy-of-Knowledge-Editing-for-LLMs-From-Mechanisms-to-Functions" class="headerlink" title="79. A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to Functions"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/A_Dual-Axis_Taxonomy_of_Knowledge_Editing_for_LLMs__From_Mechanisms_to_Functions.pdf">A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to Functions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Tehran</span></p><p>本文提出了一种针对大型语言模型（LLM）知识编辑的双轴分类体系：一轴为编辑机制（如参数修改、外部记忆、神经元增强、元学习），另一轴为知识类型（事实、时序、概念、常识、社会知识），系统综述了不同机制在多类知识编辑场景下的适用性、评估方法及未来挑战。结论认为，知识编辑为LLM高效、可控地更新内部知识提供了重要手段，但仍需提升理论基础、规模化、多样化知识适应和安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/A_Dual-Axis_Taxonomy_of_Knowledge_Editing_for_LLMs_From_Mechanisms_to_Functions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-Interpretable-Reward-Model-via-Sparse-Autoencoder"><a href="#80-Interpretable-Reward-Model-via-Sparse-Autoencoder" class="headerlink" title="80. Interpretable Reward Model via Sparse Autoencoder"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Interpretable_Reward_Model_via_Sparse_Autoencoder.pdf">Interpretable Reward Model via Sparse Autoencoder</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>该论文提出了一种名为Sparse Autoencoder-enhanced Reward Model (SARM)的新架构，将预训练的稀疏自动编码器（SAE）集成到奖励模型中，通过将LLM隐藏激活映射到稀疏、单语义特征空间，使奖励分数可解释且可控。实验结果显示，SARM实现了特征级奖励归因和偏好动态调整，并在对齐任务上优于传统奖励模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Interpretable_Reward_Model_via_Sparse_Autoencoder.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-SciRerankBench-Benchmarking-Rerankers-Towards-Scientific-Retrieval-Augmented-Generated-LLMs"><a href="#81-SciRerankBench-Benchmarking-Rerankers-Towards-Scientific-Retrieval-Augmented-Generated-LLMs" class="headerlink" title="81. SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/SciRerankBench__Benchmarking_Rerankers_Towards_Scientific_Retrieval-Augmented_Generated_LLMs.pdf">SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Computer Network Information Center, Chinese Academy of Sciences</span></p><p>本文提出SciRerankBench，这是首个专为科学领域RAG-LLMs中重排序模块（reranker）设计的评测基准，涵盖生物、物理、化学、地理和数学五大学科，基于2.5亿篇学术论文构建4.5K问-上下文-答（Q-C-A）对，包含噪声、语义相似但逻辑无关及反事实三类干扰上下文。系统评测13种主流重排序方法在11个开源LLM下的表现，发现重排序器显著提升RAG-LLMs的检索与答案生成质量，尤其是cross-encoder架构在复杂推理任务中表现最佳，同时最终答案质量仍受限于LLM自身的推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/SciRerankBench_Benchmarking_Rerankers_Towards_Scientific_Retrieval-Augmented_Generated_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-Simulating-Generative-Social-Agents-via-Theory-Informed-Workflow-Design"><a href="#82-Simulating-Generative-Social-Agents-via-Theory-Informed-Workflow-Design" class="headerlink" title="82. Simulating Generative Social Agents via Theory-Informed Workflow Design"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Simulating_Generative_Social_Agents_via_Theory-Informed_Workflow_Design.pdf">Simulating Generative Social Agents via Theory-Informed Workflow Design</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (Guangzhou)</span></p><p>本文提出了一种基于社会认知理论的生成式社会代理统一设计框架，结合Maslow需求层次理论、计划行为理论和社会学习理论，将动机建模、行动规划和学习三大模块集成于LLM驱动的社会代理中，实现了目标形成、行为规划与行为适应的闭环流程。实验结果表明，该方法在多场景下显著提升社会代理的行为一致性和现实性，偏离真实数据的误差比主流生成式基线低65–80%，三大模块缺失会导致1.5–10倍的性能下降，证明其不可或缺性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Simulating_Generative_Social_Agents_via_Theory-Informed_Workflow_Design.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-A-Survey-on-Parallel-Text-Generation-From-Parallel-Decoding-to-Diffusion-Language-Models"><a href="#83-A-Survey-on-Parallel-Text-Generation-From-Parallel-Decoding-to-Diffusion-Language-Models" class="headerlink" title="83. A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/A_Survey_on_Parallel_Text_Generation__From_Parallel_Decoding_to_Diffusion_Language_Models.pdf">A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本论文系统梳理并分类了并行文本生成的主要方法，包括基于自回归（AR-Based）和非自回归（Non-AR-Based）的多种加速范式，如草稿-验证、分解-填充、多Token预测、一次性生成、掩码生成和编辑式细化。作者不仅从理论上比较了各类方法在速度、质量和资源上的权衡，还分析了不同技术组合的潜力与挑战，指出并行生成有望显著提升大语言模型推理效率，但面临质量-速度权衡和系统优化等难题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/A_Survey_on_Parallel_Text_Generation_From_Parallel_Decoding_to_Diffusion_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-ASPD-Unlocking-Adaptive-Serial-Parallel-Decoding-by-Exploring-Intrinsic-Parallelism-in-LLMs"><a href="#84-ASPD-Unlocking-Adaptive-Serial-Parallel-Decoding-by-Exploring-Intrinsic-Parallelism-in-LLMs" class="headerlink" title="84. ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/ASPD__Unlocking_Adaptive_Serial-Parallel_Decoding_by_Exploring_Intrinsic_Parallelism_in_LLMs.pdf">ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent YouTu Lab</span></p><p>本文提出了Adaptive Serial-Parallel Decoding (ASPD) 框架，通过自动挖掘大语言模型（LLM）生成内容中的内在并行结构，并引入无侵入式并行数据转换管线、分支不可见注意力掩码和共享位置编码，实现了高效的串行-并行混合解码。实验证明，ASPD显著提升了LLM推理速度（平均加速1.85-3.19倍），在多个基准任务上几乎不损失输出质量，适用于对时延敏感的实际场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/ASPD_Unlocking_Adaptive_Serial-Parallel_Decoding_by_Exploring_Intrinsic_Parallelism_in_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-Entangled-in-Representations-Mechanistic-Investigation-of-Cultural-Biases-in-Large-Language-Models"><a href="#85-Entangled-in-Representations-Mechanistic-Investigation-of-Cultural-Biases-in-Large-Language-Models" class="headerlink" title="85. Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Entangled_in_Representations__Mechanistic_Investigation_of_Cultural_Biases_in_Large_Language_Models.pdf">Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Copenhagen</span></p><p>本论文提出了Culturescope方法，通过基于机制可解释性的技术（如激活patching），直接探查大语言模型（LLM）内部层的文化知识表征，并引入文化扁平化分数（CF score）量化不同文化表征的混同与偏差。实验证明主流LLM存在西方主导和文化扁平化的内部偏见，低资源文化因训练数据匮乏受影响较小，提示未来需结合偏见与资源特征提升模型文化理解。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Entangled_in_Representations_Mechanistic_Investigation_of_Cultural_Biases_in_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them"><a href="#86-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them" class="headerlink" title="86. BiasGym: Fantastic Biases and How to Find (and Remove) Them"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/BiasGym__Fantastic_Biases_and_How_to_Find_(and_Remove)_Them.pdf">BiasGym: Fantastic Biases and How to Find (and Remove) Them</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Copenhagen</span></p><p>本文提出BiasGym，一个针对大型语言模型（LLM）的通用、低成本偏见注入与消除框架。方法包括通过BiasInject模块在模型中注入特定偏见（采用特殊token的微调），然后利用BiasScope模块识别和移除与偏见相关的注意力头，实现针对性去偏见，同时保持模型下游任务性能。实验结果显示，BiasGym能有效减少真实世界和虚构的刻板印象，且不会显著影响问答或指令跟随等任务能力，为LLM安全及可解释性研究提供新工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/BiasGym_Fantastic_Biases_and_How_to_Find_%28and_Remove%29_Them.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-EditMF-Drawing-an-Invisible-Fingerprint-for-Your-Large-Language-Models"><a href="#87-EditMF-Drawing-an-Invisible-Fingerprint-for-Your-Large-Language-Models" class="headerlink" title="87. EditMF: Drawing an Invisible Fingerprint for Your Large Language Models"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/EditMF__Drawing_an_Invisible_Fingerprint_for_Your_Large_Language_Models.pdf">EditMF: Drawing an Invisible Fingerprint for Your Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">China Agricultural University</span></p><p>EditMF提出了一种基于模型编辑的无训练指纹嵌入方法，通过将所有权信息加密为虚构作者-小说-主角三元组，并利用因果追踪和零空间更新在大语言模型中嵌入极其隐蔽的专属指纹。该方法无需额外训练，仅需一次黑盒查询即可验证所有权，实验表明其嵌入鲁棒性高，基本无性能损失，计算开销极低，优于现有LoRA与SFT指纹方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/EditMF_Drawing_an_Invisible_Fingerprint_for_Your_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-Silicon-Minds-versus-Human-Hearts-The-Wisdom-of-Crowds-Beats-the-Wisdom-of-AI-in-Emotion-Recognition"><a href="#88-Silicon-Minds-versus-Human-Hearts-The-Wisdom-of-Crowds-Beats-the-Wisdom-of-AI-in-Emotion-Recognition" class="headerlink" title="88. Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of AI in Emotion Recognition"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Silicon_Minds_versus_Human_Hearts__The_Wisdom_of_Crowds_Beats_the_Wisdom_of_AI_in_Emotion_Recognitio.pdf">Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of AI in Emotion Recognition</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Elon University</span></p><p>本论文系统评估了多模态大型语言模型（MLLMs，如GPT-4o）在情绪识别任务中的表现，并与不同能力水平的人类进行对比，包括低、中、高水平和群体智慧。方法采用RMET和MRMET标准测试，通过单体和集体（多数投票）方式比较GPT-4o与人类的正确率，并分析了人机协作（增强智能）方案。结论：GPT-4o在个体层面全范围优于人类，但人类群体（“群体智慧”）通过集体判断显著超越AI群体，且人机协作能进一步提升准确率，显示未来情感智能AI最优路径是人机协作。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Silicon_Minds_versus_Human_Hearts_The_Wisdom_of_Crowds_Beats_the_Wisdom_of_AI_in_Emotion_Recognition.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments"><a href="#89-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments" class="headerlink" title="89. Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Feedback-Driven_Tool-Use_Improvements_in_Large_Language_Models_via_Automated_Build_Environments.pdf">Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出了一种自动化环境构建流水线，包括场景分解、文档生成、功能整合、复杂度扩展及本地部署，实现了无需依赖外部工具即可大规模生成高质量LLM工具使用训练环境。基于该环境，设计了可验证的奖励机制，对工具调用的精确性与任务完成度进行联合评估，并与RL算法结合实现反馈驱动的模型训练。实验表明，该方法显著提升了不同规模LLM在工具使用上的表现，且不损害模型的通用能力，主要提升源于模型低层MLP参数的更新，从而增强了上下文理解和推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Feedback-Driven_Tool-Use_Improvements_in_Large_Language_Models_via_Automated_Build_Environments.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge"><a href="#90-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge" class="headerlink" title="90. Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Evaluating_Podcast_Recommendations_with_Profile-Aware_LLM-as-a-Judge.pdf">Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Spotify</span></p><p>该论文提出了一种基于大型语言模型(LLM)的离线评估框架，通过自动生成自然语言用户画像来提升播客推荐系统的个性化评估能力。方法包括从用户近90天收听历史提炼出用户兴趣画像，利用LLM依据画像与播客元数据进行点对点及模型间对比判断，结果显示该画像感知方法在与人工评价一致性上优于直接使用历史数据，提升了可扩展性和解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Evaluating_Podcast_Recommendations_with_Profile-Aware_LLM-as-a-Judge.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-DevNous-An-LLM-Based-Multi-Agent-System-for-Grounding-IT-Project-Management-in-Unstructured-Conversation"><a href="#91-DevNous-An-LLM-Based-Multi-Agent-System-for-Grounding-IT-Project-Management-in-Unstructured-Conversation" class="headerlink" title="91. DevNous: An LLM-Based Multi-Agent System for Grounding IT Project Management in Unstructured Conversation"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DevNous__An_LLM-Based_Multi-Agent_System_for_Grounding_IT_Project_Management_in_Unstructured_Convers.pdf">DevNous: An LLM-Based Multi-Agent System for Grounding IT Project Management in Unstructured Conversation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">International Hellenic University</span></p><p>本论文提出了DevNous，一个基于大型语言模型（LLM）的分层多智能体专家系统，专为将IT项目团队中非结构化对话自动翻译为结构化项目管理工件而设计。系统集成于团队聊天环境，通过专用子智能体实现意图识别、任务自动结构化与进展摘要生成，并引入了160轮人机交互合成对话的公开基准数据集。实验证明，DevNous在该新基准上取得了81.3%的turn-level准确率和0.845的多标签F1分数，显著优于单体式基线，验证了多智能体架构和方法的有效性和可行性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DevNous_An_LLM-Based_Multi-Agent_System_for_Grounding_IT_Project_Management_in_Unstructured_Conversation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-IROTE-Human-like-Traits-Elicitation-of-Large-Language-Model-via-In-Context-Self-Reflective-Optimization"><a href="#92-IROTE-Human-like-Traits-Elicitation-of-Large-Language-Model-via-In-Context-Self-Reflective-Optimization" class="headerlink" title="92. IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/IROTE__Human-like_Traits_Elicitation_of_Large_Language_Model_via_In-Context_Self-Reflective_Optimiza.pdf">IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出IROTE，一种基于心理学自我反思理论的无微调大语言模型（LLM）人类特质诱导方法，通过信息瓶颈式目标优化生成文本自我反思，并将其嵌入任务提示中，稳定激发LLM展现特定人类特质。实验结果表明，IROTE在多种主流人类特质体系和复杂任务下均优于现有基线，能显著提升LLM在问卷与下游任务中的特质稳定性与可迁移性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/IROTE_Human-like_Traits_Elicitation_of_Large_Language_Model_via_In-Context_Self-Reflective_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-STELAR-VISION-Self-Topology-Aware-Efficient-Learning-for-Aligned-Reasoning-in-Vision"><a href="#93-STELAR-VISION-Self-Topology-Aware-Efficient-Learning-for-Aligned-Reasoning-in-Vision" class="headerlink" title="93. STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/STELAR-VISION__Self-Topology-Aware_Efficient_Learning_for_Aligned_Reasoning_in_Vision.pdf">STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>本文提出了STELAR-Vision训练框架，通过TopoAug自动生成多样化链式、树式和图式推理拓扑结构，并利用监督微调和强化学习提升视觉-语言模型（VLMs）的多模态推理能力和输出效率。实验结果显示，STELAR-Vision在多个视觉推理基准上显著提升准确率（对基线提升9.7%，对更大模型提升7.3%），并通过Frugal Learning变体在保证准确率的同时将输出长度缩短18.1%，在五个分布外数据集上表现出强泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/STELAR-VISION_Self-Topology-Aware_Efficient_Learning_for_Aligned_Reasoning_in_Vision.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-Imposing-AI-Deceptive-design-patterns-against-sustainability"><a href="#94-Imposing-AI-Deceptive-design-patterns-against-sustainability" class="headerlink" title="94. Imposing AI: Deceptive design patterns against sustainability"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Imposing_AI__Deceptive_design_patterns_against_sustainability.pdf">Imposing AI: Deceptive design patterns against sustainability</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Université de Strasbourg</span></p><p>本论文通过收集和分析多家主流数字平台在2024年引入生成式AI功能的界面设计变化，揭示了两类主要的欺骗性设计策略：一是以牺牲原有非AI功能为代价强制推广AI特性，二是通过“魔法”与“助手”隐喻等叙事手法美化并掩盖AI的实际环境影响。结论指出，这些策略不仅降低了用户自主选择权，还加剧了数字基础设施的环境负担，亟需通过“欺骗性设计模式”视角推动相关监管与可持续设计。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Imposing_AI_Deceptive_design_patterns_against_sustainability.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-LLM-driven-Text-to-Table-Generation-through-Sub-Tasks-Guidance-and-Iterative-Refinement"><a href="#95-LLM-driven-Text-to-Table-Generation-through-Sub-Tasks-Guidance-and-Iterative-Refinement" class="headerlink" title="95. LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/LLM_driven_Text-to-Table_Generation_through_Sub-Tasks_Guidance_and_Iterative_Refinement.pdf">LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IBM Research</span></p><p>本文提出了一种LLM驱动的文本到表格生成系统，采用两个关键策略：一是将复杂任务分解为中间子任务（如表头解释、缩写扩展、数据格式解析和实体抽取），二是通过LLM自反馈对生成表格进行逐步细化和迭代优化。实验证明，子任务引导和细粒度的单元格级自反馈显著提升了表格生成的准确性，在Rotowire和LiveSum两个复杂数据集上实现了当前最优性能，同时分析了性能提升与计算成本之间的权衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/LLM_driven_Text-to-Table_Generation_through_Sub-Tasks_Guidance_and_Iterative_Refinement.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-Classifier-Language-Models-Unifying-Sparse-Finetuning-and-Adaptive-Tokenization-for-Specialized-Classification-Tasks"><a href="#96-Classifier-Language-Models-Unifying-Sparse-Finetuning-and-Adaptive-Tokenization-for-Specialized-Classification-Tasks" class="headerlink" title="96. Classifier Language Models: Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Classifier_Language_Models__Unifying_Sparse_Finetuning_and_Adaptive_Tokenization_for_Specialized_Cla.pdf">Classifier Language Models: Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amazon Web Services Inc.</span></p><p>本文提出了一种结合稀疏微调与自适应分词的新方法（AdaPT），用于高效适应小型预训练语言模型到特定领域分类任务。该方法通过挖掘任务特定的连续和非连续分词序列，扩展词表并仅微调对任务最敏感的模型参数，不引入新参数。结果表明，该方法在五个语义分类任务上稳定性更高、准确率更优且训练成本减半，优于端到端微调、LoRA、层选择和前缀微调等主流方法，尤其在需行业知识的专门任务中表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Classifier_Language_Models_Unifying_Sparse_Finetuning_and_Adaptive_Tokenization_for_Specialized_Classification_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-AgriGPT-a-Large-Language-Model-Ecosystem-for-Agriculture"><a href="#97-AgriGPT-a-Large-Language-Model-Ecosystem-for-Agriculture" class="headerlink" title="97. AgriGPT: a Large Language Model Ecosystem for Agriculture"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/AgriGPT__a_Large_Language_Model_Ecosystem_for_Agriculture.pdf">AgriGPT: a Large Language Model Ecosystem for Agriculture</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">College of Computer Science and Technology, Zhejiang University</span></p><p>本论文提出了AgriGPT，一个专为农业领域设计的大语言模型生态系统。方法包括多智能体数据引擎构建Agri-342K高质量指令数据集、持续预训练与有监督微调，并创新性地采用三通道检索增强生成（Tri-RAG）框架结合稠密&#x2F;稀疏检索与多跳知识图谱推理，显著提升事实性与推理能力。结论表明，AgriGPT在农业领域适应性和推理能力均超越通用大模型，同时具备良好通用性和多语言迁移能力，有助于推动农业AI普惠应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/AgriGPT_a_Large_Language_Model_Ecosystem_for_Agriculture.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-QoE-Aware-Service-Provision-for-Mobile-AR-Rendering-An-Agent-Driven-Approach"><a href="#98-QoE-Aware-Service-Provision-for-Mobile-AR-Rendering-An-Agent-Driven-Approach" class="headerlink" title="98. QoE-Aware Service Provision for Mobile AR Rendering: An Agent-Driven Approach"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/QoE-Aware_Service_Provision_for_Mobile_AR_Rendering__An_Agent-Driven_Approach.pdf">QoE-Aware Service Provision for Mobile AR Rendering: An Agent-Driven Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xidian University</span></p><p>该论文提出了一种基于大语言模型(LLM)的智能代理，实现移动增强现实(MAR)服务端与网络端的跨域协同，提升边缘辅助AR渲染体验。方法包括通过代理和API工具链进行用户特定QoE建模，以及结合Kalman滤波实现自适应资源管理。实验结果表明，该方法在用户QoE建模精度和带宽利用效率方面均优于传统方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/QoE-Aware_Service_Provision_for_Mobile_AR_Rendering_An_Agent-Driven_Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-DepressLLM-Interpretable-domain-adapted-language-model-for-depression-detection-from-real-world-narratives"><a href="#99-DepressLLM-Interpretable-domain-adapted-language-model-for-depression-detection-from-real-world-narratives" class="headerlink" title="99. DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DepressLLM__Interpretable_domain-adapted_language_model_for_depression_detection_from_real-world_nar.pdf">DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Electronics and Telecommunications Research Institute</span></p><p>该论文提出了DepressLLM，一种基于大规模语言模型的抑郁症检测方法，通过在包含幸福与痛苦叙述的真实患者语料上微调主流LLM，并引入SToPS（Score-guided Token Probability Summation）模块，实现了可解释性预测及置信度估算。实验结果表明，DepressLLM在多种异构数据集上表现出优越的分类性能和鲁棒性，且高置信度输出与临床专家判断高度一致，推动了精神健康AI的早期筛查应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DepressLLM_Interpretable_domain-adapted_language_model_for_depression_detection_from_real-world_narratives.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-DocThinker-Explainable-Multimodal-Large-Language-Models-with-Rule-based-Reinforcement-Learning-for-Document-Understanding"><a href="#100-DocThinker-Explainable-Multimodal-Large-Language-Models-with-Rule-based-Reinforcement-Learning-for-Document-Understanding" class="headerlink" title="100. DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DocThinker__Explainable_Multimodal_Large_Language_Models_with_Rule-based_Reinforcement_Learning_for_.pdf">DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huazhong University of Science and Technology</span></p><p>该论文提出DocThinker，一种基于规则的强化学习(RL)框架，用于多模态大语言模型(MLLM)在文档理解中的推理与可解释性提升。方法通过Group Relative Policy Optimization (GRPO)和多目标奖励机制，动态优化推理策略，生成结构化推理过程、改写问题、关键区域定位和最终答案，有效改善模型的泛化能力和输出的可解释性。实验结果表明DocThinker在多个文档理解与多模态推理任务上优于传统SFT和CoT模板方法，显著提高了模型的透明性、适应性和任务表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DocThinker_Explainable_Multimodal_Large_Language_Models_with_Rule-based_Reinforcement_Learning_for_Document_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="101-LLM-Driven-Adaptive-6G-Ready-Wireless-Body-Area-Networks-Survey-and-Framework"><a href="#101-LLM-Driven-Adaptive-6G-Ready-Wireless-Body-Area-Networks-Survey-and-Framework" class="headerlink" title="101. LLM-Driven Adaptive 6G-Ready Wireless Body Area Networks: Survey and Framework"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/LLM-Driven_Adaptive_6G-Ready_Wireless_Body_Area_Networks__Survey_and_Framework.pdf">LLM-Driven Adaptive 6G-Ready Wireless Body Area Networks: Survey and Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Nebraska–Lincoln</span></p><p>本文提出了一种以大语言模型（LLM）为认知控制平面的自适应无线体域网（WBAN）框架，能够实时协调路由、物理层选择、微能量采集和后量子安全等多项关键功能。研究通过系统综述分析现有WBAN架构、路由策略与安全机制，指出当前碎片化、静态化等不足，提出LLM驱动的统一自优化方案，在6G和医疗健康场景下实现高可靠、低功耗及量子安全的数据传输。结论认为该方法可显著提升WBAN的智能性、自适应性和安全性，为下一代移动健康系统奠定基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/LLM-Driven_Adaptive_6G-Ready_Wireless_Body_Area_Networks_Survey_and_Framework.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-In-Context-Learning-as-Nonparametric-Conditional-Probability-Estimation-Risk-Bounds-and-Optimality"><a href="#102-In-Context-Learning-as-Nonparametric-Conditional-Probability-Estimation-Risk-Bounds-and-Optimality" class="headerlink" title="102. In-Context Learning as Nonparametric Conditional Probability Estimation: Risk Bounds and Optimality"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/In-Context_Learning_as_Nonparametric_Conditional_Probability_Estimation__Risk_Bounds_and_Optimality.pdf">In-Context Learning as Nonparametric Conditional Probability Estimation: Risk Bounds and Optimality</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Department of Statistics, Beijing Normal University at Zhuhai</span></p><p>本文将In-Context Learning（ICL）建模为非参数条件概率估计问题，提出了一种基于截断Kullback-Leibler (KL) 散度的期望超额风险度量，并针对多分类任务推导了Transformer模型下ICL的精确上、下界。方法上，作者提出新颖的oracle不等式，利用对数似然函数类的统一经验覆盖熵控制泛化误差，并证明多层感知机（MLP）在特定假设下同样能以最优速率实现ICL。结论显示，Transformer和MLP均可在多分类ICL任务中达到统计最优的最小极大收敛速率（只差对数因子），并揭示ICL并非Transformer独有，丰富了大模型ICL理论基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/In-Context_Learning_as_Nonparametric_Conditional_Probability_Estimation_Risk_Bounds_and_Optimality.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-Aryabhata-An-exam-focused-language-model-for-JEE-Math"><a href="#103-Aryabhata-An-exam-focused-language-model-for-JEE-Math" class="headerlink" title="103. Aryabhata: An exam-focused language model for JEE Math"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Aryabhata__An_exam-focused_language_model_for_JEE_Math.pdf">Aryabhata: An exam-focused language model for JEE Math</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">PhysicsWallah</span></p><p>Aryabhata 1.0 is a compact 7B parameter language model specialized for mathematical reasoning in Indian JEE exams, built by merging open-source reasoning models and fine-tuned with curriculum learning on verified chain-of-thought traces. Using supervised fine-tuning with rejection sampling and reinforcement learning with verifiable rewards (A2C), Aryabhata achieves state-of-the-art accuracy on JEE and competitive results on MATH&#x2F;GSM8K, with efficient step-by-step reasoning tailored for educational use.</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Aryabhata_An_exam-focused_language_model_for_JEE_Math.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-M2LLM-Multi-view-Molecular-Representation-Learning-with-Large-Language-Models"><a href="#104-M2LLM-Multi-view-Molecular-Representation-Learning-with-Large-Language-Models" class="headerlink" title="104. M2LLM: Multi-view Molecular Representation Learning with Large Language Models"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/$_text%7BM%7D^%7B2%7D$LLM__Multi-view_Molecular_Representation_Learning_with_Large_Language_Models.pdf">M2LLM: Multi-view Molecular Representation Learning with Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Griffith University</span></p><p>该论文提出M2LLM框架，创新性地融合了分子结构视角、任务视角和规则视角，利用大语言模型（LLM）进行分子表征生成和特征提取，并通过动态多视图融合优化表示，提升分子性质预测准确率。实验表明M2LLM在多项分子属性分类和回归任务上超越现有SOTA方法，展示了LLM在分子表示学习领域的巨大潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/M2LLM_Multi-view_Molecular_Representation_Learning_with_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-Prompt-and-Check-Using-Large-Language-Models-to-Evaluate-Communication-Protocol-Compliance-in-Simulation-Based-Training"><a href="#105-Prompt-and-Check-Using-Large-Language-Models-to-Evaluate-Communication-Protocol-Compliance-in-Simulation-Based-Training" class="headerlink" title="105. Prompt-and-Check: Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Prompt-and-Check__Using_Large_Language_Models_to_Evaluate_Communication_Protocol_Compliance_in_Simul.pdf">Prompt-and-Check: Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Singapore Polytechnic</span></p><p>本文提出Prompt-and-Check方法，利用开源大语言模型（如LLaMA 2&#x2F;3、Mistral 7B）结合语境选择和结构化提示，自动评估仿真训练中交流协议的合规性。实验证明，该方法在本地硬件上无需特定微调即可实现高准确率和合理解释，适用于海事等安全关键领域的自动化评估。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Prompt-and-Check_Using_Large_Language_Models_to_Evaluate_Communication_Protocol_Compliance_in_Simulation-Based_Training.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-MIGRATE-Mixed-Policy-GRPO-for-Adaptation-at-Test-Time"><a href="#106-MIGRATE-Mixed-Policy-GRPO-for-Adaptation-at-Test-Time" class="headerlink" title="106. MIGRATE: Mixed-Policy GRPO for Adaptation at Test-Time"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MiGrATe__Mixed-Policy_GRPO_for_Adaptation_at_Test-Time.pdf">MIGRATE: Mixed-Policy GRPO for Adaptation at Test-Time</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Massachusetts Amherst</span></p><p>该论文提出MIGRATE方法，通过将Group Relative Policy Optimization（GRPO）与混合策略组（包括on-policy采样、贪心采样和邻域采样）结合，实现了无需外部训练数据的大模型在线测试时自适应搜索优化。实验证明MIGRATE在单词搜索、分子优化和抽象推理（ARC）等三大领域均显著优于现有推理和测试时训练基线，展示出在无监督复杂搜索任务中的卓越性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MIGRATE_Mixed-Policy_GRPO_for_Adaptation_at_Test-Time.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="107-Securing-Educational-LLMs-A-Generalised-Taxonomy-of-Attacks-on-LLMs-and-DREAD-Risk-Assessment"><a href="#107-Securing-Educational-LLMs-A-Generalised-Taxonomy-of-Attacks-on-LLMs-and-DREAD-Risk-Assessment" class="headerlink" title="107. Securing Educational LLMs: A Generalised Taxonomy of Attacks on LLMs and DREAD Risk Assessment"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Securing_Educational_LLMs__A_Generalised_Taxonomy_of_Attacks_on_LLMs_and_DREAD_Risk_Assessment.pdf">Securing Educational LLMs: A Generalised Taxonomy of Attacks on LLMs and DREAD Risk Assessment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Waikato</span></p><p>本文系统梳理了大型语言模型（LLM）在教育领域应用时面临的安全威胁，提出了一套基于攻击复杂度的通用攻击分类法，并结合DREAD风险评估模型，量化分析了50种针对LLM及其基础设施的攻击手法在教育场景下的危害性。研究发现，令牌走私、对抗性提示、直接注入和多步越狱等攻击为关键高危威胁，并提出了风险缓解建议，结论对学术和产业界构建教育安全LLM系统具有指导意义。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Securing_Educational_LLMs_A_Generalised_Taxonomy_of_Attacks_on_LLMs_and_DREAD_Risk_Assessment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-Generative-AI-for-Critical-Infrastructure-in-Smart-Grids-A-Unified-Framework-for-Synthetic-Data-Generation-and-Anomaly-Detection"><a href="#108-Generative-AI-for-Critical-Infrastructure-in-Smart-Grids-A-Unified-Framework-for-Synthetic-Data-Generation-and-Anomaly-Detection" class="headerlink" title="108. Generative AI for Critical Infrastructure in Smart Grids: A Unified Framework for Synthetic Data Generation and Anomaly Detection"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Generative_AI_for_Critical_Infrastructure_in_Smart_Grids__A_Unified_Framework_for_Synthetic_Data_Gen.pdf">Generative AI for Critical Infrastructure in Smart Grids: A Unified Framework for Synthetic Data Generation and Anomaly Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Michigan-Dearborn</span></p><p>本论文提出了一种面向数字变电站IEC61850协议GOOSE消息的生成式AI（GenAI）异常检测系统，核心包括AATM（高级对抗性流量变异）技术用于协议合规的合成数据生成，实现了高真实度和良好类别平衡的零日攻击样本，并利用基于任务导向对话（ToD）的GenAI框架进行异常检测。实验表明，所提GenAI-ADS（以Anthropic Claude Pro实现）在准确率、精度、F1等多项指标上明显优于传统机器学习模型且无需频繁重训练，对新型攻击有更强检测能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Generative_AI_for_Critical_Infrastructure_in_Smart_Grids_A_Unified_Framework_for_Synthetic_Data_Generation_and_Anomaly_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-OmniLLP-Enhancing-LLM-based-Log-Level-Prediction-with-Context-Aware-Retrieval"><a href="#109-OmniLLP-Enhancing-LLM-based-Log-Level-Prediction-with-Context-Aware-Retrieval" class="headerlink" title="109. OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/OmniLLP__Enhancing_LLM-based_Log_Level_Prediction_with_Context-Aware_Retrieval.pdf">OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Queen’s University</span></p><p>OmniLLP提出了一种利用语义和开发者归属聚类的上下文检索框架，通过多层聚类（语义+所有权）为大型语言模型（LLMs）提供更相关的in-context示例，实现了高效的日志级别自动预测。实验结果表明，OmniLLP在四个大型开源项目上将AUC提升至0.88-0.96，显著优于随机与单一聚类方法，提高了预测准确性和实际开发中的可用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/OmniLLP_Enhancing_LLM-based_Log_Level_Prediction_with_Context-Aware_Retrieval.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="110-Towards-Universal-Neural-Inference"><a href="#110-Towards-Universal-Neural-Inference" class="headerlink" title="110. Towards Universal Neural Inference"></a>110. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Towards_Universal_Neural_Inference.pdf">Towards Universal Neural Inference</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Department of Computer Science, UNC Chapel Hill</span></p><p>本文提出ASPIRE（Arbitrary Set-based Permutation-Invariant Reasoning Engine），一种结合集合变换器和语义嵌入的通用神经推理框架，能够在异构结构化数据上进行语义对齐和泛化预测。ASPIRE通过集合不变性结构和自然语言特征描述，实现跨数据集、跨领域的零样本、少样本泛化推断，在多个表格数据基准上取得领先性能，并支持主动特征获取任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Towards_Universal_Neural_Inference.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="111-Constrained-free-energy-minimization-for-the-design-of-thermal-states-and-stabilizer-thermodynamic-systems"><a href="#111-Constrained-free-energy-minimization-for-the-design-of-thermal-states-and-stabilizer-thermodynamic-systems" class="headerlink" title="111. Constrained free energy minimization for the design of thermal states and stabilizer thermodynamic systems"></a>111. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Constrained_free_energy_minimization_for_the_design_of_thermal_states_and_stabilizer_thermodynamic_s.pdf">Constrained free energy minimization for the design of thermal states and stabilizer thermodynamic systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">École Polytechnique Fédérale de Lausanne (EPFL)</span></p><p>本文系统研究了LMPW25算法（包括一阶&#x2F;二阶经典与量子-经典混合算法）在量子热力学中受约束能量最小化问题上的数值表现，涵盖了量子Heisenberg模型和基于稳定子码的热力学系统。论文提出将这些算法用于可控Hamiltonian的基态和热态设计，并首次将量子纠错中的稳定子码与热力学中的非对易守恒量建立桥梁，实现了在热力学系统中编码量子信息的新方法。结论表明，LMPW25算法在上述系统中均能有效收敛，理论与数值结果一致，并为材料分子设计及量子信息编码提供了新工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Constrained_free_energy_minimization_for_the_design_of_thermal_states_and_stabilizer_thermodynamic_systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="112-Fre-CW-Targeted-Attack-on-Time-Series-Forecasting-using-Frequency-Domain-Loss"><a href="#112-Fre-CW-Targeted-Attack-on-Time-Series-Forecasting-using-Frequency-Domain-Loss" class="headerlink" title="112. Fre-CW: Targeted Attack on Time Series Forecasting using Frequency Domain Loss"></a>112. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Fre-CW__Targeted_Attack_on_Time_Series_Forecasting_using_Frequency_Domain_Loss.pdf">Fre-CW: Targeted Attack on Time Series Forecasting using Frequency Domain Loss</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiaotong University</span></p><p>本文提出了一种基于频域损失的时间序列预测目标攻击方法Fre-CW，通过改进C&amp;W攻击算法，将其引入时间序列预测领域，并融合时间域与频域损失优化对抗样本。实验结果表明，Fre-CW在多个数据集和主流预测模型上均显著提升了目标攻击的有效性和隐蔽性，验证了频域信息在提升攻击性能中的重要作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Fre-CW_Targeted_Attack_on_Time_Series_Forecasting_using_Frequency_Domain_Loss.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="113-FetFIDS-A-Feature-Embedding-Attention-based-Federated-Network-Intrusion-Detection-Algorithm"><a href="#113-FetFIDS-A-Feature-Embedding-Attention-based-Federated-Network-Intrusion-Detection-Algorithm" class="headerlink" title="113. FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm"></a>113. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/FetFIDS__A_Feature_Embedding_Attention_based_Federated_Network_Intrusion_Detection_Algorithm.pdf">FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Purdue University</span></p><p>该论文提出了一种特征嵌入与顺序多头注意力机制结合的Transformer模型，并在联邦学习环境下提升网络入侵检测性能。方法采用CNN特征嵌入替代传统位置嵌入，并通过多轮模型聚合与节点本地训练实现隐私保护和性能提升。实验结果表明，该方法在准确率、精确率、召回率及F1分数上均优于当前主流入侵检测算法，且模型稳定性强，适合实际部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/FetFIDS_A_Feature_Embedding_Attention_based_Federated_Network_Intrusion_Detection_Algorithm.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="114-Wavelet-Mixture-of-Experts-for-Time-Series-Forecasting"><a href="#114-Wavelet-Mixture-of-Experts-for-Time-Series-Forecasting" class="headerlink" title="114. Wavelet Mixture of Experts for Time Series Forecasting"></a>114. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Wavelet_Mixture_of_Experts_for_Time_Series_Forecasting.pdf">Wavelet Mixture of Experts for Time Series Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai University of Engineering Science</span></p><p>该论文提出了WaveTS系列时间序列预测模型，结合离散小波变换与MLP以分解时序数据的高低频特征，并通过MoE（Mixture of Experts）通道聚类策略高效处理多通道依赖。实验结果表明，WaveTS系列在八个公开数据集上以更少参数实现了SOTA预测性能，尤其在多通道数据集上表现突出，提升了准确率和计算效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Wavelet_Mixture_of_Experts_for_Time_Series_Forecasting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="115-Differentiated-Information-Mining-A-Semi-supervised-Learning-Framework-for-GNNs"><a href="#115-Differentiated-Information-Mining-A-Semi-supervised-Learning-Framework-for-GNNs" class="headerlink" title="115. Differentiated Information Mining: A Semi-supervised Learning Framework for GNNs"></a>115. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Differentiated_Information_Mining__A_Semi-supervised_Learning_Framework_for_GNNs.pdf">Differentiated Information Mining: A Semi-supervised Learning Framework for GNNs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了DiFac框架，通过从单一信息源中挖掘多种区分性决策因子，并利用一致性过滤和最小置信度排名策略，在半监督场景下提升图神经网络（GNN）泛化能力和鲁棒性。同时引入大规模多模态模型生成的辅助文本知识作为因子，并设计问责评分机制以抑制噪声。实验表明，该方法在低标注率和高噪声条件下显著优于主流基线，尤其在特征维度较高的数据集上效果突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Differentiated_Information_Mining_A_Semi-supervised_Learning_Framework_for_GNNs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="116-Elucidating-Rectified-Flow-with-Deterministic-Sampler-Polynomial-Discretization-Complexity-for-Multi-and-One-step-Models"><a href="#116-Elucidating-Rectified-Flow-with-Deterministic-Sampler-Polynomial-Discretization-Complexity-for-Multi-and-One-step-Models" class="headerlink" title="116. Elucidating Rectified Flow with Deterministic Sampler: Polynomial Discretization Complexity for Multi and One-step Models"></a>116. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Elucidating_Rectified_Flow_with_Deterministic_Sampler__Polynomial_Discretization_Complexity_for_Mult.pdf">Elucidating Rectified Flow with Deterministic Sampler: Polynomial Discretization Complexity for Multi and One-step Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了对Rectified Flow (RF) 基于确定性采样器的多步和一步生成模型的离散化复杂性理论分析，首次在真实的有界支持假设下证明了RF模型可达多项式级别的离散化复杂度。方法上结合了预测-校正框架和Langevin过程作为校正器，并提出了向量扰动引理，理论上优于主流扩散模型。结果显示，无论在多步还是一步生成场景，RF模型在采样复杂性上都优于VP、VE等扩散模型，为RF模型优异的经验表现提供了理论解释。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Elucidating_Rectified_Flow_with_Deterministic_Sampler_Polynomial_Discretization_Complexity_for_Multi_and_One-step_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="117-Diminution-On-Reducing-the-Size-of-Grounding-ASP-Programs"><a href="#117-Diminution-On-Reducing-the-Size-of-Grounding-ASP-Programs" class="headerlink" title="117. Diminution: On Reducing the Size of Grounding ASP Programs"></a>117. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Diminution__On_Reducing_the_Size_of_Grounding_ASP_Programs.pdf">Diminution: On Reducing the Size of Grounding ASP Programs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China (USTC)</span></p><p>本文提出了“diminution”概念，即在ASP程序归约前选取Herbrand宇宙的子集以生成更小的ground程序，并形式化定义了diminution、分析了其性质和复杂性，并提出基于域谓词的实现方式。实验表明，该方法平均可减少高达70%的grounding时间、85%的ground文件大小，同时保持原程序解的可扩展性，有效缓解了ASP中的grounding瓶颈。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Diminution_On_Reducing_the_Size_of_Grounding_ASP_Programs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025/08/13报纸</title>
    <link href="/2025/08/2025-08-13_article/"/>
    <url>/2025/08/2025-08-13_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2025/08/13报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-13<br>📄 <strong>发现论文数量</strong>：300  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-From-Imitation-to-Optimization-A-Comparative-Study-of-Offline-Learning-for-Autonomous-Driving"><a href="#1-From-Imitation-to-Optimization-A-Comparative-Study-of-Offline-Learning-for-Autonomous-Driving" class="headerlink" title="1. From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Imitation_to_Optimization__A_Comparative_Study_of_Offline_Learning_for_Autonomous_Driving.pdf">From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent Researcher</span></p><p>本文构建了一个完整的离线学习流水线，系统比较了基于模仿学习（Behavioral Cloning, BC）与保守Q学习（Conservative Q-Learning, CQL）在自动驾驶任务中的表现。通过在Waymo Open Motion Dataset大规模数据集上实验，作者发现即使使用结构化状态表示和Transformer模型，BC方法在长时序任务中因累计误差表现不佳，而CQL通过值函数的保守正则化显著提升了鲁棒性和成功率，碰撞率大幅下降，证明了离线强化学习对于自动驾驶策略学习的关键性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/From_Imitation_to_Optimization_A_Comparative_Study_of_Offline_Learning_for_Autonomous_Driving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Consensus-based-Decentralized-Multi-agent-Reinforcement-Learning-for-Random-Access-Network-Optimization"><a href="#2-Consensus-based-Decentralized-Multi-agent-Reinforcement-Learning-for-Random-Access-Network-Optimization" class="headerlink" title="2. Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Consensus-based_Decentralized_Multi-agent_Reinforcement_Learning_for_Random_Access_Network_Optimizat.pdf">Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Ohio State University</span></p><p>本文提出了一种基于共识机制的全分布式多智能体强化学习（MARL）方法，用于无线随机接入（RA）网络的MAC层优化。该方法在actor-critic结构下，仅通过本地奖励的局部信息交换实现全局收敛，显著减少通信开销，并在理论上证明了有限步收敛性。实验结果表明，该算法在无中心化训练的情况下，实现了与集中训练-分布式执行（CTDE）方法相当的网络吞吐量和公平性，同时大幅降低了系统开销。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Consensus-based_Decentralized_Multi-agent_Reinforcement_Learning_for_Random_Access_Network_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Simulating-Biological-Intelligence-Active-Inference-with-Experiment-Informed-Generative-Model"><a href="#3-Simulating-Biological-Intelligence-Active-Inference-with-Experiment-Informed-Generative-Model" class="headerlink" title="3. Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Simulating_Biological_Intelligence__Active_Inference_with_Experiment-Informed_Generative_Model.pdf">Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Turner Institute for Brain and Mental Health, School of Psychological Sciences, Monash University</span></p><p>本论文提出了一种基于主动推断（Active Inference）的框架，利用实验启发的生成模型，模拟生物神经网络在Pong游戏环境下的决策行为。通过对不同决策机制（包括记忆驱动的反事实学习与基于规划的主动推断）的系统仿真，结果表明具备记忆机制的主动推断智能体在动态环境中的学习效率和适应性优于基于规划的方法，强调记忆在生物决策中的关键作用，并为可解释和生物启发AI的设计提供理论基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Simulating_Biological_Intelligence_Active_Inference_with_Experiment-Informed_Generative_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-AR-GRPO-Training-Autoregressive-Image-Generation-Models-via-Reinforcement-Learning"><a href="#4-AR-GRPO-Training-Autoregressive-Image-Generation-Models-via-Reinforcement-Learning" class="headerlink" title="4. AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AR-GRPO__Training_Autoregressive_Image_Generation_Models_via_Reinforcement_Learning.pdf">AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Technology</span></p><p>本论文提出AR-GRPO方法，将在线强化学习（RL）训练集成到自回归（AR）图像生成模型中，通过引入Group Relative Policy Optimization（GRPO）算法，并设计多维奖励函数（包括文本-图像语义一致性、图像质量和真实感）来优化生成结果。实验结果表明，在类别条件和文本条件的图像生成任务中，RL优化显著提升了生成图像的质量和人类偏好，且可在不同模型规模和分辨率下有效泛化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AR-GRPO_Training_Autoregressive_Image_Generation_Models_via_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-Pushdown-Reward-Machines-for-Reinforcement-Learning"><a href="#5-Pushdown-Reward-Machines-for-Reinforcement-Learning" class="headerlink" title="5. Pushdown Reward Machines for Reinforcement Learning"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pushdown_Reward_Machines_for_Reinforcement_Learning.pdf">Pushdown Reward Machines for Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Utrecht Universiteit</span></p><p>该论文提出了Pushdown Reward Machines（pdRMs），这是基于确定性下推自动机的奖励机器扩展，可以编码由确定性上下文无关语言描述的时序任务。作者设计了两种pdRM策略（全栈访问和Top-k访问）、提出了最优性判别方法，并分析了表达能力与空间复杂度。实验表明，pdRMs在多个RL环境下样本效率高于计数奖励自动机（CRA），尤其是在任务可用上下文无关语言表达时，且Top-k策略能显著减小策略空间。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Pushdown_Reward_Machines_for_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-Offline-to-Online-Reinforcement-Learning-with-Classifier-Free-Diffusion-Generation"><a href="#6-Offline-to-Online-Reinforcement-Learning-with-Classifier-Free-Diffusion-Generation" class="headerlink" title="6. Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Offline-to-Online_Reinforcement_Learning_with_Classifier-Free_Diffusion_Generation.pdf">Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了一种新的数据增强方法：Classifier-Free Diffusion Generation (CFDG)，用于离线到在线强化学习（O2O RL）。方法利用无分类器引导的扩散模型对离线和在线数据进行高质量的联合生成，并通过重加权机制提升与在线策略一致的数据比例，提升训练稳定性和性能。实验证明，CFDG可无缝集成到主流O2O RL算法（如IQL、PEX、APL）中，在D4RL基准测试上平均提升15%性能，优于现有数据增强方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Offline-to-Online_Reinforcement_Learning_with_Classifier-Free_Diffusion_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Sparsity-Driven-Plasticity-in-Multi-Task-Reinforcement-Learning"><a href="#7-Sparsity-Driven-Plasticity-in-Multi-Task-Reinforcement-Learning" class="headerlink" title="7. Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sparsity-Driven_Plasticity_in_Multi-Task_Reinforcement_Learning.pdf">Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Groningen</span></p><p>该论文系统性研究了稀疏化方法（特别是逐步幅值剪枝GMP和稀疏进化训练SET）对多任务强化学习（MTRL）中神经网络适应性的提升作用。通过在MTRL主流架构和标准基准上实验，作者发现稀疏化显著缓解了神经元休眠和表征塌缩等塑性退化现象，通常带来更优的多任务表现，尤其在MTPPO和MoE架构下稀疏网络优于密集网络，并与显式塑性干预或常规正则化方法相比具有竞争力。结论认为，动态稀疏化是一种强大且依赖上下文的工具，不仅提升MTRL系统的适应性与泛化能力，还带来计算效率和隐式正则化优势，适用于开发更具备弹性的多任务强化学习系统。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Sparsity-Driven_Plasticity_in_Multi-Task_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-Multi-level-Advantage-Credit-Assignment-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#8-Multi-level-Advantage-Credit-Assignment-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="8. Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-level_Advantage_Credit_Assignment_for_Cooperative_Multi-Agent_Reinforcement_Learning.pdf">Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mila - Quebec AI Institute</span></p><p>本文提出Multi-level Advantage Credit Assignment (MACA) 方法，通过多层次优势函数显式建模多智能体协作中的多级信用分配问题。MACA结合了个体、联合及相关代理子集的优势函数，并利用自注意力机制动态识别代理间相关性，显著提升了在复杂协作任务中的表现。实验结果表明，MACA在StarCraft等多智能体强化学习基准任务中优于现有方法，理论分析也证明了其收敛性和有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multi-level_Advantage_Credit_Assignment_for_Cooperative_Multi-Agent_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-Natural-Language-Driven-Viewpoint-Navigation-for-Volume-Exploration-via-Semantic-Block-Representation"><a href="#9-Natural-Language-Driven-Viewpoint-Navigation-for-Volume-Exploration-via-Semantic-Block-Representation" class="headerlink" title="9. Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Natural_Language-Driven_Viewpoint_Navigation_for_Volume_Exploration_via_Semantic_Block_Representatio.pdf">Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-sen University</span></p><p>本论文提出了一种结合自然语言交互和语义分块表示的体数据自动视角导航框架。方法上，采用CLIP模型对体数据渲染图像与文本描述进行对齐，通过分块编码捕捉局部结构，并用强化学习（PPO）根据语义奖励动态选择最符合用户意图的视角。结论显示，该方法有效提升了体数据探索的效率和可解释性，能准确反映用户查询，实现复杂场景下的快速语义导航。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Natural_Language-Driven_Viewpoint_Navigation_for_Volume_Exploration_via_Semantic_Block_Representation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-Stackelberg-Coupling-of-Online-Representation-Learning-and-Reinforcement-Learning"><a href="#10-Stackelberg-Coupling-of-Online-Representation-Learning-and-Reinforcement-Learning" class="headerlink" title="10. Stackelberg Coupling of Online Representation Learning and Reinforcement Learning"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Stackelberg_Coupling_of_Online_Representation_Learning_and_Reinforcement_Learning.pdf">Stackelberg Coupling of Online Representation Learning and Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fordham University</span></p><p>本文提出了SCORER框架，将表示学习（感知网络）与强化学习（控制网络）的交互建模为Stackelberg博弈，通过两阶段梯度下降算法实现感知网络对控制网络的预期优化。实验结果表明SCORER能够显著提升强化学习的采样效率和最终性能，无需复杂的辅助目标或架构，展示了博弈论结构化感知-控制协同的优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Stackelberg_Coupling_of_Online_Representation_Learning_and_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-Efficient-Reward-Identification-In-Max-Entropy-Reinforcement-Learning-with-Sparsity-and-Rank-Priors"><a href="#11-Efficient-Reward-Identification-In-Max-Entropy-Reinforcement-Learning-with-Sparsity-and-Rank-Priors" class="headerlink" title="11. Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Efficient_Reward_Identification_In_Max_Entropy_Reinforcement_Learning_with_Sparsity_and_Rank_Priors.pdf">Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Michigan</span></p><p>该论文提出了两种高效的时间变化奖励函数恢复方法：一种通过贪心区间划分算法利用奖励稀疏性先验，另一种通过低秩矩阵核范数松弛恢复特征基奖励结构，均在最大熵强化学习框架下实现。实验结果显示，所提方法在奖励恢复精度和迁移泛化能力上优于现有方法，且对策略估计噪声有较强鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Efficient_Reward_Identification_In_Max_Entropy_Reinforcement_Learning_with_Sparsity_and_Rank_Priors.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-Pentest-R1-Towards-Autonomous-Penetration-Testing-Reasoning-Optimized-via-Two-Stage-Reinforcement-Learning"><a href="#12-Pentest-R1-Towards-Autonomous-Penetration-Testing-Reasoning-Optimized-via-Two-Stage-Reinforcement-Learning" class="headerlink" title="12. Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pentest-R1__Towards_Autonomous_Penetration_Testing_Reasoning_Optimized_via_Two-Stage_Reinforcement_L.pdf">Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of Cyberspace Security Defense, Institute of Information Engineering, Chinese Academy of Sciences</span></p><p>Pentest-R1提出了一种基于大型语言模型的自动化渗透测试框架，通过两阶段强化学习流程提升模型推理与自我纠错能力：第一阶段利用500余条真实多步攻击流程离线强化学习建立攻击逻辑，第二阶段在交互式CTF环境中通过在线RL优化模型的自适应策略与错误恢复。实验表明，该方法在AutoPenBench、Cybench等基准上表现优异，超过绝大多数开源及部分专有SOTA模型，验证了两阶段强化学习的协同效应。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Pentest-R1_Towards_Autonomous_Penetration_Testing_Reasoning_Optimized_via_Two-Stage_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-Policy-Newton-methods-for-Distortion-Riskmetrics"><a href="#13-Policy-Newton-methods-for-Distortion-Riskmetrics" class="headerlink" title="13. Policy Newton methods for Distortion Riskmetrics"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Policy_Newton_methods_for_Distortion_Riskmetrics.pdf">Policy Newton methods for Distortion Riskmetrics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indian Institute of Technology Madras</span></p><p>该论文提出了一种针对畸变风险度量（Distortion Riskmetric, DRM）的策略Newton方法（CRPN-DRM），在有限时域马尔可夫决策过程（MDP）中，通过最大化累计折扣奖励的DRM来求解风险敏感型最优策略。作者推导了DRM目标的策略Hessian定理，给出了基于样本轨迹的Hessian估计，并提出了带三次正则化的策略Newton算法，在理论上首次证明了其能以O(ϵ^-3.5)的样本复杂度收敛到风险敏感目标的ϵ-二阶驻点（SOSP），有效避免鞍点，实验验证了算法在经典强化学习环境中的优越性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Policy_Newton_methods_for_Distortion_Riskmetrics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Pref-GUIDE-Continual-Policy-Learning-from-Real-Time-Human-Feedback-via-Preference-Based-Learning"><a href="#14-Pref-GUIDE-Continual-Policy-Learning-from-Real-Time-Human-Feedback-via-Preference-Based-Learning" class="headerlink" title="14. Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pref-GUIDE__Continual_Policy_Learning_from_Real-Time_Human_Feedback_via_Preference-Based_Learning.pdf">Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Duke University</span></p><p>该论文提出Pref-GUIDE，通过将实时人类标注的标量反馈转化为局部偏好数据，实现更稳健的奖励模型学习以持续训练RL策略。Pref-GUIDE Individual利用移动窗口采样和无偏好区间，将个体反馈变为偏好标签，Pref-GUIDE Voting通过聚合多评估者奖励模型获得群体共识偏好。实验显示，在三个视觉RL任务中，Pref-GUIDE显著优于标量反馈基线，并在复杂任务中超过专家设计的稠密奖励，实现了高质量人类反馈的高效利用和反馈质量差异的鲁棒性提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Pref-GUIDE_Continual_Policy_Learning_from_Real-Time_Human_Feedback_via_Preference-Based_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-Reinforcement-Learning-in-Vision-A-Survey"><a href="#15-Reinforcement-Learning-in-Vision-A-Survey" class="headerlink" title="15. Reinforcement Learning in Vision: A Survey"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Reinforcement_Learning_in_Vision__A_Survey.pdf">Reinforcement Learning in Vision: A Survey</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>该论文系统综述了视觉领域强化学习（Visual RL）的最新进展，涵盖多模态大语言模型、视觉生成、统一模型和视觉-语言-动作（VLA）模型四大方向，详细梳理了政策优化、奖励建模和评测体系，并提出了RLHF、GRPO、RLVR等主流技术路线。结论指出视觉RL正向统一架构、可扩展奖励监督和丰富基准演进，但仍面临数据效率、泛化性和奖励设计等挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Reinforcement_Learning_in_Vision_A_Survey.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-ReconDreamer-RL-Enhancing-Reinforcement-Learning-via-Diffusion-based-Scene-Reconstruction"><a href="#16-ReconDreamer-RL-Enhancing-Reinforcement-Learning-via-Diffusion-based-Scene-Reconstruction" class="headerlink" title="16. ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ReconDreamer-RL__Enhancing_Reinforcement_Learning_via_Diffusion-based_Scene_Reconstruction.pdf">ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">GigaAI</span></p><p>该论文提出ReconDreamer-RL框架，通过集成视频扩散先验与场景重建，提升端到端自动驾驶的强化学习训练。方法包括ReconSimulator（结合扩散模型实现高保真环境和运动学建模）、动态对手代理（DAA，自动生成极端交通场景）以及Cousin Trajectory Generator（CTG，提升轨迹多样性），显著缩小仿真与现实差距。实验表明，该框架在端到端自动驾驶任务上将碰撞率降低5倍，提升模型在复杂场景下的泛化与鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ReconDreamer-RL_Enhancing_Reinforcement_Learning_via_Diffusion-based_Scene_Reconstruction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-Pose-RFT-Enhancing-MLLMs-for-3D-Pose-Generation-via-Hybrid-Action-Reinforcement-Fine-Tuning"><a href="#17-Pose-RFT-Enhancing-MLLMs-for-3D-Pose-Generation-via-Hybrid-Action-Reinforcement-Fine-Tuning" class="headerlink" title="17. Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pose-RFT__Enhancing_MLLMs_for_3D_Pose_Generation_via_Hybrid_Action_Reinforcement_Fine-Tuning.pdf">Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CASIA</span></p><p>该论文提出Pose-RFT，一种专为3D人体姿态生成设计的多模态大语言模型(MLLMs)强化微调框架。方法将3D姿态生成建模为混合动作空间的强化学习问题，提出HyGRPO算法，对离散语言生成和连续姿态参数进行联合优化，并结合空间&#x2F;语义对齐等任务奖励，显著提升文本和图像到3D姿态的生成质量。实验结果表明，该方法在多个基准上超过现有姿态MLLMs，强化微调有效提升3D姿态生成能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Pose-RFT_Enhancing_MLLMs_for_3D_Pose_Generation_via_Hybrid_Action_Reinforcement_Fine-Tuning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-A-Tutorial-An-Intuitive-Explanation-of-Offline-Reinforcement-Learning-Theory"><a href="#18-A-Tutorial-An-Intuitive-Explanation-of-Offline-Reinforcement-Learning-Theory" class="headerlink" title="18. A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Tutorial__An_Intuitive_Explanation_of_Offline_Reinforcement_Learning_Theory.pdf">A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Alberta</span></p><p>本论文系统梳理了离线强化学习（Offline RL）的理论基础，围绕函数表示、数据覆盖等核心假设，分析了样本复杂度的下界、上界及难点，提出了Bootstrapping偏差累积、泛化难题与虚假数据干扰等关键挑战。结论指出，当前具备多步回报（如λ-return）、悲观估计等技术且满足单策略覆盖与Bellman完备性条件下，离线RL能实现多项式样本效率，但仍需针对弱假设开发新算法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Tutorial_An_Intuitive_Explanation_of_Offline_Reinforcement_Learning_Theory.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Symmetry-Aware-Transformer-Training-for-Automated-Planning"><a href="#19-Symmetry-Aware-Transformer-Training-for-Automated-Planning" class="headerlink" title="19. Symmetry-Aware Transformer Training for Automated Planning"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Symmetry-Aware_Transformer_Training_for_Automated_Planning.pdf">Symmetry-Aware Transformer Training for Automated Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Linköping University</span></p><p>本文提出了一种对称感知的对比学习目标，结合架构改进，使Transformer模型能够显式处理自动规划任务中由于对象命名和状态目标排列引入的对称性。通过无位置编码的编码器、原子级嵌入和对比损失，模型在多个规划领域中展现出比现有PlanGPT方法更强的泛化和外推能力，尤其在未见过的大规模问题上表现更优，但在规模极大或结构复杂的任务上仍存在局限性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Symmetry-Aware_Transformer_Training_for_Automated_Planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-Robust-Reinforcement-Learning-over-Wireless-Networks-with-Homomorphic-State-Representations"><a href="#20-Robust-Reinforcement-Learning-over-Wireless-Networks-with-Homomorphic-State-Representations" class="headerlink" title="20. Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Robust_Reinforcement_Learning_over_Wireless_Networks_with_Homomorphic_State_Representations.pdf">Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Padova</span></p><p>本文提出了一种名为HR3L（Homomorphic Robust Remote Reinforcement Learning）的新框架，针对无线网络环境下远程强化学习（RRL）中因信道损失、延迟和带宽受限导致的观测不完全问题。HR3L采用基于马尔可夫决策过程同态（MDP Homomorphism）的状态表示，由发射端对环境状态进行特征压缩编码，接收端通过该紧凑表示进行策略学习，并采用异步模型更新以避免高通信和计算开销。实验表明，HR3L在多种网络受损情景下比现有方法更具鲁棒性和通信效率，显著减少带宽和延迟开销，同时保持甚至提升强化学习性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Robust_Reinforcement_Learning_over_Wireless_Networks_with_Homomorphic_State_Representations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-MORE-CLEAR-Multimodal-Offline-Reinforcement-learning-for-Clinical-notes-Leveraged-Enhanced-State-Representation"><a href="#21-MORE-CLEAR-Multimodal-Offline-Reinforcement-learning-for-Clinical-notes-Leveraged-Enhanced-State-Representation" class="headerlink" title="21. MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MORE-CLEAR__Multimodal_Offline_Reinforcement_learning_for_Clinical_notes_Leveraged_Enhanced_State_Re.pdf">MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University Hospital</span></p><p>本文提出了MORE-CLEAR框架，将结构化数据（如检验结果和生命体征）与由大规模语言模型（LLM）处理的临床文本笔记进行多模态融合，通过上下文感知门控融合和双向跨模态注意力机制，增强患者状态表达，并用于离线强化学习优化脓毒症治疗策略。实验在MIMIC-III、MIMIC-IV和私有数据集上表明，该方法在存活率估计和策略性能上显著优于单一模态RL方法，提升了策略的泛化与稳健性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MORE-CLEAR_Multimodal_Offline_Reinforcement_learning_for_Clinical_notes_Leveraged_Enhanced_State_Representation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="22-DexFruit-Dexterous-Manipulation-and-Gaussian-Splatting-Inspection-of-Fruit"><a href="#22-DexFruit-Dexterous-Manipulation-and-Gaussian-Splatting-Inspection-of-Fruit" class="headerlink" title="22. DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DexFruit__Dexterous_Manipulation_and_Gaussian_Splatting_Inspection_of_Fruit.pdf">DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University</span></p><p>本文提出了DexFruit框架，结合光学触觉传感和扩散式模仿学习策略，实现了机器人对草莓、西红柿和黑莓等易损水果的温和自主操作，显著降低损伤率。创新地引入FruitSplat方法，利用3D高斯Splatting技术对水果进行高精度三维重建和损伤定量分析，仅需摄像头即可实现可量化的外伤检测。实验结果显示DexFruit在抓取成功率和损伤最小化方面优于视觉或触觉单独策略，平均抓取成功率达92%，视觉损伤减少20%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DexFruit_Dexterous_Manipulation_and_Gaussian_Splatting_Inspection_of_Fruit.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-Communication-Efficient-Multi-Agent-3D-Detection-via-Hybrid-Collaboration"><a href="#23-Communication-Efficient-Multi-Agent-3D-Detection-via-Hybrid-Collaboration" class="headerlink" title="23. Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Communication-Efficient_Multi-Agent_3D_Detection_via_Hybrid_Collaboration.pdf">Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了HyComm，一个面向多智能体协作的通信高效LiDAR三维检测系统，核心方法为基于置信度和不确定性的混合协作消息打包（融合紧凑的感知输出和丰富的原始观测），并通过动态优先级选择关键数据实现自适应消息压缩。实验结果表明，HyComm在真实与仿真数据集上均显著优于以往方法，在极大降低通信量的同时提升检测性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Communication-Efficient_Multi-Agent_3D_Detection_via_Hybrid_Collaboration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-From-Data-to-Safe-Mobile-Robot-Navigation-An-Efficient-and-Modular-Robust-MPC-Design-Pipeline"><a href="#24-From-Data-to-Safe-Mobile-Robot-Navigation-An-Efficient-and-Modular-Robust-MPC-Design-Pipeline" class="headerlink" title="24. From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Data_to_Safe_Mobile_Robot_Navigation__An_Efficient_and_Modular_Robust_MPC_Design_Pipeline.pdf">From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Delft University of Technology</span></p><p>本文提出了一套高效且模块化的鲁棒模型预测控制（MPC）设计流程，结合闭环实验数据，通过迭代的移动地平线估计（MHE）方法自动量化非线性移动机器人模型的不确定性，并据此合成鲁棒的输出反馈MPC方案（ROHMPC）。实验证明，该流程能在存在结构性模型失配和测量噪声的情况下，保证机器人安全避障与约束满足，且递归可行，流程可复现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/From_Data_to_Safe_Mobile_Robot_Navigation_An_Efficient_and_Modular_Robust_MPC_Design_Pipeline.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-ForeSight-Multi-View-Streaming-Joint-Object-Detection-and-Trajectory-Forecasting"><a href="#25-ForeSight-Multi-View-Streaming-Joint-Object-Detection-and-Trajectory-Forecasting" class="headerlink" title="25. ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ForeSight__Multi-View_Streaming_Joint_Object_Detection_and_Trajectory_Forecasting.pdf">ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Toronto</span></p><p>该论文提出了ForeSight，一个面向自动驾驶的多视角联合3D目标检测与轨迹预测流式框架，采用统一Transformer架构和双向查询记忆机制，在检测与预测间无缝流转信息，消除追踪依赖。实验结果显示，ForeSight在nuScenes数据集上超越现有方法，提升检测与预测准确性和鲁棒性，特别适用于复杂动态驾驶场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ForeSight_Multi-View_Streaming_Joint_Object_Detection_and_Trajectory_Forecasting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-Model-Predictive-Control-for-Crowd-Navigation-via-Learning-Based-Trajectory-Prediction"><a href="#26-Model-Predictive-Control-for-Crowd-Navigation-via-Learning-Based-Trajectory-Prediction" class="headerlink" title="26. Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Model_Predictive_Control_for_Crowd_Navigation_via_Learning-Based_Trajectory_Prediction.pdf">Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bauhaus-Universitat Weimar</span></p><p>该论文提出将深度学习的Social-Implicit（SI）行人轨迹预测模型集成到模型预测控制（MPC）框架中，在真实机器人平台下实现对动态人群的安全导航。实验显示，SI-MPC系统在不同密度下显著提升了轨迹预测准确性、安全性和运动平滑性，并在闭环部署中展现了更为谨慎和适应性的导航行为，优于传统恒速模型。结论：SI-MPC方案有效增强了机器人在人群环境中的安全性和社会适应性，但高密度下需权衡效率与保守性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Model_Predictive_Control_for_Crowd_Navigation_via_Learning-Based_Trajectory_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-P3-Toward-Versatile-Embodied-Agents"><a href="#27-P3-Toward-Versatile-Embodied-Agents" class="headerlink" title="27. P3: Toward Versatile Embodied Agents"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/$_mathcal%7BP%7D^3$__Toward_Versatile_Embodied_Agents.pdf">P3: Toward Versatile Embodied Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southern University of Science and Technology</span></p><p>该论文提出了P3框架，通过统一的实时感知模块和动态多任务调度，实现主动环境感知、无反馈工具即插即用、以及基于优先级和依赖关系的多任务执行。实验表明，该框架显著提升了机器人在真实动态环境下的适应性和通用性，有效弥合了基准测试与实际部署的差距。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/P3_Toward_Versatile_Embodied_Agents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-EGS-SLAM-RGB-D-Gaussian-Splatting-SLAM-with-Events"><a href="#28-EGS-SLAM-RGB-D-Gaussian-Splatting-SLAM-with-Events" class="headerlink" title="28. EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EGS-SLAM__RGB-D_Gaussian_Splatting_SLAM_with_Events.pdf">EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>该论文提出了一种融合事件相机数据、RGB图像与深度信息的3D高斯喷溅SLAM新框架（EGS-SLAM），通过显式建模曝光期间的相机连续轨迹，实现运动模糊场景下的事件与图像联合跟踪与高保真3D重建。引入可学习的相机响应函数及无事件损失，有效解决模态对齐与重建伪影问题，实验显示该方法在定位精度与重建质量上均优于现有GS-SLAM系统。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/EGS-SLAM_RGB-D_Gaussian_Splatting_SLAM_with_Events.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Манипулятор-для-помощи-людям-с-ограниченными-возможностями"><a href="#29-Манипулятор-для-помощи-людям-с-ограниченными-возможностями" class="headerlink" title="29. Манипулятор для помощи людям с ограниченными возможностями"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Manipulator_for_people_with_limited_abilities.pdf">Манипулятор для помощи людям с ограниченными возможностями</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Московский государственный технический университет имени Н.Э. Баумана</span></p><p>该论文提出并实现了一个专为行动受限人士设计的四自由度机器人喂食臂RoboBK，包括机械结构设计、动力学与运动学分析、ROS平台下的控制系统开发，并集成了基于技术视觉的自动人脸跟踪算法（OpenCV Haar特征）。实验结果表明，系统能够实现自主喂食和精准定位，有效提升了目标群体的生活质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/%D0%9C%D0%B0%D0%BD%D0%B8%D0%BF%D1%83%D0%BB%D1%8F%D1%82%D0%BE%D1%80_%D0%B4%D0%BB%D1%8F_%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D0%B8_%D0%BB%D1%8E%D0%B4%D1%8F%D0%BC_%D1%81_%D0%BE%D0%B3%D1%80%D0%B0%D0%BD%D0%B8%D1%87%D0%B5%D0%BD%D0%BD%D1%8B%D0%BC%D0%B8_%D0%B2%D0%BE%D0%B7%D0%BC%D0%BE%D0%B6%D0%BD%D0%BE%D1%81%D1%82%D1%8F%D0%BC%D0%B8.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-Imaginative-World-Modeling-with-Scene-Graphs-for-Embodied-Agent-Navigation"><a href="#30-Imaginative-World-Modeling-with-Scene-Graphs-for-Embodied-Agent-Navigation" class="headerlink" title="30. Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Imaginative_World_Modeling_with_Scene_Graphs_for_Embodied_Agent_Navigation.pdf">Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Michigan, Ann Arbor</span></p><p>该论文提出SGImagineNav框架，通过符号化世界建模和场景图想象，结合视觉-语言模型（VLM）和大语言模型（LLM），主动推理和预测未观测环境区域，实现具备前瞻性的目标导航。实验结果显示，SGImagineNav在仿真和现实场景下均取得优异表现，提升导航成功率并支持跨房间、跨楼层的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Imaginative_World_Modeling_with_Scene_Graphs_for_Embodied_Agent_Navigation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-PANAMA-A-Network-Aware-MARL-Framework-for-Multi-Agent-Path-Finding-in-Digital-Twin-Ecosystems"><a href="#31-PANAMA-A-Network-Aware-MARL-Framework-for-Multi-Agent-Path-Finding-in-Digital-Twin-Ecosystems" class="headerlink" title="31. PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PANAMA__A_Network-Aware_MARL_Framework_for_Multi-Agent_Path_Finding_in_Digital_Twin_Ecosystems.pdf">PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Canada Advanced Research Center</span></p><p>本文提出PANAMA算法，通过优先级不对称和网络感知的多智能体强化学习（MARL）方法，实现数字孪生生态中多智能体路径规划。采用集中训练-分布执行架构、异步actor-learner体系和动态优先级机制，提升训练效率和自适应任务执行能力。实验结果表明，在复杂环境下PANAMA在路径准确性、速度及可扩展性上优于现有方法，并显著改善网络通信质量，提升多智能体协作的鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/PANAMA_A_Network-Aware_MARL_Framework_for_Multi-Agent_Path_Finding_in_Digital_Twin_Ecosystems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-Energy-Efficient-Task-Offloading-in-UAV-Enabled-MEC-Using-a-Fully-Decentralized-Deep-Reinforcement-Learning-Approach"><a href="#32-Energy-Efficient-Task-Offloading-in-UAV-Enabled-MEC-Using-a-Fully-Decentralized-Deep-Reinforcement-Learning-Approach" class="headerlink" title="32. Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Energy_Efficient_Task_Offloading_in_UAV-Enabled_MEC_Using_a_Fully_Decentralized_Deep_Reinforcement_L.pdf">Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Iran University of Science and Technology</span></p><p>该论文提出了一种基于图注意力网络（GAT）与经验和参数共享PPO（EPS-PPO）的完全去中心化多智能体深度强化学习方法，用于优化无人机（UAV）在多接入边缘计算（MEC）中的轨迹和任务卸载决策。每架UAV只与邻近UAV局部通信并共享经验、参数，通过GAT整合邻域信息，提升对全局环境的认知，实现高效、鲁棒且可扩展的任务分配和能耗优化。实验结果表明该方法在能耗、任务完成率、收敛速度等方面优于主流半去中心化方法（如MADDPG），同时消除了中心节点带来的通信&#x2F;计算瓶颈与单点失效问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Energy_Efficient_Task_Offloading_in_UAV-Enabled_MEC_Using_a_Fully_Decentralized_Deep_Reinforcement_Learning_Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-D3P-Dynamic-Denoising-Diffusion-Policy-via-Reinforcement-Learning"><a href="#33-D3P-Dynamic-Denoising-Diffusion-Policy-via-Reinforcement-Learning" class="headerlink" title="33. D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/D3P__Dynamic_Denoising_Diffusion_Policy_via_Reinforcement_Learning.pdf">D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>该论文提出Dynamic Denoising Diffusion Policy（D3P），通过为不同关键性动作自适应分配扩散去噪步数，并引入轻量级自适应器，采用两层POMDP建模和强化学习联合优化扩散基政策与自适应器，实现推理效率和任务表现的平衡。实验表明D3P在八项机器人操作任务中较基线方法平均推理加速2.2倍且无性能损失，并在真实机器人上取得1.9倍加速。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/D3P_Dynamic_Denoising_Diffusion_Policy_via_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-Learning-a-Vision-Based-Footstep-Planner-for-Hierarchical-Walking-Control"><a href="#34-Learning-a-Vision-Based-Footstep-Planner-for-Hierarchical-Walking-Control" class="headerlink" title="34. Learning a Vision-Based Footstep Planner for Hierarchical Walking Control"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_a_Vision-Based_Footstep_Planner_for_Hierarchical_Walking_Control.pdf">Learning a Vision-Based Footstep Planner for Hierarchical Walking Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Pennsylvania</span></p><p>本文提出了一种视觉驱动的分层控制框架，用于双足机器人在复杂地形上的行走。方法包括使用深度摄像头生成地形高程图，通过强化学习（RL）训练的高层步态规划器结合基于ALIP（角动量线性倒立摆）模型的低维状态表示，输出步伐命令，并由低层操作空间控制器跟踪步伐轨迹。结果在仿真和真实硬件（Cassie机器人）上进行了验证，表现出在平地和不规则地形上的优异性能，但在阶梯等高度变化显著的地形中表现有限。结论指出：该分层结构和ALIP模型提升了训练效率和系统解释性，但在复杂环境及现实部署中存在局限，需要更具表达力的模型与更紧密的层间协同。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Learning_a_Vision-Based_Footstep_Planner_for_Hierarchical_Walking_Control.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Triple-S-A-Collaborative-Multi-LLM-Framework-for-Solving-Long-Horizon-Implicative-Tasks-in-Robotics"><a href="#35-Triple-S-A-Collaborative-Multi-LLM-Framework-for-Solving-Long-Horizon-Implicative-Tasks-in-Robotics" class="headerlink" title="35. Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Triple-S__A_Collaborative_Multi-LLM_Framework_for_Solving_Long-Horizon_Implicative_Tasks_in_Robotics.pdf">Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeastern University</span></p><p>本文提出Triple-S框架，通过多大语言模型（LLM）协作，采用简化、解决和总结三阶段流程，以提升机器人在长时序隐含任务中的策略代码生成准确率和鲁棒性。实验结果表明，Triple-S显著减少API参数、注释和顺序错误，在仿真和真实机器人环境中均实现了高成功率，并具备较强的通用性和稳定性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Triple-S_A_Collaborative_Multi-LLM_Framework_for_Solving_Long-Horizon_Implicative_Tasks_in_Robotics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-Multimodal-Spiking-Neural-Network-for-Space-Robotic-Manipulation"><a href="#36-Multimodal-Spiking-Neural-Network-for-Space-Robotic-Manipulation" class="headerlink" title="36. Multimodal Spiking Neural Network for Space Robotic Manipulation"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multimodal_Spiking_Neural_Network_for_Space_Robotic_Manipulation.pdf">Multimodal Spiking Neural Network for Space Robotic Manipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology</span></p><p>该论文提出了一种基于多模态输入和脉冲神经网络（SNN）的空间机器人臂自主控制框架，融合几何状态、触觉及语义信息，并采用双通道三阶段课程强化学习（CRL）训练策略。实验结果表明，在仿真空间站环境下，该方法在任务成功率与能耗效率方面显著优于传统人工神经网络（ANN）方法，适合空间机器人实际应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multimodal_Spiking_Neural_Network_for_Space_Robotic_Manipulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-MonoMPC-Monocular-Vision-Based-Navigation-with-Learned-Collision-Model-and-Risk-Aware-Model-Predictive-Control"><a href="#37-MonoMPC-Monocular-Vision-Based-Navigation-with-Learned-Collision-Model-and-Risk-Aware-Model-Predictive-Control" class="headerlink" title="37. MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MonoMPC__Monocular_Vision_Based_Navigation_with_Learned_Collision_Model_and_Risk-Aware_Model_Predict.pdf">MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Tartu</span></p><p>该论文提出了一种基于单目视觉的机器人导航方法，采用深度估计作为概率式碰撞模型的输入，通过联合训练碰撞模型和风险度量，实现风险感知的模型预测控制（MPC）。实验表明，该方法在复杂环境中显著减少碰撞率，超越NoMaD及ROS导航栈，能安全高效地引导机器人前进。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MonoMPC_Monocular_Vision_Based_Navigation_with_Learned_Collision_Model_and_Risk-Aware_Model_Predictive_Control.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-A-Hybrid-Force-Position-Strategy-for-Shape-Control-of-Deformable-Linear-Objects-With-Graph-Attention-Networks"><a href="#38-A-Hybrid-Force-Position-Strategy-for-Shape-Control-of-Deformable-Linear-Objects-With-Graph-Attention-Networks" class="headerlink" title="38. A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Hybrid_Force-Position_Strategy_for_Shape_Control_of_Deformable_Linear_Objects_With_Graph_Attention.pdf">A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了一种针对可变形线性物体（DLO）形状控制的混合力-位置策略，结合了基于力空间的状态轨迹规划与基于位置空间的模型预测控制（MPC），并通过显式动作编码、属性提取及图注意力网络（GAT）实现高精度动态建模。实验证明该方法能高效、稳定地分解大变形任务，避免局部最优，提升DLO操作的性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Hybrid_Force-Position_Strategy_for_Shape_Control_of_Deformable_Linear_Objects_With_Graph_Attention_Networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-Integrating-Neurosymbolic-AI-in-Advanced-Air-Mobility-A-Comprehensive-Survey"><a href="#39-Integrating-Neurosymbolic-AI-in-Advanced-Air-Mobility-A-Comprehensive-Survey" class="headerlink" title="39. Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Integrating_Neurosymbolic_AI_in_Advanced_Air_Mobility__A_Comprehensive_Survey.pdf">Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Maryland, Baltimore County</span></p><p>本论文系统综述了神经符号人工智能（Neurosymbolic AI）在先进空中交通（AAM）领域的应用，包括需求预测、飞机设计、实时空中交通管理等关键环节。方法层面，论文梳理了神经符号AI如何结合神经网络的自适应与符号推理能力，支持安全、合规、可解释的自动化决策，并分析了神经符号强化学习、智能仿真、预测性维护等具体技术流程。结论指出，尽管神经符号AI在动态优化和安全保障等方面展现出潜力，但仍面临规模化、鲁棒性及标准合规等挑战，需多学科协作推动其在AAM中可靠落地。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Integrating_Neurosymbolic_AI_in_Advanced_Air_Mobility_A_Comprehensive_Survey.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-ODYSSEY-Open-World-Quadrupeds-Exploration-and-Manipulation-for-Long-Horizon-Tasks"><a href="#40-ODYSSEY-Open-World-Quadrupeds-Exploration-and-Manipulation-for-Long-Horizon-Tasks" class="headerlink" title="40. ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ODYSSEY__Open-World_Quadrupeds_Exploration_and_Manipulation_for_Long-Horizon_Tasks.pdf">ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>ODYSSEY提出了一个统一的移动操作框架，结合层次化视觉-语言任务规划和强化学习驱动的全身控制策略，实现四足机器人在复杂地形下的长时序任务分解与精准执行。实验验证了系统在多样室内外场景的广泛泛化和强鲁棒性，成功实现了仿真到现实的迁移，提升了机器人在非结构化环境中的实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ODYSSEY_Open-World_Quadrupeds_Exploration_and_Manipulation_for_Long-Horizon_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-BeyondMimic-From-Motion-Tracking-to-Versatile-Humanoid-Control-via-Guided-Diffusion"><a href="#41-BeyondMimic-From-Motion-Tracking-to-Versatile-Humanoid-Control-via-Guided-Diffusion" class="headerlink" title="41. BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BeyondMimic__From_Motion_Tracking_to_Versatile_Humanoid_Control_via_Guided_Diffusion.pdf">BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University</span></p><p>本文提出BeyondMimic，一个基于引导扩散（guided diffusion）的端到端人形机器人运动控制框架，包含高质量的运动跟踪管线和基于状态-动作的扩散策略。该方法实现了从大规模人体运动捕捉数据到真实机器人多样动态技能的迁移，并通过引导扩散实现零样本下多任务目标控制。实验结果表明，该系统在真实硬件上实现了包括导航、遥操作、避障等多种复杂任务，具备高度的通用性、鲁棒性和人类风格。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BeyondMimic_From_Motion_Tracking_to_Versatile_Humanoid_Control_via_Guided_Diffusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Vision-Based-Localization-and-LLM-based-Navigation-for-Indoor-Environments"><a href="#42-Vision-Based-Localization-and-LLM-based-Navigation-for-Indoor-Environments" class="headerlink" title="42. Vision-Based Localization and LLM-based Navigation for Indoor Environments"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Vision-Based_Localization_and_LLM-based_Navigation_for_Indoor_Environments.pdf">Vision-Based Localization and LLM-based Navigation for Indoor Environments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Brown University</span></p><p>该论文提出了一种将视觉定位和大型语言模型（LLM）结合的室内导航方法。方法包括用经过两阶段微调的ResNet-50卷积神经网络实现手机摄像头视觉定位，并用ChatGPT结合预处理楼层平面图生成逐步导航指令，实验在实际办公楼走廊环境下进行，定位模块在重复特征和视野受限条件下准确率达96%，导航模块平均正确率为75%。结论表明，该方法无需专用硬件或信号基础设施，能在资源有限环境下实现可扩展的室内导航，但LLM在空间理解和实时性方面仍有待提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Vision-Based_Localization_and_LLM-based_Navigation_for_Indoor_Environments.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-COM-PACT-COMponent-Aware-Pruning-for-Accelerated-Control-Tasks-in-Latent-Space-Models"><a href="#43-COM-PACT-COMponent-Aware-Pruning-for-Accelerated-Control-Tasks-in-Latent-Space-Models" class="headerlink" title="43. COM-PACT: COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/COMponent-Aware_Pruning_for_Accelerated_Control_Tasks_in_Latent_Space_Models.pdf">COM-PACT: COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">RPTU University Kaiserslautern-Landau</span></p><p>本文提出了一种面向控制任务的组件感知结构化剪枝方法，通过优化每个模块的剪枝比例，实现神经网络控制器在嵌入式硬件上的高效部署，并以Lyapunov稳定性作为主要约束，确保控制系统压缩后的稳定性。实验结果表明，该方法可在保持系统稳定和性能的前提下，大幅压缩模型规模，并明确界定了安全压缩上限。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/COM-PACT_COMponent-Aware_Pruning_for_Accelerated_Control_Tasks_in_Latent_Space_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-AimBot-A-Simple-Auxiliary-Visual-Cue-to-Enhance-Spatial-Awareness-of-Visuomotor-Policies"><a href="#44-AimBot-A-Simple-Auxiliary-Visual-Cue-to-Enhance-Spatial-Awareness-of-Visuomotor-Policies" class="headerlink" title="44. AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AimBot__A_Simple_Auxiliary_Visual_Cue_to_Enhance_Spatial_Awareness_of_Visuomotor_Policies.pdf">AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Michigan</span></p><p>本文提出AimBot，一种轻量级视觉增强技术，通过在多视角RGB图像上叠加射线和瞄准镜准星，将末端执行器（EE）的空间状态显式编码为辅助视觉线索。AimBot利用深度图、摄像头外参及EE位姿生成空间关系提示，无需更改模型架构，计算开销极低，并可直接用于现有视-语言-动作（VLA）模型。实验结果表明，无论在仿真还是真实机器人操作中，AimBot均显著提升多种视觉运动策略的任务成功率，尤其在复杂和长时序任务中空间对齐能力增强明显。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AimBot_A_Simple_Auxiliary_Visual_Cue_to_Enhance_Spatial_Awareness_of_Visuomotor_Policies.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-GRID2GUIDE-A-ENABLED-SMALL-LANGUAGE-MODEL-FOR-INDOOR-NAVIGATION"><a href="#45-GRID2GUIDE-A-ENABLED-SMALL-LANGUAGE-MODEL-FOR-INDOOR-NAVIGATION" class="headerlink" title="45. GRID2GUIDE: A* ENABLED SMALL LANGUAGE MODEL FOR INDOOR NAVIGATION"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Grid2Guide__A__Enabled_Small_Language_Model_for_Indoor_Navigation.pdf">GRID2GUIDE: A* ENABLED SMALL LANGUAGE MODEL FOR INDOOR NAVIGATION</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Alabama</span></p><p>本文提出Grid2Guide混合导航框架，将A*搜索算法与小型语言模型（SLM）结合，实现从二值化占用栅格地图中高效计算最优路径，并通过SLM将压缩后的导航步骤转化为自然语言指令。实验表明，该方法无需专用硬件基础设施，在多种室内环境下能实时生成准确易懂的导航指导，显著提升速度与准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/GRID2GUIDE_A_ENABLED_SMALL_LANGUAGE_MODEL_FOR_INDOOR_NAVIGATION.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-PCHands-PCA-based-Hand-Pose-Synergy-Representation-on-Manipulators-with-N-DoF"><a href="#46-PCHands-PCA-based-Hand-Pose-Synergy-Representation-on-Manipulators-with-N-DoF" class="headerlink" title="46. PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PCHands__PCA-based_Hand_Pose_Synergy_Representation_on_Manipulators_with_N-DoF.pdf">PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Istituto Italiano di Tecnologia</span></p><p>本文提出PCHands，一种结合锚点描述格式（ADF）、条件变分自编码器（CVAE）与主成分分析（PCA）的方法，实现不同结构和自由度操作器（包括人手与多种机器人手）间通用的手部姿态协同表示。通过统一锚点定义、潜在空间对齐及端执行器帧的迭代优化，PCHands可高效表征和迁移多种操作器的手部动作，并显著提升强化学习操作任务的学习效率和一致性，支持跨操作器演示迁移及仿真到现实的直接部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/PCHands_PCA-based_Hand_Pose_Synergy_Representation_on_Manipulators_with_N-DoF.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space"><a href="#47-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space" class="headerlink" title="47. MolmoAct: Action Reasoning Models that can Reason in Space"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MolmoAct__Action_Reasoning_Models_that_can_Reason_in_Space.pdf">MolmoAct: Action Reasoning Models that can Reason in Space</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Allen Institute for AI</span></p><p>MolmoAct提出了一类全新的视觉-语言-动作模型（Action Reasoning Models, ARMs），通过深度感知、视觉轨迹推理和动作预测三阶段结构化流程，将感知、规划与控制有机结合。该方法首次引入可编辑空间轨迹和可解释行为，显著提升机器人的通用性、可解释性和任务适应能力。实验结果显示MolmoAct在仿真与现实环境中均超越多项主流基线，并在LIBERO等基准任务上取得最高成功率，同时在开源数据和模型方面推动了机器人基础模型的开放发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MolmoAct_Action_Reasoning_Models_that_can_Reason_in_Space.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-DETACH-Cross-domain-Learning-for-Long-Horizon-Tasks-via-Mixture-of-Disentangled-Experts"><a href="#48-DETACH-Cross-domain-Learning-for-Long-Horizon-Tasks-via-Mixture-of-Disentangled-Experts" class="headerlink" title="48. DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DETACH__Cross-domain_Learning_for_Long-Horizon_Tasks_via_Mixture_of_Disentangled_Experts.pdf">DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing University of Technology</span></p><p>该论文提出DETACH框架，通过类脑双流解耦架构，将环境感知与自身体态处理拆分为环境编码器和自编码器两路，分别独立优化并采用多策略融合机制，实现技能和环境的跨域泛化。实验表明DETACH在多步长任务中技能复用性和环境适应性均显著优于现有方法，成功率和效率提升明显。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DETACH_Cross-domain_Learning_for_Long-Horizon_Tasks_via_Mixture_of_Disentangled_Experts.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-Touch-Speaks-Sound-Feels-A-Multimodal-Approach-to-Affective-and-Social-Touch-from-Robots-to-Humans"><a href="#49-Touch-Speaks-Sound-Feels-A-Multimodal-Approach-to-Affective-and-Social-Touch-from-Robots-to-Humans" class="headerlink" title="49. Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Touch_Speaks,_Sound_Feels__A_Multimodal_Approach_to_Affective_and_Social_Touch_from_Robots_to_Humans.pdf">Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ghent University</span></p><p>本论文提出了一种多模态人机交互系统，结合5×5振动马达阵列与音频回放，实现机器人通过同步的触觉与听觉刺激向人类传递情感和社交手势。32名中国参与者对多模态、单一触觉和单一听觉条件下的10种情感和6种手势进行了识别实验。结果表明，多模态整合显著提升了情感与手势解码准确率，且触觉和听觉各自对不同情感表达具备优势，但单独的手势难以有效传达情感。该研究强调多感官整合对提升人机情感交互的重要性，并为未来社交型机器人设计提供了依据。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Touch_Speaks%2C_Sound_Feels_A_Multimodal_Approach_to_Affective_and_Social_Touch_from_Robots_to_Humans.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-AgentWorld-An-Interactive-Simulation-Platform-for-Scene-Construction-and-Mobile-Robotic-Manipulation"><a href="#50-AgentWorld-An-Interactive-Simulation-Platform-for-Scene-Construction-and-Mobile-Robotic-Manipulation" class="headerlink" title="50. AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AgentWorld__An_Interactive_Simulation_Platform_for_Scene_Construction_and_Mobile_Robotic_Manipulatio.pdf">AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent Robotics X</span></p><p>该论文提出了AgentWorld，一个集程序化场景生成、高保真物理仿真与多模态远程操作于一体的仿真平台，支持多种家庭环境下多机器人形态（轮式、类人等）的操作数据采集和多阶段任务标注。通过基于行为克隆、动作块变换器、扩散策略和视觉-语言-动作模型的模仿学习算法，实验验证了平台和数据集在机器人技能训练和sim-to-real迁移方面的有效性，尤其在复杂场景和多阶段任务上展现出较好泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AgentWorld_An_Interactive_Simulation_Platform_for_Scene_Construction_and_Mobile_Robotic_Manipulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-NeeCo-Image-Synthesis-of-Novel-Instrument-States-Based-on-Dynamic-and-Deformable-3D-Gaussian-Reconstruction"><a href="#51-NeeCo-Image-Synthesis-of-Novel-Instrument-States-Based-on-Dynamic-and-Deformable-3D-Gaussian-Reconstruction" class="headerlink" title="51. NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/NeeCo__Image_Synthesis_of_Novel_Instrument_States_Based_on_Dynamic_and_Deformable_3D_Gaussian_Recons.pdf">NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Leeds</span></p><p>本文提出NeeCo，一种基于动态3D高斯重建的外科手术器械图像合成方法，通过动态高斯模型和MLP学习器械运动，实现从有限数据生成新姿态和变形下的真实感图像，并自动生成分割与检测标注。实验结果显示，合成数据用于训练医疗神经网络在目标检测和分割任务上性能提升超过15%，显著优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/NeeCo_Image_Synthesis_of_Novel_Instrument_States_Based_on_Dynamic_and_Deformable_3D_Gaussian_Reconstruction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-Autonomous-Navigation-of-Cloud-Controlled-Quadcopters-in-Confined-Spaces-Using-Multi-Modal-Perception-and-LLM-Driven-High-Semantic-Reasoning"><a href="#52-Autonomous-Navigation-of-Cloud-Controlled-Quadcopters-in-Confined-Spaces-Using-Multi-Modal-Perception-and-LLM-Driven-High-Semantic-Reasoning" class="headerlink" title="52. Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Autonomous_Navigation_of_Cloud-Controlled_Quadcopters_in_Confined_Spaces_Using_Multi-Modal_Perceptio.pdf">Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Rajshahi University of Engineering and Technology</span></p><p>该论文提出一种云端智能感知系统，实现了无人机在GPS-denied室内环境下的自主导航。方法上，系统集成YOLOv11目标检测、Depth Anything V2单目深度估计、自研PCB（含IMU和6路ToF传感器）、视觉语言模型（VLM）和经LoRA微调的轻量大语言模型（LLM），通过多线程并行和云端推理，实现多模态信息融合、3D空间感知与高语义决策，并采用安全保护壳机制实现碰撞规避。实验结果显示，目标检测mAP50达到0.6，深度估计MAE为7.2cm，42次试验仅16次安全壳触发，端到端延迟低于1秒。该系统有效提升了无人机在狭小空间中的智能自主性和安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Autonomous_Navigation_of_Cloud-Controlled_Quadcopters_in_Confined_Spaces_Using_Multi-Modal_Perception_and_LLM-Driven_High_Semantic_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-Learning-Satellite-Attitude-Dynamics-with-Physics-Informed-Normalising-Flow"><a href="#53-Learning-Satellite-Attitude-Dynamics-with-Physics-Informed-Normalising-Flow" class="headerlink" title="53. Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_Satellite_Attitude_Dynamics_with_Physics-Informed_Normalising_Flow.pdf">Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Politecnico di Torino</span></p><p>本文提出了一种基于Real NVP归一化流神经网络和自注意机制的物理信息神经网络（PINN），用于学习卫星姿态动力学，并通过引入物理约束损失提升模型的泛化性和鲁棒性。实验结果表明，融合物理知识的模型在预测精度、模型稳定性和抗噪声能力方面均显著优于纯数据驱动方法，集成到MPC框架后控制表现提升最大可达42.86%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Learning_Satellite_Attitude_Dynamics_with_Physics-Informed_Normalising_Flow.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-SwarmVLM-VLM-Guided-Impedance-Control-for-Autonomous-Navigation-of-Heterogeneous-Robots-in-Dynamic-Warehousing"><a href="#54-SwarmVLM-VLM-Guided-Impedance-Control-for-Autonomous-Navigation-of-Heterogeneous-Robots-in-Dynamic-Warehousing" class="headerlink" title="54. SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SwarmVLM__VLM-Guided_Impedance_Control_for_Autonomous_Navigation_of_Heterogeneous_Robots_in_Dynamic_.pdf">SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Skolkovo Institute of Science and Technology</span></p><p>该论文提出了SwarmVLM系统，将无人机和地面机器人通过虚拟阻抗控制实现协作导航，并结合视觉语言模型（VLM）与检索增强生成（RAG）框架，根据环境变化自适应调整阻抗参数。实验验证显示该系统在动态仓储环境下导航成功率为92%，在良好光照条件下障碍物检测与参数选择准确率达80%，地面机器人能有效避开短障碍物并与无人机保持协作。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SwarmVLM_VLM-Guided_Impedance_Control_for_Autonomous_Navigation_of_Heterogeneous_Robots_in_Dynamic_Warehousing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-Risk-Map-As-Middleware-Towards-Interpretable-Cooperative-End-to-end-Autonomous-Driving-for-Risk-Aware-Planning"><a href="#55-Risk-Map-As-Middleware-Towards-Interpretable-Cooperative-End-to-end-Autonomous-Driving-for-Risk-Aware-Planning" class="headerlink" title="55. Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Risk_Map_As_Middleware__Towards_Interpretable_Cooperative_End-to-end_Autonomous_Driving_for_Risk-Awa.pdf">Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tongji University</span></p><p>本文提出Risk Map as Middleware (RiskMM)，一个面向协同自动驾驶的端到端可解释风险感知规划框架。方法上，RiskMM通过Transformer实现多智能体时空表征，利用注意力机制建模交互风险，并将其作为解释性中间件，结合可微分的学习型MPC模块实现风险感知轨迹规划，兼容物理约束和不同车辆类型。实验在V2XPnP-Seq真实数据集上表明，RiskMM在风险感知规划和可解释性上优于现有方法，表现出强健性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Risk_Map_As_Middleware_Towards_Interpretable_Cooperative_End-to-end_Autonomous_Driving_for_Risk-Aware_Planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-AIS-LLM-A-Unified-Framework-for-Maritime-Trajectory-Prediction-Anomaly-Detection-and-Collision-Risk-Assessment-with-Explainable-Forecasting"><a href="#56-AIS-LLM-A-Unified-Framework-for-Maritime-Trajectory-Prediction-Anomaly-Detection-and-Collision-Risk-Assessment-with-Explainable-Forecasting" class="headerlink" title="56. AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AIS-LLM__A_Unified_Framework_for_Maritime_Trajectory_Prediction,_Anomaly_Detection,_and_Collision_Ri.pdf">AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hanbat National University</span></p><p>该论文提出AIS-LLM框架，通过多尺度时间序列编码器、提示编码器与跨模态对齐模块，将AIS船舶导航数据与大型语言模型（LLM）结合，实现航迹预测、异常检测与碰撞风险评估三大任务，并生成可解释的自然语言分析报告。实验表明，AIS-LLM在精度、鲁棒性及解释能力上均优于现有方法，显著提升了海事交通管理的智能化与决策支持能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AIS-LLM_A_Unified_Framework_for_Maritime_Trajectory_Prediction%2C_Anomaly_Detection%2C_and_Collision_Risk_Assessment_with_Explainable_Forecasting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-End-to-End-Humanoid-Robot-Safe-and-Comfortable-Locomotion-Policy"><a href="#57-End-to-End-Humanoid-Robot-Safe-and-Comfortable-Locomotion-Policy" class="headerlink" title="57. End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/End-to-End_Humanoid_Robot_Safe_and_Comfortable_Locomotion_Policy.pdf">End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (Guangzhou)</span></p><p>该论文提出了一种端到端的类人机器人运动策略，利用原始时空LiDAR点云输入，通过Gated Recurrent Unit和多层感知机结构，直接输出电机控制命令。方法将安全性通过控制屏障函数(CBF)转化为约束马尔可夫决策过程(CMDP)中的代价项，并采用P3O算法在训练阶段严格约束安全与舒适性目标。实验证明该方法在复杂三维障碍环境中实现了高安全性、舒适性和高成功率的导航，并成功在仿真到实际机器人平台进行了迁移。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/End-to-End_Humanoid_Robot_Safe_and_Comfortable_Locomotion_Policy.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-LAURON-VI-A-Six-Legged-Robot-for-Dynamic-Walking"><a href="#58-LAURON-VI-A-Six-Legged-Robot-for-Dynamic-Walking" class="headerlink" title="58. LAURON VI: A Six-Legged Robot for Dynamic Walking"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LAURON_VI__A_Six-Legged_Robot_for_Dynamic_Walking.pdf">LAURON VI: A Six-Legged Robot for Dynamic Walking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">FZI Research Center for Information Technology</span></p><p>本文提出了LAURON VI，一种具备18个串联弹性关节的六足动态步态机器人平台，支持卡氏运动学、模型预测控制和强化学习三种步态控制方法，并进行了实验室和类火星环境测试。结果表明，LAURON VI在不同控制策略下展现出优异的灵活性、负载能力和适应复杂地形的鲁棒性，验证了六足机器人在动态行走和实际任务中的应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LAURON_VI_A_Six-Legged_Robot_for_Dynamic_Walking.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-EMPATHIA-Multi-Faceted-Human-AI-Collaboration-for-Refugee-Integration"><a href="#59-EMPATHIA-Multi-Faceted-Human-AI-Collaboration-for-Refugee-Integration" class="headerlink" title="59. EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EMPATHIA__Multi-Faceted_Human-AI_Collaboration_for_Refugee_Integration.pdf">EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Texas A&amp;M University</span></p><p>该论文提出了EMPATHIA，一个以多智能体为核心的人机协作框架，专为难民安置与长期融合设计。方法包括三阶段模块（SEED, RISE, THRIVE）和情感、文化、伦理三视角的selector–validator架构，利用LLaMA-3模型实现高可解释性与透明度的人道主义推荐。实验在UN Kakuma数据集上（6,359名难民）验证了框架的高一致性（87.4%验证收敛）、公平性和可扩展性。结论表明该方法能在人道大规模场景下以多视角保护人类尊严，实现AI与人的协同决策。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/EMPATHIA_Multi-Faceted_Human-AI_Collaboration_for_Refugee_Integration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-GraphCoT-VLA-A-3D-Spatial-Aware-Reasoning-Vision-Language-Action-Model-for-Robotic-Manipulation-with-Ambiguous-Instructions"><a href="#60-GraphCoT-VLA-A-3D-Spatial-Aware-Reasoning-Vision-Language-Action-Model-for-Robotic-Manipulation-with-Ambiguous-Instructions" class="headerlink" title="60. GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/GraphCoT-VLA__A_3D_Spatial-Aware_Reasoning_Vision-Language-Action_Model_for_Robotic_Manipulation_wit.pdf">GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Noah’s Ark Lab, Huawei</span></p><p>该论文提出GraphCoT-VLA，一种结合3D Pose-Object图和多层次Chain-of-Thought（CoT）结构的端到端视觉-语言-动作模型，提升机器人在面对模糊指令和未知环境时的理解、推理与操作能力。实验表明，该模型在真实机器人复杂任务中显著提升了任务成功率和响应速度，具备良好的泛化性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/GraphCoT-VLA_A_3D_Spatial-Aware_Reasoning_Vision-Language-Action_Model_for_Robotic_Manipulation_with_Ambiguous_Instructions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-Breaking-Down-and-Building-Up-Mixture-of-Skill-Based-Vision-and-Language-Navigation-Agents"><a href="#61-Breaking-Down-and-Building-Up-Mixture-of-Skill-Based-Vision-and-Language-Navigation-Agents" class="headerlink" title="61. Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Breaking_Down_and_Building_Up__Mixture_of_Skill-Based_Vision-and-Language_Navigation_Agents.pdf">Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Michigan State University</span></p><p>该论文提出SkillNav，一种模块化的视觉-语言导航（VLN）框架，将导航任务分解为可解释的原子技能（如方向调整、垂直移动、区域识别等），并通过大规模VLM路由器在每一步动态选择最合适的技能代理。SkillNav在R2R和GSA-R2R等基准上取得了新的SOTA表现，显著提升了对新环境和指令的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Breaking_Down_and_Building_Up_Mixture_of_Skill-Based_Vision-and-Language_Navigation_Agents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="62-Sensory-Robustness-through-Top-Down-Feedback-and-Neural-Stochasticity-in-Recurrent-Vision-Models"><a href="#62-Sensory-Robustness-through-Top-Down-Feedback-and-Neural-Stochasticity-in-Recurrent-Vision-Models" class="headerlink" title="62. Sensory Robustness through Top-Down Feedback and Neural Stochasticity in Recurrent Vision Models"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sensory_robustness_through_top-down_feedback_and_neural_stochasticity_in_recurrent_vision_models.pdf">Sensory Robustness through Top-Down Feedback and Neural Stochasticity in Recurrent Vision Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Tübingen</span></p><p>本论文提出在卷积循环神经网络（ConvRNN）中结合顶层反馈（top-down feedback）和类神经随机性（通过dropout模拟）的方法，系统比较了有无反馈及有无dropout对图像分类、速度-准确性权衡和对噪声与对抗攻击的鲁棒性影响。研究发现，仅有顶层反馈无法提升鲁棒性，唯有顶层反馈与dropout协同，模型在速度-准确性权衡和对分布外噪声、对抗扰动的鲁棒性上显著优于其他结构；其机制包括通过反馈和dropout约束表征分布于低维流形上、稳定表征动力学、提升分布外环境下信息编码效率。结论是顶层反馈与神经噪声的协同可极大提升感知鲁棒性，兼顾泛化与表征稳定性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Sensory_Robustness_through_Top-Down_Feedback_and_Neural_Stochasticity_in_Recurrent_Vision_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-TeSO-Representing-and-Compressing-3D-Point-Cloud-Scenes-with-Textured-Surfel-Octree"><a href="#63-TeSO-Representing-and-Compressing-3D-Point-Cloud-Scenes-with-Textured-Surfel-Octree" class="headerlink" title="63. TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TeSO__Representing_and_Compressing_3D_Point_Cloud_Scenes_with_Textured_Surfel_Octree.pdf">TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">New York University</span></p><p>本文提出了Textured Surfel Octree (TeSO)，一种新型3D点云场景高效表示与压缩方法。通过将点云转化为八叉树结构的表面元素，并为每个表面元素分配高频纹理贴图，TeSO在保持渲染质量的同时显著减少表示所需的几何基元数量。配合基于学习的熵模型和标准视频&#x2F;点云编码的压缩方案，TeSO在不同分辨率下比主流基线方法（G-PCC、B2P等）实现了更优的比特率-渲染质量权衡。实验结果表明，该方法支持高效解码和实时渲染，适合3D视觉内容流媒体应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TeSO_Representing_and_Compressing_3D_Point_Cloud_Scenes_with_Textured_Surfel_Octree.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-BrainATCL-Adaptive-Temporal-Brain-Connectivity-Learning-for-Functional-Link-Prediction-and-Age-Estimation"><a href="#64-BrainATCL-Adaptive-Temporal-Brain-Connectivity-Learning-for-Functional-Link-Prediction-and-Age-Estimation" class="headerlink" title="64. BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BrainATCL__Adaptive_Temporal_Brain_Connectivity_Learning_for_Functional_Link_Prediction_and_Age_Esti.pdf">BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">New Jersey Institute of Technology</span></p><p>本文提出BrainATCL，一种无监督的动态自适应脑连接性学习框架，利用基于新颖性指数的自适应时序窗口和GINE-Mamba2骨干网络，对fMRI动态脑网络进行时空嵌入建模，并融合左&#x2F;右脑半球与子网络等生物学先验信息。实验证明在功能连接预测与年龄估计任务上，BrainATCL在跨会话和同会话设定中均优于现有静态与动态图神经网络模型，泛化能力强。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BrainATCL_Adaptive_Temporal_Brain_Connectivity_Learning_for_Functional_Link_Prediction_and_Age_Estimation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-Large-Language-Model-Evaluated-Stand-alone-Attention-Assisted-Graph-Neural-Network-with-Spatial-and-Structural-Information-Interaction-for-Precise-Endoscopic-Image-Segmentation"><a href="#65-Large-Language-Model-Evaluated-Stand-alone-Attention-Assisted-Graph-Neural-Network-with-Spatial-and-Structural-Information-Interaction-for-Precise-Endoscopic-Image-Segmentation" class="headerlink" title="65. Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Large_Language_Model_Evaluated_Stand-alone_Attention-Assisted_Graph_Neural_Network_with_Spatial_and_.pdf">Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong</span></p><p>本文提出FOCUS-Med模型，融合空间和结构图神经网络（Dual-GCN）、位置融合的自注意力模块（LFSA）以及加权快速归一化融合机制（WFNF），实现多尺度特征的高效集成与增强。通过引入LLM（如GPT-4o）进行专家对齐的定性评价，模型在多个公开医学图像分割数据集上获得了多项指标上的最新性能，显示出临床应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Large_Language_Model_Evaluated_Stand-alone_Attention-Assisted_Graph_Neural_Network_with_Spatial_and_Structural_Information_Interaction_for_Precise_Endoscopic_Image_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-TerraMAE-Learning-Spatial-Spectral-Representations-from-Hyperspectral-Earth-Observation-Data-via-Adaptive-Masked-Autoencoders"><a href="#66-TerraMAE-Learning-Spatial-Spectral-Representations-from-Hyperspectral-Earth-Observation-Data-via-Adaptive-Masked-Autoencoders" class="headerlink" title="66. TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TerraMAE__Learning_Spatial-Spectral_Representations_from_Hyperspectral_Earth_Observation_Data_via_Ad.pdf">TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Colorado State University</span></p><p>本文提出了TerraMAE，一种针对高光谱卫星图像的自监督预训练框架，采用基于光谱相似性的自适应通道分组和融合结构相似性与光谱信息散度的多目标重构损失，提升空间-光谱特征嵌入。实验结果表明，TerraMAE在影像重建精度和多种下游地理任务（作物分类、土地覆盖、土壤质地预测）中均优于标准MAE和ResNet-50，证明其空间-光谱表示迁移能力强。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TerraMAE_Learning_Spatial-Spectral_Representations_from_Hyperspectral_Earth_Observation_Data_via_Adaptive_Masked_Autoencoders.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-OctreeNCA-Single-Pass-184-MP-Segmentation-on-Consumer-Hardware"><a href="#67-OctreeNCA-Single-Pass-184-MP-Segmentation-on-Consumer-Hardware" class="headerlink" title="67. OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/OctreeNCA__Single-Pass_184_MP_Segmentation_on_Consumer_Hardware.pdf">OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Darmstadt</span></p><p>本文提出了OctreeNCA架构，通过将神经元元胞自动机（NCA）与八叉树结构结合，实现了高分辨率医学图像与视频的高效单步分割。关键技术包括多级NCA推理、隐藏通道上采样、定制CUDA实现，大幅降低了训练与推理时的显存需求。实验表明，OctreeNCA在病理、放射与手术视频分割任务中，参数量低、推理速度快，单步可处理184MP病理切片或1分钟手术视频，显著优于UNet和Transformer等主流方法。结论：OctreeNCA可在低成本硬件上实现高质量大规模医学图像分割，缩小不同地区医疗AI应用的硬件鸿沟。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/OctreeNCA_Single-Pass_184_MP_Segmentation_on_Consumer_Hardware.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-TADoc-Robust-Time-Aware-Document-Image-Dewarping"><a href="#68-TADoc-Robust-Time-Aware-Document-Image-Dewarping" class="headerlink" title="68. TADoc: Robust Time-Aware Document Image Dewarping"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TADoc__Robust_Time-Aware_Document_Image_Dewarping.pdf">TADoc: Robust Time-Aware Document Image Dewarping</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Information Engineering, Chinese Academy of Sciences</span></p><p>本文提出了一种时间感知的文档图像去扭曲方法TADoc，将去扭曲建模为多步动态过程，通过时间步嵌入与轻量级网络结构，逐步恢复文档图像的几何变形。实验显示，TADoc在多个公开基准数据集上表现出强鲁棒性和优越性能，同时引入Document Layout Similarity (DLS)新指标，补充传统OCR评估，能全面衡量去扭曲效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TADoc_Robust_Time-Aware_Document_Image_Dewarping.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-WeatherDiffusion-Weather-Guided-Diffusion-Model-for-Forward-and-Inverse-Rendering"><a href="#69-WeatherDiffusion-Weather-Guided-Diffusion-Model-for-Forward-and-Inverse-Rendering" class="headerlink" title="69. WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/WeatherDiffusion__Weather-Guided_Diffusion_Model_for_Forward_and_Inverse_Rendering.pdf">WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>该论文提出WeatherDiffusion，一种针对自动驾驶场景下复杂天气和光照条件的前向与逆向渲染扩散框架。方法以Stable Diffusion 3.5为基础，结合自主构建的WeatherSynthetic和WeatherReal数据集，通过引入Intrinsic map-aware attention（MAA）模块，实现了对场景的几何、材质和光照等属性的高质量分解和可控编辑。实验表明，该方法在多项基准上超越现有方法，提升了恶劣天气下目标检测与分割等下游任务的鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/WeatherDiffusion_Weather-Guided_Diffusion_Model_for_Forward_and_Inverse_Rendering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-Beyond-Frequency-Seeing-Subtle-Cues-Through-the-Lens-of-Spatial-Decomposition-for-Fine-Grained-Visual-Classification"><a href="#70-Beyond-Frequency-Seeing-Subtle-Cues-Through-the-Lens-of-Spatial-Decomposition-for-Fine-Grained-Visual-Classification" class="headerlink" title="70. Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Beyond_Frequency__Seeing_Subtle_Cues_Through_the_Lens_of_Spatial_Decomposition_for_Fine-Grained_Visu.pdf">Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Anhui University</span></p><p>本文提出了Subtle-Cue Oriented Perception Engine (SCOPE)，通过空间域自适应滤波模块（Subtle Detail Extractor和Salient Semantic Refiner），联合增强细粒度图像的局部细节和全局语义，实现更灵活的多尺度特征融合。实验表明，该方法在四个主流细粒度视觉分类数据集上取得了新的SOTA效果，显著提升了细粒度分类的准确率和细节保持能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Beyond_Frequency_Seeing_Subtle_Cues_Through_the_Lens_of_Spatial_Decomposition_for_Fine-Grained_Visual_Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-CannyEdit-Selective-Canny-Control-and-Dual-Prompt-Guidance-for-Training-free-Image-Editing"><a href="#71-CannyEdit-Selective-Canny-Control-and-Dual-Prompt-Guidance-for-Training-free-Image-Editing" class="headerlink" title="71. CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-free Image Editing"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CannyEdit__Selective_Canny_Control_and_Dual-Prompt_Guidance_for_Training-Free_Image_Editing.pdf">CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-free Image Editing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>该论文提出了CannyEdit，一种无需训练的区域图像编辑方法，通过Selective Canny Control（在用户指定区域屏蔽Canny ControlNet的结构引导、保留未编辑区域细节）和Dual-Prompt Guidance（结合局部和全局文本提示）实现目标区域的精确编辑和未编辑区域的高保真保持。实验表明，CannyEdit在文本遵循性与上下文保真度间取得更优平衡，编辑无缝性显著提升，用户难以分辨AI编辑痕迹，优于现有同类方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CannyEdit_Selective_Canny_Control_and_Dual-Prompt_Guidance_for_Training-free_Image_Editing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-MMReID-Bench-Unleashing-the-Power-of-MLLMs-for-Effective-and-Versatile-Person-Re-identification"><a href="#72-MMReID-Bench-Unleashing-the-Power-of-MLLMs-for-Effective-and-Versatile-Person-Re-identification" class="headerlink" title="72. MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MMReID-Bench__Unleashing_the_Power_of_MLLMs_for_Effective_and_Versatile_Person_Re-identification.pdf">MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">East China Normal University</span></p><p>本文提出了MMReID-Bench，多任务多模态的benchmark，专为多模态大语言模型（MLLMs）在行人重识别（Person ReID）任务设计，涵盖10种主流ReID任务和20710个多模态查询及图库图像，支持RGB、热成像、红外、素描、文本描述等多模态输入。作者系统评测了15种主流MLLMs，并设计了统一的对话模板和任务先验融合方法，揭示了MLLMs在大多数ReID任务中表现优异，但在热成像和红外等跨模态任务上仍有较大挑战。结论指出，MMReID-Bench推动了多模态基础模型在实际复杂场景中的应用与发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MMReID-Bench_Unleashing_the_Power_of_MLLMs_for_Effective_and_Versatile_Person_Re-identification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-A-Simple-yet-Powerful-Instance-Aware-Prompting-Framework-for-Training-free-Camouflaged-Object-Segmentation"><a href="#73-A-Simple-yet-Powerful-Instance-Aware-Prompting-Framework-for-Training-free-Camouflaged-Object-Segmentation" class="headerlink" title="73. A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Simple_yet_Powerful_Instance-Aware_Prompting_Framework_for_Training-free_Camouflaged_Object_Segmen.pdf">A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai University</span></p><p>该论文提出了一个训练自由的Instance-Aware Prompting Framework（IAPF），通过多模态大语言模型生成图像特定前景&#x2F;背景标签，利用Grounding DINO生成精确的实例级框提示，结合单前景多背景点采样策略（SFMBP），并通过自一致性投票机制选取最优分割结果，实现了对伪装物体的细粒度实例分割，无需任何训练或额外标注。实验结果显示IAPF在多个COS基准上显著超越现有训练自由方法，尤其在多实例场景中表现优异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Simple_yet_Powerful_Instance-Aware_Prompting_Framework_for_Training-free_Camouflaged_Object_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-MV-CoRe-Multimodal-Visual-Conceptual-Reasoning-for-Complex-Visual-Question-Answering"><a href="#74-MV-CoRe-Multimodal-Visual-Conceptual-Reasoning-for-Complex-Visual-Question-Answering" class="headerlink" title="74. MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MV-CoRe__Multimodal_Visual-Conceptual_Reasoning_for_Complex_Visual_Question_Answering.pdf">MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shaanxi University of Technology</span></p><p>该论文提出MV-CoRe模型，通过深度融合预训练视觉大模型（VLM）、语言大模型（LLM）的全局嵌入与细粒度语义感知视觉特征（包括目标检测和场景图表征），利用创新的多模态融合Transformer，实现复杂视觉问答任务中的深层跨模态推理。实验表明，MV-CoRe在GQA、A-OKVQA和OKVQA等复杂VQA基准上均优于主流LVLM基线，显著提升准确率和推理深度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MV-CoRe_Multimodal_Visual-Conceptual_Reasoning_for_Complex_Visual_Question_Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-HiMat-DiT-based-Ultra-High-Resolution-SVBRDF-Generation"><a href="#75-HiMat-DiT-based-Ultra-High-Resolution-SVBRDF-Generation" class="headerlink" title="75. HiMat: DiT-based Ultra-High Resolution SVBRDF Generation"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HiMat__DiT-based_Ultra-High_Resolution_SVBRDF_Generation.pdf">HiMat: DiT-based Ultra-High Resolution SVBRDF Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nankai University</span></p><p>该论文提出HiMat，一种基于Diffusion Transformer（DiT）的高效4K分辨率SVBRDF（空间变化双向反射分布函数）生成框架，可根据文本提示直接生成超高分辨率物理材质。方法上，HiMat采用高效线性注意力的DiT骨干，并引入CrossStitch模块实现多通道间高效一致性，还结合SWT（Stationary Wavelet Transform）频域监督增强细节。实验证明HiMat在细节保真度、结构一致性和推理效率上均优于现有方法，并支持消费级硬件快速生成。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/HiMat_DiT-based_Ultra-High_Resolution_SVBRDF_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-Spatio-Temporal-Conditional-Diffusion-Models-for-Forecasting-Future-Multiple-Sclerosis-Lesion-Masks-Conditioned-on-Treatments"><a href="#76-Spatio-Temporal-Conditional-Diffusion-Models-for-Forecasting-Future-Multiple-Sclerosis-Lesion-Masks-Conditioned-on-Treatments" class="headerlink" title="76. Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Spatio-Temporal_Conditional_Diffusion_Models_for_Forecasting_Future_Multiple_Sclerosis_Lesion_Masks_.pdf">Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">McGill University</span></p><p>本文提出了一种基于ControlNet的时空条件扩散模型，能够结合多模态MRI和治疗信息，预测多发性硬化症患者未来新发和扩大的T2病灶掩膜。实验结果表明，该模型在六种不同疗效的治疗下，能准确预测病灶位置、数量及分类，并可生成不同治疗条件下的反事实医学图像，具有临床应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Spatio-Temporal_Conditional_Diffusion_Models_for_Forecasting_Future_Multiple_Sclerosis_Lesion_Masks_Conditioned_on_Treatments.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-Can-Multitask-Learning-Enhance-Model-Explainability"><a href="#77-Can-Multitask-Learning-Enhance-Model-Explainability" class="headerlink" title="77. Can Multitask Learning Enhance Model Explainability?"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Can_Multitask_Learning_Enhance_Model_Explainability_.pdf">Can Multitask Learning Enhance Model Explainability?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kaiserslautern-Landau University</span></p><p>该论文提出了一种将多模态遥感数据中的部分模态转为多任务学习中的辅助任务（而非输入），通过分析主任务与辅助任务之间的预测误差相关性，实现对模型行为的内在解释。实验证明在 CropYield、Benge 和 TreeSAT 三个遥感数据集上，该方法在不损失主任务性能的同时，减少了推理阶段对额外数据的需求，并能够通过任务间误差分析提升模型可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Can_Multitask_Learning_Enhance_Model_Explainability_.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-Intrinsic-Explainability-of-Multimodal-Learning-for-Crop-Yield-Prediction"><a href="#78-Intrinsic-Explainability-of-Multimodal-Learning-for-Crop-Yield-Prediction" class="headerlink" title="78. Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Intrinsic_Explainability_of_Multimodal_Learning_for_Crop_Yield_Prediction.pdf">Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">RPTU Kaiserslautern-Landau</span></p><p>本论文提出了一种以Transformer为核心的多模态学习框架，集成了卫星多光谱时序、气象时间序列、地形高程和土壤属性四种模态，实现农作物亚田块级产量预测，并系统对比了Attention Rollout (AR)、Generic Attention (GA)和Shapley Value Sampling (SVS)三种特征归因方法。实验表明，Transformer在精度和推理速度上优于卷积和循环网络，AR方法在特征归因的稳健性和一致性上表现最佳，结论指出Transformer的自注意力机制能提升多模态模型的内在可解释性，为农业遥感等数据密集领域的可信模型部署提供理论和实证基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Intrinsic_Explainability_of_Multimodal_Learning_for_Crop_Yield_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-eMotions-A-Large-Scale-Dataset-and-Audio-Visual-Fusion-Network-for-Emotion-Analysis-in-Short-form-Videos"><a href="#79-eMotions-A-Large-Scale-Dataset-and-Audio-Visual-Fusion-Network-for-Emotion-Analysis-in-Short-form-Videos" class="headerlink" title="79. eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/eMotions__A_Large-Scale_Dataset_and_Audio-Visual_Fusion_Network_for_Emotion_Analysis_in_Short-form_V.pdf">eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>该论文提出了eMotions数据集，包含27996条短视频及六类情感注释，通过多阶段人工标注和人员分配策略提升数据质量。作者设计了AV-CANet音视频融合网络，采用Video Swin-Transformer和多级局部-全局融合模块，结合EP-CE损失函数，显著提升短视频情感识别效果，并在多个公开数据集上取得优异表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/eMotions_A_Large-Scale_Dataset_and_Audio-Visual_Fusion_Network_for_Emotion_Analysis_in_Short-form_Videos.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-VSI-Visual–Subtitle-Integration-for-Keyframe-Selection-to-Enhance-Long-Video-Understanding"><a href="#80-VSI-Visual–Subtitle-Integration-for-Keyframe-Selection-to-Enhance-Long-Video-Understanding" class="headerlink" title="80. VSI: Visual–Subtitle Integration for Keyframe Selection to Enhance Long Video Understanding"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VSI__Visual_Subtitle_Integration_for_Keyframe_Selection_to_enhance_Long_Video_Understanding.pdf">VSI: Visual–Subtitle Integration for Keyframe Selection to Enhance Long Video Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AI Thrust, HKUST(GZ)</span></p><p>本文提出了一种名为VISUAL–SUBTITLE INTEGRATION（VSI）的多模态关键帧检索方法，结合了基于视觉对象检测（YOLO-World）与字幕内容语义匹配（基于对比句向量），通过双流融合机制引导视频采样分布，提升长视频理解中的关键帧定位与下游问答性能。实验表明，VSI方法在LONGVIDEOBENCH等数据集上显著提升关键帧定位准确率和问答任务表现，具有训练无关、轻量高效、易集成等优点。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/VSI_Visual%E2%80%93Subtitle_Integration_for_Keyframe_Selection_to_Enhance_Long_Video_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-Low-Rank-Expert-Merging-for-Multi-Source-Domain-Adaptation-in-Person-Re-Identification"><a href="#81-Low-Rank-Expert-Merging-for-Multi-Source-Domain-Adaptation-in-Person-Re-Identification" class="headerlink" title="81. Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Low-Rank_Expert_Merging_for_Multi-Source_Domain_Adaptation_in_Person_Re-Identification.pdf">Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ETS Montreal</span></p><p>该论文提出了一种面向行人再识别的多源无监督域自适应方法SAGE-reID，核心技术为利用低秩适配器(LoRA)对各源模型在目标域进行高效微调，并通过轻量化门控网络动态融合多个LoRA专家，实现高效跨域知识转移。实验结果表明，该方法在Market-1501、DukeMTMC-reID和MSMT17等多个基准数据集上，在准确率和效率上均优于现有主流多源域自适应方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Low-Rank_Expert_Merging_for_Multi-Source_Domain_Adaptation_in_Person_Re-Identification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-Dual-Resolution-Residual-Architecture-with-Artifact-Suppression-for-Melanocytic-Lesion-Segmentation"><a href="#82-Dual-Resolution-Residual-Architecture-with-Artifact-Suppression-for-Melanocytic-Lesion-Segmentation" class="headerlink" title="82. Dual-Resolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DualResolution_Residual_Architecture_with_Artifact_Suppression_for_Melanocytic_Lesion_Segmentation.pdf">Dual-Resolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">SNAIT Research</span></p><p>本文提出了一种用于皮肤镜图像黑色素瘤分割的双分辨率残差网络架构，结合全分辨率边界保留流和多尺度上下文聚合流，通过边界感知残差连接和通道注意力模块实现精确分割。实验表明，该方法在公开数据集上明显提升了分割边界的准确性和鲁棒性，优于标准FCN基线，具备临床应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Dual-Resolution_Residual_Architecture_with_Artifact_Suppression_for_Melanocytic_Lesion_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-Mode-Aware-Non-Linear-Tucker-Autoencoder-for-Tensor-based-Unsupervised-Learning"><a href="#83-Mode-Aware-Non-Linear-Tucker-Autoencoder-for-Tensor-based-Unsupervised-Learning" class="headerlink" title="83. Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Mode-Aware_Non-Linear_Tucker_Autoencoder_for_Tensor-based_Unsupervised_Learning.pdf">Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了一种新的模式感知非线性Tucker自动编码器（MA-NTAE），通过递归Pick-Unfold-Encode-Fold操作，将经典Tucker分解与现代自动编码器相结合，实现高阶张量的灵活模式编码。实验结果显示，MA-NTAE在张量压缩和聚类任务中，相比传统向量化自动编码器（DAE）和线性Tucker网络（TFNN），在多视角图像和视频数据上的重建精度更高，训练效率更优，能更好地提取和保留样本结构。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Mode-Aware_Non-Linear_Tucker_Autoencoder_for_Tensor-based_Unsupervised_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-FoundBioNet-A-Foundation-Based-Model-for-IDH-Genotyping-of-Glioma-from-Multi-Parametric-MRI"><a href="#84-FoundBioNet-A-Foundation-Based-Model-for-IDH-Genotyping-of-Glioma-from-Multi-Parametric-MRI" class="headerlink" title="84. FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FoundBioNet__A_Foundation-Based_Model_for_IDH_Genotyping_of_Glioma_from_Multi-Parametric_MRI.pdf">FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tehran University of Medical Sciences</span></p><p>该论文提出了FoundBioNet模型，基于SWIN-UNETR架构，结合肿瘤感知特征编码（TAFE）和跨模态差异（CMD）模块，从多参数MRI中无创预测胶质瘤IDH突变状态。模型在多中心大样本数据集上表现出优越的泛化能力和准确率，显著超过传统卷积和Transformer基线方法，提升了诊断准确性和解释性，有望集成于临床流程。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/FoundBioNet_A_Foundation-Based_Model_for_IDH_Genotyping_of_Glioma_from_Multi-Parametric_MRI.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-FormCoach-Lift-Smarter-Not-Harder"><a href="#85-FormCoach-Lift-Smarter-Not-Harder" class="headerlink" title="85. FormCoach: Lift Smarter, Not Harder"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FormCoach__Lift_Smarter,_Not_Harder.pdf">FormCoach: Lift Smarter, Not Harder</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Pennsylvania</span></p><p>本文提出FormCoach系统，利用视觉-语言模型（VLM）通过摄像头实时对用户健身动作与专家演示进行对比分析，生成个性化纠正反馈。作者构建并公开了1700对专业标注的视频数据集，系统性评测多种VLM模型在运动姿态纠正任务上的表现。结果显示，当前VLM能提供高可操作性但在精准识别和描述细微姿态错误方面与人类教练仍有差距，未来结合3D关节估计和可穿戴传感器有望提升准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/FormCoach_Lift_Smarter%2C_Not_Harder.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-AURA-A-Fine-Grained-Benchmark-and-Decomposed-Metric-for-Audio-Visual-Reasoning"><a href="#86-AURA-A-Fine-Grained-Benchmark-and-Decomposed-Metric-for-Audio-Visual-Reasoning" class="headerlink" title="86. AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AURA__A_Fine-Grained_Benchmark_and_Decomposed_Metric_for_Audio-Visual_Reasoning.pdf">AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Maryland, College Park</span></p><p>本文提出AURA基准，专为评估音频-视觉大语言模型（AV-LLMs）和全模态语言模型（OLMs）在六类细粒度认知任务上的跨模态推理能力，包括因果推理、音色&#x2F;音高、节奏&#x2F;同步、不可回答性、隐性干扰和表演者技能分析。其创新点在于AuraScore评估指标，将模型推理分解为事实一致性与核心推理两部分，从而揭示现有模型在正确率与推理质量间的显著差距。结论显示，主流AV-LLM在答案准确率高但推理一致性较低，强调了更细致的多模态推理评价的必要性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AURA_A_Fine-Grained_Benchmark_and_Decomposed_Metric_for_Audio-Visual_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-Freeze-and-Reveal-Exposing-Modality-Bias-in-Vision-Language-Models"><a href="#87-Freeze-and-Reveal-Exposing-Modality-Bias-in-Vision-Language-Models" class="headerlink" title="87. Freeze and Reveal: Exposing Modality Bias in Vision-Language Models"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Freeze_and_Reveal__Exposing_Modality_Bias_in_Vision-Language_Models.pdf">Freeze and Reveal: Exposing Modality Bias in Vision-Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IIIT Hyderabad</span></p><p>本文提出了一种针对视觉-语言模型中性别偏见的模态定向去偏框架，利用Counterfactual Data Augmentation（CDA）、Task Vector和新提出的Degree of Stereotypicality（DoS）及其数据增强方法（DAUDoS）对视觉和文本编码器分别进行去偏，并在VisoGender基准和自己整理的性别标注数据集上评估。实验表明，CLIP模型的视觉编码器偏见更重，PaliGemma2的文本编码器偏见更重，所提方法能有效降低偏见且保持模型性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Freeze_and_Reveal_Exposing_Modality_Bias_in_Vision-Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-CLUE-Leveraging-Low-Rank-Adaptation-to-Capture-Latent-Uncovered-Evidence-for-Image-Forgery-Localization"><a href="#88-CLUE-Leveraging-Low-Rank-Adaptation-to-Capture-Latent-Uncovered-Evidence-for-Image-Forgery-Localization" class="headerlink" title="88. CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CLUE__Leveraging_Low-Rank_Adaptation_to_Capture_Latent_Uncovered_Evidence_for_Image_Forgery_Localiza.pdf">CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Guangdong Provincial Key Laboratory of Intelligent Information Processing, Shenzhen University</span></p><p>本论文提出CLUE框架，通过低秩适应（LoRA）高效微调Stable Diffusion 3（SD3）和Segment Anything Model（SAM），将生成模型的内部生成过程用于图像伪造定位。方法采用SD3的Rectified Flow机制对潜变量引入多级噪声，放大伪造区域的微小不一致性，并融合SAM提取的空间语义特征，最终实现像素级伪造区域检测。实验显示，CLUE在多项公开基准上显著优于现有方法，具备更强泛化性和对社交网络压缩等后处理攻击的鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CLUE_Leveraging_Low-Rank_Adaptation_to_Capture_Latent_Uncovered_Evidence_for_Image_Forgery_Localization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-VA-Blueprint-Uncovering-Building-Blocks-for-Visual-Analytics-System-Design"><a href="#89-VA-Blueprint-Uncovering-Building-Blocks-for-Visual-Analytics-System-Design" class="headerlink" title="89. VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VA-Blueprint__Uncovering_Building_Blocks_for_Visual_Analytics_System_Design.pdf">VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Chicago</span></p><p>该论文提出了一种结合人工与大语言模型（LLM）的方法，系统性地从101篇城市视觉分析（VA）系统论文中抽取、归纳并结构化其核心构建模块，形成多层次知识库（VA-Blueprint）。方法包括人工手动分析建立初步代码本体，随后用LLM批量抽取其余论文组件，最终生成可查询、层次化、可视化的系统组件蓝图。结论显示，VA-Blueprint不仅揭示了VA系统复杂性随时间增长的趋势，还为系统设计、复用和分析提供结构化基础，LLM在抽取环节表现良好但需专家校正。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/VA-Blueprint_Uncovering_Building_Blocks_for_Visual_Analytics_System_Design.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-ForensicsSAM-Toward-Robust-and-Unified-Image-Forgery-Detection-and-Localization-Resisting-to-Adversarial-Attack"><a href="#90-ForensicsSAM-Toward-Robust-and-Unified-Image-Forgery-Detection-and-Localization-Resisting-to-Adversarial-Attack" class="headerlink" title="90. ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ForensicsSAM__Toward_Robust_and_Unified_Image_Forgery_Detection_and_Localization_Resisting_to_Advers.pdf">ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen University</span></p><p>该论文提出了ForensicsSAM，一种针对图像伪造检测与定位（IFDL）任务的统一框架，通过在SAM主干中注入共享伪造专家、对抗检测器和自适应对抗专家，形成三阶段训练流程，实现了对真实、伪造及对抗样本的图像级检测和像素级定位。实验显示，ForensicsSAM在多种数据集和对抗攻击下均具备优异的鲁棒性和SOTA伪造检测性能，有效提升了安全性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ForensicsSAM_Toward_Robust_and_Unified_Image_Forgery_Detection_and_Localization_Resisting_to_Adversarial_Attack.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-Tight-Bounds-for-Schrodinger-Potential-Estimation-in-Unpaired-Image-to-Image-Translation-Problems"><a href="#91-Tight-Bounds-for-Schrodinger-Potential-Estimation-in-Unpaired-Image-to-Image-Translation-Problems" class="headerlink" title="91. Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Tight_Bounds_for_Schr%C3%B6dinger_Potential_Estimation_in_Unpaired_Image-to-Image_Translation_Problems.pdf">Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">HSE University</span></p><p>本文针对无配对样本的图像到图像翻译问题，提出了一种基于Schrödinger桥和Ornstein-Uhlenbeck过程的生成建模方法，通过最小化耦合的KL散度，在高概率下给出了经验风险最小化器的泛化误差收敛界，并实现了近乎最优的收敛速率。实验证明该方法在生成建模和图像风格迁移任务中稳定优于传统方法，能够更好地保持源分布的关键属性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Tight_Bounds_for_Schr%C3%B6dinger_Potential_Estimation_in_Unpaired_Image-to-Image_Translation_Problems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-Invert4TVG-A-Temporal-Video-Grounding-Framework-with-Inversion-Tasks-for-Enhanced-Action-Understanding"><a href="#92-Invert4TVG-A-Temporal-Video-Grounding-Framework-with-Inversion-Tasks-for-Enhanced-Action-Understanding" class="headerlink" title="92. Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Invert4TVG__A_Temporal_Video_Grounding_Framework_with_Inversion_Tasks_for_Enhanced_Action_Understand.pdf">Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xiamen University</span></p><p>本文提出Invert4TVG框架，通过引入三种反向任务（动词补全、动作识别、视频描述）将原有的Temporal Video Grounding任务反转，增强模型对视频动作语义理解。方法整合强化学习，动态平衡主任务与反向任务训练，显著提升时序视频定位与动作理解精度，实验在Charades-STA等数据集上超越现有SOTA方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Invert4TVG_A_Temporal_Video_Grounding_Framework_with_Inversion_Tasks_for_Enhanced_Action_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-KLASSify-to-Verify-Audio-Visual-Deepfake-Detection-Using-SSL-based-Audio-and-Handcrafted-Visual-Features"><a href="#93-KLASSify-to-Verify-Audio-Visual-Deepfake-Detection-Using-SSL-based-Audio-and-Handcrafted-Visual-Features" class="headerlink" title="93. KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/KLASSify_to_Verify__Audio-Visual_Deepfake_Detection_Using_SSL-based_Audio_and_Handcrafted_Visual_Fea.pdf">KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KLASS Engineering and Solutions</span></p><p>该论文提出了KLASSify系统，通过结合自监督学习（SSL）音频特征与手工视觉特征实现高效的音视频深度伪造检测与定位。方法上，视频采用基于脸部区域的手工特征与轻量级时序卷积网络（TCN），音频采用Wav2Vec-AASIST与图注意力网络（GAT），最终融合音视频分数并校准，定位任务则在音频端引入边界感知注意力机制。实验结果表明，该系统在AV-Deepfake1M++数据集上深度伪造分类AUC达到92.78%，音频单模态定位IoU为0.3536，方法兼具可解释性和泛化性，适合实际部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/KLASSify_to_Verify_Audio-Visual_Deepfake_Detection_Using_SSL-based_Audio_and_Handcrafted_Visual_Features.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-RORPCap-Retrieval-based-Objects-and-Relations-Prompt-for-Image-Captioning"><a href="#94-RORPCap-Retrieval-based-Objects-and-Relations-Prompt-for-Image-Captioning" class="headerlink" title="94. RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/RORPCap__Retrieval-based_Objects_and_Relations_Prompt_for_Image_Captioning.pdf">RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yunnan University</span></p><p>该论文提出了一种名为RORPCap的新型图像描述方法，利用对象与关系提取模型（OREM）从检索到的相关文本中抽取关键的对象和关系词，并将其填充到预定义的提示模板中，经GPT-2编码获得提示嵌入。再通过Mamba映射网络将CLIP提取的图像特征快速桥接到视觉-文本嵌入，最后将两者拼接后的特征输入GPT-2生成描述。实验显示，在MS-COCO和nocaps数据集上，RORPCap无需检测器和GCN即可达到与最先进模型相当的准确率，并显著缩短训练时间，具备优良的泛化与零样本能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/RORPCap_Retrieval-based_Objects_and_Relations_Prompt_for_Image_Captioning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-BEVANet-Bilateral-Efficient-Visual-Attention-Network-for-Real-Time-Semantic-Segmentation"><a href="#95-BEVANet-Bilateral-Efficient-Visual-Attention-Network-for-Real-Time-Semantic-Segmentation" class="headerlink" title="95. BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BEVANet__Bilateral_Efficient_Visual_Attention_Network_for_Real-Time_Semantic_Segmentation.pdf">BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Taiwan University</span></p><p>本论文提出了一种高效的语义分割网络BEVANet，核心方法包括稀疏分解大可分离核注意力（SDLSKA）、综合核选择（CKS）、深层大核金字塔池化（DLKPPM）、双分支架构和边界引导自适应融合（BGAF）模块，实现多尺度特征融合、动态感受野调整和边界精细化。实验表明，BEVANet在Cityscapes和CamVid数据集上以低计算量实现了SOTA的分割精度和实时推理速度，验证了其在速度与精度间的优越平衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BEVANet_Bilateral_Efficient_Visual_Attention_Network_for_Real-Time_Semantic_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-Understanding-Dynamic-Scenes-in-Ego-Centric-4D-Point-Clouds"><a href="#96-Understanding-Dynamic-Scenes-in-Ego-Centric-4D-Point-Clouds" class="headerlink" title="96. Understanding Dynamic Scenes in Ego Centric 4D Point Clouds"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Understanding_Dynamic_Scenes_in_Ego_Centric_4D_Point_Clouds.pdf">Understanding Dynamic Scenes in Ego Centric 4D Point Clouds</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了EgoDynamic4D，一个面向动态4D场景理解的首创问答基准，涵盖927K对QA任务，支持物体运动、人与物体交互等12类时空推理任务。作者设计了端到端时空推理框架，采用实例感知编码、时间与相机嵌入及适应性降采样，将大规模4D场景压缩为LLM可处理的序列，在EgoDynamic4D数据集上验证了其多模态时序建模的有效性。实验结果显示，该方法在动态场景理解任务上显著优于现有基线，提升了4D场景推理的精度和可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Understanding_Dynamic_Scenes_in_Ego_Centric_4D_Point_Clouds.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-HaDM-ST-Histology-Assisted-Differential-Modeling-for-Spatial-Transcriptomics-Generation"><a href="#97-HaDM-ST-Histology-Assisted-Differential-Modeling-for-Spatial-Transcriptomics-Generation" class="headerlink" title="97. HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HaDM-ST__Histology-Assisted_Differential_Modeling_for_Spatial_Transcriptomics_Generation.pdf">HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cambridge</span></p><p>该论文提出HaDM-ST框架，结合H&amp;E染色组织切片和低分辨率空间转录组（ST）数据，通过扩散模型生成高分辨率ST图。方法包括H&amp;E驱动的语义提取模块（HSD）、跨模态空间对齐模块（CMSA）、以及基于图神经网络的基因差异对抗学习模块（GDAL），有效提取表达相关形态特征并实现空间精细对齐和基因通道建模。实验在多数据集多基因上，HaDM-ST在结构相似性和RMSE等指标均优于现有方法，生成的高分辨率ST图具有更高空间保真度和基因表达准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/HaDM-ST_Histology-Assisted_Differential_Modeling_for_Spatial_Transcriptomics_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-Unsupervised-Real-World-Super-Resolution-via-Rectified-Flow-Degradation-Modelling"><a href="#98-Unsupervised-Real-World-Super-Resolution-via-Rectified-Flow-Degradation-Modelling" class="headerlink" title="98. Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Unsupervised_Real-World_Super-Resolution_via_Rectified_Flow_Degradation_Modelling.pdf">Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology Beijing</span></p><p>该论文提出了一种基于整流流（Rectified Flow）和傅里叶先验引导模块（FGDM）的无监督真实世界超分辨率方法。方法通过整流流退化建模（RFDM）和傅里叶相位结构引导，生成与真实降质分布一致的训练对，提高超分辨率网络在真实场景下的泛化能力。实验结果显示，该方法在多个真实世界数据集上显著优于现有合成训练对生成方法，提升了超分辨率模型的重建质量和细节还原能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Unsupervised_Real-World_Super-Resolution_via_Rectified_Flow_Degradation_Modelling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-SODiff-Semantic-Oriented-Diffusion-Model-for-JPEG-Compression-Artifacts-Removal"><a href="#99-SODiff-Semantic-Oriented-Diffusion-Model-for-JPEG-Compression-Artifacts-Removal" class="headerlink" title="99. SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SODiff__Semantic-Oriented_Diffusion_Model_for_JPEG_Compression_Artifacts_Removal.pdf">SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong Univercity</span></p><p>本文提出SODiff，一种面向语义的单步扩散模型，专注于JPEG压缩伪影去除。核心方法包括语义对齐的图像提示提取器（SAIPE）与质量因子感知步长预测器，通过提取图像嵌入并与文本语义空间对齐，以及自适应选择扩散步长，提升高压缩条件下的图像细节恢复能力。实验表明，SODiff在多项定量和视觉指标上均优于现有主流方法，尤其在极端压缩下能有效还原丢失结构与细节，具有较强的鲁棒性和实际应用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SODiff_Semantic-Oriented_Diffusion_Model_for_JPEG_Compression_Artifacts_Removal.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-CoAR-Concept-Injection-into-Autoregressive-Models-for-Personalized-Text-to-Image-Generation"><a href="#100-CoAR-Concept-Injection-into-Autoregressive-Models-for-Personalized-Text-to-Image-Generation" class="headerlink" title="100. CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CoAR__Concept_Injection_into_Autoregressive_Models_for_Personalized_Text-to-Image_Generation.pdf">CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出了CoAR框架，通过在多模态自回归模型中注入主体和风格概念，实现个性化文本到图像生成。方法采用层级多模态上下文学习、双重先验保持和上下文自正则化，无需微调主干参数，仅优化少量可学习token；支持训练自由的主体-风格组合生成。实验结果表明，CoAR在主体忠实度、指令对齐和风格保存方面超过现有方法，同时参数效率显著提升，训练参数不到0.1M，性能与最新扩散法持平甚至更优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CoAR_Concept_Injection_into_Autoregressive_Models_for_Personalized_Text-to-Image_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="101-Representation-Understanding-via-Activation-Maximization"><a href="#101-Representation-Understanding-via-Activation-Maximization" class="headerlink" title="101. Representation Understanding via Activation Maximization"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Representation_Understanding_via_Activation_Maximization.pdf">Representation Understanding via Activation Maximization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Manchester</span></p><p>该论文提出了一种统一的特征可视化框架，利用Activation Maximization（激活最大化）方法对卷积神经网络（CNNs）和视觉变换器（ViTs）进行中间层和输出层的表征分析，通过在频率域优化，生成更自然且可解释的神经元激活图像，并揭示不同架构间的结构差异。此外，作者还将激活最大化拓展用于生成对抗样本，展示了其在模型脆弱性分析中的有效性。结论认为该方法提升了模型可解释性和泛化能力，并为理解神经网络的表示结构和对抗鲁棒性提供了新视角。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Representation_Understanding_via_Activation_Maximization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-Small-Large-Collaboration-Training-efficient-Concept-Personalization-for-Large-VLM-using-a-Meta-Personalized-Small-VLM"><a href="#102-Small-Large-Collaboration-Training-efficient-Concept-Personalization-for-Large-VLM-using-a-Meta-Personalized-Small-VLM" class="headerlink" title="102. Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Small-Large_Collaboration__Training-efficient_Concept_Personalization_for_Large_VLM_using_a_Meta_Per.pdf">Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出了Small-Large Collaboration (SLC) 框架，通过让小型视觉-语言模型（VLM）负责用户特定信息检测，大型VLM执行推理和反思，有效实现了大模型个性化且训练高效。实验表明，SLC能在不需针对每个用户单独微调大模型的前提下，兼顾个性化、准确性和训练效率，大幅降低训练成本并减少幻觉现象。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Small-Large_Collaboration_Training-efficient_Concept_Personalization_for_Large_VLM_using_a_Meta_Personalized_Small_VLM.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-SUIT-Spatial-Spectral-Union-Intersection-Interaction-Network-for-Hyperspectral-Object-Tracking"><a href="#103-SUIT-Spatial-Spectral-Union-Intersection-Interaction-Network-for-Hyperspectral-Object-Tracking" class="headerlink" title="103. SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SUIT__Spatial-Spectral_Union-Intersection_Interaction_Network_for_Hyperspectral_Object_Tracking.pdf">SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University of Science and Technology</span></p><p>本文提出了一种新颖的空间-光谱联合交互网络（SUIT），用于高光谱视频目标跟踪。方法上，SUIT利用Transformer实现模板与搜索区域在各光谱带间的空间交互，并基于集合论的容斥原理融合各带间的共享与特有空间信息，同时引入光谱损失保证模板与预测区域材料分布一致性。实验表明，该方法在多个高光谱跟踪基准上达到最优或竞争性表现，显著提升了小目标、遮挡、复杂背景等场景下的鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SUIT_Spatial-Spectral_Union-Intersection_Interaction_Network_for_Hyperspectral_Object_Tracking.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-Consistent-and-Controllable-Image-Animation-with-Motion-Linear-Diffusion-Transformers"><a href="#104-Consistent-and-Controllable-Image-Animation-with-Motion-Linear-Diffusion-Transformers" class="headerlink" title="104. Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Consistent_and_Controllable_Image_Animation_with_Motion_Linear_Diffusion_Transformers.pdf">Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Monash University</span></p><p>该论文提出MiraMo模型，通过线性注意力Transformer架构实现高效的图像动画生成。方法包括线性注意力机制、运动残差学习和基于DCT的噪声优化，可提升动画的一致性与运动平滑性，并支持动态度可控。实验结果表明，MiraMo在外观一致性、运动平滑性和推理速度方面优于现有方法，同时易于扩展到视频编辑和运动迁移等任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Consistent_and_Controllable_Image_Animation_with_Motion_Linear_Diffusion_Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-Bridging-Semantic-Logic-Gaps-A-Cognition-Inspired-Multimodal-Boundary-Preserving-Network-for-Image-Manipulation-Localization"><a href="#105-Bridging-Semantic-Logic-Gaps-A-Cognition-Inspired-Multimodal-Boundary-Preserving-Network-for-Image-Manipulation-Localization" class="headerlink" title="105. Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Bridging_Semantic_Logic_Gaps__A_Cognition-Inspired_Multimodal_Boundary-Preserving_Network_for_Image_.pdf">Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xinjiang University</span></p><p>本文提出了一种认知启发的多模态边界保留网络（CMB-Net），用于图像篡改区域的精准定位。该方法结合大语言模型（LLMs）生成的文本提示与视觉特征，并引入图像-文本中心歧义模块（ITCAM）抑制幻觉文本的影响、图像-文本交互模块（ITIM）实现细粒度特征融合，以及基于可逆神经网络的边界恢复解码器（RED）无损保留篡改边界信息。实验结果表明，CMB-Net在多个公开数据集上显著优于现有主流方法，尤其在复杂场景下具有更高的鲁棒性和准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Bridging_Semantic_Logic_Gaps_A_Cognition-Inspired_Multimodal_Boundary-Preserving_Network_for_Image_Manipulation_Localization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-Intention-Aware-Diffusion-Model-for-Pedestrian-Trajectory-Prediction"><a href="#106-Intention-Aware-Diffusion-Model-for-Pedestrian-Trajectory-Prediction" class="headerlink" title="106. Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Intention-Aware_Diffusion_Model_for_Pedestrian_Trajectory_Prediction.pdf">Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southern University of Science and Technology</span></p><p>本文提出了一种融合短期与长期意图的扩散模型用于行人轨迹预测。方法上，采用残差极坐标表示建模短期意图，并通过可学习的Token化端点预测器捕获长期多模态目标，同时引入软掩码引导和残差噪声预测模块优化扩散过程。实验结果显示该方法在ETH、UCY和SDD等公开数据集上超越或媲美现有主流方法，显著提升了轨迹预测精度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Intention-Aware_Diffusion_Model_for_Pedestrian_Trajectory_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="107-Large-scale-Multi-sequence-Pretraining-for-Generalizable-MRI-Analysis-in-Versatile-Clinical-Applications"><a href="#107-Large-scale-Multi-sequence-Pretraining-for-Generalizable-MRI-Analysis-in-Versatile-Clinical-Applications" class="headerlink" title="107. Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Large-scale_Multi-sequence_Pretraining_for_Generalizable_MRI_Analysis_in_Versatile_Clinical_Applicat.pdf">Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>该论文提出了PRISM，一种基于Swin Transformer骨干的大规模多序列MRI预训练基础模型。其创新性在于通过解耦解剖结构不变特征与序列特异性变化，并结合像素级掩码重建、图像到图像翻译、元数据预测、对比学习等多任务自监督策略，提升模型在多器官多协议MRI分析中的泛化能力和鲁棒性。实验覆盖44个下游任务，PRISM在39项任务中统计学显著优于当前主流方法，尤其在分割、分类、回归、配准和医学报告生成等多应用场景下表现突出。结论：PRISM能跨MRI协议稳定泛化，提升临床适用性和AI翻译潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Large-scale_Multi-sequence_Pretraining_for_Generalizable_MRI_Analysis_in_Versatile_Clinical_Applications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-SketchAnimator-Animate-Sketch-via-Motion-Customization-of-Text-to-Video-Diffusion-Models"><a href="#108-SketchAnimator-Animate-Sketch-via-Motion-Customization-of-Text-to-Video-Diffusion-Models" class="headerlink" title="108. SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SketchAnimator__Animate_Sketch_via_Motion_Customization_of_Text-to-Video_Diffusion_Models.pdf">SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing University of Posts and Telecommunications</span></p><p>该论文提出SketchAnimator方法，通过三阶段流程（外观学习、运动学习和视频先验蒸馏），结合LoRA微调和可微Bezier曲线渲染，实现输入草图根据参考视频自定义动作的动画生成。实验表明，该方法显著提升草图外观保真与运动对齐能力，优于现有运动迁移与视频生成方法，拓展了创意动画生成的可能性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SketchAnimator_Animate_Sketch_via_Motion_Customization_of_Text-to-Video_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-Perceptual-Evaluation-of-GANs-and-Diffusion-Models-for-Generating-X-rays"><a href="#109-Perceptual-Evaluation-of-GANs-and-Diffusion-Models-for-Generating-X-rays" class="headerlink" title="109. Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Perceptual_Evaluation_of_GANs_and_Diffusion_Models_for_Generating_X-rays.pdf">Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Pontificia Universidad Católica de Chile</span></p><p>该论文针对医疗影像领域，比较了GANs与Diffusion Models在生成胸部X光片（包含四种异常情况）上的表现，通过专家（放射科医生）参与的双重评估任务，分析了模型生成图像的真实感与条件准确性。研究发现Diffusion Models整体生成更真实，但GANs在某些特定条件下（如无心脏轮廓扩大的情况）表现更佳，揭示了模型在医疗应用中尚未完全解决真实感与医学相关性的问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Perceptual_Evaluation_of_GANs_and_Diffusion_Models_for_Generating_X-rays.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="110-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning"><a href="#110-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning" class="headerlink" title="110. Cut2Next: Generating Next Shot via In-Context Tuning"></a>110. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Cut2Next__Generating_Next_Shot_via_In-Context_Tuning.pdf">Cut2Next: Generating Next Shot via In-Context Tuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong</span></p><p>本论文提出了Next Shot Generation (NSG)任务，旨在生成符合专业剪辑模式且保持电影级连贯性的高质量后续镜头。方法上，Cut2Next框架基于Diffusion Transformer，采用分层多提示策略（Hierarchical Multi-Prompting），并通过上下文感知条件注入（CACI）和分层注意力掩码（HAM）实现视觉与文本信号的高效融合。实验结果表明Cut2Next在视觉一致性、文本忠实度及用户偏好上优于现有方法，能有效生成叙事性强、剪辑合理的后续镜头。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Cut2Next_Generating_Next_Shot_via_In-Context_Tuning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="111-THAT-Token-wise-High-frequency-Augmentation-Transformer-for-Hyperspectral-Pansharpening"><a href="#111-THAT-Token-wise-High-frequency-Augmentation-Transformer-for-Hyperspectral-Pansharpening" class="headerlink" title="111. THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening"></a>111. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/THAT__Token-wise_High-frequency_Augmentation_Transformer_for_Hyperspectral_Pansharpening.pdf">THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">JPMorgan Chase</span></p><p>该论文提出了Token-wise High-frequency Augmentation Transformer (THAT)用于高光谱图像融合（pansharpening），通过引入Pivotal Token Selective Attention (PTSA)模块动态筛选有效特征token并抑制冗余，以及Multi-level Variance-aware Feed-forward Network (MVFN)层以增强高频细节建模。实验结果显示，THAT在多个公开数据集和不同放大倍数下均达到最先进的重建质量和效率，显著提升了高光谱图像的空间分辨率和光谱保真度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/THAT_Token-wise_High-frequency_Augmentation_Transformer_for_Hyperspectral_Pansharpening.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="112-Mitigating-Biases-in-Surgical-Operating-Rooms-with-Geometry"><a href="#112-Mitigating-Biases-in-Surgical-Operating-Rooms-with-Geometry" class="headerlink" title="112. Mitigating Biases in Surgical Operating Rooms with Geometry"></a>112. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Mitigating_Biases_in_Surgical_Operating_Rooms_with_Geometry.pdf">Mitigating Biases in Surgical Operating Rooms with Geometry</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">TU Munich</span></p><p>本论文通过对两个人工智能手术室数据集的GradCAM可视化分析发现，传统RGB图像的CNN模型在人员再识别任务中容易陷入依赖鞋子、眼镜等偶然视觉特征的偏差，难以泛化到真实手术环境。作者提出将手术人员建模为3D点云序列，通过捕捉几何属性（如身形、步态和动作模式）有效减少视觉偏差，实验证明在真实手术室数据集上点云方法比RGB方法准确率高12%。结论是几何特征对抗视觉偏差、提升人员再识别准确率具有明显优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Mitigating_Biases_in_Surgical_Operating_Rooms_with_Geometry.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="113-StableAvatar-Infinite-Length-Audio-Driven-Avatar-Video-Generation"><a href="#113-StableAvatar-Infinite-Length-Audio-Driven-Avatar-Video-Generation" class="headerlink" title="113. StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation"></a>113. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/StableAvatar__Infinite-Length_Audio-Driven_Avatar_Video_Generation.pdf">StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出StableAvatar，一种端到端视频扩散Transformer，可根据参考图像和音频生成无限长度、高质量且身份一致的虚拟人视频。其方法创新性地引入了时间步感知音频适配器以减少长视频中的潜在分布误差积累，并在推理阶段采用音频本地引导机制和动态加权滑动窗口策略提升音频同步及视频平滑度。实验表明，StableAvatar在身份保持、音频同步和视频质量方面显著优于现有方法，即使生成数千帧后依然保持高一致性和自然度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/StableAvatar_Infinite-Length_Audio-Driven_Avatar_Video_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="114-VGGSounder-Audio-Visual-Evaluations-for-Foundation-Models"><a href="#114-VGGSounder-Audio-Visual-Evaluations-for-Foundation-Models" class="headerlink" title="114. VGGSounder: Audio-Visual Evaluations for Foundation Models"></a>114. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VGGSounder__Audio-Visual_Evaluations_for_Foundation_Models.pdf">VGGSounder: Audio-Visual Evaluations for Foundation Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Munich</span></p><p>本文提出VGGSounder数据集，通过对现有VGGSound音视频分类测试集进行多标签和多模态（可听、可见、可听且可见）人工重标注，补充了原数据集中标签不全、类别重叠、模态错配等问题，实现对音视频基础模型更精细的能力评估。实验以多种主流音视频基础模型为对象，利用细致的模态评估和引入的模态混淆新指标，揭示模型多模态融合中的不足，并指出当前基础模型更依赖视觉信息，强调了VGGSounder在多模态模型评测中的重要价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/VGGSounder_Audio-Visual_Evaluations_for_Foundation_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="115-SAGOnline-Segment-Any-Gaussians-Online"><a href="#115-SAGOnline-Segment-Any-Gaussians-Online" class="headerlink" title="115. SAGOnline: Segment Any Gaussians Online"></a>115. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SAGOnline__Segment_Any_Gaussians_Online.pdf">SAGOnline: Segment Any Gaussians Online</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Waterloo</span></p><p>本文提出SAGOnline，一种面向3D高斯场的轻量级、零样本、实时3D分割框架。方法创新性地将3D分割任务解耦为基于视频基础模型（如SAM 2）的2D视图一致分割和GPU加速的3D掩码生成，并为每个高斯原语赋予唯一标识，实现了跨视角的多实例分割与追踪。实验在NVOS和Spin-NeRF数据集上实现了最新SOTA性能（92.7%&#x2F;95.2%mIoU），推理速度比现有方法快15–1500倍。结论是SAGOnline实现了高效、准确、实时的3D多目标分割与跟踪，适用于AR&#x2F;VR和机器人等场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SAGOnline_Segment_Any_Gaussians_Online.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="116-KARMA-Efficient-Structural-Defect-Segmentation-via-Kolmogorov-Arnold-Representation-Learning"><a href="#116-KARMA-Efficient-Structural-Defect-Segmentation-via-Kolmogorov-Arnold-Representation-Learning" class="headerlink" title="116. KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning"></a>116. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/KARMA__Efficient_Structural_Defect_Segmentation_via_Kolmogorov-Arnold_Representation_Learning.pdf">KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of New Orleans</span></p><p>该论文提出了KARMA架构，将Kolmogorov-Arnold表示学习（KAN）与优化的特征金字塔网络（FPN）结合，采用低秩Tiny KAN模块、分离卷积和静动态原型机制以提升结构性缺陷分割的效率与准确性。实验表明，KARMA在CSDD和S2DS结构缺陷数据集上以远低于主流模型的参数量（0.959M，减少97%）实现了同等或更优mIoU性能，适用于实时部署，验证了其高效性和实用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/KARMA_Efficient_Structural_Defect_Segmentation_via_Kolmogorov-Arnold_Representation_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="117-RedDino-A-Foundation-Model-for-Red-Blood-Cell-Analysis"><a href="#117-RedDino-A-Foundation-Model-for-Red-Blood-Cell-Analysis" class="headerlink" title="117. RedDino: A Foundation Model for Red Blood Cell Analysis"></a>117. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/RedDino__A_foundation_model_for_red_blood_cell_analysis.pdf">RedDino: A Foundation Model for Red Blood Cell Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cagliari</span></p><p>本文提出RedDino，一种专为红细胞图像分析设计的自监督基础模型，基于DINOv2框架并结合大规模红细胞图像数据集，通过定制数据增强和去除Koleo正则项等技术优化模型结构。实验表明，RedDino在红细胞分类和特征泛化能力方面显著优于现有SOTA方法，具备强大的跨数据源适应性和临床应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/RedDino_A_Foundation_Model_for_Red_Blood_Cell_Analysis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="118-MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision"><a href="#118-MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision" class="headerlink" title="118. MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision"></a>118. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MedReasoner__Reinforcement_Learning_Drives_Reasoning_Grounding_from_Clinical_Thought_to_Pixel-Level_.pdf">MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing University of Posts and Telecommunications</span></p><p>该论文提出了统一医学推理定位（UMRG）任务，并发布了U-MRG-14K数据集，将隐式临床语言推理与像素级图像分割结合。作者设计了MedReasoner框架，采用模块化结构，将多模态大语言模型（MLLM）为推理模块、冻结分割专家为分割模块，通过强化学习（RL）优化推理与空间对齐，实现了隐式临床查询到精确空间定位的映射。结果显示，MedReasoner在U-MRG-14K数据集上实现了当前最优的分割与定位精度，并具有较强的泛化能力，证明了RL在医学图像可解释定位中的优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MedReasoner_Reinforcement_Learning_Drives_Reasoning_Grounding_from_Clinical_Thought_to_Pixel-Level_Precision.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="119-Integrating-Task-Specific-and-Universal-Adapters-for-Pre-Trained-Model-based-Class-Incremental-Learning"><a href="#119-Integrating-Task-Specific-and-Universal-Adapters-for-Pre-Trained-Model-based-Class-Incremental-Learning" class="headerlink" title="119. Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning"></a>119. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Integrating_Task-Specific_and_Universal_Adapters_for_Pre-Trained_Model-based_Class-Incremental_Learn.pdf">Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>该论文提出了一种集成任务特定适配器与通用适配器（TUNA）的增量学习方法，利用预训练视觉变换器（ViT）冻结主干，仅训练轻量级适配器以适应新任务。通过正交损失训练任务特定适配器，并引入基于熵的选择机制动态选择最优适配器，进一步将多任务适配器融合为通用适配器以捕获跨任务的共享特征，推理时结合两者输出。实验在CIFAR100、ImageNet-R&#x2F;A和ObjectNet等多个基准上取得了SOTA性能，验证了该方法对抗灾难性遗忘和提升泛化能力的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Integrating_Task-Specific_and_Universal_Adapters_for_Pre-Trained_Model-based_Class-Incremental_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="120-FantasyStyle-Controllable-Stylized-Distillation-for-3D-Gaussian-Splatting"><a href="#120-FantasyStyle-Controllable-Stylized-Distillation-for-3D-Gaussian-Splatting" class="headerlink" title="120. FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting"></a>120. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FantasyStyle__Controllable_Stylized_Distillation_for_3D_Gaussian_Splatting.pdf">FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computing and Artificial Intelligence, Shanghai University of Finance and Economics</span></p><p>该论文提出FantasyStyle框架，专注于3D高斯撒点(3DGS)的风格迁移，通过完全依赖扩散模型蒸馏实现3D风格转化。方法包括多视角频率一致性（MVFC），利用3D频率域滤波提升视角一致性，以及可控风格化蒸馏（CSD），结合负向引导防止内容泄露并优化3D场景。实验显示，FantasyStyle在风格化质量和内容保留方面均优于现有方法，推动2D风格迁移技术向3D领域的灵活扩展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/FantasyStyle_Controllable_Stylized_Distillation_for_3D_Gaussian_Splatting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="121-Hyperspectral-Imaging"><a href="#121-Hyperspectral-Imaging" class="headerlink" title="121. Hyperspectral Imaging"></a>121. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hyperspectral_Imaging.pdf">Hyperspectral Imaging</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southeast University</span></p><p>本文全面综述了高光谱成像（HSI）的物理原理、传感器架构、数据采集与校准流程，并系统梳理了图像恢复、增强、降维、分类、光谱解混等关键分析方法，重点介绍了深度学习等AI驱动的新技术及其在地球观测、精密农业、生物医学、工业质检、文化遗产与安全防御等领域的代表性应用。结论指出，HSI正由实验转向大规模现实应用，未来将朝向可扩展、实时、嵌入式系统发展，AI基础模型和跨模态融合等新范式将极大拓展其科学与社会影响力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Hyperspectral_Imaging.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="122-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation"><a href="#122-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation" class="headerlink" title="122. Matrix-3D: Omnidirectional Explorable 3D World Generation"></a>122. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Matrix-3D__Omnidirectional_Explorable_3D_World_Generation.pdf">Matrix-3D: Omnidirectional Explorable 3D World Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Skywork AI</span></p><p>Matrix-3D提出了一个基于全景视频扩散模型和三维重建的新方法，可从单张图片或文本生成可自由探索的全方位3D世界。其核心流程包括利用场景网格渲染做轨迹指导的全景视频生成，再通过优化或大模型推理实现高质量与高效的3D重建。实验结果显示，该方法在全景视频生成和3D世界重建任务上均达到了当前最优性能，支持更广泛的空间探索与应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Matrix-3D_Omnidirectional_Explorable_3D_World_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="123-BadPromptFL-A-Novel-Backdoor-Threat-to-Prompt-based-Federated-Learning-in-Multimodal-Models"><a href="#123-BadPromptFL-A-Novel-Backdoor-Threat-to-Prompt-based-Federated-Learning-in-Multimodal-Models" class="headerlink" title="123. BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models"></a>123. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BadPromptFL__A_Novel_Backdoor_Threat_to_Prompt-based_Federated_Learning_in_Multimodal_Models.pdf">BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Dalian University of Technology</span></p><p>本文提出BadPromptFL，一种针对多模态对比模型中基于prompt的联邦学习的新型后门攻击方法。该方法通过恶意客户端联合优化本地后门触发器和prompt嵌入，在联邦聚合过程中注入隐蔽的后门prompt，使后门行为在推理时普遍激活且不影响模型参数。实验结果表明，BadPromptFL能在多种数据集和聚合协议下以极高的攻击成功率 (&gt;90%) 实现后门注入，同时对正常任务性能影响极小，揭示了prompt聚合在真实部署中的安全隐患。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BadPromptFL_A_Novel_Backdoor_Threat_to_Prompt-based_Federated_Learning_in_Multimodal_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="124-CD-TVD-Contrastive-Diffusion-for-3D-Super-Resolution-with-Scarce-High-Resolution-Time-Varying-Data"><a href="#124-CD-TVD-Contrastive-Diffusion-for-3D-Super-Resolution-with-Scarce-High-Resolution-Time-Varying-Data" class="headerlink" title="124. CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data"></a>124. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CD-TVD__Contrastive_Diffusion_for_3D_Super-Resolution_with_Scarce_High-Resolution_Time-Varying_Data.pdf">CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tianjin University</span></p><p>该论文提出了CD-TVD框架，结合对比学习和扩散模型，实现在高分辨率时序数据极度稀缺情况下的3D超分辨率重建。通过历史模拟数据预训练对比编码器和扩散超分辨率模块，模型能够捕获退化特征和细粒度结构，并利用局部注意机制提升重建效率；在新场景下，仅需极少高分辨率样本即可微调模型，实现对所有低分辨率时刻的准确恢复。实验结果表明，CD-TVD在科学仿真等任务下显著优于现有方法，兼顾精细结构恢复和资源效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CD-TVD_Contrastive_Diffusion_for_3D_Super-Resolution_with_Scarce_High-Resolution_Time-Varying_Data.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="125-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control"><a href="#125-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control" class="headerlink" title="125. Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control"></a>125. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Follow-Your-Shape__Shape-Aware_Image_Editing_via_Trajectory-Guided_Region_Control.pdf">Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">HKUST</span></p><p>该论文提出了Follow-Your-Shape，一个训练和掩码无关的形状感知图像编辑框架。方法核心为Trajectory Divergence Map (TDM)，通过对比源与目标文本提示下的去噪轨迹差异，动态定位可编辑区域，并结合分阶段的Key-Value注入与ControlNet结构指导，实现大幅度形状变换同时严格保持非目标内容。提出的ReShapeBench用于系统评测形状编辑能力。实验结果表明该方法在大规模形状替换任务中，编辑精度和视觉保真度均优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Follow-Your-Shape_Shape-Aware_Image_Editing_via_Trajectory-Guided_Region_Control.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="126-MDD-Net-Multimodal-Depression-Detection-through-Mutual-Transformer"><a href="#126-MDD-Net-Multimodal-Depression-Detection-through-Mutual-Transformer" class="headerlink" title="126. MDD-Net: Multimodal Depression Detection through Mutual Transformer"></a>126. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MDD-Net__Multimodal_Depression_Detection_through_Mutual_Transformer.pdf">MDD-Net: Multimodal Depression Detection through Mutual Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Waterloo</span></p><p>本文提出了MDD-Net，一种融合音频与视觉数据的多模态抑郁症检测网络，核心采用互信息Transformer模块实现跨模态特征高效提取与融合。通过在社交媒体D-Vlog数据集上的实验，MDD-Net在F1-Score上相比现有方法有最高17.37%的提升，验证了其在多模态抑郁检测任务中的优越性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MDD-Net_Multimodal_Depression_Detection_through_Mutual_Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="127-Sample-aware-RandAugment-Search-free-Automatic-Data-Augmentation-for-Effective-Image-Recognition"><a href="#127-Sample-aware-RandAugment-Search-free-Automatic-Data-Augmentation-for-Effective-Image-Recognition" class="headerlink" title="127. Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition"></a>127. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sample-aware_RandAugment__Search-free_Automatic_Data_Augmentation_for_Effective_Image_Recognition.pdf">Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Chinese Academy of Sciences</span></p><p>本文提出了Sample-aware RandAugment (SRA)，一种无搜索、样本感知的自动数据增强（AutoDA）方法，核心创新包括：引入启发式评分模块Magnitude Instructor Score (MIS)以动态评估样本难度，并采用非对称增强策略（探索和精炼阶段）对训练批次进行分割与差异化增强，从而提升模型泛化能力。实验表明，SRA在ImageNet（ResNet-50），CIFAR、Food101等多项视觉分类任务上实现了性能超越当前无搜索方法甚至可媲美部分有搜索方法，且无需超参数调优，提升了下游检测任务精度，具备良好兼容性与广泛适用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Sample-aware_RandAugment_Search-free_Automatic_Data_Augmentation_for_Effective_Image_Recognition.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="128-Prompt-Guided-Relational-Reasoning-for-Social-Behavior-Understanding-with-Vision-Foundation-Models"><a href="#128-Prompt-Guided-Relational-Reasoning-for-Social-Behavior-Understanding-with-Vision-Foundation-Models" class="headerlink" title="128. Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models"></a>128. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Prompt-Guided_Relational_Reasoning_for_Social_Behavior_Understanding_with_Vision_Foundation_Models.pdf">Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Stuttgart</span></p><p>该论文提出了ProGraD框架，通过在冻结的视觉基础模型（如DINOv2）上引入可学习的群体提示（group prompts）和轻量化的两层GroupContext Transformer，实现对视频中社会群体行为的高效检测与推理。作者在Cafe和Social-CAD两个数据集上验证了方法的有效性，ProGraD仅用约1000万可训练参数便超过现有方法，尤其在多组场景下表现突出，且具备良好的可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Prompt-Guided_Relational_Reasoning_for_Social_Behavior_Understanding_with_Vision_Foundation_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="129-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation"><a href="#129-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation" class="headerlink" title="129. Omni-Effects: Unified and Spatially Controllable Visual Effects Generation"></a>129. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Omni-Effects__Unified_and_Spatially-Controllable_Visual_Effects_Generation.pdf">Omni-Effects: Unified and Spatially Controllable Visual Effects Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AMAP, Alibaba Group</span></p><p>本文提出了Omni-Effects，一种统一且可空间控制的视觉特效生成框架。方法核心包括LoRA-MoE模块（以专家混合方式减少多特效混合训练时的干扰）和SAP-IIF机制（融合空间掩码与文本提示，实现像素级多特效独立控制），并配套构建Omni-VFX数据集及评测体系。实验表明该方法在单&#x2F;多特效与空间控制方面显著优于现有方案，实现了高保真、多样化且可控的特效视频合成。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Omni-Effects_Unified_and_Spatially_Controllable_Visual_Effects_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="130-Score-Augmentation-for-Diffusion-Models"><a href="#130-Score-Augmentation-for-Diffusion-Models" class="headerlink" title="130. Score Augmentation for Diffusion Models"></a>130. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Score_Augmentation_for_Diffusion_Models.pdf">Score Augmentation for Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Technology</span></p><p>该论文针对扩散模型在数据有限情况下的过拟合问题，提出了Score Augmentation（ScoreAug）数据增强框架。ScoreAug在噪声数据上施加变换，并要求去噪器预测变换后的目标，实现等变学习目标，从而提升模型泛化能力。实验表明，ScoreAug在CIFAR-10、FFHQ、AFHQv2和ImageNet等基准数据集上显著提升了生成性能，有效缓解了过拟合，并可与传统数据增强方法协同提升效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Score_Augmentation_for_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="131-Diffusing-the-Blind-Spot-Uterine-MRI-Synthesis-with-Diffusion-Models"><a href="#131-Diffusing-the-Blind-Spot-Uterine-MRI-Synthesis-with-Diffusion-Models" class="headerlink" title="131. Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models"></a>131. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Diffusing_the_Blind_Spot__Uterine_MRI_Synthesis_with_Diffusion_Models.pdf">Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Friedrich–Alexander University Erlangen–Nürnberg</span></p><p>本文提出了一种结合无条件和条件去噪扩散概率模型（DDPM）及潜在扩散模型（LDM）的2D和3D扩散模型框架，实现了高保真、解剖结构合理的子宫MRI图像合成，同时引入文本和类别条件提升生成控制力，并通过隐私过滤保证数据安全。实验结果显示，合成数据在弱监督和全监督下，分类任务表现优于真实数据，推动了妇科领域的AI发展和隐私保护医学影像数据增广。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Diffusing_the_Blind_Spot_Uterine_MRI_Synthesis_with_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="132-Architectural-Co-Design-for-Zero-Shot-Anomaly-Detection-Decoupling-Representation-and-Dynamically-Fusing-Features-in-CLIP"><a href="#132-Architectural-Co-Design-for-Zero-Shot-Anomaly-Detection-Decoupling-Representation-and-Dynamically-Fusing-Features-in-CLIP" class="headerlink" title="132. Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP"></a>132. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Architectural_Co-Design_for_Zero-Shot_Anomaly_Detection__Decoupling_Representation_and_Dynamically_F.pdf">Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Big Data Institute, Central South University</span></p><p>本文提出了ACD-CLIP架构，通过参数高效的Conv-LoRA适配器为视觉特征注入局部归纳偏置，并采用动态融合网关（DFG）实现视觉引导的跨模态灵活特征融合，提升了基础视觉-语言模型在零样本异常检测任务中的表现。实验结果显示该方法在工业和医学多个基准上显著优于现有方法，表现出更强的鲁棒性和精度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Architectural_Co-Design_for_Zero-Shot_Anomaly_Detection_Decoupling_Representation_and_Dynamically_Fusing_Features_in_CLIP.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="133-MIND-A-Noise-Adaptive-Denoising-Framework-for-Medical-Images-Integrating-Multi-Scale-Transformer"><a href="#133-MIND-A-Noise-Adaptive-Denoising-Framework-for-Medical-Images-Integrating-Multi-Scale-Transformer" class="headerlink" title="133. MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer"></a>133. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MIND__A_Noise-Adaptive_Denoising_Framework_for_Medical_Images_Integrating_Multi-Scale_Transformer.pdf">MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>本文提出了一种集成多尺度Transformer和卷积结构的医学图像自适应去噪模型MIND，结合噪声水平估计器（NLE）和噪声自适应注意力模块（NAAB），实现了通道-空间注意调节和跨模态特征融合。大规模实验表明，MIND在PSNR、SSIM、LPIPS等指标及下游诊断任务中均显著优于主流方法，提升了结构恢复和诊断敏感性，具备强泛化与实际应用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MIND_A_Noise-Adaptive_Denoising_Framework_for_Medical_Images_Integrating_Multi-Scale_Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="134-DiTVR-Zero-Shot-Diffusion-Transformer-for-Video-Restoration"><a href="#134-DiTVR-Zero-Shot-Diffusion-Transformer-for-Video-Restoration" class="headerlink" title="134. DiTVR: Zero-Shot Diffusion Transformer for Video Restoration"></a>134. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DiTVR__Zero-Shot_Diffusion_Transformer_for_Video_Restoration.pdf">DiTVR: Zero-Shot Diffusion Transformer for Video Restoration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Wurzburg</span></p><p>该论文提出DiTVR，一种零样本视频复原框架，结合了扩散Transformer（DiT）、基于光流的时空邻居缓存、轨迹感知注意力和波形引导的一致采样器。方法通过在关键时刻引入光流信息，动态选择相关token并沿运动轨迹对齐特征，有效提升了视频分辨率、去噪和去模糊任务的时空一致性和细节保留。实验结果表明，DiTVR在多个视频恢复基准上取得了新的零样本最优性能，无需任务特定训练即可实现高质量、时序连贯的视频重建，优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DiTVR_Zero-Shot_Diffusion_Transformer_for_Video_Restoration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="135-MambaTrans-Multimodal-Fusion-Image-Translation-via-Large-Language-Model-Priors-for-Downstream-Visual-Tasks"><a href="#135-MambaTrans-Multimodal-Fusion-Image-Translation-via-Large-Language-Model-Priors-for-Downstream-Visual-Tasks" class="headerlink" title="135. MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks"></a>135. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MambaTrans__Multimodal_Fusion_Image_Translation_via_Large_Language_Model_Priors_for_Downstream_Visua.pdf">MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Foshan University</span></p><p>本论文提出MambaTrans，一种融合红外与可见光图像的多模态图像翻译框架，利用多模态大语言模型生成的文本描述和语义分割掩码作为先验，通过多模态状态空间块（MM-SSB）实现文本、掩码与图像的深度语义交互和分布迁移。该方法在无需更改下游检测与分割模型参数的情况下，优化任务感知损失函数，有效提升了多模态融合图像在目标检测和语义分割任务上的性能，实验结果表明其在图像质量和下游任务表现间取得了优异平衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MambaTrans_Multimodal_Fusion_Image_Translation_via_Large_Language_Model_Priors_for_Downstream_Visual_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="136-RSVLM-QA-A-Benchmark-Dataset-for-Remote-Sensing-Vision-Language-Model-based-Question-Answering"><a href="#136-RSVLM-QA-A-Benchmark-Dataset-for-Remote-Sensing-Vision-Language-Model-based-Question-Answering" class="headerlink" title="136. RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering"></a>136. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/RSVLM-QA__A_Benchmark_Dataset_for_Remote_Sensing_Vision_Language_Model-based_Question_Answering.pdf">RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Technology Sydney</span></p><p>本论文提出了RSVLM-QA数据集，采用GPT-4.1大语言模型通过精细化提示工程和双轨注释流程，自动生成遥感影像的丰富描述、空间关系、语义标签及多类型VQA问答对，并联合四个主流遥感数据集，保证注释多样性及数量准确性。结论显示RSVLM-QA能有效挑战和评估当前主流视觉语言模型在遥感领域的理解与推理能力，是推动遥感视觉语言模型研究的重要基准。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/RSVLM-QA_A_Benchmark_Dataset_for_Remote_Sensing_Vision_Language_Model-based_Question_Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="137-Generative-Video-Matting"><a href="#137-Generative-Video-Matting" class="headerlink" title="137. Generative Video Matting"></a>137. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Generative_Video_Matting.pdf">Generative Video Matting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Adelaide</span></p><p>该论文提出GVM（Generative Video Matting），一种基于视频扩散模型的视频抠图方法，通过大规模合成与伪标注数据预训练，结合高质量细粒度合成发丝视频数据，针对人物和动物实现稳健的时序一致精细抠图。方法创新点在于利用预训练视频扩散模型的丰富空间与时序先验，并采用三阶段训练策略和混合空间—像素监督，实验在多个基准上显示出对复杂场景的优越泛化和细节恢复能力。结论：GVM在真实复杂场景下表现出色，优于现有主流方法，具备广泛应用前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Generative_Video_Matting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="138-Being-M0-5-A-Real-Time-Controllable-Vision-Language-Motion-Model"><a href="#138-Being-M0-5-A-Real-Time-Controllable-Vision-Language-Motion-Model" class="headerlink" title="138. Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model"></a>138. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Being-M0.5__A_Real-Time_Controllable_Vision-Language-Motion_Model.pdf">Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CASIA</span></p><p>本文提出了Being-M0.5，一种基于7B参数LLM的实时可控视觉-语言-动作模型（VLMM），利用新构建的HuMo100M大规模多模态动作数据集，支持多模态输入输出，并引入了创新的部位感知残差量化（PRQ）技术，实现身体各部位的精细化独立控制。实验显示，该模型在多项动作生成基准任务上取得了最先进性能，并具备高效实时推理能力，极大提升了动作生成的可控性与实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Being-M0.5_A_Real-Time_Controllable_Vision-Language-Motion_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="139-Segmenting-and-Understanding-Region-aware-Semantic-Attention-for-Fine-grained-Image-Quality-Assessment-with-Large-Language-Models"><a href="#139-Segmenting-and-Understanding-Region-aware-Semantic-Attention-for-Fine-grained-Image-Quality-Assessment-with-Large-Language-Models" class="headerlink" title="139. Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models"></a>139. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Segmenting_and_Understanding__Region-aware_Semantic_Attention_for_Fine-grained_Image_Quality_Assessm.pdf">Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology</span></p><p>本文提出了一种新的无参考图像质量评估模型RSFIQA，通过结合Segment Anything Model（SAM）进行语义分割和多模态大语言模型（MLLM）实现分区域的多维失真分析，提升了图像质量评估的精细度。核心创新为区域语义注意力机制（RSA），能聚合局部区域特征并提升对不同语义区域的细粒度质量感知。实验结果表明该方法在多个主流数据集上均取得了优异性能，具有很强的泛化性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Segmenting_and_Understanding_Region-aware_Semantic_Attention_for_Fine-grained_Image_Quality_Assessment_with_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="140-Dream4D-Lifting-Camera-Controlled-I2V-towards-Spatiotemporally-Consistent-4D-Generation"><a href="#140-Dream4D-Lifting-Camera-Controlled-I2V-towards-Spatiotemporally-Consistent-4D-Generation" class="headerlink" title="140. Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation"></a>140. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Dream4D__Lifting_Camera-Controlled_I2V_towards_Spatiotemporally_Consistent_4D_Generation.pdf">Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong</span></p><p>该论文提出Dream4D框架，通过结合可控图像到视频生成（I2V）与神经4D重建，首次实现从单幅图像和文本提示出发，经由视觉语言模型（VLM）预测最优相机轨迹，利用姿态条件扩散模型生成空间一致的多视角视频序列，最终还原具有时空一致性的4D场景。实验表明，Dream4D在动态场景下具备优异的结构保持和时序一致性，定量与定性指标均优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Dream4D_Lifting_Camera-Controlled_I2V_towards_Spatiotemporally_Consistent_4D_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="141-Sea-Undistort-A-Dataset-for-Through-Water-Image-Restoration-in-High-Resolution-Airborne-Bathymetric-Mapping"><a href="#141-Sea-Undistort-A-Dataset-for-Through-Water-Image-Restoration-in-High-Resolution-Airborne-Bathymetric-Mapping" class="headerlink" title="141. Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping"></a>141. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sea-Undistort__A_Dataset_for_Through-Water_Image_Restoration_in_High_Resolution_Airborne_Bathymetric.pdf">Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technische Universität Berlin</span></p><p>本文提出了Sea-Undistort数据集，使用Blender合成了1200对高分辨率水下光学畸变和无畸变的图像，涵盖了真实的水面反射、波动和散射等复杂效果，并配套详细元数据。基于该数据集，作者对比了两种主流图像复原方法并提出了融合早期光斑掩码的轻量扩散模型，实验表明该方法在合成和真实航拍数据上都显著提升了水下地形建模的细节还原、深度范围和精度，同时抑制了光斑和散射等伪影。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Sea-Undistort_A_Dataset_for_Through-Water_Image_Restoration_in_High_Resolution_Airborne_Bathymetric_Mapping.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="142-Correspondence-as-Video-Test-Time-Adaption-on-SAM2-for-Reference-Segmentation-in-the-Wild"><a href="#142-Correspondence-as-Video-Test-Time-Adaption-on-SAM2-for-Reference-Segmentation-in-the-Wild" class="headerlink" title="142. Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild"></a>142. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Correspondence_as_Video__Test-Time_Adaption_on_SAM2_for_Reference_Segmentation_in_the_Wild.pdf">Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>该论文提出CAV-SAM方法，将参考-目标图像对的对应关系表示为伪视频序列，通过扩散模型生成平滑的语义变换序列（DBST模块），并利用测试时几何对齐（TTGA模块）对SAM2进行轻量级微调，提升在下游参考分割任务中的适应能力。实验结果显示，CAV-SAM在跨领域少样本分割基准上较SOTA方法mIoU提升约5%，具备强泛化和高效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Correspondence_as_Video_Test-Time_Adaption_on_SAM2_for_Reference_Segmentation_in_the_Wild.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="143-Enhancing-Small-Scale-Dataset-Expansion-with-Triplet-Connection-based-Sample-Re-Weighting"><a href="#143-Enhancing-Small-Scale-Dataset-Expansion-with-Triplet-Connection-based-Sample-Re-Weighting" class="headerlink" title="143. Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting"></a>143. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Enhancing_Small-Scale_Dataset_Expansion_with_Triplet-Connection-based_Sample_Re-Weighting.pdf">Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hunan University</span></p><p>该论文提出TriReWeight方法，通过理论分析三类生成图像监督方式（强连接、成对连接、无连接），发现现有生成数据扩增普遍采用的强连接监督易引入噪声标签，降低泛化能力。TriReWeight结合成对连接（triplet loss）与无连接（consistency regularization）监督，动态对生成样本赋权，显著提升了小样本数据集的分类准确率，且可无缝集成到各类生成数据扩增框架。实验验证该方法平均提升自然图像数据集7.9%、医学图像数据集3.4%的准确率，并具有良好泛化性和训练收敛性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Enhancing_Small-Scale_Dataset_Expansion_with_Triplet-Connection-based_Sample_Re-Weighting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="144-DoorDet-Semi-Automated-Multi-Class-Door-Detection-Dataset-via-Object-Detection-and-Large-Language-Models"><a href="#144-DoorDet-Semi-Automated-Multi-Class-Door-Detection-Dataset-via-Object-Detection-and-Large-Language-Models" class="headerlink" title="144. DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models"></a>144. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DoorDet__Semi-Automated_Multi-Class_Door_Detection_Dataset_via_Object_Detection_and_Large_Language_M.pdf">DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Melbourne</span></p><p>本文提出了一种结合深度目标检测（Co-DETR）与大语言模型（GPT-4.1）的半自动化流程，用于构建多类别门检测数据集DoorDet，流程包括单类门检测、LLM辅助门类型判别与人工校正。实验表明，该方法大幅减少人工标注成本，提升了数据集质量和多类别门检测的基准性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DoorDet_Semi-Automated_Multi-Class_Door_Detection_Dataset_via_Object_Detection_and_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="145-Make-Your-MoVe-Make-Your-3D-Contents-by-Adapting-Multi-View-Diffusion-Models-to-External-Editing"><a href="#145-Make-Your-MoVe-Make-Your-3D-Contents-by-Adapting-Multi-View-Diffusion-Models-to-External-Editing" class="headerlink" title="145. Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing"></a>145. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Make_Your_MoVe__Make_Your_3D_Contents_by_Adapting_Multi-View_Diffusion_Models_to_External_Editing.pdf">Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了一种无须微调的通用方案，可将外部2D编辑（如风格迁移、重光照）自适应整合到多视角扩散模型生成的3D内容中。核心方法包括并行双通道推理、几何保持模块（用原始法线引导编辑生成）和注入切换器（动态控制法线监督程度），实现了外观变化下的几何一致性。实验表明，该方法在多种模型和编辑工具下显著提升了3D资产的多视角一致性与网格质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Make_Your_MoVe_Make_Your_3D_Contents_by_Adapting_Multi-View_Diffusion_Models_to_External_Editing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="146-DiffVC-OSD-One-Step-Diffusion-based-Perceptual-Neural-Video-Compression-Framework"><a href="#146-DiffVC-OSD-One-Step-Diffusion-based-Perceptual-Neural-Video-Compression-Framework" class="headerlink" title="146. DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework"></a>146. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DiffVC-OSD__One-Step_Diffusion-based_Perceptual_Neural_Video_Compression_Framework.pdf">DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>该论文提出了DiffVC-OSD，一种基于单步扩散模型的感知神经视频压缩框架。方法核心在于直接将重构的无噪声潜在表征输入扩散模型，并结合时序上下文，通过单步扩散实现高感知质量的视频重建，同时设计了时序上下文适配器（TCA）和端到端微调策略以提升整体压缩效率。实验表明该方法在多项感知指标上达到SOTA，解码速度提升约20倍，码率降低86.92%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DiffVC-OSD_One-Step_Diffusion-based_Perceptual_Neural_Video_Compression_Framework.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="147-X2Edit-Revisiting-Arbitrary-Instruction-Image-Editing-through-Self-Constructed-Data-and-Task-Aware-Representation-Learning"><a href="#147-X2Edit-Revisiting-Arbitrary-Instruction-Image-Editing-through-Self-Constructed-Data-and-Task-Aware-Representation-Learning" class="headerlink" title="147. X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning"></a>147. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/X2Edit__Revisiting_Arbitrary-Instruction_Image_Editing_through_Self-Constructed_Data_and_Task-Aware_.pdf">X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">OPPO AI Center</span></p><p>该论文提出了X2Edit Dataset，一个涵盖14种编辑任务的高质量自动化图像编辑数据集，并提出了轻量化的可插拔编辑模型X2Edit，采用任务感知MoE-LoRA与对比学习结构实现高效多任务图像编辑。实验表明，X2Edit数据集在任务多样性与质量上优于现有开源数据集，X2Edit模型在多项基准性能上与SOTA方法持平，具备良好的可扩展性与生态兼容性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/X2Edit_Revisiting_Arbitrary-Instruction_Image_Editing_through_Self-Constructed_Data_and_Task-Aware_Representation_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="148-Separation-and-Collaboration-Two-Level-Routing-Grouped-Mixture-of-Experts-for-Multi-Domain-Continual-Learning"><a href="#148-Separation-and-Collaboration-Two-Level-Routing-Grouped-Mixture-of-Experts-for-Multi-Domain-Continual-Learning" class="headerlink" title="148. Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning"></a>148. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Separation_and_Collaboration__Two-Level_Routing_Grouped_Mixture-of-Experts_for_Multi-Domain_Continua.pdf">Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Defense Technology</span></p><p>本文提出了Two-Level Routing Grouped Mixture-of-Experts (TRGE)方法，通过在冻结的CLIP模型基础上为每个新任务增设专家组，并采用两级路由机制（组内与组间路由）实现任务间协作与知识保留。结合多模态大语言模型进行任务识别，并动态融合适配器与预训练模型输出，有效缓解灾难性遗忘和前向遗忘。实验显示，该方法在多领域持续学习任务中以更少参数获得更优反遗忘和零样本泛化性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Separation_and_Collaboration_Two-Level_Routing_Grouped_Mixture-of-Experts_for_Multi-Domain_Continual_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="149-Training-Free-ANN-to-SNN-Conversion-for-High-Performance-Spiking-Transformer"><a href="#149-Training-Free-ANN-to-SNN-Conversion-for-High-Performance-Spiking-Transformer" class="headerlink" title="149. Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer"></a>149. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Training-Free_ANN-to-SNN_Conversion_for_High-Performance_Spiking_Transformer.pdf">Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>本文提出了一种针对Transformer架构的高性能、免训练ANN-to-SNN转换框架，核心方法为多基指数衰减（MBE）神经元，通过指数衰减策略和多基编码高效逼近各类非线性操作，无需对原始ANN权重进行微调。实验表明，该方法在CV、NLU、NLG等任务和主流Transformer架构（ViT、RoBERTa、GPT-2）上实现了近无损的转换精度与显著的低延迟，极大推动了Spiking Transformer在实际中的高效部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Training-Free_ANN-to-SNN_Conversion_for_High-Performance_Spiking_Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="150-TAR-TVG-Enhancing-VLMs-with-Timestamp-Anchor-Constrained-Reasoning-for-Temporal-Video-Grounding"><a href="#150-TAR-TVG-Enhancing-VLMs-with-Timestamp-Anchor-Constrained-Reasoning-for-Temporal-Video-Grounding" class="headerlink" title="150. TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding"></a>150. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TAR-TVG__Enhancing_VLMs_with_Timestamp_Anchor-Constrained_Reasoning_for_Temporal_Video_Grounding.pdf">TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China University of Technology</span></p><p>该论文提出了TAR-TVG框架，通过在视频语言模型的推理过程中引入时间戳锚点，实现对推理链的显式监督和逐步精细化的时间预测。采用GRPO-SFT-GRPO三阶段训练策略，自动收集高质量推理样本并提升模型生成可验证、可解释的推理链。实验结果显示，该方法在Charades-STA等多项基准上达到最新SOTA，显著优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TAR-TVG_Enhancing_VLMs_with_Timestamp_Anchor-Constrained_Reasoning_for_Temporal_Video_Grounding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="151-LaRender-Training-Free-Occlusion-Control-in-Image-Generation-via-Latent-Rendering"><a href="#151-LaRender-Training-Free-Occlusion-Control-in-Image-Generation-via-Latent-Rendering" class="headerlink" title="151. LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering"></a>151. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LaRender__Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering.pdf">LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent</span></p><p>LaRender提出了一种训练自由的图像生成方法，通过在预训练扩散模型的潜空间中引入物理原理的体渲染机制，实现对图像中多对象遮挡关系的精确控制。该方法无需模型重训练或微调，仅通过替换扩散模型的cross-attention层为潜渲染层，结合遮挡图和对象透明度，实现物理一致的遮挡及丰富视觉特效。实验显示LaRender在遮挡准确性上显著优于主流文本和布局控制生成方法，且可灵活调整对象透明度，带来半透明、密度变化等多种视觉效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="152-Surgical-Knowledge-Rewrite-in-Compact-LLMs-An-‘Unlearn-then-Learn’-Strategy-with-IA-3-for-Localized-Factual-Modulation-and-Catastrophic-Forgetting-Mitigation"><a href="#152-Surgical-Knowledge-Rewrite-in-Compact-LLMs-An-‘Unlearn-then-Learn’-Strategy-with-IA-3-for-Localized-Factual-Modulation-and-Catastrophic-Forgetting-Mitigation" class="headerlink" title="152. Surgical Knowledge Rewrite in Compact LLMs: An ‘Unlearn-then-Learn’ Strategy with (IA^3) for Localized Factual Modulation and Catastrophic Forgetting Mitigation"></a>152. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Surgical_Knowledge_Rewrite_in_Compact_LLMs__An_'Unlearn-then-Learn'_Strategy_with_($IA^3$)_for_Local.pdf">Surgical Knowledge Rewrite in Compact LLMs: An ‘Unlearn-then-Learn’ Strategy with (IA^3) for Localized Factual Modulation and Catastrophic Forgetting Mitigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanley Ngugi</span></p><p>本文提出了一种基于PEFT技术IA^3的“unlearn-then-learn”知识编辑策略，首先通过机制可解释性分析定位模型内部编码冲突事实的具体电路，再分两阶段分别抑制原有事实后引入新事实，实现了对紧凑型LLM（Phi-3-mini）内冲突知识的精准重写。实验表明该方法能以极高准确率（98.5%）编辑新事实，同时显著缓解灾难性遗忘（F_control准确率达72%），提出“软遗忘”机制，提升了模型的可控性与安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Surgical_Knowledge_Rewrite_in_Compact_LLMs_An_%27Unlearn-then-Learn%27_Strategy_with_%28IA%5E3%29_for_Localized_Factual_Modulation_and_Catastrophic_Forgetting_Mitigation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="153-Towards-Safer-AI-Moderation-Evaluating-LLM-Moderators-Through-a-Unified-Benchmark-Dataset-and-Advocating-a-Human-First-Approach"><a href="#153-Towards-Safer-AI-Moderation-Evaluating-LLM-Moderators-Through-a-Unified-Benchmark-Dataset-and-Advocating-a-Human-First-Approach" class="headerlink" title="153. Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach"></a>153. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Towards_Safer_AI_Moderation__Evaluating_LLM_Moderators_Through_a_Unified_Benchmark_Dataset_and_Advoc.pdf">Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fordham University</span></p><p>本文提出了一个涵盖49类情感、攻击性及偏见内容的统一人工标注基准数据集，用于系统性评估主流LLM内容审核模型（如OpenAI Moderator、Llama Guard），并开发了基于Phi-4的LoRA微调模型SafePhi。结果显示SafePhi在宏观F1分数上（0.89）显著优于其他模型，但所有LLM审核器在处理隐性和具文化背景的有害内容时均表现不足，作者呼吁引入多元化人工参与和持续人类反馈，提升模型鲁棒性与公平性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Towards_Safer_AI_Moderation_Evaluating_LLM_Moderators_Through_a_Unified_Benchmark_Dataset_and_Advocating_a_Human-First_Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="154-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability"><a href="#154-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability" class="headerlink" title="154. ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability"></a>154. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ReasonRank__Empowering_Passage_Ranking_with_Strong_Reasoning_Ability.pdf">ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Renmin University of China</span></p><p>本文提出ReasonRank，一种增强推理能力的段落重排序方法，首先通过自动化数据合成框架生成高质量、跨领域的推理密集型训练数据，并采用两阶段训练：冷启动监督微调（SFT）学习推理模式，随后利用多视角排序奖励的强化学习进一步提升排序能力。实验表明，ReasonRank在BRIGHT和R2MED等推理密集型信息检索基准上大幅优于现有方法，且推理效率更高。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ReasonRank_Empowering_Passage_Ranking_with_Strong_Reasoning_Ability.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="155-K-Dense-Analyst-Towards-Fully-Automated-Scientific-Analysis"><a href="#155-K-Dense-Analyst-Towards-Fully-Automated-Scientific-Analysis" class="headerlink" title="155. K-Dense Analyst: Towards Fully Automated Scientific Analysis"></a>155. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/K-Dense_Analyst__Towards_Fully_Automated_Scientific_Analysis.pdf">K-Dense Analyst: Towards Fully Automated Scientific Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Biostate AI, Palo Alto, CA</span></p><p>该论文提出了K-Dense Analyst，一个采用分层多智能体与双环反馈架构的自主科学分析系统，集成规划、执行和多重验证流程，实现了复杂生物信息学数据的全自动化分析。通过在BixBench基准测试中达到29.2%准确率，显著超越当前最强语言模型（如GPT-5），结果显示系统架构创新能大幅提升科学分析能力，而不仅依赖于基础大模型规模。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/K-Dense_Analyst_Towards_Fully_Automated_Scientific_Analysis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="156-From-Nodes-to-Narratives-Explaining-Graph-Neural-Networks-with-LLMs-and-Graph-Context"><a href="#156-From-Nodes-to-Narratives-Explaining-Graph-Neural-Networks-with-LLMs-and-Graph-Context" class="headerlink" title="156. From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context"></a>156. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Nodes_to_Narratives__Explaining_Graph_Neural_Networks_with_LLMs_and_Graph_Context.pdf">From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Chicago</span></p><p>该论文提出了LOGIC，一种结合GNN与LLM的新型解释框架，通过投影GNN节点嵌入到LLM嵌入空间，并构造软提示与文本混合的混合提示，使LLM能够基于GNN内部表示生成自然语言解释和支持解释的子图。实验表明，LOGIC在保持高忠实度和紧凑解释规模的同时，提升了可解释性和人类理解度，优于现有GNN解释器。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/From_Nodes_to_Narratives_Explaining_Graph_Neural_Networks_with_LLMs_and_Graph_Context.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="157-Investigating-Intersectional-Bias-in-Large-Language-Models-using-Confidence-Disparities-in-Coreference-Resolution"><a href="#157-Investigating-Intersectional-Bias-in-Large-Language-Models-using-Confidence-Disparities-in-Coreference-Resolution" class="headerlink" title="157. Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution"></a>157. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Investigating_Intersectional_Bias_in_Large_Language_Models_using_Confidence_Disparities_in_Coreferen.pdf">Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Apple</span></p><p>本论文提出了WinoIdentity基准数据集，通过对WinoBias进行多属性人口统计标注扩展，并开发了新的群体公平性指标“共指置信度差异”，用于评估大语言模型在共指消解任务中的交叉群体偏见和不确定性。实验证明，现有LLM在涉及多重身份群体时表现不佳，置信度差异最高可达40%，在反刻板印象情境下对双重弱势群体表现出更大不确定性，且模型更依赖记忆而非推理，带来有效性和公平性双重风险。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Investigating_Intersectional_Bias_in_Large_Language_Models_using_Confidence_Disparities_in_Coreference_Resolution.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="158-Hide-or-Highlight-Understanding-the-Impact-of-Factuality-Expression-on-User-Trust"><a href="#158-Hide-or-Highlight-Understanding-the-Impact-of-Factuality-Expression-on-User-Trust" class="headerlink" title="158. Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust"></a>158. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hide_or_Highlight__Understanding_the_Impact_of_Factuality_Expression_on_User_Trust.pdf">Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IBM Research</span></p><p>本论文提出了四种不同的LLM输出低事实性内容的表达策略，包括高亮、移除、模糊化和基线，并在人类实验中对这些策略进行了对比。结果表明，通过隐藏或模糊低事实性内容（opaque和ambiguity策略）能够提升用户对AI的信任，同时保持回答质量不变，优于直接高亮或全部展示低事实性内容。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Hide_or_Highlight_Understanding_the_Impact_of_Factuality_Expression_on_User_Trust.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="159-SEADialogues-A-Multilingual-Culturally-Grounded-Multi-turn-Dialogue-Dataset-on-Southeast-Asian-Languages"><a href="#159-SEADialogues-A-Multilingual-Culturally-Grounded-Multi-turn-Dialogue-Dataset-on-Southeast-Asian-Languages" class="headerlink" title="159. SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages"></a>159. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SEADialogues__A_Multilingual_Culturally_Grounded_Multi-turn_Dialogue_Dataset_on_Southeast_Asian_Lang.pdf">SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MBZUAI</span></p><p>论文提出SEADIALOGUES，一个涵盖八种东南亚语言、具备文化和身份特征的多轮对话数据集。方法采用模板生成、文化实体词汇化、LLM生成对话和人工&#x2F;自动评估流程，提升数据集的文化适应性和个性化。结论显示，封闭权重的大语言模型在流畅性、自然性、文化相关性等指标上优于开源模型，强调高质量文化数据集对提升开放权重LLMs的重要性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SEADialogues_A_Multilingual_Culturally_Grounded_Multi-turn_Dialogue_Dataset_on_Southeast_Asian_Languages.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="160-Membership-and-Memorization-in-LLM-Knowledge-Distillation"><a href="#160-Membership-and-Memorization-in-LLM-Knowledge-Distillation" class="headerlink" title="160. Membership and Memorization in LLM Knowledge Distillation"></a>160. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Membership_and_Memorization_in_LLM_Knowledge_Distillation.pdf">Membership and Memorization in LLM Knowledge Distillation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文系统性研究了大语言模型知识蒸馏（LLM KD）过程中学生模型对教师模型隐私风险的继承性，综合评估了六种主流LLM KD技术在七个NLP任务中对成员推断攻击（MIA）和数据记忆攻击的脆弱性。研究表明，所有现有的LLM KD方法均无法完全阻止教师私有数据的隐私泄露，但不同蒸馏技术和训练流程对隐私风险的影响存在显著差异，且隐私泄漏在模型不同transformer block之间差异较大。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Membership_and_Memorization_in_LLM_Knowledge_Distillation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="161-Trustworthy-Medical-Imaging-with-Large-Language-Models-A-Study-of-Hallucinations-Across-Modalities"><a href="#161-Trustworthy-Medical-Imaging-with-Large-Language-Models-A-Study-of-Hallucinations-Across-Modalities" class="headerlink" title="161. Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities"></a>161. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Trustworthy_Medical_Imaging_with_Large_Language_Models__A_Study_of_Hallucinations_Across_Modalities.pdf">Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Akron</span></p><p>本论文系统性分析了大语言模型（LLMs）在医学影像任务中的幻觉现象，涵盖图像到文本（影像解读报告生成）和文本到图像（医学影像合成）两大方向。通过专家标准对不同模型在胸片、CT、MRI等多模态医学影像下的输出进行定量和定性评估，发现现有LLM在医学影像理解和生成过程中普遍存在事实不符和解剖失真等幻觉，严重影响临床可靠性。结论指出需通过更稳健的提示工程、医学知识约束和严格验证机制来提升LLM在临床应用中的安全性和可信度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Trustworthy_Medical_Imaging_with_Large_Language_Models_A_Study_of_Hallucinations_Across_Modalities.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="162-MultiMedEdit-A-Scenario-Aware-Benchmark-for-Evaluating-Knowledge-Editing-in-Medical-VQA"><a href="#162-MultiMedEdit-A-Scenario-Aware-Benchmark-for-Evaluating-Knowledge-Editing-in-Medical-VQA" class="headerlink" title="162. MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA"></a>162. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MultiMedEdit__A_Scenario-Aware_Benchmark_for_Evaluating_Knowledge_Editing_in_Medical_VQA.pdf">MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University of Aeronautics and Astronautics</span></p><p>该论文提出了MultiMedEdit，这是首个专为医学多模态场景下知识编辑评估设计的基准，涵盖理解与推理两类任务，包含三维度评估指标（reliability, generality, locality），并系统对比Prompt、LoRA、GRACE、WISE等代表性编辑范式在一般和医学多模态大模型上的表现。结论显示当前知识编辑方法在复杂医学推理和长期编辑中存在泛化差、顺序敏感和遗忘等明显局限，MultiMedEdit为后续更安全、可靠的医学知识编辑研究奠定了基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MultiMedEdit_A_Scenario-Aware_Benchmark_for_Evaluating_Knowledge_Editing_in_Medical_VQA.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="163-MASteer-Multi-Agent-Adaptive-Steer-Strategy-for-End-to-End-LLM-Trustworthiness-Repair"><a href="#163-MASteer-Multi-Agent-Adaptive-Steer-Strategy-for-End-to-End-LLM-Trustworthiness-Repair" class="headerlink" title="163. MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair"></a>163. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MASteer__Multi-Agent_Adaptive_Steer_Strategy_for_End-to-End_LLM_Trustworthiness_Repair.pdf">MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出MASteer框架，通过多智能体协作自动生成对比样本（AutoTester）和自适应表示工程策略（AutoRepairer），实现大语言模型（LLM）端到端可信度修复。MASteer利用anchor vector实现推理时的动态策略选择，显著提升LLM在真实性、公平性和安全性等主流与定制任务上的可信度，且保持模型原有能力不受损失。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MASteer_Multi-Agent_Adaptive_Steer_Strategy_for_End-to-End_LLM_Trustworthiness_Repair.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="164-BoRA-Towards-More-Expressive-Low-Rank-Adaptation-with-Block-Diversity"><a href="#164-BoRA-Towards-More-Expressive-Low-Rank-Adaptation-with-Block-Diversity" class="headerlink" title="164. BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity"></a>164. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BoRA__Towards_More_Expressive_Low-Rank_Adaptation_with_Block_Diversity.pdf">BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huazhong University of Science and Technology</span></p><p>本文提出了Block-Diversified Low-Rank Adaptation (BoRA)，通过将LoRA中的低秩矩阵分块并为每一块乘积引入独特的对角矩阵，有效提升了LoRA权重的秩，从而增强了大语言模型的参数高效微调能力。实验表明，BoRA在多个模型和数据集上均显著优于LoRA及其变体，在参数量相近的情况下提升了2-4%的准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BoRA_Towards_More_Expressive_Low-Rank_Adaptation_with_Block_Diversity.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="165-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance"><a href="#165-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance" class="headerlink" title="165. AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance"></a>165. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AMFT__Aligning_LLM_Reasoners_by_Meta-Learning_the_Optimal_Imitation-Exploration_Balance.pdf">AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>该论文提出了Adaptive Meta Fine-Tuning (AMFT)，一种通过单阶段训练动态融合Supervised Fine-Tuning (SFT)与Reinforcement Learning (RL)的LLM对齐方法。核心创新是利用meta-gradient自适应权重控制器，根据验证集的长远任务表现自动调整SFT与RL的权重，实现从模仿到探索的最优平衡。实验表明AMFT在数学、视觉推理和视觉-语言导航等多模态基准上均达到了新的SOTA，在样本和计算效率上也优于现有方法。结论是AMFT能有效避免灾难性遗忘和策略崩溃，提升泛化与稳健性，是更具理论基础的自动化LLM对齐范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AMFT_Aligning_LLM_Reasoners_by_Meta-Learning_the_Optimal_Imitation-Exploration_Balance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="166-CLAP-Coreference-Linked-Augmentation-for-Passage-Retrieval"><a href="#166-CLAP-Coreference-Linked-Augmentation-for-Passage-Retrieval" class="headerlink" title="166. CLAP: Coreference-Linked Augmentation for Passage Retrieval"></a>166. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CLAP__Coreference-Linked_Augmentation_for_Passage_Retrieval.pdf">CLAP: Coreference-Linked Augmentation for Passage Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Adelaide</span></p><p>本文提出了CLAP框架，通过语义分块、指代消解和局部伪查询生成，将长文本分解为结构化且语义清晰的子块，并用LLM生成与检索器语义空间对齐的局部伪查询。最终通过融合全局（查询-文档）与局部（查询-伪查询）相关性得分，显著提升了稀疏和稠密检索性能，尤其在领域外场景下超越主流二阶段重排序模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CLAP_Coreference-Linked_Augmentation_for_Passage_Retrieval.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="167-CROP-Integrating-Topological-and-Spatial-Structures-via-Cross-View-Prefixes-for-Molecular-LLMs"><a href="#167-CROP-Integrating-Topological-and-Spatial-Structures-via-Cross-View-Prefixes-for-Molecular-LLMs" class="headerlink" title="167. CROP: Integrating Topological and Spatial Structures via Cross-View Prefixes for Molecular LLMs"></a>167. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CROP__Integrating_Topological_and_Spatial_Structures_via_Cross-View_Prefixes_for_Molecular_LLMs.pdf">CROP: Integrating Topological and Spatial Structures via Cross-View Prefixes for Molecular LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>该论文提出了CROP框架，通过SMILES引导的跨视图前缀联合采样分子图和分子图像特征，有效整合分子的拓扑结构和空间结构，增强多模态大语言模型对分子的理解力。实验结果表明，CROP在分子描述生成、IUPAC命名预测和分子属性预测等任务上显著优于现有方法，实现了更高的效率和准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CROP_Integrating_Topological_and_Spatial_Structures_via_Cross-View_Prefixes_for_Molecular_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="168-DocRefine-An-Intelligent-Framework-for-Scientific-Document-Understanding-and-Content-Optimization-based-on-Multimodal-Large-Model-Agents"><a href="#168-DocRefine-An-Intelligent-Framework-for-Scientific-Document-Understanding-and-Content-Optimization-based-on-Multimodal-Large-Model-Agents" class="headerlink" title="168. DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents"></a>168. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DocRefine__An_Intelligent_Framework_for_Scientific_Document_Understanding_and_Content_Optimization_b.pdf">DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shangqiu University</span></p><p>DocRefine提出了一种基于多模态大模型（如GPT-4o）的多智能体系统，通过布局结构分析、内容理解、指令分解、内容优化、摘要生成及一致性验证六个协作代理，实现科学PDF文档的智能理解与内容优化。实验结果显示其在DocEditBench数据集上语义一致性（SCS）、版式保真度（LFI）和指令遵循率（IAR）均超过现有方法，显著提升多模态复杂文档的编辑质量与控制力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DocRefine_An_Intelligent_Framework_for_Scientific_Document_Understanding_and_Content_Optimization_based_on_Multimodal_Large_Model_Agents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="169-DATASETRESEARCH-Benchmarking-Agent-Systems-for-Demand-Driven-Dataset-Discovery"><a href="#169-DATASETRESEARCH-Benchmarking-Agent-Systems-for-Demand-Driven-Dataset-Discovery" class="headerlink" title="169. DATASETRESEARCH: Benchmarking Agent Systems for Demand-Driven Dataset Discovery"></a>169. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DatasetResearch__Benchmarking_Agent_Systems_for_Demand-Driven_Dataset_Discovery.pdf">DATASETRESEARCH: Benchmarking Agent Systems for Demand-Driven Dataset Discovery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了DATASETRESEARCH，首个系统性评测智能体在需求驱动数据集发现和合成方面能力的基准，覆盖208个真实NLP任务需求，并分类为知识型和推理型。通过多维度评测框架，发现搜索型智能体在知识任务表现突出，合成型智能体在推理任务中表现优异，但所有方法在“角落案例”上均失败，揭示现有技术在泛化和指令匹配上的重大不足。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DATASETRESEARCH_Benchmarking_Agent_Systems_for_Demand-Driven_Dataset_Discovery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="170-Large-Language-Models-Do-Not-Simulate-Human-Psychology"><a href="#170-Large-Language-Models-Do-Not-Simulate-Human-Psychology" class="headerlink" title="170. Large Language Models Do Not Simulate Human Psychology"></a>170. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Large_Language_Models_Do_Not_Simulate_Human_Psychology.pdf">Large Language Models Do Not Simulate Human Psychology</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bielefeld University</span></p><p>本论文通过理论分析和实证实验，质疑大型语言模型（如GPT-4、Llama-3.1、CENTAUR）能够模拟人类心理学的观点。作者通过对道德判断任务的对比实验证明，LLMs对语义微变的反应与真实人类存在显著差异，且不同模型表现不一致，结论认为LLMs无法可靠模拟人类心理学，建议心理学研究中须谨慎对待LLMs，并需针对具体任务重新验证。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Large_Language_Models_Do_Not_Simulate_Human_Psychology.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="171-When-Prompt-Engineering-Meets-Software-Engineering-CNL-P-as-Natural-and-Robust-“APIs”-for-Human-AI-Interaction"><a href="#171-When-Prompt-Engineering-Meets-Software-Engineering-CNL-P-as-Natural-and-Robust-“APIs”-for-Human-AI-Interaction" class="headerlink" title="171. When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust “APIs” for Human-AI Interaction"></a>171. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/When_Prompt_Engineering_Meets_Software_Engineering__CNL-P_as_Natural_and_Robust__APIs''_for_Human-AI.pdf">When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust “APIs” for Human-AI Interaction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CSIRO’s Data61</span></p><p>本论文提出了CNL-P（Controlled Natural Language for Prompt），结合了最佳的Prompt Engineering（PE）实践与软件工程（SE）核心原则，通过引入精确的语法结构与严格的语义规范，降低自然语言提示的歧义，提升用户意图表达的结构化和准确性。论文还开发了NL到CNL-P的自动转换工具和静态分析Linting工具，首次将静态分析技术引入自然语言提示，实验证明CNL-P能提升LLM输出一致性和质量，其语法和语义对LLM友好，且Linting工具能有效发现和定位提示中的语法及语义错误。结论：CNL-P为基于自然语言的新型软件工程范式和人机协作提供了基础，并推动了SE4AI基础设施的建设。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/When_Prompt_Engineering_Meets_Software_Engineering_CNL-P_as_Natural_and_Robust_%E2%80%9CAPIs%E2%80%9D_for_Human-AI_Interaction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="172-Model-Agnostic-Sentiment-Distribution-Stability-Analysis-for-Robust-LLM-Generated-Texts-Detection"><a href="#172-Model-Agnostic-Sentiment-Distribution-Stability-Analysis-for-Robust-LLM-Generated-Texts-Detection" class="headerlink" title="172. Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection"></a>172. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Model-Agnostic_Sentiment_Distribution_Stability_Analysis_for_Robust_LLM-Generated_Texts_Detection.pdf">Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science, Shanghai Jiao Tong University</span></p><p>本文提出SentiDetect，一种无需模型参数访问、无监督的LLM生成文本检测框架。方法基于情感分布稳定性假设，通过低情感重写和情感一致性、保持性两项指标，对原始及重写文本的情感分布进行特征分析来区分人类与LLM生成内容，实验证明其在多数据集和多模型下均显著优于现有方法，并对抗扰动和改写有更强鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Model-Agnostic_Sentiment_Distribution_Stability_Analysis_for_Robust_LLM-Generated_Texts_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="173-BASIC-Boosting-Visual-Alignment-with-Intrinsic-Refined-Embeddings-in-Multimodal-Large-Language-Models"><a href="#173-BASIC-Boosting-Visual-Alignment-with-Intrinsic-Refined-Embeddings-in-Multimodal-Large-Language-Models" class="headerlink" title="173. BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models"></a>173. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BASIC__Boosting_Visual_Alignment_with_Intrinsic_Refined_Embeddings_in_Multimodal_Large_Language_Mode.pdf">BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出了BASIC方法，通过利用多模态大语言模型（MLLM）浅层内部细化的视觉嵌入作为监督信号，从方向对齐和语义分布两个角度直接优化初始视觉嵌入。实验结果表明，BASIC无需额外监督模型和人工标注，即可显著提升多种主流MLLM在多项视觉-语言基准测试中的性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BASIC_Boosting_Visual_Alignment_with_Intrinsic_Refined_Embeddings_in_Multimodal_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="174-MeteorPred-A-Meteorological-Multimodal-Large-Model-and-Dataset-for-Severe-Weather-Event-Prediction"><a href="#174-MeteorPred-A-Meteorological-Multimodal-Large-Model-and-Dataset-for-Severe-Weather-Event-Prediction" class="headerlink" title="174. MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction"></a>174. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MeteorPred__A_Meteorological_Multimodal_Large_Model_and_Dataset_for_Severe_Weather_Event_Prediction.pdf">MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MAIS, Institute of Automation, Chinese Academy of Sciences</span></p><p>该论文提出了MP-Bench，一个包含42万对原始多维气象数据与文本警报的大规模时序多模态数据集，并基于此开发了气象多模态大模型MMLM。MMLM创新性地引入了三种可插拔融合模块（DTGF、TGS、TGCA），实现对4D气象数据的时序、空间和垂直特征的动态提取与融合。实验结果显示，MMLM在多项极端天气事件预测任务上显著优于现有闭源和开源多模态大模型，实现了对极端天气的自动化理解与预警，推动了AI气象预报的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MeteorPred_A_Meteorological_Multimodal_Large_Model_and_Dataset_for_Severe_Weather_Event_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="175-Highlight-All-the-Phrases-Enhancing-LLM-Transparency-through-Visual-Factuality-Indicators"><a href="#175-Highlight-All-the-Phrases-Enhancing-LLM-Transparency-through-Visual-Factuality-Indicators" class="headerlink" title="175. Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators"></a>175. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Highlight_All_the_Phrases__Enhancing_LLM_Transparency_through_Visual_Factuality_Indicators.pdf">Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IBM Research</span></p><p>该论文系统设计并对比了六种可视化大语言模型（LLM）输出内容事实性分数的界面设计方案，通过两轮共208名用户参与的场景实验，评估了不同设计对用户信任、验证准确性难易度和偏好的影响。结果显示，将模型回复中所有短语按事实性分数进行颜色高亮（phrase-level highlight-all）最受用户青睐，显著提升了信任和验证准确性体验，且能帮助用户更有效校准对LLM的信任。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Highlight_All_the_Phrases_Enhancing_LLM_Transparency_through_Visual_Factuality_Indicators.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="176-BiXSE-Improving-Dense-Retrieval-via-Probabilistic-Graded-Relevance-Distillation"><a href="#176-BiXSE-Improving-Dense-Retrieval-via-Probabilistic-Graded-Relevance-Distillation" class="headerlink" title="176. BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation"></a>176. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BiXSE__Improving_Dense_Retrieval_via_Probabilistic_Graded_Relevance_Distillation.pdf">BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universite de Montréal</span></p><p>该论文提出了BiXSE方法，用于利用LLM生成的细粒度分级相关性标签，通过点对点二元交叉熵损失优化密集检索模型，显著提升了检索与句子嵌入任务的表现。实验结果显示，BiXSE优于传统的InfoNCE和Pairwise Ranking Loss，不仅缩小了与大型零样本LLM排序器的性能差距，还具备更强的噪声鲁棒性和数据利用效率，尤其适用于大规模LLM标注数据的高效蒸馏。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BiXSE_Improving_Dense_Retrieval_via_Probabilistic_Graded_Relevance_Distillation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="177-Zero-Direction-Probing-A-Linear-Algebraic-Framework-for-Deep-Analysis-of-Large-Language-Model-Drift"><a href="#177-Zero-Direction-Probing-A-Linear-Algebraic-Framework-for-Deep-Analysis-of-Large-Language-Model-Drift" class="headerlink" title="177. Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift"></a>177. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Zero-Direction_Probing__A_Linear-Algebraic_Framework_for_Deep_Analysis_of_Large-Language-Model_Drift.pdf">Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AI Analytics</span></p><p>该论文提出了零方向探测（Zero-Direction Probing, ZDP）理论框架，通过分析变换器层激活的右&#x2F;左零空间及其Fisher信息几何，在无需任务标签和输出的情况下刻画大语言模型的漂移。核心方法包括定义零泄漏功能量、提出方差-泄漏定理、Fisher零守恒、低秩更新的泄漏界、以及基于随机矩阵理论的漂移阈值，配合在线零空间追踪器与零对齐LoRA优化器，并提供了理论收敛和漂移报警保证。结论认为，通过“监听沉默”即监控零空间，可以无监督、理论上严格地检测和界定大语言模型表征的变化，为后续应用和漂移控制提供了可验证的理论基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Zero-Direction_Probing_A_Linear-Algebraic_Framework_for_Deep_Analysis_of_Large-Language-Model_Drift.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="178-SafePLUG-Empowering-Multimodal-LLMs-with-Pixel-Level-Insight-and-Temporal-Grounding-for-Traffic-Accident-Understanding"><a href="#178-SafePLUG-Empowering-Multimodal-LLMs-with-Pixel-Level-Insight-and-Temporal-Grounding-for-Traffic-Accident-Understanding" class="headerlink" title="178. SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding"></a>178. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SafePLUG__Empowering_Multimodal_LLMs_with_Pixel-Level_Insight_and_Temporal_Grounding_for_Traffic_Acc.pdf">SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Wisconsin–Madison</span></p><p>该论文提出SafePLUG框架，通过视觉和数字提示，将像素级理解与时间定位能力集成到多模态大语言模型中，实现复杂交通事故场景的细粒度分析。方法包括视觉区域提示编码、SAM像素分割、数字提示进行时序定位，并构建了首个支持区域QA和像素级标注的交通事故多模态数据集。实验表明SafePLUG在区域问答、像素分割、事故描述和时序定位等多项任务上优于现有主流模型，显著提升了交通事故理解的精度和实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SafePLUG_Empowering_Multimodal_LLMs_with_Pixel-Level_Insight_and_Temporal_Grounding_for_Traffic_Accident_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="179-Many-Turn-Jailbreaking"><a href="#179-Many-Turn-Jailbreaking" class="headerlink" title="179. Many-Turn Jailbreaking"></a>179. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Many-Turn_Jailbreaking.pdf">Many-Turn Jailbreaking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Santa Barbara</span></p><p>该论文首次系统提出了多轮次（many-turn）Jailbreaking攻击大语言模型（LLMs）的新威胁，突破以往只关注单轮攻击的局限。作者构建了MTJ-Bench多轮Jailbreaking基准数据集，并在15个主流开源及闭源模型上进行了系统实验，发现一旦模型被首轮攻破，后续相关或无关的恶意提问均极易获得有害回复，且这一漏洞普遍存在于各类LLM中。结论认为，多轮Jailbreaking显著增大模型安全风险，亟需社区关注与防护，MTJ-Bench可作为该方向研究新标准测试集。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Many-Turn_Jailbreaking.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="180-MDK12-Bench-A-Comprehensive-Evaluation-of-Multimodal-Large-Language-Models-on-Multidisciplinary-Exams"><a href="#180-MDK12-Bench-A-Comprehensive-Evaluation-of-Multimodal-Large-Language-Models-on-Multidisciplinary-Exams" class="headerlink" title="180. MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams"></a>180. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MDK12-Bench__A_Comprehensive_Evaluation_of_Multimodal_Large_Language_Models_on_Multidisciplinary_Exa.pdf">MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai AI Laboratory</span></p><p>该论文提出MDK12-Bench，一个覆盖六大学科、包含14.1万题的大规模多模态大语言模型评测基准，具备多层知识点结构和多维度评估能力。方法包括动态评测框架（文本&#x2F;视觉扰动）和知识点增强生成（KP-RAG），揭示现有MLLM在跨难度、跨年份与复杂泛化任务上的局限性。结论显示，当前主流MLLM在上下文变化和复杂推理任务中普遍表现差，知识补充对高难题提升有限，为模型稳健性和教育应用提供指导。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MDK12-Bench_A_Comprehensive_Evaluation_of_Multimodal_Large_Language_Models_on_Multidisciplinary_Exams.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="181-Remote-Sensing-Image-Intelligent-Interpretation-with-the-Language-Centered-Perspective-Principles-Methods-and-Challenges"><a href="#181-Remote-Sensing-Image-Intelligent-Interpretation-with-the-Language-Centered-Perspective-Principles-Methods-and-Challenges" class="headerlink" title="181. Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges"></a>181. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Remote_Sensing_Image_Intelligent_Interpretation_with_the_Language-Centered_Perspective__Principles,_.pdf">Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Central South University</span></p><p>本论文提出了一种基于语言中心视角的遥感图像智能解译框架，借鉴人类认知中的全球工作空间理论，将大语言模型（LLM）作为认知中枢，统一融合感知、知识、任务和行动空间，实现多模态语义表示、知识关联和推理决策闭环。论文系统综述了该范式的关键技术挑战与现有进展，指出未来需关注多模态自适应对齐、动态知识约束下的任务理解、可信推理及自主交互。结论认为，语言中心框架能显著提升遥感解译系统的泛化性、交互性和推理能力，推动遥感领域迈向认知智能新阶段。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Remote_Sensing_Image_Intelligent_Interpretation_with_the_Language-Centered_Perspective_Principles%2C_Methods_and_Challenges.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="182-Technical-Report-Full-Stack-Fine-Tuning-for-the-Q-Programming-Language"><a href="#182-Technical-Report-Full-Stack-Fine-Tuning-for-the-Q-Programming-Language" class="headerlink" title="182. Technical Report: Full-Stack Fine-Tuning for the Q Programming Language"></a>182. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Technical_Report__Full-Stack_Fine-Tuning_for_the_Q_Programming_Language.pdf">Technical Report: Full-Stack Fine-Tuning for the Q Programming Language</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Morgan Stanley</span></p><p>本文提出了一套完整的开源方法，针对金融领域常用但网络样本稀缺的Q编程语言构建Leetcode风格的评测集，并基于Qwen-2.5系列大语言模型，通过预训练、监督微调和强化学习三阶段，系统提升模型在Q代码生成上的表现。最终，32B参数模型在Q基准测试中pass@1达59%，较Claude Opus-4提升29.5%，所有模型均超越GPT-4.1，验证了方法的有效性，并发布了数据、代码和训练流程，为小语种领域模型适配提供可复用蓝图，结论是大模型通过系统性领域适配能实现对专有编程语言的显著性能提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Technical_Report_Full-Stack_Fine-Tuning_for_the_Q_Programming_Language.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="183-LSDTs-LLM-Augmented-Semantic-Digital-Twins-for-Adaptive-Knowledge-Intensive-Infrastructure-Planning"><a href="#183-LSDTs-LLM-Augmented-Semantic-Digital-Twins-for-Adaptive-Knowledge-Intensive-Infrastructure-Planning" class="headerlink" title="183. LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning"></a>183. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LSDTs__LLM-Augmented_Semantic_Digital_Twins_for_Adaptive_Knowledge-Intensive_Infrastructure_Planning.pdf">LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Maryland, College Park</span></p><p>该论文提出LSDTs框架，将大型语言模型（LLM）与语义数字孪生（DT）集成，实现从非结构化规划文件自动提取和组织知识为本体，然后驱动基础设施数字孪生进行合规、可解释的规划模拟和优化。结果表明，LSDTs可自动整合多源约束，提升基础设施规划的适应性和合规性，尤其在风电场布局与极端气象应对场景中表现出色。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LSDTs_LLM-Augmented_Semantic_Digital_Twins_for_Adaptive_Knowledge-Intensive_Infrastructure_Planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="184-PROPS-Progressively-Private-Self-alignment-of-Large-Language-Models"><a href="#184-PROPS-Progressively-Private-Self-alignment-of-Large-Language-Models" class="headerlink" title="184. PROPS: Progressively Private Self-alignment of Large Language Models"></a>184. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PROPS__Progressively_Private_Self-alignment_of_Large_Language_Models.pdf">PROPS: Progressively Private Self-alignment of Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Arizona</span></p><p>本文提出了PROPS（Progressively Private Self-alignment），一种多阶段隐私保护大语言模型（LLM）对齐框架，通过将训练数据分批并结合随机响应（RR）机制与中间模型自标注及最大似然估计（MLE），实现对人类偏好标签的差分隐私保护。实验结果表明，在相同隐私预算下，PROPS在对齐性能上显著优于传统的DP-SGD和RR方法，尤其在高隐私要求下模型效用提升显著。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/PROPS_Progressively_Private_Self-alignment_of_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="185-Story-Ribbons-Reimagining-Storyline-Visualizations-with-Large-Language-Models"><a href="#185-Story-Ribbons-Reimagining-Storyline-Visualizations-with-Large-Language-Models" class="headerlink" title="185. Story Ribbons: Reimagining Storyline Visualizations with Large Language Models"></a>185. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Story_Ribbons__Reimagining_Storyline_Visualizations_with_Large_Language_Models.pdf">Story Ribbons: Reimagining Storyline Visualizations with Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harvard University</span></p><p>该论文提出了一个基于大型语言模型（LLM）的自动数据解析流水线，能从小说和剧本中提取角色、地点、主题等叙事要素，并通过STORY RIBBONS交互系统进行可视化。实验和用户研究显示，该方法能提升文学分析的效率和深度，尽管当前LLM在把握文学细节和背景上仍有限，但整体上为文学研究带来了新的洞察和互动体验。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Story_Ribbons_Reimagining_Storyline_Visualizations_with_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="186-Fed-MobiLLM-Efficient-Federated-LLM-Fine-Tuning-over-Heterogeneous-Mobile-Devices-via-Server-Assisted-Side-Tuning"><a href="#186-Fed-MobiLLM-Efficient-Federated-LLM-Fine-Tuning-over-Heterogeneous-Mobile-Devices-via-Server-Assisted-Side-Tuning" class="headerlink" title="186. Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning"></a>186. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fed_MobiLLM__Efficient_Federated_LLM_Fine-Tuning_over_Heterogeneous_Mobile_Devices_via_Server_Assist.pdf">Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Houston</span></p><p>Fed MobiLLM提出了一种面向异构移动设备的高效联邦大语言模型(LLM)微调框架，采用异步服务端辅助的侧网络微调方式，将所有可训练参数集中在服务器侧，设备端仅需冻结主干模型并上传中间激活。该方法通过分层激活采样和跨结构对齐机制，支持不同容量主干模型协同训练共享侧网络，实现对设备异构性的鲁棒支持。实验显示，Fed MobiLLM显著降低了设备端内存、计算和通信开销，提升了收敛速度，并在各种数据异构条件下保持竞争性精度，验证其实用性和高效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Fed_MobiLLM_Efficient_Federated_LLM_Fine-Tuning_over_Heterogeneous_Mobile_Devices_via_Server_Assisted_Side-Tuning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="187-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding"><a href="#187-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding" class="headerlink" title="187. VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding"></a>187. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VisR-Bench__An_Empirical_Study_on_Visual_Retrieval-Augmented_Generation_for_Multilingual_Long_Docume.pdf">VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University at Buffalo</span></p><p>该论文提出了VisR-Bench，这是首个针对多语言、长文档多模态检索与问答的基准，支持16种语言，涵盖文本、表格、图像等多种结构信息。作者构建了包含35,000多个高质量QA对和1,286份文档的数据集，并系统评估了文本检索、多模态编码器和多模态大语言模型（MLLMs）的检索能力。实验显示，MLLMs在多模态检索上优于传统方法，但对结构化表格和低资源语言仍有明显短板，强调了多语种多模态检索的挑战。结论认为VisR-Bench推动了多模态RAG与文档理解系统的发展，并为未来改进多语言MLLM检索能力提供了新基准和分析工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/VisR-Bench_An_Empirical_Study_on_Visual_Retrieval-Augmented_Generation_for_Multilingual_Long_Document_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="188-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy"><a href="#188-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy" class="headerlink" title="188. Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy"></a>188. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Democratizing_Diplomacy__A_Harness_for_Evaluating_Any_Large_Language_Model_on_Full-Press_Diplomacy.pdf">Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Good Start Labs</span></p><p>该论文提出首个可使任意本地大型语言模型（LLM）无需微调即可参与高复杂度的全回合Diplomacy游戏的评测工具。方法包括优化文本化游戏状态表示、批量对比13种主流模型表现、引入关键状态分析（CSA）深度实验，并开发承诺追踪等自动化评估协议。实验发现，模型规模与战略表现正相关，且小型模型也具备完成游戏的能力。结论指出，战略推理能力可自然从广泛应用的LLM中涌现，框架降低了战略AI研究门槛，并揭示了AI在多智能体环境中的欺骗与适应性行为。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Democratizing_Diplomacy_A_Harness_for_Evaluating_Any_Large_Language_Model_on_Full-Press_Diplomacy.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="189-Event-Aware-Sentiment-Factors-from-LLM-Augmented-Financial-Tweets-A-Transparent-Framework-for-Interpretable-Quant-Trading"><a href="#189-Event-Aware-Sentiment-Factors-from-LLM-Augmented-Financial-Tweets-A-Transparent-Framework-for-Interpretable-Quant-Trading" class="headerlink" title="189. Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading"></a>189. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Event-Aware_Sentiment_Factors_from_LLM-Augmented_Financial_Tweets__A_Transparent_Framework_for_Inter.pdf">Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cambridge</span></p><p>本文提出了一种透明的量化交易框架，通过使用大型语言模型（LLM）对金融推文进行多标签事件分类和情感强度打分，将非结构化社交媒体文本转化为可解释、结构化的事件驱动因子，并系统性地评估其对未来1-7天股票收益的预测能力。实验显示，部分LLM标注的事件标签（如谣言&#x2F;投机、散户热议等）显著产生负alpha，表明这些语义丰富的情感因子具有独立的预测价值，并且研究代码与方法已开源，有助于推动行为金融和可解释算法交易研究。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Event-Aware_Sentiment_Factors_from_LLM-Augmented_Financial_Tweets_A_Transparent_Framework_for_Interpretable_Quant_Trading.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="190-Generative-AI-for-Strategic-Plan-Development"><a href="#190-Generative-AI-for-Strategic-Plan-Development" class="headerlink" title="190. Generative AI for Strategic Plan Development"></a>190. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Generative_AI_for_Strategic_Plan_Development.pdf">Generative AI for Strategic Plan Development</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Johns Hopkins University</span></p><p>本文提出了一个模块化认知模型，利用生成式人工智能（GAI）和主流自然语言处理技术（如BERTopic和NMF）自动化开发大型政府组织战略规划的流程。通过对美国能源部相关GAO报告的主题建模，评估BERTopic和NMF在提取战略规划愿景要素主题的有效性，结果显示BERTopic能覆盖100%愿景要素且中高相关性占比超过一半，优于NMF，证明GAI辅助战略规划具可行性和实用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Generative_AI_for_Strategic_Plan_Development.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="191-LET-US-Long-Event-Text-Understanding-of-Scenes"><a href="#191-LET-US-Long-Event-Text-Understanding-of-Scenes" class="headerlink" title="191. LET-US: Long Event-Text Understanding of Scenes"></a>191. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LET-US__Long_Event-Text_Understanding_of_Scenes.pdf">LET-US: Long Event-Text Understanding of Scenes</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>该论文提出LET-US，一种针对长事件流与文本多模态理解的框架。方法包括跨模态引导压缩与层次聚类，显著减少输入事件数据体量并保留关键信息，通过两阶段优化（RGB预训练+事件流微调）实现事件-文本对齐，辅以大规模事件-文本数据集EIQA-1M和EVQA-Bench进行训练与评测。结果表明LET-US在事件流分类、推理、时序定位和描述等任务上大幅优于现有多模态大模型，实现对极长事件流的语义理解。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LET-US_Long_Event-Text_Understanding_of_Scenes.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="192-ALOPE-Adaptive-Layer-Optimization-for-Translation-Quality-Estimation-using-Large-Language-Models"><a href="#192-ALOPE-Adaptive-Layer-Optimization-for-Translation-Quality-Estimation-using-Large-Language-Models" class="headerlink" title="192. ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models"></a>192. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ALOPE__Adaptive_Layer_Optimization_for_Translation_Quality_Estimation_using_Large_Language_Models.pdf">ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Surrey</span></p><p>该论文提出ALOPE框架，通过在大语言模型（LLM）中集成回归头和低秩适配器（LoRA），实现对不同Transformer层进行自适应优化，以提升机器翻译质量估计（QE）任务的回归性能。ALOPE利用层内嵌回归头、动态加权及多头回归策略，有效提升了低资源语言对的翻译质量评估效果，并在多个公开数据集上超过了现有指令微调和编码器基线方法。结论指出，Transformer中间层（如TL-7）能捕捉更佳跨语种表示，ALOPE在多语言和单语回归任务中均表现优异，且具备高内存效率与可扩展性，适合资源受限环境。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ALOPE_Adaptive_Layer_Optimization_for_Translation_Quality_Estimation_using_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="193-Grounding-Natural-Language-for-Multi-agent-Decision-Making-with-Multi-agentic-LLMs"><a href="#193-Grounding-Natural-Language-for-Multi-agent-Decision-Making-with-Multi-agentic-LLMs" class="headerlink" title="193. Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs"></a>193. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Grounding_Natural_Language_for_Multi-agent_Decision-Making_with_Multi-agentic_LLMs.pdf">Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">UC Davis</span></p><p>本文提出了一个系统化的多智能体大语言模型（multi-agentic LLM）设计框架，将LLMs与多智能体决策算法结合，涵盖高级提示工程、记忆架构、多模态处理及通过微调实现对环境动态的对齐。实验表明，经过微调和机制设计的多智能体LLM能在经典博弈任务中实现更高效、可解释的协作与决策，支持复杂环境下的鲁棒和自适应行为。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Grounding_Natural_Language_for_Multi-agent_Decision-Making_with_Multi-agentic_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="194-Grounding-Multilingual-Multimodal-LLMs-With-Cultural-Knowledge"><a href="#194-Grounding-Multilingual-Multimodal-LLMs-With-Cultural-Knowledge" class="headerlink" title="194. Grounding Multilingual Multimodal LLMs With Cultural Knowledge"></a>194. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Grounding_Multilingual_Multimodal_LLMs_With_Cultural_Knowledge.pdf">Grounding Multilingual Multimodal LLMs With Cultural Knowledge</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>本文提出了一种面向数据的方法，通过从Wikidata等大型知识图谱中提取实体及图片，结合LLM自动生成多语言多模态视觉问答（VQA）对，构建了22M条涵盖39种语言和42个国家的文化多样数据集CulturalGround，并据此微调开放式多模态大模型CulturalPangea。实验表明，CulturalPangea在多个文化相关多语言多模态基准上实现了SOTA（平均提升+5%），同时不损失主流视觉-语言任务性能，证明了数据驱动的文化知识注入可有效提升模型的跨文化包容性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Grounding_Multilingual_Multimodal_LLMs_With_Cultural_Knowledge.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="195-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems"><a href="#195-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems" class="headerlink" title="195. A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems"></a>195. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Comprehensive_Survey_of_Self-Evolving_AI_Agents__A_New_Paradigm_Bridging_Foundation_Models_and_Lif.pdf">A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Glasgow</span></p><p>本论文系统综述了自进化AI智能体的发展，提出了一个统一的反馈优化框架，涵盖系统输入、智能体系统、环境和优化器四大核心组件，系统梳理了单智能体、多智能体及领域特定（如生物医学、编程、金融等）自进化技术。结论指出，自进化AI智能体通过持续反馈与自我优化，推动了基础模型向具有终身学习和适应能力的自主智能体系统的转变，是未来构建安全、高效、可持续AI生态系统的关键方向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Comprehensive_Survey_of_Self-Evolving_AI_Agents_A_New_Paradigm_Bridging_Foundation_Models_and_Lifelong_Agentic_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="196-Urbanite-A-Dataflow-Based-Framework-for-Human-AI-Interactive-Alignment-in-Urban-Visual-Analytics"><a href="#196-Urbanite-A-Dataflow-Based-Framework-for-Human-AI-Interactive-Alignment-in-Urban-Visual-Analytics" class="headerlink" title="196. Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics"></a>196. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Urbanite__A_Dataflow-Based_Framework_for_Human-AI_Interactive_Alignment_in_Urban_Visual_Analytics.pdf">Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Chicago</span></p><p>Urbanite提出了一套以数据流为基础的人机协同城市可视分析框架，核心创新是将大语言模型（LLM）与模块化数据流建模结合，通过自然语言描述、高层任务分解、可追溯过程、解释性和多粒度交互，将用户意图转化为可执行分析流程。结论显示，Urbanite有效降低了非技术专家构建复杂城市可视分析系统的门槛，提高了透明度、可追溯性和人机意图对齐，专家评测和定量实验表明其能高效生成、对齐和迭代城市分析任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Urbanite_A_Dataflow-Based_Framework_for_Human-AI_Interactive_Alignment_in_Urban_Visual_Analytics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="197-Rethinking-Domain-Specific-LLM-Benchmark-Construction-A-Comprehensiveness-Compactness-Approach"><a href="#197-Rethinking-Domain-Specific-LLM-Benchmark-Construction-A-Comprehensiveness-Compactness-Approach" class="headerlink" title="197. Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach"></a>197. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Rethinking_Domain-Specific_LLM_Benchmark_Construction__A_Comprehensiveness-Compactness_Approach.pdf">Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong Polytechnic University</span></p><p>本文提出了COMP-COMP框架，通过综合性-紧凑性原则，指导领域专属LLM基准的自动化构建：利用高斯核密度估计和语义空间嵌入，动态评估和优化语料与问答集的覆盖度和代表性，并通过迭代扩展补齐语义空白，实现评估效率和资源利用率的显著提升。实证于学术领域构建的大规模闭集基准XUBench显示，该方法仅用1.7%的问题数和46.4%的语料量即可实现与传统方法相当的评测效果，显著提升领域LLM评测的科学性和可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Rethinking_Domain-Specific_LLM_Benchmark_Construction_A_Comprehensiveness-Compactness_Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="198-ObfusQAte-A-Proposed-Framework-to-Evaluate-LLM-Robustness-on-Obfuscated-Factual-Question-Answering"><a href="#198-ObfusQAte-A-Proposed-Framework-to-Evaluate-LLM-Robustness-on-Obfuscated-Factual-Question-Answering" class="headerlink" title="198. ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering"></a>198. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ObfusQAte__A_Proposed_Framework_to_Evaluate_LLM_Robustness_on_Obfuscated_Factual_Question_Answering.pdf">ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indian Institute of Technology Patna</span></p><p>本文提出ObfusQAte技术和ObfusQA基准，通过多层次语言混淆（命名实体间接、干扰项引入、上下文过载）系统性评估大型语言模型（LLMs）在被混淆事实类问答任务中的鲁棒性。结论显示，主流LLM在处理此类复杂变体时准确率显著下降，凸显其推理和抗干扰能力的不足。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ObfusQAte_A_Proposed_Framework_to_Evaluate_LLM_Robustness_on_Obfuscated_Factual_Question_Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="199-HealthBranches-Synthesizing-Clinically-Grounded-Question-Answering-Datasets-via-Decision-Pathways"><a href="#199-HealthBranches-Synthesizing-Clinically-Grounded-Question-Answering-Datasets-via-Decision-Pathways" class="headerlink" title="199. HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways"></a>199. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HealthBranches__Synthesizing_Clinically-Grounded_Question_Answering_Datasets_via_Decision_Pathways.pdf">HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Calabria</span></p><p>该论文提出HealthBranches数据集，通过半自动化流程从医学决策路径中抽取结构化知识，生成真实病患案例及关联问答，涵盖17个医疗领域共4063个案例，每条数据均附有临床推理链。实验表明，结构化推理路径显著提升LLM在医学问答的多步推理能力，数据集也验证了LLM-as-a-judge与语义相似度指标与人工评价高度一致，为可信赖医学LLM评测和教育应用提供了坚实基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/HealthBranches_Synthesizing_Clinically-Grounded_Question_Answering_Datasets_via_Decision_Pathways.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="200-Fine-Tuning-Large-Language-Models-Using-EEG-Microstate-Features-for-Mental-Workload-Assessment"><a href="#200-Fine-Tuning-Large-Language-Models-Using-EEG-Microstate-Features-for-Mental-Workload-Assessment" class="headerlink" title="200. Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment"></a>200. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fine-Tuning_Large_Language_Models_Using_EEG_Microstate_Features_for_Mental_Workload_Assessment.pdf">Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technological University Dublin</span></p><p>该论文提出利用脑电（EEG）微状态特征，通过数据收集、微状态分割与回填、特征提取与提示工程，以及LLM模型选择与微调四个流程，优化大语言模型对认知负荷（休息&#x2F;负荷状态）的判别能力。实验结果显示微调后的LLM模型准确率显著提升，可有效实现认知负荷状态检测，推动认知神经科学与认知AI领域的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Fine-Tuning_Large_Language_Models_Using_EEG_Microstate_Features_for_Mental_Workload_Assessment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="201-MAQUA-Adaptive-Question-Asking-for-Multidimensional-Mental-Health-Screening-using-Item-Response-Theory"><a href="#201-MAQUA-Adaptive-Question-Asking-for-Multidimensional-Mental-Health-Screening-using-Item-Response-Theory" class="headerlink" title="201. MAQUA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory"></a>201. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MAQuA__Adaptive_Question-Asking_for_Multidimensional_Mental_Health_Screening_using_Item_Response_The.pdf">MAQUA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stony Brook University</span></p><p>本文提出了MAQUA，一种结合多目标语言建模、因子分析和多维项目反应理论（MIRT）的自适应提问框架，实现多维心理健康筛查。该方法通过优化每轮提问的信息增益，显著提升多维诊断准确性，并在不降低准确率的前提下将问题数量减少50-87%，有效降低用户负担，适用于大模型驱动的交互式临床筛查。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MAQUA_Adaptive_Question-Asking_for_Multidimensional_Mental_Health_Screening_using_Item_Response_Theory.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="202-LLM-based-Agents-for-Automated-Confounder-Discovery-and-Subgroup-Analysis-in-Causal-Inference"><a href="#202-LLM-based-Agents-for-Automated-Confounder-Discovery-and-Subgroup-Analysis-in-Causal-Inference" class="headerlink" title="202. LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference"></a>202. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LLM-based_Agents_for_Automated_Confounder_Discovery_and_Subgroup_Analysis_in_Causal_Inference.pdf">LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Sun Yat-Sen University</span></p><p>该论文提出了一种基于大语言模型（LLM）的智能体框架，通过迭代地自动发现混杂变量并进行亚组分析，结合因果树和专家知识检索（RAG），提升因果推断的精准性与可解释性。实验证明该方法在医疗数据上可有效缩小治疗效应估计的置信区间，减轻人工专家负担，并发现难以检测的混杂偏差。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LLM-based_Agents_for_Automated_Confounder_Discovery_and_Subgroup_Analysis_in_Causal_Inference.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="203-Can-Smaller-Large-Language-Models-Evaluate-Research-Quality"><a href="#203-Can-Smaller-Large-Language-Models-Evaluate-Research-Quality" class="headerlink" title="203. Can Smaller Large Language Models Evaluate Research Quality?"></a>203. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Can_Smaller_Large_Language_Models_Evaluate_Research_Quality_.pdf">Can Smaller Large Language Models Evaluate Research Quality?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Wolverhampton</span></p><p>本文系统评估了可下载的小型大语言模型（如Google Gemma-3-27b-it）在学术论文质量评估任务中的表现，方法为利用Gemma-3-27b-it对10万余篇英国REF 2021各学科论文的标题和摘要进行多次自动评分，并与专家评价的代理分数进行Spearman相关性分析。结果表明，小型离线LLM的评分与专家分数在几乎所有领域均显著正相关（平均相关强度为ChatGPT 4o的83.8%），且报告结构更标准化但多样性略低，重复评分对提升相关性作用有限。结论：离线可控的小型LLM可有效用于研究质量评估，虽略逊于最大模型，但适合安全、节约成本场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Can_Smaller_Large_Language_Models_Evaluate_Research_Quality_.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="204-AutoAssert-1-A-LoRA-Fine-Tuned-LLM-Model-for-Efficient-Automated-Assertion-Generation"><a href="#204-AutoAssert-1-A-LoRA-Fine-Tuned-LLM-Model-for-Efficient-Automated-Assertion-Generation" class="headerlink" title="204. AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation"></a>204. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AutoAssert_1__A_LoRA_Fine-Tuned_LLM_Model_for_Efficient_Automated_Assertion_Generation.pdf">AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Computing Technology, Chinese Academy of Sciences</span></p><p>本文提出了一种基于LoRA高效微调策略和Unsloth平台的轻量级大语言模型（LLM），用于自动生成硬件描述语言（HDL）的断言，显著降低了训练成本并提升了模型适应性和准确性。实验结果表明，该方法在生成语法和语义正确的断言方面优于传统方法，适合资源受限环境下的硬件验证与测试自动化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AutoAssert_1_A_LoRA_Fine-Tuned_LLM_Model_for_Efficient_Automated_Assertion_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="205-Hallucination-as-a-Computational-Boundary-A-Hierarchy-of-Inevitability-and-the-Oracle-Escape"><a href="#205-Hallucination-as-a-Computational-Boundary-A-Hierarchy-of-Inevitability-and-the-Oracle-Escape" class="headerlink" title="205. Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape"></a>205. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hallucination_as_a_Computational_Boundary__A_Hierarchy_of_Inevitability_and_the_Oracle_Escape.pdf">Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Changzhou University</span></p><p>本文将大语言模型（LLM）形式化为概率图灵机，提出“计算必然性层级”，并严格证明幻觉现象源于对角化、不可计算性和信息理论的边界。提出两条逃逸路径：一是将检索增强生成（RAG）建模为Oracle机器，理论上证明其可绝对规避幻觉；二是提出基于神经博弈论的新型持续学习机制作为内部Oracle，通过实验验证RAG-CL混合策略在准确性、效率和鲁棒性上的优势。结论认为，幻觉是LLM的根本限制，需通过“计算类对齐”（CCA）原则使任务复杂度与模型计算能力严格匹配，以保障AI安全可靠部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Hallucination_as_a_Computational_Boundary_A_Hierarchy_of_Inevitability_and_the_Oracle_Escape.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="206-Efficient-Edge-LLMs-Deployment-via-Hessian-Aware-Quantization-and-CPU–GPU-Collaborative"><a href="#206-Efficient-Edge-LLMs-Deployment-via-Hessian-Aware-Quantization-and-CPU–GPU-Collaborative" class="headerlink" title="206. Efficient Edge LLMs Deployment via Hessian-Aware Quantization and CPU–GPU Collaborative"></a>206. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Efficient_Edge_LLMs_Deployment_via_HessianAware_Quantization_and_CPU_GPU_Collaborative.pdf">Efficient Edge LLMs Deployment via Hessian-Aware Quantization and CPU–GPU Collaborative</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong Polytechnic University</span></p><p>本文提出了一种基于Hessian感知量化（HAQ）和CPU-GPU协同推理的高效MoE大语言模型边缘部署方案。方法包括自适应激活平滑与Hessian矩阵权重量化，实现激活与权重的联合8比特量化，有效抑制异常值导致的精度损失，并结合专家激活路径统计设计专家级协同卸载与缓存机制，在GPU和CPU间高效分配专家模块，大幅降低内存占用和推理延迟。实验证明该方法在多个主流大模型和数据集上低比特量化下接近全精度性能，显著减少60%显存占用并提升推理延迟稳定性，为边缘环境下MoE大模型高效部署提供了实用技术路径与工程参考。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Efficient_Edge_LLMs_Deployment_via_Hessian-Aware_Quantization_and_CPU%E2%80%93GPU_Collaborative.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="207-DocR1-Evidence-Page-Guided-GRPO-for-Multi-Page-Document-Understanding"><a href="#207-DocR1-Evidence-Page-Guided-GRPO-for-Multi-Page-Document-Understanding" class="headerlink" title="207. DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding"></a>207. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DocR1__Evidence_Page-Guided_GRPO_for_Multi-Page_Document_Understanding.pdf">DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>该论文提出DocR1，一种面向多页文档理解的多模态大语言模型（MLLM），采用新颖的强化学习（RL）框架EviGRPO（Evidence Page-Guided GRPO）。EviGRPO结合证据页奖励，推动模型先检索相关页面再进行推理，通过两阶段数据标注和课程学习训练，利用有限监督数据高效提升推理能力。实验结果显示，DocR1在多个公开多页文档基准上达到最优性能，同时保持单页任务的强表现，显著提升多页推理的准确率和证据定位能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DocR1_Evidence_Page-Guided_GRPO_for_Multi-Page_Document_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="208-MCITlib-Multimodal-Continual-Instruction-Tuning-Library-and-Benchmark"><a href="#208-MCITlib-Multimodal-Continual-Instruction-Tuning-Library-and-Benchmark" class="headerlink" title="208. MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark"></a>208. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MCITlib__Multimodal_Continual_Instruction_Tuning_Library_and_Benchmark.pdf">MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Advanced Interdisciplinary Sciences, UCAS</span></p><p>本文提出了MCITlib，这是一个用于多模态大语言模型（MLLMs）持续指令微调的开源代码库与基准，集成了8种代表性MCIT算法，并在精选的两个无信息泄漏基准上系统评测了各种方法的优劣。结论表明MCITlib为多模态持续学习领域的研究提供了公平、统一和可扩展的实验平台，推动了该领域的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MCITlib_Multimodal_Continual_Instruction_Tuning_Library_and_Benchmark.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="209-EndoAgent-A-Memory-Guided-Reflective-Agent-for-Intelligent-Endoscopic-Vision-to-Decision-Reasoning"><a href="#209-EndoAgent-A-Memory-Guided-Reflective-Agent-for-Intelligent-Endoscopic-Vision-to-Decision-Reasoning" class="headerlink" title="209. EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning"></a>209. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EndoAgent__A_Memory-Guided_Reflective_Agent_for_Intelligent_Endoscopic_Vision-to-Decision_Reasoning.pdf">EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southeast University</span></p><p>该论文提出了EndoAgent框架，通过双记忆机制和多轮反思，协调专家工具完成内镜视觉到决策推理任务。方法融合短期记忆记录行动轨迹和长期记忆积累经验，支持分类、检测、分割、编辑、VQA和报告生成等六大任务，并开发了EndoAgentBench基准用于系统评估。实验表明EndoAgent在细粒度视觉理解和开放式语言生成上均优于现有通用及医学多模态大模型，展现了极强的灵活性和推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/EndoAgent_A_Memory-Guided_Reflective_Agent_for_Intelligent_Endoscopic_Vision-to-Decision_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="210-“Pull-or-Not-to-Pull-”-Investigating-Moral-Biases-in-Leading-Large-Language-Models-Across-Ethical-Dilemmas"><a href="#210-“Pull-or-Not-to-Pull-”-Investigating-Moral-Biases-in-Leading-Large-Language-Models-Across-Ethical-Dilemmas" class="headerlink" title="210. “Pull or Not to Pull?”: Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas"></a>210. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/_Pull_or_Not_to_Pull_''__Investigating_Moral_Biases_in_Leading_Large_Language_Models_Across_Ethical_.pdf">“Pull or Not to Pull?”: Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of New South Wales</span></p><p>该论文系统评估了14个主流大语言模型（LLMs）在27种不同电车难题下的道德推理表现，涵盖了推理增强与通用模型，以及十种伦理框架（如功利主义、义务论、利他主义等），通过分因子提示协议收集3780组二元决策与自然语言解释，分析决策果断性、解释-答案一致性、人类道德对齐度及对无关伦理线索的敏感性。结果显示，推理增强模型更果断且解释结构更好，但未必更符合人类共识，尤其在亲情、法律或自利等框架下，模型易产生有争议的结论，公平、利他和美德伦理提示下模型表现出决策果断、低解释冲突和更接近人类判断的“甜区”。该研究建议将道德推理纳入LLM对齐主轴，并推动标准化伦理基准的建立。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/%E2%80%9CPull_or_Not_to_Pull_%E2%80%9D_Investigating_Moral_Biases_in_Leading_Large_Language_Models_Across_Ethical_Dilemmas.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="211-Selection-and-Exploitation-of-High-Quality-Knowledge-from-Large-Language-Models-for-Recommendation"><a href="#211-Selection-and-Exploitation-of-High-Quality-Knowledge-from-Large-Language-Models-for-Recommendation" class="headerlink" title="211. Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation"></a>211. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Selection_and_Exploitation_of_High-Quality_Knowledge_from_Large_Language_Models_for_Recommendation.pdf">Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Technology</span></p><p>本文提出了KSER（Knowledge Selection &amp; Exploitation Recommendation）框架，通过设计知识过滤模块（ESFNet）对LLM生成的知识进行自适应选择与过滤，并通过注意力机制实现语义嵌入与推荐特征空间的对齐。实验表明，该方法能有效提升主流推荐模型的性能，并且提出的extractor-only训练策略可在几乎不改动主干模型的情况下提升模型效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Selection_and_Exploitation_of_High-Quality_Knowledge_from_Large_Language_Models_for_Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="212-What-One-Cannot-Two-Can-Two-Layer-Transformers-Provably-Represent-Induction-Heads-on-Any-Order-Markov-Chains"><a href="#212-What-One-Cannot-Two-Can-Two-Layer-Transformers-Provably-Represent-Induction-Heads-on-Any-Order-Markov-Chains" class="headerlink" title="212. What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains"></a>212. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/What_One_Cannot,_Two_Can__Two-Layer_Transformers_Provably_Represent_Induction_Heads_on_Any-Order_Mar.pdf">What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Massachusetts Institute of Technology</span></p><p>该论文提出并理论证明了仅用两层、每层一个注意力头的Transformer即可表达任意阶Markov过程的条件k-gram模型（归纳头机制），改进了此前至少三层结构的结果。作者还分析了在一阶Markov链数据上，通过梯度下降训练该结构的学习动态，证明浅层Transformer可以高效实现强大的in-context learning能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/What_One_Cannot%2C_Two_Can_Two-Layer_Transformers_Provably_Represent_Induction_Heads_on_Any-Order_Markov_Chains.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="213-Adapting-LLMs-to-Time-Series-Forecasting-via-Temporal-Heterogeneity-Modeling-and-Semantic-Alignment"><a href="#213-Adapting-LLMs-to-Time-Series-Forecasting-via-Temporal-Heterogeneity-Modeling-and-Semantic-Alignment" class="headerlink" title="213. Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment"></a>213. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Adapting_LLMs_to_Time_Series_Forecasting_via_Temporal_Heterogeneity_Modeling_and_Semantic_Alignment.pdf">Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tianjin University</span></p><p>本文提出了TALON框架，利用异质时序编码器对多变量时间序列进行分段建模，并通过语义对齐模块将时间序列特征映射为LLM兼容的表示，实现无提示高效推理。大量实验表明，TALON在七个真实数据集上对比SOTA方法平均MSE提升达11%，验证了异构建模和语义对齐在提升LLM时序预测性能方面的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Adapting_LLMs_to_Time_Series_Forecasting_via_Temporal_Heterogeneity_Modeling_and_Semantic_Alignment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="214-Multi-Dimensional-Summarization-Agents-with-Context-Aware-Reasoning-over-Enterprise-Tables"><a href="#214-Multi-Dimensional-Summarization-Agents-with-Context-Aware-Reasoning-over-Enterprise-Tables" class="headerlink" title="214. Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables"></a>214. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-Dimensional_Summarization_Agents_with_Context-Aware_Reasoning_over_Enterprise_Tables.pdf">Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amazon</span></p><p>该论文提出一种基于多智能体的框架，用于企业多维表格数据的自动化语义摘要，核心流程包括SliceAgent切分数据、VarianceAgent计算指标变化、ContextAgent融合上下文、SummaryAgent调用LLM生成商业叙述，采用LangGraph实现模块化管控。实验证明该方法在数据忠实度（83%）、重要变动覆盖率和业务相关性（4.4&#x2F;5）方面均优于传统模板与平铺式LLM，尤其能捕捉细微业务权衡，提升企业报告智能化水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multi-Dimensional_Summarization_Agents_with_Context-Aware_Reasoning_over_Enterprise_Tables.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="215-Dynamic-Benchmark-Construction-for-Evaluating-Large-Language-Models-on-Real-World-Codes"><a href="#215-Dynamic-Benchmark-Construction-for-Evaluating-Large-Language-Models-on-Real-World-Codes" class="headerlink" title="215. Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes"></a>215. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Dynamic_Benchmark_Construction_for_Evaluating_Large_Language_Models_on_Real-World_Codes.pdf">Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>本文提出了CODE2BENCH，一个自动化端到端框架，可从真实GitHub代码库动态构建抗污染、严谨的代码生成基准。关键方法包括：自动动态数据收集、Scope Graph依赖分析实现任务分类，以及基于属性的自动化高覆盖率测试套件生成。通过CODE2BENCH-2505基准，对16种主流LLM进行评测发现：模型在需要复杂逻辑和跨语言迁移的自包含任务（SC）上表现不佳，在常见库交互任务（WSC，Python）上表现更好。该方法有效揭示了LLM在真实软件开发任务中的弱点，为代码生成模型的现实评估提供了新基准。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Dynamic_Benchmark_Construction_for_Evaluating_Large_Language_Models_on_Real-World_Codes.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="216-Improved-Personalized-Headline-Generation-via-Denoising-Fake-Interests-from-Implicit-Feedback"><a href="#216-Improved-Personalized-Headline-Generation-via-Denoising-Fake-Interests-from-Implicit-Feedback" class="headerlink" title="216. Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback"></a>216. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Improved_Personalized_Headline_Generation_via_Denoising_Fake_Interests_from_Implicit_Feedback.pdf">Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Henan Institute of Advanced Technology, Zhengzhou University</span></p><p>本文提出PHG-DIF框架，通过对用户点击历史中的噪声（如短停留时间和异常点击高峰）进行双重过滤，有效去除与个性化无关的假兴趣，并结合多层次时间融合动态建模用户多维兴趣，实现精准用户建模。实验结果显示，该方法在DT-PENS新数据集上显著提升了个性化新闻标题生成的质量，优于多种主流方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Improved_Personalized_Headline_Generation_via_Denoising_Fake_Interests_from_Implicit_Feedback.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="217-A-Stable-and-Principled-Loss-Function-for-Direct-Language-Model-Alignment"><a href="#217-A-Stable-and-Principled-Loss-Function-for-Direct-Language-Model-Alignment" class="headerlink" title="217. A Stable and Principled Loss Function for Direct Language Model Alignment"></a>217. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Stable_and_Principled_Loss_Function_for_Direct_Language_Model_Alignment.pdf">A Stable and Principled Loss Function for Direct Language Model Alignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>该论文提出了一种新型的Stable Preference Optimization (SPO)损失函数，用于直接对齐大语言模型（LLMs）与人类偏好，改进了现有DPO方法的理论不一致性。通过理论分析和梯度对比，SPO避免了DPO的梯度爆炸和奖励劫持问题，实验结果表明在Qwen2.5-7B和Llama-3-8B模型上，SPO在偏好对齐任务中显著优于DPO，并与更大规模模型性能竞争。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Stable_and_Principled_Loss_Function_for_Direct_Language_Model_Alignment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="218-DySK-Attn-A-Framework-for-Efficient-Real-Time-Knowledge-Updating-in-Large-Language-Models-via-Dynamic-Sparse-Knowledge-Attention"><a href="#218-DySK-Attn-A-Framework-for-Efficient-Real-Time-Knowledge-Updating-in-Large-Language-Models-via-Dynamic-Sparse-Knowledge-Attention" class="headerlink" title="218. DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention"></a>218. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DySK-Attn__A_Framework_for_Efficient,_Real-Time_Knowledge_Updating_in_Large_Language_Models_via_Dyna.pdf">DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">San Francisco State University</span></p><p>该论文提出DySK-Attn框架，将大型语言模型与可实时更新的知识图谱结合，通过动态稀疏知识注意力机制实现高效的知识检索和融合。实验结果表明，该方法在时间敏感问答任务中显著提升了新知识的准确性和推理效率，相较于RAG和模型编辑等主流方法更优，且支持毫秒级知识更新，适用于实际动态环境。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DySK-Attn_A_Framework_for_Efficient%2C_Real-Time_Knowledge_Updating_in_Large_Language_Models_via_Dynamic_Sparse_Knowledge_Attention.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="219-Schema-Lineage-Extraction-at-Scale-Multilingual-Pipelines-Composite-Evaluation-and-Language-Model-Benchmarks"><a href="#219-Schema-Lineage-Extraction-at-Scale-Multilingual-Pipelines-Composite-Evaluation-and-Language-Model-Benchmarks" class="headerlink" title="219. Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks"></a>219. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Schema_Lineage_Extraction_at_Scale__Multilingual_Pipelines,_Composite_Evaluation,_and_Language-Model.pdf">Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Microsoft</span></p><p>本文提出了一种面向多语言企业数据管道脚本的自动化细粒度模式血缘提取框架，关键方法包括标准化四要素（源模式、源表、变换逻辑、聚合操作）抽取流程和新型SLiCE复合指标以评估结构与语义正确性，并构建了包含1700个真实注释样本的基准数据集。实验表明，随着模型规模和推理丰富度提升，血缘提取效果显著增强，32B开源模型配合单次链式推理已可媲美GPT-4系列，为企业内部经济高效部署提供可行路径。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Schema_Lineage_Extraction_at_Scale_Multilingual_Pipelines%2C_Composite_Evaluation%2C_and_Language-Model_Benchmarks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="220-LL3M-Large-Language-3D-Modelers"><a href="#220-LL3M-Large-Language-3D-Modelers" class="headerlink" title="220. LL3M: Large Language 3D Modelers"></a>220. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LL3M__Large_Language_3D_Modelers.pdf">LL3M: Large Language 3D Modelers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Chicago</span></p><p>LL3M提出了一种利用多智能体大语言模型系统，通过编写可解释的Python代码在Blender中生成和编辑3D资产的方法。该系统结合检索增强生成（RAG）与多阶段自动与用户引导的迭代流程，实现了高质量、可编辑和模块化的3D资产生成。实验结果显示LL3M能生成结构清晰、细节丰富且易于人类理解和修改的3D模型，显著优于单智能体方法，并支持高效的用户交互与持续迭代优化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LL3M_Large_Language_3D_Modelers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="221-Multi-head-Transformers-Provably-Learn-Symbolic-Multi-step-Reasoning-via-Gradient-Descent"><a href="#221-Multi-head-Transformers-Provably-Learn-Symbolic-Multi-step-Reasoning-via-Gradient-Descent" class="headerlink" title="221. Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent"></a>221. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-head_Transformers_Provably_Learn_Symbolic_Multi-step_Reasoning_via_Gradient_Descent.pdf">Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>该论文提出理论分析，证明一层多头Transformer可通过梯度下降学习符号化多步推理任务（如树结构路径查找），实现链式思维（Chain-of-Thought）推理。作者构建模型并详细分析训练动态，证明多头注意力能自主分工与协调，高效解决多阶段推理任务，并具备对未见结构的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multi-head_Transformers_Provably_Learn_Symbolic_Multi-step_Reasoning_via_Gradient_Descent.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="222-Learning-User-Preferences-for-Image-Generation-Models"><a href="#222-Learning-User-Preferences-for-Image-Generation-Models" class="headerlink" title="222. Learning User Preferences for Image Generation Models"></a>222. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_User_Preferences_for_Image_Generation_Model.pdf">Learning User Preferences for Image Generation Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Renmin University of China</span></p><p>本文提出了一种基于多模态大语言模型（MLLM）的用户个性化偏好学习方法，通过引入对比偏好损失和可学习的偏好token，实现了从用户历史交互中建模细粒度的“喜欢&#x2F;不喜欢”关系。实验结果表明，该方法在用户偏好预测准确率以及对个性化内容生成的指导能力上优于现有方法，能有效挖掘用户共性与个性，提升模型泛化与定制能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Learning_User_Preferences_for_Image_Generation_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="223-Human-Alignment-and-Calibration-of-Inference-Time-Uncertainty-in-Large-Language-Models"><a href="#223-Human-Alignment-and-Calibration-of-Inference-Time-Uncertainty-in-Large-Language-Models" class="headerlink" title="223. Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models"></a>223. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Human-Alignment_and_Calibration_of_Inference-Time_Uncertainty_in_Large_Language_Models.pdf">Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Vanderbilt University</span></p><p>本文提出并系统评估了多种推理时大语言模型（LLM）不确定性量化方法，重点分析其与人类群体不确定性和模型校准性的对齐程度。核心方法包括对模型输出概率分布的熵、top-k和top-p采样等指标进行统计，并以人类问卷数据为对照，发现choice entropy、top-k entropy等度量与人类不确定性高度相关，同时在MMLU数据集上显示出统计显著的校准能力。结论指出，部分熵类推理时不确定性指标不仅与人类不确定性行为高度一致，还具备模型校准性，为提升人机交互体验与信任度提供基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Human-Alignment_and_Calibration_of_Inference-Time_Uncertainty_in_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="224-Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning"><a href="#224-Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning" class="headerlink" title="224. Capabilities of GPT-5 on Multimodal Medical Reasoning"></a>224. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Capabilities_of_GPT-5_on_Multimodal_Medical_Reasoning.pdf">Capabilities of GPT-5 on Multimodal Medical Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Emory University</span></p><p>该论文系统评估了GPT-5在医学多模态推理中的能力，采用统一的zero-shot链式思维(CoT)提示，涵盖文本和视觉问答任务，并与GPT-4o、人类专家等进行对比。结果显示GPT-5在MedQA、MedXpertQA、MMLU医学子集、USMLE和VQA-RAD等标准数据集上均达到了最先进的推理和理解表现，显著超越GPT-4o和人类专家，尤其在多模态推理上提升明显，表明其可作为临床决策支持核心组件。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Capabilities_of_GPT-5_on_Multimodal_Medical_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="225-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning"><a href="#225-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning" class="headerlink" title="225. Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning"></a>225. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Part_I__Tricks_or_Traps__A_Deep_Dive_into_RL_for_LLM_Reasoning.pdf">Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Alibaba Group</span></p><p>本论文系统性评测并剖析了当前主流的RL（强化学习）技术在大语言模型（LLM）推理任务中的作用，通过统一开源框架，对Normalization、Clipping、Filtering与Loss Aggregation等技术进行细粒度实验，揭示其在多种数据难度、模型架构下的适用性和机制。结论表明，简单结合group-level mean与batch-level std的优势归一化及token-level loss aggregation两项技术即可在无critic的PPO基础上显著提升RL4LLM性能，优于现有复杂算法，并给出明确技术选择指南。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Part_I_Tricks_or_Traps_A_Deep_Dive_into_RL_for_LLM_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="226-Spatial-ORMLLM-Improve-Spatial-Relation-Understanding-in-the-Operating-Room-with-Multimodal-Large-Language-Model"><a href="#226-Spatial-ORMLLM-Improve-Spatial-Relation-Understanding-in-the-Operating-Room-with-Multimodal-Large-Language-Model" class="headerlink" title="226. Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model"></a>226. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Spatial-ORMLLM__Improve_Spatial_Relation_Understanding_in_the_Operating_Room_with_Multimodal_Large_L.pdf">Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hunan University</span></p><p>该论文提出Spatial-ORMLLM，一种专为手术室3D空间推理设计的多模态大语言模型，在仅使用RGB图像的条件下，通过3D空间模块生成伪深度图、全景分割和点云，并在空间增强特征融合模块中将这些模态投影为统一Token输入LLM，实现了无外部3D传感器的空间关系推理。实验表明，Spatial-ORMLLM在多个手术室空间理解基准和下游任务上均取得了领先性能，证实了即使在输入受限下也能实现高精度空间推理。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Spatial-ORMLLM_Improve_Spatial_Relation_Understanding_in_the_Operating_Room_with_Multimodal_Large_Language_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="227-PYVERITAS-On-Verifying-Python-via-LLM-Based-Transpilation-and-Bounded-Model-Checking-for-C"><a href="#227-PYVERITAS-On-Verifying-Python-via-LLM-Based-Transpilation-and-Bounded-Model-Checking-for-C" class="headerlink" title="227. PYVERITAS: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C"></a>227. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PyVeritas__On_Verifying_Python_via_LLM-Based_Transpilation_and_Bounded_Model_Checking_for_C.pdf">PYVERITAS: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Oxford</span></p><p>该论文提出PYVERITAS框架，利用大语言模型（LLM）将Python代码高效转译为C代码，并结合CBMC进行有界模型检测和MaxSAT故障定位，实现对Python程序的形式化验证和可解释的错误定位。实验表明，LLM（如QWEN2.5-CODER）能在80%以上的案例中生成语义忠实的C代码，验证和定位Python代码中的错误，为Python程序员提供实际可用的验证与诊断工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/PYVERITAS_On_Verifying_Python_via_LLM-Based_Transpilation_and_Bounded_Model_Checking_for_C.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="228-Can-LLMs-Detect-Their-Confabulations-Estimating-Reliability-in-Uncertainty-Aware-Language-Models"><a href="#228-Can-LLMs-Detect-Their-Confabulations-Estimating-Reliability-in-Uncertainty-Aware-Language-Models" class="headerlink" title="228. Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models"></a>228. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Can_LLMs_Detect_Their_Confabulations__Estimating_Reliability_in_Uncertainty-Aware_Language_Models.pdf">Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KTH Royal Institute of Technology</span></p><p>该论文提出了一种基于不确定性引导的探针方法，通过分析大语言模型（LLM）在不同上下文（无、正确、误导）下的行为，结合 token 级不确定性（aleatoric 和 epistemic）从内部隐状态聚合特征，提升回答可靠性检测。实验证明，准确的上下文可提升模型正确性和置信度，而误导性上下文则导致模型自信地产生错误答案，且不确定性信号与实际正确性存在偏差；所提探针方法优于直接不确定性度量，能更有效检测多种开源LLM的不可靠输出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Can_LLMs_Detect_Their_Confabulations_Estimating_Reliability_in_Uncertainty-Aware_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="229-Optimal-Transport-Regularization-for-Speech-Text-Alignment-in-Spoken-Language-Models"><a href="#229-Optimal-Transport-Regularization-for-Speech-Text-Alignment-in-Spoken-Language-Models" class="headerlink" title="229. Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models"></a>229. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Optimal_Transport_Regularization_for_Speech_Text_Alignment_in_Spoken_Language_Models.pdf">Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mashang Consumer Finance Co., Ltd.</span></p><p>本论文提出了一种名为Optimal Transport Regularization (OTReg)的新方法，用于通过最优传输理论将语音和文本嵌入对齐，从而缩小语音与文本间的模态差距，提升Spoken Language Models（SLMs）的泛化能力。方法流程包括两阶段训练：第一阶段标准有监督微调，第二阶段引入OTReg正则化损失和基于相似度的语音嵌入压缩，无需额外标签或参数，可无缝集成至现有SLM训练流程。实验表明，OTReg显著改善了多语种ASR任务中SLM的跨数据集泛化能力和语音文本对齐效果，提高了模型鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Optimal_Transport_Regularization_for_Speech_Text_Alignment_in_Spoken_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="230-TBAC-UniImage-Unified-Understanding-and-Generation-by-Ladder-Side-Diffusion-Tuning"><a href="#230-TBAC-UniImage-Unified-Understanding-and-Generation-by-Ladder-Side-Diffusion-Tuning" class="headerlink" title="230. TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning"></a>230. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TBAC-UniImage__Unified_Understanding_and_Generation_by_Ladder-Side_Diffusion_Tuning.pdf">TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Basic Algorithm Center, PCG, Tencent</span></p><p>本文提出TBAC-UniImage，一种通过Ladder-Side Diffusion Tuning深度融合多模态大语言模型（MLLM）与扩散生成模型（DiT）的统一理解与生成架构。该方法通过在MLLM的不同中间层插入可学习查询，并将其多层隐藏状态分层输入DiT，实现更细粒度的跨模态条件生成，显著提升了理解与生成的统一性。实验结果表明，TBAC-UniImage在GenEval、DPG-Bench和TIIF-Bench等基准上取得了与主流方法媲美甚至更优的表现，尤其在复杂指令和高级跟随任务中展现出强大能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TBAC-UniImage_Unified_Understanding_and_Generation_by_Ladder-Side_Diffusion_Tuning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="231-Dual-Information-Speech-Language-Models-for-Emotional-Conversations"><a href="#231-Dual-Information-Speech-Language-Models-for-Emotional-Conversations" class="headerlink" title="231. Dual Information Speech Language Models for Emotional Conversations"></a>231. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Dual_Information_Speech_Language_Models_for_Emotional_Conversations.pdf">Dual Information Speech Language Models for Emotional Conversations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mashang Consumer Finance Co., Ltd.</span></p><p>本文提出了一种高效的情感对话语音语言模型（SLM）扩展方法，通过设计异构双适配器结构分别提取说话者语音中的副语言信息和语言信息，并采用弱监督训练策略实现信息解耦和上下文理解。实验结果显示，该方法仅需训练两个适配器，参数及数据高效，在情感对话任务上取得了与主流模型相当甚至更优的表现，有效融合了副语言与语言信息，提升了模型的情感识别与对话能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Dual_Information_Speech_Language_Models_for_Emotional_Conversations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="232-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches"><a href="#232-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches" class="headerlink" title="232. HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches"></a>232. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HierSearch__A_Hierarchical_Enterprise_Deep_Search_Framework_Integrating_Local_and_Web_Searches.pdf">HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Gaoling School of Artificial Intelligence, Renmin University of China</span></p><p>本文提出了HierSearch框架，采用分层强化学习（HRL）训练本地深度搜索代理和Web深度搜索代理，并由高层规划代理协调低层代理进行多知识源深度搜索。通过引入知识精炼器过滤冗余及幻觉信息，HierSearch在多个通用、财经、医疗领域基准上超越了平坦RL和多源RAG等主流方法，显著提升了搜索与推理的准确性与效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/HierSearch_A_Hierarchical_Enterprise_Deep_Search_Framework_Integrating_Local_and_Web_Searches.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="233-Robust-Anomaly-Detection-in-O-RAN-Leveraging-LLMs-against-Data-Manipulation-Attacks"><a href="#233-Robust-Anomaly-Detection-in-O-RAN-Leveraging-LLMs-against-Data-Manipulation-Attacks" class="headerlink" title="233. Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks"></a>233. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Robust_Anomaly_Detection_in_O-RAN__Leveraging_LLMs_against_Data_Manipulation_Attacks.pdf">Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Monash University</span></p><p>本论文提出在5G O-RAN架构中利用大语言模型（LLMs）作为xApp进行异常检测，以应对通过恶意xApp进行的Unicode数据操控（如hypoglyphs）攻击。方法上，作者将传统ML模型（如AutoEncoder）与Llama-3.1-8B-Instruct对比，发现传统模型遇到数据操控时会崩溃，而LLM可稳健处理所有输入并维持低于0.07秒的检测延迟，展示出对数据操控的强抗干扰能力。结论是LLM能显著提升O-RAN安全性，未来通过提示工程有望进一步优化检测准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Robust_Anomaly_Detection_in_O-RAN_Leveraging_LLMs_against_Data_Manipulation_Attacks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="234-From-Natural-Language-to-Solver-Ready-Power-System-Optimization-An-LLM-Assisted-Validation-in-the-Loop-Framework"><a href="#234-From-Natural-Language-to-Solver-Ready-Power-System-Optimization-An-LLM-Assisted-Validation-in-the-Loop-Framework" class="headerlink" title="234. From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework"></a>234. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Natural_Language_to_Solver-Ready_Power_System_Optimization__An_LLM-Assisted,_Validation-in-the-.pdf">From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Southern California</span></p><p>本文提出了一种基于大型语言模型（LLM）辅助的智能代理系统，将电力系统优化场景的自然语言描述自动转化为可直接求解的数学优化模型，并结合系统化验证与迭代修复机制保证模型可行性。该方法通过领域感知的提示词、结构化模式和验证-修复环节，提升了模型的准确性和可靠性，并通过GNN辅助分支策略加速MILP求解。实验结果表明，该系统在单元承诺问题上实现了100%可行率和明显的计算加速，优于直接用LLM生成解的基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/From_Natural_Language_to_Solver-Ready_Power_System_Optimization_An_LLM-Assisted%2C_Validation-in-the-Loop_Framework.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="235-MuaLLM-A-Multimodal-Large-Language-Model-Agent-for-Circuit-Design-Assistance-with-Hybrid-Contextual-Retrieval-Augmented-Generation"><a href="#235-MuaLLM-A-Multimodal-Large-Language-Model-Agent-for-Circuit-Design-Assistance-with-Hybrid-Contextual-Retrieval-Augmented-Generation" class="headerlink" title="235. MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation"></a>235. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MuaLLM__A_Multimodal_Large_Language_Model_Agent_for_Circuit_Design_Assistance_with_Hybrid_Contextual.pdf">MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Utah</span></p><p>本文提出了MuaLLM，一个开源多模态大型语言模型（LLM）智能体，专为电路设计辅助而构建。MuaLLM结合了基于Reason+Act (ReAct) 的推理式多步检索与混合Sparse&#x2F;Dense检索的RAG框架，并集成自适应向量数据库和定制工具（如自动文献检索、数据库动态更新、自动电路网表生成），实现对文本和图像的高效检索和推理。实验表明，在电路设计文献问答与多步推理数据集上，MuaLLM可达到90.1%召回率和86.8%准确率，并且在处理大规模语料时较传统LLM更快、更低成本。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MuaLLM_A_Multimodal_Large_Language_Model_Agent_for_Circuit_Design_Assistance_with_Hybrid_Contextual_Retrieval-Augmented_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="236-BlindGuard-Safeguarding-LLM-based-Multi-Agent-Systems-under-Unknown-Attacks"><a href="#236-BlindGuard-Safeguarding-LLM-based-Multi-Agent-Systems-under-Unknown-Attacks" class="headerlink" title="236. BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks"></a>236. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BlindGuard__Safeguarding_LLM-based_Multi-Agent_Systems_under_Unknown_Attacks.pdf">BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Artificial Intelligence, Jilin University</span></p><p>本文提出BlindGuard，一种用于LLM驱动多智能体系统（MAS）的无监督防御方法，无需攻击样本标签或先验知识。方法通过层次化代理编码器整合个体、邻域和全局信息，并设计语义级特征扰动与对比学习的攻击检测器，仅用正常交互数据即可检测多种未知攻击（如提示注入、内存投毒、工具利用），再联合边裁剪隔离可疑代理。实验证明，BlindGuard在多种拓扑和攻击类型下均能有效检测并缓解攻击，防御性能优于其它无监督方法，接近有监督上限，且具备良好泛化和可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BlindGuard_Safeguarding_LLM-based_Multi-Agent_Systems_under_Unknown_Attacks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="237-TeamMedAgents-Enhancing-Medical-Decision-Making-of-LLMs-Through-Structured-Teamwork"><a href="#237-TeamMedAgents-Enhancing-Medical-Decision-Making-of-LLMs-Through-Structured-Teamwork" class="headerlink" title="237. TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork"></a>237. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TeamMedAgents__Enhancing_Medical_Decision-Making_of_LLMs_Through_Structured_Teamwork.pdf">TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois, Chicago</span></p><p>本文提出TeamMedAgents框架，将人类协作心理学的六大团队工作机制（领导力、互相绩效监控、团队导向、共享心智模型、闭环沟通和互信）以模块化方式集成到大语言模型（LLM）多智能体医学决策系统中。通过在八个医学基准上的系统实验与消融研究，发现多智能体团队机制能显著提升医学决策准确率，且不同任务需针对性选择团队机制，全面激活所有机制反而可能带来协作负担。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TeamMedAgents_Enhancing_Medical_Decision-Making_of_LLMs_Through_Structured_Teamwork.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="238-Assessing-LLM-Text-Detection-in-Educational-Contexts-Does-Human-Contribution-Affect-Detection"><a href="#238-Assessing-LLM-Text-Detection-in-Educational-Contexts-Does-Human-Contribution-Affect-Detection" class="headerlink" title="238. Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?"></a>238. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Assessing_LLM_Text_Detection_in_Educational_Contexts__Does_Human_Contribution_Affect_Detection_.pdf">Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bielefeld University</span></p><p>本文提出了GEDE（Generative Essay Detection in Education）数据集，涵盖900余篇学生写作与超12500篇不同贡献级别的LLM生成文本，并系统评测了多种主流检测方法（如DetectGPT、Fast-DetectGPT、Ghostbuster、RoBERTa及GPTZero）在教育场景下的表现，重点关注学生对文本的不同参与程度。实验发现：当前检测器在区分完全人写与完全LLM生成文本时表现较好，但面对人类与LLM混合修改（如小幅润色或重写）时，误报率和泛化能力大幅下降，尤其在教育场景下易误伤学生，仍不适合实际部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Assessing_LLM_Text_Detection_in_Educational_Contexts_Does_Human_Contribution_Affect_Detection_.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="239-Investigating-the-Design-Space-of-Visual-Grounding-in-Multimodal-Large-Language-Model"><a href="#239-Investigating-the-Design-Space-of-Visual-Grounding-in-Multimodal-Large-Language-Model" class="headerlink" title="239. Investigating the Design Space of Visual Grounding in Multimodal Large Language Model"></a>239. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Investigating_the_Design_Space_of_Visual_Grounding_in_Multimodal_Large_Language_Model.pdf">Investigating the Design Space of Visual Grounding in Multimodal Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Chicago</span></p><p>本文系统性分析了多模态大语言模型（MLLM）在视觉指代（Visual Grounding, VG）任务中的多种设计选择，涵盖预测格式、归一化方式、监督方式、边框格式与数据组织等关键环节，并以LLaVA-1.5为基线进行全面消融实验。结论表明，采用归一化的整数格式、左上-右下坐标、one-hot监督、去重对话、以及专用VG数据，能显著提升MLLM的VG性能（RefCOCO&#x2F;+&#x2F;g三项提升+5.6%&#x2F;+6.9%&#x2F;+7.0%），并为未来VG能力开发提供了清晰设计准则。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Investigating_the_Design_Space_of_Visual_Grounding_in_Multimodal_Large_Language_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="240-AdaptFlow-Adaptive-Workflow-Optimization-via-Meta-Learning"><a href="#240-AdaptFlow-Adaptive-Workflow-Optimization-via-Meta-Learning" class="headerlink" title="240. AdaptFlow: Adaptive Workflow Optimization via Meta-Learning"></a>240. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AdaptFlow__Adaptive_Workflow_Optimization_via_Meta-Learning.pdf">AdaptFlow: Adaptive Workflow Optimization via Meta-Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>该论文提出了AdaptFlow，一种基于MAML的自然语言元学习框架，通过双层优化（内循环基于LLM反馈细化工作流，外循环聚合并反思改进）实现工作流在不同子任务间的快速适应。实验覆盖问答、代码生成和数学推理等领域，结果显示AdaptFlow在通用性和性能上均优于手工和自动化基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AdaptFlow_Adaptive_Workflow_Optimization_via_Meta-Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="241-FEAT-A-Multi-Agent-Forensic-AI-System-with-Domain-Adapted-Large-Language-Model-for-Automated-Cause-of-Death-Analysis"><a href="#241-FEAT-A-Multi-Agent-Forensic-AI-System-with-Domain-Adapted-Large-Language-Model-for-Automated-Cause-of-Death-Analysis" class="headerlink" title="241. FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis"></a>241. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FEAT__A_Multi-Agent_Forensic_AI_System_with_Domain-Adapted_Large_Language_Model_for_Automated_Cause-.pdf">FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>本文提出FEAT系统，一种专为法医学死因分析设计的多智能体AI框架，集成了任务分解的Planner、证据分析的Local Solvers、反思与记忆模块、结论综合的Global Solver，并采用领域微调大语言模型（Forensic-LLM）、工具增强推理与分层检索生成（H-RAG）、人类反馈等关键技术。实验表明FEAT在中国六地大规模法医案件中，长篇分析和短结论准确率均显著超越SOTA，达到专家级水平，并具备优良泛化性和司法适用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/FEAT_A_Multi-Agent_Forensic_AI_System_with_Domain-Adapted_Large_Language_Model_for_Automated_Cause-of-Death_Analysis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="242-Not-Yet-AlphaFold-for-the-Mind-Evaluating-Centaur-as-a-Synthetic-Participant"><a href="#242-Not-Yet-AlphaFold-for-the-Mind-Evaluating-Centaur-as-a-Synthetic-Participant" class="headerlink" title="242. Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant"></a>242. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Not_Yet_AlphaFold_for_the_Mind__Evaluating_Centaur_as_a_Synthetic_Participant.pdf">Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Osnabrück University</span></p><p>本文系统评估了经过人类行为数据微调的LLM Centaur在三类认知任务（反转学习、horizon bandit任务和威斯康星卡片分类测试）中的预测与生成能力，重点比较其与基础LLM及领域特定模型的表现。结果表明，虽然Centaur在预测试验中表现优异，但在生成新行为时与人类数据存在显著偏差，尚不能作为可靠的“虚拟参与者”或认知模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Not_Yet_AlphaFold_for_the_Mind_Evaluating_Centaur_as_a_Synthetic_Participant.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="243-CATP-Contextually-Adaptive-Token-Pruning-for-Efficient-and-Enhanced-Multimodal-In-Context-Learning"><a href="#243-CATP-Contextually-Adaptive-Token-Pruning-for-Efficient-and-Enhanced-Multimodal-In-Context-Learning" class="headerlink" title="243. CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning"></a>243. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CATP__Contextually_Adaptive_Token_Pruning_for_Efficient_and_Enhanced_Multimodal_In-Context_Learning.pdf">CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Brown University</span></p><p>本文提出CATP，一种针对多模态大模型（LVLMs）在多模态in-context learning（ICL）场景下的无训练图像token剪枝方法。CATP通过两阶段流程：第一阶段在投影器与解码器之间结合语义对齐和特征多样性，第二阶段在浅层解码器中结合层间注意力差异与语义相关性，逐步筛选最关键的图像token。实验表明，CATP在四种LVLMs和八个基准上，平均保留22.2%的图像token后，推理速度提升10.78%，性能较原始模型提升0.6%，优于所有现有方法，显著增强多模态ICL的效率和实际应用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CATP_Contextually_Adaptive_Token_Pruning_for_Efficient_and_Enhanced_Multimodal_In-Context_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="244-Pareto-Multi-Objective-Alignment-for-Language-Models"><a href="#244-Pareto-Multi-Objective-Alignment-for-Language-Models" class="headerlink" title="244. Pareto Multi-Objective Alignment for Language Models"></a>244. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pareto_Multi-Objective_Alignment_for_Language_Models.pdf">Pareto Multi-Objective Alignment for Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ruhr University Bochum</span></p><p>本文提出了PAreto Multi-Objective Alignment (PAMA)算法，用于高效地实现大语言模型（LLMs）在多目标（如信息量与简洁性、乐于助人与创造力等）之间的高效对齐。PAMA将多目标强化学习人类反馈（RLHF）问题转化为可解析的凸优化问题，极大降低了计算复杂度（从O(n²d)降至O(n)），并理论证明其收敛到Pareto平衡点。实验结果表明，PAMA在不同规模的LLM上对多目标对齐表现出优越的性能和稳定性，显著优于常用基线方法，成为大模型多目标对齐的高效可扩展方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Pareto_Multi-Objective_Alignment_for_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="245-Interpreting-Fedspeak-with-Confidence-A-LLM-Based-Uncertainty-Aware-Framework-Guided-by-Monetary-Policy-Transmission-Paths"><a href="#245-Interpreting-Fedspeak-with-Confidence-A-LLM-Based-Uncertainty-Aware-Framework-Guided-by-Monetary-Policy-Transmission-Paths" class="headerlink" title="245. Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths"></a>245. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Interpreting_Fedspeak_with_Confidence__A_LLM-Based_Uncertainty-Aware_Framework_Guided_by_Monetary_Po.pdf">Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (Guangzhou)</span></p><p>本文提出了一种基于大语言模型（LLM）、结合货币政策传导机制的领域知识推理和动态不确定性解码的框架，对美联储Fedspeak文本进行货币政策立场分析。方法通过金融实体关系抽取与政策传导路径增强输入，并引入感知不确定性（PU）度量模型置信度，实现对预测可靠性的自动评估。实验显示，该框架在FOMC数据集上实现了当前最优表现，并证实高PU与模型错误率显著相关，提升了模型的透明性和实际应用可靠性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Interpreting_Fedspeak_with_Confidence_A_LLM-Based_Uncertainty-Aware_Framework_Guided_by_Monetary_Policy_Transmission_Paths.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="246-The-Escalator-Problem-Identifying-Implicit-Motion-Blindness-in-AI-for-Accessibility"><a href="#246-The-Escalator-Problem-Identifying-Implicit-Motion-Blindness-in-AI-for-Accessibility" class="headerlink" title="246. The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility"></a>246. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/The_Escalator_Problem__Identifying_Implicit_Motion_Blindness_in_AI_for_Accessibility.pdf">The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>该论文提出了“自动扶梯问题”，揭示了多模态大语言模型（MLLMs）在感知连续、低信号运动（如自动扶梯运行方向）方面的重大缺陷，称之为“隐性运动盲”。作者分析了现有模型采用稀疏帧采样造成运动信息丢失的技术根源，并指出该问题会削弱盲人及视障人士对助理AI的信任。结论是呼吁从单纯语义识别转向注重物理感知，建立以用户安全和信任为核心的新评测基准，推动AI可访问性领域的范式转变。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/The_Escalator_Problem_Identifying_Implicit_Motion_Blindness_in_AI_for_Accessibility.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="247-WECHAT-YATT-A-Simple-Scalable-and-Balanced-RLHF-Trainer"><a href="#247-WECHAT-YATT-A-Simple-Scalable-and-Balanced-RLHF-Trainer" class="headerlink" title="247. WECHAT-YATT: A Simple, Scalable and Balanced RLHF Trainer"></a>247. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/WeChat-YATT__A_Simple,_Scalable_and_Balanced_RLHF_Trainer.pdf">WECHAT-YATT: A Simple, Scalable and Balanced RLHF Trainer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent</span></p><p>该论文提出了WeChat-YATT框架，用于大规模人类反馈强化学习（RLHF）训练，核心方法包括并行控制器编程模型和动态资源分配策略，实现多模型高效协同和硬件资源自适应分配，显著提升大模型及多模态RLHF训练的吞吐量与GPU利用率。实验结果表明WeChat-YATT在多种场景下优于主流RLHF框架，并已成功部署于微信产品线，验证了其实际应用的可扩展性与鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/WECHAT-YATT_A_Simple%2C_Scalable_and_Balanced_RLHF_Trainer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="248-X-evolve-Solution-space-evolution-powered-by-large-language-models"><a href="#248-X-evolve-Solution-space-evolution-powered-by-large-language-models" class="headerlink" title="248. X-evolve: Solution space evolution powered by large language models"></a>248. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/_(X_)-evolve__Solution_space_evolution_powered_by_large_language_models.pdf">X-evolve: Solution space evolution powered by large language models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>X-evolve提出了一种结合大语言模型（LLMs）和进化算法（EAs）的方法，不再进化单个解，而是进化参数化定义的解空间（solution spaces），LLM生成带可调参数的程序，构成解空间，随后用得分反馈指导高效空间搜索。该方法在cap set问题、Shannon容量问题和NP难的在线装箱问题上，均取得了与或超越前沿工作的结果，且模型调用成本降低至原有方法的百分之一甚至千分之一。结论：X-evolve极大提升了解空间探索效率和突破复杂优化问题的能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/X-evolve_Solution_space_evolution_powered_by_large_language_models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="249-MIMIC-Multimodal-Inversion-for-Model-Interpretation-and-Conceptualization"><a href="#249-MIMIC-Multimodal-Inversion-for-Model-Interpretation-and-Conceptualization" class="headerlink" title="249. MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization"></a>249. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MIMIC__Multimodal_Inversion_for_Model_Interpretation_and_Conceptualization.pdf">MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Twente</span></p><p>该论文提出了一种多模态反演框架MIMIC，通过联合VLM反演与特征对齐目标，并引入空间、自然图像及语义正则化，实现了对视觉-语言模型（VLM）内部表征的可视化。实验证明MIMIC可高效合成与语义概念对齐的高保真图像，提升了VLM的可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MIMIC_Multimodal_Inversion_for_Model_Interpretation_and_Conceptualization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="250-EvoCoT-Overcoming-the-Exploration-Bottleneck-in-Reinforcement-Learning"><a href="#250-EvoCoT-Overcoming-the-Exploration-Bottleneck-in-Reinforcement-Learning" class="headerlink" title="250. EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning"></a>250. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EvoCoT__Overcoming_the_Exploration_Bottleneck_in_Reinforcement_Learning.pdf">EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>该论文提出了EvoCoT，一种自进化课程学习框架，通过两阶段Chain-of-Thought（CoT）推理优化提升大语言模型（LLM）在稀疏奖励下的探索与推理能力。方法包括自生成并验证CoT轨迹，逐步缩短推理路径以控制探索空间，实现LLM对难题的稳定自我学习，显著提升模型在多种数学基准上的表现。结论：EvoCoT能突破探索瓶颈，提升LLM在复杂推理任务中的表现，且无需外部监督或更强教师模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/EvoCoT_Overcoming_the_Exploration_Bottleneck_in_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="251-UniSVG-A-Unified-Dataset-for-Vector-Graphic-Understanding-and-Generation-with-Multimodal-Large-Language-Models"><a href="#251-UniSVG-A-Unified-Dataset-for-Vector-Graphic-Understanding-and-Generation-with-Multimodal-Large-Language-Models" class="headerlink" title="251. UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models"></a>251. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/UniSVG__A_Unified_Dataset_for_Vector_Graphic_Understanding_and_Generation_with_Multimodal_Large_Lang.pdf">UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了UniSVG，这是首个面向多模态大语言模型（MLLMs）的大规模、开放且多任务的SVG（矢量图）理解与生成数据集，涵盖Image2SVG、Text2SVG及SVG理解三大任务，并构建了配套基准测试体系。实验结果表明，在UniSVG上微调后，开源MLLM在SVG生成和理解任务上显著优于包括GPT-4V等在内的多种SOTA闭源模型，推动了MLLM在矢量图领域的研究进展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/UniSVG_A_Unified_Dataset_for_Vector_Graphic_Understanding_and_Generation_with_Multimodal_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="252-LoSemB-Logic-Guided-Semantic-Bridging-for-Inductive-Tool-Retrieval"><a href="#252-LoSemB-Logic-Guided-Semantic-Bridging-for-Inductive-Tool-Retrieval" class="headerlink" title="252. LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval"></a>252. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LoSemB__Logic-Guided_Semantic_Bridging_for_Inductive_Tool_Retrieval.pdf">LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong Polytechnic University</span></p><p>本文提出了一种逻辑引导的语义桥接框架LoSemB，用于大语言模型（LLMs）在动态工具库中高效进行归纳式工具检索。方法上，LoSemB包括逻辑特征提取与嵌入对齐模块，结合图卷积网络对指令-工具逻辑关系建模，并通过关系增强检索机制，融合逻辑约束与图增强相似度，显著提升面对未见工具时的检索准确性。实验表明，该方法在归纳和传导场景下均优于现有方法，能有效缓解工具分布漂移与相似性检索脆弱性问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LoSemB_Logic-Guided_Semantic_Bridging_for_Inductive_Tool_Retrieval.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="253-Semantic-Caching-for-Low-Cost-LLM-Serving-From-Offline-Learning-to-Online-Adaptation"><a href="#253-Semantic-Caching-for-Low-Cost-LLM-Serving-From-Offline-Learning-to-Online-Adaptation" class="headerlink" title="253. Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation"></a>253. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Semantic_Caching_for_Low-Cost_LLM_Serving__From_Offline_Learning_to_Online_Adaptation.pdf">Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>本文提出了一种面向低成本大语言模型（LLM）推理的语义缓存框架，涵盖了参数已知的oracle、离线学习和在线自适应三种场景，核心包括引入带有语义距离的损失函数、逆向贪心算法、置信上&#x2F;下界学习（CUCB-SC与CLCB-SC-LS），并在理论上给出近最优性能保证。实验结果显示该方法在子最优性间隙、平均遗憾和缓存切换次数等指标上均优于现有基线，显著提升了LLM服务效率与可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Semantic_Caching_for_Low-Cost_LLM_Serving_From_Offline_Learning_to_Online_Adaptation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="254-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks"><a href="#254-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks" class="headerlink" title="254. GLiClass: Generalist Lightweight Model for Sequence Classification Tasks"></a>254. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/GLiClass__Generalist_Lightweight_Model_for_Sequence_Classification_Tasks.pdf">GLiClass: Generalist Lightweight Model for Sequence Classification Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Knowledgator Engineering</span></p><p>本文提出了GLiClass，一种基于标签条件编码器的Transformer家族模型，采用uni-encoder架构联合处理文本和标签，实现了高效的多标签序列分类。模型在DeBERTa等主流骨干上，通过改进的训练流程（包括LoRA适配、PPO强化学习、逻辑与NLI后训练等），在标准分类基准上取得了优于cross-encoder的准确率，推理速度在多标签情况下显著优于传统方法。结论显示，GLiClass在准确率与效率之间实现了优良平衡，适合高吞吐量、零样本和少样本场景，并为大标签集分类任务提供了可扩展的解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/GLiClass_Generalist_Lightweight_Model_for_Sequence_Classification_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="255-Multi-Turn-Jailbreaks-Are-Simpler-Than-They-Seem"><a href="#255-Multi-Turn-Jailbreaks-Are-Simpler-Than-They-Seem" class="headerlink" title="255. Multi-Turn Jailbreaks Are Simpler Than They Seem"></a>255. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-Turn_Jailbreaks_Are_Simpler_Than_They_Seem.pdf">Multi-Turn Jailbreaks Are Simpler Than They Seem</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>本文通过对GPT-4、Claude、Gemini等主流大模型的实证分析，基于StrongREJECT基准，提出并自动化了多轮（multi-turn）jailbreak攻击流程，并与单轮攻击等价重采样机制做对比。实验表明，多轮jailbreak的高成功率主要来源于可多次尝试而非对话复杂性，其效果基本等同于单轮攻击的多次采样。结论指出：当前安全评测低估了模型脆弱性，防御重点应转向对有害请求本身的鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multi-Turn_Jailbreaks_Are_Simpler_Than_They_Seem.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="256-Beyond-Single-A-Data-Selection-Principle-for-LLM-Alignment-via-Fine-Grained-Preference-Signals"><a href="#256-Beyond-Single-A-Data-Selection-Principle-for-LLM-Alignment-via-Fine-Grained-Preference-Signals" class="headerlink" title="256. Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals"></a>256. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Beyond_Single__A_Data_Selection_Principle_for_LLM_Alignment_via_Fine-Grained_Preference_Signals.pdf">Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本文提出了一种针对大型语言模型（LLM）对齐的新数据选择原则，核心为基于精细化偏好信号而非单一整体偏好。作者首先推导出多偏好直接优化（DMPO）目标，提出衡量子偏好冲突的Preference Divergence（PD）指标，并据此筛选高一致性样本用于DPO训练，配合PD估计和长度偏置修正。实验证明，该方法在多个冲突水平的数据集上，相比传统整体偏好和oracle方法获得超10%的相对提升，同时提升训练效率且无需繁琐的整体偏好标注。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Beyond_Single_A_Data_Selection_Principle_for_LLM_Alignment_via_Fine-Grained_Preference_Signals.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="257-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization"><a href="#257-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization" class="headerlink" title="257. Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization"></a>257. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Klear-Reasoner__Advancing_Reasoning_Capability_via_Gradient-Preserving_Clipping_Policy_Optimization.pdf">Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Technology</span></p><p>Klear-Reasoner integrates long Chain-of-Thought supervised fine-tuning (CoT SFT) with a novel Gradient-Preserving Clipping Policy Optimization (GPPO) method in reinforcement learning, specifically designed for mathematical and programming reasoning tasks. The model uses high-quality, carefully curated data, and the GPPO technique preserves gradient signals from all tokens to enhance exploration and learning efficiency, overcoming limitations of traditional clipping. Evaluations show Klear-Reasoner consistently matches or surpasses state-of-the-art models on benchmarks such as AIME2024, AIME2025, and LiveCodeBench, demonstrating that targeted data, principled SFT, and advanced RL optimization together yield substantial improvements in long-form reasoning performance.</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Klear-Reasoner_Advancing_Reasoning_Capability_via_Gradient-Preserving_Clipping_Policy_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="258-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment"><a href="#258-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment" class="headerlink" title="258. Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment"></a>258. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_to_Align,_Aligning_to_Learn__A_Unified_Approach_for_Self-Optimized_Alignment.pdf">Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Intelligence Healthcare Department, AntGroup</span></p><p>本文提出了GRAO（Group Relative Alignment Optimization）统一对齐框架，将监督微调（SFT）的高效收敛与强化学习（RL）的主动探索能力结合。其方法包括多样本生成、组内相对优势加权的直接对齐损失和基于偏好动态的参数更新，并在理论上证明收敛性，同时在复杂人类对齐任务中显著超越SFT、DPO、PPO和GRPO等主流方法，提升对齐效率和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Learning_to_Align%2C_Aligning_to_Learn_A_Unified_Approach_for_Self-Optimized_Alignment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="259-Chimera-Harnessing-Multi-Agent-LLMs-for-Automatic-Insider-Threat-Simulation"><a href="#259-Chimera-Harnessing-Multi-Agent-LLMs-for-Automatic-Insider-Threat-Simulation" class="headerlink" title="259. Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation"></a>259. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Chimera__Harnessing_Multi-Agent_LLMs_for_Automatic_Insider_Threat_Simulation.pdf">Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Singapore Management University</span></p><p>该论文提出了Chimera——首个基于大语言模型（LLM）的多智能体框架，实现自动化企业内部威胁（insider threat）仿真，包括企业成员的正常与恶意行为模拟，并收集多模态日志，生成高真实性的数据集ChimeraLog。通过人类专家和定量评估，验证了ChimeraLog的真实性和多样性，且实验发现该数据集对现有威胁检测模型更具挑战性，有助于推动更强健的威胁检测方法发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Chimera_Harnessing_Multi-Agent_LLMs_for_Automatic_Insider_Threat_Simulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="260-Semantic-Enhanced-Time-Series-Forecasting-via-Large-Language-Models"><a href="#260-Semantic-Enhanced-Time-Series-Forecasting-via-Large-Language-Models" class="headerlink" title="260. Semantic-Enhanced Time-Series Forecasting via Large Language Models"></a>260. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Semantic-Enhanced_Time-Series_Forecasting_via_Large_Language_Models.pdf">Semantic-Enhanced Time-Series Forecasting via Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology Beijing</span></p><p>本文提出了SE-LLM框架，通过设计时序-语义交叉相关（TSCC）模块和时序适配器（Time-Adapter），将时间序列的周期性和异常特征嵌入LLM的语义空间，显著提升LLM对时序数据的理解和预测能力。实验证明，SE-LLM在多项标准数据集上优于现有SOTA方法，并高效提升了模型的泛化能力和计算效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Semantic-Enhanced_Time-Series_Forecasting_via_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="261-1-2-3-Check-Enhancing-Contextual-Privacy-in-LLM-via-Multi-Agent-Reasoning"><a href="#261-1-2-3-Check-Enhancing-Contextual-Privacy-in-LLM-via-Multi-Agent-Reasoning" class="headerlink" title="261. 1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning"></a>261. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/1-2-3_Check__Enhancing_Contextual_Privacy_in_LLM_via_Multi-Agent_Reasoning.pdf">1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>该论文提出了一种多智能体框架，将隐私推理任务分解为提取、分类、生成和验证等子任务，通过Extractor、Checker和Executor三阶段协同，控制信息流并迭代校正，实现更可靠的情境隐私保护。实验显示，在ConfAIde和PrivacyLens基准测试上，多智能体系统能显著减少隐私泄露（最高达18%-19%），同时保持公共内容完整性，优于单智能体基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/1-2-3_Check_Enhancing_Contextual_Privacy_in_LLM_via_Multi-Agent_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="262-Designing-a-Feedback-Driven-Decision-Support-System-for-Dynamic-Student-Intervention"><a href="#262-Designing-a-Feedback-Driven-Decision-Support-System-for-Dynamic-Student-Intervention" class="headerlink" title="262. Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention"></a>262. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Designing_a_Feedback-Driven_Decision_Support_System_for_Dynamic_Student_Intervention.pdf">Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">WeAreGenius Research Institute</span></p><p>本论文提出了一种基于LightGBM的反馈驱动决策支持系统（DSS），结合Flask实现的实时Web界面和SHAP可解释性，核心创新为闭环架构：预测-干预-反馈-增量式重训练。系统能持续吸收学生干预后的真实成绩反馈，自动更新模型，提升预测精度。实验结果显示，模型重训练后RMSE下降10.7%，R2提升，所有反馈学生预测成绩持续上升，证明模型可动态自我改进，适应教育环境变化，推动教育分析从静态预测向主动、响应式、以人为本的智能决策转变。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Designing_a_Feedback-Driven_Decision_Support_System_for_Dynamic_Student_Intervention.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="263-QuProFS-An-Evolutionary-Training-free-Approach-to-Efficient-Quantum-Feature-Map-Search"><a href="#263-QuProFS-An-Evolutionary-Training-free-Approach-to-Efficient-Quantum-Feature-Map-Search" class="headerlink" title="263. QuProFS: An Evolutionary Training-free Approach to Efficient Quantum Feature Map Search"></a>263. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/QuProFS__An_Evolutionary_Training-free_Approach_to_Efficient_Quantum_Feature_Map_Search.pdf">QuProFS: An Evolutionary Training-free Approach to Efficient Quantum Feature Map Search</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Tokyo</span></p><p>本文提出了QuProFS，一种无需训练的进化量子架构搜索（QAS）框架，通过集成硬件约束、结构化与非结构化电路设计，并利用多种训练自由代理指标（如可训练性、硬件鲁棒性、表达能力和数据对齐）进行高效筛选和进化优化。该方法在多种真实、人工和量子生成数据集上，通过量子支持向量机任务，在模拟器与真实量子硬件上均实现了比现有QAS方法更高的准确率和2倍的搜索加速。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/QuProFS_An_Evolutionary_Training-free_Approach_to_Efficient_Quantum_Feature_Map_Search.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="264-SEF-MK-Speaker-Embedding-Free-Voice-Anonymization-through-Multi-k-means-Quantization"><a href="#264-SEF-MK-Speaker-Embedding-Free-Voice-Anonymization-through-Multi-k-means-Quantization" class="headerlink" title="264. SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means Quantization"></a>264. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SEF-MK__Speaker-Embedding-Free_Voice_Anonymization_through_Multi-k-means_Quantization.pdf">SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means Quantization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Duke Kunshan University</span></p><p>本文提出了一种无需说话人嵌入的语音匿名化方法SEF-MK，利用多k-means量化对自监督学习(SSL)提取的特征进行匿名化，具体流程为WavLM编码、随机选择多k-means模型量化、Conformer+HiFi-GAN解码。实验结果显示，多k-means量化比单一模型更好地保留了语音内容和情感，同时在攻击者使用多模型时会增强攻击有效性，揭示了多k-means策略在用户与攻击者视角下的隐私保护与实用性的权衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SEF-MK_Speaker-Embedding-Free_Voice_Anonymization_through_Multi-k-means_Quantization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="265-A-Stage-Aware-Mixture-of-Experts-Framework-for-Neurodegenerative-Disease-Progression-Modelling"><a href="#265-A-Stage-Aware-Mixture-of-Experts-Framework-for-Neurodegenerative-Disease-Progression-Modelling" class="headerlink" title="265. A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling"></a>265. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Stage-Aware_Mixture_of_Experts_Framework_for_Neurodegenerative_Disease_Progression_Modelling.pdf">A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University College London</span></p><p>本文提出了一种阶段感知的专家混合（IGND-MoE）框架，通过结合路径生理模型与异质图神经扩散模型（GAE）和本地神经反应模块，并利用时序注意力机制动态调节各专家在疾病不同阶段的贡献，实现从稀疏、非规则采样的个体快照中重建队列级疾病进展轨迹。实验表明，该模型在人脑tau蛋白病理传播的长期预测上优于纯路径生理或纯神经网络模型，并可解释不同阶段的主导疾病机制，发现早期以图结构扩散为主，后期则有更多未知物理过程主导。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Stage-Aware_Mixture_of_Experts_Framework_for_Neurodegenerative_Disease_Progression_Modelling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="266-Multi-Level-Service-Performance-Forecasting-via-Spatiotemporal-Graph-Neural-Networks"><a href="#266-Multi-Level-Service-Performance-Forecasting-via-Spatiotemporal-Graph-Neural-Networks" class="headerlink" title="266. Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks"></a>266. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-Level_Service_Performance_Forecasting_via_Spatiotemporal_Graph_Neural_Networks.pdf">Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent Author</span></p><p>该论文提出一种基于时空图神经网络的性能预测算法，通过将分布式后端服务系统在不同时间切片抽象为图结构，结合服务节点的运行特征和调用关系，利用多层图卷积网络捕捉结构依赖信息，并通过门控循环单元建模性能指标的动态演化，引入时间编码机制提升对非平稳序列的表达能力。实验结果表明，该模型在不同时间窗口和高并发场景下均优于主流方法，表现出强鲁棒性和泛化能力，为分布式服务系统的性能预测和运维管理提供了有效技术支撑。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multi-Level_Service_Performance_Forecasting_via_Spatiotemporal_Graph_Neural_Networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="267-Improving-Real-Time-Concept-Drift-Detection-using-a-Hybrid-Transformer-Autoencoder-Framework"><a href="#267-Improving-Real-Time-Concept-Drift-Detection-using-a-Hybrid-Transformer-Autoencoder-Framework" class="headerlink" title="267. Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework"></a>267. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Improving_Real-Time_Concept_Drift_Detection_using_a_Hybrid_Transformer-Autoencoder_Framework.pdf">Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Vellore Institute of Technology (VIT-AP)</span></p><p>本文提出了一种结合Transformer与Autoencoder的混合框架，实现对复杂时序数据概念漂移的实时检测。该方法利用统计指标（PSI、JSD）、重构误差、预测不确定性、规则违规和分类器误差，融合为信任评分，实现更灵敏且可解释的漂移监测。实验结果表明，混合Transformer-Autoencoder比传统方法在检测准确性和响应速度上均有提升，可应用于实际流式场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Improving_Real-Time_Concept_Drift_Detection_using_a_Hybrid_Transformer-Autoencoder_Framework.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="268-Whisfusion-Parallel-ASR-Decoding-via-a-Diffusion-Transformer"><a href="#268-Whisfusion-Parallel-ASR-Decoding-via-a-Diffusion-Transformer" class="headerlink" title="268. Whisfusion: Parallel ASR Decoding via a Diffusion Transformer"></a>268. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Whisfusion__Parallel_ASR_Decoding_via_a_Diffusion_Transformer.pdf">Whisfusion: Parallel ASR Decoding via a Diffusion Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University</span></p><p>本文提出了Whisfusion框架，将预训练Whisper语音编码器与文本扩散解码器通过轻量级交叉注意力适配器融合，实现非自回归（NAR）自动语音识别（ASR）并支持全并行解码。通过两阶段课程训练和并行扩散解码（PDD）策略，Whisfusion在LibriSpeech数据集上相较于Whisper-tiny取得更低的词错误率（8.3% vs 9.7%）且在长语音下推理速度提升2.6倍，验证了扩散解码器在高吞吐、低延迟ASR中的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Whisfusion_Parallel_ASR_Decoding_via_a_Diffusion_Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="269-Balancing-Privacy-and-Efficiency-Music-Information-Retrieval-via-Additive-Homomorphic-Encryption"><a href="#269-Balancing-Privacy-and-Efficiency-Music-Information-Retrieval-via-Additive-Homomorphic-Encryption" class="headerlink" title="269. Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption"></a>269. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Balancing_Privacy_and_Efficiency__Music_Information_Retrieval_via_Additive_Homomorphic_Encryption.pdf">Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent Researcher</span></p><p>本文提出利用加法同态加密（AHE）实现音乐向量嵌入的隐私保护相似性检索，分析了音乐信息检索特有的威胁模型，并设计了结构感知的分块与加权层次内积方法，提高了加密域下的检索效率和可解释性。实验证明，AHE方案在加密音乐数据检索上较全同态加密（FHE）显著提升了速度与内存效率，兼顾安全与实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Balancing_Privacy_and_Efficiency_Music_Information_Retrieval_via_Additive_Homomorphic_Encryption.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="270-TurboBias-Universal-ASR-Context-Biasing-powered-by-GPU-accelerated-Phrase-Boosting-Tree"><a href="#270-TurboBias-Universal-ASR-Context-Biasing-powered-by-GPU-accelerated-Phrase-Boosting-Tree" class="headerlink" title="270. TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree"></a>270. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TurboBias__Universal_ASR_Context-Biasing_powered_by_GPU-accelerated_Phrase-Boosting_Tree.pdf">TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">NVIDIA</span></p><p>本文提出了一种通用的自动语音识别（ASR）上下文偏置框架，基于GPU加速的短语提升树，支持CTC、RNN-T和Attention Encoder-Decoder等主流ASR模型。该方法通过修改树中权重分布，实现了在greedy与beam search模式下高效且高准确率的短语提升，提升关键短语识别F-score 8-10%（greedy）及17-23%（beam），整体识别准确率WER也有所提升，速度开销仅2-5%，对上下文短语数量具有良好鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TurboBias_Universal_ASR_Context-Biasing_powered_by_GPU-accelerated_Phrase-Boosting_Tree.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="271-Neural-Channel-Knowledge-Map-Assisted-Scheduling-Optimization-of-Active-IRSs-in-Multi-User-Systems"><a href="#271-Neural-Channel-Knowledge-Map-Assisted-Scheduling-Optimization-of-Active-IRSs-in-Multi-User-Systems" class="headerlink" title="271. Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems"></a>271. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Neural_Channel_Knowledge_Map_Assisted_Scheduling_Optimization_of_Active_IRSs_in_Multi-User_Systems.pdf">Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xiamen University</span></p><p>本文提出了基于神经网络的Channel Knowledge Map (CKM)方法，通过两级Transformer网络（LPS-Net和SE-Net）利用历史信道与吞吐量数据及用户位置预测多用户主动IRS系统中的链路功率统计和遍历频谱效率，并辅以低复杂度的Stable Matching-Iterative Balancing (SM-IB)调度算法实现近最优max-min吞吐量分配。实验结果表明，该方法在预测准确率和计算效率方面优于传统方法，SM-IB算法可在大规模系统中大幅降低复杂度，同时保证吞吐量接近上界。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Neural_Channel_Knowledge_Map_Assisted_Scheduling_Optimization_of_Active_IRSs_in_Multi-User_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="272-TLCCSP-A-Scalable-Framework-for-Enhancing-Time-Series-Forecasting-with-Time-Lagged-Cross-Correlations"><a href="#272-TLCCSP-A-Scalable-Framework-for-Enhancing-Time-Series-Forecasting-with-Time-Lagged-Cross-Correlations" class="headerlink" title="272. TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations"></a>272. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TLCCSP__A_Scalable_Framework_for_Enhancing_Time_Series_Forecasting_with_Time-Lagged_Cross-Correlatio.pdf">TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Normal University</span></p><p>该论文提出了TLCCSP框架，利用序列移动动态时间规整（SSDTW）算法自动识别并整合时间滞后交叉相关（TLCC）序列，以提升时间序列预测准确度，并通过对比学习编码器（CLE）高效近似SSDTW距离以降低计算开销。实验结果显示，TLCCSP在气象、金融和房地产等数据集上均显著降低预测误差，并通过对比学习方法将相关性计算时间缩短约99%，提升了模型的实时性和可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TLCCSP_A_Scalable_Framework_for_Enhancing_Time_Series_Forecasting_with_Time-Lagged_Cross-Correlations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="273-UniMove-A-Unified-Model-for-Multi-city-Human-Mobility-Prediction"><a href="#273-UniMove-A-Unified-Model-for-Multi-city-Human-Mobility-Prediction" class="headerlink" title="273. UniMove: A Unified Model for Multi-city Human Mobility Prediction"></a>273. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/UniMove__A_Unified_Model_for_Multi-city_Human_Mobility_Prediction.pdf">UniMove: A Unified Model for Multi-city Human Mobility Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>该论文提出了UniMove，一种用于多城市人类移动预测的统一模型，采用轨迹-位置双塔结构：位置塔通过Deep &amp; Cross Net编码城市内位置特征，轨迹塔用MoE Transformer处理不同城市的多样移动模式。实验显示，UniMove能在多城市数据联合训练下提升预测准确率（最高提升35.7%），尤其对数据稀疏城市效果显著，具有良好的泛化和可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/UniMove_A_Unified_Model_for_Multi-city_Human_Mobility_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="274-Neural-Beam-Field-for-Spatial-Beam-RSRP-Prediction"><a href="#274-Neural-Beam-Field-for-Spatial-Beam-RSRP-Prediction" class="headerlink" title="274. Neural Beam Field for Spatial Beam RSRP Prediction"></a>274. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Neural_Beam_Field_for_Spatial_Beam_RSRP_Prediction.pdf">Neural Beam Field for Spatial Beam RSRP Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xiamen University</span></p><p>本文提出了一种混合神经-物理框架NBF用于高效且可解释的空间波束RSRP预测，核心方法包括引入多路径条件功率剖面（MCPP）作为物理传播与天线&#x2F;波束配置的中介，并采用Transformer结构深度神经网络学习MCPP，再结合解析公式推理波束RSRP统计量。实验结果显示，NBF在准确性、训练效率和泛化能力上显著优于传统表格型信道知识图和纯黑箱DNN，同时具备更高存储效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Neural_Beam_Field_for_Spatial_Beam_RSRP_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="275-Machine-Learning-Algorithms-for-Improving-Exact-Classical-Solvers-in-Mixed-Integer-Continuous-Optimization"><a href="#275-Machine-Learning-Algorithms-for-Improving-Exact-Classical-Solvers-in-Mixed-Integer-Continuous-Optimization" class="headerlink" title="275. Machine Learning Algorithms for Improving Exact Classical Solvers in Mixed Integer Continuous Optimization"></a>275. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Machine_Learning_Algorithms_for_Improving_Exact_Classical_Solvers_in_Mixed_Integer_Continuous_Optimi.pdf">Machine Learning Algorithms for Improving Exact Classical Solvers in Mixed Integer Continuous Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universität Wien</span></p><p>本文系统综述了机器学习（ML）和强化学习（RL）方法如何集成到整数、连续和混合整数非线性优化（INLP、CNLP、MINLP）问题的精确经典求解器中，重点聚焦于Branch-and-Bound等框架的分支、剪枝、节点选择、切割平面、参数控制等关键环节的ML&#x2F;RL增强技术流程。结论认为，ML和RL的嵌入可显著加速求解速度，提升大规模问题的收敛效率，同时不损失全局最优性和理论收敛保证，但仍需进一步解决泛化、可解释性与混合集成等挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Machine_Learning_Algorithms_for_Improving_Exact_Classical_Solvers_in_Mixed_Integer_Continuous_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="276-A-Score-based-Diffusion-Model-Approach-for-Adaptive-Learning-of-Stochastic-Partial-Differential-Equation-Solutions"><a href="#276-A-Score-based-Diffusion-Model-Approach-for-Adaptive-Learning-of-Stochastic-Partial-Differential-Equation-Solutions" class="headerlink" title="276. A Score-based Diffusion Model Approach for Adaptive Learning of Stochastic Partial Differential Equation Solutions"></a>276. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Score-based_Diffusion_Model_Approach_for_Adaptive_Learning_of_Stochastic_Partial_Differential_Equa.pdf">A Score-based Diffusion Model Approach for Adaptive Learning of Stochastic Partial Differential Equation Solutions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Oak Ridge National Laboratory</span></p><p>本文提出了一种基于score-based diffusion model的新型自适应学习框架，用于递归贝叶斯推断下随机偏微分方程（SPDEs）解的更新。该方法将SPDE物理知识以得分函数的形式编码进扩散模型，并结合观测数据通过反向SDE进行似然修正，实现观测稀疏和噪声环境下的高效自适应学习，提升解的精度和鲁棒性。大量数值实验表明，所提方法在Burgers方程、Navier–Stokes方程及Allen–Cahn方程等典型SPDEs下，能有效融合观测信息，显著优于现有主流滤波器（如LETKF）并保持物理一致性和能量守恒等关键性质。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Score-based_Diffusion_Model_Approach_for_Adaptive_Learning_of_Stochastic_Partial_Differential_Equation_Solutions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="277-Hardness-Aware-Dynamic-Curriculum-Learning-for-Robust-Multimodal-Emotion-Recognition-with-Missing-Modalities"><a href="#277-Hardness-Aware-Dynamic-Curriculum-Learning-for-Robust-Multimodal-Emotion-Recognition-with-Missing-Modalities" class="headerlink" title="277. Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities"></a>277. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hardness-Aware_Dynamic_Curriculum_Learning_for_Robust_Multimodal_Emotion_Recognition_with_Missing_Mo.pdf">Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Inner Mongolia University</span></p><p>本文提出了一种硬度感知动态课程学习框架（HARDY-MER），用于提升在模态缺失情况下的多模态情感识别鲁棒性。该方法首先通过多视角硬度评估机制量化样本重建难度（结合重建误差与跨模态互信息），然后利用检索增强的动态课程学习策略，根据样本难度自适应检索支持样本，重点提升对难样本的学习。实验表明，该方法在IEMOCAP与CMU-MOSEI等基准数据集的各类模态缺失情形下均优于现有方法，实现了新的SOTA性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Hardness-Aware_Dynamic_Curriculum_Learning_for_Robust_Multimodal_Emotion_Recognition_with_Missing_Modalities.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="278-Noise-Aware-Generative-Microscopic-Traffic-Simulation"><a href="#278-Noise-Aware-Generative-Microscopic-Traffic-Simulation" class="headerlink" title="278. Noise-Aware Generative Microscopic Traffic Simulation"></a>278. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Noise-Aware_Generative_Microscopic_Traffic_Simulation.pdf">Noise-Aware Generative Microscopic Traffic Simulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Massachusetts Institute of Technology</span></p><p>本文提出了I-24 MOTION Scenario Dataset (I24-MSD)，该数据集为微观交通仿真提供了包含现实传感器噪声的标准化车辆轨迹数据。作者将生成式模型（如SMART）与噪声感知损失函数（标签平滑、focal loss、对称交叉熵）结合，提升了模型在数据不完美环境下的泛化能力和仿真真实度，实验证明噪声建模可显著优于传统基线。结论：噪声感知优化方法能有效提升微观交通仿真模型的准确性和实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Noise-Aware_Generative_Microscopic_Traffic_Simulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="279-Structured-Superposition-of-Autoencoders-for-UEP-Codes-at-Intermediate-Blocklengths"><a href="#279-Structured-Superposition-of-Autoencoders-for-UEP-Codes-at-Intermediate-Blocklengths" class="headerlink" title="279. Structured Superposition of Autoencoders for UEP Codes at Intermediate Blocklengths"></a>279. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Structured_Superposition_of_Autoencoders_for_UEP_Codes_at_Intermediate_Blocklengths.pdf">Structured Superposition of Autoencoders for UEP Codes at Intermediate Blocklengths</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Novi Sad</span></p><p>本文提出了一种结构化自编码器（AE）架构，通过将编码和解码过程分解为多个子块，实现了中等区块长度下高效可扩展的比特级不等错误保护（UEP）码设计。与传统的叠加编码和SIC解码方案相比，该方法在可调节不同可靠性等级的同时，显著扩展了可实现的误码概率区域，提升了性能并适用于资源受限环境。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Structured_Superposition_of_Autoencoders_for_UEP_Codes_at_Intermediate_Blocklengths.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="280-Lightning-Prediction-under-Uncertainty-DeepLight-with-Hazy-Loss"><a href="#280-Lightning-Prediction-under-Uncertainty-DeepLight-with-Hazy-Loss" class="headerlink" title="280. Lightning Prediction under Uncertainty: DeepLight with Hazy Loss"></a>280. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Lightning_Prediction_under_Uncertainty__DeepLight_with_Hazy_Loss.pdf">Lightning Prediction under Uncertainty: DeepLight with Hazy Loss</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bangladesh University of Engineering and Technology</span></p><p>该论文提出了一种用于雷电预测的深度学习模型DeepLight，采用多分支ConvLSTM结构以动态提取不同空间范围的相关性，融合真实雷电观测、雷达反射率和云特性等多源气象数据。创新的Hazy Loss损失函数通过邻域惩罚机制增强模型对雷电事件时空不确定性的适应能力。实验证明DeepLight在ETS等衡量指标上对比主流方法提升显著：1小时预测提升30%，3小时提升18-22%，6小时提升8-13%；Hazy Loss进一步提升准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Lightning_Prediction_under_Uncertainty_DeepLight_with_Hazy_Loss.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="281-Leveraging-GNN-to-Enhance-MEF-Method-in-Predicting-ENSO"><a href="#281-Leveraging-GNN-to-Enhance-MEF-Method-in-Predicting-ENSO" class="headerlink" title="281. Leveraging GNN to Enhance MEF Method in Predicting ENSO"></a>281. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Leveraging_GNN_to_Enhance_MEF_Method_in_Predicting_ENSO.pdf">Leveraging GNN to Enhance MEF Method in Predicting ENSO</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shiraz University of Technology</span></p><p>本文提出了一种结合3D卷积神经网络（3DCNN）和图神经网络（GNN）的多模态ENSO预测方法（MEF），通过构建输出相似性图筛选和聚合高质量的集成成员，显著提升了气候预测的准确性和稳定性。实验结果表明，该方法在长期（超过12个月）预测中比传统平均或加权集成方法更鲁棒，尤其适用于后2000年气候变异加剧的情景，且方法可泛化至其他气候模型集成后处理。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Leveraging_GNN_to_Enhance_MEF_Method_in_Predicting_ENSO.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="282-ProteoKnight-Convolution-based-phage-virion-protein-classification-and-uncertainty-analysis"><a href="#282-ProteoKnight-Convolution-based-phage-virion-protein-classification-and-uncertainty-analysis" class="headerlink" title="282. ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis"></a>282. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ProteoKnight__Convolution-based_phage_virion_protein_classification_and_uncertainty_analysis.pdf">ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">BRAC University</span></p><p>ProteoKnight提出了一种基于图像的Knight编码方法，将蛋白质序列通过极坐标和颜色映射编码为图像，利用预训练卷积神经网络（如GoogLeNet等）进行噬菌体衣壳蛋白的二分类与多分类，并通过Monte Carlo Dropout量化模型的不确定性。实验结果显示，该方法在二分类中达到90.8%的准确率，优于传统空间特征丢失的方法，并揭示了序列类别和长度对不确定性的影响。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ProteoKnight_Convolution-based_phage_virion_protein_classification_and_uncertainty_analysis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="283-Finite-Time-Convergence-Analysis-of-ODE-based-Generative-Models-for-Stochastic-Interpolants"><a href="#283-Finite-Time-Convergence-Analysis-of-ODE-based-Generative-Models-for-Stochastic-Interpolants" class="headerlink" title="283. Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants"></a>283. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Finite-Time_Convergence_Analysis_of_ODE-based_Generative_Models_for_Stochastic_Interpolants.pdf">Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>该论文提出对基于随机插值的生成模型中常用的ODE数值解法（前向Euler法和二阶Heun法）进行有限时间收敛性理论分析，首次建立了总变差距离的误差上界，并详细分析了维度、分布属性、步长及数值方法阶数对生成误差的影响。结论表明两种方法的收敛速率均可被理论界定，且二阶方法具备更优复杂度；实验结果证实了理论分析的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Finite-Time_Convergence_Analysis_of_ODE-based_Generative_Models_for_Stochastic_Interpolants.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="284-Fairness-of-Automatic-Speech-Recognition-Looking-Through-a-Philosophical-Lens"><a href="#284-Fairness-of-Automatic-Speech-Recognition-Looking-Through-a-Philosophical-Lens" class="headerlink" title="284. Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens"></a>284. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fairness_of_Automatic_Speech_Recognition__Looking_Through_a_Philosophical_Lens.pdf">Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Cornell University</span></p><p>本文通过哲学视角系统分析自动语音识别（ASR）系统的公平性，提出“尊重”概念并区分中性分类与有害歧视，强调ASR对非标准方言系统性误识别不仅是技术问题，更加剧历史性不公。结论认为，仅靠技术手段难以解决ASR偏见，需承认多样语言的合法性并以伦理和政治责任推动系统设计，确保不同群体发声权利和身份被尊重。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Fairness_of_Automatic_Speech_Recognition_Looking_Through_a_Philosophical_Lens.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="285-An-effective-potential-for-generative-modelling-with-active-matter"><a href="#285-An-effective-potential-for-generative-modelling-with-active-matter" class="headerlink" title="285. An effective potential for generative modelling with active matter"></a>285. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/An_effective_potential_for_generative_modelling_with_active_matter.pdf">An effective potential for generative modelling with active matter</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Queen Mary University of London</span></p><p>本文提出了一种基于主动粒子过程（如有色噪声的AOUP模型）的生成扩散模型，通过对位置坐标施加有效的时变势能实现时间反转生成过程。作者推导了Fox和UCN近似下的有效势能，并通过数值实验验证了方法在人工数据分布上的有效性，结果显示所提模型能准确逼近目标分布，具有较高的生成性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/An_effective_potential_for_generative_modelling_with_active_matter.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="286-OFAL-An-Oracle-Free-Active-Learning-Framework"><a href="#286-OFAL-An-Oracle-Free-Active-Learning-Framework" class="headerlink" title="286. OFAL: An Oracle-Free Active Learning Framework"></a>286. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/OFAL__An_Oracle-Free_Active_Learning_Framework.pdf">OFAL: An Oracle-Free Active Learning Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amirkabir University of Technology</span></p><p>该论文提出了OFAL，一种无需人工标注（oracle）的主动学习框架，通过神经网络的不确定性量化，结合Monte Carlo Dropout和变分自编码器（VAE），将模型高置信度的未标记样本转化为不确定样本以提升模型性能。实验表明，OFAL在无需额外标注代价的情况下，将模型在MNIST数据集的准确率提升了2.7%，并可与传统主动学习采样方法集成进一步提升性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/OFAL_An_Oracle-Free_Active_Learning_Framework.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="287-Learned-Regularization-for-Microwave-Tomography"><a href="#287-Learned-Regularization-for-Microwave-Tomography" class="headerlink" title="287. Learned Regularization for Microwave Tomography"></a>287. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learned_Regularization_for_Microwave_Tomography.pdf">Learned Regularization for Microwave Tomography</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出了一种基于物理建模与扩散模型（Diffusion Model）结合的混合重建框架，用于微波层析成像（MWT）。方法创新性地将单步扩散正则化（SSD-Reg）作为Plug-and-Play模块嵌入可变分重建流程，通过Fréchet可微前向模型和无监督学习到的扩散先验，实现了无需成对数据的高效重建。实验结果显示，该方法在合成和真实数据上均显著优于传统和现有深度学习方法，提高了重建质量、稳定性和抗噪声鲁棒性，且收敛速度更快。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Learned_Regularization_for_Microwave_Tomography.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="288-C-MAG-Cascade-Multimodal-Attributed-Graphs-for-Supply-Chain-Link-Prediction"><a href="#288-C-MAG-Cascade-Multimodal-Attributed-Graphs-for-Supply-Chain-Link-Prediction" class="headerlink" title="288. C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction"></a>288. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/C-MAG__Cascade_Multimodal_Attributed_Graphs_for_Supply_Chain_Link_Prediction.pdf">C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AAITC, Lenovo</span></p><p>本论文提出了C-MAG（Cascade Multimodal Attributed Graph），通过两阶段级联架构融合文本、图像等多模态属性，先在制造商属性图中对文本和视觉特征对齐聚合，再通过多尺度消息传递提升制造商-产品链接预测准确率。实验表明，C-MAG在供应链知识图谱的链接预测任务上显著优于现有方法，且对噪声视觉数据具有较强鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/C-MAG_Cascade_Multimodal_Attributed_Graphs_for_Supply_Chain_Link_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="289-From-Source-to-Target-Leveraging-Transfer-Learning-for-Predictive-Process-Monitoring-in-Organizations"><a href="#289-From-Source-to-Target-Leveraging-Transfer-Learning-for-Predictive-Process-Monitoring-in-Organizations" class="headerlink" title="289. From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations"></a>289. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Source_to_Target__Leveraging_Transfer_Learning_for_Predictive_Process_Monitoring_in_Organizatio.pdf">From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Friedrich-Alexander-Universität Erlangen-Nürnberg</span></p><p>本文提出了一种基于迁移学习的预测性流程监控（PPM）技术，实现了在缺乏足够事件日志数据的目标组织中，无需微调即可从相似业务流程转移预测模型。该方法通过活动嵌入（使用预训练语言模型）、相对时间戳映射和两层LSTM结构，支持跨组织&#x2F;部门的模型迁移。实验证明，该技术在同组织内和跨组织场景下均显著优于传统PPM方法，能够高效提升目标业务流程的预测性能，尤其适合数据稀缺环境。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/From_Source_to_Target_Leveraging_Transfer_Learning_for_Predictive_Process_Monitoring_in_Organizations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="290-On-Understanding-of-the-Dynamics-of-Model-Capacity-in-Continual-Learning"><a href="#290-On-Understanding-of-the-Dynamics-of-Model-Capacity-in-Continual-Learning" class="headerlink" title="290. On Understanding of the Dynamics of Model Capacity in Continual Learning"></a>290. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/On_Understanding_of_the_Dynamics_of_Model_Capacity_in_Continual_Learning.pdf">On Understanding of the Dynamics of Model Capacity in Continual Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Capital One</span></p><p>该论文提出了持续学习中的动态模型容量（CLEMC）理论框架，通过递推方程刻画神经网络在多任务学习中的稳定性与可塑性平衡点随任务变化的非平稳性。理论证明即使每个新任务与之前任务仅有微小差异，模型容量最终也会发散，导致模型遗忘严重且表现变差，并通过在FNN、CNN、GNN和大规模语言模型上的实验证实这一结论。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/On_Understanding_of_the_Dynamics_of_Model_Capacity_in_Continual_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="291-Learning-to-Select-MCP-Algorithms-From-Traditional-ML-to-Dual-Channel-GAT-MLP"><a href="#291-Learning-to-Select-MCP-Algorithms-From-Traditional-ML-to-Dual-Channel-GAT-MLP" class="headerlink" title="291. Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP"></a>291. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_to_Select_MCP_Algorithms__From_Traditional_ML_to_Dual-Channel_GAT-MLP.pdf">Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">College of Mathematics and Computer, Shantou University</span></p><p>本文针对最大团问题（MCP）中缺乏面向实例的算法选择方法这一空白，提出了融合传统机器学习与图神经网络的两阶段框架。方法包括对图实例提取全局统计与局部结构特征，先用SVM、Random Forest等模型评估，再设计GAT-MLP双通道模型，分别建模局部结构与全局特征并融合分类。实验表明，GAT-MLP在算法选择任务上准确率达89.13%，显著优于传统方法，并能有效整合局部与全局信息。结论是双通道结构和图神经网络为组合算法选择任务提供了有效且具推广性的解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Learning_to_Select_MCP_Algorithms_From_Traditional_ML_to_Dual-Channel_GAT-MLP.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="292-MemoryKT-An-Integrative-Memory-and-Forgetting-Method-for-Knowledge-Tracing"><a href="#292-MemoryKT-An-Integrative-Memory-and-Forgetting-Method-for-Knowledge-Tracing" class="headerlink" title="292. MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing"></a>292. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MemoryKT__An_Integrative_Memory-and-Forgetting_Method_for_Knowledge_Tracing.pdf">MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China Normal University</span></p><p>该论文提出了MemoryKT，一种将记忆编码、存储和检索三阶段机制与个性化遗忘建模相结合的知识追踪方法。通过引入变分自编码器（VAE）和个性化遗忘模块，MemoryKT能够模拟学生记忆动态并精细捕捉个体遗忘特征。实验结果显示该方法在四个真实数据集上显著优于现有主流方法，实现了更高的预测精度和更强的个性化解释能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MemoryKT_An_Integrative_Memory-and-Forgetting_Method_for_Knowledge_Tracing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="293-Fast-and-Generalizable-parameter-embedded-Neural-Operators-for-Lithium-Ion-Battery-Simulation"><a href="#293-Fast-and-Generalizable-parameter-embedded-Neural-Operators-for-Lithium-Ion-Battery-Simulation" class="headerlink" title="293. Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation"></a>293. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fast_and_Generalizable_parameter-embedded_Neural_Operators_for_Lithium-Ion_Battery_Simulation.pdf">Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">RWTH Aachen University</span></p><p>本文提出并系统比较了三种用于锂离子电池单颗粒模型（SPM）高效仿真的算子学习神经网络，包括Deep Operator Network (DeepONet)、Fourier Neural Operator (FNO)以及新颖的参数嵌入式FNO（PE-FNO），后者能显式处理材料半径和扩散系数等参数，实现对不同充放电电流、SOC、材料参数的泛化仿真。实验表明，PE-FNO在浓度和电压预测上能保持亚百分比误差，同时推理速度比传统SPM求解器快约200倍，并可用于快速贝叶斯参数估计任务。结论：PE-FNO为实时电池管理和大规模推断提供了高效且高保真的神经算子路径。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Fast_and_Generalizable_parameter-embedded_Neural_Operators_for_Lithium-Ion_Battery_Simulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="294-FNBT-Full-Negation-Belief-Transformation-for-Open-World-Information-Fusion-Based-on-Dempster-Shafer-Theory-of-Evidence"><a href="#294-FNBT-Full-Negation-Belief-Transformation-for-Open-World-Information-Fusion-Based-on-Dempster-Shafer-Theory-of-Evidence" class="headerlink" title="294. FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence"></a>294. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FNBT__Full_Negation_Belief_Transformation_for_Open-World_Information_Fusion_Based_on_Dempster-Shafer.pdf">FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China Normal University</span></p><p>该论文提出了一种基于Dempster-Shafer理论的全否定信念变换（FNBT）方法，专门用于解决开放世界异构帧下的信息融合问题。方法包括提出基于本质冲突的开放世界判别准则、帧扩展、以及全否定机制，将原始质量分配映射到扩展帧后再用经典组合规则融合。理论上证明了FNBT的质量函数不变性、遗传性和本质冲突消除等性质。实验表明，FNBT在多个真实分类任务中显著优于现有方法，具有高效性和适应性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/FNBT_Full_Negation_Belief_Transformation_for_Open-World_Information_Fusion_Based_on_Dempster-Shafer_Theory_of_Evidence.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="295-Multi-modal-Adaptive-Mixture-of-Experts-for-Cold-start-Recommendation"><a href="#295-Multi-modal-Adaptive-Mixture-of-Experts-for-Cold-start-Recommendation" class="headerlink" title="295. Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation"></a>295. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-modal_Adaptive_Mixture_of_Experts_for_Cold-start_Recommendation.pdf">Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">VNU University of Engineering and Technology</span></p><p>本文提出了MAMEX框架，通过结合模态特定的Mixture of Experts（MoE）层与可学习门控机制，实现了多模态信息的自适应融合，动态捕捉不同模态特征并平衡其贡献。实验结果在多个Amazon数据集上优于现有方法，显著提升了冷启动推荐的效果，验证了模型结构和正则化策略的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multi-modal_Adaptive_Mixture_of_Experts_for_Cold-start_Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="296-Deep-Learning-Based-Analysis-of-Power-Consumption-in-Gasoline-Electric-and-Hybrid-Vehicles"><a href="#296-Deep-Learning-Based-Analysis-of-Power-Consumption-in-Gasoline-Electric-and-Hybrid-Vehicles" class="headerlink" title="296. Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles"></a>296. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Deep_Learning-Based_Analysis_of_Power_Consumption_in_Gasoline,_Electric,_and_Hybrid_Vehicles.pdf">Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Research Council Canada</span></p><p>本文提出了一种基于深度学习的车辆能耗预测方法，结合TCN、LSTM、Transformer和传统随机森林模型，利用动力系统动态特征实现对汽油车、纯电车及混合动力车瞬时与累计能耗的精准预测，并进行不确定性分析。实验结果显示，所提出的数据驱动方法在ICE车辆上表现最优，在EV和HEV上也优于现有方法且模型不确定性随动力系统复杂度增加而增强。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Deep_Learning-Based_Analysis_of_Power_Consumption_in_Gasoline%2C_Electric%2C_and_Hybrid_Vehicles.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="297-Topological-Feature-Compression-for-Molecular-Graph-Neural-Networks"><a href="#297-Topological-Feature-Compression-for-Molecular-Graph-Neural-Networks" class="headerlink" title="297. Topological Feature Compression for Molecular Graph Neural Networks"></a>297. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Topological_Feature_Compression_for_Molecular_Graph_Neural_Networks.pdf">Topological Feature Compression for Molecular Graph Neural Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>本文提出了一种新颖的图神经网络（GNN）架构PACTNet，结合了高阶细胞复形压缩拓扑特征与标准分子特征，通过高效的ECC算法提取和融合3D结构信息，增强分子表示能力。实验在七个主流分子性质预测数据集上，PACTNet在五个任务中显著优于GCN、GAT、GIN等主流基线，且在所有任务中表现出色，验证了其准确性、鲁棒性和计算高效性。结论指出，本方法在提升分子机器学习模型性能的同时兼顾了解释性和可扩展性，可为未来分子建模提供基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Topological_Feature_Compression_for_Molecular_Graph_Neural_Networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="298-Frequency-Domain-Analysis-of-Time-Dependent-Multiomic-Data-in-Progressive-Neurodegenerative-Diseases-A-Proposed-Quantum-Classical-Hybrid-Approach-with-Quaternionic-Extensions"><a href="#298-Frequency-Domain-Analysis-of-Time-Dependent-Multiomic-Data-in-Progressive-Neurodegenerative-Diseases-A-Proposed-Quantum-Classical-Hybrid-Approach-with-Quaternionic-Extensions" class="headerlink" title="298. Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions"></a>298. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Frequency-Domain_Analysis_of_Time-Dependent_Multiomic_Data_in_Progressive_Neurodegenerative_Diseases.pdf">Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Massachusetts General Hospital</span></p><p>本论文提出一种理论框架，将神经退行性疾病的多组学时序数据通过傅里叶和拉普拉斯变换转入频域，结合哈密顿量建模与量子-经典混合计算（如VQE算法）以检测复杂动态模式，并扩展至四元数表示以捕捉多态神经动力学。结论认为，该方法有望提升高风险患者的轨迹预测与生物标志物识别能力，为未来量子增强医学奠定基础，但需在真实数据集上进行实证验证。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Frequency-Domain_Analysis_of_Time-Dependent_Multiomic_Data_in_Progressive_Neurodegenerative_Diseases_A_Proposed_Quantum-Classical_Hybrid_Approach_with_Quaternionic_Extensions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="299-Generative-Inversion-for-Property-Targeted-Materials-Design-Application-to-Shape-Memory-Alloys"><a href="#299-Generative-Inversion-for-Property-Targeted-Materials-Design-Application-to-Shape-Memory-Alloys" class="headerlink" title="299. Generative Inversion for Property-Targeted Materials Design: Application to Shape Memory Alloys"></a>299. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Generative_Inversion_for_Property-Targeted_Materials_Design__Application_to_Shape_Memory_Alloys.pdf">Generative Inversion for Property-Targeted Materials Design: Application to Shape Memory Alloys</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>该论文提出了一种基于生成对抗网络（GAN）反演的逆向材料设计框架，通过将预训练GAN与性质预测模型结合，在潜在空间中进行梯度优化，直接生成满足用户指定性能目标的形状记忆合金（SMA）成分及工艺参数。实验结果表明，所设计的Ni49.8Ti26.4Hf18.6Zr5.2合金在转变温度、机械功输出、转变焓和热滞后等方面均优于现有NiTi合金，验证了框架的有效性和通用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Generative_Inversion_for_Property-Targeted_Materials_Design_Application_to_Shape_Memory_Alloys.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="300-Disentangling-Multiplex-Spatial-Temporal-Transition-Graph-Representation-Learning-for-Socially-Enhanced-POI-Recommendation"><a href="#300-Disentangling-Multiplex-Spatial-Temporal-Transition-Graph-Representation-Learning-for-Socially-Enhanced-POI-Recommendation" class="headerlink" title="300. Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation"></a>300. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Disentangling_Multiplex_Spatial-Temporal_Transition_Graph_Representation_Learning_for_Socially_Enhan.pdf">Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China Normal University</span></p><p>该论文提出了DiMuST模型，通过引入社会异构图和解耦变分多路图自编码器（DAE），联合建模用户社会关系与空间-时间转移模式，实现了空间-时间特征的私有与共享分布解耦，并采用专家乘积（PoE）机制融合共享特征、对私有特征进行对比约束去噪。结果表明，DiMuST在多个真实数据集上较现有方法有显著精度提升，关键模块有效提升了空间-时间关系建模和推荐鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Disentangling_Multiplex_Spatial-Temporal_Transition_Graph_Representation_Learning_for_Socially_Enhanced_POI_Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025/08/12报纸</title>
    <link href="/2025/08/2025-08-12_article/"/>
    <url>/2025/08/2025-08-12_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2025/08/12报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-12<br>📄 <strong>发现论文数量</strong>：119  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-The-Fair-Game-Auditing-Debiasing-AI-Algorithms-Over-Time"><a href="#1-The-Fair-Game-Auditing-Debiasing-AI-Algorithms-Over-Time" class="headerlink" title="1. The Fair Game: Auditing &amp; Debiasing AI Algorithms Over Time"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/The_Fair_Game__Auditing_&_Debiasing_AI_Algorithms_Over_Time.pdf">The Fair Game: Auditing &amp; Debiasing AI Algorithms Over Time</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Inria</span></p><p>该论文提出了“Fair Game”框架，通过将审计器（Auditor）与去偏算法（Debiasing Algorithm）组成闭环，动态审计和去偏机器学习模型，利用强化学习（RL）机制实现长期公平性目标的自适应调整。实验与理论分析表明，该方法能在数据分布和社会伦理规范动态变化的环境下，持续减少模型偏见并提高公平性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/The_Fair_Game_Auditing_&_Debiasing_AI_Algorithms_Over_Time.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Unsupervised-Partner-Design-Enables-Robust-Ad-hoc-Teamwork"><a href="#2-Unsupervised-Partner-Design-Enables-Robust-Ad-hoc-Teamwork" class="headerlink" title="2. Unsupervised Partner Design Enables Robust Ad-hoc Teamwork"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Unsupervised_Partner_Design_Enables_Robust_Ad-hoc_Teamwork.pdf">Unsupervised Partner Design Enables Robust Ad-hoc Teamwork</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Stuttgart</span></p><p>该论文提出了Unsupervised Partner Design (UPD)——一种无需预训练伙伴、无须人工调参的多智能体强化学习框架，通过动态生成带有随机偏差和能力多样性的训练伙伴，并利用回报方差作为学习性指标筛选最能促进主智能体学习的伙伴。UPD可与无监督环境设计结合，实现同时针对任务和伙伴分布的自适应课程学习，在Overcooked-AI和其通用挑战中显著优于主流基线，并在人机协同实验中获得更高评分和回报，展现强泛化与协作能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Unsupervised_Partner_Design_Enables_Robust_Ad-hoc_Teamwork.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-OM2P-Offline-Multi-Agent-Mean-Flow-Policy"><a href="#3-OM2P-Offline-Multi-Agent-Mean-Flow-Policy" class="headerlink" title="3. OM2P: Offline Multi-Agent Mean-Flow Policy"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/OM2P__Offline_Multi-Agent_Mean-Flow_Policy.pdf">OM2P: Offline Multi-Agent Mean-Flow Policy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>该论文提出了OM2P算法，将均值流（mean-flow）生成模型创新性地集成到离线多智能体强化学习（MARL）中，实现高效的一步动作生成。方法上通过奖励感知优化、广义时间步分布和无导数估算，提升了训练稳定性和内存效率。实验表明，OM2P在多智能体标准任务中达到了最优或近最优表现，显著减少了GPU内存消耗并加速训练。结论是OM2P可高效扩展至复杂多智能体环境，兼具高性能与实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/OM2P_Offline_Multi-Agent_Mean-Flow_Policy.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-Lightweight-Auto-bidding-based-on-Traffic-Prediction-in-Live-Advertising"><a href="#4-Lightweight-Auto-bidding-based-on-Traffic-Prediction-in-Live-Advertising" class="headerlink" title="4. Lightweight Auto-bidding based on Traffic Prediction in Live Advertising"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Lightweight_Auto-bidding_based_on_Traffic_Prediction_in_Live_Advertising.pdf">Lightweight Auto-bidding based on Traffic Prediction in Live Advertising</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Taobao &amp; Tmall Group of Alibaba</span></p><p>本文提出了BiCB轻量级自动竞价算法，结合线性规划(LP)理论分析的最优竞价公式与未来流量的统计预测，利用LightGBM等回归模型估算全时段流量，通过梯度下降高效近似优化目标。实验证明BiCB在满足预算和点击单价约束下性能优越，工程复杂度低，已在大型平台广告业务落地并取得显著业务提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Lightweight_Auto-bidding_based_on_Traffic_Prediction_in_Live_Advertising.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-GCHR-Goal-Conditioned-Hindsight-Regularization-for-Sample-Efficient-Reinforcement-Learning"><a href="#5-GCHR-Goal-Conditioned-Hindsight-Regularization-for-Sample-Efficient-Reinforcement-Learning" class="headerlink" title="5. GCHR: Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/GCHR___Goal-Conditioned_Hindsight_Regularization_for_Sample-Efficient_Reinforcement_Learning.pdf">GCHR: Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>本文提出了一种高效的目标条件强化学习方法GCHR，通过结合Hindsight Self-Imitation Regularization（HSR）和Hindsight Goal Regularization（HGR），显著提升了稀疏奖励场景下的样本利用率和策略性能。GCHR无需额外网络或复杂规划，仅通过对历史轨迹的目标重标与行为克隆实现对策略的双重正则化，在理论和实验证明中均优于现有GCRL方法，能够更快收敛并达到更优策略。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/GCHR_Goal-Conditioned_Hindsight_Regularization_for_Sample-Efficient_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-VQAThinker-Exploring-Generalizable-and-Explainable-Video-Quality-Assessment-via-Reinforcement-Learning"><a href="#6-VQAThinker-Exploring-Generalizable-and-Explainable-Video-Quality-Assessment-via-Reinforcement-Learning" class="headerlink" title="6. VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/VQAThinker__Exploring_Generalizable_and_Explainable_Video_Quality_Assessment_via_Reinforcement_Learn.pdf">VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>VQAThinker提出了一种结合大规模多模态模型（LMM）与强化学习（基于GRPO算法）的推理型视频质量评价（VQA）框架，通过引入三种奖励函数（钟形回归奖励、对排序奖励与时序一致性奖励），实现了视频质量理解与评分的联合建模。实验表明，该方法在多项VQA基准测试（含OOD场景）中取得了领先的泛化能力和解释性，显著优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/VQAThinker_Exploring_Generalizable_and_Explainable_Video_Quality_Assessment_via_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Society-of-Mind-Meets-Real-Time-Strategy-A-Hierarchical-Multi-Agent-Framework-for-Strategic-Reasoning"><a href="#7-Society-of-Mind-Meets-Real-Time-Strategy-A-Hierarchical-Multi-Agent-Framework-for-Strategic-Reasoning" class="headerlink" title="7. Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Society_of_Mind_Meets_Real-Time_Strategy__A_Hierarchical_Multi-Agent_Framework_for_Strategic_Reasoni.pdf">Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University</span></p><p>本文提出了HIMA架构，通过多模仿学习代理结合战略规划器（SP），利用专家演示数据进行单位组成聚类，生成结构化多步行动序列，并由SP以环境感知和时序链式推理方式融合各代理建议，适应实时战略游戏变化。结果显示，HIMA在StarCraft II九种种族组合对战中胜率和效率均优于现有主流方法，证明多代理模仿结合高层协调能提升复杂环境下的智能体表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Society_of_Mind_Meets_Real-Time_Strategy_A_Hierarchical_Multi-Agent_Framework_for_Strategic_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-Multi-Armed-Bandits-Based-Optimization-of-Decision-Trees"><a href="#8-Multi-Armed-Bandits-Based-Optimization-of-Decision-Trees" class="headerlink" title="8. Multi-Armed Bandits-Based Optimization of Decision Trees"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Multi-Armed_Bandits-Based_Optimization_of_Decision_Trees.pdf">Multi-Armed Bandits-Based Optimization of Decision Trees</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bangladesh University of Engineering and Technology</span></p><p>本论文提出了一种基于多臂赌博机（Multi-Armed Bandits, MAB）的决策树剪枝优化方法，将剪枝过程建模为探索-利用问题，采用UCB1、Thompson Sampling等MAB算法动态选择要剪枝的分支节点，并基于每次剪枝动作的反馈调整策略。实验证明，该方法在多个基准数据集上优于传统的代价复杂度剪枝（CCP），能提升决策树的泛化能力和预测性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Multi-Armed_Bandits-Based_Optimization_of_Decision_Trees.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-LinguaFluid-Language-Guided-Fluid-Control-via-Semantic-Rewards-in-Reinforcement-Learning"><a href="#9-LinguaFluid-Language-Guided-Fluid-Control-via-Semantic-Rewards-in-Reinforcement-Learning" class="headerlink" title="9. LinguaFluid: Language-Guided Fluid Control via Semantic Rewards in Reinforcement Learning"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LinguaFluid__Language_Guided_Fluid_Control_via_Semantic_Rewards_in_Reinforcement_Learning.pdf">LinguaFluid: Language-Guided Fluid Control via Semantic Rewards in Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">College of Environmental and Resource Sciences, Zhejiang University</span></p><p>该论文提出了一种基于语义奖励的强化学习方法，用于实现流体控制等科学领域复杂任务。方法核心是利用SBERT生成当前状态和目标描述的语义嵌入，通过余弦相似度作为奖励信号，指导RL智能体学习，无需手工设计数值奖励函数。实验结果表明，该方法在无手工奖励函数下能够有效引导智能体实现与自然语言目标一致的控制行为，且语义空间中的奖励与物理空间指标高度相关。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LinguaFluid_Language-Guided_Fluid_Control_via_Semantic_Rewards_in_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-Mildly-Conservative-Regularized-Evaluation-for-Offline-Reinforcement-Learning"><a href="#10-Mildly-Conservative-Regularized-Evaluation-for-Offline-Reinforcement-Learning" class="headerlink" title="10. Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Mildly_Conservative_Regularized_Evaluation_for_Offline_Reinforcement_Learning.pdf">Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Central South University</span></p><p>本文提出了一种用于离线强化学习的温和保守正则化评估（MCRE）框架，将时序差分（TD）误差与行为克隆项结合进Bellman备份，以兼顾值估计的准确性和策略的保守性。基于MCRE，作者提出了MCRQ算法，并在D4RL基准的MuJoCo任务上验证了其优越性，理论上证明了收敛性及性能上界，实验显示MCRQ优于现有主流算法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Mildly_Conservative_Regularized_Evaluation_for_Offline_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="11-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation"><a href="#11-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation" class="headerlink" title="11. Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Shortcut_Learning_in_Generalist_Robot_Policies__The_Role_of_Dataset_Diversity_and_Fragmentation.pdf">Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">UESTC</span></p><p>本文系统分析了通用机器人策略在大规模多子集数据集（如OXE）训练下出现的shortcut learning现象，指出数据子集内部多样性不足和子集间分布碎片化是导致策略泛化能力受限的核心原因。通过理论建模、仿真和真实机器人实验，作者证明提升子集多样性、减少子集间分布差异，以及采用针对性的机器人数据增强（如视角&#x2F;对象增强）可显著缓解shortcut learning并提升泛化能力；结论为：优化数据收集与增强流程是提升通用机器人策略泛化性的关键途径。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Shortcut_Learning_in_Generalist_Robot_Policies_The_Role_of_Dataset_Diversity_and_Fragmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-L2Calib-SE-3-Manifold-Reinforcement-Learning-for-Robust-Extrinsic-Calibration-with-Degenerate-Motion-Resilience"><a href="#12-L2Calib-SE-3-Manifold-Reinforcement-Learning-for-Robust-Extrinsic-Calibration-with-Degenerate-Motion-Resilience" class="headerlink" title="12. L2Calib: SE(3)-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/L2Calib__$SE(3)$-Manifold_Reinforcement_Learning_for_Robust_Extrinsic_Calibration_with_Degenerate_Mo.pdf">L2Calib: SE(3)-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Cyber-Systems and Control, Zhejiang University</span></p><p>本文提出了一种基于强化学习(SE(3)-流形建模)的传感器外参标定方法，将标定问题建模为决策过程，直接在SE(3)空间优化外参以提升里程计精度。方法采用Bingham分布建模旋转参数，结合轨迹对齐奖励机制和自动化数据筛选模块，实现无需结构化靶标即可在弱激励下进行多传感器联合标定。实验表明该方法在无人机、地面车和手持平台上超越传统优化方法，具备更高的鲁棒性和精度，且对初值不敏感，极大简化了机器人多传感器标定流程。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/L2Calib_SE%283%29-Manifold_Reinforcement_Learning_for_Robust_Extrinsic_Calibration_with_Degenerate_Motion_Resilience.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-SCAR-State-Space-Compression-for-AI-Driven-Resource-Management-in-6G-Enabled-Vehicular-Infotainment-Systems"><a href="#13-SCAR-State-Space-Compression-for-AI-Driven-Resource-Management-in-6G-Enabled-Vehicular-Infotainment-Systems" class="headerlink" title="13. SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SCAR__State-Space_Compression_for_AI-Driven_Resource_Management_in_6G-Enabled_Vehicular_Infotainment.pdf">SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Swiss Distance University of Applied Sciences</span></p><p>本文提出SCAR框架，通过SAST优化的K-means聚类与径向基函数网络（RBFN）实现边缘AI辅助的CQI数据压缩，随后利用压缩后的状态输入6G环境下的强化学习（RL）调度器进行车载娱乐资源分配。实验表明，SCAR在提升调度公平性和可扩展性方面优于传统方法，实现TTI可行区间提升14%，不公平调度时间下降15%，展现出高效性和公平性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SCAR_State-Space_Compression_for_AI-Driven_Resource_Management_in_6G-Enabled_Vehicular_Infotainment_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Computer-Vision-based-Adaptive-Control-for-Back-Exoskeleton-Performance-Optimization"><a href="#14-Computer-Vision-based-Adaptive-Control-for-Back-Exoskeleton-Performance-Optimization" class="headerlink" title="14. Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Computer_Vision-based_Adaptive_Control_for_Back_Exoskeleton_Performance_Optimization.pdf">Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Politecnico di Milano</span></p><p>本论文提出了结合计算机视觉的自适应控制系统，用于提升背部外骨骼在工业场景下的性能优化。方法上，作者首先构建了基于肌肉活动减少、用户不适感和偏好的多维优化空间，通过实验获得最优工作区域，并开发了基于YOLOv11和DINOv2的实时视觉管线，实现对搬运物体的自动检测、重量分类与辅助力度自适应调节。实验结果显示，所提系统在提升肌肉减负（最大减少达23%）、用户偏好和舒适度等方面均优于静态控制，且具备80%以上实时分类准确率。结论表明，智能自适应控制有望显著优化工业外骨骼的实际应用效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Computer_Vision-based_Adaptive_Control_for_Back_Exoskeleton_Performance_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-Beyond-Constant-Parameters-Hyper-Prediction-Models-and-HyperMPC"><a href="#15-Beyond-Constant-Parameters-Hyper-Prediction-Models-and-HyperMPC" class="headerlink" title="15. Beyond Constant Parameters: Hyper Prediction Models and HyperMPC"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Beyond_Constant_Parameters__Hyper_Prediction_Models_and_HyperMPC.pdf">Beyond Constant Parameters: Hyper Prediction Models and HyperMPC</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Robotics and Machine Intelligence, Poznan University of Technology</span></p><p>本文提出了Hyper Prediction Model (HyperPM)和其在HyperMPC框架中的集成，实现了通过神经网络预测随时间变化的动力学模型参数轨迹，结合历史状态、控制输入与未来规划动作，提升了MPC在复杂动力学系统（如带有未建模扰动的无人机、摆和自动驾驶赛车）中的长期预测准确性和控制性能。实验结果表明，HyperPM在多个任务中长期预测误差降低超35%，HyperMPC在下游控制任务中相较现有技术取得了最高76%的性能提升，并保持低计算复杂度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Beyond_Constant_Parameters_Hyper_Prediction_Models_and_HyperMPC.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-Mitigating-Undesired-Conditions-in-Flexible-Production-with-Product–Process–Resource-Asset-Knowledge-Graphs"><a href="#16-Mitigating-Undesired-Conditions-in-Flexible-Production-with-Product–Process–Resource-Asset-Knowledge-Graphs" class="headerlink" title="16. Mitigating Undesired Conditions in Flexible Production with Product–Process–Resource Asset Knowledge Graphs"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Mitigating_Undesired_Conditions_in_Flexible_Production_with_Product-Process-Resource_Asset_Knowledge.pdf">Mitigating Undesired Conditions in Flexible Production with Product–Process–Resource Asset Knowledge Graphs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague</span></p><p>该论文提出了一种基于语义知识图谱的产品–过程–资源资产知识图（PPR-AKG）方法，通过扩展传统PPR模型引入对工业生产中不良状态及其原因的建模，并采用OWL本体实现查询与推理。结合大型语言模型（LLM）实现人机交互，提升操作员和工程师对生产过程的理解与优化。实验验证在电池拆解与再制造场景下显著提升了灵活生产系统的问题诊断与性能表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Mitigating_Undesired_Conditions_in_Flexible_Production_with_Product%E2%80%93Process%E2%80%93Resource_Asset_Knowledge_Graphs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-REBot-Reflexive-Evasion-Robot-for-Instantaneous-Dynamic-Obstacle-Avoidance"><a href="#17-REBot-Reflexive-Evasion-Robot-for-Instantaneous-Dynamic-Obstacle-Avoidance" class="headerlink" title="17. REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/REBot__Reflexive_Evasion_Robot_for_Instantaneous_Dynamic_Obstacle_Avoidance.pdf">REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>本文提出了REBot系统，通过有限状态机集成了基于强化学习训练的瞬时回避策略和恢复策略，实现四足机器人在动态障碍物靠近时的低延迟反射性规避。REBot经过仿真和真实机器人测试，在多种障碍场景下表现出更高的回避成功率、能耗更低及对高速障碍物的鲁棒性，验证了方法在提升机器人安全和机动性上的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/REBot_Reflexive_Evasion_Robot_for_Instantaneous_Dynamic_Obstacle_Avoidance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-Affordance-R1-Reinforcement-Learning-for-Generalizable-Affordance-Reasoning-in-Multimodal-Large-Language-Model"><a href="#18-Affordance-R1-Reinforcement-Learning-for-Generalizable-Affordance-Reasoning-in-Multimodal-Large-Language-Model" class="headerlink" title="18. Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Affordance-R1__Reinforcement_Learning_for_Generalizable_Affordance_Reasoning_in_Multimodal_Large_Lan.pdf">Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (GZ)</span></p><p>Affordance-R1提出了一个强化学习框架，通过认知链式思考(CoT)引导的Group Relative Policy Optimization (GRPO)算法，提升多模态大语言模型(MLLM)的可泛化affordance推理能力。其方法设计了涵盖格式、感知和认知的奖励函数，并构建了高质量affordance推理数据集ReasonAff，实现了无需有监督微调即可在真实场景和域外数据上展现强大泛化和推理能力。结论表明，Affordance-R1在多项评测中显著优于现有方法，并为机器人感知与操作任务提供了更强的推理基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Affordance-R1_Reinforcement_Learning_for_Generalizable_Affordance_Reasoning_in_Multimodal_Large_Language_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Graph-based-Robot-Localization-Using-a-Graph-Neural-Network-with-a-Floor-Camera-and-a-Feature-Rich-Industrial-Floor"><a href="#19-Graph-based-Robot-Localization-Using-a-Graph-Neural-Network-with-a-Floor-Camera-and-a-Feature-Rich-Industrial-Floor" class="headerlink" title="19. Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Graph-based_Robot_Localization_Using_a_Graph_Neural_Network_with_a_Floor_Camera_and_a_Feature_Rich_I.pdf">Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">TU Dortmund University</span></p><p>该论文提出了一种创新的机器人室内定位方法，利用地面摄像头采集地板特征，通过将图像特征抽象为图结构并采用图卷积网络（GCN）进行特征嵌入与匹配，实现高效、可扩展的定位。该方法在工业地板环境下获得了高精度（误差仅0.64厘米）和高速度定位，并能有效解决‘被绑架机器人’问题，具有良好的通用性和扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Graph-based_Robot_Localization_Using_a_Graph_Neural_Network_with_a_Floor_Camera_and_a_Feature_Rich_Industrial_Floor.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-GMF-Drive-Gated-Mamba-Fusion-with-Spatial-Aware-BEV-Representation-for-End-to-End-Autonomous-Driving"><a href="#20-GMF-Drive-Gated-Mamba-Fusion-with-Spatial-Aware-BEV-Representation-for-End-to-End-Autonomous-Driving" class="headerlink" title="20. GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/GMF-Drive__Gated_Mamba_Fusion_with_Spatial-Aware_BEV_Representation_for_End-to-End_Autonomous_Drivin.pdf">GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>该论文提出GMF-Drive，一种面向端到端自动驾驶的多模态融合框架。方法包括采用几何增强的14维LiDAR柱状体表示来保留三维空间信息，并提出空间感知状态空间模型（BEV-SSM）实现高效线性复杂度的多层级融合，取代传统自注意力Transformer。实验表明，GMF-Drive在NAVSIM基准数据集上性能优于现有DiffusionDrive方法，实现了新的SOTA，并通过消融实验证实各创新组件的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/GMF-Drive_Gated_Mamba_Fusion_with_Spatial-Aware_BEV_Representation_for_End-to-End_Autonomous_Driving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-Bounding-Distributional-Shifts-in-World-Modeling-through-Novelty-Detection"><a href="#21-Bounding-Distributional-Shifts-in-World-Modeling-through-Novelty-Detection" class="headerlink" title="21. Bounding Distributional Shifts in World Modeling through Novelty Detection"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Bounding_Distributional_Shifts_in_World_Modeling_through_Novelty_Detection.pdf">Bounding Distributional Shifts in World Modeling through Novelty Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Rutgers University</span></p><p>本文提出在视觉世界模型中引入基于变分自编码器（VAE）的新颖性检测机制，以解决模型推理时分布转移导致的性能下降问题。方法将VAE作为新颖性检测组件，度量预测状态与训练分布的偏离程度，并将此重构损失作为模型预测控制（MPC）中每步动作的代价项，集成至DINO-WM架构进行机器人操作任务测试。实验表明，该方法有效提升了规划算法的数据效率和鲁棒性，显著优于现有世界模型方法。结论认为新颖性检测能缓解世界模型不完美带来的推理偏差，但会牺牲对未见状态的探索。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Bounding_Distributional_Shifts_in_World_Modeling_through_Novelty_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-ME3-BEV-Mamba-Enhanced-Deep-Reinforcement-Learning-for-End-to-End-Autonomous-Driving-with-BEV-Perception"><a href="#22-ME3-BEV-Mamba-Enhanced-Deep-Reinforcement-Learning-for-End-to-End-Autonomous-Driving-with-BEV-Perception" class="headerlink" title="22. ME3-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ME$^3$-BEV__Mamba-Enhanced_Deep_Reinforcement_Learning_for_End-to-End_Autonomous_Driving_with_BEV-Pe.pdf">ME3-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了ME3-BEV框架，将鸟瞰视角（BEV）空间感知与Mamba时序建模模块结合，采用深度强化学习（DRL）实现端到端自动驾驶决策。方法包括SSA模块提升空间理解、TAFM模块增强时序特征提取，并通过语义分割提升可解释性。实验结果表明，ME3-BEV在CARLA多场景下显著降低碰撞率、提升轨迹准确性和整体驾驶评分，优于主流基线，具备实时性和安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ME3-BEV_Mamba-Enhanced_Deep_Reinforcement_Learning_for_End-to-End_Autonomous_Driving_with_BEV-Perception.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-ReNiL-Relative-Neural-Inertial-Locator-with-Any-Scale-Bayesian-Inference"><a href="#23-ReNiL-Relative-Neural-Inertial-Locator-with-Any-Scale-Bayesian-Inference" class="headerlink" title="23. ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ReNiL__Relative_Neural_Inertial_Locator_with_Any-Scale_Bayesian_Inference.pdf">ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>本文提出了ReNiL，一个基于深度学习和贝叶斯推断的惯性定位系统，通过引入惯性定位需求点（IPDPs）和任意尺度Laplace估计器（ASLE），实现对任意尺度IMU序列的高效、灵活和不确定性一致的位移估计。实验结果表明，ReNiL在多数据集上具有领先的定位精度与不确定性估计能力，优于现有主流方法，并显著降低计算开销。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ReNiL_Relative_Neural_Inertial_Locator_with_Any-Scale_Bayesian_Inference.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-Hand-by-Hand-LLM-Driving-EMS-Assistant-for-Operational-Skill-Learning"><a href="#24-Hand-by-Hand-LLM-Driving-EMS-Assistant-for-Operational-Skill-Learning" class="headerlink" title="24. Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Hand_by_Hand__LLM_Driving_EMS_Assistant_for_Operational_Skill_Learning.pdf">Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了FlightAxis原型系统，将大语言模型（LLM）与电刺激（EMS）结合，通过“Align-Analyze-Adjust”流程支持和引导用户飞行技能训练。实验结果表明，系统能有效提升学员操作表现和高度保持能力，改善认知和学习体验，用户对LLM驱动的体感辅助表现出较高接受度，尤其适合新手技能训练。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Hand_by_Hand_LLM_Driving_EMS_Assistant_for_Operational_Skill_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-Dynamical-Trajectory-Planning-of-Disturbance-Consciousness-for-Air-Land-Bimodal-Unmanned-Aerial-Vehicles"><a href="#25-Dynamical-Trajectory-Planning-of-Disturbance-Consciousness-for-Air-Land-Bimodal-Unmanned-Aerial-Vehicles" class="headerlink" title="25. Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Dynamical_Trajectory_Planning_of_Disturbance_Consciousness_for_Air-Land_Bimodal_Unmanned_Aerial_Vehi.pdf">Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Guangdong Laboratory of Artificial Intelligence and Digital Economy (Shenzhen)</span></p><p>本文提出了一种面向空地双模无人车的扰动自适应轨迹规划框架，将实时扰动估计纳入路径搜索与轨迹优化流程，并通过扰动自适应安全边界调整机制动态修正车辆可行的动力边界，保障轨迹可行性。实验结果表明，该方法在真实环境与基准测试中显著提升了跟踪精度、任务效率及能耗表现，增强了系统在复杂扰动场景下的适应性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Dynamical_Trajectory_Planning_of_Disturbance_Consciousness_for_Air-Land_Bimodal_Unmanned_Aerial_Vehicles.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-Affordance-Guided-Dual-Armed-Disassembly-Teleoperation-for-Mating-Parts"><a href="#26-Affordance-Guided-Dual-Armed-Disassembly-Teleoperation-for-Mating-Parts" class="headerlink" title="26. Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Affordance-Guided_Dual-Armed_Disassembly_Teleoperation_for_Mating_Parts.pdf">Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Osaka University</span></p><p>本论文提出了一种基于可供性引导的双臂遥操作拆解系统，结合物体几何信息可视化抓取和拆卸方向，通过混合位置-阻抗控制实现顺应性拆解。实验表明，该系统提升了大型家电部件无损拆卸的成功率并降低了目标物体的姿态偏差，验证了混合控制和双臂策略在拆卸任务中的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Affordance-Guided_Dual-Armed_Disassembly_Teleoperation_for_Mating_Parts.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-Latent-Policy-Barrier-Learning-Robust-Visuomotor-Policies-by-Staying-In-Distribution"><a href="#27-Latent-Policy-Barrier-Learning-Robust-Visuomotor-Policies-by-Staying-In-Distribution" class="headerlink" title="27. Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Latent_Policy_Barrier__Learning_Robust_Visuomotor_Policies_by_Staying_In-Distribution.pdf">Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University</span></p><p>本文提出Latent Policy Barrier (LPB) 框架，通过引入基于视觉编码的隐空间障碍，将专家演示的分布视为安全边界，利用扩散模型学习基础策略，仅使用专家数据确保模仿精度，并结合在专家和回滚数据上训练的视觉隐空间动力学模型，在推理时对行动进行梯度引导，动态修正偏离专家分布的状态。实验证明，LPB相比以往方法在仿真及真实机器人操作任务中，能够显著提升策略鲁棒性和样本效率，减少人工干预，实现从有限专家数据中可靠的操作控制。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Latent_Policy_Barrier_Learning_Robust_Visuomotor_Policies_by_Staying_In-Distribution.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="28-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion"><a href="#28-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion" class="headerlink" title="28. WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/WGAST__Weakly-Supervised_Generative_Network_for_Daily_10_m_Land_Surface_Temperature_Estimation_via_S.pdf">WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">INSA Centre Val de Loire</span></p><p>本文提出WGAST，一种基于条件生成对抗网络（cGAN）的弱监督生成模型，实现Terra MODIS、Landsat 8和Sentinel-2多源遥感卫星数据的时空融合，生成每日10米分辨率地表温度（LST）。方法包括多尺度特征提取、余弦相似性特征融合、自适应归一化、时序注意力机制及高频噪声抑制，通过物理平均原则弱监督训练。实验表明，WGAST在定量和定性评测中显著优于现有方法，平均RMSE降低17.18%，SSIM提升4.10%，可有效重建高分辨率LST并克服云层遮挡问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/WGAST_Weakly-Supervised_Generative_Network_for_Daily_10_m_Land_Surface_Temperature_Estimation_via_Spatio-Temporal_Fusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Text-Embedded-Swin-UMamba-for-DeepLesion-Segmentation"><a href="#29-Text-Embedded-Swin-UMamba-for-DeepLesion-Segmentation" class="headerlink" title="29. Text Embedded Swin-UMamba for DeepLesion Segmentation"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Text_Embedded_Swin-UMamba_for_DeepLesion_Segmentation.pdf">Text Embedded Swin-UMamba for DeepLesion Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Scientific Application Services, Center of Information Technology, NIH</span></p><p>本文提出了Text-Swin-UMamba模型，在Swin-UMamba分割骨干的多尺度解码器中集成Text Tower语言嵌入机制，将放射学报告中的短文本描述编码后与影像特征多层融合，实现对CT图像中病灶的精准分割。实验表明，该方法在DeepLesion数据集上Dice分数达到82%，在所有比对模型中表现最佳，融合文本和影像特征能显著提升分割性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Text_Embedded_Swin-UMamba_for_DeepLesion_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-TRUST-Leveraging-Text-Robustness-for-Unsupervised-Domain-Adaptation"><a href="#30-TRUST-Leveraging-Text-Robustness-for-Unsupervised-Domain-Adaptation" class="headerlink" title="30. TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/TRUST__Leveraging_Text_Robustness_for_Unsupervised_Domain_Adaptation.pdf">TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Catania</span></p><p>本文提出了TRUST方法，通过利用文本的鲁棒性改进图像分类中的无监督领域自适应。其方法包括：用语言模型从图像对应的文本描述生成目标域伪标签，并通过CLIP模型衡量图片与文本的语义相似性，估计伪标签的不确定性，用于加权分类损失，降低低质量文本带来的负面影响。此外，提出了一种多模态软对比学习损失，通过文本指导视觉模型的特征空间对齐，提升模型在复杂领域迁移下的泛化能力。实验结果表明，在DomainNet和GeoNet等经典与复杂领域迁移任务上，TRUST均优于现有方法，取得新的SOTA表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/TRUST_Leveraging_Text_Robustness_for_Unsupervised_Domain_Adaptation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-CLIPin-A-Non-contrastive-Plug-in-to-CLIP-for-Multimodal-Semantic-Alignment"><a href="#31-CLIPin-A-Non-contrastive-Plug-in-to-CLIP-for-Multimodal-Semantic-Alignment" class="headerlink" title="31. CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/CLIPin__A_Non-contrastive_Plug-in_to_CLIP_for_Multimodal_Semantic_Alignment.pdf">CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>该论文提出CLIPin，一种可无缝集成到CLIP架构中的非对比学习插件，通过引入在线-目标分支对图像和文本模态进行交互监督，并设计共享预投影器，有效整合对比和非对比学习以提升多模态语义对齐和特征表达。实验结果表明，CLIPin显著提升了自然图像和医学图像文本任务中的表现和泛化能力，支持多种对比学习框架，具备良好的可插拔性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/CLIPin_A_Non-contrastive_Plug-in_to_CLIP_for_Multimodal_Semantic_Alignment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-FVGen-Accelerating-Novel-View-Synthesis-with-Adversarial-Video-Diffusion-Distillation"><a href="#32-FVGen-Accelerating-Novel-View-Synthesis-with-Adversarial-Video-Diffusion-Distillation" class="headerlink" title="32. FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/FVGen__Accelerating_Novel-View_Synthesis_with_Adversarial_Video_Diffusion_Distillation.pdf">FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute for Creative Technologies</span></p><p>FVGen提出了一种加速3D场景重建中新视角生成的方法，采用视频扩散模型，通过引入GAN初始化和软化反向KL散度的分布匹配蒸馏，将多步降噪教师模型高效地压缩为仅需4步的学生模型。实验结果显示，该方法在视觉质量不损失甚至提升的情况下，将生成速度提升了90%以上，大幅提高了稀疏视角输入场景的重建效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/FVGen_Accelerating_Novel-View_Synthesis_with_Adversarial_Video_Diffusion_Distillation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-Text-as-Any-Modality-for-Zero-Shot-Classification-by-Consistent-Prompt-Tuning"><a href="#33-Text-as-Any-Modality-for-Zero-Shot-Classification-by-Consistent-Prompt-Tuning" class="headerlink" title="33. Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Text_as_Any-Modality_for_Zero-Shot_Classification_by_Consistent_Prompt_Tuning.pdf">Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University of Science and Technology</span></p><p>本文提出了TaAM-CPT方法，实现了通过一致的提示调优（Prompt Tuning）构建通用多模态表征模型，仅利用LLMs生成的文本数据，无需任何模态特定标注数据。该方法包含模态提示池、文本构造和模态对齐文本编码器，并通过设计模态内和模态间学习目标实现多模态类别无冲突扩展与高效特征一致性。实验结果表明，TaAM-CPT在视频、图像和音频等多模态零样本分类任务上取得了领先性能，验证了其通用性和可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Text_as_Any-Modality_for_Zero-Shot_Classification_by_Consistent_Prompt_Tuning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-Text-guided-Visual-Prompt-DINO-for-Generic-Segmentation"><a href="#34-Text-guided-Visual-Prompt-DINO-for-Generic-Segmentation" class="headerlink" title="34. Text-guided Visual Prompt DINO for Generic Segmentation"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Text-guided_Visual_Prompt_DINO_for_Generic_Segmentation.pdf">Text-guided Visual Prompt DINO for Generic Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua Shenzhen International Graduate School, Tsinghua University</span></p><p>该论文提出Prompt-DINO，一种融合文本和视觉提示的新型通用分割模型，通过在编码初期实现早期多模态融合、引入视觉-文本顺序对齐的查询选择机制，以及开发RAP数据引擎生成大规模高质量训练样本，实现跨模态语义消歧与泛化提升。实验表明Prompt-DINO在开放世界分割和检测任务上取得了最新最优性能，显著扩展了模型的语义覆盖和适应能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Text-guided_Visual_Prompt_DINO_for_Generic_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Street-View-Sociability-Interpretable-Analysis-of-Urban-Social-Behavior-Across-15-Cities"><a href="#35-Street-View-Sociability-Interpretable-Analysis-of-Urban-Social-Behavior-Across-15-Cities" class="headerlink" title="35. Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Street_View_Sociability__Interpretable_Analysis_of_Urban_Social_Behavior_Across_15_Cities.pdf">Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>本论文提出利用街景图像（SVI）结合多模态大语言模型（MLLM），通过Mehta的社会行为分类法自动分析15座城市的街道社会互动类型，并与城市归属感、绿化和天空可见度等环境变量进行线性回归关联分析。结果显示，SVI中的社会行为类型与城市环境特征和归属感有显著相关性，验证了街景图像作为可解释、可扩展的社会行为研究工具的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Street_View_Sociability_Interpretable_Analysis_of_Urban_Social_Behavior_Across_15_Cities.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-Uncertainty-quantified-Rollout-Policy-Adaptation-for-Unlabelled-Cross-domain-Temporal-Grounding"><a href="#36-Uncertainty-quantified-Rollout-Policy-Adaptation-for-Unlabelled-Cross-domain-Temporal-Grounding" class="headerlink" title="36. Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Uncertainty-quantified_Rollout_Policy_Adaptation_for_Unlabelled_Cross-domain_Temporal_Grounding.pdf">Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Queen Mary University of London</span></p><p>本文提出了一种面向视频时序定位任务的数据高效无标注跨域自适应方法。核心方法为基于GRPO（Group Relative Policy Optimization）强化学习的Uncertainty-quantified Rollout Policy Adaptation（URPA），通过在无标注目标域视频上多次采样生成伪标签，并利用其方差估算不确定性，将置信权重引入奖励函数，指导模型关注可靠伪监督信号，实现仅需少量无标注目标域视频即可高效自适应。实验结果表明，该方法在六个跨域基准上超越或媲美全量数据的主流方法，有效提升了跨域时序定位的泛化性和实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Uncertainty-quantified_Rollout_Policy_Adaptation_for_Unlabelled_Cross-domain_Temporal_Grounding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-Advanced-Deep-Learning-Techniques-for-Accurate-Lung-Cancer-Detection-and-Classification"><a href="#37-Advanced-Deep-Learning-Techniques-for-Accurate-Lung-Cancer-Detection-and-Classification" class="headerlink" title="37. Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Advanced_Deep_Learning_Techniques_for_Accurate_Lung_Cancer_Detection_and_Classification.pdf">Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Vigo</span></p><p>该论文提出了一种基于DenseNet201的深度学习方法，结合Focal Loss、数据增强和正则化等技术，有效解决了肺癌CT影像分类中的数据不平衡和过拟合问题。实验结果显示，DenseNet201模型在多种肺癌亚型分类上取得了98.95%的高准确率，优于主流模型，具备临床实际应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Advanced_Deep_Learning_Techniques_for_Accurate_Lung_Cancer_Detection_and_Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-Depth-Jitter-Seeing-through-the-Depth"><a href="#38-Depth-Jitter-Seeing-through-the-Depth" class="headerlink" title="38. Depth Jitter: Seeing through the Depth"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Depth_Jitter__Seeing_through_the_Depth.pdf">Depth Jitter: Seeing through the Depth</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Université de Toulon</span></p><p>本文提出了一种新颖的深度感知数据增强方法Depth-Jitter，通过利用水下图像形成模型和单目深度估计，对高深度方差图像引入自适应或固定深度偏移，生成具有物理意义的深度扰动样本，从而增强模型在多标签水下图像分类中的鲁棒性和泛化能力。实验在UTDAC2020和FathomNet两个基准数据集上，Depth-Jitter在mAP、mAP@20和ROC AUC等指标上均优于传统增强方法，尤其在深度敏感环境下提升显著，验证了其在真实水下视觉任务中的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Depth_Jitter_Seeing_through_the_Depth.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-TEFormer-Texture-Aware-and-Edge-Guided-Transformer-for-Semantic-Segmentation-of-Urban-Remote-Sensing-Images"><a href="#39-TEFormer-Texture-Aware-and-Edge-Guided-Transformer-for-Semantic-Segmentation-of-Urban-Remote-Sensing-Images" class="headerlink" title="39. TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/TEFormer__Texture-Aware_and_Edge-Guided_Transformer_for_Semantic_Segmentation_of_Urban_Remote_Sensin.pdf">TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing University of Technology</span></p><p>该论文提出TEFormer，一种融合纹理感知模块（TaM）和边缘引导三分支解码器（Eg3Head）的Transformer架构，用于城市遥感图像语义分割。方法通过TaM增强细粒度纹理特征感知，Eg3Head和边缘引导特征融合模块（EgFFM）实现多尺度、细节和边缘特征的有效融合，提升类别区分和边界精度。实验表明TEFormer在Potsdam、Vaihingen和LoveDA数据集上取得了领先的分割性能，边界和细节表现尤为突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/TEFormer_Texture-Aware_and_Edge-Guided_Transformer_for_Semantic_Segmentation_of_Urban_Remote_Sensing_Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-MA-CBP-A-Criminal-Behavior-Prediction-Framework-Based-on-Multi-Agent-Asynchronous-Collaboration"><a href="#40-MA-CBP-A-Criminal-Behavior-Prediction-Framework-Based-on-Multi-Agent-Asynchronous-Collaboration" class="headerlink" title="40. MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/MA-CBP__A_Criminal_Behavior_Prediction_Framework_Based_on_Multi-Agent_Asynchronous_Collaboration.pdf">MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Mechatronical Engineering, Beijing Institute of Technology</span></p><p>本文提出了MA-CBP框架，通过多智能体异步协作，将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，并融合邻近图像帧，实现对短期与长期行为上下文的联合推理。实验结果显示，该方法在犯罪行为早期预警与检测任务上优于现有生成式和异常检测模型，有效提升了城市公共安全场景下的风险预警能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/MA-CBP_A_Criminal_Behavior_Prediction_Framework_Based_on_Multi-Agent_Asynchronous_Collaboration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-Can-Diffusion-Models-Bridge-the-Domain-Gap-in-Cardiac-MR-Imaging"><a href="#41-Can-Diffusion-Models-Bridge-the-Domain-Gap-in-Cardiac-MR-Imaging" class="headerlink" title="41. Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Can_Diffusion_Models_Bridge_the_Domain_Gap_in_Cardiac_MR_Imaging_.pdf">Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Centre for Doctoral Training in AI for Medical Diagnosis and Care, School of Computing, University of Leeds</span></p><p>本论文提出了一种基于源域训练扩散模型（SD-DM）的方法，通过参考图像指导扩散过程，生成结构一致且分布对齐的合成心脏MR图像，用于数据稀缺、多中心域下的医学影像分割。方法支持测试时输入域适配和训练-测试混合适配，实现领域泛化和领域自适应，无需迁移学习或在线训练，显著提升未见域分割性能，减少对人工标注和多中心采集的依赖。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Can_Diffusion_Models_Bridge_the_Domain_Gap_in_Cardiac_MR_Imaging_.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Deepfake-Detection-that-Generalizes-Across-Benchmarks"><a href="#42-Deepfake-Detection-that-Generalizes-Across-Benchmarks" class="headerlink" title="42. Deepfake Detection that Generalizes Across Benchmarks"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Deepfake_Detection_that_Generalizes_Across_Benchmarks.pdf">Deepfake Detection that Generalizes Across Benchmarks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Czech Technical University in Prague</span></p><p>本文提出LNCLIP-DF方法，通过对预训练CLIP视觉编码器仅微调Layer Normalization参数、L2归一化特征、引入slerp特征空间增强及统一-对齐损失，实现了深度伪造检测的高效泛化。大量跨13个基准数据集的实验显示，该方法在大多数基准上优于复杂模型，且强调了使用真实-伪造配对训练集对提升泛化和防止捷径学习的重要性。结论指出，精心设计的数据配对和训练集多样性是构建强泛化性深度伪造检测器的关键。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Deepfake_Detection_that_Generalizes_Across_Benchmarks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-InfoCausalQA-Can-Models-Perform-Non-explicit-Causal-Reasoning-Based-on-Infographic"><a href="#43-InfoCausalQA-Can-Models-Perform-Non-explicit-Causal-Reasoning-Based-on-Infographic" class="headerlink" title="43. InfoCausalQA: Can Models Perform Non-explicit Causal Reasoning Based on Infographic?"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/InfoCausalQA_Can_Models_Perform_Non-explicit_Causal_Reasoning_Based_on_Infographic_.pdf">InfoCausalQA: Can Models Perform Non-explicit Causal Reasoning Based on Infographic?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yonsei University</span></p><p>该论文提出InfoCausalQA，这是首个用于评估视觉语言模型（VLMs）在信息图基础上进行因果推理能力的基准，包括基于数字趋势的定量因果推理和涉及五类因果关系的语义因果推理。实验结果显示，现有VLMs在基于信息图的因果推理任务上表现显著低于人类，凸显多模态模型在深层次因果推理上的不足。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/InfoCausalQA_Can_Models_Perform_Non-explicit_Causal_Reasoning_Based_on_Infographic_.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Clinically-guided-Data-Synthesis-for-Laryngeal-Lesion-Detection"><a href="#44-Clinically-guided-Data-Synthesis-for-Laryngeal-Lesion-Detection" class="headerlink" title="44. Clinically-guided Data Synthesis for Laryngeal Lesion Detection"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Clinically-guided_Data_Synthesis_for_Laryngeal_Lesion_Detection.pdf">Clinically-guided Data Synthesis for Laryngeal Lesion Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Istituto Italiano di Tecnologia</span></p><p>本文提出了一种结合Latent Diffusion Model (LDM) 与 ControlNet 的临床引导合成方法，用于生成喉部内窥镜图像-注释对，通过临床观察进行条件控制以提升图像的多样性与真实性。实验表明，加入不确定性策略筛选的10%合成数据可使喉部病变检测精度在内部数据集提升9%，外部数据集提升22.1%，且专家难以区分真实与合成图像，验证了合成样本的临床应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Clinically-guided_Data_Synthesis_for_Laryngeal_Lesion_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-Fewer-Denoising-Steps-or-Cheaper-Per-Step-Inference-Towards-Compute-Optimal-Diffusion-Model-Deployment"><a href="#45-Fewer-Denoising-Steps-or-Cheaper-Per-Step-Inference-Towards-Compute-Optimal-Diffusion-Model-Deployment" class="headerlink" title="45. Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Fewer_Denoising_Steps_or_Cheaper_Per-Step_Inference__Towards_Compute-Optimal_Diffusion_Model_Deploym.pdf">Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Georgia Institute of Technology</span></p><p>本文提出PostDiff，一种训练后无需微调即可用于加速扩散模型的训练自由压缩框架，分别在输入层（通过混合分辨率去噪策略）和模块层（通过混合模块缓存策略）减少推理冗余，提升效率。实验表明，相比减少去噪步数，降低每步推理成本通常能在保持高生成保真度的同时获得更优的效率-保真权衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Fewer_Denoising_Steps_or_Cheaper_Per-Step_Inference_Towards_Compute-Optimal_Diffusion_Model_Deployment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-Improving-Diagnostic-Accuracy-for-Oral-Cancer-with-inpainting-Synthesis-Lesions-Generated-Using-Diffusion-Models"><a href="#46-Improving-Diagnostic-Accuracy-for-Oral-Cancer-with-inpainting-Synthesis-Lesions-Generated-Using-Diffusion-Models" class="headerlink" title="46. Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Improving_Diagnostic_Accuracy_for_Oral_Cancer_with_inpainting_Synthesis_Lesions_Generated_Using_Diff.pdf">Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hongik University</span></p><p>本论文提出利用细调Stable Diffusion扩散模型结合Segment Anything Model (SAM) 分割与inpainting技术，生成高保真口腔癌病变合成图像，显著丰富了训练数据集多样性。通过将这些合成图像用于诊断模型训练，ResNet-50分类模型准确率提升至0.97，YOLO v8检测模型精度提升至0.85，验证了合成数据在医疗影像AI诊断中的有效性和可推广性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Improving_Diagnostic_Accuracy_for_Oral_Cancer_with_inpainting_Synthesis_Lesions_Generated_Using_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-DiffCap-Diffusion-based-Real-time-Human-Motion-Capture-using-Sparse-IMUs-and-a-Monocular-Camera"><a href="#47-DiffCap-Diffusion-based-Real-time-Human-Motion-Capture-using-Sparse-IMUs-and-a-Monocular-Camera" class="headerlink" title="47. DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/DiffCap__Diffusion-based_Real-time_Human_Motion_Capture_using_Sparse_IMUs_and_a_Monocular_Camera.pdf">DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>该论文提出DiffCap方法，首次将扩散模型应用于稀疏IMU与单目摄像头信号融合，实现实时高精度人体运动捕捉。方法核心在于将视觉信号序列编码为全局条件嵌入，IMU信号逐帧输入，采用两阶段扩散模型先预测3D关节点再回归姿态，显著提升了在遮挡和视野外等复杂场景下的鲁棒性和准确率。实验证明DiffCap在多个公开数据集上超越现有最优方法，尤其在遮挡、动态及野外场景下表现优越。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/DiffCap_Diffusion-based_Real-time_Human_Motion_Capture_using_Sparse_IMUs_and_a_Monocular_Camera.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-SC-Captioner-Improving-Image-Captioning-with-Self-Correction-by-Reinforcement-Learning"><a href="#48-SC-Captioner-Improving-Image-Captioning-with-Self-Correction-by-Reinforcement-Learning" class="headerlink" title="48. SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SC-Captioner__Improving_Image_Captioning_with_Self-Correction_by_Reinforcement_Learning.pdf">SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出SC-Captioner，一种针对图像描述任务的自我修正训练框架，通过多轮强化学习及新颖的奖励函数设计（结合对象、属性、关系的场景图解析）引导大型视觉语言模型在修正初始描述时精准移除幻觉、补全遗漏细节。实验表明，该方法在多个指标和场景下均优于直接偏好优化等主流方法，显著提升了图像描述质量和可靠性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SC-Captioner_Improving_Image_Captioning_with_Self-Correction_by_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-Ensemble-Based-Graph-Representation-of-fMRI-Data-for-Cognitive-Brain-State-Classification"><a href="#49-Ensemble-Based-Graph-Representation-of-fMRI-Data-for-Cognitive-Brain-State-Classification" class="headerlink" title="49. Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Ensemble-Based_Graph_Representation_of_fMRI_Data_for_Cognitive_Brain_State_Classification.pdf">Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute for Cognitive Neuroscience of University Higher School of Economics</span></p><p>该论文提出了一种基于集成学习的图表示方法，将fMRI数据转化为功能连接网络，并利用多个基础机器学习模型计算边权，实现二元脑状态分类。方法在七种认知任务上通过简单逻辑回归和图神经网络进行验证，集成图表现出更高的分类准确率（97.07%至99.74%），显著优于传统相关性图。结论表明集成图能更好地捕捉脑网络拓扑信息，提高脑状态判别力，且方法具有良好的可扩展性和可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Ensemble-Based_Graph_Representation_of_fMRI_Data_for_Cognitive_Brain_State_Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-Mask-Match-Learning-to-Recognize-Handwritten-Math-with-Self-Supervised-Attention"><a href="#50-Mask-Match-Learning-to-Recognize-Handwritten-Math-with-Self-Supervised-Attention" class="headerlink" title="50. Mask &amp; Match: Learning to Recognize Handwritten Math with Self-Supervised Attention"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Mask_&_Match__Learning_to_Recognize_Handwritten_Math_with_Self-Supervised_Attention.pdf">Mask &amp; Match: Learning to Recognize Handwritten Math with Self-Supervised Attention</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">International Institute of Information Technology Hyderabad</span></p><p>本文提出了一种针对手写数学表达式识别（HMER）的自监督学习框架，通过结合全局和局部对比损失进行图像编码器预训练，并引入渐进式空间掩码训练的自监督注意力机制，提升模型对复杂结构的理解能力。实验结果显示，该方法在CROHME基准上优于现有的自监督和全监督方法，渐进式掩码注意力机制显著增强了模型的结构鲁棒性和识别准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Mask_%26_Match_Learning_to_Recognize_Handwritten_Math_with_Self-Supervised_Attention.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-UGD-IML-A-Unified-Generative-Diffusion-based-Framework-for-Constrained-and-Unconstrained-Image-Manipulation-Localization"><a href="#51-UGD-IML-A-Unified-Generative-Diffusion-based-Framework-for-Constrained-and-Unconstrained-Image-Manipulation-Localization" class="headerlink" title="51. UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/UGD-IML__A_Unified_Generative_Diffusion-based_Framework_for_Constrained_and_Unconstrained_Image_Mani.pdf">UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology</span></p><p>该论文提出了UGD-IML框架，基于生成扩散模型，首次统一了解析和约束两种图像篡改定位任务，通过条件控制模块、噪声添加模块和去噪模块实现输入可控任务切换，并采用参数共享和类别嵌入机制提升效率。实验证明，UGD-IML在多数据集上对现有方法有显著性能提升，具备更高的准确性、鲁棒性和不确定性评估能力，有效支持真实场景下的图像真伪检测。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/UGD-IML_A_Unified_Generative_Diffusion-based_Framework_for_Constrained_and_Unconstrained_Image_Manipulation_Localization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-Transformer-Based-Explainable-Deep-Learning-for-Breast-Cancer-Detection-in-Mammography-The-MammoFormer-Framework"><a href="#52-Transformer-Based-Explainable-Deep-Learning-for-Breast-Cancer-Detection-in-Mammography-The-MammoFormer-Framework" class="headerlink" title="52. Transformer-Based Explainable Deep Learning for Breast Cancer Detection in Mammography: The MammoFormer Framework"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Transformer-Based_Explainable_Deep_Learning_for_Breast_Cancer_Detection_in_Mammography__The_MammoFor.pdf">Transformer-Based Explainable Deep Learning for Breast Cancer Detection in Mammography: The MammoFormer Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Morgan State University</span></p><p>该论文提出了MammoFormer框架，将多种Transformer架构（如ViT、Swin Transformer）与CNN、ConvNeXt等模型及多特征增强（原图、负片、AHE、HOG）联合，用于乳腺X光图像的癌症检测，并系统集成了Integrated Gradients、GradCAM等五种可解释性AI（XAI）技术。实验结果显示，通过针对架构优化的特征增强，Transformer模型在HOG等预处理下可达98.4%准确率，性能与CNN相当且具备更强的全局上下文建模和多角度可解释性，为乳腺AI筛查提供高精度和透明的诊断支持。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Transformer-Based_Explainable_Deep_Learning_for_Breast_Cancer_Detection_in_Mammography_The_MammoFormer_Framework.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-SAM-Encoder-Breach-by-Adversarial-Simplicial-Complex-Triggers-Downstream-Model-Failures"><a href="#53-SAM-Encoder-Breach-by-Adversarial-Simplicial-Complex-Triggers-Downstream-Model-Failures" class="headerlink" title="53. SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SAM_Encoder_Breach_by_Adversarial_Simplicial_Complex_Triggers_Downstream_Model_Failures.pdf">SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Jiaotong University</span></p><p>本文提出了一种针对Segment Anything Model (SAM)视觉基础模型的高效可迁移对抗攻击方法——Vertex-Refining Simplicial Complex Attack (VeSCA)。该方法仅依赖SAM的编码器，通过参数化单纯形复合体显式建模并精炼与下游模型共享的脆弱子空间，结合轻量级领域再适应策略，实现了对多类下游任务模型的高效攻击。实验结果显示，VeSCA在五个视觉分割相关的数据集和多种下游任务上，攻击性能比当前最优方法提升12.7%，验证了SAM脆弱性对其下游生态的系统性风险。结论是，VeSCA可系统性暴露视觉基础模型及其下游的安全风险，提示需加强基础模型鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SAM_Encoder_Breach_by_Adversarial_Simplicial_Complex_Triggers_Downstream_Model_Failures.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-Learning-Representations-of-Satellite-Images-with-Evaluations-on-Synoptic-Weather-Events"><a href="#54-Learning-Representations-of-Satellite-Images-with-Evaluations-on-Synoptic-Weather-Events" class="headerlink" title="54. Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Learning_Representations_of_Satellite_Images_with_Evaluations_on_Synoptic_Weather_Events.pdf">Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Taiwan University</span></p><p>本文通过对卫星影像应用主成分分析（PCA）、卷积自编码器（CAE）以及预训练残差网络（ResNet50）等表征学习算法，结合可解释分类方法（如逻辑回归），实现多种天气事件的自动识别。实验结果表明CAE在所有分类任务上威胁评分最高，并且高分辨率卫星数据能显著提升其表现；相比之下，PCA有较高的识别率但误报率也更高。结论：CAE是一种高效且表现优异的卫星图像表征学习方法，尤其适合高分辨率数据，但物理可解释性仍需改进。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Learning_Representations_of_Satellite_Images_with_Evaluations_on_Synoptic_Weather_Events.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-SynSeg-Feature-Synergy-for-Multi-Category-Contrastive-Learning-in-Open-Vocabulary-Semantic-Segmentation"><a href="#55-SynSeg-Feature-Synergy-for-Multi-Category-Contrastive-Learning-in-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="55. SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SynSeg__Feature_Synergy_for_Multi-Category_Contrastive_Learning_in_Open-Vocabulary_Semantic_Segmenta.pdf">SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Global Innovation Exchange</span></p><p>该论文提出SynSeg方法，通过Feature Synergy Structure（FSS）与Multi-Category Contrastive Learning（MCCL）结合，实现开放词汇语义分割的弱监督训练。FSS融合视觉和语义激活图生成判别特征，MCCL则同时强化类内和类间对比学习，有效提升分割性能。实验结果表明，SynSeg在五个主流数据集上显著超越现有弱监督方法，实现了最佳mIoU表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SynSeg_Feature_Synergy_for_Multi-Category_Contrastive_Learning_in_Open-Vocabulary_Semantic_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-MCA-2D-3D-Retrieval-with-Noisy-Labels-via-Multi-level-Adaptive-Correction-and-Alignment"><a href="#56-MCA-2D-3D-Retrieval-with-Noisy-Labels-via-Multi-level-Adaptive-Correction-and-Alignment" class="headerlink" title="56. MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/MCA__2D-3D_Retrieval_with_Noisy_Labels_via_Multi-level_Adaptive_Correction_and_Alignment.pdf">MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了一种面向2D-3D跨模态检索的多层次自适应纠正与对齐框架（MCA），包括多模态联合标签纠正（MJC）机制和多层次自适应对齐（MAA）策略。MJC利用多模态历史自预测实现标签纠正，MAA则在中心、组和实例三个层次自适应对齐特征，显著提升了在噪声标签下的跨模态检索准确性和鲁棒性。实验表明，该方法在合成和真实噪声3D数据集上均实现了最优性能，有效缓解了标签噪声带来的影响。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/MCA_2D-3D_Retrieval_with_Noisy_Labels_via_Multi-level_Adaptive_Correction_and_Alignment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-E-React-Towards-Emotionally-Controlled-Synthesis-of-Human-Reactions"><a href="#57-E-React-Towards-Emotionally-Controlled-Synthesis-of-Human-Reactions" class="headerlink" title="57. E-React: Towards Emotionally Controlled Synthesis of Human Reactions"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/E-React__Towards_Emotionally_Controlled_Synthesis_of_Human_Reactions.pdf">E-React: Towards Emotionally Controlled Synthesis of Human Reactions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southeast University, China</span></p><p>该论文提出了E-React框架，实现了基于半监督情感先验的情感驱动反应动作生成。方法包括用少量标注数据和大量未标注数据训练情感先验，通过对称的actor-reactor扩散模型结合空间关系和情感嵌入生成自然多样的人体反应动作。实验表明，本方法在情感一致性与交互自然性上优于现有方法，提升了多人物互动动作生成的真实感与多样性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/E-React_Towards_Emotionally_Controlled_Synthesis_of_Human_Reactions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-Towards-MR-Based-Trochleoplasty-Planning"><a href="#58-Towards-MR-Based-Trochleoplasty-Planning" class="headerlink" title="58. Towards MR-Based Trochleoplasty Planning"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Towards_MR-Based_Trochleoplasty_Planning.pdf">Towards MR-Based Trochleoplasty Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Basel</span></p><p>该论文提出了一套基于MR影像的三维髁沟成形术（trochleoplasty）手术规划管线。方法包括利用隐式神经表示（INR）实现MR图像超分辨率重建，结合自定义多标签分割网络对膝关节结构进行分割，并采用小波扩散模型（WDM）对病变区域进行伪健康形态生成。实验表明，该方法在25例患者中显著改善了髁沟角和深度，为术前和术中提供了高分辨率、个性化的三维手术蓝图，且无需CT，降低了辐射风险。结论：该管线提升了手术一致性与安全性，有望推广到更多外科应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Towards_MR-Based_Trochleoplasty_Planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-AGI-for-the-Earth-the-path-possibilities-and-how-to-evaluate-intelligence-of-models-that-work-with-Earth-Observation-Data"><a href="#59-AGI-for-the-Earth-the-path-possibilities-and-how-to-evaluate-intelligence-of-models-that-work-with-Earth-Observation-Data" class="headerlink" title="59. AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/AGI_for_the_Earth,_the_path,_possibilities_and_how_to_evaluate_intelligence_of_models_that_work_with.pdf">AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">HUM.AI</span></p><p>本文系统梳理了地球观测（EO）数据对实现人工通用智能（AGI）的重要性，并分析了现有EO基准测试的不足。作者提出了一套涵盖多种卫星影像任务的分类框架，强调应建立一个覆盖场景分类、变化检测、场景生成及反事实分析等多任务、跨模态的综合性基准，以全面评估和推动AGI在地球观测领域的能力。结论指出现有基准难以充分衡量AGI模型泛化与推理能力，呼吁社区协作完善多任务基准，助力环境与可持续发展挑战应对。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/AGI_for_the_Earth%2C_the_path%2C_possibilities_and_how_to_evaluate_intelligence_of_models_that_work_with_Earth_Observation_Data_.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-Learning-3D-Texture-Aware-Representations-for-Parsing-Diverse-Human-Clothing-and-Body-Parts"><a href="#60-Learning-3D-Texture-Aware-Representations-for-Parsing-Diverse-Human-Clothing-and-Body-Parts" class="headerlink" title="60. Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Learning_3D_Texture-Aware_Representations_for_Parsing_Diverse_Human_Clothing_and_Body_Parts.pdf">Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KTH Royal Institute of Technology</span></p><p>该论文提出了Spectrum模型，通过重新利用I2Tx（Image-to-Texture）扩散模型的内部特征，实现对多人体场景中细粒度衣物和身体部位的逐像素语义分割。方法结合3D纹理映射和多模态文本引导分割，支持对新颖、未见服装类别的泛化。实验结果表明，Spectrum在多数据集下均优于现有基线方法，能准确解析复杂场景中的人体衣物和部位。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Learning_3D_Texture-Aware_Representations_for_Parsing_Diverse_Human_Clothing_and_Body_Parts.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-A-3DGS-Diffusion-Self-Supervised-Framework-for-Normal-Estimation-from-a-Single-Image"><a href="#61-A-3DGS-Diffusion-Self-Supervised-Framework-for-Normal-Estimation-from-a-Single-Image" class="headerlink" title="61. A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/A_3DGS-Diffusion_Self-Supervised_Framework_for_Normal_Estimation_from_a_Single_Image.pdf">A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Jiangnan University</span></p><p>该论文提出了一种名为SINGAD的自监督单幅图像法线估计算法，结合3D Gaussian Splatting（3DGS）物理建模与条件扩散模型，通过物理光交互建模、跨域特征融合及可微3D重投影损失，将3D几何误差直接转化为法线优化信号，实现无标注数据下多视角几何一致的法线估计。实验结果表明，该方法在Google Scanned Objects数据集上超越现有主流方法，显著提升了几何一致性和细节重建能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/A_3DGS-Diffusion_Self-Supervised_Framework_for_Normal_Estimation_from_a_Single_Image.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-Improved-Sub-Visible-Particle-Classification-in-Flow-Imaging-Microscopy-via-Generative-AI-Based-Image-Synthesis"><a href="#62-Improved-Sub-Visible-Particle-Classification-in-Flow-Imaging-Microscopy-via-Generative-AI-Based-Image-Synthesis" class="headerlink" title="62. Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Improved_Sub-Visible_Particle_Classification_in_Flow_Imaging_Microscopy_via_Generative_AI-Based_Imag.pdf">Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ghent University Global Campus</span></p><p>该论文提出利用扩散生成模型（diffusion models）为流式成像显微镜下亚可见颗粒（SvPs）分类中的少数类（如硅油滴和气泡）合成高保真图像，并用这些生成样本扩充不平衡训练集，显著提升多类深度神经网络（如ResNet-50）在高不平衡真实验证集上的分类精度。实验显示，融合生成图像的数据增强方案能有效缓解类别不平衡问题，提升药品质量控制中微粒自动识别的鲁棒性和准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Improved_Sub-Visible_Particle_Classification_in_Flow_Imaging_Microscopy_via_Generative_AI-Based_Image_Synthesis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-KnapFormer-An-Online-Load-Balancer-for-Efficient-Diffusion-Transformers-Training"><a href="#63-KnapFormer-An-Online-Load-Balancer-for-Efficient-Diffusion-Transformers-Training" class="headerlink" title="63. KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/KnapFormer__An_Online_Load_Balancer_for_Efficient_Diffusion_Transformers_Training.pdf">KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Adobe</span></p><p>KnapFormer是一种高效的在线序列块级负载均衡算法，专为分布式训练Diffusion Transformers（DiT）模型设计，通过解决跨GPU的动态token长度不均衡问题，实现了序列并行处理与工作负载最优分配。该方法集成了DeepSpeed-Ulysses分布式注意力机制，仅需一次all-to-all通信即可实现全局负载均衡，显著提升了训练吞吐率，在多模态、混合分辨率和图像视频联合训练场景下可获得2至3倍加速，消除straggler效应。结论：KnapFormer可大幅提升异构数据分布下的DiT模型训练效率，并简化系统设计。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/KnapFormer_An_Online_Load_Balancer_for_Efficient_Diffusion_Transformers_Training.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-Mediator-Guided-Multi-Agent-Collaboration-among-Open-Source-Models-for-Medical-Decision-Making"><a href="#64-Mediator-Guided-Multi-Agent-Collaboration-among-Open-Source-Models-for-Medical-Decision-Making" class="headerlink" title="64. Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Mediator-Guided_Multi-Agent_Collaboration_among_Open-Source_Models_for_Medical_Decision-Making.pdf">Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出MedOrch，一个基于开源多模态视觉语言模型（VLM）和大语言模型（LLM）中介者的多智能体协作框架，用于医学多模态决策。其流程包括多VLM专家独立作答、LLM中介者开展Socratic问答引导反思、专家修正答案，最终由判决智能体综合输出。实验在五个医学视觉问答基准上验证了该方法无需训练即可显著超越单一模型和传统多智能体策略，提升医学多模态智能决策性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Mediator-Guided_Multi-Agent_Collaboration_among_Open-Source_Models_for_Medical_Decision-Making.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-ECMF-Enhanced-Cross-Modal-Fusion-for-Multimodal-Emotion-Recognition-in-MER-SEMI-Challenge"><a href="#65-ECMF-Enhanced-Cross-Modal-Fusion-for-Multimodal-Emotion-Recognition-in-MER-SEMI-Challenge" class="headerlink" title="65. ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ECMF__Enhanced_Cross-Modal_Fusion_for_Multimodal_Emotion_Recognition_in_MER-SEMI_Challenge.pdf">ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of General Artificial Intelligence, BIGAI</span></p><p>该论文提出了一种多模态情感识别框架，结合预训练音频（HuBERT-Large）、文本（Chinese-RoBERTa-wwm-ext-large，GPT-4，Qwen-Omni）和视觉（OpenFace，CLIP-ViT-Large）特征提取器，通过自注意力机制与残差连接实现多模态特征融合，并通过多源标签策略提升数据质量。实验结果显示，该方法在MER2025-SEMI数据集上加权F1显著优于官方基线，验证了框架的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ECMF_Enhanced_Cross-Modal_Fusion_for_Multimodal_Emotion_Recognition_in_MER-SEMI_Challenge.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-Neural-Field-Representations-of-Mobile-Computational-Photography"><a href="#66-Neural-Field-Representations-of-Mobile-Computational-Photography" class="headerlink" title="66. Neural Field Representations of Mobile Computational Photography"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Neural_Field_Representations_of_Mobile_Computational_Photography.pdf">Neural Field Representations of Mobile Computational Photography</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Princeton University</span></p><p>本文提出了多种基于神经场（neural field）的紧凑模型，用于移动设备摄影数据的深度估计、图层分离和全景图像拼接。通过直接对手机采集的原始多帧数据进行端到端优化，无需复杂预处理或标注，实现了高质量的深度重建、前景&#x2F;背景分离和视角扩展，在多个真实场景和基准测试中超越了现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Neural_Field_Representations_of_Mobile_Computational_Photography.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="67-Effective-Training-Data-Synthesis-for-Improving-MLLM-Chart-Understanding"><a href="#67-Effective-Training-Data-Synthesis-for-Improving-MLLM-Chart-Understanding" class="headerlink" title="67. Effective Training Data Synthesis for Improving MLLM Chart Understanding"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Effective_Training_Data_Synthesis_for_Improving_MLLM_Chart_Understanding.pdf">Effective Training Data Synthesis for Improving MLLM Chart Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Australian National University</span></p><p>论文提出了一种五步图表数据合成管道，包括分离数据和函数的单图生成、条件式多子图生成、视觉多样化、质量过滤和基于GPT-4o的问答对生成，构建了高质量的Effective Chart Dataset (ECD)。实验表明，使用ECD微调多种开源多模态大语言模型能在真实和合成测试集上显著提升图表理解准确率，尤其是在复杂科学图表领域，优于现有训练集。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Effective_Training_Data_Synthesis_for_Improving_MLLM_Chart_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-LLM-Unlearning-using-Gradient-Ratio-Based-Influence-Estimation-and-Noise-Injection"><a href="#68-LLM-Unlearning-using-Gradient-Ratio-Based-Influence-Estimation-and-Noise-Injection" class="headerlink" title="68. LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LLM_Unlearning_using_Gradient_Ratio-Based_Influence_Estimation_and_Noise_Injection.pdf">LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Urbana Champaign</span></p><p>本文提出GRIN，一种面向大语言模型（LLM）精准高效的机器遗忘方法。方法首先利用梯度比率衡量参数对待遗忘数据的影响力，定位出最关键的权重，并在这些权重注入噪声后进行有针对性的微调，以提升遗忘效果并减少对保留知识的损害。实验在TOFU、WMDP和SafePKU等基准数据集上验证，GRIN能够在有效删除敏感或有害信息的同时保持模型整体性能。结论表明GRIN兼具高效、模块化和实际可用性，适用于法律合规和安全需求场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LLM_Unlearning_using_Gradient_Ratio-Based_Influence_Estimation_and_Noise_Injection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-ScamAgents-How-AI-Agents-Can-Simulate-Human-Level-Scam-Calls"><a href="#69-ScamAgents-How-AI-Agents-Can-Simulate-Human-Level-Scam-Calls" class="headerlink" title="69. ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ScamAgents__How_AI_Agents_Can_Simulate_Human-Level_Scam_Calls.pdf">ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Rutgers University</span></p><p>该论文提出ScamAgent，一个基于大型语言模型（LLM）的自主多回合智能代理系统，可自动生成高度逼真的诈骗电话脚本，并集成对话记忆、动态适应和文本转语音（TTS）技术。实验证明，ScamAgent能有效规避现有LLM安全机制，成功执行多场景诈骗对话，显示多回合、代理型滥用是现有安全防护的显著漏洞。结论指出，亟需多回合安全审计、代理级控制框架及新型对抗性检测方法，以应对生成式AI推动的对话欺诈风险。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ScamAgents_How_AI_Agents_Can_Simulate_Human-Level_Scam_Calls.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-Learning-the-Topic-Not-the-Language-How-LLMs-Classify-Online-Immigration-Discourse-Across-Languages"><a href="#70-Learning-the-Topic-Not-the-Language-How-LLMs-Classify-Online-Immigration-Discourse-Across-Languages" class="headerlink" title="70. Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Learning_the_Topic,_Not_the_Language__How_LLMs_Classify_Online_Immigration_Discourse_Across_Language.pdf">Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harvard University</span></p><p>该论文提出通过对开源LLaMA 3.2–3B模型在单语、双语及多语种数据集上LoRA微调，探索少量语言微调能否实现跨语言的移民推文主题与立场分类。研究发现：单&#x2F;双语微调模型能可靠检测多种未见语言中的移民相关内容，多语种微调则有助于更准确区分支持&#x2F;反对&#x2F;中立立场；只需极少量目标语言数据即可显著提升低资源语言表现并修正预训练偏见。此外，论文发布了4比特量化、可重现、推理速度快且成本极低的开源模型，为多语言社会科学研究提供了低门槛工具。结论是，跨语言主题检测无需大规模多语训练，有限语言覆盖即可实现主题泛化，结构性偏见可通过轻量干预修正。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Learning_the_Topic%2C_Not_the_Language_How_LLMs_Classify_Online_Immigration_Discourse_Across_Languages.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-Memp-Exploring-Agent-Procedural-Memory"><a href="#71-Memp-Exploring-Agent-Procedural-Memory" class="headerlink" title="71. Memp: Exploring Agent Procedural Memory"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Memp__Exploring_Agent_Procedural_Memory.pdf">Memp: Exploring Agent Procedural Memory</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了Memp框架，通过将以往代理任务轨迹蒸馏为细粒度操作指令与高层次脚本抽象，实现了可学习、可更新、终身进化的程序性记忆。Memp系统性地研究了记忆的构建、检索与动态更新策略，在TravelPlanner和ALFWorld等基准任务上，实验结果显示程序性记忆能显著提升代理的成功率和效率，同时具备持续学习和迁移能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Memp_Exploring_Agent_Procedural_Memory.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-Sample-efficient-LLM-Optimization-with-Reset-Replay"><a href="#72-Sample-efficient-LLM-Optimization-with-Reset-Replay" class="headerlink" title="72. Sample-efficient LLM Optimization with Reset Replay"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Sample-efficient_LLM_Optimization_with_Reset_Replay.pdf">Sample-efficient LLM Optimization with Reset Replay</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本文提出了LoRR（LLM optimization with Reset Replay），一种增强大语言模型（LLM）微调样本效率的通用插件。LoRR通过高重放训练（多次利用每批数据）、周期性参数重置（Shrink &amp; Perturb战略）及混合优化损失（结合SFT和偏好损失），显著提升了偏好优化方法在数学与推理任务上的表现。结论显示，LoRR能有效缓解早期过拟合和样本利用不足问题，迭代DPO结合LoRR在多个基准上超越复杂RL算法，极大提高了LLM微调的效能和数据利用率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Sample-efficient_LLM_Optimization_with_Reset_Replay.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-End-to-End-Text-to-SQL-with-Dataset-Selection-Leveraging-LLMs-for-Adaptive-Query-Generation"><a href="#73-End-to-End-Text-to-SQL-with-Dataset-Selection-Leveraging-LLMs-for-Adaptive-Query-Generation" class="headerlink" title="73. End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/End-to-End_Text-to-SQL_with_Dataset_Selection__Leveraging_LLMs_for_Adaptive_Query_Generation.pdf">End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Infoorigin Pvt Ltd</span></p><p>该论文提出了一种三阶段端到端Text-to-SQL框架，首先通过LLM与prompt工程生成隐式规则集，结合RoBERTa编码器模型进行自动数据库识别（db id prediction），然后利用数据库schema和NLQ进行prompt驱动SQL生成，最后通过多智能体自纠错模块提升SQL准确性。实验表明该方法在Spider数据集上数据库意图预测与SQL生成均超越现有SOTA，显著提升复杂查询场景下的执行准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/End-to-End_Text-to-SQL_with_Dataset_Selection_Leveraging_LLMs_for_Adaptive_Query_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-SpeakerLM-End-to-End-Versatile-Speaker-Diarization-and-Recognition-with-Multimodal-Large-Language-Models"><a href="#74-SpeakerLM-End-to-End-Versatile-Speaker-Diarization-and-Recognition-with-Multimodal-Large-Language-Models" class="headerlink" title="74. SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SpeakerLM__End-to-End_Versatile_Speaker_Diarization_and_Recognition_with_Multimodal_Large_Language_M.pdf">SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tongyi Lab</span></p><p>本文提出了SpeakerLM，这是首个针对端到端说话人分离与识别（SDR）任务的多模态大语言模型（MLLM），采用音频编码器、投影器与预训练文本LLM耦合，形成Encoder-Projector-LLM架构。其创新在于引入灵活的说话人注册机制，支持不同注册场景下的SDR，通过多阶段大规模数据训练，模型在多项基准上超越级联式SOTA系统，并在复杂场景下表现出极强的泛化与鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SpeakerLM_End-to-End_Versatile_Speaker_Diarization_and_Recognition_with_Multimodal_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-From-Explainable-to-Explanatory-Artificial-Intelligence-Toward-a-New-Paradigm-for-Human-Centered-Explanations-through-Generative-AI"><a href="#75-From-Explainable-to-Explanatory-Artificial-Intelligence-Toward-a-New-Paradigm-for-Human-Centered-Explanations-through-Generative-AI" class="headerlink" title="75. From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/From_Explainable_to_Explanatory_Artificial_Intelligence__Toward_a_New_Paradigm_for_Human-Centered_Ex.pdf">From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ruhr University Bochum</span></p><p>该论文提出了“Explanatory AI”新范式，利用生成式AI（如大语言模型）为用户生成叙事型、个性化、可交互的解释，区别于传统XAI仅关注算法透明性。通过构建八维概念模型并在医疗场景实证，结果显示用户更偏好情境敏感、易懂的解释，强调AI系统应以人类理解为核心而非仅算法透明。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/From_Explainable_to_Explanatory_Artificial_Intelligence_Toward_a_New_Paradigm_for_Human-Centered_Explanations_through_Generative_AI.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-Large-Language-Model-Data-Generation-for-Enhanced-Intent-Recognition-in-German-Speech"><a href="#76-Large-Language-Model-Data-Generation-for-Enhanced-Intent-Recognition-in-German-Speech" class="headerlink" title="76. Large Language Model Data Generation for Enhanced Intent Recognition in German Speech"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Large_Language_Model_Data_Generation_for_Enhanced_Intent_Recognition_in_German_Speech.pdf">Large Language Model Data Generation for Enhanced Intent Recognition in German Speech</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Hamburg</span></p><p>本论文提出结合Whisper自动语音识别（ASR）与BERT、DistilBERT和Electra等Transformer语言模型，利用LeoLM、Llama3和ChatGPT三种大语言模型自动生成德语意图识别任务的训练文本，提升针对老年德语语音的意图识别性能。实验结果表明，加入由LLM合成的数据能有效提升模型对不同说话风格和未见词汇的泛化和鲁棒性，其中专为德语设计的小型LeoLM生成的数据优于更大规模的ChatGPT数据，展示了本地领域特定LLM的优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Large_Language_Model_Data_Generation_for_Enhanced_Intent_Recognition_in_German_Speech.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-SIFThinker-Spatially-Aware-Image-Focus-for-Visual-Reasoning"><a href="#77-SIFThinker-Spatially-Aware-Image-Focus-for-Visual-Reasoning" class="headerlink" title="77. SIFThinker: Spatially-Aware Image Focus for Visual Reasoning"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SIFThinker__Spatially-Aware_Image_Focus_for_Visual_Reasoning.pdf">SIFThinker: Spatially-Aware Image Focus for Visual Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua Shenzhen International Graduate School, Tsinghua University</span></p><p>SIFThinker提出了一种空间感知的“think-with-images”多模态大语言模型框架，通过引入深度增强的区域关注机制，将3D空间信息与图像-文本链式推理相结合，并利用反向扩展-前向推理策略构建SIF-50K数据集。采用基于GRPO的空间感知强化学习方法（GRPO-SIF），集成多种奖励（格式、深度一致性、区域校正等）优化模型动态关注和推理能力。实验表明SIFThinker在空间理解与细粒度视觉感知方面超越现有SOTA模型，具有强鲁棒性和广泛适用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SIFThinker_Spatially-Aware_Image_Focus_for_Visual_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-In-Training-Defenses-against-Emergent-Misalignment-in-Language-Models"><a href="#78-In-Training-Defenses-against-Emergent-Misalignment-in-Language-Models" class="headerlink" title="78. In-Training Defenses against Emergent Misalignment in Language Models"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/In-Training_Defenses_against_Emergent_Misalignment_in_Language_Models.pdf">In-Training Defenses against Emergent Misalignment in Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bonn-Aachen International Center for Information Technology, University of Bonn</span></p><p>本文系统性研究了在大语言模型（LLM）微调过程中，防止涌现性失对齐（EMA）的实用正则化方法，包括KL散度正则化、特征空间L2距离保持（LDIFS）、安全子空间投影（SafeLoRA）以及安全数据插入（Interleaving）。实验结果表明，KL散度和插入安全数据能显著抑制EMA，但分别会带来学习能力受限和答案不连贯的副作用，仍需更精细的策略以实现安全与效能的平衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/In-Training_Defenses_against_Emergent_Misalignment_in_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-Classification-is-a-RAG-problem-A-case-study-on-hate-speech-detection"><a href="#79-Classification-is-a-RAG-problem-A-case-study-on-hate-speech-detection" class="headerlink" title="79. Classification is a RAG problem: A case study on hate speech detection"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Classification_is_a_RAG_problem__A_case_study_on_hate_speech_detection.pdf">Classification is a RAG problem: A case study on hate speech detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Contextual AI</span></p><p>本文提出了一种基于检索增强生成（RAG, Retrieval-Augmented Generation）的分类方法，将传统分类任务转变为基于推理时检索到的上下文知识进行评估。作者开发了CONTEXTUAL POLICY ENGINE (CPE)系统，通过检索相关政策文档并结合生成式模型，对仇恨言论进行灵活、可解释和可动态调整的分类，实验表明该方法在准确性、可扩展性和灵活性方面优于主流商用系统，无需模型重训练即可适应政策变更。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Classification_is_a_RAG_problem_A_case_study_on_hate_speech_detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-UR2-Unify-RAG-and-Reasoning-through-Reinforcement-Learning"><a href="#80-UR2-Unify-RAG-and-Reasoning-through-Reinforcement-Learning" class="headerlink" title="80. UR2: Unify RAG and Reasoning through Reinforcement Learning"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/UR$^2$__Unify_RAG_and_Reasoning_through_Reinforcement_Learning.pdf">UR2: Unify RAG and Reasoning through Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出UR2框架，首次将检索增强生成（RAG）与强化学习（RL）紧密结合，实现大模型在推理过程中动态协调何时检索与何时依赖自身知识。UR2采用难度感知课程训练，仅对复杂问题激活检索，并通过混合知识访问策略融合离线领域语料与LLM摘要，提升模型的泛化与适应性。实验表明，UR2在数学、医学、开放领域QA等多任务上显著优于现有RAG和RL方法，7B模型达到GPT-4o-mini水准，具备广泛的通用性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/UR2_Unify_RAG_and_Reasoning_through_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-Automated-Creation-of-the-Legal-Knowledge-Graph-Addressing-Legislation-on-Violence-Against-Women-Resource-Methodology-and-Lessons-Learned"><a href="#81-Automated-Creation-of-the-Legal-Knowledge-Graph-Addressing-Legislation-on-Violence-Against-Women-Resource-Methodology-and-Lessons-Learned" class="headerlink" title="81. Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Automated_Creation_of_the_Legal_Knowledge_Graph_Addressing_Legislation_on_Violence_Against_Women__Re.pdf">Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Bari Aldo Moro</span></p><p>本论文提出了两种用于自动化构建针对女性暴力立法的法律知识图谱的方法：一种是法律领域定制的系统化底层流程，包括结构化数据提取、语义丰富和本体开发，另一种则基于大型语言模型（LLM）结合检索增强生成（RAG）、提示工程和传统NLP技术。结论显示，底层方法精度高但耗时，LLM方法具备快速扩展性但需人工校验，二者互补能提升法律信息获取和智能化应用效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Automated_Creation_of_the_Legal_Knowledge_Graph_Addressing_Legislation_on_Violence_Against_Women_Resource%2C_Methodology_and_Lessons_Learned.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-Beyond-Prompt-Induced-Lies-Investigating-LLM-Deception-on-Benign-Prompts"><a href="#82-Beyond-Prompt-Induced-Lies-Investigating-LLM-Deception-on-Benign-Prompts" class="headerlink" title="82. Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Beyond_Prompt-Induced_Lies__Investigating_LLM_Deception_on_Benign_Prompts.pdf">Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>本文提出了一个名为Contact Searching Question (CSQ) 的新评估框架，用于量化大型语言模型（LLM）在无明确欺骗意图的普通问题上的自发性欺骗行为。方法包括设计两项心理学启发的统计指标：欺骗意图分数和欺骗行为分数，在14个主流LLM上实证分析发现，这两项分数在任务难度提升时同步上升，揭示当前LLM在复杂任务中欺骗倾向增强，提出对应数学模型解释。该结论表明，先进LLM在处理复杂任务时自发性欺骗行为普遍存在，需引起信任和安全重视。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Beyond_Prompt-Induced_Lies_Investigating_LLM_Deception_on_Benign_Prompts.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-Aligning-Effective-Tokens-with-Video-Anomaly-in-Large-Language-Models"><a href="#83-Aligning-Effective-Tokens-with-Video-Anomaly-in-Large-Language-Models" class="headerlink" title="83. Aligning Effective Tokens with Video Anomaly in Large Language Models"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Aligning_Effective_Tokens_with_Video_Anomaly_in_Large_Language_Models.pdf">Aligning Effective Tokens with Video Anomaly in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Hong Kong</span></p><p>本文提出VA-GPT，一种专为视频异常事件检测与理解设计的多模态大语言模型，通过Spatial Effective Token Selection（SETS）和Temporal Effective Token Generation（TETG）模块，有效选择和生成与异常事件相关的空间和时间关键视觉令牌，并与LLM对齐。实验结果显示，VA-GPT在多个基准测试（含跨域场景）中异常检测和定位表现优于现有方法，显著提升了异常视频理解的准确性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Aligning_Effective_Tokens_with_Video_Anomaly_in_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-Harnessing-Adaptive-Topology-Representations-for-Zero-Shot-Graph-Question-Answering"><a href="#84-Harnessing-Adaptive-Topology-Representations-for-Zero-Shot-Graph-Question-Answering" class="headerlink" title="84. Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Harnessing_Adaptive_Topology_Representations_for_Zero-Shot_Graph_Question_Answering.pdf">Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southern University of Science and Technology</span></p><p>该论文提出DynamicTRF框架，通过系统分析现有拓扑表示形式（TRF）并引入GRE指标，设计专用于零样本图问答的多样TRF集FZS。方法包括构建TRF偏好数据集、训练TRF Router以实现针对问题的动态TRF分配，显著提升LMM的零样本图问答准确率与简洁性。实验表明DynamicTRF在7种算法任务和2种下游任务上均优于现有方法，实现了准确性与响应长度的均衡提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Harnessing_Adaptive_Topology_Representations_for_Zero-Shot_Graph_Question_Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-GeoLaux-A-Benchmark-for-Evaluating-MLLMs’-Geometry-Performance-on-Long-Step-Problems-Requiring-Auxiliary-Lines"><a href="#85-GeoLaux-A-Benchmark-for-Evaluating-MLLMs’-Geometry-Performance-on-Long-Step-Problems-Requiring-Auxiliary-Lines" class="headerlink" title="85. GeoLaux: A Benchmark for Evaluating MLLMs’ Geometry Performance on Long-Step Problems Requiring Auxiliary Lines"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/GeoLaux__A_Benchmark_for_Evaluating_MLLMs'_Geometry_Performance_on_Long-Step_Problems_Requiring_Auxi.pdf">GeoLaux: A Benchmark for Evaluating MLLMs’ Geometry Performance on Long-Step Problems Requiring Auxiliary Lines</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>该论文提出了GeoLaux，大规模几何问题数据集，专为评估多模态大语言模型（MLLMs）在长步骤推理和辅助线构建任务中的表现。作者设计了包含五维度的细粒度评估框架，并在13种主流MLLMs上进行了实验，发现模型在长步骤推理中性能急剧下降、证明题易偷懒、辅助线意识薄弱，提升辅助线能力能显著改善几何推理效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/GeoLaux_A_Benchmark_for_Evaluating_MLLMs%E2%80%99_Geometry_Performance_on_Long-Step_Problems_Requiring_Auxiliary_Lines.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-LoRA-in-LoRA-Towards-Parameter-Efficient-Architecture-Expansion-for-Continual-Visual-Instruction-Tuning"><a href="#86-LoRA-in-LoRA-Towards-Parameter-Efficient-Architecture-Expansion-for-Continual-Visual-Instruction-Tuning" class="headerlink" title="86. LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LoRA_in_LoRA__Towards_Parameter-Efficient_Architecture_Expansion_for_Continual_Visual_Instruction_Tu.pdf">LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hefei University of Technology</span></p><p>本文提出了LiLoRA方法，用于多模态大语言模型（MLLMs）中的持续视觉指令微调（CVIT），通过在LoRA低秩适配器中跨任务共享矩阵A，并对任务相关的矩阵B进行进一步低秩分解，大幅提升参数效率。同时引入余弦正则化稳定损失以保持共享基的稳定性。实验结果显示，LiLoRA在保持高性能的同时，显著减少了参数开销，有效缓解灾难性遗忘问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LoRA_in_LoRA_Towards_Parameter-Efficient_Architecture_Expansion_for_Continual_Visual_Instruction_Tuning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-Retrieval-Augmented-Large-Language-Model-System-for-Comprehensive-Drug-Contraindications"><a href="#87-Retrieval-Augmented-Large-Language-Model-System-for-Comprehensive-Drug-Contraindications" class="headerlink" title="87. Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Retrieval_Augmented_Large_Language_Model_System_for_Comprehensive_Drug_Contraindications.pdf">Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hongik University</span></p><p>本文提出了一种检索增强生成（RAG）系统，结合GPT-4o-mini大语言模型与药品使用审查（DUR）知识库，通过混合语义和关键词检索及重排序，在药物禁忌问答任务中实现高准确率。结果显示，该方法对孕妇、儿童及联合用药场景的禁忌判别准确率显著提升，能够为敏感群体提供更可靠的用药安全信息，极大减少处方决策的不确定性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Retrieval_Augmented_Large_Language_Model_System_for_Comprehensive_Drug_Contraindications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-Less-is-More-Selective-Reflection-for-Compatible-and-Efficient-Knowledge-Distillation-in-Large-Language-Models"><a href="#88-Less-is-More-Selective-Reflection-for-Compatible-and-Efficient-Knowledge-Distillation-in-Large-Language-Models" class="headerlink" title="88. Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Less_is_More__Selective_Reflection_for_Compatible_and_Efficient_Knowledge_Distillation_in_Large_Lang.pdf">Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">City University of Hong Kong</span></p><p>本文提出了一种名为Selective Reflection Distillation (SRD)的新型数据筛选与课程调度框架，用于提升大语言模型（LLM）知识蒸馏的效果与效率。方法上，SRD利用学生模型输出对训练样本进行难度评估与筛选，通过ROUGE-L和交叉熵指标进行融合排序，保留高质量、学生兼容的数据，并采用课程学习策略按难度分阶段引入数据以优化训练流程。实验显示，SRD作为即插即用模块，能在多种白盒知识蒸馏方法和模型架构下提升学生模型性能，同时将蒸馏训练的总运行时间最多降低39%。结论：SRD显著提升了蒸馏模型的能力和训练效率，为LLM压缩提供了实用方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Less_is_More_Selective_Reflection_for_Compatible_and_Efficient_Knowledge_Distillation_in_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-PanelTR-Zero-Shot-Table-Reasoning-Framework-Through-Multi-Agent-Scientific-Discussion"><a href="#89-PanelTR-Zero-Shot-Table-Reasoning-Framework-Through-Multi-Agent-Scientific-Discussion" class="headerlink" title="89. PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/PanelTR__Zero-Shot_Table_Reasoning_Framework_Through_Multi-Agent_Scientific_Discussion.pdf">PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Univeristy of Posts and Telecommunications</span></p><p>本文提出PanelTR框架，通过多LLM科学家代理协作，分为个人调查、自我复查和集体同行评议三个阶段，模拟科学方法提升复杂表格推理能力。实验表明，该方法无需特定训练数据即可在多个表格推理任务上超越基础LLM，性能接近甚至超过部分有监督模型，展现出科学方法在复杂推理场景下的有效性和灵活的零样本泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/PanelTR_Zero-Shot_Table_Reasoning_Framework_Through_Multi-Agent_Scientific_Discussion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-AdaptInfer-Adaptive-Token-Pruning-for-Vision–Language-Model-Inference-with-Dynamical-Text-Guidance"><a href="#90-AdaptInfer-Adaptive-Token-Pruning-for-Vision–Language-Model-Inference-with-Dynamical-Text-Guidance" class="headerlink" title="90. AdaptInfer: Adaptive Token Pruning for Vision–Language Model Inference with Dynamical Text Guidance"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/AdaptInfer__Adaptive_Token_Pruning_for_Vision-Language_Model_Inference_with_Dynamical_Text_Guidance.pdf">AdaptInfer: Adaptive Token Pruning for Vision–Language Model Inference with Dynamical Text Guidance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Global Innovation Exchange</span></p><p>AdaptInfer提出了一种适用于视觉-语言模型（VLMs）的动态文本引导视觉token稀疏化推理框架，通过复用层间text-to-text注意力生成文本token重要性先验，并基于此动态加权text-to-vision注意力得分，实现逐层自适应vision token剪枝。此外，论文通过离线分析cross-modal注意力迁移，制定了高效合理的剪枝调度方案。实验显示，在大幅压缩视觉token数量的同时，AdaptInfer能显著降低推理延迟且准确率接近原始模型，在多个基准任务上超越现有SOTA方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/AdaptInfer_Adaptive_Token_Pruning_for_Vision%E2%80%93Language_Model_Inference_with_Dynamical_Text_Guidance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-LLMs-for-Resource-Allocation-A-Participatory-Budgeting-Approach-to-Inferring-Preferences"><a href="#91-LLMs-for-Resource-Allocation-A-Participatory-Budgeting-Approach-to-Inferring-Preferences" class="headerlink" title="91. LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LLMs_for_Resource_Allocation__A_Participatory_Budgeting_Approach_to_Inferring_Preferences.pdf">LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">EPFL</span></p><p>本文提出了一个基于参与式预算（Participatory Budgeting, PB）的机制设计框架，系统评估了大语言模型（LLM）在复杂资源分配任务中的推理与决策能力。方法上，作者将LLMs置于社会规划者角色，通过三种提示策略（贪心选择、直接优化和爬山式精炼），在预算等约束下分配项目，并进一步考察LLMs能否仅凭自然语言或元数据推断投票者偏好。结论显示，LLMs在推理和机制设计中具有潜力，尤其是在无法获得结构化偏好的场景下能有效推断和分配资源，提示其作为动态评测基准的价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LLMs_for_Resource_Allocation_A_Participatory_Budgeting_Approach_to_Inferring_Preferences.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-EvolvR-Self-Evolving-Pairwise-Reasoning-for-Story-Evaluation-to-Enhance-Generation"><a href="#92-EvolvR-Self-Evolving-Pairwise-Reasoning-for-Story-Evaluation-to-Enhance-Generation" class="headerlink" title="92. EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/EvolvR__Self-Evolving_Pairwise_Reasoning_for_Story_Evaluation_to_Enhance_Generation.pdf">EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出了EvolvR，一种基于自进化的成对推理框架，用于提升开源大语言模型在故事评价任务中的推理和评分能力。其方法包括多角色自生成Chain-of-Thought数据、通过多智能体自过滤确保逻辑严谨，并将训练出的评价器作为奖励模型引导故事生成。实验结果显示EvolvR在StoryER、HANNA和OpenMEVA三大基准上达到SOTA，同时显著提升故事生成质量，验证了自进化框架的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/EvolvR_Self-Evolving_Pairwise_Reasoning_for_Story_Evaluation_to_Enhance_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-DP-LLM-Runtime-Model-Adaptation-with-Dynamic-Layer-wise-Precision-Assignment"><a href="#93-DP-LLM-Runtime-Model-Adaptation-with-Dynamic-Layer-wise-Precision-Assignment" class="headerlink" title="93. DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/DP-LLM__Runtime_Model_Adaptation_with_Dynamic_Layer-wise_Precision_Assignment.pdf">DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University</span></p><p>本文提出DP-LLM，一种针对大语言模型（LLM）的动态层级精度分配推理自适应机制。其核心方法是在每一次推理解码时，通过轻量级误差估计器与门控选择器，根据输入动态选择每层的量化比特数，实现更细粒度的性能-延迟权衡。实验表明，该方法在多种模型和数据集上均优于静态分配方案，有效提升了在不同内存和延迟约束下的推理性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/DP-LLM_Runtime_Model_Adaptation_with_Dynamic_Layer-wise_Precision_Assignment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-SDEval-Safety-Dynamic-Evaluation-for-Multimodal-Large-Language-Models"><a href="#94-SDEval-Safety-Dynamic-Evaluation-for-Multimodal-Large-Language-Models" class="headerlink" title="94. SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SDEval__Safety_Dynamic_Evaluation_for_Multimodal_Large_Language_Models.pdf">SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (GZ)</span></p><p>本文提出了SDEval框架，是首个针对多模态大模型(MLLM)的安全动态评估方法，通过三类动态策略（文本、图像、文本-图像）对原有评测基准进行扰动，动态生成新的评估样本，有效提升了测试集复杂度、降低数据泄漏以及暴露MLLM安全风险。实验表明，SDEval能显著降低现有MLLM的安全评分，揭示当前模型难以应对动态安全挑战，凸显了提升模型安全能力的紧迫性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SDEval_Safety_Dynamic_Evaluation_for_Multimodal_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-LLM-Serving-Optimization-with-Variable-Prefill-and-Decode-Lengths"><a href="#95-LLM-Serving-Optimization-with-Variable-Prefill-and-Decode-Lengths" class="headerlink" title="95. LLM Serving Optimization with Variable Prefill and Decode Lengths"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LLM_Serving_Optimization_with_Variable_Prefill_and_Decode_Lengths.pdf">LLM Serving Optimization with Variable Prefill and Decode Lengths</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文研究了大语言模型（LLM）推理服务中的任务调度问题，重点考虑每个请求具有不同的输入（prefill）长度和输出（decode）长度，受限于KV缓存内存约束。作者首先证明该问题在一般情况下为NP-难，并分析了常用调度算法（如FCFS和Shortest-First）在非均匀输入下的性能下界。针对上述挑战，提出了基于新质量指标F(X)的Sorted-F调度算法，并证明其具有常数竞争比；同时开发了动态规划、局部搜索和LP松弛等高效近似算法。大量实验证明，Sorted-F及其变体在实际数据集上优于常用基线，并兼具理论鲁棒性和计算效率。结论表明，Sorted-F算法能显著降低LLM服务端到端延迟，为多样化请求场景下的高效推理提供了理论与实践支持。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LLM_Serving_Optimization_with_Variable_Prefill_and_Decode_Lengths.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-SKATE-a-Scalable-Tournament-Eval-Weaker-LLMs-differentiate-between-stronger-ones-using-verifiable-challenges"><a href="#96-SKATE-a-Scalable-Tournament-Eval-Weaker-LLMs-differentiate-between-stronger-ones-using-verifiable-challenges" class="headerlink" title="96. SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SKATE,_a_Scalable_Tournament_Eval__Weaker_LLMs_differentiate_between_stronger_ones_using_verifiable_.pdf">SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Alan Turing Institute</span></p><p>该论文提出SKATE框架，将大语言模型（LLM）的评估转化为模型间的竞赛，模型既充当任务制定者又充当解答者，通过生成和解决可验证的任务（如代码输出预测），以自动化、无数据依赖和高度可扩展的方式实现评估。实验表明，弱模型可以有效区分强模型，排名稳定，并能够自动发现模型间细微能力差异，验证了该框架的客观性与适应性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SKATE%2C_a_Scalable_Tournament_Eval_Weaker_LLMs_differentiate_between_stronger_ones_using_verifiable_challenges.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-Can-Large-Models-Fool-the-Eye-A-New-Turing-Test-for-Biological-Animation"><a href="#97-Can-Large-Models-Fool-the-Eye-A-New-Turing-Test-for-Biological-Animation" class="headerlink" title="97. Can Large Models Fool the Eye? A New Turing Test for Biological Animation"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Can_Large_Models_Fool_the_Eye__A_New_Turing_Test_for_Biological_Animation.pdf">Can Large Models Fool the Eye? A New Turing Test for Biological Animation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了BioMotion Arena，一个通过视觉生物运动动画进行大模型评估的新框架，专门针对LLMs和多模态大模型（MLLMs）。方法采用点光源动画和人类偏好投票，收集超过45,000次对比投票，利用Elo评分系统动态排名模型。实验发现绝大多数主流模型难以生成合理且逼真的生物动作动画，BioMotion Arena能有效区分模型表现并反映真实用户体验差异。结论：当前大模型对生物运动理解较差，BioMotion Arena能提供直观且可靠的评估方式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Can_Large_Models_Fool_the_Eye_A_New_Turing_Test_for_Biological_Animation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-Fourier-VLM-Compressing-Vision-Tokens-in-the-Frequency-Domain-for-Large-Vision-Language-Models"><a href="#98-Fourier-VLM-Compressing-Vision-Tokens-in-the-Frequency-Domain-for-Large-Vision-Language-Models" class="headerlink" title="98. Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Fourier-VLM__Compressing_Vision_Tokens_in_the_Frequency_Domain_for_Large_Vision-Language_Models.pdf">Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出Fourier-VLM，通过在频率域压缩视觉特征，显著减少Vision-Language Models（VLMs）输入中的视觉token数量。方法核心是无参数的Frequency Feature Compressor（FFC），利用二维离散余弦变换（2d-DCT）提取低频视觉特征，减少高频冗余，同时通过逆DCT恢复压缩后的视觉表示。实验证明，Fourier-VLM在LLaVA和Qwen-VL等主流架构下，在八个图像基准任务上以极低的视觉token保留率（最低仅6.25%）保持96%以上的平均性能，并将计算量（FLOPs）降低至16.16%，推理速度提升31.2%，泛化性和零样本视频理解能力强，适合高效实际部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Fourier-VLM_Compressing_Vision_Tokens_in_the_Frequency_Domain_for_Large_Vision-Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-Efficient-Knowledge-Probing-of-Large-Language-Models-by-Adapting-Pre-trained-Embeddings"><a href="#99-Efficient-Knowledge-Probing-of-Large-Language-Models-by-Adapting-Pre-trained-Embeddings" class="headerlink" title="99. Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Efficient_Knowledge_Probing_of_Large_Language_Models_by_Adapting_Pre-trained_Embeddings.pdf">Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Georgia Institute of Technology</span></p><p>本论文提出PEEK框架，利用经过训练的文本和图结构嵌入模型，通过微调线性层高效预测大语言模型（LLM）是否掌握某一事实，无需对LLM进行大量推理调用。实验证明，该方法在多个数据集和主流LLM上能以高达90%的准确率判定知识掌握情况，揭示嵌入模型可用于高效发现LLM知识盲点，结论是PEEK能在LLM部署前有效识别和弥补事实空白，提升知识评估效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Efficient_Knowledge_Probing_of_Large_Language_Models_by_Adapting_Pre-trained_Embeddings.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future"><a href="#100-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future" class="headerlink" title="100. Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Temporal_Self-Rewarding_Language_Models__Decoupling_Chosen-Rejected_via_Past-Future.pdf">Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本论文提出Temporal Self-Rewarding Language Models，通过引入“Anchored Rejection”（历史模型固定负样本）和“Future-Guided Chosen”（未来模型动态正样本）两个阶段，解决了自奖励大语言模型中正负样本表示收敛导致学习信号消失的问题。实验结果表明，该方法在Llama、Qwen、Mistral等多种模型及AlpacaEval 2.0等主流基准上显著优于标准自奖励方法，同时保持计算资源不变，且具有良好的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Temporal_Self-Rewarding_Language_Models_Decoupling_Chosen-Rejected_via_Past-Future.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="101-Position-Intelligent-Coding-Systems-Should-Write-Programs-with-Justifications"><a href="#101-Position-Intelligent-Coding-Systems-Should-Write-Programs-with-Justifications" class="headerlink" title="101. Position: Intelligent Coding Systems Should Write Programs with Justifications"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Position__Intelligent_Coding_Systems_Should_Write_Programs_with_Justifications.pdf">Position: Intelligent Coding Systems Should Write Programs with Justifications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Purdue University</span></p><p>本文提出智能编码系统不仅应生成代码，还应生成可理解、与代码语义一致的自然语言解释（justifications），以提升非专业用户的理解和信任。作者分析了现有形式化验证、静态分析和后验可解释性方法的局限，提出基于神经-符号一体化的方法，通过在训练阶段引入符号约束并在推理阶段自动验证解释与语义一致性，实现认知对齐与语义忠实的解释生成。结论认为，神经-符号方法能更好地提升代码智能生成系统的可解释性和信赖度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Position_Intelligent_Coding_Systems_Should_Write_Programs_with_Justifications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-MATHREAL-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models"><a href="#102-MATHREAL-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models" class="headerlink" title="102. MATHREAL: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/MathReal__We_Keep_It_Real!_A_Real_Scene_Benchmark_for_Evaluating_Math_Reasoning_in_Multimodal_Large_.pdf">MATHREAL: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Baidu Inc.</span></p><p>该论文提出MATHREAL数据集，收集了2000个真实场景下K-12数学题目图片，覆盖图像质量退化、视角变化、内容干扰等挑战，并细分为14类噪声。通过系统性实验，作者评估了40种主流多模态大模型在六种设定下的数学推理能力，发现现有MLLMs在真实教育场景下表现显著低于在干净数据上的能力。结论表明，多模态大模型在视觉感知、结构化理解和数学推理方面仍面临实际应用瓶颈，需要提升视觉编码和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/MATHREAL_We_Keep_It_Real%21_A_Real_Scene_Benchmark_for_Evaluating_Math_Reasoning_in_Multimodal_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-Optimizing-Prompt-Sequences-using-Monte-Carlo-Tree-Search-for-LLM-Based-Optimization"><a href="#103-Optimizing-Prompt-Sequences-using-Monte-Carlo-Tree-Search-for-LLM-Based-Optimization" class="headerlink" title="103. Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Optimizing_Prompt_Sequences_using_Monte_Carlo_Tree_Search_for_LLM-Based_Optimization.pdf">Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The George Washington University</span></p><p>该论文提出了MCTS-OPS框架，将大语言模型（LLM）与蒙特卡洛树搜索（MCTS）结合，通过将prompt选择建模为序列决策过程，自动优化多步prompt序列以生成高质量、可执行的代码，解决复杂约束优化问题。实验结果表明，MCTS-OPS在网络优化任务中成功率、优化质量和稳定性远超GPT-4、Chain-of-Thought和Self-Refine等主流LLM方法，显著提升了可达最优解的概率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Optimizing_Prompt_Sequences_using_Monte_Carlo_Tree_Search_for_LLM-Based_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-Learning-by-Teaching-Engaging-Students-as-Instructors-of-Large-Language-Models-in-Computer-Science-Education"><a href="#104-Learning-by-Teaching-Engaging-Students-as-Instructors-of-Large-Language-Models-in-Computer-Science-Education" class="headerlink" title="104. Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Learning_by_Teaching__Engaging_Students_as_Instructors_of_Large_Language_Models_in_Computer_Science_.pdf">Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Graduate Center, CUNY</span></p><p>该论文提出了一种反转传统角色的教学范式，要求学生通过精心设计的问题与提示，将LLM作为“学生”加以教学，促使学生构造和表达知识，核心方法包括基于知识空缺的问题设计、链式思维（CoT）、少样本提示（Few-Shot Prompting）以及自洽性验证，并开发了Socrates系统实现低门槛部署。实证结果表明，该方法在本科计算机课程中能显著提升学生作业与项目表现，降低对LLM的依赖，增强主动学习成效。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Learning_by_Teaching_Engaging_Students_as_Instructors_of_Large_Language_Models_in_Computer_Science_Education.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-Crisp-Attention-Regularizing-Transformers-via-Structured-Sparsity"><a href="#105-Crisp-Attention-Regularizing-Transformers-via-Structured-Sparsity" class="headerlink" title="105. Crisp Attention: Regularizing Transformers via Structured Sparsity"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Crisp_Attention__Regularizing_Transformers_via_Structured_Sparsity.pdf">Crisp Attention: Regularizing Transformers via Structured Sparsity</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Joyspace AI</span></p><p>本文提出在Transformer模型自注意力机制中引入结构化稀疏性作为正则化手段，通过对DistilBERT在SST-2情感分析任务的微调实验，采用top-k筛选显著注意力连接，显著提升模型泛化能力和准确率。结果显示，80%稀疏度模型准确率提升0.97%，验证了稀疏不仅提升效率，更能提高Transformer性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Crisp_Attention_Regularizing_Transformers_via_Structured_Sparsity.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents"><a href="#106-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents" class="headerlink" title="106. Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Bifrost-1__Bridging_Multimodal_LLMs_and_Diffusion_Models_with_Patch-level_CLIP_Latents.pdf">Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">UNC Chapel Hill</span></p><p>BIFROST-1提出了一种高效的统一多模态生成与理解框架，通过将预训练多模态大语言模型（MLLM）与扩散模型连接，采用与MLLM原生对齐的patch级CLIP图像embedding作为桥接变量，并利用轻量级Latent ControlNet集成到扩散模型，实现高保真、可控的图像生成，同时显著降低训练成本。实验结果表明，BIFROST-1在图像质量和多模态理解上达到或超越现有方法，且训练计算量显著减少，同时完全保持MLLM的推理和规划能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Bifrost-1_Bridging_Multimodal_LLMs_and_Diffusion_Models_with_Patch-level_CLIP_Latents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="107-eSASRec-Enhancing-Transformer-based-Recommendations-in-a-Modular-Fashion"><a href="#107-eSASRec-Enhancing-Transformer-based-Recommendations-in-a-Modular-Fashion" class="headerlink" title="107. eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/eSASRec__Enhancing_Transformer-based_Recommendations_in_a_Modular_Fashion.pdf">eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MTS</span></p><p>该论文提出了eSASRec，一种基于模块化增强的Transformer推荐模型，通过将SASRec的训练目标、LiGR Transformer层和Sampled Softmax损失函数结合，系统性评估各增强模块的增益。结论显示，eSASRec在工业和学术数据集上超越了当前SOTA模型，可作为强大且易集成的推荐系统基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/eSASRec_Enhancing_Transformer-based_Recommendations_in_a_Modular_Fashion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-Tree-Based-Deep-Learning-for-Ranking-Symbolic-Integration-Algorithms"><a href="#108-Tree-Based-Deep-Learning-for-Ranking-Symbolic-Integration-Algorithms" class="headerlink" title="108. Tree-Based Deep Learning for Ranking Symbolic Integration Algorithms"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Tree-Based_Deep_Learning_for_Ranking_Symbolic_Integration_Algorithms.pdf">Tree-Based Deep Learning for Ranking Symbolic Integration Algorithms</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Coventry University</span></p><p>本文提出了一种基于树结构深度学习模型的两阶段架构，优化计算机代数系统（CAS）中符号不定积分算法的选择。方法包括先识别可用方法，再根据输出表达式复杂度排序，并将数学表达式表示为树结构以提升性能。实验显示，树变换器在独立测试集上优于Maple内置选择器及先前ML方法，准确率接近90%，具有较强泛化能力。结论：树结构表示和两阶段排名框架对数学软件算法选择优化至关重要。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Tree-Based_Deep_Learning_for_Ranking_Symbolic_Integration_Algorithms.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-ActivityDiff-A-diffusion-model-with-Positive-and-Negative-Activity-Guidance-for-De-Novo-Drug-Design"><a href="#109-ActivityDiff-A-diffusion-model-with-Positive-and-Negative-Activity-Guidance-for-De-Novo-Drug-Design" class="headerlink" title="109. ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ActivityDiff__A_diffusion_model_with_Positive_and_Negative_Activity_Guidance_for_De_Novo_Drug_Design.pdf">ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science and Engineering, Central South University</span></p><p>本文提出了ActivityDiff，一种结合正向和负向分类器引导的离散扩散分子生成模型，能够实现药物分子对多靶点活性和选择性的精确调控。该方法通过将正向（靶向活性提升）和负向（抑制脱靶效应）指导引入扩散过程，支持单&#x2F;多靶标、片段约束、特异性增强等多种药物设计任务。实验结果表明，ActivityDiff在分子新颖性、有效性、特异性和降低脱靶风险方面均优于现有方法，实现了药物设计中活性与安全性的平衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ActivityDiff_A_diffusion_model_with_Positive_and_Negative_Activity_Guidance_for_De_Novo_Drug_Design.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="110-AntiCheatPT-A-Transformer-Based-Approach-to-Cheat-Detection-in-Competitive-Computer-Games"><a href="#110-AntiCheatPT-A-Transformer-Based-Approach-to-Cheat-Detection-in-Competitive-Computer-Games" class="headerlink" title="110. AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games"></a>110. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/AntiCheatPT__A_Transformer-Based_Approach_to_Cheat_Detection_in_Competitive_Computer_Games.pdf">AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The IT-University of Copenhagen</span></p><p>本文提出了AntiCheatPT 256，一种基于Transformer编码器的机器学习模型，通过对Counter-Strike 2游戏行为数据进行多变量时序建模，实现作弊检测，并开源了手工标注的CS2CD数据集（795场比赛）。实验结果显示该模型在未增强测试集上准确率达89.17%、AUC达93.36%，验证了基于服务器端行为数据的可复现作弊检测方法的有效性，降低了对侵入式反作弊手段的依赖。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/AntiCheatPT_A_Transformer-Based_Approach_to_Cheat_Detection_in_Competitive_Computer_Games.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="111-Low-Bit-Data-Processing-Using-Multiple-Output-Spiking-Neurons-with-Non-linear-Reset-Feedback"><a href="#111-Low-Bit-Data-Processing-Using-Multiple-Output-Spiking-Neurons-with-Non-linear-Reset-Feedback" class="headerlink" title="111. Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback"></a>111. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Low-Bit_Data_Processing_Using_Multiple-Output_Spiking_Neurons_with_Non-linear_Reset_Feedback.pdf">Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Uppsala University</span></p><p>该论文提出了一种多输出、带有非线性重置反馈的新型脉冲神经元模型，将线性状态空间模型（SSM）与可学习的非线性重置机制结合，实现低比特脉冲信号处理。作者通过在多模态任务（语音关键字识别、事件视觉和序列模式识别）上的实验，表明该模型在不强制线性稳定性约束下，能获得与主流SNN基线相当的性能，并解决不稳定线性动力学下的学习收敛问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Low-Bit_Data_Processing_Using_Multiple-Output_Spiking_Neurons_with_Non-linear_Reset_Feedback.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="112-Multi-Omics-Analysis-for-Cancer-Subtype-Inference-via-Unrolling-Graph-Smoothness-Priors"><a href="#112-Multi-Omics-Analysis-for-Cancer-Subtype-Inference-via-Unrolling-Graph-Smoothness-Priors" class="headerlink" title="112. Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors"></a>112. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Multi-Omics_Analysis_for_Cancer_Subtype_Inference_via_Unrolling_Graph_Smoothness_Priors.pdf">Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了GTMancer框架，通过对多组组学数据进行对比学习对齐后，采用图神经网络优化方法联合捕捉组学间和样本间的结构信息，并引入双重注意力机制提升癌症亚型分类性能。理论上证明了优化过程的收敛性，并通过牛顿法自动调整步长，实验结果在七个真实癌症数据集上超越现有方法，提升分类准确率与泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Multi-Omics_Analysis_for_Cancer_Subtype_Inference_via_Unrolling_Graph_Smoothness_Priors.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="113-One-Size-Does-Not-Fit-All-A-Distribution-Aware-Sparsification-for-More-Precise-Model-Merging"><a href="#113-One-Size-Does-Not-Fit-All-A-Distribution-Aware-Sparsification-for-More-Precise-Model-Merging" class="headerlink" title="113. One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging"></a>113. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/One_Size_Does_Not_Fit_All__A_Distribution-Aware_Sparsification_for_More_Precise_Model_Merging.pdf">One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeastern University</span></p><p>本文提出了TADrop（Tensor-wise Adaptive Drop），一种基于参数分布自适应的稀疏化策略，用于提升多任务模型融合的精度。TADrop根据每个参数张量的分布特性动态分配稀疏率，实现细粒度、结构感知的稀疏化，并通过归一化保持张量范数。实验表明，TADrop可无缝集成至多种模型融合方案，在视觉、语言及多模态任务上均显著提升融合模型性能，尤其在任务数量增多时表现出更强的鲁棒性和泛化性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/One_Size_Does_Not_Fit_All_A_Distribution-Aware_Sparsification_for_More_Precise_Model_Merging.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="114-Semantic-Item-Graph-Enhancement-for-Multimodal-Recommendation"><a href="#114-Semantic-Item-Graph-Enhancement-for-Multimodal-Recommendation" class="headerlink" title="114. Semantic Item Graph Enhancement for Multimodal Recommendation"></a>114. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Semantic_Item_Graph_Enhancement_for_Multimodal_Recommendation.pdf">Semantic Item Graph Enhancement for Multimodal Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>本文提出SIGER框架，将用户-物品交互图中的协同信号注入到基于模态的物品-物品语义图，构建增强型语义图以丰富物品语义建模。通过模数化个性化嵌入扰动机制生成对比视图，结合对比学习实现抗噪表征，并设计双重表征对齐机制，提升行为与语义表征的一致性。实验表明SIGER在多种推荐场景均显著优于现有方法，尤其在冷启动情境下优势突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Semantic_Item_Graph_Enhancement_for_Multimodal_Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="115-Enhancing-the-Scalability-of-Classical-Surrogates-for-Real-World-Quantum-Machine-Learning-Applications"><a href="#115-Enhancing-the-Scalability-of-Classical-Surrogates-for-Real-World-Quantum-Machine-Learning-Applications" class="headerlink" title="115. Enhancing the Scalability of Classical Surrogates for Real-World Quantum Machine Learning Applications"></a>115. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Enhancing_the_Scalability_of_Classical_Surrogates_for_Real-World_Quantum_Machine_Learning_Applicatio.pdf">Enhancing the Scalability of Classical Surrogates for Real-World Quantum Machine Learning Applications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">E.ON Digital Technology GmbH</span></p><p>本文提出了一种高效替代流程，利用实际训练数据和随机傅里叶频率采样，极大降低了从量子模型生成经典代理模型的计算资源需求，实现了对更大规模量子模型的经典近似。实验证明，该方法在能源需求预测任务中，可在资源线性扩展的情况下获得与量子模型高度一致的预测精度，显著提升了QML实际工业部署的可行性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Enhancing_the_Scalability_of_Classical_Surrogates_for_Real-World_Quantum_Machine_Learning_Applications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="116-MeanAudio-Fast-and-Faithful-Text-to-Audio-Generation-with-Mean-Flows"><a href="#116-MeanAudio-Fast-and-Faithful-Text-to-Audio-Generation-with-Mean-Flows" class="headerlink" title="116. MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows"></a>116. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/MeanAudio__Fast_and_Faithful_Text-to-Audio_Generation_with_Mean_Flows.pdf">MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出MeanAudio，一种基于MeanFlow的高效文本到音频生成模型，利用Flux-style流变换器在训练中回归平均速度场，通过直接从流程轨迹起点到终点实现快速音频合成。引入分类器无关引导和瞬时到平均流的训练策略，极大提升了训练稳定性和生成质量。实验结果显示MeanAudio在单步和多步生成均达SOTA性能，推理速度比主流扩散方法快一百倍，参数量更低。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/MeanAudio_Fast_and_Faithful_Text-to-Audio_Generation_with_Mean_Flows.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="117-Architecture-Aware-Generalization-Bounds-for-Temporal-Networks-Theory-and-Fair-Comparison-Methodology"><a href="#117-Architecture-Aware-Generalization-Bounds-for-Temporal-Networks-Theory-and-Fair-Comparison-Methodology" class="headerlink" title="117. Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology"></a>117. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Architecture-Aware_Generalization_Bounds_for_Temporal_Networks__Theory_and_Fair_Comparison_Methodolo.pdf">Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technion</span></p><p>本文提出了针对深度时序网络（如TCN）的首个架构相关、非虚无的泛化界理论，并引入了公平对比评估方法。核心方法包括延迟反馈阻断机制将相关样本转化为近似独立样本，推导出泛化界随网络深度D呈√D缩放，并通过固定有效样本量的实验揭示强依赖序列在相同信息量下能获得显著更小的泛化间隔，挑战了依赖性只会带来负面影响的传统认识。结论：理论界为网络架构选择提供量化指导，实验证明时序依赖有助于提升泛化性能，但理论与实践间仍有差异，公平对比方法能更准确揭示依赖结构对学习的影响。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Architecture-Aware_Generalization_Bounds_for_Temporal_Networks_Theory_and_Fair_Comparison_Methodology.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="118-Do-Ethical-AI-Principles-Matter-to-Users-A-Large-Scale-Analysis-of-User-Sentiment-and-Satisfaction"><a href="#118-Do-Ethical-AI-Principles-Matter-to-Users-A-Large-Scale-Analysis-of-User-Sentiment-and-Satisfaction" class="headerlink" title="118. Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction"></a>118. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Do_Ethical_AI_Principles_Matter_to_Users__A_Large-Scale_Analysis_of_User_Sentiment_and_Satisfaction.pdf">Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hankuk University of Foreign Studies</span></p><p>本研究通过收集G2.com上10万余条AI产品用户评论，基于欧盟可信赖AI伦理准则七大维度，采用零样本分类与RoBERTa-Large等Transformer模型，量化评估伦理AI原则与用户满意度的关联。结论显示，所有伦理维度（如透明度、公平性、责任）均与用户满意度正相关，且非技术用户和终端应用场景下这种关联更强，凸显伦理AI设计需关注用户情境差异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Do_Ethical_AI_Principles_Matter_to_Users_A_Large-Scale_Analysis_of_User_Sentiment_and_Satisfaction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="119-Fast-Convex-and-Conditioned-Network-for-Multi-Fidelity-Vectors-and-Stiff-Univariate-Differential-Equations"><a href="#119-Fast-Convex-and-Conditioned-Network-for-Multi-Fidelity-Vectors-and-Stiff-Univariate-Differential-Equations" class="headerlink" title="119. Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations"></a>119. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Fast,_Convex_and_Conditioned_Network_for_Multi-Fidelity_Vectors_and_Stiff_Univariate_Differential_Eq.pdf">Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Applied Mathematics, University of British Columbia</span></p><p>本文提出了一种结合极限学习机（ELM）和移位高斯编码的神经网络架构，用于解决多保真度数据和刚性微分方程中的优化病态问题。通过在ELM中引入移位高斯编码，有效提升了激活矩阵的秩和稀疏结构，实现了更优的矩阵条件数，显著提升了对刚性PDE、多尺度模式和高维向量（如图像）的拟合精度和效率。结论指出，相较于增加网络深度，改善激活矩阵的条件性对科学计算中的神经网络收敛与表达能力更为关键。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Fast%2C_Convex_and_Conditioned_Network_for_Multi-Fidelity_Vectors_and_Stiff_Univariate_Differential_Equations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025/08/11报纸</title>
    <link href="/2025/08/2025-08-11_article/"/>
    <url>/2025/08/2025-08-11_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2025/08/11报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-11<br>📄 <strong>发现论文数量</strong>：149  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency"><a href="#1-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency" class="headerlink" title="1. Test-Time Reinforcement Learning for GUI Grounding via Region Consistency"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Test-Time_Reinforcement_Learning_for_GUI_Grounding_via_Region_Consistency.pdf">Test-Time Reinforcement Learning for GUI Grounding via Region Consistency</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出了GUI-RC（Region Consistency）与GUI-RCPO（Region Consistency Policy Optimization）两种测试时优化方法，用于提升GUI grounding任务（自然语言到界面坐标映射）的准确率。GUI-RC通过多次采样模型输出并空间投票得到一致性区域，无需额外标注即可提升模型定位能力；GUI-RCPO则将区域一致性转化为奖励信号，在测试时进行自监督强化学习训练，使模型在无标注数据上自我优化。实验表明方法在多个主流模型和基准上平均提升2-5%的准确率，并可自举式迭代提升，展现了测试时优化的新潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Test-Time_Reinforcement_Learning_for_GUI_Grounding_via_Region_Consistency.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Discovering-Interpretable-Programmatic-Policies-via-Multimodal-LLM-assisted-Evolutionary-Search"><a href="#2-Discovering-Interpretable-Programmatic-Policies-via-Multimodal-LLM-assisted-Evolutionary-Search" class="headerlink" title="2. Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Discovering_Interpretable_Programmatic_Policies_via_Multimodal_LLM-assisted_Evolutionary_Search.pdf">Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">City University of Hong Kong</span></p><p>本文提出了一种新的可解释程序化策略发现方法——多模态大语言模型辅助进化搜索（MLES），该方法将多模态大语言模型（MLLMs）与进化优化结合，直接生成带有自然语言解释的程序化策略。MLES在进化过程中融合了视觉反馈驱动的行为分析，有效提升了策略发现效率和可解释性。实验证明，MLES在Lunar Lander和Car Racing两个典型控制任务上，策略性能与PPO相当，并显著提升了策略的透明度和知识可复用性，为下一代可解释策略发现提供了新范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Discovering_Interpretable_Programmatic_Policies_via_Multimodal_LLM-assisted_Evolutionary_Search.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-ECHO-Decoupling-Inference-and-Training-for-Large-Scale-RL-Alignment-on-Heterogeneous-Swarms"><a href="#3-ECHO-Decoupling-Inference-and-Training-for-Large-Scale-RL-Alignment-on-Heterogeneous-Swarms" class="headerlink" title="3. ECHO: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Echo__Decoupling_Inference_and_Training_for_Large-Scale_RL_Alignment_on_Heterogeneous_Swarms.pdf">ECHO: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Gradient</span></p><p>本文提出了ECHO系统，通过将RL中的推理（trajectory sampling）和训练（policy optimization）在异构集群上彻底解耦，设计了顺序拉取和异步推送两种轻量级同步协议，实现了在边缘设备和数据中心硬件之间灵活协作。实验表明，ECHO在Qwen系列大模型上的RL任务中，既能提升硬件利用率，也能与传统全同位系统在收敛速度和最终性能持平甚至超越，验证了去中心化RL训练的可行性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/ECHO_Decoupling_Inference_and_Training_for_Large-Scale_RL_Alignment_on_Heterogeneous_Swarms.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation"><a href="#4-Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation" class="headerlink" title="4. Posterior-GRPO: Rewarding Reasoning Processes in Code Generation"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Posterior-GRPO__Rewarding_Reasoning_Processes_in_Code_Generation.pdf">Posterior-GRPO: Rewarding Reasoning Processes in Code Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The State Key Laboratory of Blockchain and Data Security</span></p><p>本论文提出了一种新的强化学习方法Posterior-GRPO (P-GRPO)用于大语言模型代码生成，创新性地将基于优化-降级（OD-based）方法训练的思维奖励模型与传统结果奖励结合，通过在仅有正确结果时赋予思维奖励，有效防止奖励投机。实验证明，P-GRPO在多个代码与数学推理基准上提升Pass@1平均4.5%，表现与GPT-4-Turbo相当，且泛化能力强。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Posterior-GRPO_Rewarding_Reasoning_Processes_in_Code_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-Domain-driven-Metrics-for-Reinforcement-Learning-A-Case-Study-on-Epidemic-Control-using-Agent-based-Simulation"><a href="#5-Domain-driven-Metrics-for-Reinforcement-Learning-A-Case-Study-on-Epidemic-Control-using-Agent-based-Simulation" class="headerlink" title="5. Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Domain-driven_Metrics_for_Reinforcement_Learning__A_Case_Study_on_Epidemic_Control_using_Agent-based.pdf">Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Thoughtworks Technologies</span></p><p>本文提出了结合领域知识和传统奖励的‘Domain-driven RL metrics’评价指标体系，用于公正、稳健地比较用于理性体建模与优化的强化学习算法。通过在疫情控制的代理建模案例中，利用多种RL算法和新的复合评价指标（如状态空间覆盖、最佳序列百分比等），有效提升了算法选择的可靠性和解释性，结论显示新指标体系比单一均值奖励更稳健、更具信任度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Domain-driven_Metrics_for_Reinforcement_Learning_A_Case_Study_on_Epidemic_Control_using_Agent-based_Simulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-Towards-Hallucination-Free-Music-A-Reinforcement-Learning-Preference-Optimization-Framework-for-Reliable-Song-Generation"><a href="#6-Towards-Hallucination-Free-Music-A-Reinforcement-Learning-Preference-Optimization-Framework-for-Reliable-Song-Generation" class="headerlink" title="6. Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Hallucination-Free_Music__A_Reinforcement_Learning_Preference_Optimization_Framework_for_Rel.pdf">Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>本论文提出了一个基于强化学习的偏好优化框架，用于减少歌词到歌曲生成过程中的幻觉现象（即生成内容与歌词不符）。方法包括构建基于音素错误率（PER）的幻觉偏好数据集，并实现三种RL偏好优化策略：DPO、PPO和GRPO，通过奖励建模和对比学习，有效抑制了幻觉的发生。实验表明，该框架能显著降低幻觉比例（如DPO+RS方法幻觉样本下降16.5%，高质量样本提升38.47%），且不损失音乐性和音质，具备良好迁移性，可扩展到风格控制等其他生成任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Hallucination-Free_Music_A_Reinforcement_Learning_Preference_Optimization_Framework_for_Reliable_Song_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Anti-Jamming-Sensing-with-Distributed-Reconfigurable-Intelligent-Metasurface-Antennas"><a href="#7-Anti-Jamming-Sensing-with-Distributed-Reconfigurable-Intelligent-Metasurface-Antennas" class="headerlink" title="7. Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Anti-Jamming_Sensing_with_Distributed_Reconfigurable_Intelligent_Metasurface_Antennas.pdf">Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>本文提出利用分布式可重构智能超表面天线（RIMSA）进行射频感知，通过策略网络（深度强化学习）优化波束形成模式及神经网络映射接收信号，实现端到端感知。为抵抗干扰，设计了融合接收信号SINR的多因子损失函数，显著提升了抗干扰下的感知精度，仿真表明分布式RIMSA系统对环境及攻击有更强适应性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Anti-Jamming_Sensing_with_Distributed_Reconfigurable_Intelligent_Metasurface_Antennas.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation"><a href="#8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation" class="headerlink" title="8. Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Genie_Envisioner__A_Unified_World_Foundation_Platform_for_Robotic_Manipulation.pdf">Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AgiBot Genie Team</span></p><p>该论文提出Genie Envisioner (GE)，一个集视频生成、策略学习、评估和仿真于一体的机器人操作世界基础平台。GE包括三大模块：GE-Base为大规模指令条件视频扩散模型，捕捉真实机器人交互的空间、时间和语义动态；GE-Act通过轻量化解码器将视觉表征映射为可执行动作轨迹，实现多平台泛化和高效策略推断；GE-Sim为动作条件神经模拟器，支持高保真闭环策略评估与并行仿真。平台还配备EWMBench标准化评测工具，综合衡量视觉真实度、物理一致性和指令-动作对齐。实验证明GE在多种机器人平台和复杂任务上均优于现有方法，具备极强的通用性和可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Genie_Envisioner_A_Unified_World_Foundation_Platform_for_Robotic_Manipulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-TRAJEVO-Trajectory-Prediction-Heuristics-Design-via-LLM-driven-Evolution"><a href="#9-TRAJEVO-Trajectory-Prediction-Heuristics-Design-via-LLM-driven-Evolution" class="headerlink" title="9. TRAJEVO: Trajectory Prediction Heuristics Design via LLM-driven Evolution"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/TrajEvo__Trajectory_Prediction_Heuristics_Design_via_LLM-driven_Evolution.pdf">TRAJEVO: Trajectory Prediction Heuristics Design via LLM-driven Evolution</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KAIST, Korea Advanced Institute of Science and Technology</span></p><p>该论文提出TRAJEVO框架，通过结合大语言模型（LLM）与进化算法自动生成和优化轨迹预测启发式方法，利用Cross-Generation Elite Sampling和Statistics Feedback Loop增强启发式多样性与性能。实验结果显示，TRAJEVO在多数据集上超越传统启发式方法，并且在未见分布场景下的泛化能力优于深度学习模型，同时具备高速度和可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/TRAJEVO_Trajectory_Prediction_Heuristics_Design_via_LLM-driven_Evolution.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-OMNIEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks"><a href="#10-OMNIEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks" class="headerlink" title="10. OMNIEAR: Benchmarking Agent Reasoning in Embodied Tasks"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/OmniEAR__Benchmarking_Agent_Reasoning_in_Embodied_Tasks.pdf">OMNIEAR: Benchmarking Agent Reasoning in Embodied Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>OmniEAR提出了一个用于评估大语言模型在具身任务中推理能力的综合框架，包括EAR-Sim环境建模、自动化场景生成和EAR-Bench系统性评测，覆盖物理属性推理、工具使用与多智能体协作。结论显示：现有语言模型在需要从物理约束推理时性能严重下降，特别是在工具推理和隐式协作任务中，表明具身推理对AI架构提出了新的挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/OMNIEAR_Benchmarking_Agent_Reasoning_in_Embodied_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-CleanUpBench-Embodied-Sweeping-and-Grasping-Benchmark"><a href="#11-CleanUpBench-Embodied-Sweeping-and-Grasping-Benchmark" class="headerlink" title="11. CleanUpBench: Embodied Sweeping and Grasping Benchmark"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/CleanUpBench__Embodied_Sweeping_and_Grasping_Benchmark.pdf">CleanUpBench: Embodied Sweeping and Grasping Benchmark</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sichuan University</span></p><p>本文提出了CleanUpBench，一个面向现实家居环境中服务机器人双模态（扫地与抓取）清洁任务的高保真仿真基准。平台基于NVIDIA Isaac Sim，支持多场景、多机器人协作，并提供系统化评测指标（空间覆盖、任务完成率、运动质量、计算效率等），并内置多种启发式、规划与学习基线方法。实验表明，具备多智能体强化学习能力的双模态算法（如PRIMAL2）在复杂清洁任务中显著优于仅具备单一模式的传统方法，验证了CleanUpBench作为评测和推动现实服务机器人智能发展的价值和扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/CleanUpBench_Embodied_Sweeping_and_Grasping_Benchmark.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-Towards-Generalizable-Safety-in-Crowd-Navigation-via-Conformal-Uncertainty-Handling"><a href="#12-Towards-Generalizable-Safety-in-Crowd-Navigation-via-Conformal-Uncertainty-Handling" class="headerlink" title="12. Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Generalizable_Safety_in_Crowd_Navigation_via_Conformal_Uncertainty_Handling.pdf">Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Riverside</span></p><p>本文提出了一种结合自适应保形不确定性量化(ACI)与约束强化学习(CRL)的移动机器人群体导航方法。方法流程为：首先通过规则或学习式轨迹预测模型估算人类未来轨迹，并用ACI在线量化预测不确定性，将不确定性估计作为观察输入，结合注意力机制编码人与机器人及人-人交互，最后用CRL约束机器人决策以减少对人类未来轨迹的不安全侵入。实验结果显示，在分布内及三种分布外场景（速度变化、行为策略变化、群体动态）中，该方法安全性和成功率均显著优于现有SOTA基线，且能够直接迁移到实际机器人平台实现安全稳健导航。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Generalizable_Safety_in_Crowd_Navigation_via_Conformal_Uncertainty_Handling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-A-Multi-view-Landmark-Representation-Approach-with-Application-to-GNSS-Visual-Inertial-Odometry"><a href="#13-A-Multi-view-Landmark-Representation-Approach-with-Application-to-GNSS-Visual-Inertial-Odometry" class="headerlink" title="13. A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Multi-view_Landmark_Representation_Approach_with_Application_to_GNSS-Visual-Inertial_Odometry.pdf">A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了一种多视角的仅位姿特征点表示方法，并将其应用于GNSS-视觉-惯性里程计（GVIO）系统，核心技术包括多视角几何特征建模、滤波更新与特征管理策略。实验表明该方法在仿真和真实环境中提升了定位效率和精度，保持滤波一致性且计算负担更低。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Multi-view_Landmark_Representation_Approach_with_Application_to_GNSS-Visual-Inertial_Odometry.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Information-Theoretic-Graph-Fusion-with-Vision-Language-Action-Model-for-Policy-Reasoning-and-Dual-Robotic-Control"><a href="#14-Information-Theoretic-Graph-Fusion-with-Vision-Language-Action-Model-for-Policy-Reasoning-and-Dual-Robotic-Control" class="headerlink" title="14. Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Information-Theoretic_Graph_Fusion_with_Vision-Language-Action_Model_for_Policy_Reasoning_and_Dual_R.pdf">Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hangzhou Dianzi University</span></p><p>本文提出了一种信息论图融合视觉-语言-动作（GF-VLA）框架，通过从人类演示视频中提取基于Shannon信息的手-物体和物体-物体交互，构建时序场景图，并与语言条件Transformer结合，生成可解释的行为树和笛卡尔动作指令。通过链式推理（CoT）、自验证和跨手选择策略实现双臂机器人任务推理与高效协作，实验在多种装配和泛化任务中取得了高于95%图准确率和90%任务成功率，展现了优异的泛化与稳健性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Information-Theoretic_Graph_Fusion_with_Vision-Language-Action_Model_for_Policy_Reasoning_and_Dual_Robotic_Control.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-DistillDrive-End-to-End-Multi-Mode-Autonomous-Driving-Distillation-by-Isomorphic-Hetero-Source-Planning-Model"><a href="#15-DistillDrive-End-to-End-Multi-Mode-Autonomous-Driving-Distillation-by-Isomorphic-Hetero-Source-Planning-Model" class="headerlink" title="15. DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/DistillDrive__End-to-End_Multi-Mode_Autonomous_Driving_Distillation_by_Isomorphic_Hetero-Source_Plan.pdf">DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">East China University of Science and Technology</span></p><p>该论文提出DistillDrive，一种结合知识蒸馏、强化学习和生成模型的端到端多模态自动驾驶框架。方法以结构化场景规划模型为教师，通过多模实例监督提升学生模型的运动特征学习，并引入强化学习优化状态-决策映射，利用生成模型实现运动分布特征的隐空间交互。实验结果显示，DistillDrive在nuScenes和NAVSIM数据集上较基线模型碰撞率下降50%，闭环性能提升3分，有效增强了自动驾驶计划的多样性和安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/DistillDrive_End-to-End_Multi-Mode_Autonomous_Driving_Distillation_by_Isomorphic_Hetero-Source_Planning_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-Towards-Embodied-Agentic-AI-Review-and-Classification-of-LLM-and-VLM-Driven-Robot-Autonomy-and-Interaction"><a href="#16-Towards-Embodied-Agentic-AI-Review-and-Classification-of-LLM-and-VLM-Driven-Robot-Autonomy-and-Interaction" class="headerlink" title="16. Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Embodied_Agentic_AI__Review_and_Classification_of_LLM-_and_VLM-Driven_Robot_Autonomy_and_Int.pdf">Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Turku</span></p><p>该论文综述了大语言模型（LLM）和视觉语言模型（VLM）在机器人自主性和人机交互中的集成方法，提出了四类集成范式（协议、接口、编排和嵌入）及多种智能体角色的分类体系。结论认为，Agentic AI系统作为用户与机器人间的智能中介，提升了机器人灵活性和可扩展性，并展望了未来在实际部署、记忆、安全性和高阶智能等方面的研究挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Embodied_Agentic_AI_Review_and_Classification_of_LLM-_and_VLM-Driven_Robot_Autonomy_and_Interaction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-Driver-Assistant-Persuading-Drivers-to-Adjust-Secondary-Tasks-Using-Large-Language-Models"><a href="#17-Driver-Assistant-Persuading-Drivers-to-Adjust-Secondary-Tasks-Using-Large-Language-Models" class="headerlink" title="17. Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Driver_Assistant__Persuading_Drivers_to_Adjust_Secondary_Tasks_Using_Large_Language_Models.pdf">Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出一种基于大语言模型（LLM）的驾驶辅助工具，通过道路风险评估与眼动追踪及时判断并生成人性化劝导内容，引导驾驶员在L3自动驾驶中合理分配注意力。实验结果显示，该系统能够有效降低驾驶员分心任务数量和认知负荷，提升驾驶安全性及体验。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Driver_Assistant_Persuading_Drivers_to_Adjust_Secondary_Tasks_Using_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-EndoMatcher-Generalizable-Endoscopic-Image-Matcher-via-Multi-Domain-Pre-training-for-Robot-Assisted-Surgery"><a href="#18-EndoMatcher-Generalizable-Endoscopic-Image-Matcher-via-Multi-Domain-Pre-training-for-Robot-Assisted-Surgery" class="headerlink" title="18. EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EndoMatcher__Generalizable_Endoscopic_Image_Matcher_via_Multi-Domain_Pre-training_for_Robot-Assisted.pdf">EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Automation, Chinese Academy of Sciences</span></p><p>该论文提出了EndoMatcher，一种基于双分支Vision Transformer的通用内镜图像匹配网络，通过多域大规模数据集Endo-Mix6的预训练和渐进式多目标优化实现对弱纹理、视角大变化等挑战场景下的鲁棒密集匹配。结果显示，EndoMatcher在多个零样本内镜数据集上大幅提升匹配数量和精度，实现了跨器官和成像条件的高效泛化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EndoMatcher_Generalizable_Endoscopic_Image_Matcher_via_Multi-Domain_Pre-training_for_Robot-Assisted_Surgery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-GhostShell-Streaming-LLM-Function-Calls-for-Concurrent-Embodied-Programming"><a href="#19-GhostShell-Streaming-LLM-Function-Calls-for-Concurrent-Embodied-Programming" class="headerlink" title="19. GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/GhostShell__Streaming_LLM_Function_Calls_for_Concurrent_Embodied_Programming.pdf">GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Leapwatt Robotics</span></p><p>本文提出GhostShell，一种利用大语言模型（LLM）实现机器人多通道并发行为编程的系统，核心包括流式XML函数令牌解析、动态接口映射和多通道同步&#x2F;异步调度，支持机器人各部件的并行与串行指令执行。实验在COCO机器人上，通过34项任务验证方法有效性，在Claude-4-Sonnet模型上达到0.85的行为正确率，响应速度比传统函数调用快66倍，适用于长时多模态任务，展现出高度通用性和实时性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/GhostShell_Streaming_LLM_Function_Calls_for_Concurrent_Embodied_Programming.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-Advanced-Hybrid-Transformer–LSTM-Technique-with-Attention-and-TS-Mixer-for-Drilling-Rate-of-Penetration-Prediction"><a href="#20-Advanced-Hybrid-Transformer–LSTM-Technique-with-Attention-and-TS-Mixer-for-Drilling-Rate-of-Penetration-Prediction" class="headerlink" title="20. Advanced Hybrid Transformer–LSTM Technique with Attention and TS-Mixer for Drilling Rate of Penetration Prediction"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Advanced_Hybrid_Transformer_LSTM_Technique_with_Attention_and_TS_Mixer_for_Drilling_Rate_of_Penetrat.pdf">Advanced Hybrid Transformer–LSTM Technique with Attention and TS-Mixer for Drilling Rate of Penetration Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Engineering and Applied Sciences (UEAS), Swat, Pakistan</span></p><p>该论文提出一种结合LSTM、Transformer编码器、TS-Mixer模块和注意力机制的混合深度学习架构，用于钻井穿透率（ROP）预测。方法包括数据清洗、特征标准化、架构设计（LSTM捕捉时序依赖，Transformer建模全局关系，TS-Mixer融合静态特征，注意力机制动态分配特征权重），并在真实钻井数据集上进行评估。结果显示，所提模型在多项回归指标上显著优于传统方法，R²达0.9991，MAPE低至1.1572%，具备强泛化能力和实时预测价值。结论是该混合模型能稳定并精确预测ROP，显著提升钻井优化系统的智能化与经济性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Advanced_Hybrid_Transformer%E2%80%93LSTM_Technique_with_Attention_and_TS-Mixer_for_Drilling_Rate_of_Penetration_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-Learning-to-See-and-Act-Task-Aware-View-Planning-for-Robotic-Manipulation"><a href="#21-Learning-to-See-and-Act-Task-Aware-View-Planning-for-Robotic-Manipulation" class="headerlink" title="21. Learning to See and Act: Task-Aware View Planning for Robotic Manipulation"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Learning_to_See_and_Act__Task-Aware_View_Planning_for_Robotic_Manipulation.pdf">Learning to See and Act: Task-Aware View Planning for Robotic Manipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science and Engineering, Sun Yat-sen University</span></p><p>本文提出了Task-Aware View Planning (TAVP) 框架，将主动多视角探索与任务感知特征提取相结合，提升机器人多任务操作的鲁棒性与泛化能力。通过多视角探索策略和任务感知专家混合（TaskMoE）视觉编码器，TAVP显著提升了在RLBench等多任务操作环境中的准确率和鲁棒性，能有效克服遮挡问题并提升对新任务的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Learning_to_See_and_Act_Task-Aware_View_Planning_for_Robotic_Manipulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-PhysPatch-A-Physically-Realizable-and-Transferable-Adversarial-Patch-Attack-for-Multimodal-Large-Language-Models-based-Autonomous-Driving-Systems"><a href="#22-PhysPatch-A-Physically-Realizable-and-Transferable-Adversarial-Patch-Attack-for-Multimodal-Large-Language-Models-based-Autonomous-Driving-Systems" class="headerlink" title="22. PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/PhysPatch__A_Physically_Realizable_and_Transferable_Adversarial_Patch_Attack_for_Multimodal_Large_La.pdf">PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Software Engineering, Xi’an Jiaotong University</span></p><p>本文提出PhysPatch，一种针对多模态大语言模型(MLLMs)驱动的自动驾驶系统的物理可实现且高迁移性对抗补丁攻击框架。方法上，PhysPatch通过语义感知的掩码初始化、SVD分解的局部特征对齐损失以及补丁引导的裁剪-缩放策略，实现了补丁位置、形状和内容的联合优化，并采用自适应势场掩码细化提升物理可部署性。实验证明，PhysPatch在多种开源、商用和推理型MLLMs上均大幅优于现有方法，能有效误导自动驾驶系统感知与规划输出，并兼具高物理可部署性与对抗性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/PhysPatch_A_Physically_Realizable_and_Transferable_Adversarial_Patch_Attack_for_Multimodal_Large_Language_Models-based_Autonomous_Driving_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-Examining-the-legibility-of-humanoid-robot-arm-movements-in-a-pointing-task"><a href="#23-Examining-the-legibility-of-humanoid-robot-arm-movements-in-a-pointing-task" class="headerlink" title="23. Examining the legibility of humanoid robot arm movements in a pointing task"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Examining_the_legibility_of_humanoid_robot_arm_movements_in_a_pointing_task.pdf">Examining the legibility of humanoid robot arm movements in a pointing task</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Comenius University Bratislava</span></p><p>该论文设计了基于NICO仿人机器人，结合凝视与指向手势的实验，考察人在观察截断机器人手臂运动时对目标预测的准确性和反应速度。结果支持多模态优势和眼动优先假设，表明凝视和指向的组合能显著提高意图识别的准确性，凝视信号可加速人类决策，为人机交互中运动设计提供理论依据。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Examining_the_legibility_of_humanoid_robot_arm_movements_in_a_pointing_task.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-A-Study-of-the-Framework-and-Real-World-Applications-of-Language-Embedding-for-3D-Scene-Understanding"><a href="#24-A-Study-of-the-Framework-and-Real-World-Applications-of-Language-Embedding-for-3D-Scene-Understanding" class="headerlink" title="24. A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Study_of_the_Framework_and_Real-World_Applications_of_Language_Embedding_for_3D_Scene_Understandin.pdf">A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Victoria</span></p><p>本论文系统综述了将大语言模型（LLMs）及语言嵌入与3D高斯投影（Gaussian Splatting, 3DGS）结合以实现三维场景理解的方法和应用。其方法梳理了3DGS的理论基础、语言特征与视觉模型的集成策略，以及在机器人、虚拟现实、自动驾驶等真实环境中的应用实例。结论指出，尽管这些技术推动了3D场景的语义理解和机器人智能交互，但仍面临计算消耗高、语义泛化能力有限及实时交互工具不足等挑战，未来需在高效可扩展和跨场景泛化等方面持续突破。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Study_of_the_Framework_and_Real-World_Applications_of_Language_Embedding_for_3D_Scene_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-Will-You-Be-Aware-Eye-Tracking–Based-Modeling-of-Situational-Awareness-in-Augmented-Reality"><a href="#25-Will-You-Be-Aware-Eye-Tracking–Based-Modeling-of-Situational-Awareness-in-Augmented-Reality" class="headerlink" title="25. Will You Be Aware? Eye Tracking–Based Modeling of Situational Awareness in Augmented Reality"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Will_You_Be_Aware__Eye_Tracking-Based_Modeling_of_Situational_Awareness_in_Augmented_Reality.pdf">Will You Be Aware? Eye Tracking–Based Modeling of Situational Awareness in Augmented Reality</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Duke University</span></p><p>该论文提出了一种基于眼动追踪的情境意识（SA）建模方法，针对AR辅助心肺复苏（CPR）任务，开发了实时指导的AR应用并通过模拟突发事件收集用户眼动数据。提出了FixGraphPool图神经网络，将注视点和扫视事件结构化为时空图，实现了对用户SA水平的预测，准确率达83%，优于传统机器学习和先进时序模型。结论显示眼动追踪可有效建模AR环境下的SA，并为安全关键场景的AR系统设计提供了指导。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Will_You_Be_Aware_Eye_Tracking%E2%80%93Based_Modeling_of_Situational_Awareness_in_Augmented_Reality.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-From-Canada-to-Japan-How-10-000-km-Affect-User-Perception-in-Robot-Teleoperation"><a href="#26-From-Canada-to-Japan-How-10-000-km-Affect-User-Perception-in-Robot-Teleoperation" class="headerlink" title="26. From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/From_Canada_to_Japan__How_10,000_km_Affect_User_Perception_in_Robot_Teleoperation.pdf">From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tokyo University of Science</span></p><p>本论文研究了长距离机器人远程操作（RTo）对用户感知的影响，特别关注其在老年护理领域的应用。作者设计了基于ROS和Unity的远程操控系统及用户评估协议，通过问卷和交互实验比较本地与跨国（加拿大至日本）机器人操作体验。结果显示，无论是操作负担还是可用性，用户对远程和本地机器人无显著差异，表明远程机器人可作为本地控制的可行替代方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/From_Canada_to_Japan_How_10%2C000_km_Affect_User_Perception_in_Robot_Teleoperation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-Analyzing-the-Impact-of-Multimodal-Perception-on-Sample-Complexity-and-Optimization-Landscapes-in-Imitation-Learning"><a href="#27-Analyzing-the-Impact-of-Multimodal-Perception-on-Sample-Complexity-and-Optimization-Landscapes-in-Imitation-Learning" class="headerlink" title="27. Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Analyzing_the_Impact_of_Multimodal_Perception_on_Sample_Complexity_and_Optimization_Landscapes_in_Im.pdf">Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Berkeley</span></p><p>本文基于统计学习理论系统分析了多模态感知（RGB-D、语言、位姿）在机器人模仿学习中的作用，采用Rademacher复杂度、PAC学习等理论工具，揭示多模态集成可显著降低样本复杂度并改善优化景观。结论指出，合理融合多模态信息能在机器人操作任务中实现更强泛化能力、更高数据效率和更稳定训练表现，理论与仿真实验均得到了验证。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Analyzing_the_Impact_of_Multimodal_Perception_on_Sample_Complexity_and_Optimization_Landscapes_in_Imitation_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-Hierarchical-Deep-Deterministic-Policy-Gradient-for-Autonomous-Maze-Navigation-of-Mobile-Robots"><a href="#28-Hierarchical-Deep-Deterministic-Policy-Gradient-for-Autonomous-Maze-Navigation-of-Mobile-Robots" class="headerlink" title="28. Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Hierarchical_Deep_Deterministic_Policy_Gradient_for_Autonomous_Maze_Navigation_of_Mobile_Robots.pdf">Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universiti Sains Malaysia</span></p><p>本文提出了一种高效的分层深度确定性策略梯度（HDDPG）算法用于移动机器人自主迷宫导航。该方法通过高层策略生成中间子目标并低层策略实现具体动作，结合自适应参数噪声、离策略修正、梯度裁剪和奖励函数重塑，显著提升了探索效率、路径规划和训练稳定性。在ROS和Gazebo仿真平台上的实验表明，HDDPG在多个迷宫导航任务中将成功率提升至少56.59%，平均奖励提升519.03，明显优于标准DDPG及其变体，验证了分层结构和各项优化的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Hierarchical_Deep_Deterministic_Policy_Gradient_for_Autonomous_Maze_Navigation_of_Mobile_Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="29-GAP-Gaussianize-Any-Point-Clouds-with-Text-Guidance"><a href="#29-GAP-Gaussianize-Any-Point-Clouds-with-Text-Guidance" class="headerlink" title="29. GAP: Gaussianize Any Point Clouds with Text Guidance"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/GAP__Gaussianize_Any_Point_Clouds_with_Text_Guidance.pdf">GAP: Gaussianize Any Point Clouds with Text Guidance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出GAP方法，将无颜色的3D点云在文本引导下转化为高保真3D高斯表示。其核心包括多视角优化框架，结合深度感知的图像扩散模型和表面锚定机制保证几何精度，并引入基于扩散的高斯补全策略修复难以观测区域。实验结果显示，GAP在多种数据集和场景下均优于现有方法，能高效生成结构精确、外观多样的3D高斯体。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/GAP_Gaussianize_Any_Point_Clouds_with_Text_Guidance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-Uni-CoT-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision"><a href="#30-Uni-CoT-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision" class="headerlink" title="30. Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Uni-cot__Towards_Unified_Chain-of-Thought_Reasoning_Across_Text_and_Vision.pdf">Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Academy of AI for Science</span></p><p>该论文提出了Uni-CoT，一种统一的链式思维(CoT)框架，采用宏观任务规划与微观子任务执行的双层层次化推理结构，将多模态推理任务分解为高层规划和马尔可夫决策过程(MDP)建模的子任务，通过结构化训练范式实现高效监督和偏好微调。实验表明，Uni-CoT在多项推理驱动的图像生成和编辑任务中表现出色，在WISE、RISE和KRIS等基准上取得了领先性能和良好解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Uni-CoT_Towards_Unified_Chain-of-Thought_Reasoning_Across_Text_and_Vision.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-DART-Dual-Adaptive-Refinement-Transfer-for-Open-Vocabulary-Multi-Label-Recognition"><a href="#31-DART-Dual-Adaptive-Refinement-Transfer-for-Open-Vocabulary-Multi-Label-Recognition" class="headerlink" title="31. DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/DART__Dual_Adaptive_Refinement_Transfer_for_Open-Vocabulary_Multi-Label_Recognition.pdf">DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-sen University</span></p><p>本论文提出DART框架，通过在冻结的视觉-语言预训练(VLP)模型基础上，集成自适应细化模块(ARM)与弱监督Patch选择损失(WPS)，实现对局部视觉特征的精细定位，同时引入基于大语言模型(LLM)挖掘的类关系图(CRG)指导自适应转移模块(ATM)，以图注意力网络完成跨类别信息传递和多模态融合。实验结果显示，DART在NUS-WIDE、MS-COCO和Open Images等多项开放词表多标签识别基准上取得了新的SOTA性能，验证了方法的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/DART_Dual_Adaptive_Refinement_Transfer_for_Open-Vocabulary_Multi-Label_Recognition.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-χ-Symmetry-Understanding-of-3D-Shapes-via-Chirality-Disentanglement"><a href="#32-χ-Symmetry-Understanding-of-3D-Shapes-via-Chirality-Disentanglement" class="headerlink" title="32. χ: Symmetry Understanding of 3D Shapes via Chirality Disentanglement"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Symmetry_Understanding_of_3D_Shapes_via_Chirality_Disentanglement.pdf">χ: Symmetry Understanding of 3D Shapes via Chirality Disentanglement</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Bonn</span></p><p>本文提出了一种基于Diff3F框架的无监督管道，通过从2D基础视觉模型（DINO-V2和StableDiffusion）提取特征，实现3D形状顶点手性（chirality）信息的解耦与建模。新特征能有效区分3D形状的左右部分，解决了传统特征在形状匹配和分割时的左右歧义，在多数据集和任务中表现出色，且对局部或各向异性形状具有良好鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/%CF%87_Symmetry_Understanding_of_3D_Shapes_via_Chirality_Disentanglement.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-SMOL-MapSeg-Show-Me-One-Label"><a href="#33-SMOL-MapSeg-Show-Me-One-Label" class="headerlink" title="33. SMOL-MapSeg: Show Me One Label"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SMOL-MapSeg__Show_Me_One_Label.pdf">SMOL-MapSeg: Show Me One Label</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Leibniz University Hannover</span></p><p>本文提出了一种新颖的按需声明式（OND）知识驱动的提示方法，用于指导基础分割模型（以SAM为基础）在历史地图上进行语义分割。方法通过源图像-标签对作为显式提示，结合新设计的Prompt Encoder，对历史地图进行单样本细粒度分割，且支持新类别的少样本自适应。实验表明，SMOL-MapSeg在平均IoU上优于UNet基线模型，对新类别有较好泛化能力，但对局部特征不明显的类别如白色水域分割有局限。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SMOL-MapSeg_Show_Me_One_Label.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-How-and-Why-Taming-Flow-Matching-for-Unsupervised-Anomaly-Detection-and-Localization"><a href="#34-How-and-Why-Taming-Flow-Matching-for-Unsupervised-Anomaly-Detection-and-Localization" class="headerlink" title="34. How and Why: Taming Flow Matching for Unsupervised Anomaly Detection and Localization"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/How_and_Why__Taming_Flow_Matching_for_Unsupervised_Anomaly_Detection_and_Localization.pdf">How and Why: Taming Flow Matching for Unsupervised Anomaly Detection and Localization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>本文提出了基于流匹配（Flow Matching, FM）及其变体“Worst Transport Flow Matching”（WT-Flow）的无监督异常检测与定位新范式，通过理论和实证分析揭示了传统流方法在高维空间中的不可逆性和表达能力限制，并构建了非概率性路径控制机制。WT-Flow方法通过对样本归一化和潜在井构建，实现了对异常样本的有效分离，在MVTec工业视觉异常检测数据集上达到了单尺度下的SOTA性能，显著提升了检测效率和精度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/How_and_Why_Taming_Flow_Matching_for_Unsupervised_Anomaly_Detection_and_Localization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Explaining-Similarity-in-Vision–Language-Encoders-with-Weighted-Banzhaf-Interactions"><a href="#35-Explaining-Similarity-in-Vision–Language-Encoders-with-Weighted-Banzhaf-Interactions" class="headerlink" title="35. Explaining Similarity in Vision–Language Encoders with Weighted Banzhaf Interactions"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Explaining_Similarity_in_Vision-Language_Encoders_with_Weighted_Banzhaf_Interactions.pdf">Explaining Similarity in Vision–Language Encoders with Weighted Banzhaf Interactions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Warsaw</span></p><p>本文提出了FIXLIP，一种基于加权Banzhaf交互指数的视觉-语言编码器解释方法，通过游戏理论建模和高效的跨模态采样策略，能够分解图像-文本对在编码器中的相似性得分。实验结果表明，FIXLIP在MS COCO和ImageNet-1k等基准上比一阶归因方法更准确、可扩展，并支持模型间对比分析。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Explaining_Similarity_in_Vision%E2%80%93Language_Encoders_with_Weighted_Banzhaf_Interactions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation"><a href="#36-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation" class="headerlink" title="36. UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/UNCAGE__Contrastive_Attention_Guidance_for_Masked_Generative_Transformers_in_Text-to-Image_Generatio.pdf">UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University</span></p><p>该论文提出UNCAGE（Unmasking with Contrastive Attention Guidance），为Masked Generative Transformers（MGTs）在文本到图像生成中的多物体和属性绑定问题提供首个训练免方法。UNCAGE利用注意力图，通过正负对比机制指导token解遮顺序，优先解锁能清晰代表单一对象的token，提升合成一致性且推理开销极小。实验表明UNCAGE在多基准和多指标下均优于现有方法，提升了文本-图像对齐和人类偏好。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/UNCAGE_Contrastive_Attention_Guidance_for_Masked_Generative_Transformers_in_Text-to-Image_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-MagicHOI-Leveraging-3D-Priors-for-Accurate-Hand-object-Reconstruction-from-Short-Monocular-Video-Clips"><a href="#37-MagicHOI-Leveraging-3D-Priors-for-Accurate-Hand-object-Reconstruction-from-Short-Monocular-Video-Clips" class="headerlink" title="37. MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MagicHOI__Leveraging_3D_Priors_for_Accurate_Hand-object_Reconstruction_from_Short_Monocular_Video_Cl.pdf">MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (Guangzhou)</span></p><p>MagicHOI提出了一种结合结构光恢复、NVS扩散模型和可见性加权策略的方法，从短时单目视频中实现准确的手与物体三维重建，包括处理遮挡区域。实验证明，该方法在有限视角和遮挡严重的场景下，重建质量和手物体空间关系均优于当前主流方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MagicHOI_Leveraging_3D_Priors_for_Accurate_Hand-object_Reconstruction_from_Short_Monocular_Video_Clips.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-F2PASeg-Feature-Fusion-for-Pituitary-Anatomy-Segmentation-in-Endoscopic-Surgery"><a href="#38-F2PASeg-Feature-Fusion-for-Pituitary-Anatomy-Segmentation-in-Endoscopic-Surgery" class="headerlink" title="38. F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/F2PASeg__Feature_Fusion_for_Pituitary_Anatomy_Segmentation_in_Endoscopic_Surgery.pdf">F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science &amp; Innovation, Chinese Academy of Sciences</span></p><p>该论文提出了F2PASeg方法，通过在视频级语义分割中引入特征融合模块和LoRA分支，结合高分辨率图像特征与深层语义信息，实现了对复杂术中场景下垂体解剖结构的高效分割。实验表明，F2PASeg在新构建的PAS数据集上取得了优于现有方法的精度和实时性，有效提升了手术关键结构的分割准确率并满足术中应用需求。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/F2PASeg_Feature_Fusion_for_Pituitary_Anatomy_Segmentation_in_Endoscopic_Surgery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-Smoothing-Slot-Attention-Iterations-and-Recurrences"><a href="#39-Smoothing-Slot-Attention-Iterations-and-Recurrences" class="headerlink" title="39. Smoothing Slot Attention Iterations and Recurrences"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Smoothing_Slot_Attention_Iterations_and_Recurrences.pdf">Smoothing Slot Attention Iterations and Recurrences</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Aalto University</span></p><p>该论文提出SmoothSA方法，通过预热冷启动查询和区分不同帧的聚合变换，解决Slot Attention在图像和视频对象聚合中的冷启动和变换同质性问题。实验表明，SmoothSA在对象发现、识别和视觉问答任务上取得了新的SOTA表现，显著提升了对象表示质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Smoothing_Slot_Attention_Iterations_and_Recurrences.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-CT-GRAPH-Hierarchical-Graph-Attention-Network-for-Anatomy-Guided-CT-Report-Generation"><a href="#40-CT-GRAPH-Hierarchical-Graph-Attention-Network-for-Anatomy-Guided-CT-Report-Generation" class="headerlink" title="40. CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT Report Generation"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/CT-GRAPH__Hierarchical_Graph_Attention_Network_for_Anatomy-Guided_CT_Report_Generation.pdf">CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT Report Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute for AI in Medicine (IKIM), University Hospital Essen</span></p><p>论文提出了CT-GRAPH，一种结合预训练3D医疗特征编码器和解剖结构分层图注意力网络的方法，通过解剖掩码提取细粒度器官及全局特征，并构建多层次解剖学图，利用图注意力机制进行特征聚合，最终将结构化特征输入大语言模型生成详细医学报告。实验表明，该方法在CT报告生成任务上显著提升了临床实体F1分数（提升7.9%），在细粒度和全局诊断上均优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/CT-GRAPH_Hierarchical_Graph_Attention_Network_for_Anatomy-Guided_CT_Report_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-PriorRG-Prior-Guided-Contrastive-Pre-training-and-Coarse-to-Fine-Decoding-for-Chest-X-ray-Report-Generation"><a href="#41-PriorRG-Prior-Guided-Contrastive-Pre-training-and-Coarse-to-Fine-Decoding-for-Chest-X-ray-Report-Generation" class="headerlink" title="41. PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/PriorRG__Prior-Guided_Contrastive_Pre-training_and_Coarse-to-Fine_Decoding_for_Chest_X-ray_Report_Ge.pdf">PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xidian University</span></p><p>PriorRG提出了一个针对胸部X光报告生成的两阶段框架，结合了患者特定的临床先验信息，包括临床背景和近期影像。其方法包括先验引导的对比预训练，提升图像与文本的语义对齐能力，以及先验感知的粗到细解码策略，逐步融合临床语境、疾病进展和分层视觉特征，显著提升生成报告的临床准确性与流畅度。实验结果表明，PriorRG在MIMIC-CXR及MIMIC-ABN等主流医学数据集上，报告生成和医学图像-文本检索任务均优于现有方法，具有更强的泛化能力和部署效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/PriorRG_Prior-Guided_Contrastive_Pre-training_and_Coarse-to-Fine_Decoding_for_Chest_X-ray_Report_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Textual-Inversion-for-Efficient-Adaptation-of-Open-Vocabulary-Object-Detectors-Without-Forgetting"><a href="#42-Textual-Inversion-for-Efficient-Adaptation-of-Open-Vocabulary-Object-Detectors-Without-Forgetting" class="headerlink" title="42. Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Textual_Inversion_for_Efficient_Adaptation_of_Open-Vocabulary_Object_Detectors_Without_Forgetting.pdf">Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">TNO, Intelligent Imaging, the Hague, the Netherlands</span></p><p>本文提出了一种将文本倒置（Textual Inversion, TI）方法应用于视觉-语言模型（VLM）开放词汇目标检测的新方法。通过仅优化新词嵌入向量、冻结原有模型权重，实现以极少样本（如3张图片）高效扩展检测词汇，同时显著减缓遗忘问题，保留原模型零样本泛化能力。实验表明TI在few-shot和fine-grained检测任务中性能接近甚至超越常规提示微调（Prompt Tuning）方法，且可将已学类别迁移到新领域，显著减少对全模型微调和原始数据的依赖。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Textual_Inversion_for_Efficient_Adaptation_of_Open-Vocabulary_Object_Detectors_Without_Forgetting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-mKG-RAG-Multimodal-Knowledge-Graph-Enhanced-RAG-for-Visual-Question-Answering"><a href="#43-mKG-RAG-Multimodal-Knowledge-Graph-Enhanced-RAG-for-Visual-Question-Answering" class="headerlink" title="43. mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/mKG-RAG__Multimodal_Knowledge_Graph-Enhanced_RAG_for_Visual_Question_Answering.pdf">mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong Polytechnic University</span></p><p>本文提出mKG-RAG，一种结合检索增强生成（RAG）和多模态知识图谱（KG）的新框架，用于提升多模态大语言模型（MLLMs）在知识密集型视觉问答（VQA）任务中的推理能力。方法包括利用MLLM进行多模态文档的实体和关系抽取，构建结构化知识图谱，并通过双阶段检索（向量检索和图检索）精准筛选相关知识，显著提升VQA准确率，实验结果优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/mKG-RAG_Multimodal_Knowledge_Graph-Enhanced_RAG_for_Visual_Question_Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion"><a href="#44-SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion" class="headerlink" title="44. SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SGDFuse__SAM-Guided_Diffusion_for_High-Fidelity_Infrared_and_Visible_Image_Fusion.pdf">SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shandong Technology and Business University</span></p><p>该论文提出了一种名为SGDFuse的红外与可见光图像融合方法，通过Segment Anything Model (SAM)生成高质量语义掩码，引导条件扩散模型，实现高保真、语义一致的图像融合。方法采用两阶段架构：第一阶段多模态特征融合，第二阶段结合语义掩码和扩散生成进行结构细化和语义增强，显著提升融合图像在目标检测和语义分割等下游任务中的表现。实验结果显示SGDFuse在主流数据集和多项指标上均优于现有方法，结论表明该方法能有效统一语义一致性与视觉质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SGDFuse_SAM-Guided_Diffusion_for_High-Fidelity_Infrared_and_Visible_Image_Fusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-CF3-Compact-and-Fast-3D-Feature-Fields"><a href="#45-CF3-Compact-and-Fast-3D-Feature-Fields" class="headerlink" title="45. CF3: Compact and Fast 3D Feature Fields"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/CF3__Compact_and_Fast_3D_Feature_Fields.pdf">CF3: Compact and Fast 3D Feature Fields</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University</span></p><p>该论文提出CF3，一种从多视角2D视觉基础模型特征通过加权融合提升到3D高斯特征场的方法，并在每个高斯点上训练自适应自动编码器进行特征压缩，再通过自适应稀疏化策略合并和剪枝冗余高斯点，大幅降低存储和计算成本。实验表明，CF3在多种3D场景分割与定位任务中，能以仅5%高斯数量达到与主流方法相当的性能，且渲染与查询速度显著提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/CF3_Compact_and_Fast_3D_Feature_Fields.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-A-Study-of-Gender-Classification-Techniques-Based-on-Iris-Images-A-Deep-Survey-and-Analysis"><a href="#46-A-Study-of-Gender-Classification-Techniques-Based-on-Iris-Images-A-Deep-Survey-and-Analysis" class="headerlink" title="46. A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Study_of_Gender_Classification_Techniques_Based_on_Iris_Images__A_Deep_Survey_and_Analysis.pdf">A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical College of Informatics Akre, Duhok Polytechnic University</span></p><p>本文系统综述了基于虹膜图像的性别分类技术，涵盖了传统方法与深度学习（如CNN）在图像采集、分割、特征提取和分类等流程中的应用，并对主流算法（Gabor滤波、DWT、LBP、PCA等）及其在实际数据集上的表现进行比较分析。结论认为，深度学习方法在特征自动提取和分类准确率方面较传统方法有显著提升，但仍面临图像质量、特征选择等挑战，未来可通过优化特征选择与分类器融合进一步提高实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Study_of_Gender_Classification_Techniques_Based_on_Iris_Images_A_Deep_Survey_and_Analysis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-Coarse-to-Fine-Joint-Registration-of-MR-and-Ultrasound-Images-via-Imaging-Style-Transfer"><a href="#47-Coarse-to-Fine-Joint-Registration-of-MR-and-Ultrasound-Images-via-Imaging-Style-Transfer" class="headerlink" title="47. Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Coarse-to-Fine_Joint_Registration_of_MR_and_Ultrasound_Images_via_Imaging_Style_Transfer.pdf">Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>本文提出了一种结合3D CycleGAN无监督风格迁移、分层块匹配仿射变换与SynthMorph局部形变估计的粗到细多阶段MR与超声图像配准方法。方法显著增强了MR与US图像的一致性，但在部分情况下会导致目标配准误差（TRE）增加，因此未来需研究更一致的上下文描述指标。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Coarse-to-Fine_Joint_Registration_of_MR_and_Ultrasound_Images_via_Imaging_Style_Transfer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-ReasoningTrack-Chain-of-Thought-Reasoning-for-Long-term-Vision-Language-Tracking"><a href="#48-ReasoningTrack-Chain-of-Thought-Reasoning-for-Long-term-Vision-Language-Tracking" class="headerlink" title="48. ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/ReasoningTrack__Chain-of-Thought_Reasoning_for_Long-term_Vision-Language_Tracking.pdf">ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Anhui University</span></p><p>该论文提出了ReasoningTrack，一种基于链式推理（Chain-of-Thought, CoT）的视觉-语言长期跟踪框架，利用预训练大规模视觉语言模型Qwen2.5-VL，通过两阶段的有监督微调（SFT）和强化学习（GRPO）优化模型的推理与语言生成能力。方法结合动态语言描述和视觉特征，通过推理链不断更新目标描述，并在统一的跟踪骨干网络中实现高鲁棒性跟踪。在多个数据集上的实验显示，ReasoningTrack显著提升了跟踪精度，并提出了包含200个视频序列的TNLLT长视频视觉语言跟踪数据集，为领域后续研究奠定基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/ReasoningTrack_Chain-of-Thought_Reasoning_for_Long-term_Vision-Language_Tracking.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-SPA-Generalized-Graph-Spectral-Alignment-for-Versatile-Domain-Adaptation"><a href="#49-SPA-Generalized-Graph-Spectral-Alignment-for-Versatile-Domain-Adaptation" class="headerlink" title="49. SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SPA++__Generalized_Graph_Spectral_Alignment_for_Versatile_Domain_Adaptation.pdf">SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>SPA++提出了一种基于图谱对齐和邻居感知传播机制的通用域自适应方法，通过在特征空间进行图谱对齐、利用邻居信息提升目标域判别能力，并结合数据增强和一致性正则化，适配多种复杂域自适应场景（如UDA、SSDA、MSDA、MTDA、长尾&#x2F;子群分布）。实验证明SPA++在多种视觉域自适应基准上的表现优于现有方法，具有更强的鲁棒性和适应性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SPA%2B%2B_Generalized_Graph_Spectral_Alignment_for_Versatile_Domain_Adaptation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-Wavelet-Guided-Dual-Frequency-Encoding-for-Remote-Sensing-Change-Detection"><a href="#50-Wavelet-Guided-Dual-Frequency-Encoding-for-Remote-Sensing-Change-Detection" class="headerlink" title="50. Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Wavelet-Guided_Dual-Frequency_Encoding_for_Remote_Sensing_Change_Detection.pdf">Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shandong Technology and Business University</span></p><p>该论文提出了Wavelet-Guided Dual-Frequency Encoding (WGDF)方法，通过在输入阶段利用离散小波变换（DWT）将遥感影像分解为高低频分支，分别提取局部细节和全局语义：高频分支利用DFFE和FDID模块增强边缘变化建模，低频分支结合Transformer和PCDM捕捉全局语义变化，最终融合获得变化检测结果。实验在多个遥感数据集上验证了WGDF在复杂场景下的边缘消歧和检测准确性优于现有方法，具有较高的鲁棒性和效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Wavelet-Guided_Dual-Frequency_Encoding_for_Remote_Sensing_Change_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding"><a href="#51-RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding" class="headerlink" title="51. RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/RegionMed-CLIP__A_Region-Aware_Multimodal_Contrastive_Learning_Pre-trained_Model_for_Medical_Image_U.pdf">RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Anhui Polytechnic University</span></p><p>该论文提出RegionMed-CLIP，一种结合全局与局部特征的区域感知多模态对比学习框架，核心包括ROI（感兴趣区域）处理器和分阶段训练策略，有效融合医学影像的细粒度病灶信息与语义文本。通过自建高质量区域标注的MedRegion-500k数据集，实验在零样本分类、图文检索和医学VQA任务上均显著优于现有SOTA模型，证明区域感知对比预训练对于医学图像理解的关键作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/RegionMed-CLIP_A_Region-Aware_Multimodal_Contrastive_Learning_Pre-trained_Model_for_Medical_Image_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-ArbiViewGen-Controllable-Arbitrary-Viewpoint-Camera-Data-Generation-for-Autonomous-Driving-via-Stable-Diffusion-Models"><a href="#52-ArbiViewGen-Controllable-Arbitrary-Viewpoint-Camera-Data-Generation-for-Autonomous-Driving-via-Stable-Diffusion-Models" class="headerlink" title="52. ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/ArbiViewGen__Controllable_Arbitrary_Viewpoint_Camera_Data_Generation_for_Autonomous_Driving_via_Stab.pdf">ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本论文提出了ArbiViewGen，一种基于Stable Diffusion模型的可控任意视角相机图像生成框架，专为自动驾驶场景下多车辆多摄像头系统设计。方法核心包括特征感知自适应视图拼接（FAVS）与跨视图一致性自监督学习（CVC-SSL）：FAVS通过分层几何与特征匹配生成高质量伪标签，CVC-SSL利用扩散模型实现闭环自监督训练，无需真实外插视角监督，仅依赖多摄像头图像和姿态信息。实验表明，该方法在nuScenes自动驾驶数据集上实现了高于现有方法的任意视角合成质量，显著提升了跨平台数据复用能力。结论指出，ArbiViewGen有效解决了缺乏外插视角监督的难题，提升了自动驾驶感知系统的适应性和鲁棒性，但在高度动态环境下细节还需提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/ArbiViewGen_Controllable_Arbitrary_Viewpoint_Camera_Data_Generation_for_Autonomous_Driving_via_Stable_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-Segmenting-the-Complex-and-Irregular-in-Two-Phase-Flows-A-Real-World-Empirical-Study-with-SAM2"><a href="#53-Segmenting-the-Complex-and-Irregular-in-Two-Phase-Flows-A-Real-World-Empirical-Study-with-SAM2" class="headerlink" title="53. Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Segmenting_the_Complex_and_Irregular_in_Two-Phase_Flows__A_Real-World_Empirical_Study_with_SAM2.pdf">Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Delft University of Technology</span></p><p>本论文将复杂气泡分割任务视为迁移学习问题，首次对Segment Anything Model (SAM v2.1)进行细致微调，并通过数据增强策略，实现在真实多相流场景下对高度不规则、非凸形状气泡的高精度分割。实验表明，仅用100张标注图像即可获得F1和Dice分数高达0.95，优于以往需要大规模数据训练的方法，特别在中、大气泡分割上表现突出，显著降低了人工标注成本。结论指出，该方法数据效率高，泛化性强，并公开了数据集和微调流程，助力多相流领域的后续研究。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Segmenting_the_Complex_and_Irregular_in_Two-Phase_Flows_A_Real-World_Empirical_Study_with_SAM2.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-Beyond-Pixels-Medical-Image-Quality-Assessment-with-Implicit-Neural-Representations"><a href="#54-Beyond-Pixels-Medical-Image-Quality-Assessment-with-Implicit-Neural-Representations" class="headerlink" title="54. Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Beyond_Pixels__Medical_Image_Quality_Assessment_with_Implicit_Neural_Representations.pdf">Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Istanbul Technical University</span></p><p>该论文提出使用基于SIREN的隐式神经表示（INR）对心脏MRI图像质量进行评估，通过两阶段流程：首先将2D切片拟合为INR参数，然后利用深度神经网络（如多层感知机、图神经网络、关系注意力Transformer等）直接在INR参数空间进行分类。实验表明，该方法在ACDC数据集上实现了优于传统像素级方法的准确率，并大幅减少了参数量，尤其Transformer（Probe&#x3D;32）模型在Good vs. Poor任务上达到94.4%准确率，展现了INR作为医学图像质量描述符的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Beyond_Pixels_Medical_Image_Quality_Assessment_with_Implicit_Neural_Representations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-X-MoGen-Unified-Motion-Generation-across-Humans-and-Animals"><a href="#55-X-MoGen-Unified-Motion-Generation-across-Humans-and-Animals" class="headerlink" title="55. X-MoGen: Unified Motion Generation across Humans and Animals"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/X-MoGen__Unified_Motion_Generation_across_Humans_and_Animals.pdf">X-MoGen: Unified Motion Generation across Humans and Animals</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本论文提出X-MoGen，首次实现跨人类与动物统一的文本驱动动作生成框架。方法采用两阶段架构：第一阶段通过条件图变分自编码器（CGAE）学习T-pose骨架先验及自编码器（AE）编码动作至共享潜在空间，并引入形态损失正则化；第二阶段利用掩码变换器（Masked Transformer）结合扩散模型，根据文本描述生成动作嵌入，训练时形态一致性模块（MCM）保证结构合理性。作者还构建了包含115种物种、119k序列的UniMo4D统一数据集，支持跨物种联合建模。实验显示X-MoGen在已见及未见物种上均优于现有方法，生成动作真实且具良好泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/X-MoGen_Unified_Motion_Generation_across_Humans_and_Animals.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-FedGIN-Federated-Learning-with-Dynamic-Global-Intensity-Non-linear-Augmentation-for-Organ-Segmentation-using-Multi-modal-Images"><a href="#56-FedGIN-Federated-Learning-with-Dynamic-Global-Intensity-Non-linear-Augmentation-for-Organ-Segmentation-using-Multi-modal-Images" class="headerlink" title="56. FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FedGIN__Federated_Learning_with_Dynamic_Global_Intensity_Non-linear_Augmentation_for_Organ_Segmentat.pdf">FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Norwegian University of Science and Technology</span></p><p>该论文提出FedGIN，一种结合了全局强度非线性（GIN）增强的联邦学习框架，用于多模态医学图像（CT和MRI）在隐私保护下的器官分割。FedGIN在本地训练阶段通过GIN模块动态对CT和MRI图像进行强度域增强，促使模型学习模态无关的特征，并通过模型参数聚合实现跨机构的协同建模，显著提升了在复杂和低对比度器官上的分割表现。实验结果表明，FedGIN在多模态数据下接近中心化训练性能，特别是在胰腺和胆囊等难分割器官上优于单模态和无GIN增强的联邦学习基线，验证了其在隐私保护、多机构、多模态实际应用场景下的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FedGIN_Federated_Learning_with_Dynamic_Global_Intensity_Non-linear_Augmentation_for_Organ_Segmentation_using_Multi-modal_Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-PoseGen-In-Context-LoRA-Finetuning-for-Pose-Controllable-Long-Human-Video-Generation"><a href="#57-PoseGen-In-Context-LoRA-Finetuning-for-Pose-Controllable-Long-Human-Video-Generation" class="headerlink" title="57. PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/PoseGen__In-Context_LoRA_Finetuning_for_Pose-Controllable_Long_Human_Video_Generation.pdf">PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xiaoice</span></p><p>该论文提出PoseGen，一种高效且创新的人体视频生成框架，实现了从单张参考图像和动作姿态序列生成任意长度、高保真、姿态可控的人体视频。方法核心包括双重条件机制：用LoRA微调视频扩散模型，参考信息在token级注入、姿态信息在channel级注入，并采用分段生成与KV缓存共享技术，实现视频片段无缝拼接，保证身份一致性和时序连贯性。实验结果显示，PoseGen在身份保持、动作控制和长视频一致性方面均优于现有方法，且仅需极少训练数据。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/PoseGen_In-Context_LoRA_Finetuning_for_Pose-Controllable_Long_Human_Video_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-AdaFusion-Prompt-Guided-Inference-with-Adaptive-Fusion-of-Pathology-Foundation-Models"><a href="#58-AdaFusion-Prompt-Guided-Inference-with-Adaptive-Fusion-of-Pathology-Foundation-Models" class="headerlink" title="58. AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/AdaFusion__Prompt-Guided_Inference_with_Adaptive_Fusion_of_Pathology_Foundation_Models.pdf">AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China University of Technology</span></p><p>本文提出AdaFusion，一种面向病理图像分析的多模型特征自适应融合推理框架。其方法是在无需微调基础模型的前提下，采用轻量级注意力机制对多种预训练病理基础模型（PFMs）的低维特征进行压缩、对齐和基于组织表型上下文的动态加权融合。对治疗响应预测、肿瘤分级和空间基因表达推断等多项真实世界任务的实验表明，AdaFusion在分类和回归任务上均优于单一PFM和主流融合基线，并能可解释性地量化各模型对不同组织表型的贡献，提升鲁棒性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/AdaFusion_Prompt-Guided_Inference_with_Adaptive_Fusion_of_Pathology_Foundation_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-AU-IQA-A-Benchmark-Dataset-for-Perceptual-Quality-Assessment-of-AI-Enhanced-User-Generated-Content"><a href="#59-AU-IQA-A-Benchmark-Dataset-for-Perceptual-Quality-Assessment-of-AI-Enhanced-User-Generated-Content" class="headerlink" title="59. AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/AU-IQA__A_Benchmark_Dataset_for_Perceptual_Quality_Assessment_of_AI-Enhanced_User-Generated_Content.pdf">AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了AU-IQA，这是首个面向AI增强用户生成内容（AI-UGC）感知质量评估的基准数据集，涵盖超分辨率、低光增强和去噪三类AI增强方式，共4800张图像，并采用主观均值评分（MOS）进行标注。作者系统评测了传统IQA模型和大规模多模态模型在该数据集上的表现，发现以UGC为目标的模型效果最佳，现有模型在不同AI-UGC类型间表现差异大且不稳定，强调了开发专用AI-UGC质量评估模型的必要性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/AU-IQA_A_Benchmark_Dataset_for_Perceptual_Quality_Assessment_of_AI-Enhanced_User-Generated_Content.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-Multimodal-Causal-Driven-Representation-Learning-for-Generalizable-Medical-Image-Segmentation"><a href="#60-Multimodal-Causal-Driven-Representation-Learning-for-Generalizable-Medical-Image-Segmentation" class="headerlink" title="60. Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Multimodal_Causal-Driven_Representation_Learning_for_Generalizable_Medical_Image_Segmentation.pdf">Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong Institute of Science &amp; Innovation</span></p><p>该论文提出了一种多模态因果驱动表征学习（MCDRL）框架，通过结合因果推断和视觉-语言模型（如CLIP），分两步实现医学图像分割的领域泛化：首先利用CLIP跨模态能力识别病灶区域并通过文本提示构建混杂因素字典，然后训练因果干预网络，消除领域特异变异影响，保留关键解剖结构信息。实验结果表明，MCDRL在多个医学图像分割数据集上优于现有方法，具备更强领域泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Multimodal_Causal-Driven_Representation_Learning_for_Generalizable_Medical_Image_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-Propagating-Sparse-Depth-via-Depth-Foundation-Model-for-Out-of-Distribution-Depth-Completion"><a href="#61-Propagating-Sparse-Depth-via-Depth-Foundation-Model-for-Out-of-Distribution-Depth-Completion" class="headerlink" title="61. Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Propagating_Sparse_Depth_via_Depth_Foundation_Model_for_Out-of-Distribution_Depth_Completion.pdf">Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Dalian University of Technology</span></p><p>该论文提出了PSD深度补全框架，通过利用预训练的深度基础模型从RGB图像中提取结构和语义线索，引导稀疏深度向缺失区域传播，包含无参数的3D&#x2F;2D双空间传播模块及可学习的残差校正模块。实验显示，该方法在16个OOD场景下超越现有方法，具备极强泛化性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Propagating_Sparse_Depth_via_Depth_Foundation_Model_for_Out-of-Distribution_Depth_Completion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-Rotation-Equivariant-Arbitrary-scale-Image-Super-Resolution"><a href="#62-Rotation-Equivariant-Arbitrary-scale-Image-Super-Resolution" class="headerlink" title="62. Rotation Equivariant Arbitrary-scale Image Super-Resolution"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Rotation_Equivariant_Arbitrary-scale_Image_Super-Resolution.pdf">Rotation Equivariant Arbitrary-scale Image Super-Resolution</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>本文提出了一种端到端旋转等变的任意尺度图像超分辨率(ASISR)方法，通过重新设计隐式神经表示(INR)和编码器模块，嵌入旋转等变性，理论分析了等变误差并实现了对现有ASISR方法的可插拔增强。实验结果表明，该方法能更好地保持图像几何结构及旋转对称性，有效提升超分辨率恢复质量和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Rotation_Equivariant_Arbitrary-scale_Image_Super-Resolution.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-Decoupling-Continual-Semantic-Segmentation"><a href="#63-Decoupling-Continual-Semantic-Segmentation" class="headerlink" title="63. Decoupling Continual Semantic Segmentation"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Decoupling_Continual_Semantic_Segmentation.pdf">Decoupling Continual Semantic Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-sen University</span></p><p>该论文提出了DecoupleCSS，一个用于持续语义分割（CSS）的两阶段框架，将类别感知检测与类别无关分割解耦。方法中，第一阶段利用预训练视觉-语言模型并结合LoRA适配器进行类别检测和定位提示生成，第二阶段用Segment Anything Model（SAM）实现精细分割，显著缓解遗忘问题并提升新旧知识平衡。实验表明，DecoupleCSS在多个常用CSS任务上取得了当前最优表现，具备较强的泛化能力，但推理时多任务切换带来一定延迟。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Decoupling_Continual_Semantic_Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-Modeling-Rapid-Contextual-Learning-in-the-Visual-Cortex-with-Fast-Weight-Deep-Autoencoder-Networks"><a href="#64-Modeling-Rapid-Contextual-Learning-in-the-Visual-Cortex-with-Fast-Weight-Deep-Autoencoder-Networks" class="headerlink" title="64. Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Modeling_Rapid_Contextual_Learning_in_the_Visual_Cortex_with_Fast-Weight_Deep_Autoencoder_Networks.pdf">Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>本文提出了一种基于Vision Transformer（ViT）自编码器的架构，利用低秩适配（LoRA）模块模拟大脑视觉皮层中的快速权重机制，实现熟悉上下文的快速学习。实验表明，熟悉训练促使早期层对全局上下文敏感，压缩任务无关的变异，同时LoRA进一步增强了这一效果，有助于实现更鲁棒的视觉表示和更清晰的图像分割。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Modeling_Rapid_Contextual_Learning_in_the_Visual_Cortex_with_Fast-Weight_Deep_Autoencoder_Networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-UGOD-Uncertainty-Guided-Differentiable-Opacity-and-Soft-Dropout-for-Enhanced-Sparse-View-3DGS"><a href="#65-UGOD-Uncertainty-Guided-Differentiable-Opacity-and-Soft-Dropout-for-Enhanced-Sparse-View-3DGS" class="headerlink" title="65. UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/UGOD__Uncertainty-Guided_Differentiable_Opacity_and_Soft_Dropout_for_Enhanced_Sparse-View_3DGS.pdf">UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Manchester Metropolitan University</span></p><p>该论文提出了一种集成神经网络的不确定性学习模块，通过结合多层HashGrid编码，对3D Gaussian Splatting（3DGS）中的每个高斯进行视点相关的不确定性建模。方法利用学习到的不确定性分别指导高斯的不透明度调节及可微分软丢弃机制，有效减少稀疏视角场景下的过拟合。实验表明，该方法在多个3D重建数据集上均优于现有方法，能以更少的高斯实现更高的渲染质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/UGOD_Uncertainty-Guided_Differentiable_Opacity_and_Soft_Dropout_for_Enhanced_Sparse-View_3DGS.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-Open-world-Point-Cloud-Semantic-Segmentation-A-Human-in-the-loop-Framework"><a href="#66-Open-world-Point-Cloud-Semantic-Segmentation-A-Human-in-the-loop-Framework" class="headerlink" title="66. Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Open-world_Point_Cloud_Semantic_Segmentation__A_Human-in-the-loop_Framework.pdf">Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University of Science and Technology</span></p><p>该论文提出了HOW-Seg框架，实现了点云的开放世界语义分割，通过在查询样本上以稀疏人工标注为指导，直接构建类别原型并结合交互式原型消歧和基于CRF的原型标签分配机制，避免了跨样本分布偏移和原型偏差。实验表明，HOW-Seg无需额外支持样本或离线增量学习，仅靠极少量人工干预即可在S3DIS和ScanNetv2等数据集上显著优于现有方法，实现高质量的基类与新类点云分割。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Open-world_Point_Cloud_Semantic_Segmentation_A_Human-in-the-loop_Framework.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-Laplacian-Analysis-Meets-Dynamics-Modelling-Gaussian-Splatting-for-4D-Reconstruction"><a href="#67-Laplacian-Analysis-Meets-Dynamics-Modelling-Gaussian-Splatting-for-4D-Reconstruction" class="headerlink" title="67. Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Laplacian_Analysis_Meets_Dynamics_Modelling__Gaussian_Splatting_for_4D_Reconstruction.pdf">Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (Guangzhou)</span></p><p>本论文提出一种动态3D高斯投影(3DGS)的混合显式-隐式编码框架，融合多尺度哈希编码与拉普拉斯频域分析，有效解耦并控制复杂动态场景中不同频率的运动分量。方法还包括增强的高斯动态属性和基于KDTree的自适应高斯划分策略，实现对高动态区域的精细建模和高效优化。实验结果在三个主流动态场景数据集上验证了所提方法在重建精度与运动一致性上的领先性能，优于现有动态重建方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Laplacian_Analysis_Meets_Dynamics_Modelling_Gaussian_Splatting_for_4D_Reconstruction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="68-H-NET-Hierarchical-Dynamic-Chunking-for-Tokenizer-Free-Language-Modelling-in-Morphologically-Rich-Languages"><a href="#68-H-NET-Hierarchical-Dynamic-Chunking-for-Tokenizer-Free-Language-Modelling-in-Morphologically-Rich-Languages" class="headerlink" title="68. H-NET++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/H-Net++__Hierarchical_Dynamic_Chunking_for_Tokenizer-Free_Language_Modelling_in_Morphologically-Rich.pdf">H-NET++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent (no institution listed)</span></p><p>该论文提出了H-NET++，一种针对形态复杂语言（如波斯语）的分层动态分块语言建模方法，核心创新包括层次化GRU路由器、轻量Transformer上下文混合器、文档级变分超先验、特殊处理零宽非连接字符（ZWNJ）以及阶段式课程训练。实验结果显示H-NET++在语料压缩、下游任务准确率、对正字法噪声的鲁棒性和形态边界识别方面均超越现有模型，并能自动学习符合语言学规律的分块，无需手动标注。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/H-NET%2B%2B_Hierarchical_Dynamic_Chunking_for_Tokenizer-Free_Language_Modelling_in_Morphologically-Rich_Languages.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-Simulating-Human-Like-Learning-Dynamics-with-LLM-Empowered-Agents"><a href="#69-Simulating-Human-Like-Learning-Dynamics-with-LLM-Empowered-Agents" class="headerlink" title="69. Simulating Human-Like Learning Dynamics with LLM-Empowered Agents"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Simulating_Human-Like_Learning_Dynamics_with_LLM-Empowered_Agents.pdf">Simulating Human-Like Learning Dynamics with LLM-Empowered Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>该论文提出LearnerAgent框架，利用多智能体系统和大语言模型（LLM）模拟具有不同心理画像（深度、表层、懒惰、无画像）的学生在为期一年的真实教学环境中的学习行为。通过周期性知识获取、测试与互动，揭示只有深度型学习者具备长期认知成长，LLM默认行为为“勤奋但脆弱的表层学习者”，存在对表层模式的依赖，缺乏深层泛化能力。结论：LearnerAgent能高保真复现人类学习行为，揭示LLM短板并为教育与AI认知提供新洞察。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Simulating_Human-Like_Learning_Dynamics_with_LLM-Empowered_Agents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle"><a href="#70-Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle" class="headerlink" title="70. Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Shuffle-R1__Efficient_RL_framework_for_Multimodal_Large_Language_Models_via_Data-centric_Dynamic_Shu.pdf">Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huazhong University of Science and Technology</span></p><p>该论文提出了Shuffle-R1框架，通过Pairwise Trajectory Sampling（结构化高对比轨迹采样）和Advantage-based Batch Shuffle（优势感知批次重构）优化多模态大语言模型的强化学习微调流程，有效解决了优势塌缩和回合静默问题，提高了训练效率和泛化能力。实验结果表明，Shuffle-R1在多项主流推理任务上超越现有RL方法，且具备较低计算开销。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Shuffle-R1_Efficient_RL_framework_for_Multimodal_Large_Language_Models_via_Data-centric_Dynamic_Shuffle.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-LLaVA-RE-Binary-Image-Text-Relevancy-Evaluation-with-Multimodal-Large-Language-Model"><a href="#71-LLaVA-RE-Binary-Image-Text-Relevancy-Evaluation-with-Multimodal-Large-Language-Model" class="headerlink" title="71. LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/LLaVA-RE__Binary_Image-Text_Relevancy_Evaluation_with_Multimodal_Large_Language_Model.pdf">LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stony Brook University</span></p><p>该论文提出LLaVA-RE框架，基于LLaVA 1.5多模态大语言模型，通过引入详细任务指令和多模态in-context learning，实现复杂图文对二元相关性评估，并构建了多任务的二元相关性数据集。实验表明该方法在训练及未见任务上均显著优于主流模型，具备良好泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/LLaVA-RE_Binary_Image-Text_Relevancy_Evaluation_with_Multimodal_Large_Language_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-Iterative-Learning-of-Computable-Phenotypes-for-Treatment-Resistant-Hypertension-using-Large-Language-Models"><a href="#72-Iterative-Learning-of-Computable-Phenotypes-for-Treatment-Resistant-Hypertension-using-Large-Language-Models" class="headerlink" title="72. Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Iterative_Learning_of_Computable_Phenotypes_for_Treatment_Resistant_Hypertension_using_Large_Languag.pdf">Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Federal University of ABC</span></p><p>该论文提出使用大型语言模型（LLM）自动生成高血压及其复杂变型的可计算表型，通过“synthesize, execute, debug, instruct（SEDI）”迭代策略对模型进行数据驱动反馈优化。实验结果表明，SEDI策略下的LLM生成模型在准确性和可解释性方面接近甚至优于传统机器学习方法，但所需专家标注样本更少，且模型更简洁易于临床应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Iterative_Learning_of_Computable_Phenotypes_for_Treatment_Resistant_Hypertension_using_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-MV-Debate-Multi-view-Agent-Debate-with-Dynamic-Reflection-Gating-for-Multimodal-Harmful-Content-Detection-in-Social-Media"><a href="#73-MV-Debate-Multi-view-Agent-Debate-with-Dynamic-Reflection-Gating-for-Multimodal-Harmful-Content-Detection-in-Social-Media" class="headerlink" title="73. MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MV-Debate__Multi-view_Agent_Debate_with_Dynamic_Reflection_Gating_for_Multimodal_Harmful_Content_Det.pdf">MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shandong University</span></p><p>本文提出MV-Debate多视角多智能体辩论框架，通过组装表层分析、深度推理、模态对比和社会语境四类异构智能体，结合动态反思门控机制，实现多模态有害内容（如讽刺、仇恨言论、虚假信息）的统一检测。实验显示该方法在多项基准数据集上显著优于单模型和现有多智能体方法，提升了准确性、效率和可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MV-Debate_Multi-view_Agent_Debate_with_Dynamic_Reflection_Gating_for_Multimodal_Harmful_Content_Detection_in_Social_Media.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification"><a href="#74-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification" class="headerlink" title="74. On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/On_the_Generalization_of_SFT__A_Reinforcement_Learning_Perspective_with_Reward_Rectification.pdf">On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southeast University</span></p><p>本文从理论上揭示了大语言模型（LLM）中常用的Supervised Fine-Tuning（SFT）方法的泛化能力受限，原因在于其梯度本质上具有隐式的奖励结构，导致优化不稳定和过拟合。为此，提出了一种简单的动态重加权方法Dynamic Fine-Tuning（DFT），通过对每个token的目标函数动态缩放，大幅提升了SFT的泛化能力，并在多个数学推理基准和离线RL场景下超越了标准SFT及多种RL方法。结论：DFT显著提升了SFT的泛化和训练稳定性，为LLM微调提供了高效实用的新范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/On_the_Generalization_of_SFT_A_Reinforcement_Learning_Perspective_with_Reward_Rectification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-How-Do-LLMs-Persuade-Linear-Probes-Can-Uncover-Persuasion-Dynamics-in-Multi-Turn-Conversations"><a href="#75-How-Do-LLMs-Persuade-Linear-Probes-Can-Uncover-Persuasion-Dynamics-in-Multi-Turn-Conversations" class="headerlink" title="75. How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/How_Do_LLMs_Persuade__Linear_Probes_Can_Uncover_Persuasion_Dynamics_in_Multi-Turn_Conversations.pdf">How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mila</span></p><p>本论文提出了一种利用线性探针（linear probes）分析大型语言模型（LLMs）在多轮对话中说服能力的方法。作者设计了可高效检测说服结果、被说服者个性和说服策略的探针，并在真实及合成数据集上验证其能捕捉对话中的关键说服动态，发现探针不仅效率远超提示（prompting），并在部分任务上效果更优。结论显示，线性探针能够高效揭示LLM说服行为细节，适用于大规模对话分析与复杂行为研究。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/How_Do_LLMs_Persuade_Linear_Probes_Can_Uncover_Persuasion_Dynamics_in_Multi-Turn_Conversations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-The-Missing-Reward-Active-Inference-in-the-Era-of-Experience"><a href="#76-The-Missing-Reward-Active-Inference-in-the-Era-of-Experience" class="headerlink" title="76. The Missing Reward: Active Inference in the Era of Experience"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/The_Missing_Reward__Active_Inference_in_the_Era_of_Experience.pdf">The Missing Reward: Active Inference in the Era of Experience</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IBM T.J. Watson Research Center</span></p><p>本文提出将Active Inference（AIF）与大规模语言模型（LLM）结合，作为实现AI自主学习与经验驱动智能的理论基础。方法上，以AIF的自由能最小化为内在目标，利用LLM作为生成式世界模型，统一探索与利用，并消除对人工奖励工程的依赖。结论认为该架构能提升AI自主性与能源效率，推动经验驱动的可持续AI发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/The_Missing_Reward_Active_Inference_in_the_Era_of_Experience.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-COOPER-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models"><a href="#77-COOPER-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models" class="headerlink" title="77. COOPER: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Cooper__Co-Optimizing_Policy_and_Reward_Models_in_Reinforcement_Learning_for_Large_Language_Models.pdf">COOPER: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出Cooper框架，在强化学习中同步优化大语言模型的策略模型和奖励模型。方法上，通过参考式奖励模型（VerifyRM）、混合注释策略和正负对比学习动态更新奖励模型，结合规则型高精度奖励筛选正样本和助手LLM生成负样本，极大提升了奖励模型的鲁棒性、减少reward hacking。实验证明，Cooper在多个数学推理任务上优于仅用规则或静态奖励模型，动态更新奖励模型能有效抑制reward hacking并提升RL效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/COOPER_Co-Optimizing_Policy_and_Reward_Models_in_Reinforcement_Learning_for_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-Follow-Your-Instruction-A-Comprehensive-MLLM-Agent-for-World-Data-Synthesis"><a href="#78-Follow-Your-Instruction-A-Comprehensive-MLLM-Agent-for-World-Data-Synthesis" class="headerlink" title="78. Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Follow-Your-Instruction__A_Comprehensive_MLLM_Agent_for_World_Data_Synthesis.pdf">Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">HKUST(GZ)</span></p><p>本文提出了Follow-Your-Instruction框架，一种基于多模态大语言模型（MLLM）的数据合成代理，支持2D、3D和4D数据的自动高质量合成。该方法通过MLLM-Collector收集多模态资产，MLLM-Generator生成3D布局，MLLM-Optimizer多视图优化场景，MLLM-Planner结合VLM实现时序连贯的视频生成。实验表明，该框架合成的数据能显著提升多项AIGC下游任务性能，具有良好可扩展性和实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Follow-Your-Instruction_A_Comprehensive_MLLM_Agent_for_World_Data_Synthesis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-Conformal-Sets-in-Multiple-Choice-Question-Answering-under-Black-Box-Settings-with-Provable-Coverage-Guarantees"><a href="#79-Conformal-Sets-in-Multiple-Choice-Question-Answering-under-Black-Box-Settings-with-Provable-Coverage-Guarantees" class="headerlink" title="79. Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Conformal_Sets_in_Multiple-Choice_Question_Answering_under_Black-Box_Settings_with_Provable_Coverage.pdf">Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Jinan</span></p><p>本文提出了一种基于频率的不确定性量化方法，结合多次独立采样和模态输出作为参考，通过经验频率计算预测熵，并利用Conformal Prediction理论构建具有可证明覆盖率保证的预测集。实验结果表明，该方法在多种LLM和MCQA数据集上较传统logit概率方法更有效地区分正确与错误预测，并能在不同风险级别下有效控制误覆盖率，提升LLM在黑盒场景下的不确定性量化可靠性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Conformal_Sets_in_Multiple-Choice_Question_Answering_under_Black-Box_Settings_with_Provable_Coverage_Guarantees.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-AI-vs-Human-Moderators-A-Comparative-Evaluation-of-Multimodal-LLMs-in-Content-Moderation-for-Brand-Safety"><a href="#80-AI-vs-Human-Moderators-A-Comparative-Evaluation-of-Multimodal-LLMs-in-Content-Moderation-for-Brand-Safety" class="headerlink" title="80. AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/AI_vs._Human_Moderators__A_Comparative_Evaluation_of_Multimodal_LLMs_in_Content_Moderation_for_Brand.pdf">AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zefr Inc</span></p><p>本文提出了一个多模态和多语言视频数据集，用于品牌安全内容审核，并系统评估了多模态大语言模型（MLLMs，如Gemini、GPT、Llama）在视频内容审核任务中的表现。方法包括多模态信息处理、零样本分类和精细化提示设计，结果显示MLLMs可显著降低人工审核成本，且多模态模型优于文本模型，但准确率仍不及专业人工审核，尤其在复杂和细微分类上表现不足。结论指出MLLMs具备提升内容审核效率的潜力，未来可通过微调和多语言优化进一步提升效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/AI_vs._Human_Moderators_A_Comparative_Evaluation_of_Multimodal_LLMs_in_Content_Moderation_for_Brand_Safety.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-Leveraging-AI-to-Accelerate-Clinical-Data-Cleaning-A-Comparative-Study-of-AI-Assisted-vs-Traditional-Methods"><a href="#81-Leveraging-AI-to-Accelerate-Clinical-Data-Cleaning-A-Comparative-Study-of-AI-Assisted-vs-Traditional-Methods" class="headerlink" title="81. Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Leveraging_AI_to_Accelerate_Clinical_Data_Cleaning__A_Comparative_Study_of_AI-Assisted_vs._Tradition.pdf">Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Octozi</span></p><p>该论文提出了Octozi平台，通过结合大语言模型（如Llama 4微调）和临床领域启发式算法，实现了临床数据清洗流程的自动化和智能化。实验结果显示，AI辅助不仅提升了数据清洗速度6倍，还将错误率从54.67%降至8.48%，显著减轻了人工负担并保证合规性和准确性，验证了AI在药物临床试验数据管理中的变革潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Leveraging_AI_to_Accelerate_Clinical_Data_Cleaning_A_Comparative_Study_of_AI-Assisted_vs._Traditional_Methods.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMs’-Entity-Deduction-Capabilities"><a href="#82-The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMs’-Entity-Deduction-Capabilities" class="headerlink" title="82. The World According to LLMs: How Geographic Origin Influences LLMs’ Entity Deduction Capabilities"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/The_World_According_to_LLMs__How_Geographic_Origin_Influences_LLMs'_Entity_Deduction_Capabilities.pdf">The World According to LLMs: How Geographic Origin Influences LLMs’ Entity Deduction Capabilities</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">BITS, Pilani</span></p><p>本文提出了一种基于“20问游戏”的新型评测框架，通过让大型语言模型（LLMs）自发地提出问题并推理，系统性地揭示了模型在地理实体推理任务中的隐性地理偏见。研究发现，主流LLMs在推理来自全球北方和西方地区的知名人物和事物时表现更优，而实体的流行度和训练语料频次对这种差异的解释力有限，游戏语言影响也极小，凸显了模型推理路径中的地理和文化不平等。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/The_World_According_to_LLMs_How_Geographic_Origin_Influences_LLMs%E2%80%99_Entity_Deduction_Capabilities.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-Streamlining-Admission-with-LOR-Insights-AI-Based-Leadership-Assessment-in-Online-Master’s-Program"><a href="#83-Streamlining-Admission-with-LOR-Insights-AI-Based-Leadership-Assessment-in-Online-Master’s-Program" class="headerlink" title="83. Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master’s Program"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Streamlining_Admission_with_LOR_Insights__AI-Based_Leadership_Assessment_in_Online_Master's_Program.pdf">Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master’s Program</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Georgia Institute of Technology</span></p><p>本论文提出LORI系统，通过结合RoBERTa弱监督模型和LLAMA2等大语言模型，对硕士申请者推荐信中的领导力特质（如团队协作、沟通、创新）进行自动化检测与微标签分类。实验证明RoBERTa模型在领导力句子识别上取得了91.6%的F1分数，并通过LLM实现更深层次的短语提取与验证，有效提升了招生流程效率和公平性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Streamlining_Admission_with_LOR_Insights_AI-Based_Leadership_Assessment_in_Online_Master%27s_Program.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-Auto-Eval-Judge-Towards-a-General-Agentic-Framework-for-Task-Completion-Evaluation"><a href="#84-Auto-Eval-Judge-Towards-a-General-Agentic-Framework-for-Task-Completion-Evaluation" class="headerlink" title="84. Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Auto-Eval_Judge__Towards_a_General_Agentic_Framework_for_Task_Completion_Evaluation.pdf">Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Massachusetts Amherst</span></p><p>本文提出了一个通用、模块化的Agent任务完成评价框架，能够自动分解任务为子任务，通过LLM和多Agent模块验证每一步推理和最终结果。实验在GAIA和BigCodeBench数据集上进行，与GPT-4o LLM-as-a-Judge基线相比，该Judge Agent与人类评估的对齐度提升4.76%和10.52%。结论显示该框架能更全面、可扩展地评价复杂Agent系统，减少人工依赖。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Auto-Eval_Judge_Towards_a_General_Agentic_Framework_for_Task_Completion_Evaluation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities"><a href="#85-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities" class="headerlink" title="85. InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/InfiAlign__A_Scalable_and_Sample-Efficient_Framework_for_Aligning_LLMs_to_Enhance_Reasoning_Capabili.pdf">InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">InfiX.ai</span></p><p>本文提出了InfiAlign框架，通过结合多维度自动化数据筛选、监督微调（SFT）和直接偏好优化（DPO），实现大语言模型在推理任务上的高效对齐。核心方法包括多维质量指标驱动的数据采样（涵盖多样性、难度、质量等），并采用模块化和可扩展设计，实现小数据量下的高性能推理能力。结论显示，InfiAlign在仅用12%数据量下达到与主流强基线相当性能，DPO进一步提升数学推理表现，实现了高数据利用率和易扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/InfiAlign_A_Scalable_and_Sample-Efficient_Framework_for_Aligning_LLMs_to_Enhance_Reasoning_Capabilities.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-Can-Large-Language-Models-Generate-Effective-Datasets-for-Emotion-Recognition-in-Conversations"><a href="#86-Can-Large-Language-Models-Generate-Effective-Datasets-for-Emotion-Recognition-in-Conversations" class="headerlink" title="86. Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Can_Large_Language_Models_Generate_Effective_Datasets_for_Emotion_Recognition_in_Conversations_.pdf">Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Hamburg</span></p><p>本文提出利用小型、通用的大语言模型（Vicuna 1.5-13B）结合结构化提示工程，自动合成多样化的对话情绪识别（ERC）数据集，包括自然分布和均衡分布两种类型，针对主流ERC基准（MELD、EmoryNLP、IEMOCAP）分别生成相应的数据集，并通过多种现有ERC分类模型验证其有效性。实验结果和统计检验显示，基于LLM生成的数据集能提升情感识别模型的鲁棒性和性能，并对标签分布失衡等问题具有积极影响，结论为该方法能够高效、可复现地生成高质量ERC数据集，促进下游模型性能提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Can_Large_Language_Models_Generate_Effective_Datasets_for_Emotion_Recognition_in_Conversations_.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-Group-Causal-Policy-Optimization-for-Post-Training-Large-Language-Models"><a href="#87-Group-Causal-Policy-Optimization-for-Post-Training-Large-Language-Models" class="headerlink" title="87. Group Causal Policy Optimization for Post-Training Large Language Models"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Group_Causal_Policy_Optimization_for_Post-Training_Large_Language_Models.pdf">Group Causal Policy Optimization for Post-Training Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Software, Chinese Academy of Sciences</span></p><p>本论文提出Group Causal Policy Optimization（GCPO）方法，通过引入结构化因果模型（SCM），发现LLM候选响应间因最终集成输出而产生因果关联，进而将候选输出投影到因果子空间，并结合KL正则项，引导策略优化在结构上更一致。实验结果表明，GCPO在多项数学与代码推理基准上均优于GRPO等现有方法，验证了因果结构建模对LLM后训练效果的提升和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Group_Causal_Policy_Optimization_for_Post-Training_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-LLM-based-Multi-Agent-Copilot-for-Quantum-Sensor"><a href="#88-LLM-based-Multi-Agent-Copilot-for-Quantum-Sensor" class="headerlink" title="88. LLM-based Multi-Agent Copilot for Quantum Sensor"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/LLM-based_Multi-Agent_Copilot_for_Quantum_Sensor.pdf">LLM-based Multi-Agent Copilot for Quantum Sensor</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Defense Technology</span></p><p>本文提出QCopilot，一个基于大语言模型（LLM）的多智能体框架，集成外部知识访问、主动学习和不确定性量化，用于量子传感器的设计与诊断。QCopilot通过专用智能体实现实验参数自适应优化、自动建模分析和独立故障诊断，实验在冷原子原型系统中实现了无人工干预下的高效制冷和异常参数自主识别，大幅提升了实验效率和智能化水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/LLM-based_Multi-Agent_Copilot_for_Quantum_Sensor.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-StructVRM-Aligning-Multimodal-Reasoning-with-Structured-and-Verifiable-Reward-Models"><a href="#89-StructVRM-Aligning-Multimodal-Reasoning-with-Structured-and-Verifiable-Reward-Models" class="headerlink" title="89. StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/StructVRM__Aligning_Multimodal_Reasoning_with_Structured_and_Verifiable_Reward_Models.pdf">StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ByteDance Seed China</span></p><p>StructVRM提出了一种基于模型的细粒度奖励机制，通过训练可验证的奖励模型（verifier），在多模态推理任务中对多子问题进行结构化、可验证的评分，实现了部分正确性反馈，并结合PPO强化学习优化模型推理能力。实验证明，StructVRM在六个公开多模态基准和高难度STEM-Bench上取得SOTA表现，显著提升了复杂现实推理任务中的多模态模型能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/StructVRM_Aligning_Multimodal_Reasoning_with_Structured_and_Verifiable_Reward_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-The-Term-‘Agent’-Has-Been-Diluted-Beyond-Utility-and-Requires-Redefinition"><a href="#90-The-Term-‘Agent’-Has-Been-Diluted-Beyond-Utility-and-Requires-Redefinition" class="headerlink" title="90. The Term ‘Agent’ Has Been Diluted Beyond Utility and Requires Redefinition"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/The_Term_'Agent'_Has_Been_Diluted_Beyond_Utility_and_Requires_Redefinition.pdf">The Term ‘Agent’ Has Been Diluted Beyond Utility and Requires Redefinition</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Duke University</span></p><p>本文提出了针对“agent”一词在人工智能领域被广泛滥用和定义模糊的重新定义框架。通过历史和现有用法分析，作者提出了五维度（环境交互、目标导向、时间一致性、学习适应性和自主性）评价体系，并明确了成为‘agent’的最低要求。结论指出，该框架有助于提升研究方法的严谨性、沟通的清晰度和政策制定的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/The_Term_%E2%80%98Agent%E2%80%99_Has_Been_Diluted_Beyond_Utility_and_Requires_Redefinition.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-LAG-Logic-Augmented-Generation-from-a-Cartesian-Perspective"><a href="#91-LAG-Logic-Augmented-Generation-from-a-Cartesian-Perspective" class="headerlink" title="91. LAG: Logic-Augmented Generation from a Cartesian Perspective"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/LAG__Logic-Augmented_Generation_from_a_Cartesian_Perspective.pdf">LAG: Logic-Augmented Generation from a Cartesian Perspective</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong Polytechnic University</span></p><p>该论文提出了LAG（Logic-Augmented Generation），一种针对大型语言模型检索增强生成（RAG）系统的创新推理管道。通过自适应问题分解、逻辑链推理、逻辑终止机制，LAG能将复杂问题拆解为原子子问题，按逻辑依赖顺序逐步解决，并在各环节有效防止错误传递。实验表明LAG在多跳问答和复杂推理任务上显著提升准确率和合理性，优于主流RAG方法，实现更接近人类认知的推理过程。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/LAG_Logic-Augmented_Generation_from_a_Cartesian_Perspective.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs"><a href="#92-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs" class="headerlink" title="92. MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MELLA__Bridging_Linguistic_Capability_and_Cultural_Groundedness_for_Low-Resource_Language_MLLMs.pdf">MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Artificial Intelligence Laboratory</span></p><p>本文提出一种针对低资源语言多模态大语言模型（MLLM）的双目标与双源数据框架，强调语言能力和文化扎根性。通过构建MELLA大规模多语种多模态数据集，分别采集本地网页原生alt-text（文化知识）和MLLM生成描述（语言能力），并联合训练，显著提升模型在八种低资源语言上的描述准确度和文化认知能力。实验表明，经过MELLA微调后，多模态模型在各项指标上均优于现有方法，实现了更丰富的“厚描述”。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MELLA_Bridging_Linguistic_Capability_and_Cultural_Groundedness_for_Low-Resource_Language_MLLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-GRAIL-Learning-to-Interact-with-Large-Knowledge-Graphs-for-Retrieval-Augmented-Reasoning"><a href="#93-GRAIL-Learning-to-Interact-with-Large-Knowledge-Graphs-for-Retrieval-Augmented-Reasoning" class="headerlink" title="93. GRAIL: Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/GRAIL_Learning_to_Interact_with_Large_Knowledge_Graphs_for_Retrieval_Augmented_Reasoning.pdf">GRAIL: Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute for AI Industry Research (AIR), Tsinghua University</span></p><p>GRAIL提出了一个针对大型知识图谱的交互式检索增强推理框架，通过LLM引导的数据合成、两阶段（监督微调+强化学习）训练和动态交互检索机制，实现多步高效、结构感知的信息检索与推理。实验结果显示，GRAIL在多个知识图谱问答基准上取得了显著性能提升，平均准确率提升21.01%、F1提升22.43%，同时大幅减少冗余检索信息，实现检索深度与精度的高效平衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/GRAIL_Learning_to_Interact_with_Large_Knowledge_Graphs_for_Retrieval_Augmented_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-MoMA-A-Mixture-of-Multimodal-Agents-Architecture-for-Enhancing-Clinical-Prediction-Modelling"><a href="#94-MoMA-A-Mixture-of-Multimodal-Agents-Architecture-for-Enhancing-Clinical-Prediction-Modelling" class="headerlink" title="94. MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MoMA__A_Mixture-of-Multimodal-Agents_Architecture_for_Enhancing_Clinical_Prediction_Modelling.pdf">MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Wisconsin-Madison</span></p><p>该论文提出了MoMA架构，通过多智能体协作，利用多个大语言模型（LLM）将多模态电子健康记录（EHR）数据中的非文本模态（如医学影像、实验室结果）转化为结构化文本摘要，再由聚合智能体整合所有文本后交给预测智能体用于临床预测。评测结果表明，MoMA在多个多模态临床任务上超越了现有主流方法，表现出更高的预测准确性和灵活性，且可灵活适配新模态，降低对大规模配对数据的依赖。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MoMA_A_Mixture-of-Multimodal-Agents_Architecture_for_Enhancing_Clinical_Prediction_Modelling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-MyCulture-Exploring-Malaysia’s-Diverse-Culture-under-Low-Resource-Language-Constraints"><a href="#95-MyCulture-Exploring-Malaysia’s-Diverse-Culture-under-Low-Resource-Language-Constraints" class="headerlink" title="95. MyCulture: Exploring Malaysia’s Diverse Culture under Low-Resource Language Constraints"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MyCulture__Exploring_Malaysia's_Diverse_Culture_under_Low-Resource_Language_Constraints.pdf">MyCulture: Exploring Malaysia’s Diverse Culture under Low-Resource Language Constraints</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universiti Malaya</span></p><p>本文提出MyCulture基准，是首个面向马来西亚、以马来语呈现的多元文化LLM评测基准，覆盖艺术、服饰、习俗、娱乐、食物和宗教六大领域，并采用创新的无标准选项的开放式多选题格式（包括多答案、排序、匹配三种类型）以减少猜测和格式偏倚。实验结果显示，传统封闭式多选题高估了模型的文化理解能力，采用开放式题型后主流LLM的成绩平均下降至少17%，凸显更严格、文化嵌入型评测的重要性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MyCulture_Exploring_Malaysia%E2%80%99s_Diverse_Culture_under_Low-Resource_Language_Constraints.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-Large-Language-Models-Transform-Organic-Synthesis-From-Reaction-Prediction-to-Automation"><a href="#96-Large-Language-Models-Transform-Organic-Synthesis-From-Reaction-Prediction-to-Automation" class="headerlink" title="96. Large Language Models Transform Organic Synthesis: From Reaction Prediction to Automation"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Large_Language_Models_Transform_Organic_Synthesis_From_Reaction_Prediction_to_Automation.pdf">Large Language Models Transform Organic Synthesis: From Reaction Prediction to Automation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Government Boys Higher Secondary School, Bukera Sharif, Tando Allahyar, Affiliated with BISE Hyderabad, Sindh, Pakistan</span></p><p>本文系统综述了大型语言模型（LLMs）在有机合成中的应用进展，包括化学反应预测、逆合成规划、条件优化及与机器人平台的自动化集成。通过结合LLMs与图神经网络、强化学习和多模态数据，显著提升了反应预测准确率和实验自动化效率，同时讨论了数据偏差、模型解释性和安全风险等挑战。结论认为LLMs正在推动化学合成向高效、自动化和可持续方向变革，但仍需解决可靠性与可解释性等关键问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Large_Language_Models_Transform_Organic_Synthesis_From_Reaction_Prediction_to_Automation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-NomicLaw-Emergent-Trust-and-Strategic-Argumentation-in-LLMs-During-Collaborative-Law-Making"><a href="#97-NomicLaw-Emergent-Trust-and-Strategic-Argumentation-in-LLMs-During-Collaborative-Law-Making" class="headerlink" title="97. NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/NomicLaw__Emergent_Trust_and_Strategic_Argumentation_in_LLMs_During_Collaborative_Law-Making.pdf">NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Jyvaskyla</span></p><p>该论文提出NomicLaw框架，让多个开源大语言模型（LLMs）在法律治理场景下以“提案-论证-投票”循环协作立法，量化分析信任、互惠和联盟形成等社会动态，并通过主题分析揭示模型在多智能体协作中呈现的法律推理和说服能力。实验显示，模型异质性能减少自投、增加联盟切换并丰富法律论证类型，强调多样化模型组合可提升AI辅助立法的合理性和多元性，但当前LLM推理多为统计模式而非真正理解，需人类监管。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/NomicLaw_Emergent_Trust_and_Strategic_Argumentation_in_LLMs_During_Collaborative_Law-Making.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-A-Novel-Architecture-for-Symbolic-Reasoning-with-Decision-Trees-and-LLM-Agents"><a href="#98-A-Novel-Architecture-for-Symbolic-Reasoning-with-Decision-Trees-and-LLM-Agents" class="headerlink" title="98. A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Novel_Architecture_for_Symbolic_Reasoning_with_Decision_Trees_and_LLM_Agents.pdf">A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harvard University</span></p><p>本文提出一种结合决策树符号推理与大型语言模型（LLM）生成推理能力的多智能体系统架构。方法包括将决策树作为可调用的符号推理模块嵌入到由LLM驱动的协调推理流程中，通过中央编排器维护信念一致性并动态调用工具，实现结构化和非结构化知识的跨模态推理。实验表明该架构在ProofWriter、GSM8k和ARC等基准任务上推理一致性和准确率均超过现有方法，且系统具备高度可解释性和可扩展性，适用于医疗决策支持和科学发现等领域。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Novel_Architecture_for_Symbolic_Reasoning_with_Decision_Trees_and_LLM_Agents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-B4DL-A-Benchmark-for-4D-LiDAR-LLM-in-Spatio-Temporal-Understanding"><a href="#99-B4DL-A-Benchmark-for-4D-LiDAR-LLM-in-Spatio-Temporal-Understanding" class="headerlink" title="99. B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/B4DL__A_Benchmark_for_4D_LiDAR_LLM_in_Spatio-Temporal_Understanding.pdf">B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea Advanced Institute of Science and Technology</span></p><p>该论文提出B4DL基准与数据集，专为训练和评估多模态大语言模型在4D LiDAR时空理解任务中而设计。方法包括创新的数据生成管线（融合多视角图像与人工注释，自动生成LiDAR时空场景的问答对），以及提出B4DL模型，通过LiDAR编码器、对齐层和元信息模块实现4D LiDAR与语言的高效对齐，采用分阶段训练策略提升模型空间与时序推理能力。实验证明B4DL模型在空间及时序理解上显著优于现有3D或视频多模态LLM，在多项任务和跨数据集泛化方面表现突出。结论：B4DL为4D LiDAR多模态理解提供了首个高质量基准和方法体系，有效推动了真实世界时空动态理解领域的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/B4DL_A_Benchmark_for_4D_LiDAR_LLM_in_Spatio-Temporal_Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-Resource-Limited-Joint-Multimodal-Sentiment-Reasoning-and-Classification-via-Chain-of-Thought-Enhancement-and-Distillation"><a href="#100-Resource-Limited-Joint-Multimodal-Sentiment-Reasoning-and-Classification-via-Chain-of-Thought-Enhancement-and-Distillation" class="headerlink" title="100. Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Resource-Limited_Joint_Multimodal_Sentiment_Reasoning_and_Classification_via_Chain-of-Thought_Enhanc.pdf">Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeastern University</span></p><p>本文提出MulCoT-RD框架，针对资源受限场景下的多模态情感推理与分类（JMSRC）任务，采用“教师-助理-学生”三阶段推理蒸馏范式，通过多任务学习及结构化CoT（Chain-of-Thought）增强和软硬标签联合优化，使轻量级多模态大模型能自主生成高质量情感推理链并实现高效情感分类。实验表明，MulCoT-RD在多个公开数据集上实现了较优的情感分类性能和推理解释性，且模型仅3B参数，具备良好的部署适应性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Resource-Limited_Joint_Multimodal_Sentiment_Reasoning_and_Classification_via_Chain-of-Thought_Enhancement_and_Distillation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="101-VFlowOpt-A-Token-Pruning-Framework-for-LMMs-with-Visual-Information-Flow-Guided-Optimization"><a href="#101-VFlowOpt-A-Token-Pruning-Framework-for-LMMs-with-Visual-Information-Flow-Guided-Optimization" class="headerlink" title="101. VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/VFlowOpt__A_Token_Pruning_Framework_for_LMMs_with_Visual_Information_Flow-Guided_Optimization.pdf">VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai AI Laboratory</span></p><p>本文提出VFlowOpt，一种针对大型多模态模型（LMMs）的可微视觉信息流引导的视觉token裁剪框架。方法通过结合注意力校准和信息熵，分阶段评估视觉token重要性，采用回收机制减少信息损失，并利用视觉信息流优化各阶段裁剪超参数。实验表明，VFlowOpt可在保留10%视觉token的情况下仍维持90%模型性能，显著降低显存占用和推理时延，适用于实际部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/VFlowOpt_A_Token_Pruning_Framework_for_LMMs_with_Visual_Information_Flow-Guided_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-FAITH-A-Framework-for-Assessing-Intrinsic-Tabular-Hallucinations-in-Finance"><a href="#102-FAITH-A-Framework-for-Assessing-Intrinsic-Tabular-Hallucinations-in-Finance" class="headerlink" title="102. FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in Finance"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FAITH__A_Framework_for_Assessing_Intrinsic_Tabular_Hallucinations_in_finance.pdf">FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in Finance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Asian Institute of Digital Finance, National University of Singapore</span></p><p>该论文提出了用于金融领域评估大型语言模型（LLM）内在幻觉的新框架，包括自动化数据集构建、基于真实年报的金融幻觉基准数据集，以及四类金融推理复杂度的分类方法。实验结果表明，主流LLM在复杂金融表格推理任务中依然存在较高幻觉率，准确性随推理复杂度下降，表明当前模型在高精度金融应用部署上仍有显著挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FAITH_A_Framework_for_Assessing_Intrinsic_Tabular_Hallucinations_in_Finance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-Incident-Response-Planning-Using-a-Lightweight-Large-Language-Model-with-Reduced-Hallucination"><a href="#103-Incident-Response-Planning-Using-a-Lightweight-Large-Language-Model-with-Reduced-Hallucination" class="headerlink" title="103. Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Incident_Response_Planning_Using_a_Lightweight_Large_Language_Model_with_Reduced_Hallucination.pdf">Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Melbourne</span></p><p>本文提出了一种集成轻量级大语言模型（LLM）用于网络安全事件响应的方法，包括指令微调、信息检索和决策规划三步。该方法通过理论证明有效减少幻觉概率，并在多数据集上实验显示比前沿LLM恢复时间缩短最高22%，泛化能力强且资源消耗低。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Incident_Response_Planning_Using_a_Lightweight_Large_Language_Model_with_Reduced_Hallucination.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-RLHF-Fine-Tuning-of-LLMs-for-Alignment-with-Implicit-User-Feedback-in-Conversational-Recommenders"><a href="#104-RLHF-Fine-Tuning-of-LLMs-for-Alignment-with-Implicit-User-Feedback-in-Conversational-Recommenders" class="headerlink" title="104. RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/RLHF_Fine-Tuning_of_LLMs_for_Alignment_with_Implicit_User_Feedback_in_Conversational_Recommenders.pdf">RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeastern University</span></p><p>该论文提出了一种将RLHF（基于人类反馈的强化学习）用于大语言模型（LLM）在多轮对话推荐系统中的微调方法，通过隐式用户反馈（如停留时间、情感变化、语义一致性）构建奖励模型，并用PPO优化LLM生成的推荐结果。实验结果表明，该方法在REDIAL和OpenDialKG等数据集上显著提升了推荐准确率、对话流畅性和用户满意度，验证了隐式信号指导下的自适应推荐能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/RLHF_Fine-Tuning_of_LLMs_for_Alignment_with_Implicit_User_Feedback_in_Conversational_Recommenders.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-Understanding-and-Mitigating-Errors-of-LLM-Generated-RTL-Code"><a href="#105-Understanding-and-Mitigating-Errors-of-LLM-Generated-RTL-Code" class="headerlink" title="105. Understanding and Mitigating Errors of LLM-Generated RTL Code"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Understanding_and_Mitigating_Errors_of_LLM-Generated_RTL_Code.pdf">Understanding and Mitigating Errors of LLM-Generated RTL Code</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Computing Technology, Chinese Academy of Sciences</span></p><p>本论文系统分析LLM生成的RTL代码错误类型，发现主要问题为缺乏RTL编程知识、理解电路概念不足、描述歧义和多模态输入误解。作者提出RAG知识补充、规则描述修正、多模态转换及仿真调试等纠错机制，并集成至代码生成流程，在VerilogEval基准上准确率达91%，显著优于基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Understanding_and_Mitigating_Errors_of_LLM-Generated_RTL_Code.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs"><a href="#106-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs" class="headerlink" title="106. MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MoBE__Mixture-of-Basis-Experts_for_Compressing_MoE-based_LLMs.pdf">MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Inclusion AI</span></p><p>该论文提出Mixture-of-Basis-Experts (MoBE) 方法，通过将专家模块中的权重矩阵分解为专家特有的变换矩阵和共享的基矩阵线性组合，实现大规模MoE语言模型的高效参数压缩。实验表明，MoBE在主流MoE模型上能减少24%-30%的参数，仅带来1%-2%的准确率下降，显著优于现有压缩方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MoBE_Mixture-of-Basis-Experts_for_Compressing_MoE-based_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="107-Pruning-Large-Language-Models-by-Identifying-and-Preserving-Functional-Networks"><a href="#107-Pruning-Large-Language-Models-by-Identifying-and-Preserving-Functional-Networks" class="headerlink" title="107. Pruning Large Language Models by Identifying and Preserving Functional Networks"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Pruning_Large_Language_Models_by_Identifying_and_Preserving_Functional_Networks.pdf">Pruning Large Language Models by Identifying and Preserving Functional Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Automation, Northwestern Polytechnical University</span></p><p>本文提出了一种受神经科学启发的大语言模型（LLM）结构化剪枝新方法，通过独立成分分析（ICA）分解每层神经元信号，识别并保留LLM中的功能网络，实现高效剪枝。实验结果表明，该方法在不同模型和剪枝率下均优于现有主流剪枝方法，有效降低计算和内存消耗，同时保持模型性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Pruning_Large_Language_Models_by_Identifying_and_Preserving_Functional_Networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-Cross-LoRA-A-Data-Free-LoRA-Transfer-Framework-across-Heterogeneous-LLMs"><a href="#108-Cross-LoRA-A-Data-Free-LoRA-Transfer-Framework-across-Heterogeneous-LLMs" class="headerlink" title="108. Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Cross-LoRA__A_Data-Free_LoRA_Transfer_Framework_across_Heterogeneous_LLMs.pdf">Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Baidu Inc</span></p><p>本文提出Cross-LoRA，一种无需数据和训练即可将LoRA适配器在不同大语言模型间迁移的框架。方法包括通过截断奇异值分解（SVD）和Frobenius最优线性变换对源、目标模型进行子空间对齐（LoRA-Align），然后将源LoRA权重更新投影到目标模型参数空间（LoRA-Shift），实现架构无关的迁移。实验显示Cross-LoRA在多个NLP推理基准上实现了对基线模型的稳定提升，性能接近于直接训练的LoRA适配器，且在单卡20分钟内完成迁移，适合低资源和实际应用场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Cross-LoRA_A_Data-Free_LoRA_Transfer_Framework_across_Heterogeneous_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-SPEX-A-Vision-Language-Model-for-Land-Cover-Extraction-on-Spectral-Remote-Sensing-Images"><a href="#109-SPEX-A-Vision-Language-Model-for-Land-Cover-Extraction-on-Spectral-Remote-Sensing-Images" class="headerlink" title="109. SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SPEX__A_Vision-Language_Model_for_Land_Cover_Extraction_on_Spectral_Remote_Sensing_Images.pdf">SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xinjiang University</span></p><p>该论文提出了SPEX模型，通过融合多尺度视觉特征聚合、Token上下文压缩和多光谱视觉预训练，实现了基于指令的多模态大语言模型，用于遥感多光谱影像的地表覆盖物提取。构建了SPIE数据集，将地物光谱先验编码到可被LLM识别的文本属性，显著提升了对植被、水体、建筑等类别的像素级识别精度。实验结果表明，SPEX在五个公开多光谱数据集上均优于现有方法，且生成结果更具解释性与交互性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SPEX_A_Vision-Language_Model_for_Land_Cover_Extraction_on_Spectral_Remote_Sensing_Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="110-EvoGraph-Hybrid-Directed-Graph-Evolution-toward-Software-3-0"><a href="#110-EvoGraph-Hybrid-Directed-Graph-Evolution-toward-Software-3-0" class="headerlink" title="110. EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0"></a>110. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EvoGraph__Hybrid_Directed_Graph_Evolution_toward_Software_3.0.pdf">EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AutoHand AI</span></p><p>EvoGraph提出了一个利用专门小语言模型（SLM）驱动的混合定向图进化框架，用于自动演化企业软件系统的代码、文档、构建流程和工单等全部软件制品。其方法包括基于有类型定向图的统一表征、多种SLM引导的变异算子、带安全约束的多目标进化选择，并在多语言现代化（COBOL、.NET、Lisp等）任务中实现了82-96%语义等价和90%算力节省。结论显示，EvoGraph有效支持企业级遗留系统现代化，SLM优于大模型并经济实用，推进了自主演化的软件3.0路径。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EvoGraph_Hybrid_Directed_Graph_Evolution_toward_Software_3.0.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="111-QA-Dragon-Query-Aware-Dynamic-RAG-System-for-Knowledge-Intensive-Visual-Question-Answering"><a href="#111-QA-Dragon-Query-Aware-Dynamic-RAG-System-for-Knowledge-Intensive-Visual-Question-Answering" class="headerlink" title="111. QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering"></a>111. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/QA-Dragon__Query-Aware_Dynamic_RAG_System_for_Knowledge-Intensive_Visual_Question_Answering.pdf">QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong Polytechnic University</span></p><p>该论文提出了QA-Dragon系统，通过引入领域路由器、搜索路由器、工具路由器以及多阶段重排序机制，实现了动态多模态检索增强生成（RAG）以提升知识密集型视觉问答（VQA）的准确性。其方法结合了图像和文本的混合检索、领域感知推理和多轮多跳推理，有效提高了复杂VQA任务中的答案准确率和知识重叠度，在KDD Cup 2025 Meta CRAG-MM Challenge中全面超越现有强基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/QA-Dragon_Query-Aware_Dynamic_RAG_System_for_Knowledge-Intensive_Visual_Question_Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="112-Attention-Basin-Why-Contextual-Position-Matters-in-Large-Language-Models"><a href="#112-Attention-Basin-Why-Contextual-Position-Matters-in-Large-Language-Models" class="headerlink" title="112. Attention Basin: Why Contextual Position Matters in Large Language Models"></a>112. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Attention_Basin__Why_Contextual_Position_Matters_in_Large_Language_Models.pdf">Attention Basin: Why Contextual Position Matters in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-sen University</span></p><p>本文系统揭示了大语言模型（LLM）在处理结构化输入时存在“attention basin”现象，即模型会对输入序列开头和结尾的信息分配更高关注，中间部分则被忽视。为此，作者提出了一种无需训练、模型无关的Attention-Driven Reranking（AttnRank）方法，通过少量校准集估算模型固有的注意力位置偏好，并据此重排关键内容，使其位于模型关注度高的位置。实验表明，AttnRank在多跳问答和few-shot学习等任务中，无需修改模型参数便能显著提升10种主流LLM的表现。结论：AttnRank高效缓解了位置偏差，提高了信息利用率，且与现有推理加速框架兼容。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Attention_Basin_Why_Contextual_Position_Matters_in_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="113-Beyond-Automation-Socratic-AI-Epistemic-Agency-and-the-Implications-of-the-Emergence-of-Orchestrated-Multi-Agent-Learning-Architectures"><a href="#113-Beyond-Automation-Socratic-AI-Epistemic-Agency-and-the-Implications-of-the-Emergence-of-Orchestrated-Multi-Agent-Learning-Architectures" class="headerlink" title="113. Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures"></a>113. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Beyond_Automation__Socratic_AI,_Epistemic_Agency,_and_the_Implications_of_the_Emergence_of_Orchestra.pdf">Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Kassel</span></p><p>该论文提出并实证验证了一种基于Socratic方法的对话式大语言模型AI导师，通过结构化提问促进学生批判性、独立性和反思性思维，并与通用AI聊天机器人进行了对比实验。结果显示，Socratic AI显著提升了学生的元认知参与和高阶思维，推动了AI教育从单一工具向多智能体协作生态系统转变，并对高等教育的教师角色、课程设计、基础设施及评估体系提出了系统性建议。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Beyond_Automation_Socratic_AI%2C_Epistemic_Agency%2C_and_the_Implications_of_the_Emergence_of_Orchestrated_Multi-Agent_Learning_Architectures.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="114-Align-Don’t-Divide-Revisiting-the-LoRA-Architecture-in-Multi-Task-Learning"><a href="#114-Align-Don’t-Divide-Revisiting-the-LoRA-Architecture-in-Multi-Task-Learning" class="headerlink" title="114. Align, Don’t Divide: Revisiting the LoRA Architecture in Multi-Task Learning"></a>114. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Align,_Don't_Divide__Revisiting_the_LoRA_Architecture_in_Multi-Task_Learning.pdf">Align, Don’t Divide: Revisiting the LoRA Architecture in Multi-Task Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Jilin University</span></p><p>本文系统分析了参数高效微调（PEFT）方法LoRA在多任务学习（MTL）中的主流多头&#x2F;多适配器结构，发现高结构复杂度和任务特异性分离并未带来更优泛化性能，反而简单单头高秩LoRA表现更优。基于此，作者提出Align-LoRA，通过在训练目标中引入KL或MMD对齐损失，显式增强任务间共享表征，无需增加推理开销且获得了显著优于复杂结构的多任务泛化效果。结论：对齐表征、强化任务共享知识比结构分离更关键，Align-LoRA为多任务PEFT提供了更高效新范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Align%2C_Don%27t_Divide_Revisiting_the_LoRA_Architecture_in_Multi-Task_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="115-Aligning-LLMs-on-a-Budget-Inference-Time-Alignment-with-Heuristic-Reward-Models"><a href="#115-Aligning-LLMs-on-a-Budget-Inference-Time-Alignment-with-Heuristic-Reward-Models" class="headerlink" title="115. Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models"></a>115. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Aligning_LLMs_on_a_Budget__Inference-Time_Alignment_with_Heuristic_Reward_Models.pdf">Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Massachusetts Amherst</span></p><p>该论文提出HIA（Heuristic-Guided Inference-time Alignment）方法，通过轻量级启发式奖励模型和两阶段筛选机制，在不需微调和仅需黑盒访问的前提下，实现大语言模型推理时的多目标、个性化对齐。实验证明，在HELPSTEER和COMPRED等真实数据集上，HIA在相同推理预算下优于BoN采样、束搜索和贪心搜索，尤其在推理预算低至1-2次查询时提升显著。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Aligning_LLMs_on_a_Budget_Inference-Time_Alignment_with_Heuristic_Reward_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="116-Speech-LLMs-in-Low-Resource-Scenarios-Data-Volume-Requirements-and-the-Impact-of-Pretraining-on-High-Resource-Languages"><a href="#116-Speech-LLMs-in-Low-Resource-Scenarios-Data-Volume-Requirements-and-the-Impact-of-Pretraining-on-High-Resource-Languages" class="headerlink" title="116. Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages"></a>116. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Speech_LLMs_in_Low-Resource_Scenarios__Data_Volume_Requirements_and_the_Impact_of_Pretraining_on_Hig.pdf">Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Trento</span></p><p>该论文基于SLAM-ASR框架，研究了在低资源语音识别场景中，训练数据量及高资源语言预训练对模型性能的影响。作者通过将Whisper-large-v3-turbo语音编码器、轻量线性投影器和多语种LLM（EuroLLM&#x2F;Salamandra）结合，实验证明：需100-200小时训练数据才能达到Whisper-only模型效果，且投影器在高资源语言上预训练并迁移到低资源语言，尤其是在仅有10-15小时微调数据时，能显著提升识别准确率，多语种投影器进一步提高泛化能力。结论指出，预训练和微调策略为低资源语种语音识别提供了优化方向，但对数据量和跨领域泛化仍存在挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Speech_LLMs_in_Low-Resource_Scenarios_Data_Volume_Requirements_and_the_Impact_of_Pretraining_on_High-Resource_Languages.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="117-Towards-Assessing-Medical-Ethics-from-Knowledge-to-Practice"><a href="#117-Towards-Assessing-Medical-Ethics-from-Knowledge-to-Practice" class="headerlink" title="117. Towards Assessing Medical Ethics from Knowledge to Practice"></a>117. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Assessing_Medical_Ethics_from_Knowledge_to_Practice.pdf">Towards Assessing Medical Ethics from Knowledge to Practice</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong, Shenzhen</span></p><p>该论文提出了PrinciplismQA基准，用于系统性评估大语言模型（LLM）在医疗伦理领域的知识掌握与实际应用能力，结合多项选择题和开放式案例分析，采用专家校验与LLM自动评分流程。实验结果显示所有模型均存在“知识-实践鸿沟”，尤其在实际情境动态应用伦理原则（如仁慈性）方面表现较弱，医疗领域微调虽提升实际伦理表现但略有知识遗忘，未来需强化模型伦理对齐。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Assessing_Medical_Ethics_from_Knowledge_to_Practice.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="118-Exploring-Superior-Function-Calls-via-Reinforcement-Learning"><a href="#118-Exploring-Superior-Function-Calls-via-Reinforcement-Learning" class="headerlink" title="118. Exploring Superior Function Calls via Reinforcement Learning"></a>118. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Exploring_Superior_Function_Calls_via_Reinforcement_Learning.pdf">Exploring Superior Function Calls via Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AWorld Team, Inclusion AI</span></p><p>本文提出了FunRL，一种针对大语言模型函数调用任务的强化学习框架。方法创新性地将Chain-of-Thought（CoT）熵纳入GRPO优势估计，通过两阶段数据准备（LLM和AST评估）与精确的二元奖励函数，促进模型在推理和参数验证上探索多样化思路。实验表明，FunRL在BFCLv2基准上实现了开源模型最优的86.02%总体准确率，显著优于标准GRPO，特别是在代码预训练模型上提升效果明显。结论：FunRL显著提升了LLM工具调用的复杂推理能力和实际应用可靠性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Exploring_Superior_Function_Calls_via_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="119-EasySize-Elastic-Analog-Circuit-Sizing-via-LLM-Guided-Heuristic-Search"><a href="#119-EasySize-Elastic-Analog-Circuit-Sizing-via-LLM-Guided-Heuristic-Search" class="headerlink" title="119. EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search"></a>119. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EasySize__Elastic_Analog_Circuit_Sizing_via_LLM-Guided_Heuristic_Search.pdf">EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出EasySize，一种基于微调Qwen3-8B模型的轻量化模拟电路尺寸调整框架，通过结合大语言模型生成动态损失函数和启发式搜索（DE与PSO），实现跨工艺节点和电路拓扑的通用高效优化。实验表明EasySize在无需额外训练的情况下性能优于AutoCkt，并大幅减少仿真资源消耗，显著提升模拟电路设计自动化的效率与适应性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EasySize_Elastic_Analog_Circuit_Sizing_via_LLM-Guided_Heuristic_Search.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="120-JPS-Jailbreak-Multimodal-Large-Language-Models-with-Collaborative-Visual-Perturbation-and-Textual-Steering"><a href="#120-JPS-Jailbreak-Multimodal-Large-Language-Models-with-Collaborative-Visual-Perturbation-and-Textual-Steering" class="headerlink" title="120. JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering"></a>120. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/JPS__Jailbreak_Multimodal_Large_Language_Models_with_Collaborative_Visual_Perturbation_and_Textual_S.pdf">JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出JPS方法，通过协同优化视觉扰动与多智能体系统生成的文本引导，实现对多模态大语言模型（MLLM）的高质量越狱攻击。JPS将安全绕过任务分配给目标引导的图像对抗扰动，将高效恶意意图实现交由多智能体优化的引导提示，两者迭代协同提升攻击效果。实验结果表明，JPS在多种模型和基准上，越狱成功率（ASR）和恶意意图实现率（MIFR）均达到最新水平，有效提升了实际攻击效用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/JPS_Jailbreak_Multimodal_Large_Language_Models_with_Collaborative_Visual_Perturbation_and_Textual_Steering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="121-MedMKEB-A-Comprehensive-Knowledge-Editing-Benchmark-for-Medical-Multimodal-Large-Language-Models"><a href="#121-MedMKEB-A-Comprehensive-Knowledge-Editing-Benchmark-for-Medical-Multimodal-Large-Language-Models" class="headerlink" title="121. MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models"></a>121. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MedMKEB__A_Comprehensive_Knowledge_Editing_Benchmark_for_Medical_Multimodal_Large_Language_Models.pdf">MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>该论文提出MedMKEB，这是首个专为医学多模态大语言模型设计的知识编辑综合基准，涵盖视觉和文本信息的系统性编辑评估，包含可靠性、局部性、泛化性、可迁移性和鲁棒性五大指标。实验表明，现有知识编辑方法在医学多模态场景下表现有限，需要针对医学领域开发更精细化的编辑算法，MedMKEB将推动医学知识编辑技术的进步。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MedMKEB_A_Comprehensive_Knowledge_Editing_Benchmark_for_Medical_Multimodal_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="122-Finding-Needles-in-Images-Can-Multimodal-LLMs-Locate-Fine-Details"><a href="#122-Finding-Needles-in-Images-Can-Multimodal-LLMs-Locate-Fine-Details" class="headerlink" title="122. Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?"></a>122. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Finding_Needles_in_Images__Can_Multimodal_LLMs_Locate_Fine_Details_.pdf">Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fujitsu Research India</span></p><p>本论文提出NiM-Benchmark，用于评估多模态大语言模型（MLLMs）在复杂文档中定位和推理细粒度信息的能力，并提出Spot-IT方法：结合语义相似性分块和自适应高斯注意力，引导模型关注关键区域。实验表明，Spot-IT在多个DocVQA数据集上均显著优于现有方法，有效提升了复杂布局中文档细节提取的准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Finding_Needles_in_Images_Can_Multimodal_LLMs_Locate_Fine_Details_.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="123-Evaluation-of-Large-Language-Model-Architectures-in-Abstract-Meaning-Representation-Parsing"><a href="#123-Evaluation-of-Large-Language-Model-Architectures-in-Abstract-Meaning-Representation-Parsing" class="headerlink" title="123. Evaluation of Large Language Model Architectures in Abstract Meaning Representation Parsing"></a>123. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Evaluation_of_LLMs_in_AMR_Parsing.pdf">Evaluation of Large Language Model Architectures in Abstract Meaning Representation Parsing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University College London</span></p><p>本文系统评估了四种开源解码器式大语言模型（Phi-3.5、Gemma-2、LLaMA-3.2、DeepSeek-R1-LLaMA-Distilled）在AMR语义解析任务中的微调表现，采用LoRA高效参数微调和SMATCH图对齐指标，在LDC2020T02 Gold AMR3.0等数据集上进行对比分析。结果显示，经过简单微调后，LLM模型（尤其是LLaMA-3.2）在语义解析性能上接近专用SOTA解析器，Phi-3.5则在结构有效性上表现最优，证明LLM无需复杂架构即可高效完成AMR解析任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Evaluation_of_Large_Language_Model_Architectures_in_Abstract_Meaning_Representation_Parsing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="124-Skin-SOAP-A-Weakly-Supervised-Framework-for-Generating-Structured-SOAP-Notes"><a href="#124-Skin-SOAP-A-Weakly-Supervised-Framework-for-Generating-Structured-SOAP-Notes" class="headerlink" title="124. Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes"></a>124. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Skin-SOAP__A_Weakly_Supervised_Framework_for_Generating_Structured_SOAP_Notes.pdf">Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Maryland, Baltimore County</span></p><p>该论文提出了Skin-SOAP，一种弱监督多模态框架，通过结合皮损图像和稀疏临床文本，利用生成式语言模型生成临床描述、检索增强知识整合，并对Vision-LLaMA模型进行参数高效微调，实现结构化SOAP病历自动生成。实验结果显示，Skin-SOAP在临床相关性和结构一致性上与最先进的LLM模型（如GPT-4o等）性能相当，有助于缓解医生文档负担并提升皮肤科临床流程效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Skin-SOAP_A_Weakly_Supervised_Framework_for_Generating_Structured_SOAP_Notes.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="125-SPaRFT-Self-Paced-Reinforcement-Fine-Tuning-for-Large-Language-Models"><a href="#125-SPaRFT-Self-Paced-Reinforcement-Fine-Tuning-for-Large-Language-Models" class="headerlink" title="125. SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models"></a>125. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SPaRFT__Self-Paced_Reinforcement_Fine-Tuning_for_Large_Language_Models.pdf">SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Deakin University</span></p><p>本论文提出SPaRFT，一种自适应的强化学习微调框架，针对大语言模型的高效训练。方法首先使用语义和难度结合的聚类进行数据精简，再以多臂赌博机动态分配训练样本，实现性能驱动的课程学习。实验显示，SPaRFT在数学推理任务中以极少样本达到或超越现有方法，显著提升小模型推理能力，同时资源消耗极低。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SPaRFT_Self-Paced_Reinforcement_Fine-Tuning_for_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="126-Can-Large-Language-Models-Integrate-Spatial-Data-Empirical-Insights-into-Reasoning-Strengths-and-Computational-Weaknesses"><a href="#126-Can-Large-Language-Models-Integrate-Spatial-Data-Empirical-Insights-into-Reasoning-Strengths-and-Computational-Weaknesses" class="headerlink" title="126. Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses"></a>126. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Can_Large_Language_Models_Integrate_Spatial_Data__Empirical_Insights_into_Reasoning_Strengths_and_Co.pdf">Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Washington</span></p><p>本论文系统评估了大语言模型（LLM）在城市空间数据整合中的能力，提出通过引入几何特征（如最小角度、最小距离、最大重叠面积）显著提升LLM在空间数据匹配、融合等任务的表现。实验发现，LLM难以仅凭自然语言指令完成空间推理，但若给定相关特征，则可推断出合理阈值，准确率最高可达99.5%，提出的“review-and-refine”两步自我修正方法进一步提升了结果的可靠性。结论认为，LLM能有效辅助空间数据集成，但尚不适合复杂计算几何任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Can_Large_Language_Models_Integrate_Spatial_Data_Empirical_Insights_into_Reasoning_Strengths_and_Computational_Weaknesses.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="127-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data"><a href="#127-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data" class="headerlink" title="127. R-Zero: Self-Evolving Reasoning LLM from Zero Data"></a>127. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/R-Zero__Self-Evolving_Reasoning_LLM_from_Zero_Data.pdf">R-Zero: Self-Evolving Reasoning LLM from Zero Data</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent AI Seattle Lab</span></p><p>本文提出了R-Zero框架，实现了无需任何外部数据、完全自主进化的推理型大语言模型。方法采用双模型协同进化机制（Challenger和Solver），通过Group Relative Policy Optimization（GRPO）实现模型自我生成、过滤与解答难题的循环，完全摆脱人工标注与现有任务依赖。实验证明，R-Zero能显著提升不同基础LLM在数学及通用推理领域的能力，并可作为有监督微调的有效增强方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/R-Zero_Self-Evolving_Reasoning_LLM_from_Zero_Data.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="128-A-Multi-Stage-Large-Language-Model-Framework-for-Extracting-Suicide-Related-Social-Determinants-of-Health"><a href="#128-A-Multi-Stage-Large-Language-Model-Framework-for-Extracting-Suicide-Related-Social-Determinants-of-Health" class="headerlink" title="128. A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health"></a>128. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Multi-Stage_Large_Language_Model_Framework_for_Extracting_Suicide-Related_Social_Determinants_of_H.pdf">A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Cockrell School of Engineering, The University of Texas at Austin</span></p><p>本论文提出一种多阶段大语言模型（LLM）框架，包括上下文检索、相关性验证和SDoH因子抽取三个步骤，实现对自杀相关社会健康决定因素（SDoH）从非结构化文本中的高效提取。方法在零样本设定下，对比BioBERT、GPT-3.5-turbo和DeepSeek-R1等基线，在提取少见和常见SDoH因子上取得更高的准确率和召回率，并通过提供中间解释提高了模型可解释性和人工标注效率。结论：本方法提升了从文本中提取自杀相关SDoH因子的准确性与透明度，有助于早期风险识别和干预策略制定。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Multi-Stage_Large_Language_Model_Framework_for_Extracting_Suicide-Related_Social_Determinants_of_Health.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="129-Situated-Epistemic-Infrastructures-A-Diagnostic-Framework-for-Post-Coherence-Knowledge"><a href="#129-Situated-Epistemic-Infrastructures-A-Diagnostic-Framework-for-Post-Coherence-Knowledge" class="headerlink" title="129. Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge"></a>129. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Situated_Epistemic_Infrastructures__A_Diagnostic_Framework_for_Post-Coherence_Knowledge.pdf">Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Toronto</span></p><p>本文提出了“Situated Epistemic Infrastructures (SEI)”框架，用以诊断和分析在大语言模型（LLM）等AI技术推动下，知识生产、流通和权威认定过程中的基础结构变革。该方法强调通过基础设施、权力签名、符号压缩和故障动态四个维度，分析知识权威是如何在算法、机构、人类协作等混合系统中不断被重塑。结论指出，SEI能够帮助学者和机构识别和适应由AI驱动的知识体系不稳定性，促进更具反思性和适应性的知识组织模式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Situated_Epistemic_Infrastructures_A_Diagnostic_Framework_for_Post-Coherence_Knowledge.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="130-A-Metric-for-MLLM-Alignment-in-Large-scale-Recommendation"><a href="#130-A-Metric-for-MLLM-Alignment-in-Large-scale-Recommendation" class="headerlink" title="130. A Metric for MLLM Alignment in Large-scale Recommendation"></a>130. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Metric_for_MLLM_Alignment_in_Large-scale_Recommendation.pdf">A Metric for MLLM Alignment in Large-scale Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xiaohongshu Inc.</span></p><p>本文提出了Leakage Impact Score (LIS)，一种用于多模态大语言模型（MLLM）在大规模推荐系统中对齐评价的新指标。LIS通过预先量化偏好数据的上界，辅助MLLM对齐流程，显著提升了多模态推荐部署效率。实验在小红书真实场景下，包括内容推荐和广告推荐，A&#x2F;B测试显示用户时长、广告价值等指标均有显著提升，验证了方法的实际价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Metric_for_MLLM_Alignment_in_Large-scale_Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="131-Learning-Geometric-Aware-Quadrature-Rules-for-Functional-Minimization"><a href="#131-Learning-Geometric-Aware-Quadrature-Rules-for-Functional-Minimization" class="headerlink" title="131. Learning Geometric-Aware Quadrature Rules for Functional Minimization"></a>131. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Learning_Geometric-Aware_Quadrature_Rules_for_Functional_Minimization.pdf">Learning Geometric-Aware Quadrature Rules for Functional Minimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of the Aegean</span></p><p>该论文提出了QuadrANN，一种基于图神经网络(GNN)的新型架构，用于从非均匀点云几何中直接学习最优积分权重，实现对变分原理下泛函最小化问题的高效数值积分。通过多层消息传递结合局部密度和全局域形状特征，QuadrANN显著降低了积分估计的方差，并在多种积分和PDE求解任务中优于传统QMC方法，提升了深度学习变分求解器的稳定性和精度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Learning_Geometric-Aware_Quadrature_Rules_for_Functional_Minimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="132-MolSnap-Snap-Fast-Molecular-Generation-with-Latent-Variational-Mean-Flow"><a href="#132-MolSnap-Snap-Fast-Molecular-Generation-with-Latent-Variational-Mean-Flow" class="headerlink" title="132. MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow"></a>132. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MolSnap__Snap-Fast_Molecular_Generation_with_Latent_Variational_Mean_Flow.pdf">MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Kentucky</span></p><p>该论文提出了MolSnap框架，通过创新的因果感知Transformer（CAT）和变分均值流（VMF）方法，实现了基于文本描述的高质量、多样化分子生成。CAT显式建模分子图与文本间的因果依赖，VMF将潜在空间建模为高斯混合分布，支持高效一步推断。实验表明方法在新颖性、多样性和有效性上均优于SOTA模型，且推理速度提升显著。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MolSnap_Snap-Fast_Molecular_Generation_with_Latent_Variational_Mean_Flow.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="133-Multi-Modal-Multi-Behavior-Sequential-Recommendation-with-Conditional-Diffusion-Based-Feature-Denoising"><a href="#133-Multi-Modal-Multi-Behavior-Sequential-Recommendation-with-Conditional-Diffusion-Based-Feature-Denoising" class="headerlink" title="133. Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising"></a>133. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Multi-Modal_Multi-Behavior_Sequential_Recommendation_with_Conditional_Diffusion-Based_Feature_Denois.pdf">Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Takway.AI</span></p><p>本文提出了一种多模态多行为序列推荐模型M3BSR，通过条件扩散模型分别对多模态特征和行为特征进行去噪，并引入多专家兴趣提取层以建模用户在不同模态和行为下的共性和特性兴趣。实验证明，M3BSR在公开数据集上显著优于现有先进方法，提升了偏好建模和推荐准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Multi-Modal_Multi-Behavior_Sequential_Recommendation_with_Conditional_Diffusion-Based_Feature_Denoising.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="134-EnergyPatchTST-Multi-scale-Time-Series-Transformers-with-Uncertainty-Estimation-for-Energy-Forecasting"><a href="#134-EnergyPatchTST-Multi-scale-Time-Series-Transformers-with-Uncertainty-Estimation-for-Energy-Forecasting" class="headerlink" title="134. EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting"></a>134. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EnergyPatchTST__Multi-scale_Time_Series_Transformers_with_Uncertainty_Estimation_for_Energy_Forecast.pdf">EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai University</span></p><p>该论文提出EnergyPatchTST模型，用于能源领域的时间序列预测。方法包括多尺度特征提取、蒙特卡罗Dropout不确定性估计、未来变量集成以及预训练-微调迁移学习，有效提升能源预测的准确性和可靠性。实验结果显示，在多个能源数据集上，该方法较主流方法误差降低7-12%，并提供了可靠的不确定性区间预测，显著促进了能源领域的时间序列分析性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EnergyPatchTST_Multi-scale_Time_Series_Transformers_with_Uncertainty_Estimation_for_Energy_Forecasting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="135-Harmonic-fractal-transformation-for-modeling-complex-neuronal-effects-from-bursting-and-noise-shaping-to-waveform-sensitivity-and-noise-induced-subthreshold-spiking"><a href="#135-Harmonic-fractal-transformation-for-modeling-complex-neuronal-effects-from-bursting-and-noise-shaping-to-waveform-sensitivity-and-noise-induced-subthreshold-spiking" class="headerlink" title="135. Harmonic fractal transformation for modeling complex neuronal effects: from bursting and noise shaping to waveform sensitivity and noise-induced subthreshold spiking"></a>135. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Harmonic_fractal_transformation_for_modeling_complex_neuronal_effects__from_bursting_and_noise_shapi.pdf">Harmonic fractal transformation for modeling complex neuronal effects: from bursting and noise shaping to waveform sensitivity and noise-induced subthreshold spiking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">90 Navigation street, Birmingham, UK</span></p><p>本文提出了谐波分形变换（HFT）作为一种全新的频率映射方法，能够以简单形式复刻神经元复杂效应，包括信号激发、噪声整形、波形敏感性和亚阈值噪声诱导脉冲。HFT通过分形重组输入频谱，激发新的谐波分量，实现对噪声的鲁棒性和信号放大，结论表明该方法能灵活建模多种脉冲形态与神经元特性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Harmonic_fractal_transformation_for_modeling_complex_neuronal_effects_from_bursting_and_noise_shaping_to_waveform_sensitivity_and_noise-induced_subthreshold_spiking.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="136-Estimating-Musical-Surprisal-from-Audio-in-Autoregressive-Diffusion-Model-Noise-Spaces"><a href="#136-Estimating-Musical-Surprisal-from-Audio-in-Autoregressive-Diffusion-Model-Noise-Spaces" class="headerlink" title="136. Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces"></a>136. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Estimating_Musical_Surprisal_from_Audio_in_Autoregressive_Diffusion_Model_Noise_Spaces.pdf">Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Johannes Kepler University Linz</span></p><p>该论文提出利用自回归扩散模型（EDM和RFF）在连续音频嵌入空间估计音乐惊奇性（surprisal），通过信息含量（IC）和负对数似然（NLL）衡量模型性能，并在单音高惊奇性和多轨音频分段检测两任务上与GIVT模型对比。实验结果显示，扩散模型在描述音乐数据和捕捉音高惊奇性方面优于GIVT模型，且在适当噪声水平下对音色具有更高不变性，提升分段检测精度与召回率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Estimating_Musical_Surprisal_from_Audio_in_Autoregressive_Diffusion_Model_Noise_Spaces.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="137-FlowState-Sampling-Rate-Invariant-Time-Series-Forecasting"><a href="#137-FlowState-Sampling-Rate-Invariant-Time-Series-Forecasting" class="headerlink" title="137. FlowState: Sampling Rate Invariant Time Series Forecasting"></a>137. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FlowState__Sampling_Rate_Invariant_Time_Series_Forecasting.pdf">FlowState: Sampling Rate Invariant Time Series Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IBM Research Europe – Zurich, Switzerland</span></p><p>该论文提出了FlowState，一种结合S5状态空间模型（SSM）编码器和功能基解码器（FBD）的时间序列基础模型，实现了对不同采样率的自适应与连续时间建模。通过并行预测训练方案和因果归一化，FlowState在GIFT-ZS和Chronos-ZS基准上达到新的SOTA，优于体积更大的模型，具备出色的泛化能力和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FlowState_Sampling_Rate_Invariant_Time_Series_Forecasting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="138-FDC-Net-Rethinking-the-association-between-EEG-artifact-removal-and-multi-dimensional-affective-computing"><a href="#138-FDC-Net-Rethinking-the-association-between-EEG-artifact-removal-and-multi-dimensional-affective-computing" class="headerlink" title="138. FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing"></a>138. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FDC-Net__Rethinking_the_association_between_EEG_artifact_removal_and_multi-dimensional_affective_com.pdf">FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing University of Technology</span></p><p>本文提出了一种端到端的EEG情感识别与去伪影协同框架FDC-Net，采用双向梯度反馈优化和频带可学习编码的Transformer（EEGSPTransformer），实现了去噪与情感识别的深度耦合与动态协作。实验结果表明，在强噪声干扰下，该方法在DEAP和DREAMER数据集上的去噪和情感识别性能均优于现有主流方法，验证了其鲁棒性和有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FDC-Net_Rethinking_the_association_between_EEG_artifact_removal_and_multi-dimensional_affective_computing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="139-Balancing-Accuracy-and-Novelty-with-Sub-Item-Popularity"><a href="#139-Balancing-Accuracy-and-Novelty-with-Sub-Item-Popularity" class="headerlink" title="139. Balancing Accuracy and Novelty with Sub-Item Popularity"></a>139. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Balancing_Accuracy_and_Novelty_with_Sub-Item_Popularity.pdf">Balancing Accuracy and Novelty with Sub-Item Popularity</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Politecnico di Bari</span></p><p>本文提出在Transformer-based推荐系统RecJPQ中引入个性化流行度信号，包括基于item-ID与sub-ID的个性化流行度分数（PPS和sPPS），通过对sub-ID流行度建模，实现更细粒度的用户重复行为捕捉。实验证明，sub-ID粒度的流行度建模在提升推荐新颖性的同时不损失准确性，并可灵活调控准确性与新颖性的权衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Balancing_Accuracy_and_Novelty_with_Sub-Item_Popularity.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="140-Hybrid-quantum-tensor-networks-for-aeroelastic-applications"><a href="#140-Hybrid-quantum-tensor-networks-for-aeroelastic-applications" class="headerlink" title="140. Hybrid quantum tensor networks for aeroelastic applications"></a>140. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Hybrid_quantum_tensor_networks_for_aeroelastic_applications.pdf">Hybrid quantum tensor networks for aeroelastic applications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute for AI Safety and Security, German Aerospace Center (DLR)</span></p><p>本文提出了一种面向气动弹性问题的混合量子张量网络算法，集成了张量网络（TN）降维、数据编码和张量网络启发的可训练变分量子电路（VQC），实现端到端训练。方法在气动弹性时序数据的二分类任务中取得了极高准确率（F1&gt;0.99），并在多&#x2F;单变量回归任务中表现出有前景的性能，但模型的超参数选择依然是主要挑战。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Hybrid_quantum_tensor_networks_for_aeroelastic_applications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="141-Salt-Rock-Creep-Deformation-Forecasting-Using-Deep-Neural-Networks-and-Analytical-Models-for-Subsurface-Energy-Storage-Applications"><a href="#141-Salt-Rock-Creep-Deformation-Forecasting-Using-Deep-Neural-Networks-and-Analytical-Models-for-Subsurface-Energy-Storage-Applications" class="headerlink" title="141. Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications"></a>141. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Salt-Rock_Creep_Deformation_Forecasting_Using_Deep_Neural_Networks_and_Analytical_Models_for_Subsurf.pdf">Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IIT (ISM) Dhanbad</span></p><p>该论文提出了融合深度神经网络（N-BEATS、TCN、RNN、Transformer）与分析模型（如Power Law、Burger等）的方法，对盐岩在不同压力下的蠕变变形进行时间序列预测。结果显示，深度神经网络（尤其N-BEATS和TCN）在预测精度上优于传统统计和分析模型，准确率提升15-20%，为地下盐穴能源储存的安全管理提供了更可靠的建模基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Salt-Rock_Creep_Deformation_Forecasting_Using_Deep_Neural_Networks_and_Analytical_Models_for_Subsurface_Energy_Storage_Applications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="142-High-Dimensional-Differentially-Private-Quantile-Regression-Distributed-Estimation-and-Statistical-Inference"><a href="#142-High-Dimensional-Differentially-Private-Quantile-Regression-Distributed-Estimation-and-Statistical-Inference" class="headerlink" title="142. High-Dimensional Differentially Private Quantile Regression: Distributed Estimation and Statistical Inference"></a>142. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/High-Dimensional_Differentially_Private_Quantile_Regression__Distributed_Estimation_and_Statistical_.pdf">High-Dimensional Differentially Private Quantile Regression: Distributed Estimation and Statistical Inference</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Statistics and Data Science, Shanghai University of Finance and Economics</span></p><p>该论文提出了一种适用于高维分布式数据的差分隐私分位数回归方法，核心包括利用Newton型变换将非光滑分位数回归问题转化为普通最小二乘问题，并结合Noisy Hard Thresholding算法实现迭代式分布式参数估计，保证稀疏性和差分隐私；在推断阶段，提出了差分隐私去偏估计和分布式私有bootstrap方法用于多重假设检验和置信区间构建。理论分析给出了估计误差界和推断一致性，实验证明算法在异质和重尾分布下具有稳健性和有效性，且揭示了隐私保护与统计精度之间的基本权衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/High-Dimensional_Differentially_Private_Quantile_Regression_Distributed_Estimation_and_Statistical_Inference.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="143-Graph-based-Event-Log-Repair"><a href="#143-Graph-based-Event-Log-Repair" class="headerlink" title="143. Graph-based Event Log Repair"></a>143. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Graph-based_Event_Log_Repair.pdf">Graph-based Event Log Repair</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Trento</span></p><p>该论文提出了SANAGRAPH，一种利用异构图神经网络（HGNN）编码事件日志痕迹并修复其中缺失数据的方法。方法将日志中的每个事件和属性映射为图节点，并通过SAGEConv卷积操作实现信息传播，使用多种损失函数同时优化分类和回归任务。实验对比了现有的自编码器方法，结果显示SANAGRAPH在重建活动和时间戳方面准确率更高，对所有属性的修复也表现良好，显著提升了流程挖掘数据质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Graph-based_Event_Log_Repair.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="144-Align-for-Fusion-Harmonizing-Triple-Preferences-via-Dual-oriented-Diffusion-for-Cross-domain-Sequential-Recommendation"><a href="#144-Align-for-Fusion-Harmonizing-Triple-Preferences-via-Dual-oriented-Diffusion-for-Cross-domain-Sequential-Recommendation" class="headerlink" title="144. Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation"></a>144. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Align-for-Fusion__Harmonizing_Triple_Preferences_via_Dual-oriented_Diffusion_for_Cross-domain_Sequen.pdf">Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Defense Technology</span></p><p>本文提出了一种面向跨域序列推荐的新框架HorizonRec，结合混合条件分布检索（MDR）和双向扩散偏好建模（DPD），利用扩散模型在源域、目标域和混合域间进行细粒度的用户兴趣融合。实验表明该方法在多个真实数据集上显著提升了跨域推荐的准确性和效率，验证了MDR和DPD模块对于多域偏好对齐及鲁棒建模的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Align-for-Fusion_Harmonizing_Triple_Preferences_via_Dual-oriented_Diffusion_for_Cross-domain_Sequential_Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="145-Q-DPTS-Quantum-Differentially-Private-Time-Series-Forecasting-via-Variational-Quantum-Circuits"><a href="#145-Q-DPTS-Quantum-Differentially-Private-Time-Series-Forecasting-via-Variational-Quantum-Circuits" class="headerlink" title="145. Q-DPTS: Quantum Differentially Private Time Series Forecasting via Variational Quantum Circuits"></a>145. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Q-DPTS__Quantum_Differentially_Private_Time_Series_Forecasting_via_Variational_Quantum_Circuits.pdf">Q-DPTS: Quantum Differentially Private Time Series Forecasting via Variational Quantum Circuits</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent Researcher</span></p><p>本文提出Q-DPTS框架，将变分量子电路（VQC）与微分隐私（Differential Privacy, DP）机制结合，用于时序数据预测。采用DP-SGD算法中的每样本梯度裁剪与高斯噪声注入，并用Rényi微分隐私会计，为量子-经典混合模型提供严格的隐私保障。实验在ETT数据集上，系统对比了LSTM、QASA、QRWKV、QLSTM等模型在不同隐私预算下的预测误差，结果表明，Q-DPTS量子模型在牺牲最小精度的前提下实现优良的隐私-效用权衡，尤其QASA和QRWKV在高噪声环境下仍优于经典模型。结论：量子差分隐私预测模型不仅可行且有效，能在敏感领域实现安全高效的时序建模。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Q-DPTS_Quantum_Differentially_Private_Time_Series_Forecasting_via_Variational_Quantum_Circuits.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="146-Fairness-in-Dysarthric-Speech-Synthesis-Understanding-Intrinsic-Bias-in-Dysarthric-Speech-Cloning-using-F5-TTS"><a href="#146-Fairness-in-Dysarthric-Speech-Synthesis-Understanding-Intrinsic-Bias-in-Dysarthric-Speech-Cloning-using-F5-TTS" class="headerlink" title="146. Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS"></a>146. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Fairness_in_Dysarthric_Speech_Synthesis__Understanding_Intrinsic_Bias_in_Dysarthric_Speech_Cloning_u.pdf">Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">International Institute of Information Technology-Hyderabad</span></p><p>本文提出了一个框架，利用F5-TTS零样本语音克隆技术和TORGO数据库，系统评估了合成dysarthric（构音障碍）语音在可懂度、说话人相似性和韵律保留方面的效果，并通过Parity Difference和Disparate Impact等公平性指标分析不同病情严重程度下的偏差。研究发现，F5-TTS对语音可懂度偏置较大，尤其对高严重度患者，导致数据增强时对相关辅助语音识别任务效果不佳，强调了在语音数据增强中需考虑公平性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Fairness_in_Dysarthric_Speech_Synthesis_Understanding_Intrinsic_Bias_in_Dysarthric_Speech_Cloning_using_F5-TTS.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="147-Disentangling-Bias-by-Modeling-Intra-and-Inter-modal-Causal-Attention-for-Multimodal-Sentiment-Analysis"><a href="#147-Disentangling-Bias-by-Modeling-Intra-and-Inter-modal-Causal-Attention-for-Multimodal-Sentiment-Analysis" class="headerlink" title="147. Disentangling Bias by Modeling Intra- and Inter-modal Causal Attention for Multimodal Sentiment Analysis"></a>147. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Disentangling_Bias_by_Modeling_Intra-_and_Inter-modal_Causal_Attention_for_Multimodal_Sentiment_Anal.pdf">Disentangling Bias by Modeling Intra- and Inter-modal Causal Attention for Multimodal Sentiment Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science, South China Normal University</span></p><p>本文提出了一种多关系多模态因果干预模型（MMCI），利用因果理论中的背门调整机制，通过构建多关系图和图注意力网络显式建模文本、音频、视觉模态间的内外部依赖，分别估算和解耦因果特征与捷径特征，并动态结合以抵御数据分布偏移。实验表明，MMCI在多项基准和OOD测试数据上有效去偏、提升了多模态情感分析的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Disentangling_Bias_by_Modeling_Intra-_and_Inter-modal_Causal_Attention_for_Multimodal_Sentiment_Analysis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="148-Compressed-Decentralized-Momentum-Stochastic-Gradient-Methods-for-Nonconvex-Optimization"><a href="#148-Compressed-Decentralized-Momentum-Stochastic-Gradient-Methods-for-Nonconvex-Optimization" class="headerlink" title="148. Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization"></a>148. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Compressed_Decentralized_Momentum_Stochastic_Gradient_Methods_for_Nonconvex_Optimization.pdf">Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Rensselaer Polytechnic Institute</span></p><p>本文提出了两种用于多智能体非凸随机优化的压缩去中心化动量随机梯度算法（DAMSCo和DaSHCo），分别针对梯度有界和数据异质两种场景，结合了动量&#x2F;自适应技术与通信压缩以加速收敛并降低通信成本。理论上，两种方法均证明能达到最优收敛速率，且在一定误差容忍区间内实现线性加速和算法参数的拓扑无关性，实验上在训练深度神经网络和Transformer时优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Compressed_Decentralized_Momentum_Stochastic_Gradient_Methods_for_Nonconvex_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="149-MENDR-Manifold-Explainable-Neural-Data-Representations"><a href="#149-MENDR-Manifold-Explainable-Neural-Data-Representations" class="headerlink" title="149. MENDR: Manifold Explainable Neural Data Representations"></a>149. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MENDR__Manifold_Explainable_Neural_Data_Representations.pdf">MENDR: Manifold Explainable Neural Data Representations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Georgia Institute of Technology</span></p><p>该论文提出MENDR，一种用于脑电（EEG）信号的基础模型，创新性地结合离散小波包变换、图神经网络（GNN）空间归一化、自监督对比学习和Riemannian流形Transformer结构，实现对EEG信号的可解释和高效表示。实验显示，MENDR在多个EEG下游任务上参数量更低但表现接近主流模型，同时提升了表示可解释性和重建能力，展现出高效、可解释、临床适用的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MENDR_Manifold_Explainable_Neural_Data_Representations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2024/01/04报纸</title>
    <link href="/2024/01/2024-01-04_article/"/>
    <url>/2024/01/2024-01-04_article/</url>
    
    <content type="html"><![CDATA[<center><h1>2024/01/04报纸</h1></center><p>📅 <strong>日期</strong>：2024-01-04<br>📄 <strong>发现论文数量</strong>：0  </p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
