<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>今日论文报纸</title>
    <link href="/2025/08/2025-08-19_article/"/>
    <url>/2025/08/2025-08-19_article/</url>
    
    <content type="html"><![CDATA[<center><h1>今日论文报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-19<br>📄 <strong>发现论文数量</strong>：98  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Optimal-CO2-storage-management-considering-safety-constraints-in-multi-stakeholder-multi-site-CCS-projects-a-game-theoretic-perspective"><a href="#1-Optimal-CO2-storage-management-considering-safety-constraints-in-multi-stakeholder-multi-site-CCS-projects-a-game-theoretic-perspective" class="headerlink" title="1. Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Optimal_CO2_storage_management_considering_safety_constraints_in_multi-stakeholder_multi-site_CCS_pr.pdf">Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bureau of Economic Geology, University of Texas at Austin</span></p><p>本文提出了一种基于安全多智能体深度确定性策略梯度（MADDPG）和受限Markov博弈的框架，用于多利益相关方地质碳储存（GCS）项目的优化管理。方法结合Embed-to-Control（E2C）神经网络代理模型和分区奖励&#x2F;惩罚机制，实现了在安全约束下各方目标的最优兼顾。实验结果表明，该方法在协作与竞争场景下均能有效提升整体经济回报，确保注入安全，并优于传统多目标优化算法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Optimal_CO2_storage_management_considering_safety_constraints_in_multi-stakeholder_multi-site_CCS_projects_a_game_theoretic_perspective.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-SeamlessFlow-A-Trainer–Agent-Isolation-RL-Framework-Achieving-Bubble-Free-Pipelines-via-Tag-Scheduling"><a href="#2-SeamlessFlow-A-Trainer–Agent-Isolation-RL-Framework-Achieving-Bubble-Free-Pipelines-via-Tag-Scheduling" class="headerlink" title="2. SeamlessFlow: A Trainer–Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/SeamlessFlow__A_Trainer_Agent_Isolation_RL_Framework_Achieving_Bubble-Free_Pipelines_via_Tag_Schedul.pdf">SeamlessFlow: A Trainer–Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou</span></p><p>SeamlessFlow提出了一种服务器端强化学习框架，通过数据面层实现RL训练与多样化Agent的彻底隔离，并通过标签驱动的资源调度，将硬件抽象为能力标签资源，实现统一的空间-时间复用管道。该方法消除了分布式系统中的管道空闲，显著提升了大规模多Agent RL任务的训练效率和稳定性。实验显示其在工业级异构集群上具备极高的吞吐和可扩展性，显著优于现有主流RL框架。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SeamlessFlow_A_Trainer%E2%80%93Agent_Isolation_RL_Framework_Achieving_Bubble-Free_Pipelines_via_Tag_Scheduling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Fusing-Rewards-and-Preferences-in-Reinforcement-Learning"><a href="#3-Fusing-Rewards-and-Preferences-in-Reinforcement-Learning" class="headerlink" title="3. Fusing Rewards and Preferences in Reinforcement Learning"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Fusing_Rewards_and_Preferences_in_Reinforcement_Learning.pdf">Fusing Rewards and Preferences in Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer and Communication Sciences, EPFL</span></p><p>本文提出了Dual-Feedback Actor (DFA)算法，将数值奖励和成对偏好反馈统一到单一的策略更新规则中，关键技术包括直接利用策略的对数概率建模偏好概率、无须奖励建模、支持on-&#x2F;off-policy学习，以及通过Q值合成偏好对。理论上，DFA的偏好损失最小化等价于恢复熵正则化Soft Actor-Critic (SAC)策略，实验表明在六个控制任务和GridWorld环境中，DFA的性能与SAC相当或优于SAC，对比奖励建模的RLHF基线表现更优且训练更平稳。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Fusing_Rewards_and_Preferences_in_Reinforcement_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-HOID-R1-Reinforcement-Learning-for-Open-World-Human-Object-Interaction-Detection-Reasoning-with-Multimodal-Large-Language-Model"><a href="#4-HOID-R1-Reinforcement-Learning-for-Open-World-Human-Object-Interaction-Detection-Reasoning-with-Multimodal-Large-Language-Model" class="headerlink" title="4. HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/HOID-R1__Reinforcement_Learning_for_Open-World_Human-Object_Interaction_Detection_Reasoning_with_Mul.pdf">HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ShanghaiTech University</span></p><p>本文提出了HOID-R1，一种结合链式思维（CoT）监督微调和群组相对策略优化（GRPO）的强化学习框架，实现开放世界下的人体-物体交互（HOI）检测。该方法利用多模态大语言模型作为判官监督推理过程，通过多重奖励信号优化策略，并有效缓解推理幻觉，显著提升了模型在各类开放词汇场景下的泛化和检测准确性。在HICO-DET和SWIG-HOI数据集上的实验结果表明，HOID-R1在已见和未见场景均超越现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/HOID-R1_Reinforcement_Learning_for_Open-World_Human-Object_Interaction_Detection_Reasoning_with_Multimodal_Large_Language_Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="5-Investigating-Sensors-and-Methods-in-Grasp-State-Classification-in-Agricultural-Manipulation"><a href="#5-Investigating-Sensors-and-Methods-in-Grasp-State-Classification-in-Agricultural-Manipulation" class="headerlink" title="5. Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Investigating_Sensors_and_Methods_in_Grasp_State_Classification_in_Agricultural_Manipulation.pdf">Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois at Urbana-Champaign</span></p><p>本文提出了一种用于农业抓取状态分类的多传感器系统，将IMU、红外、张力、触觉和RGB摄像头集成到顺应性机械手中，并利用随机森林和LSTM模型对抓取状态（滑移、抓取失败、成功采摘等）进行分类。实验结果表明，随机森林模型在实验室训练并迁移到真实番茄植株上，能以100%准确率识别多种抓取状态，IMU与张力传感器的组合已足以实现高效分类，这为果实自动采摘提供了实时反馈和纠正动作的依据。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Investigating_Sensors_and_Methods_in_Grasp_State_Classification_in_Agricultural_Manipulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-Visual-Perception-Engine-Fast-and-Flexible-Multi-Head-Inference-for-Robotic-Vision-Tasks"><a href="#6-Visual-Perception-Engine-Fast-and-Flexible-Multi-Head-Inference-for-Robotic-Vision-Tasks" class="headerlink" title="6. Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Visual_Perception_Engine__Fast_and_Flexible_Multi-Head_Inference_for_Robotic_Vision_Tasks.pdf">Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Jet Propulsion Laboratory, California Institute of Technology</span></p><p>本文提出了VPEngine框架，通过共享视觉基础模型（如DINOv2）骨干，实现多任务（深度估计、目标检测、语义分割）模型头高效并行推理，采用CUDA MPS和GPU内存共享结构，显著提升推理速度和GPU利用率。实验表明，该方法在NVIDIA Jetson Orin AGX上可实现高达3.3倍速度提升，且具备可动态调整任务频率、内存可控、模块化扩展性，适合多种机器人视觉场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Visual_Perception_Engine_Fast_and_Flexible_Multi-Head_Inference_for_Robotic_Vision_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Towards-Fully-Onboard-State-Estimation-and-Trajectory-Tracking-for-UAVs-with-Suspended-Payloads"><a href="#7-Towards-Fully-Onboard-State-Estimation-and-Trajectory-Tracking-for-UAVs-with-Suspended-Payloads" class="headerlink" title="7. Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Towards_Fully_Onboard_State_Estimation_and_Trajectory_Tracking_for_UAVs_with_Suspended_Payloads.pdf">Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Czech Technical University in Prague</span></p><p>该论文提出了一种面向无人机悬挂载荷的全自主、实用、低硬件需求的状态估计与轨迹跟踪框架。方法采用标准机载RTK GNSS与IMU，结合线性卡尔曼滤波器进行状态估计、模型预测轮廓控制（MPCC）规划平滑轨迹，并用增量式模型预测控制（MPC）实现鲁棒闭环控制。大量仿真和实地实验表明，该系统在仅用标准传感器的条件下，表现接近理想地面真值基准，跟踪误差增加小于6%，且对载荷参数变化具有很强鲁棒性，展示了面向实际无人机悬挂运输的可行性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Towards_Fully_Onboard_State_Estimation_and_Trajectory_Tracking_for_UAVs_with_Suspended_Payloads.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-MultiPark-Multimodal-Parking-Transformer-with-Next-Segment-Prediction"><a href="#8-MultiPark-Multimodal-Parking-Transformer-with-Next-Segment-Prediction" class="headerlink" title="8. MultiPark: Multimodal Parking Transformer with Next-Segment Prediction"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/MultiPark__Multimodal_Parking_Transformer_with_Next-Segment_Prediction.pdf">MultiPark: Multimodal Parking Transformer with Next-Segment Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了MultiPark，一种基于自回归Transformer的多模态自动泊车方法。其方法创新包括数据高效的next-segment预测范式、结合齿轮&#x2F;纵向&#x2F;横向因子的可学习泊车查询、并行多模态轨迹解码，以及两阶段训练策略（模仿损失+目标导向损失）以缓解因果混淆并提升泛化能力。实验表明MultiPark在真实数据集和实际车辆部署中均取得了优异的泊车成功率和安全性，显著优于同类方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MultiPark_Multimodal_Parking_Transformer_with_Next-Segment_Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-Sim2Dust-Mastering-Dynamic-Waypoint-Tracking-on-Granular-Media"><a href="#9-Sim2Dust-Mastering-Dynamic-Waypoint-Tracking-on-Granular-Media" class="headerlink" title="9. Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Sim2Dust__Mastering_Dynamic_Waypoint_Tracking_on_Granular_Media.pdf">Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Luxembourg</span></p><p>本论文提出了一个完整的sim-to-real框架，用于在类月球颗粒介质上实现轮式机器人动态航点跟踪。方法上，利用Space Robotics Bench进行大规模并行仿真，结合程序化生成环境与物理参数域随机化，采用DreamerV3等强化学习算法训练策略，并通过零样本迁移在真实月壤实验场地实现精确稳定的导航。实验结果表明，程序化多样性训练显著提升了真实部署的泛化和稳定性，模型基DreamerV3在样本效率和精度上表现最佳，简单动作平滑可有效提升硬件部署的安全性和稳定性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Sim2Dust_Mastering_Dynamic_Waypoint_Tracking_on_Granular_Media.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-Relative-Position-Matters-Trajectory-Prediction-and-Planning-with-Polar-Representation"><a href="#10-Relative-Position-Matters-Trajectory-Prediction-and-Planning-with-Polar-Representation" class="headerlink" title="10. Relative Position Matters: Trajectory Prediction and Planning with Polar Representation"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Relative_Position_Matters__Trajectory_Prediction_and_Planning_with_Polar_Representation.pdf">Relative Position Matters: Trajectory Prediction and Planning with Polar Representation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出Polaris框架，首次在轨迹预测与规划中采用极坐标系，通过Polar场景上下文编码和Polar关系细化模块，并引入Relative Embedding Transformer显式建模交通要素间的相对距离与方向。实验在Argoverse 2和nuPlan数据集上表明，该方法在预测准确性和推理效率上均达到SOTA水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Relative_Position_Matters_Trajectory_Prediction_and_Planning_with_Polar_Representation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-i2Nav-Robot-A-Large-Scale-Indoor-Outdoor-Robot-Dataset-for-Multi-Sensor-Fusion-Navigation-and-Mapping"><a href="#11-i2Nav-Robot-A-Large-Scale-Indoor-Outdoor-Robot-Dataset-for-Multi-Sensor-Fusion-Navigation-and-Mapping" class="headerlink" title="11. i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/i2Nav-Robot__A_Large-Scale_Indoor-Outdoor_Robot_Dataset_for_Multi-Sensor_Fusion_Navigation_and_Mappi.pdf">i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>该论文提出了i2Nav-Robot，一个用于室内外环境多传感器融合导航与建图的大规模数据集。方法包括集成最新固态激光雷达、4D毫米波雷达、立体摄像头、里程计、GNSS和高精度IMU于全向轮式机器人，并采用硬件和离线校准实现微秒级时间同步，采集覆盖多样场景的十条长序列，提供厘米级高频地面真实数据。结论是该数据集已被十余种开源多传感器系统验证，数据质量优异，可广泛用于机器人导航、SLAM及融合方法研究。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/i2Nav-Robot_A_Large-Scale_Indoor-Outdoor_Robot_Dataset_for_Multi-Sensor_Fusion_Navigation_and_Mapping.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-OVSegDT-Segmenting-Transformer-for-Open-Vocabulary-Object-Goal-Navigation"><a href="#12-OVSegDT-Segmenting-Transformer-for-Open-Vocabulary-Object-Goal-Navigation" class="headerlink" title="12. OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/OVSegDT__Segmenting_Transformer_for_Open-Vocabulary_Object_Goal_Navigation.pdf">OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AIRI</span></p><p>本文提出OVSegDT，一种轻量级transformer策略，通过集成目标二值分割掩码编码器和辅助分割损失，实现对开放词汇物体目标导航任务的高效学习。方法还引入熵自适应损失调节（EALM），根据策略熵自动平衡模仿学习与强化学习信号，无需手动切换阶段。实验结果表明，该方法在未见类别上导航性能与见过类别一致，训练样本复杂度降低33%，碰撞次数减少一半，且优于现有RGB-only和部分大模型方法。结论：OVSegDT显著提升了机器人在新环境下对未知物体类别的导航能力，具备高泛化性和安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/OVSegDT_Segmenting_Transformer_for_Open-Vocabulary_Object_Goal_Navigation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-ReachVox-Clutter-free-Reachability-Visualization-for-Robot-Motion-Planning-in-Virtual-Reality"><a href="#13-ReachVox-Clutter-free-Reachability-Visualization-for-Robot-Motion-Planning-in-Virtual-Reality" class="headerlink" title="13. ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ReachVox__Clutter-free_Reachability_Visualization_for_Robot_Motion_Planning_in_Virtual_Reality.pdf">ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Coburg University of Applied Sciences and Arts</span></p><p>该论文提出了一种名为REACHVOX的新型可视化方法，通过在虚拟现实中以半透明有色体素的方式显示机器人在任务空间的可达区域，显著减少了视觉杂乱，帮助远程操作者高效规划机器人运动。用户实验表明，REACHVOX相比传统点可达性检查，在任务完成时间、操作次数、易用性和信任度等主客观指标上均有显著提升，用户更倾向于使用该方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ReachVox_Clutter-free_Reachability_Visualization_for_Robot_Motion_Planning_in_Virtual_Reality.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Optimizing-ROS-2-Communication-for-Wireless-Robotic-Systems"><a href="#14-Optimizing-ROS-2-Communication-for-Wireless-Robotic-Systems" class="headerlink" title="14. Optimizing ROS 2 Communication for Wireless Robotic Systems"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Optimizing_ROS_2_Communication_for_Wireless_Robotic_Systems.pdf">Optimizing ROS 2 Communication for Wireless Robotic Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">DGIST</span></p><p>该论文针对无线环境下ROS 2 DDS通信栈在大数据载体传输时的瓶颈（如IP分片、重传时机、缓冲区突发）进行了系统建模和网络层深入分析，并提出了一种基于标准XML QoS配置的轻量级DDS优化框架，无需协议或应用修改即可提升无线通信的鲁棒性和实时性。实验证明该方法在各种无线场景下显著提高了数据接收率、降低了延迟和抖动，尤其在高丢包和链路中断条件下表现优异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Optimizing_ROS_2_Communication_for_Wireless_Robotic_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-CRAFT-GUI-Curriculum-Reinforced-Agent-For-GUI-Tasks"><a href="#15-CRAFT-GUI-Curriculum-Reinforced-Agent-For-GUI-Tasks" class="headerlink" title="15. CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CRAFT-GUI__Curriculum-Reinforced_Agent_For_GUI_Tasks.pdf">CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ant Group</span></p><p>该论文提出了CRAFT-GUI，一种结合分阶段课程学习和分组相对策略优化（GRPO）的强化学习框架，专门针对图形用户界面（GUI）任务。方法通过难度分层的任务训练和细粒度混合奖励机制提升了agent在移动设备上的操作与理解能力。实验结果表明，CRAFT-GUI在多个公开和内部基准上超过现有方法，任务成功率提升5.6%和10.3%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CRAFT-GUI_Curriculum-Reinforced_Agent_For_GUI_Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-Swarm-in-Blocks-Simplifying-Drone-Swarm-Programming-with-Block-Based-Language"><a href="#16-Swarm-in-Blocks-Simplifying-Drone-Swarm-Programming-with-Block-Based-Language" class="headerlink" title="16. Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Swarm-in-Blocks__Simplifying_Drone_Swarm_Programming_with_Block-Based_Language.pdf">Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Sao Paulo</span></p><p>本文提出了Swarm-in-Blocks，一个基于模块化（Block-Based）编程语言的无人机集群高层接口，用户可通过拼接代码块轻松实现无人机编队的循环、条件与传感器操作，降低了ROS等底层知识门槛。该平台在Clover系统基础上，具备直观用户界面、实时监控、碰撞规避、队形与变换等关键技术，并已在仿真和实际应用中表现出良好可扩展性与易用性，适用于工业、农业、娱乐和教育等多场景。结论认为，该系统极大简化了无人机集群编程，提高了非专业用户和教育领域的可及性，未来将继续优化与拓展应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Swarm-in-Blocks_Simplifying_Drone_Swarm_Programming_with_Block-Based_Language.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-ImagiDrive-A-Unified-Imagination-and-Planning-Framework-for-Autonomous-Driving"><a href="#17-ImagiDrive-A-Unified-Imagination-and-Planning-Framework-for-Autonomous-Driving" class="headerlink" title="17. ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ImagiDrive__A_Unified_Imagination-and-Planning_Framework_for_Autonomous_Driving.pdf">ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出ImagiDrive，一个将视觉语言模型（VLM）与驾驶世界模型（DWM）集成的自动驾驶端到端框架，通过想象与规划循环实现更安全高效的驾驶决策。方法包括VLM驱动的多模态驾驶代理、DWM驱动的场景想象器、早停和轨迹选择机制，框架在nuScenes和NAVSIM等数据集上显著优于现有方法。结论：ImagiDrive能提升复杂环境下的安全性和规划表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ImagiDrive_A_Unified_Imagination-and-Planning_Framework_for_Autonomous_Driving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-Pedestrian-Dead-Reckoning-using-Invariant-Extended-Kalman-Filter"><a href="#18-Pedestrian-Dead-Reckoning-using-Invariant-Extended-Kalman-Filter" class="headerlink" title="18. Pedestrian Dead Reckoning using Invariant Extended Kalman Filter"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Pedestrian_Dead_Reckoning_using_Invariant_Extended_Kalman_Filter.pdf">Pedestrian Dead Reckoning using Invariant Extended Kalman Filter</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ShanghaiTech University</span></p><p>本文提出了一种面向双足机器人在无GPS环境下的低成本惯性行人航位推算方法，核心采用基于矩阵李群理论的InEKF（不变扩展卡尔曼滤波器），利用足部IMU的静止伪测量实现状态创新，有效提升估计精度。实验表明，在人体与双足机器人上，所提InEKF方法优于传统EKF，表现为轨迹误差更小、参数调优更容易，且对噪声协方差的敏感性更低。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Pedestrian_Dead_Reckoning_using_Invariant_Extended_Kalman_Filter.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Tactile-Robotics-An-Outlook"><a href="#19-Tactile-Robotics-An-Outlook" class="headerlink" title="19. Tactile Robotics: An Outlook"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Tactile_Robotics__An_Outlook.pdf">Tactile Robotics: An Outlook</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">King’s College London</span></p><p>本论文系统性回顾了机器人触觉领域的关键技术，包括触觉材料、传感器、传感网络、仿真、数据采集与解释、多模态融合，以及主动触觉感知方法。文章总结认为，未来机器人触觉的发展需整合新型材料与高密度分布式传感、仿真工具、机器学习算法和多模态感知，推动机器人实现类人触觉灵巧，拓展其在制造、医疗、农业等多领域的应用。结论指出，尽管已取得显著进展，但触觉机器人的传感集成、数据处理、能量供给与大规模网络等仍具挑战，未来需跨学科协作持续创新以实现人机自然交互和环境敏感性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Tactile_Robotics_An_Outlook.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-Multi-Group-Equivariant-Augmentation-for-Reinforcement-Learning-in-Robot-Manipulation"><a href="#20-Multi-Group-Equivariant-Augmentation-for-Reinforcement-Learning-in-Robot-Manipulation" class="headerlink" title="20. Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Multi-Group_Equivariant_Augmentation_for_Reinforcement_Learning_in_Robot_Manipulation.pdf">Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong</span></p><p>该论文提出了一种基于多组非等距对称性的POMDP新建模方法，并设计了多组等变数据增强（MEA）策略，通过在空间和时间维度上对操作轨迹施加独立群变换，实现了更高多样性和有效性的数据扩充，结合离线强化学习和基于体素的视觉等变表征，显著提升了机器人操作任务的采样效率和泛化能力。实验结果表明，该方法在通用操作和外科抓取两大机器人领域均大幅提升了学习收敛速度与成功率，减少了97.5%以上的示范数据需求并降低训练时间超过58%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Multi-Group_Equivariant_Augmentation_for_Reinforcement_Learning_in_Robot_Manipulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-Visuomotor-Grasping-with-World-Models-for-Surgical-Robots"><a href="#21-Visuomotor-Grasping-with-World-Models-for-Surgical-Robots" class="headerlink" title="21. Visuomotor Grasping with World Models for Surgical Robots"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Visuomotor_Grasping_with_World_Models_for_Surgical_Robots.pdf">Visuomotor Grasping with World Models for Surgical Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Chinese University of Hong Kong</span></p><p>本文提出了GASv2框架，结合世界模型（DreamerV2）、专用视觉感知管线和混合控制策略，实现了在模拟环境中训练的视觉运动策略可直接迁移至实际外科机器人，仅依赖标准立体内窥镜视觉输入。实验表明，该方法在虚拟和实际外科场景中抓取成功率达65%，对未见物体、抓手及多类干扰具有强泛化和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Visuomotor_Grasping_with_World_Models_for_Surgical_Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-Actor-Critic-for-Continuous-Action-Chunks-A-Reinforcement-Learning-Framework-for-Long-Horizon-Robotic-Manipulation-with-Sparse-Reward"><a href="#22-Actor-Critic-for-Continuous-Action-Chunks-A-Reinforcement-Learning-Framework-for-Long-Horizon-Robotic-Manipulation-with-Sparse-Reward" class="headerlink" title="22. Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Actor-Critic_for_Continuous_Action_Chunks__A_Reinforcement_Learning_Framework_for_Long-Horizon_Robot.pdf">Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出了AC3，一种新颖的强化学习框架，针对长时序、稀疏奖励的机器人操作任务，直接学习生成高维连续动作序列。方法包含基于DDPG的actor-critic结构：actor采用不对称更新，仅从成功轨迹学习以保证策略稳定改进；critic利用intra-chunk n-step返回和自监督奖励模块提升学习稳定性和数据效率。实验证明，AC3在BiGym和RLBench 25个任务上，仅用少量专家演示和简单模型结构即优于现有方法，展现出在复杂操作任务中的高效稳定性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Actor-Critic_for_Continuous_Action_Chunks_A_Reinforcement_Learning_Framework_for_Long-Horizon_Robotic_Manipulation_with_Sparse_Reward.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-Geometry-Aware-Predictive-Safety-Filters-on-Humanoids-From-Poisson-Safety-Functions-to-CBF-Constrained-MPC"><a href="#23-Geometry-Aware-Predictive-Safety-Filters-on-Humanoids-From-Poisson-Safety-Functions-to-CBF-Constrained-MPC" class="headerlink" title="23. Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Geometry-Aware_Predictive_Safety_Filters_on_Humanoids__From_Poisson_Safety_Functions_to_CBF_Constrai.pdf">Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">California Institute of Technology</span></p><p>本文提出了一种基于非线性模型预测控制（MPC）与控制屏障函数（CBF）的几何感知预测安全滤波方法，针对动态和非结构化环境中的人形及四足机器人安全导航。方法通过扩展泊松安全函数，将其定义于时间和姿态的高维空间，并结合MPC实现实时安全约束轨迹生成，且利用Minkowski运算精确建模机器人几何。实验在多种安全关键场景下验证了该方法对机器人安全避障和复杂环境导航的有效性和实时性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Geometry-Aware_Predictive_Safety_Filters_on_Humanoids_From_Poisson_Safety_Functions_to_CBF_Constrained_MPC.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="24-LoRAtorio-An-intrinsic-approach-to-LoRA-Skill-Composition"><a href="#24-LoRAtorio-An-intrinsic-approach-to-LoRA-Skill-Composition" class="headerlink" title="24. LoRAtorio: An intrinsic approach to LoRA Skill Composition"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/LoRAtorio__An_intrinsic_approach_to_LoRA_Skill_Composition.pdf">LoRAtorio: An intrinsic approach to LoRA Skill Composition</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Cambridge Research Laboratory, Toshiba Europe</span></p><p>本文提出了一种无需训练的多LoRA技能组合方法LoRAtorio，通过在扩散模型的潜空间按空间patch划分，并计算LoRA与基础模型每个patch的余弦相似度，动态构建权重矩阵实现多LoRA输出的加权融合；同时在分类器自由引导机制中引入基础模型的无条件分数以缓解领域漂移。实验显示该方法在ComposLoRA基准和动态模块选择场景下均取得SOTA，提升了多概念生成的鲁棒性和图像质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LoRAtorio_An_intrinsic_approach_to_LoRA_Skill_Composition.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-DashCam-Video-A-complementary-low-cost-data-stream-for-on-demand-forest-infrastructure-system-monitoring"><a href="#25-DashCam-Video-A-complementary-low-cost-data-stream-for-on-demand-forest-infrastructure-system-monitoring" class="headerlink" title="25. DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/DashCam_Video__A_complementary_low-cost_data_stream_for_on-demand_forest-infrastructure_system_monit.pdf">DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Connecticut</span></p><p>本论文提出了一种基于车辆仪表盘摄像头（dashcam）视频的低成本、可扩展的城市道路绿化和基础设施实时监测框架。方法包含单目深度估计（Depth-anything V2模型）、XGBoost深度修正、GPS与图像三角测量定位、以及基于针孔相机几何的对象结构参数提取。实验结果显示，在内置摄像头和低速行驶条件下，定位误差平均为2.83米，结构参数（如高度）估计误差最低，并对树木等复杂目标仍有一定误差但整体表现优异。结论认为，该方法具备实时、低成本、自动化监测城市绿地与基础设施的潜力，适用于城市管理和智能交通等场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/DashCam_Video_A_complementary_low-cost_data_stream_for_on-demand_forest-infrastructure_system_monitoring.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-TrajSV-A-Trajectory-based-Model-for-Sports-Video-Representations-and-Applications"><a href="#26-TrajSV-A-Trajectory-based-Model-for-Sports-Video-Representations-and-Applications" class="headerlink" title="26. TrajSV: A Trajectory-based Model for Sports Video Representations and Applications"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/TrajSV__A_Trajectory-based_Model_for_Sports_Video_Representations_and_Applications.pdf">TrajSV: A Trajectory-based Model for Sports Video Representations and Applications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Technologies, Co., Ltd.</span></p><p>本文提出了TrajSV，一种基于轨迹的体育视频表征框架，包括数据预处理、轨迹增强Transformer模块的Clip Representation Network（CRNet）、以及聚合可视与轨迹特征的Video Representation Network（VRNet）。TrajSV采用三重对比损失在无监督方式下优化视频和片段表征，在体育视频检索、动作定位、视频字幕等多个任务上取得最优性能，显著提升了检索准确率和下游任务效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/TrajSV_A_Trajectory-based_Model_for_Sports_Video_Representations_and_Applications.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-Reinforcing-Video-Reasoning-Segmentation-to-Think-Before-It-Segments"><a href="#27-Reinforcing-Video-Reasoning-Segmentation-to-Think-Before-It-Segments" class="headerlink" title="27. Reinforcing Video Reasoning Segmentation to Think Before It Segments"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Reinforcing_Video_Reasoning_Segmentation_to_Think_Before_It_Segments.pdf">Reinforcing Video Reasoning Segmentation to Think Before It Segments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Dalian University of Technology</span></p><p>该论文提出Veason-R1模型，通过两阶段训练实现视频推理分割：首先利用链式思考(CoT)数据进行有监督微调，赋予模型分层推理能力，然后采用基于GRPO的强化学习进一步优化时空定位和分割表现。结果表明，Veason-R1在多个基准上取得了领先性能，显著提升了分割准确性与鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Reinforcing_Video_Reasoning_Segmentation_to_Think_Before_It_Segments.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-Handwritten-Text-Recognition-of-Historical-Manuscripts-Using-Transformer-Based-Models"><a href="#28-Handwritten-Text-Recognition-of-Historical-Manuscripts-Using-Transformer-Based-Models" class="headerlink" title="28. Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Handwritten_Text_Recognition_of_Historical_Manuscripts_Using_Transformer-Based_Models.pdf">Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mississippi State University</span></p><p>本文采用TrOCR这一Transformer架构的手写文本识别模型，针对16世纪拉丁文历史手稿，提出并系统评估了多种针对性图像预处理与数据增强技术（包括四种新设计的历史手稿专用增强方法），并结合集成学习方法以提升识别精度。实验结果表明，Elastic Distortion增强和Top-5投票集成显著提升了字符错误率（CER），新方法在Gwalther数据集上CER降至1.60，相比先前最佳结果提升达50%，有效推动了历史手稿自动识别领域的性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Handwritten_Text_Recognition_of_Historical_Manuscripts_Using_Transformer-Based_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Data-Driven-Deepfake-Image-Detection-Method-The-2024-Global-Deepfake-Image-Detection-Challenge"><a href="#29-Data-Driven-Deepfake-Image-Detection-Method-The-2024-Global-Deepfake-Image-Detection-Challenge" class="headerlink" title="29. Data-Driven Deepfake Image Detection Method - The 2024 Global Deepfake Image Detection Challenge"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Data-Driven_Deepfake_Image_Detection_Method_--_The_2024_Global_Deepfake_Image_Detection_Challenge.pdf">Data-Driven Deepfake Image Detection Method - The 2024 Global Deepfake Image Detection Challenge</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AI Innovation Center, China Unicom</span></p><p>本文提出了一种基于Swin Transformer V2-B分类网络的深度伪造图像检测方法，通过多样的数据增强（如随机人脸区域Cutout、本地裁剪、卡通化等）和样本生成，提升模型泛化能力和鲁棒性。实验结果显示，该方法在多样化的深度伪造攻击下表现出较好的检测准确率和鲁棒性，最终在全球挑战赛中获得优异成绩。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Data-Driven_Deepfake_Image_Detection_Method_-_The_2024_Global_Deepfake_Image_Detection_Challenge.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-RMFAT-Recurrent-Multi-scale-Feature-Atmospheric-Turbulence-Mitigator"><a href="#30-RMFAT-Recurrent-Multi-scale-Feature-Atmospheric-Turbulence-Mitigator" class="headerlink" title="30. RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/RMFAT__Recurrent_Multi-scale_Feature_Atmospheric_Turbulence_Mitigator.pdf">RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Bristol</span></p><p>该论文提出了一种轻量级循环多尺度特征大气湍流缓解模型（RMFAT），通过将多帧视频恢复任务转化为双帧循环推理，结合多尺度特征提取、时序对齐和频率&#x2F;语义指导损失，有效提升了大气湍流视频的空间细节和时序一致性。实验表明，RMFAT在合成和真实数据集上均优于现有方法，恢复质量最佳且推理速度提升4倍以上，适合实时和资源受限场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/RMFAT_Recurrent_Multi-scale_Feature_Atmospheric_Turbulence_Mitigator.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration"><a href="#31-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration" class="headerlink" title="31. G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/G-CUT3R__Guided_3D_Reconstruction_with_Camera_and_Depth_Prior_Integration.pdf">G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Skoltech</span></p><p>该论文提出G-CUT3R，一种基于CUT3R的轻量级、模态无关的前馈式3D场景重建方法，通过整合深度、相机内参和位姿等先验信息，有效提升重建精度。方法采用多模态编码与ZeroConv融合技术，使模型可灵活整合多种数据源，实验表明相比当前主流方法在多项3D重建和相关任务上均实现性能提升，结论是G-CUT3R能生成更准确、细致的3D场景重建结果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/G-CUT3R_Guided_3D_Reconstruction_with_Camera_and_Depth_Prior_Integration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-Leveraging-the-RETFound-foundation-model-for-optic-disc-segmentation-in-retinal-images"><a href="#32-Leveraging-the-RETFound-foundation-model-for-optic-disc-segmentation-in-retinal-images" class="headerlink" title="32. Leveraging the RETFound foundation model for optic disc segmentation in retinal images"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Leveraging_the_RETFound_foundation_model_for_optic_disc_segmentation_in_retinal_images.pdf">Leveraging the RETFound foundation model for optic disc segmentation in retinal images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Dundee</span></p><p>本论文首次将RETFound视觉基础模型（基于ViT-large Transformer和自监督学习）适配并用于视网膜图像中的视盘分割任务，结合Segmenter的解码器，通过冻结基础模型权重，仅用少量带注释样本进行微调，实现了高效特征提取和分割。结果显示，在内部验证、领域泛化和领域适应实验中，模型在多个公开和私有数据集上取得约96% Dice分数，性能优于现有分割专用网络，且无需复杂数据增强，表现稳定且泛化能力强。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Leveraging_the_RETFound_foundation_model_for_optic_disc_segmentation_in_retinal_images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-Noise-Matters-Optimizing-Matching-Noise-for-Diffusion-Classifiers"><a href="#33-Noise-Matters-Optimizing-Matching-Noise-for-Diffusion-Classifiers" class="headerlink" title="33. Noise Matters: Optimizing Matching Noise for Diffusion Classifiers"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Noise_Matters__Optimizing_Matching_Noise_for_Diffusion_Classifiers.pdf">Noise Matters: Optimizing Matching Noise for Diffusion Classifiers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>该论文针对扩散分类器在噪声采样上的不稳定性首次进行系统分析，提出了“频率匹配”和“空间匹配”两大原则，并据此设计了噪声优化方法NoOp：通过优化数据集特定噪声和训练Meta-Network生成图像特定噪声偏置，替换随机噪声以实现稳定高效的分类。实验表明NoOp能提升Few-shot分类性能，兼容其他优化方法，具有较强稳定性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Noise_Matters_Optimizing_Matching_Noise_for_Diffusion_Classifiers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-AIM-Amending-Inherent-Interpretability-via-Self-Supervised-Masking"><a href="#34-AIM-Amending-Inherent-Interpretability-via-Self-Supervised-Masking" class="headerlink" title="34. AIM: Amending Inherent Interpretability via Self-Supervised Masking"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/AIM__Amending_Inherent_Interpretability_via_Self-Supervised_Masking.pdf">AIM: Amending Inherent Interpretability via Self-Supervised Masking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Max-Planck-Institute for Informatics</span></p><p>该论文提出了AIM方法，通过自监督特征掩码机制在卷积网络的多尺度特征融合过程中自动区分和保留与任务相关的“可靠”特征，同时抑制与标签无关的“伪”特征。AIM无需额外注释，仅依赖图像标签，通过可学习的二值掩码在网络前向阶段筛选特征，实现模型决策过程的内在可解释性。实验表明，AIM在ImageNet100、Waterbirds、CUB-200等多种数据集上提升了模型对真实特征的定位能力（EPG指标显著提升），并在分布外泛化和细粒度分类任务中提高了准确率。结论：AIM能够以极低的计算开销实现更高准确率和人类一致性的可解释性，适用于高鲁棒性和泛化性需求的视觉模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Automated-Building-Heritage-Assessment-Using-Street-Level-Imagery"><a href="#35-Automated-Building-Heritage-Assessment-Using-Street-Level-Imagery" class="headerlink" title="35. Automated Building Heritage Assessment Using Street-Level Imagery"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Automated_Building_Heritage_Assessment_Using_Street-Level_Imagery.pdf">Automated Building Heritage Assessment Using Street-Level Imagery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Forschungszentrum Jülich GmbH</span></p><p>本论文提出了一种结合GPT大语言模型与机器学习，对街景建筑立面图像自动提取文化遗产价值特征的方法。通过GPT对建筑图像及辅助信息进行特征提取，再利用XGBoost等模型进行分类验证，最高宏F1分数达0.71。结论是该自动评估流程提升了建筑遗产价值识别的效率和准确性，可为能效改造和城市遗产保护提供支持。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Automated_Building_Heritage_Assessment_Using_Street-Level_Imagery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-CineTrans-Learning-to-Generate-Videos-with-Cinematic-Transitions-via-Masked-Diffusion-Models"><a href="#36-CineTrans-Learning-to-Generate-Videos-with-Cinematic-Transitions-via-Masked-Diffusion-Models" class="headerlink" title="36. CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CineTrans__Learning_to_Generate_Videos_with_Cinematic_Transitions_via_Masked_Diffusion_Models.pdf">CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出了CineTrans框架，通过分析扩散模型中的注意力图，发现镜头内外的相关性差异，并基于此设计了块对角掩码机制，有效控制多镜头视频中的电影化转场。作者还构建了大规模多镜头视频文本数据集Cine250K，并提出专用评价指标。实验表明CineTrans在转场控制、时序一致性和整体质量均显著优于现有方法，推动了可控多镜头视频生成的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CineTrans_Learning_to_Generate_Videos_with_Cinematic_Transitions_via_Masked_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-SPG-Style-Prompting-Guidance-for-Style-Specific-Content-Creation"><a href="#37-SPG-Style-Prompting-Guidance-for-Style-Specific-Content-Creation" class="headerlink" title="37. SPG: Style-Prompting Guidance for Style-Specific Content Creation"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/SPG__Style-Prompting_Guidance_for_Style-Specific_Content_Creation.pdf">SPG: Style-Prompting Guidance for Style-Specific Content Creation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen University</span></p><p>本文提出了一种名为Style-Prompting Guidance (SPG)的训练自由采样策略，用于基于扩散模型的风格化图像生成。SPG通过构建风格噪声向量并结合无条件噪声的方向偏移，引导扩散过程朝向目标风格分布，并与Classifier-Free Guidance (CFG)集成，实现了语义一致性与风格一致性的平衡。实验结果表明，该方法在风格保真度和语义对齐方面优于现有方法，并可无缝集成现有扩散插件如ControlNet和IP-Adapter，具有高效、通用和实用性。结论是SPG能高效实现风格和内容双重一致的图像生成。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SPG_Style-Prompting_Guidance_for_Style-Specific_Content_Creation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-Inside-Knowledge-Graph-based-Path-Generation-with-Explainable-Data-Augmentation-and-Curriculum-Learning-for-Visual-Indoor-Navigation"><a href="#38-Inside-Knowledge-Graph-based-Path-Generation-with-Explainable-Data-Augmentation-and-Curriculum-Learning-for-Visual-Indoor-Navigation" class="headerlink" title="38. Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Inside_Knowledge__Graph-based_Path_Generation_with_Explainable_Data_Augmentation_and_Curriculum_Lear.pdf">Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Science and Technology POLITEHNICA Bucharest</span></p><p>本文提出了一种基于视觉输入的室内导航深度学习方法，通过创新的图结构路径生成、解释性数据增强（如Grad-CAM引导的遮挡）及课程学习，自动高效生成训练路径并提升模型鲁棒性。实验表明，该方法在无需额外传感器或场景地图的情况下，实现了高效、准确的室内导航，并能适应拥挤环境和分布外场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inside_Knowledge_Graph-based_Path_Generation_with_Explainable_Data_Augmentation_and_Curriculum_Learning_for_Visual_Indoor_Navigation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-LKFMixer-Exploring-Large-Kernel-Feature-For-Efficient-Image-Super-Resolution"><a href="#39-LKFMixer-Exploring-Large-Kernel-Feature-For-Efficient-Image-Super-Resolution" class="headerlink" title="39. LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/LKFMixer__Exploring_Large_Kernel_Feature_For_Efficient_Image_Super-Resolution.pdf">LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yanshan University</span></p><p>本文提出了一种高效的轻量级图像超分辨率方法LKFMixer，该方法基于纯卷积神经网络（CNN），采用大卷积核（kernel size 31）配合坐标分解和部分通道卷积，模拟Transformer的非局部特征提取能力，并设计了空间特征调制模块和特征选择模块以适应局部与非局部特征融合。实验表明，LKFMixer在超分辨率性能和重建质量上优于当前主流轻量级方法，并显著提升推理速度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LKFMixer_Exploring_Large_Kernel_Feature_For_Efficient_Image_Super-Resolution.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-AnatoMaskGAN-GNN-Driven-Slice-Feature-Fusion-and-Noise-Augmentation-for-Medical-Semantic-Image-Synthesis"><a href="#40-AnatoMaskGAN-GNN-Driven-Slice-Feature-Fusion-and-Noise-Augmentation-for-Medical-Semantic-Image-Synthesis" class="headerlink" title="40. AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/AnatoMaskGAN__GNN-Driven_Slice_Feature_Fusion_and_Noise_Augmentation_for_Medical_Semantic_Image_Synt.pdf">AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southwest University</span></p><p>本文提出了AnatoMaskGAN，一种用于医学语义掩码图像合成的新型生成框架，结合了基于图神经网络（GNN）的切片特征融合模块，三维空间噪声注入机制，以及灰度-纹理联合分类器。实验结果显示，该方法在L2R-OASIS和L2R-Abdomen CT数据集上，PSNR和SSIM均显著超越现有主流方法，验证了其在结构一致性、纹理多样性和感知质量上的优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AnatoMaskGAN_GNN-Driven_Slice_Feature_Fusion_and_Noise_Augmentation_for_Medical_Semantic_Image_Synthesis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-Index-Aligned-Query-Distillation-for-Transformer-based-Incremental-Object-Detection"><a href="#41-Index-Aligned-Query-Distillation-for-Transformer-based-Incremental-Object-Detection" class="headerlink" title="41. Index-Aligned Query Distillation for Transformer-based Incremental Object Detection"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Index-Aligned_Query_Distillation_for_Transformer-based_Incremental_Object_Detection.pdf">Index-Aligned Query Distillation for Transformer-based Incremental Object Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>本文提出了Index-Aligned Query Distillation (IAQD)方法，用于Transformer架构下的增量式目标检测。IAQD通过对关键查询索引进行一对一知识蒸馏，并结合Proxy Query Selection和标签重对齐策略，大幅缓解了旧类别的灾难性遗忘问题。实验结果表明，该方法在COCO和VOC等基准数据集的多种协议下均优于现有方法，有效提升了旧类别检测能力并兼顾新类别学习。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Index-Aligned_Query_Distillation_for_Transformer-based_Incremental_Object_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Delving-into-Dynamic-Scene-Cue-Consistency-for-Robust-3D-Multi-Object-Tracking"><a href="#42-Delving-into-Dynamic-Scene-Cue-Consistency-for-Robust-3D-Multi-Object-Tracking" class="headerlink" title="42. Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Delving_into_Dynamic_Scene_Cue-Consistency_for_Robust_3D_Multi-Object_Tracking.pdf">Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出了DSC-Track，一种基于Transformer的3D多目标跟踪算法，通过统一的时空聚合模块和旋转不变的点对特征（PPF）编码，聚合每个轨迹的历史和空间上下文。核心创新在于引入线索一致性注意力机制，将轨迹与检测的邻域结构进行匹配，从而增强特征判别性并抑制干扰，实现鲁棒的数据关联。实验表明，DSC-Track在nuScenes和Waymo数据集上达到当前最优性能，有效减少了身份切换，证实了建模高阶关系一致性对复杂场景中多目标跟踪的价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Delving_into_Dynamic_Scene_Cue-Consistency_for_Robust_3D_Multi-Object_Tracking.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-Logic-Unseen-Revealing-the-Logical-Blindspots-of-Vision-Language-Models"><a href="#43-Logic-Unseen-Revealing-the-Logical-Blindspots-of-Vision-Language-Models" class="headerlink" title="43. Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Logic_Unseen__Revealing_the_Logical_Blindspots_of_Vision-Language_Models.pdf">Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-sen University</span></p><p>本文提出了LogicBench基准，系统评估现有视觉-语言模型（VLMs）的逻辑推理能力，并发现现有VLMs在理解逻辑结构（如因果、条件、否定等）方面存在显著盲区。为此，作者提出了LogicCLIP训练框架，通过多LLM生成逻辑扰动负样本和逻辑敏感对比学习目标，大幅提升VLMs的逻辑理解能力，同时在通用任务上保持或提升性能，且对医学、视频等领域具备优良泛化性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Logic_Unseen_Revealing_the_Logical_Blindspots_of_Vision-Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models"><a href="#44-Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models" class="headerlink" title="44. Probing the Representational Power of Sparse Autoencoders in Vision Models"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Probing_the_Representational_Power_of_Sparse_Autoencoders_in_Vision_Models.pdf">Probing the Representational Power of Sparse Autoencoders in Vision Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Intel Labs</span></p><p>本论文系统性地研究了Sparse Autoencoders (SAEs) 在视觉模型中的表征能力，涵盖了视觉编码器、多模态大模型和扩散模型。方法上，作者将SAE应用于DINOv2等视觉基础模型、LLaVA多模态模型和Stable Diffusion扩散模型，通过特征提取、层级语义分析与生成可控性实验，提出了新颖的评估指标。实验显示，SAE提取的特征具有良好的语义解释性，可提升模型在分布外泛化和生成可控性，多模态实验还初步揭示了视觉与语言共享的表示。结论认为，SAE是提升视觉模型可解释性、泛化性和可控性的统一工具，具有广阔应用前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Probing_the_Representational_Power_of_Sparse_Autoencoders_in_Vision_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation"><a href="#45-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation" class="headerlink" title="45. FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/FantasyTalking2__Timestep-Layer_Adaptive_Preference_Optimization_for_Audio-Driven_Portrait_Animation.pdf">FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AMAP, Alibaba Group</span></p><p>本文提出了一种多目标偏好优化框架FantasyTalking2（TLPO）用于音频驱动的肖像动画，创新地将人类偏好分解为运动自然性、唇同步和视觉质量等多个维度，通过多专家LoRA模块分别优化，并在扩散模型的时间步和网络层上自适应融合，实现细粒度无冲突的多目标协同优化。此外，作者构建了Talking-Critic多模态奖励模型和41万对大规模多维偏好数据集Talking-NSQ。实验表明，该方法在运动自然性、唇同步和视觉质量等关键指标上显著优于现有方法，实现了更契合人类偏好的高保真肖像动画生成。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/FantasyTalking2_Timestep-Layer_Adaptive_Preference_Optimization_for_Audio-Driven_Portrait_Animation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-Efficient-Image-to-Image-Schrodinger-Bridge-for-CT-Field-of-View-Extension"><a href="#46-Efficient-Image-to-Image-Schrodinger-Bridge-for-CT-Field-of-View-Extension" class="headerlink" title="46. Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Efficient_Image-to-Image_Schr%C3%B6dinger_Bridge_for_CT_Field_of_View_Extension.pdf">Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Cancer Center, Fudan University</span></p><p>该论文提出了一种基于Image-to-Image Schrödinger Bridge (I2SB)扩散模型的高效CT视野扩展方法。I2SB通过直接学习有限视野与扩展视野CT图像之间的随机映射，显著提升重建速度与图像结构一致性。在模拟和真实数据测试中，I2SB在重建质量及推理效率上均优于主流扩散和深度学习方法，单步推理达0.19秒&#x2F;切片，极适合临床实时应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Efficient_Image-to-Image_Schr%C3%B6dinger_Bridge_for_CT_Field_of_View_Extension.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-UAV-VL-R1-Generalizing-Vision-Language-Models-via-Supervised-Fine-Tuning-and-Multi-Stage-GRPO-for-UAV-Visual-Reasoning"><a href="#47-UAV-VL-R1-Generalizing-Vision-Language-Models-via-Supervised-Fine-Tuning-and-Multi-Stage-GRPO-for-UAV-Visual-Reasoning" class="headerlink" title="47. UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/UAV-VL-R1__Generalizing_Vision-Language_Models_via_Supervised_Fine-Tuning_and_Multi-Stage_GRPO_for_U.pdf">UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>本文提出UAV-VL-R1，一种专为无人机(UAV)高分辨率航拍图像结构化视觉推理任务设计的轻量级视觉语言模型。方法采用基于LoRA的监督微调(SFT)与多阶段群组相对策略优化(GRPO)强化学习结合，利用新建的HRVQA-VL数据集进行逐阶段训练，奖励函数兼顾结构化输出与答案准确性。实验表明，UAV-VL-R1在八类UAV视觉推理任务上的多任务泛化能力显著优于主流大模型（如Qwen2-VL-72B），且仅需3.9GB显存，适于资源受限的UAV平台实时部署。结论：融合SFT和GRPO的训练框架显著提升了模型在UAV航拍图像推理的泛化性、结构化输出能力和推理可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/UAV-VL-R1_Generalizing_Vision-Language_Models_via_Supervised_Fine-Tuning_and_Multi-Stage_GRPO_for_UAV_Visual_Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-CHARM3R-Towards-Unseen-Camera-Height-Robust-Monocular-3D-Detector"><a href="#48-CHARM3R-Towards-Unseen-Camera-Height-Robust-Monocular-3D-Detector" class="headerlink" title="48. CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CHARM3R__Towards_Unseen_Camera_Height_Robust_Monocular_3D_Detector.pdf">CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Michigan State University</span></p><p>该论文系统性分析了当前单目3D检测模型在未见相机高度下的表现，发现深度估计是主要影响因素，并数学证明了回归深度与地面深度在相机高度变化下的误差趋势相反。为此，作者提出了CHARM3R方法，将回归深度与地面深度在模型内简单平均，有效抵消两者趋势，大幅提升模型在未见相机高度下的泛化能力，在CARLA数据集上取得了SOTA表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CHARM3R_Towards_Unseen_Camera_Height_Robust_Monocular_3D_Detector.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-HistoViT-Vision-Transformer-for-Accurate-and-Scalable-Histopathological-Cancer-Diagnosis"><a href="#49-HistoViT-Vision-Transformer-for-Accurate-and-Scalable-Histopathological-Cancer-Diagnosis" class="headerlink" title="49. HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/HistoViT__Vision_Transformer_for_Accurate_and_Scalable_Histopathological_Cancer_Diagnosis.pdf">HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Embry-Riddle Aeronautical University</span></p><p>本论文提出了一种基于Vision Transformer (ViT) 的深度学习框架，结合轻量级预处理流水线，对乳腺、前列腺、骨和宫颈等多种肿瘤的病理图像进行多分类任务。方法流程包括图像归一化、区域切片、PyTorch张量转换及ViT微调，模型在四个公开数据集上取得了超越当前深度学习方法的表现（所有任务准确率均超95%，AUC超99%），展示了优异的泛化能力和临床应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/HistoViT_Vision_Transformer_for_Accurate_and_Scalable_Histopathological_Cancer_Diagnosis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-VFM-Guided-Semi-Supervised-Detection-Transformer-for-Source-Free-Object-Detection-in-Remote-Sensing-Images"><a href="#50-VFM-Guided-Semi-Supervised-Detection-Transformer-for-Source-Free-Object-Detection-in-Remote-Sensing-Images" class="headerlink" title="50. VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/VFM-Guided_Semi-Supervised_Detection_Transformer_for_Source-Free_Object_Detection_in_Remote_Sensing_.pdf">VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>该论文提出了VG-DETR，一种融合视觉基础模型（VFM）指导的半监督检测Transformer，用于遥感图像的源自由目标检测。方法创新性地利用VFM的语义先验，通过VFM引导的伪标签挖掘和双层对齐（实例级和图像级），提升伪标签的可靠性和检测器的特征泛化能力，有效缓解因伪标签噪声导致的训练崩溃。实验结果表明，在多种遥感跨域场景下，VG-DETR以极低标注成本实现了显著优于现有方法的检测性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/VFM-Guided_Semi-Supervised_Detection_Transformer_for_Source-Free_Object_Detection_in_Remote_Sensing_Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-LEARN-A-Story-Driven-Layout-to-Image-Generation-Framework-for-STEM-Instruction"><a href="#51-LEARN-A-Story-Driven-Layout-to-Image-Generation-Framework-for-STEM-Instruction" class="headerlink" title="51. LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/LEARN__A_Story-Driven_Layout-to-Image_Generation_Framework_for_STEM_Instruction.pdf">LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hiroshima University</span></p><p>LEARN提出了一种基于布局感知扩散模型的框架，用于生成与教学目标一致的STEM教育插图。其方法包括利用BookCover数据集进行布局语义学习、Caption2LayoutNet生成结构化布局、布局条件扩散生成图像，并通过CLIP对比损失实现语义一致性。实验和用户调研表明，该方法能有效提升插图的结构准确性和解释性，降低学生认知负担，促进课堂参与和概念理解。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LEARN_A_Story-Driven_Layout-to-Image_Generation_Framework_for_STEM_Instruction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-A-Cross-Modal-Rumor-Detection-Scheme-via-Contrastive-Learning-by-Exploring-Text–Image-internal-Correlations"><a href="#52-A-Cross-Modal-Rumor-Detection-Scheme-via-Contrastive-Learning-by-Exploring-Text–Image-internal-Correlations" class="headerlink" title="52. A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text–Image internal Correlations"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/A_Cross-Modal_Rumor_Detection_Scheme_via_Contrastive_Learning_by_Exploring_Text_and_Image_internal_C.pdf">A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text–Image internal Correlations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing University of Posts and Telecommunications</span></p><p>本论文提出了一种基于对比学习的跨模态谣言检测方法（MICC），结合多尺度卷积视觉特征提取（SCLIP编码器）、跨模态多尺度对齐模块和尺度感知融合网络，实现对文本与图像内部相关性的高效建模。实验结果表明，该方法在Weibo和PHEME两个真实谣言检测数据集上显著优于现有方法，提升了跨模态谣言检测的准确性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A_Cross-Modal_Rumor_Detection_Scheme_via_Contrastive_Learning_by_Exploring_Text%E2%80%93Image_internal_Correlations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-Denoise-then-Retrieve-Text-Conditioned-Video-Denoising-for-Video-Moment-Retrieval"><a href="#53-Denoise-then-Retrieve-Text-Conditioned-Video-Denoising-for-Video-Moment-Retrieval" class="headerlink" title="53. Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Denoise-then-Retrieve__Text-Conditioned_Video_Denoising_for_Video_Moment_Retrieval.pdf">Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southeast University</span></p><p>该论文提出了Denoise-then-Retrieve Network (DRNet)，通过文本条件化去噪模块（TCD）和文本重建反馈（TRF）模块，过滤掉与文本无关的视频片段，利用多模态交互和噪声掩码生成纯净的视频表示进行视频片段检索。实验结果表明，在Charades-STA和QVHighlights数据集上，方法在所有指标上超过现有主流方法，且去噪范式可无缝集成到其他视频检索模型中提升性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Denoise-then-Retrieve_Text-Conditioned_Video_Denoising_for_Video_Moment_Retrieval.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-Vision-Language-Models-display-a-strong-gender-bias"><a href="#54-Vision-Language-Models-display-a-strong-gender-bias" class="headerlink" title="54. Vision-Language Models display a strong gender bias"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Vision-Language_Models_display_a_strong_gender_bias.pdf">Vision-Language Models display a strong gender bias</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Togo AI Labs</span></p><p>本文提出了一种透明可复现的方法，利用CLIP风格的对比视觉-语言模型（VLM）和余弦相似度，量化模型在面部图像与职业&#x2F;活动短语之间的性别关联，并通过自助法估计不确定性及标签置换检验显著性。实验结果表明，不同架构的主流VLM在多种职业和活动类别上均显示出显著的性别偏见，Transformer类模型偏见略强于ResNet类，提示预训练数据和模型结构影响偏见方向和程度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Vision-Language_Models_display_a_strong_gender_bias.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-Generalized-Decoupled-Learning-for-Enhancing-Open-Vocabulary-Dense-Perception"><a href="#55-Generalized-Decoupled-Learning-for-Enhancing-Open-Vocabulary-Dense-Perception" class="headerlink" title="55. Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Generalized_Decoupled_Learning_for_Enhancing_Open-Vocabulary_Dense_Perception.pdf">Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology (Shenzhen)</span></p><p>该论文提出了DeCLIP，一种创新的无监督微调框架，通过将CLIP的自注意力模块解耦为“内容”和“上下文”特征，并分别采用自蒸馏、VFM语义引导与扩散模型边界补全进行优化，增强了CLIP在开放词汇密集感知任务中的局部判别能力和空间一致性。实验结果表明，DeCLIP在2D&#x2F;3D检测与分割、视频实例分割、6D姿态估计等多项开放词汇密集视觉任务中均取得了显著提升，证明其作为通用密集感知基础模型的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Generalized_Decoupled_Learning_for_Enhancing_Open-Vocabulary_Dense_Perception.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-A-CLIP-based-Uncertainty-Modal-Modeling-UMM-Framework-for-Pedestrian-Re-Identification-in-Autonomous-Driving"><a href="#56-A-CLIP-based-Uncertainty-Modal-Modeling-UMM-Framework-for-Pedestrian-Re-Identification-in-Autonomous-Driving" class="headerlink" title="56. A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/A_CLIP-based_Uncertainty_Modal_Modeling_(UMM)_Framework_for_Pedestrian_Re-Identification_in_Autonomo.pdf">A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Jiangsu Zhenjiang Technician College</span></p><p>本文提出了基于CLIP的轻量级不确定性模态建模（UMM）框架，用于自动驾驶中的行人再识别，融合多模态token映射器、合成模态增强和跨模态线索交互模块，实现RGB、红外、素描和文本描述的统一特征表示与高效融合。实验表明，该方法在模态缺失和跨模态检索任务中表现出更强的鲁棒性和泛化能力，且计算效率高，适用于资源有限的自动驾驶平台。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A_CLIP-based_Uncertainty_Modal_Modeling_%28UMM%29_Framework_for_Pedestrian_Re-Identification_in_Autonomous_Driving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation"><a href="#57-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation" class="headerlink" title="57. StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/StyleMM__Stylized_3D_Morphable_Face_Model_via_Text-Driven_Aligned_Image_Translation.pdf">StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KAIST</span></p><p>该论文提出StyleMM，一种通过文本驱动的对齐图像翻译，实现风格化3D可变形人脸模型的方法。核心流程包括利用扩散模型生成风格化人脸、显式属性保持模块（EAM）确保表情与结构一致，并通过三阶段训练（几何预热、形状与纹理联合微调、纹理精细化）优化变形网络和纹理生成器。实验结果显示，StyleMM在保持人脸多样性、风格一致性及可动画性方面优于现有方法，能广泛应用于影视、动画和游戏。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/StyleMM_Stylized_3D_Morphable_Face_Model_via_Text-Driven_Aligned_Image_Translation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-E-CaTCH-Event-Centric-Cross-Modal-Attention-with-Temporal-Consistency-and-Class-Imbalance-Handling-for-Misinformation-Detection"><a href="#58-E-CaTCH-Event-Centric-Cross-Modal-Attention-with-Temporal-Consistency-and-Class-Imbalance-Handling-for-Misinformation-Detection" class="headerlink" title="58. E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/E-CaTCH__Event-Centric_Cross-Modal_Attention_with_Temporal_Consistency_and_Class-Imbalance_Handling_.pdf">E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">American University</span></p><p>该论文提出E-CaTCH框架，通过事件聚类、BERT与ResNet特征提取、层次化跨模态自注意力和软门控融合，结合重叠时间窗口及LSTM建模信息传播的动态趋势，并采用自适应类别加权和时序一致性正则，解决多模态社交媒体虚假信息检测中的模态不一致、时间变化和类别不平衡等核心挑战。实验显示E-CaTCH在Fakeddit、印度选举和COVID-19数据集上表现优于现有多模态检测模型，具备强泛化性与高计算效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/E-CaTCH_Event-Centric_Cross-Modal_Attention_with_Temporal_Consistency_and_Class-Imbalance_Handling_for_Misinformation_Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-Versatile-Video-Tokenization-with-Generative-2D-Gaussian-Splatting"><a href="#59-Versatile-Video-Tokenization-with-Generative-2D-Gaussian-Splatting" class="headerlink" title="59. Versatile Video Tokenization with Generative 2D Gaussian Splatting"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Versatile_Video_Tokenization_with_Generative_2D_Gaussian_Splatting.pdf">Versatile Video Tokenization with Generative 2D Gaussian Splatting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Newcastle</span></p><p>本文提出Gaussian Video Transformer（GVT），通过生成式2D高斯泼洒（2DGS）策略进行视频分词，核心包含时空高斯嵌入（STGE）和高斯集合划分（GSP）机制。GVT能够自适应地为不同信息量的区域分配高斯分布，利用静态和动态高斯划分减少时域冗余，实现高效、紧凑的视频表示。实验表明，GVT在UCF101、Kinetics和DAVIS等数据集上，重建质量达到SOTA，并在动作识别和视频压缩任务上表现优异，证明其高效且可扩展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Versatile_Video_Tokenization_with_Generative_2D_Gaussian_Splatting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-Exploring-the-Tradeoff-Between-Diversity-and-Discrimination-for-Continuous-Category-Discovery"><a href="#60-Exploring-the-Tradeoff-Between-Diversity-and-Discrimination-for-Continuous-Category-Discovery" class="headerlink" title="60. Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Exploring_the_Tradeoff_Between_Diversity_and_Discrimination_for_Continuous_Category_Discovery.pdf">Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ocean University of China</span></p><p>本文提出了IDOD方法，针对连续类别发现（CCD）任务，包含独立多样性增强模块、联合新颖性发现模块和正交判别增量模块。通过独立训练骨干网络和投影头，结合正交原型与联合发现机制，方法在提升新颖类别发现与已知类别分类之间取得更优平衡，减少伪标签误差累积且显著降低存储开销，实验验证在多个细粒度视觉数据集上优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Exploring_the_Tradeoff_Between_Diversity_and_Discrimination_for_Continuous_Category_Discovery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-Semi-supervised-Image-Dehazing-via-Expectation-Maximization-and-Bidirectional-Brownian-Bridge-Diffusion-Models"><a href="#61-Semi-supervised-Image-Dehazing-via-Expectation-Maximization-and-Bidirectional-Brownian-Bridge-Diffusion-Models" class="headerlink" title="61. Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Semi-supervised_Image_Dehazing_via_Expectation-Maximization_and_Bidirectional_Brownian_Bridge_Diffus.pdf">Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">China University of Mining and Technology</span></p><p>本论文提出了一种结合期望最大化（EM）算法与双向布朗桥扩散模型（B3DM）的半监督图像去雾方法EM-B3DM。方法分为两阶段：首先利用EM算法将有雾与无雾图像对的联合分布解耦为条件分布，并用统一的布朗桥扩散模型建模结构与内容相关性；随后利用预训练模型和大量无配对数据进行半监督训练，并引入残差差分卷积块（RDC）以增强梯度特征表达。实验结果表明，该方法在合成与真实数据集上均优于或可比现有最优方法，显著降低对成对数据的依赖，提升了去雾效果和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Semi-supervised_Image_Dehazing_via_Expectation-Maximization_and_Bidirectional_Brownian_Bridge_Diffusion_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-Residual-based-Efficient-Bidirectional-Diffusion-Model-for-Image-Dehazing-and-Haze-Generation"><a href="#62-Residual-based-Efficient-Bidirectional-Diffusion-Model-for-Image-Dehazing-and-Haze-Generation" class="headerlink" title="62. Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Residual-based_Efficient_Bidirectional_Diffusion_Model_for_Image_Dehazing_and_Haze_Generation.pdf">Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">China University of Mining and Technology</span></p><p>本文提出了一种残差驱动的高效双向扩散模型（RBDM），可在单一模型内实现图像去雾与生成雾的条件分布建模。方法核心包括通过残差构建双马尔可夫链与基于图像patch的统一评分函数，支持任意尺寸的高效双向转换。实验结果表明，RBDM在多个合成及真实数据集上去雾和生成雾效果优于或至少等同于现有主流方法，并能自然保留图像细节和色彩。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Residual-based_Efficient_Bidirectional_Diffusion_Model_for_Image_Dehazing_and_Haze_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="63-Is-ChatGPT-5-Ready-for-Mammogram-VQA"><a href="#63-Is-ChatGPT-5-Ready-for-Mammogram-VQA" class="headerlink" title="63. Is ChatGPT-5 Ready for Mammogram VQA?"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Is_ChatGPT-5_Ready_for_Mammogram_VQA_.pdf">Is ChatGPT-5 Ready for Mammogram VQA?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Emory University School of Medicine</span></p><p>本文系统评估了ChatGPT-5及GPT-4o在四个公开乳腺癌筛查数据集上的视觉问答（VQA）任务，包括BI-RADS评估、异常检测及恶性分类，采用零样本链式推理多模态提示。结果显示，ChatGPT-5在各任务上优于小模型和前代GPT-4o，但其准确性和敏感性仍明显低于领域专家及医学专用AI模型，尚不能用于高风险临床场景。结论是：ChatGPT-5虽有显著进步，但需针对医学影像领域进行深度定向优化后才能满足临床需求。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Is_ChatGPT-5_Ready_for_Mammogram_VQA_.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding"><a href="#64-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding" class="headerlink" title="64. Controlling Multimodal LLMs via Reward-guided Decoding"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Controlling_Multimodal_LLMs_via_Reward-guided_Decoding.pdf">Controlling Multimodal LLMs via Reward-guided Decoding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mila - Quebec AI Institute</span></p><p>本文提出了一种针对多模态大语言模型（MLLM）的推理期自适应控制方法——多模态奖励引导解码（MRGD），通过构建视觉锚定相关的奖励模型（分别评估目标精度和召回率），并将其线性组合动态指导生成过程，实现对模型输出目标精度与召回、以及推理计算资源消耗的精细可控。实验结果表明，该方法在多个对象幻觉基准上显著优于现有方法，实现了更高的可控性和更低的幻觉率，同时兼容不同架构的MLLM，无需重新训练。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Controlling_Multimodal_LLMs_via_Reward-guided_Decoding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-CRYPTOSCOPE-UTILIZING-LARGE-LANGUAGE-MODELS-FOR-AUTOMATED-CRYPTOGRAPHIC-LOGIC-VULNERABILITY-DETECTION"><a href="#65-CRYPTOSCOPE-UTILIZING-LARGE-LANGUAGE-MODELS-FOR-AUTOMATED-CRYPTOGRAPHIC-LOGIC-VULNERABILITY-DETECTION" class="headerlink" title="65. CRYPTOSCOPE: UTILIZING LARGE LANGUAGE MODELS FOR AUTOMATED CRYPTOGRAPHIC LOGIC VULNERABILITY DETECTION"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CryptoScope__Utilizing_Large_Language_Models_for_Automated_Cryptographic_Logic_Vulnerability_Detecti.pdf">CRYPTOSCOPE: UTILIZING LARGE LANGUAGE MODELS FOR AUTOMATED CRYPTOGRAPHIC LOGIC VULNERABILITY DETECTION</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sichuan University</span></p><p>该论文提出了CRYPTOSCOPE框架，结合了链式思维（CoT）提示和检索增强生成（RAG），依托超过12000项的加密知识库，实现多语言、无需执行代码的自动化加密逻辑漏洞检测。实验表明，CRYPTOSCOPE在LLM-CLVA基准上显著提升了多种大模型的漏洞检测性能，并成功发现了9个真实开源项目中未公开的漏洞，验证了其实用性和通用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CRYPTOSCOPE_UTILIZING_LARGE_LANGUAGE_MODELS_FOR_AUTOMATED_CRYPTOGRAPHIC_LOGIC_VULNERABILITY_DETECTION.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-Aware-First-Think-Less-Dynamic-Boundary-Self-Awareness-Drives-Extreme-Reasoning-Efficiency-in-Large-Language-Models"><a href="#66-Aware-First-Think-Less-Dynamic-Boundary-Self-Awareness-Drives-Extreme-Reasoning-Efficiency-in-Large-Language-Models" class="headerlink" title="66. Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Aware_First,_Think_Less__Dynamic_Boundary_Self-Awareness_Drives_Extreme_Reasoning_Efficiency_in_Larg.pdf">Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology</span></p><p>本文提出了动态推理边界自觉框架（DR. SAF），通过模型自我识别推理能力边界，动态调整推理深度以匹配每道题的复杂度。该方法包含边界自觉对齐、适应性长度管理和边界保持机制，显著提高了LLM推理效率，实现了平均响应长度减少49.27%、训练时间缩短5倍，并在极端压缩下精度提升超过16%。实验结果显示该框架在六项数学推理基准上均保持高准确率和极致token效率，适合资源受限场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Aware_First%2C_Think_Less_Dynamic_Boundary_Self-Awareness_Drives_Extreme_Reasoning_Efficiency_in_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-Causality-Matters-How-Temporal-Information-Emerges-in-Video-Language-Models"><a href="#67-Causality-Matters-How-Temporal-Information-Emerges-in-Video-Language-Models" class="headerlink" title="67. Causality Matters: How Temporal Information Emerges in Video Language Models"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Causality_Matters__How_Temporal_Information_Emerges_in_Video_Language_Models.pdf">Causality Matters: How Temporal Information Emerges in Video Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>该论文系统分析了Video Language Models（VideoLMs）中的时序理解机制，发现时序信息主要通过因果注意力机制自发涌现，而非传统认为的时序位置编码（PEs）。作者提出并实验证明，时序理解依赖于跨帧因果信息流动，并据此设计了两种高效推理策略，在主流基准上验证了其有效性。结论是，未来VideoLMs时序建模应聚焦于因果注意力机制与信息流路径优化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Causality_Matters_How_Temporal_Information_Emerges_in_Video_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-ADMIRE-BayesOpt-Accelerated-Data-MIxture-RE-weighting-for-Language-Models-with-Bayesian-Optimization"><a href="#68-ADMIRE-BayesOpt-Accelerated-Data-MIxture-RE-weighting-for-Language-Models-with-Bayesian-Optimization" class="headerlink" title="68. ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ADMIRE-BayesOpt__Accelerated_Data_MIxture_RE-weighting_for_Language_Models_with_Bayesian_Optimizatio.pdf">ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Thomson Reuters Foundational Research</span></p><p>本文提出了ADMIRE-BayesOpt方法，将大型语言模型训练中的数据混合权重选择问题建模为黑盒超参数优化问题，并采用贝叶斯优化（包括多保真优化）以高效搜索最优数据混合配置。通过在多种模型规模和数据集上的实验，ADMIRE-BayesOpt在收敛速度和最终性能上均显著优于现有的回归或代理模型方法，且极大降低了计算成本，提升了数据混合方案在不同模型规模间的可迁移性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ADMIRE-BayesOpt_Accelerated_Data_MIxture_RE-weighting_for_Language_Models_with_Bayesian_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-Inclusion-Arena-An-Open-Platform-for-Evaluating-Large-Foundation-Models-with-Real-World-Apps"><a href="#69-Inclusion-Arena-An-Open-Platform-for-Evaluating-Large-Foundation-Models-with-Real-World-Apps" class="headerlink" title="69. Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Inclusion_Arena__An_Open_Platform_for_Evaluating_Large_Foundation_Models_with_Real-World_Apps.pdf">Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Inclusion AI</span></p><p>该论文提出了Inclusion Arena平台，通过集成真实AI应用中的用户交互，收集并利用人类偏好数据，用于大模型（LLMs和MLLMs）评测与排名。方法上，平台引入了基于Bradley-Terry模型的最大似然估计排名、Placement Matches冷启动机制和Proximity Sampling智能采样算法，聚焦于相近能力模型间的对比以提升评测稳定性和抗攻击性。实证分析表明，该平台能够高效、稳健地反映模型在真实场景下的能力差异，同时显著减少数据操控风险，有助于推动大模型向用户体验和实用性优化发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inclusion_Arena_An_Open_Platform_for_Evaluating_Large_Foundation_Models_with_Real-World_Apps.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-MM-R1-Unleashing-the-Power-of-Unified-Multimodal-Large-Language-Models-for-Personalized-Image-Generation"><a href="#70-MM-R1-Unleashing-the-Power-of-Unified-Multimodal-Large-Language-Models-for-Personalized-Image-Generation" class="headerlink" title="70. MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/MM-R1__Unleashing_the_Power_of_Unified_Multimodal_Large_Language_Models_for_Personalized_Image_Gener.pdf">MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>本论文提出MM-R1框架，通过跨模态Chain-of-Thought (X-CoT)推理和Grouped Reward Proximal Policy Optimization (GRPO)强化学习，提升统一多模态大语言模型(MLLM)在个性化图像生成中的能力。方法包括：先对用户输入图片和文本进行理解和视觉概念分离，再结合用户提示进行图像生成，并设计多重奖励机制进行优化。实验结果表明，MM-R1在零样本个性化图像生成中显著提升了主题保真度和文本一致性，优于现有统一MLLM方法且无需针对每个主体单独微调，具备更强泛化和可控性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MM-R1_Unleashing_the_Power_of_Unified_Multimodal_Large_Language_Models_for_Personalized_Image_Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-Trustworthy-AI-Psychotherapy-Multi-Agent-LLM-Workflow-for-Counseling-and-Explainable-Mental-Disorder-Diagnosis"><a href="#71-Trustworthy-AI-Psychotherapy-Multi-Agent-LLM-Workflow-for-Counseling-and-Explainable-Mental-Disorder-Diagnosis" class="headerlink" title="71. Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Trustworthy_AI_Psychotherapy__Multi-Agent_LLM_Workflow_for_Counseling_and_Explainable_Mental_Disorde.pdf">Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Vrije Universiteit Amsterdam</span></p><p>本文提出DSM5AgentFlow，一种多智能体大语言模型（LLM）工作流，可自动生成并进行DSM-5精神障碍问卷，通过三类LLM代理（治疗师、患者、诊断师）模拟真实对话，实现逐步、可溯源的诊断与解释。实验表明，该框架能生成高覆盖、连贯对话，诊断准确率领先，显著提升了精神健康AI系统的透明度和可信度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Trustworthy_AI_Psychotherapy_Multi-Agent_LLM_Workflow_for_Counseling_and_Explainable_Mental_Disorder_Diagnosis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-Rationalizing-Transformer-Predictions-via-End-To-End-Differentiable-Self-Training"><a href="#72-Rationalizing-Transformer-Predictions-via-End-To-End-Differentiable-Self-Training" class="headerlink" title="72. Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Rationalizing_Transformer_Predictions_via_End-To-End_Differentiable_Self-Training.pdf">Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bielefeld University</span></p><p>本文提出了一种端到端可微分的自训练范式，用于训练能够生成忠实解释（rationales）和做出分类决策的Transformer模型。该方法通过单一模型同时完成分类、输入token相关性评分，并采用自训练机制优化class-wise rationale掩码，显著提升了与人类注释的一致性与解释的忠实度。实验结果表明，该方法在两个NLP基准数据集上实现了与人类rationale高度一致且忠实性优异的解释，同时保持甚至略超标准分类器的分类性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Rationalizing_Transformer_Predictions_via_End-To-End_Differentiable_Self-Training.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-Inspire-or-Predict-Exploring-New-Paradigms-in-Assisting-Classical-Planners-with-Large-Language-Models"><a href="#73-Inspire-or-Predict-Exploring-New-Paradigms-in-Assisting-Classical-Planners-with-Large-Language-Models" class="headerlink" title="73. Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Inspire_or_Predict__Exploring_New_Paradigms_in_Assisting_Classical_Planners_with_Large_Language_Mode.pdf">Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Guizhou University</span></p><p>本文提出了一种结合问题分解与大语言模型（LLM）辅助的规划方法，将大型规划问题通过有向无环图分解为子任务，并探索两种LLM集成范式：LLM4Inspire（通用知识启发）和LLM4Predict（领域知识推理中间状态）。实验证明，LLM4Predict在缩减搜索空间和提升规划成功率上优于仅依赖通用知识的LLM4Inspire，验证了领域知识在复杂规划中的不可替代性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inspire_or_Predict_Exploring_New_Paradigms_in_Assisting_Classical_Planners_with_Large_Language_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-Reference-Points-in-LLM-Sentiment-Analysis-The-Role-of-Structured-Context"><a href="#74-Reference-Points-in-LLM-Sentiment-Analysis-The-Role-of-Structured-Context" class="headerlink" title="74. Reference Points in LLM Sentiment Analysis: The Role of Structured Context"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Reference_Points_in_LLM_Sentiment_Analysis__The_Role_of_Structured_Context.pdf">Reference Points in LLM Sentiment Analysis: The Role of Structured Context</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Meijo University</span></p><p>本论文提出在LLM情感分析中引入用户和商家历史评分等结构化参考点（如JSON格式）以补充文本上下文，实验使用轻量级Llama-3.2-3B模型。结果表明，结构化（尤其JSON格式）补充信息可显著提升模型在Yelp餐厅和夜生活场景下的准确率和回归性能，无需微调即可在资源受限设备上部署；进一步分析证实性能提升源于真实的上下文推理，而非标签代理效应。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Reference_Points_in_LLM_Sentiment_Analysis_The_Role_of_Structured_Context.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-AIM-Bench-Evaluating-Decision-making-Biases-of-Agentic-LLM-as-Inventory-Manager"><a href="#75-AIM-Bench-Evaluating-Decision-making-Biases-of-Agentic-LLM-as-Inventory-Manager" class="headerlink" title="75. AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/AIM-Bench__Evaluating_Decision-making_Biases_of_Agentic_LLM_as_Inventory_Manager.pdf">AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本论文提出AIM-Bench基准，用于系统评估大语言模型（LLM）在不确定库存管理环境下的决策能力和偏差，包括新建和改编的五种供应链环境，并量化人类常见的决策偏差如均值锚定和牛鞭效应。研究发现，不同LLM普遍表现出类似人类的决策偏差，但通过认知反思和信息共享等策略可有效缓解部分偏差，提示在实际部署LLM时需重视偏差识别与消除。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AIM-Bench_Evaluating_Decision-making_Biases_of_Agentic_LLM_as_Inventory_Manager.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-ON-POLICY-RL-MEETS-OFF-POLICY-EXPERTS-HARMONIZING-SUPERVISED-FINE-TUNING-AND-REINFORCEMENT-LEARNING-VIA-DYNAMIC-WEIGHTING"><a href="#76-ON-POLICY-RL-MEETS-OFF-POLICY-EXPERTS-HARMONIZING-SUPERVISED-FINE-TUNING-AND-REINFORCEMENT-LEARNING-VIA-DYNAMIC-WEIGHTING" class="headerlink" title="76. ON-POLICY RL MEETS OFF-POLICY EXPERTS: HARMONIZING SUPERVISED FINE-TUNING AND REINFORCEMENT LEARNING VIA DYNAMIC WEIGHTING"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/On-Policy_RL_Meets_Off-Policy_Experts__Harmonizing_Supervised_Fine-Tuning_and_Reinforcement_Learning.pdf">ON-POLICY RL MEETS OFF-POLICY EXPERTS: HARMONIZING SUPERVISED FINE-TUNING AND REINFORCEMENT LEARNING VIA DYNAMIC WEIGHTING</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Alibaba Group</span></p><p>本文提出CHORD框架，将监督微调（SFT）和强化学习（RL）以动态加权的方式统一于大语言模型（LLM）后训练，通过全局系数和逐token权重函数，灵活控制离策略专家数据对模型的影响。实验表明，CHORD在平衡专家知识吸收与模型自主探索能力方面优于传统SFT-then-RL范式，提升推理与泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ON-POLICY_RL_MEETS_OFF-POLICY_EXPERTS_HARMONIZING_SUPERVISED_FINE-TUNING_AND_REINFORCEMENT_LEARNING_VIA_DYNAMIC_WEIGHTING.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs"><a href="#77-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs" class="headerlink" title="77. When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/When_Punctuation_Matters__A_Large-Scale_Comparison_of_Prompt_Robustness_Methods_for_LLMs.pdf">When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AIRI</span></p><p>本论文提出了首个系统性评估大型语言模型（LLMs）在不同提示（prompt）格式下鲁棒性的方法，对5种主流鲁棒性提升技术（如Batch Calibration、Template Ensembles等）在Llama、Qwen、Gemma等8个模型和52项任务上进行了统一实验框架下的大规模对比。研究发现，Batch Calibration在无分布偏移时显著提升鲁棒性但对类别失衡敏感，LoRA微调虽提升准确率但对格式变化鲁棒性有限，而规模更大的前沿模型表现出更强鲁棒性，但仍存在部分任务下格式敏感问题，基于投票的Template Ensembles可进一步缓解这一问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/When_Punctuation_Matters_A_Large-Scale_Comparison_of_Prompt_Robustness_Methods_for_LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-ETTRL-Balancing-Exploration-and-Exploitation-in-LLM-Test-Time-Reinforcement-Learning-via-Entropy-Mechanism"><a href="#78-ETTRL-Balancing-Exploration-and-Exploitation-in-LLM-Test-Time-Reinforcement-Learning-via-Entropy-Mechanism" class="headerlink" title="78. ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning via Entropy Mechanism"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ETTRL__Balancing_Exploration_and_Exploitation_in_LLM_Test-Time_Reinforcement_Learning_Via_Entropy_Me.pdf">ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning via Entropy Mechanism</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Technology</span></p><p>本文提出了一种基于熵机制的测试时强化学习框架ETTRL，核心包括熵分叉树多数采样（ETMR）与基于熵的优势重塑（EAR）两大策略，有效提升LLM在无监督推理任务中的探索-利用平衡。实验结果表明，该方法在AIME 2024等数学推理基准上显著提升了Pass@1准确率，同时大幅降低了推理所需的token预算，优化了模型无监督自适应能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ETTRL_Balancing_Exploration_and_Exploitation_in_LLM_Test-Time_Reinforcement_Learning_via_Entropy_Mechanism.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-SGSimEval-A-Comprehensive-Multifaceted-and-Similarity-Enhanced-Benchmark-for-Automatic-Survey-Generation-Systems"><a href="#79-SGSimEval-A-Comprehensive-Multifaceted-and-Similarity-Enhanced-Benchmark-for-Automatic-Survey-Generation-Systems" class="headerlink" title="79. SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/SGSimEval__A_Comprehensive_Multifaceted_and_Similarity-Enhanced_Benchmark_for_Automatic_Survey_Gener.pdf">SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong Polytechnic University</span></p><p>本文提出了SGSimEval，一个用于自动化综述生成系统的多维综合评测基准，集成了结构、内容和参考文献三方面的评价，并将LLM打分与定量指标结合，创新性引入了语义相似度增强及人类偏好对齐的评测框架。实验结果表明，领域特定（如计算机科学）系统在结构和内容生成上能超越人类，但在参考文献质量方面仍有较大提升空间，SGSimEval的评测结果与人类判断高度一致。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SGSimEval_A_Comprehensive_Multifaceted_and_Similarity-Enhanced_Benchmark_for_Automatic_Survey_Generation_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-CSGO-Generalized-Optimization-for-Cold-Start-in-Wireless-Collaborative-Edge-LLM-Systems"><a href="#80-CSGO-Generalized-Optimization-for-Cold-Start-in-Wireless-Collaborative-Edge-LLM-Systems" class="headerlink" title="80. CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CSGO__Generalized_Optimization_for_Cold_Start_in_Wireless_Collaborative_Edge_LLM_Systems.pdf">CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文针对在无线异构边缘设备上部署大语言模型（LLM）面临的推理冷启动延迟问题，提出了一种基于动态规划的分层分配算法，通过重叠模型加载与计算通信过程，动态优化模型分片和设备分配以最小化总体延迟。实验结果表明，所提方法显著降低了端到端冷启动延迟，相较于多种基线策略平均提升17.43%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CSGO_Generalized_Optimization_for_Cold_Start_in_Wireless_Collaborative_Edge_LLM_Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-Hallucination-in-LLM-Based-Code-Generation-An-Automotive-Case-Study"><a href="#81-Hallucination-in-LLM-Based-Code-Generation-An-Automotive-Case-Study" class="headerlink" title="81. Hallucination in LLM-Based Code Generation: An Automotive Case Study"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Hallucination_in_LLM-Based_Code_Generation__An_Automotive_Case_Study.pdf">Hallucination in LLM-Based Code Generation: An Automotive Case Study</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Munich</span></p><p>本文针对汽车领域的代码生成任务，系统评估了多种主流代码生成型大语言模型（如GPT-4.1、Codex、GPT-4o）在不同提示复杂度下的幻觉现象，并采用迭代反馈修复策略提升代码可靠性。结果表明，在基线和信号增强提示下，幻觉现象频发且难以完全消除，仅通过模板增强提示与迭代反馈组合，部分模型可生成正确代码，凸显幻觉问题需针对性缓解方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Hallucination_in_LLM-Based_Code_Generation_An_Automotive_Case_Study.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-ORFuzz-Fuzzing-the-‘Other-Side’-of-LLM-Safety-–-Testing-Over-Refusal"><a href="#82-ORFuzz-Fuzzing-the-‘Other-Side’-of-LLM-Safety-–-Testing-Over-Refusal" class="headerlink" title="82. ORFuzz: Fuzzing the ‘Other Side’ of LLM Safety – Testing Over-Refusal"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ORFuzz__Fuzzing_the__Other_Side__of_LLM_Safety_--_Testing_Over-Refusal.pdf">ORFuzz: Fuzzing the ‘Other Side’ of LLM Safety – Testing Over-Refusal</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了ORFUZZ，这是首个针对大语言模型（LLM）过度拒答（over-refusal）现象的进化式自动化检测框架。ORFUZZ集成了安全类别感知的种子选择、基于LLM推理的自适应变异器优化及人类对齐的评测模型OR-JUDGE，能够自动生成多样且高质量的测试用例，有效挖掘和分析LLM的过度拒答漏洞。实验证明，ORFUZZ在生成有效过度拒答用例上的触发率是现有方法的两倍以上，并构建了覆盖八大安全类别、在10个主流LLM均表现出高触发率（平均63.56%）的ORFUZZSET基准数据集。结论：ORFUZZ和ORFUZZSET为LLM安全测试和基准评测提供了高效、可靠、自动化的解决方案，显著提升了LLM的安全性与可用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ORFuzz_Fuzzing_the_%27Other_Side%27_of_LLM_Safety_%E2%80%93_Testing_Over-Refusal.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-MONACO-More-Natural-and-Complex-Questions-for-Reasoning-Across-Dozens-of-Documents"><a href="#83-MONACO-More-Natural-and-Complex-Questions-for-Reasoning-Across-Dozens-of-Documents" class="headerlink" title="83. MONACO: More Natural and Complex Questions for Reasoning Across Dozens of Documents"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/MoNaCo__More_Natural_and_Complex_Questions_for_Reasoning_Across_Dozens_of_Documents.pdf">MONACO: More Natural and Complex Questions for Reasoning Across Dozens of Documents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Pennsylvania</span></p><p>该论文提出了MONACO基准数据集，包含1315个由人工撰写的自然且复杂的问题，这些问题需要跨越数十至数百份文档进行推理和信息整合。通过分解注释流程和人工标注，MONACO为每个问题提供详细的中间推理链及证据，系统评测了前沿大语言模型（LLM）的表现，发现即使最新LLM在该基准上的F1分数仅为61.2%，存在召回率低和幻觉严重等问题，凸显了LLM在真实复杂信息检索和推理任务上的显著不足。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MONACO_More_Natural_and_Complex_Questions_for_Reasoning_Across_Dozens_of_Documents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-Dynamic-Quality-Latency-Aware-Routing-for-LLM-Inference-in-Wireless-Edge-Device-Networks"><a href="#84-Dynamic-Quality-Latency-Aware-Routing-for-LLM-Inference-in-Wireless-Edge-Device-Networks" class="headerlink" title="84. Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Dynamic_Quality-Latency_Aware_Routing_for_LLM_Inference_in_Wireless_Edge-Device_Networks.pdf">Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了一种动态质量-延迟感知路由框架，针对无线边缘设备协作环境下大语言模型（LLM）推理的质量与延迟权衡问题。通过结合轻量级BERT路由器进行语义难度预测与精细化延迟成本模型，并在多轮对话中显式建模KV-cache重计算开销，实现在保证推理质量的前提下平均响应延迟降低5-15%，大模型调用率减少10-20%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Dynamic_Quality-Latency_Aware_Routing_for_LLM_Inference_in_Wireless_Edge-Device_Networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-LETToT-Label-Free-Evaluation-of-Large-Language-Models-On-Tourism-Using-Expert-Tree-of-Thought"><a href="#85-LETToT-Label-Free-Evaluation-of-Large-Language-Models-On-Tourism-Using-Expert-Tree-of-Thought" class="headerlink" title="85. LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/LETToT__Label-Free_Evaluation_of_Large_Language_Models_On_Tourism_Using_Expert_Tree-of-Thought.pdf">LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Warwick</span></p><p>该论文提出LETToT框架，通过专家知识构建的树状推理结构（Tree-of-Thought, ToT）实现旅游领域大语言模型的无标签评估。方法包括对旅游问答任务进行域适配、分层ToT组件的迭代优化与AHP加权评分，并用优化的ToT指标评估五种开源LLM。结论显示，LETToT能显著提升模型在主题相关性、实用性等多个维度的表现，推理增强型小模型可缩小与大模型的性能差距。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LETToT_Label-Free_Evaluation_of_Large_Language_Models_On_Tourism_Using_Expert_Tree-of-Thought.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-Group-Fairness-Meets-the-Black-Box-Enabling-Fair-Algorithms-on-Closed-LLMs-via-Post-Processing"><a href="#86-Group-Fairness-Meets-the-Black-Box-Enabling-Fair-Algorithms-on-Closed-LLMs-via-Post-Processing" class="headerlink" title="86. Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Group_Fairness_Meets_the_Black_Box__Enabling_Fair_Algorithms_on_Closed_LLMs_via_Post-Processing.pdf">Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Urbana-Champaign</span></p><p>本文提出了一种在闭源大语言模型（LLM）上实现群体公平性的通用后处理框架：将LLM视为特征提取器，通过精心设计的提示（prompt）获取与公平性相关的概率预测（如token log probabilities），并利用这些特征应用传统公平算法，在不需访问模型权重或内部嵌入的情况下构建轻量级公平分类器。实验证明该方法在多个数据集和公平性标准下均能兼顾准确性与公平性，并在低样本场景下优于基于LLM嵌入或原始特征训练的公平模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Group_Fairness_Meets_the_Black_Box_Enabling_Fair_Algorithms_on_Closed_LLMs_via_Post-Processing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-Generating-Dialogues-from-Egocentric-Instructional-Videos-for-Task-Assistance-Dataset-Method-and-Benchmark"><a href="#87-Generating-Dialogues-from-Egocentric-Instructional-Videos-for-Task-Assistance-Dataset-Method-and-Benchmark" class="headerlink" title="87. Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Generating_Dialogues_from_Egocentric_Instructional_Videos_for_Task_Assistance__Dataset,_Method_and_B.pdf">Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Google</span></p><p>本论文提出了一种基于大语言模型的自动化方法，将单人叙述的教学视频转化为专家-新手的任务指导对话，流程包括步骤提取、对话生成与视频定位，并结合现有数据集构建了HowToDIV多模态对话数据集。实验结果表明，该方法能高效生成高质量、覆盖多任务领域的任务指导对话，为AI助手在实际情境下的训练和评估提供了强基线和新数据资源。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Generating_Dialogues_from_Egocentric_Instructional_Videos_for_Task_Assistance_Dataset%2C_Method_and_Benchmark.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-Role-Augmented-Intent-Driven-Generative-Search-Engine-Optimization"><a href="#88-Role-Augmented-Intent-Driven-Generative-Search-Engine-Optimization" class="headerlink" title="88. Role-Augmented Intent-Driven Generative Search Engine Optimization"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Role-Augmented_Intent-Driven_Generative_Search_Engine_Optimization.pdf">Role-Augmented Intent-Driven Generative Search Engine Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出了Role-Augmented Intent-Driven Generative Search Engine Optimization (RAID G-SEO)，通过四阶段流程（内容摘要、意图推断及反思、步骤规划、内容重写）并结合4W多角色深度反思机制，对生成式搜索引擎中内容进行针对性优化。实验结果表明，基于意图建模的方法在提升内容可见性和适应多样化检索场景方面显著优于传统和现有基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Role-Augmented_Intent-Driven_Generative_Search_Engine_Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-AlphaAgents-Large-Language-Model-based-Multi-Agents-for-Equity-Portfolio-Constructions"><a href="#89-AlphaAgents-Large-Language-Model-based-Multi-Agents-for-Equity-Portfolio-Constructions" class="headerlink" title="89. AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/AlphaAgents__Large_Language_Model_based_Multi-Agents_for_Equity_Portfolio_Constructions.pdf">AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">BlackRock, Inc.</span></p><p>该论文提出了一种基于大型语言模型（LLM）的多智能体系统AlphaAgents，用于股票分析与投资组合构建。方法上，设计了基金基本面、情绪和估值三类专用代理，通过角色提示和工具增强，协作分析股票并采用内部辩论机制达成投资共识，有效支持风险偏好建模和认知偏差缓解。结论显示，多智能体框架在股票筛选和投资决策中优于单智能体方案，能提升投资分析的透明度和稳健性，为未来智能投资系统奠定基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AlphaAgents_Large_Language_Model_based_Multi-Agents_for_Equity_Portfolio_Constructions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="90-Pretrained-Conformers-for-Audio-Fingerprinting-and-Retrieval"><a href="#90-Pretrained-Conformers-for-Audio-Fingerprinting-and-Retrieval" class="headerlink" title="90. Pretrained Conformers for Audio Fingerprinting and Retrieval"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Pretrained_Conformers_for_Audio_Fingerprinting_and_Retrieval.pdf">Pretrained Conformers for Audio Fingerprinting and Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Infobip</span></p><p>本文提出了一种基于自监督对比学习预训练的Conformer编码器架构，用于音频指纹提取和检索，通过提取短音频片段的独特嵌入，实现对未见音频的高效检索。实验表明，该模型对时序错位、噪声、混响及极端时间拉伸等失真具有极强鲁棒性，在多个公开数据集上取得了与当前最优方法相当甚至更优的效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Pretrained_Conformers_for_Audio_Fingerprinting_and_Retrieval.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-A-Comprehensive-Perspective-on-Explainable-AI-across-the-Machine-Learning-Workflow"><a href="#91-A-Comprehensive-Perspective-on-Explainable-AI-across-the-Machine-Learning-Workflow" class="headerlink" title="91. A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/A_Comprehensive_Perspective_on_Explainable_AI_across_the_Machine_Learning_Workflow.pdf">A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">JADBio Gnosis DA S.A.</span></p><p>本文提出了Holistic Explainable AI (HXAI) 框架，将可解释性嵌入到机器学习工作流的所有阶段，并针对不同用户（领域专家、数据分析师、数据科学家）定制解释。通过构建六大核心组件和112项问题库，文章系统分析现有工具覆盖不足，总结出有效解释的特征，并提出利用AI Agent（基于LLM）自动聚合与个性化生成解释，促进AI系统的透明性、可用性和信任度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A_Comprehensive_Perspective_on_Explainable_AI_across_the_Machine_Learning_Workflow.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-Physics-Informed-Diffusion-Models-for-Unsupervised-Anomaly-Detection-in-Multivariate-Time-Series"><a href="#92-Physics-Informed-Diffusion-Models-for-Unsupervised-Anomaly-Detection-in-Multivariate-Time-Series" class="headerlink" title="92. Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Physics-Informed_Diffusion_Models_for_Unsupervised_Anomaly_Detection_in_Multivariate_Time_Series.pdf">Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fraunhofer IOSB-INA</span></p><p>该论文提出了一种面向多变量时序数据的无监督异常检测方法，通过在扩散模型训练过程中引入加权物理信息损失函数（基于静态权重调度），使模型更好地拟合数据分布并提升异常检测性能。实验证明，物理信息指导的训练方法在合成和真实数据集上提高了F1分数、数据多样性和对数似然值，优于现有的物理信息和纯数据驱动扩散模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Physics-Informed_Diffusion_Models_for_Unsupervised_Anomaly_Detection_in_Multivariate_Time_Series.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-A-Remedy-for-Over-Squashing-in-Graph-Learning-via-Forman-Ricci-Curvature-based-Graph-to-Hypergraph-Structural-Lifting"><a href="#93-A-Remedy-for-Over-Squashing-in-Graph-Learning-via-Forman-Ricci-Curvature-based-Graph-to-Hypergraph-Structural-Lifting" class="headerlink" title="93. A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/A_Remedy_for_Over-Squashing_in_Graph_Learning_via_Forman-Ricci_Curvature_based_Graph-to-Hypergraph_S.pdf">A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Perelyn GmbH</span></p><p>该论文提出利用Forman-Ricci曲率度量图结构，识别信息流瓶颈后进行结构提升，将图映射为超图，从而通过引入高阶拓扑关系缓解图神经网络中的过度压缩问题。实验表明该方法能提升多种图和超图神经网络模型在分类和分子属性预测任务上的表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A_Remedy_for_Over-Squashing_in_Graph_Learning_via_Forman-Ricci_Curvature_based_Graph-to-Hypergraph_Structural_Lifting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-NeMo-A-Neuron-Level-Modularizing-While-Training-Approach-for-Decomposing-DNN-Models"><a href="#94-NeMo-A-Neuron-Level-Modularizing-While-Training-Approach-for-Decomposing-DNN-Models" class="headerlink" title="94. NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/NeMo__A_Neuron-Level_Modularizing-While-Training_Approach_for_Decomposing_DNN_Models.pdf">NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of Complex &amp; Critical Software Environment, Beihang University</span></p><p>本文提出了一种用于深度神经网络模型结构化分解的神经元级联训练模块化方法NeMo，通过在训练过程中结合神经元识别器和基于对比学习的复合损失函数，实现对Transformer和CNN等多种架构的模块化训练和分解。实验结果表明NeMo在保持模型准确率的同时大幅减少模型尺寸和计算开销，优于现有的主流模块化方法，有效支持按需模型复用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/NeMo_A_Neuron-Level_Modularizing-While-Training_Approach_for_Decomposing_DNN_Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-Mitigating-Modality-Quantity-and-Quality-Imbalance-in-Multimodal-Online-Federated-Learning"><a href="#95-Mitigating-Modality-Quantity-and-Quality-Imbalance-in-Multimodal-Online-Federated-Learning" class="headerlink" title="95. Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Mitigating_Modality_Quantity_and_Quality_Imbalance_in_Multimodal_Online_Federated_Learning.pdf">Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peng Cheng Laboratory</span></p><p>本文系统分析了多模态在线联邦学习（MMO-FL）中，由于物联网设备不稳定导致的模态数量和质量失衡问题，并提出了理论后悔界限来量化其对学习性能的影响。为解决该问题，作者提出了基于原型学习的模态数量与质量重平衡（QQR）算法，包括在线全局原型构建、原型数量重平衡和原型质量重平衡模块，有效地在训练过程中并行补偿模态失衡。实验结果表明，QQR算法在UCI-HAR和MVSA-Single两个真实多模态数据集上均优于现有基线方法，在模态失衡场景下具备较强泛化能力和通信效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Mitigating_Modality_Quantity_and_Quality_Imbalance_in_Multimodal_Online_Federated_Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-Towards-the-Next-generation-Bayesian-Network-Classifiers"><a href="#96-Towards-the-Next-generation-Bayesian-Network-Classifiers" class="headerlink" title="96. Towards the Next-generation Bayesian Network Classifiers"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Towards_the_Next-generation_Bayesian_Network_Classifiers.pdf">Towards the Next-generation Bayesian Network Classifiers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhengzhou University</span></p><p>本文提出了一种新范式，通过引入分布式表示学习来设计高阶贝叶斯网络分类器，突破了传统方法在参数爆炸和数据稀疏性上的限制。具体方法是在K-Dependence Bayesian classifier的基础上，构建三层神经网络（NeuralKDB），学习特征和标签值的分布式表示，并通过随机梯度下降高效训练模型，使其能够捕捉高阶特征依赖关系。实验结果显示，NeuralKDB在60个UCI数据集上显著优于传统贝叶斯网络分类器及其它主流分类方法，验证了分布式表示学习在提升高阶依赖建模和分类性能上的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Towards_the_Next-generation_Bayesian_Network_Classifiers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-Graph-Neural-Diffusion-via-Generalized-Opinion-Dynamics"><a href="#97-Graph-Neural-Diffusion-via-Generalized-Opinion-Dynamics" class="headerlink" title="97. Graph Neural Diffusion via Generalized Opinion Dynamics"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Graph_Neural_Diffusion_via_Generalized_Opinion_Dynamics.pdf">Graph Neural Diffusion via Generalized Opinion Dynamics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Australian National University</span></p><p>本文提出了GODNF（Generalized Opinion Dynamics Neural Framework），一种基于多种意见动力学理论的图神经网络扩散统一框架。该方法通过节点特性建模和动态邻域影响，结合特征保持、初始特征依赖、邻域影响和结构正则化，实现高效、可解释且理论收敛的消息传播机制。理论分析证明GODNF能适应多样化的扩散收敛模式，实验结果在节点分类和影响力估计任务上优于现有主流GNN方法。结论表明，GODNF不仅提升了表达能力，还兼具高效性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Graph_Neural_Diffusion_via_Generalized_Opinion_Dynamics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-Meta-learning-Structure-Preserving-Dynamics"><a href="#98-Meta-learning-Structure-Preserving-Dynamics" class="headerlink" title="98. Meta-learning Structure-Preserving Dynamics"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Meta-learning_Structure-Preserving_Dynamics.pdf">Meta-learning Structure-Preserving Dynamics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Arizona State University</span></p><p>本文提出了一种基于调制（modulation）的元学习框架，用于学习结构保持的动力学系统，包括能量守恒与耗散系统。方法创新点在于引入低维潜在向量，通过自编码方式对模型参数进行条件化，并提出了两种新型调制技术（Rank-One与Multi-Rank），显著提升模型的表达能力和泛化性。实验结果表明，所提方法在多组参数化物理系统的少样本学习场景下实现了高精度预测，并保持了动力学系统的物理约束，优于现有优化型元学习和调制方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Meta-learning_Structure-Preserving_Dynamics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>今日论文报纸</title>
    <link href="/2025/08/2025-08-18_article/"/>
    <url>/2025/08/2025-08-18_article/</url>
    
    <content type="html"><![CDATA[<center><h1>今日论文报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-18<br>📄 <strong>发现论文数量</strong>：132  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Non-Stationary-Restless-Multi-Armed-Bandits-with-Provable-Guarantee"><a href="#1-Non-Stationary-Restless-Multi-Armed-Bandits-with-Provable-Guarantee" class="headerlink" title="1. Non-Stationary Restless Multi-Armed Bandits with Provable Guarantee"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Non-Stationary_Restless_Multi-Armed_Bandits_with_Provable_Guarantee.pdf">Non-Stationary Restless Multi-Armed Bandits with Provable Guarantee</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Yang Ming Chiao Tung University</span></p><p>本文提出NS-Whittle算法，针对带有有限变化预算的非平稳Restless Multi-Armed Bandits（RMABs），将滑动窗口强化学习（RL）与上置信界（UCB）机制结合，实现对转移动态及其变化的自适应学习。方法采用每臂独立的滑动窗口和Whittle指数，显著降低维度和计算复杂度，并首次在非平稳RMAB下给出Oe(N²B^{1&#x2F;4}T^{3&#x2F;4})的理论后悔界限。结论表明，该方法兼顾理论严谨性和实际可扩展性，为动态资源受限环境下的序贯决策提供了高效基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Non-Stationary%20Restless%20Multi-Armed%20Bandits%20with%20Provable%20Guarantee.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Variance-Reduced-Policy-Gradient-Method-for-Multi-Objective-Reinforcement-Learning"><a href="#2-Variance-Reduced-Policy-Gradient-Method-for-Multi-Objective-Reinforcement-Learning" class="headerlink" title="2. Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Variance_Reduced_Policy_Gradient_Method_for_Multi-Objective_Reinforcement_Learning.pdf">Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ETH Zurich</span></p><p>本论文提出了MO-TSIVR-PG算法，通过在多目标强化学习(MORL)中对策略梯度方法引入方差降低技术，显著提升了样本效率，支持大规模和连续状态-动作空间。理论分析和实验结果表明，该算法在收敛速度和样本复杂度上均优于现有方法，且在多目标任务中表现优异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Variance%20Reduced%20Policy%20Gradient%20Method%20for%20Multi-Objective%20Reinforcement%20Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Pass-k-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models"><a href="#3-Pass-k-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models" class="headerlink" title="3. Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Pass@k_Training_for_Adaptively_Balancing_Exploration_and_Exploitation_of_Large_Reasoning_Models.pdf">Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Renmin University of China</span></p><p>本文提出了在大型语言模型（LLMs）推理任务中的RLVR训练中，采用Pass@k作为奖励指标的方法（Pass@k Training），通过全采样、bootstrap采样和解析推导三种流程，有效提升模型的探索能力并保持其利用能力。结论显示，Pass@k Training不仅提升了模型在下游任务中的Pass@k表现，还能通过后续Pass@1训练将探索收益转化为更强的Pass@1表现，具备良好的泛化性和实际应用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Pass%40k%20Training%20for%20Adaptively%20Balancing%20Exploration%20and%20Exploitation%20of%20Large%20Reasoning%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-REFN-A-Reinforcement-Learning-From-Network-Framework-against-1-day-n-day-Exploitations"><a href="#4-REFN-A-Reinforcement-Learning-From-Network-Framework-against-1-day-n-day-Exploitations" class="headerlink" title="4. REFN: A Reinforcement-Learning-From-Network Framework against 1-day&#x2F;n-day Exploitations"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/REFN__A_Reinforcement-Learning-From-Network_Framework_against_1-day_n-day_Exploitations.pdf">REFN: A Reinforcement-Learning-From-Network Framework against 1-day&#x2F;n-day Exploitations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Artificial Intelligence, Hubei University</span></p><p>该论文提出REFN框架，通过网络驱动的强化学习（RL）训练大型语言模型（LLMs），自动生成并部署网络过滤器，有效防御大规模1-day&#x2F;n-day漏洞利用。REFN采用Agentic-RAG知识蒸馏、RL-from-VNF管道和在线Agentic验证，显著提升过滤规则生成的准确性、部署效率和可扩展性。实验表明，REFN在准确率和F1分数方面均优于现有方法，平均修复时间缩短至3.65小时，能高效扩展至万台设备。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/REFN%3A%20A%20Reinforcement-Learning-From-Network%20Framework%20against%201-day/n-day%20Exploitations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-Nonlocal-Monte-Carlo-via-Reinforcement-Learning"><a href="#5-Nonlocal-Monte-Carlo-via-Reinforcement-Learning" class="headerlink" title="5. Nonlocal Monte Carlo via Reinforcement Learning"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Nonlocal_Monte_Carlo_via_Reinforcement_Learning.pdf">Nonlocal Monte Carlo via Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peter Grünberg Institut (PGI-14), Forschungszentrum Jülich GmbH</span></p><p>该论文提出RLNMC方法，将深度强化学习与Nonequilibrium Nonlocal Monte Carlo（NMC）算法结合，通过训练图神经网络（GNN）策略自动发现组合优化中的非局部跳跃步骤。实验表明，RLNMC在典型难解4-SAT基准问题上，相较于标准MCMC和传统NMC方法，能够在残差能量、解的多样性和求解时间等指标上取得明显提升，并展现出较好的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Nonlocal%20Monte%20Carlo%20via%20Reinforcement%20Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-Stabilizing-Long-term-Multi-turn-Reinforcement-Learning-with-Gated-Rewards"><a href="#6-Stabilizing-Long-term-Multi-turn-Reinforcement-Learning-with-Gated-Rewards" class="headerlink" title="6. Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Stabilizing_Long-term_Multi-turn_Reinforcement_Learning_with_Gated_Rewards.pdf">Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology (Shenzhen)</span></p><p>该论文提出了SWE-oriented RL Framework和Gated Reward Accumulation (G-RA)方法，针对软件工程任务中的多轮长时序强化学习，结合多层奖励设计及docker环境支持，实现了奖励的门控累积，仅在高层(长期)奖励达到阈值时累积即时奖励，从而平衡稀疏长期目标与密集即时反馈。实验表明，G-RA显著提升了任务完成率与修改率，有效避免了奖励错配和策略退化，验证了其在复杂长时序任务中稳定RL优化的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Stabilizing%20Long-term%20Multi-turn%20Reinforcement%20Learning%20with%20Gated%20Rewards.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning"><a href="#7-Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning" class="headerlink" title="7. Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Few-shot_Vision-based_Human_Activity_Recognition_with_MLLM-based_Visual_Reinforcement_Learning.pdf">Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kyushu University</span></p><p>该论文提出FAVOR方法，将多模态大语言模型（MLLM）与基于视觉的强化学习（使用Group Relative Policy Optimization, GRPO和可验证奖励函数）结合，用于少样本人类活动识别。实验表明FAVOR在HAR任务的少样本学习中显著优于传统监督微调方法，尤其在数据稀缺情况下提升泛化和推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Few-shot%20Vision-based%20Human%20Activity%20Recognition%20with%20MLLM-based%20Visual%20Reinforcement%20Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-Multi-Agent-Trust-Region-Policy-Optimisation-A-Joint-Constraint-Approach"><a href="#8-Multi-Agent-Trust-Region-Policy-Optimisation-A-Joint-Constraint-Approach" class="headerlink" title="8. Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Multi-Agent_Trust_Region_Policy_Optimisation__A_Joint_Constraint_Approach.pdf">Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Maryland</span></p><p>本文提出了针对多智能体信任域策略优化（HATRPO）均匀KL约束低效问题的两种自适应扩展方法：HATRPO-G（基于改进-散度比的贪心分配）和HATRPO-W（基于KKT条件的全局优化分配），实现总KL阈值在智能体间的灵活分配。实验结果表明，提出方法能更快收敛并显著提升最终奖励，尤其在异质和不平衡智能体场景下表现优异，超过22.5%的性能提升，并有效捕获智能体间优势差异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Agent%20Trust%20Region%20Policy%20Optimisation%3A%20A%20Joint%20Constraint%20Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-Integrating-Reinforcement-Learning-with-Visual-Generative-Models-Foundations-and-Advances"><a href="#9-Integrating-Reinforcement-Learning-with-Visual-Generative-Models-Foundations-and-Advances" class="headerlink" title="9. Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Integrating_Reinforcement_Learning_with_Visual_Generative_Models__Foundations_and_Advances.pdf">Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Artificial Intelligence (TeleAI), China Telecom</span></p><p>本文系统综述了强化学习（RL）与视觉生成模型（包括图像、视频、3D&#x2F;4D内容生成）结合的最新进展，涵盖RL理论演变、主流RL方法（如PPO、GRPO、DPO）在提升生成模型可控性、语义对齐、人类偏好匹配等方面的应用流程。结论指出，RL不仅作为生成模型的微调和优化层，更成为实现复杂目标和人类意图对齐的结构性组件，对未来多模态、交互式生成系统具有重要意义。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Integrating%20Reinforcement%20Learning%20with%20Visual%20Generative%20Models%3A%20Foundations%20and%20Advances.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-A-Curriculum-Learning-Approach-to-Reinforcement-Learning-Leveraging-RAG-for-Multimodal-Question-Answering"><a href="#10-A-Curriculum-Learning-Approach-to-Reinforcement-Learning-Leveraging-RAG-for-Multimodal-Question-Answering" class="headerlink" title="10. A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Curriculum_Learning_Approach_to_Reinforcement_Learning__Leveraging_RAG_for_Multimodal_Question_Ans.pdf">A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Meituan</span></p><p>本文提出了结合视觉大语言模型（VLLM）、知识蒸馏、RAG检索增强生成和课程学习的强化学习训练方法，用于多模态、多轮次问题回答。该方法通过分阶段课程学习稳定强化学习训练，有效提升模型在多模态检索与问题回答任务中的准确率和可靠性，在KDD Cup 2025 CRAG-MM竞赛的多项任务中取得领先成绩。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Curriculum%20Learning%20Approach%20to%20Reinforcement%20Learning%3A%20Leveraging%20RAG%20for%20Multimodal%20Question%20Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-Multi-Agent-Reinforcement-Learning-for-Adaptive-Resource-Orchestration-in-Cloud-Native-Clusters"><a href="#11-Multi-Agent-Reinforcement-Learning-for-Adaptive-Resource-Orchestration-in-Cloud-Native-Clusters" class="headerlink" title="11. Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Multi-Agent_Reinforcement_Learning_for_Adaptive_Resource_Orchestration_in_Cloud-Native_Clusters.pdf">Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northwestern University</span></p><p>本文提出了一种基于多智能体强化学习（MARL）的自适应资源编排方法，采用异构角色驱动的代理合作机制（HRAC）和局部-全局奖励塑形（LGRS）策略，实现对计算节点、存储节点和调度器等异构组件的分工学习与协作。实验表明，该方法在资源利用率、调度延迟和策略收敛速度等方面优于传统方法，并在多租户公平性和不完全信息场景下表现出良好的稳定性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Agent%20Reinforcement%20Learning%20for%20Adaptive%20Resource%20Orchestration%20in%20Cloud-Native%20Clusters.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="12-TLE-Based-A2C-Agent-for-Terrestrial-Coverage-Orbital-Path-Planning"><a href="#12-TLE-Based-A2C-Agent-for-Terrestrial-Coverage-Orbital-Path-Planning" class="headerlink" title="12. TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/TLE-Based_A2C_Agent_for_Terrestrial_Coverage_Orbital_Path_Planning.pdf">TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sardar Vallabhbhai National Institute of Technology Surat</span></p><p>本文提出了一种基于TLE的轨道仿真环境，利用Advantage Actor-Critic (A2C) 强化学习算法自动优化地球观测卫星的轨道参数，实现对地表目标的精确覆盖。实验表明，A2C在收敛速度和累计奖励上均优于Proximal Policy Optimization (PPO)，能高效满足任务目标，验证了强化学习在低轨卫星轨道规划中的实用性和高效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/TLE-Based%20A2C%20Agent%20for%20Terrestrial%20Coverage%20Orbital%20Path%20Planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-A-Multimodal-Neural-Network-for-Recognizing-Subjective-Self-Disclosure-Towards-Social-Robots"><a href="#13-A-Multimodal-Neural-Network-for-Recognizing-Subjective-Self-Disclosure-Towards-Social-Robots" class="headerlink" title="13. A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Multimodal_Neural_Network_for_Recognizing_Subjective_Self-Disclosure_Towards_Social_Robots.pdf">A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amazon</span></p><p>本文提出了一种专为人机交互中主观自我表露建模设计的多模态注意力神经网络，结合音频（MFCC、wav2vec2.0）与视觉（OpenFace、InceptionV1-ResNet）特征输入，并创新性地引入了尺度保持交叉熵损失函数，有效处理了分类与回归之间的问题。结果显示，该模型在自建大规模数据集上F1分数达0.83，显著优于基线方法，为社会机器人感知人类自我表露能力提供了重要突破。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Multimodal%20Neural%20Network%20for%20Recognizing%20Subjective%20Self-Disclosure%20Towards%20Social%20Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT"><a href="#14-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT" class="headerlink" title="14. UI-Venus Technical Report: Building High-performance UI Agents with RFT"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/UI-Venus_Technical_Report__Building_High-performance_UI_Agents_with_RFT.pdf">UI-Venus Technical Report: Building High-performance UI Agents with RFT</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ant Group</span></p><p>本论文提出了UI-Venus，一种基于多模态大语言模型（Qwen2.5-VL）并仅需截图输入的原生UI智能体，通过引入基于GRPO的强化学习微调（RFT）方法，实现了UI界面元素定位与导航任务的业界最优（SOTA）性能。方法包括高质量数据清洗、精细化奖励函数设计以及自进化轨迹历史对齐与稀疏动作增强框架，显著提升了复杂UI任务中的泛化和规划能力。结论是，UI-Venus在多个标准基准上超越现有开源与闭源模型，验证了强化学习策略和数据处理流程在UI Agent训练中的有效性，并已开源相关模型及评测代码。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/UI-Venus%20Technical%20Report%3A%20Building%20High-performance%20UI%20Agents%20with%20RFT.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-Scaling-Up-without-Fading-Out-Goal-Aware-Sparse-GNN-for-RL-based-Generalized-Planning"><a href="#15-Scaling-Up-without-Fading-Out-Goal-Aware-Sparse-GNN-for-RL-based-Generalized-Planning" class="headerlink" title="15. Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Scaling_Up_without_Fading_Out__Goal-Aware_Sparse_GNN_for_RL-based_Generalized_Planning.pdf">Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">LIG Nex1</span></p><p>本文提出了一种结合稀疏、目标感知图神经网络（GNN）表示和课程学习的强化学习（RL）方法，专门用于解决大规模基于PDDL的无人机任务规划问题。通过仅编码局部邻接关系并显式整合目标空间特征，有效提升了训练效率和策略泛化能力，显著降低了GPU内存需求，实现了在大规模网格环境中高效的任务规划。实验证明，该方法在复杂无人机任务场景中具有优越的扩展性和稳定性，克服了以往全连接图方法的瓶颈。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Scaling%20Up%20without%20Fading%20Out%3A%20Goal-Aware%20Sparse%20GNN%20for%20RL-based%20Generalized%20Planning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-Synthesis-of-Deep-Neural-Networks-with-Safe-Robust-Adaptive-Control-for-Reliable-Operation-of-Wheeled-Mobile-Robots"><a href="#16-Synthesis-of-Deep-Neural-Networks-with-Safe-Robust-Adaptive-Control-for-Reliable-Operation-of-Wheeled-Mobile-Robots" class="headerlink" title="16. Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Synthesis_of_Deep_Neural_Networks_with_Safe_Robust_Adaptive_Control_for_Reliable_Operation_of_Wheele.pdf">Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tampere University</span></p><p>本文提出了一种结合深度神经网络（DNN）与安全鲁棒自适应控制（RAC）的分层控制策略，实现重型轮式移动机器人（WMR）在复杂环境下的可靠运行。方法包括以DNN为主控制，在遇到外部干扰时由低层安全机制切换为RAC，系统始终受两级安全层监控，保障精度与稳定性。实验结果表明，所提方法能在满足安全标准的前提下，实现高精度和鲁棒性，并有效应对实际工况下的扰动。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Synthesis%20of%20Deep%20Neural%20Networks%20with%20Safe%20Robust%20Adaptive%20Control%20for%20Reliable%20Operation%20of%20Wheeled%20Mobile%20Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-Learning-Task-Execution-Hierarchies-for-Redundant-Robots"><a href="#17-Learning-Task-Execution-Hierarchies-for-Redundant-Robots" class="headerlink" title="17. Learning Task Execution Hierarchies for Redundant Robots"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Learning_Task_Execution_Hierarchies_for_Redundant_Robots.pdf">Learning Task Execution Hierarchies for Redundant Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Padova</span></p><p>该论文提出了一种结合强化学习（RL）和遗传编程（Genetic Programming, GP）的框架，能够根据用户自定义的高层任务目标和代价函数，自动为冗余机器人学习任务优先级、控制参数和任务激活逻辑（Stack of Tasks, SoTs）。方法流程包括：用户通过GUI设定任务目标权重，系统在仿真中以强化学习方式评估每组任务栈的表现，通过遗传编程演化优化任务顺序和参数，最终选取最优SoT进行真实机器人测试。实验在ABB移动双臂机器人YuMi平台的仿真和现实环境中表明，该方法能实现零样本迁移、适应动态环境、无需专家手工调参，显著提升了冗余机器人任务管理的通用性和自适应性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning%20Task%20Execution%20Hierarchies%20for%20Redundant%20Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-MLM-Learning-Multi-task-Loco-Manipulation-Whole-Body-Control-for-Quadruped-Robot-with-Arm"><a href="#18-MLM-Learning-Multi-task-Loco-Manipulation-Whole-Body-Control-for-Quadruped-Robot-with-Arm" class="headerlink" title="18. MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MLM__Learning_Multi-task_Loco-Manipulation_Whole-Body_Control_for_Quadruped_Robot_with_Arm.pdf">MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了一种结合真实和仿真数据的强化学习框架MLM，实现了带机械臂四足机器人多任务全身运动-操作一体化控制。方法包括引入基于自适应课程的轨迹采样机制和轨迹-速度预测策略网络，实现多任务自适应学习与历史轨迹下的预测。实验表明，该方法可以在仿真和真实环境中实现多任务零样本迁移，提升了四足机器人运动与操作协同的通用性和任务表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MLM%3A%20Learning%20Multi-task%20Loco-Manipulation%20Whole-Body%20Control%20for%20Quadruped%20Robot%20with%20Arm.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Projected-Coupled-Diffusion-for-Test-Time-Constrained-Joint-Generation"><a href="#19-Projected-Coupled-Diffusion-for-Test-Time-Constrained-Joint-Generation" class="headerlink" title="19. Projected Coupled Diffusion for Test-Time Constrained Joint Generation"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Projected_Coupled_Diffusion_for_Test-Time_Constrained_Joint_Generation.pdf">Projected Coupled Diffusion for Test-Time Constrained Joint Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>本文提出了一种名为Projected Coupled Diffusion (PCD)的新型测试时约束联合生成框架，在无需重新训练的前提下，通过引入耦合引导项和每步投影操作，使多个预训练扩散模型在生成相关样本时能精确满足实时约束。方法在图像对生成、机器人操作和多机器人路径规划等任务中验证，实验表明PCD能有效提升样本相关性并严格满足任务约束，且计算开销可控。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Projected%20Coupled%20Diffusion%20for%20Test-Time%20Constrained%20Joint%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-MASH-Cooperative-Heterogeneous-Multi-Agent-Reinforcement-Learning-for-Single-Humanoid-Robot-Locomotion"><a href="#20-MASH-Cooperative-Heterogeneous-Multi-Agent-Reinforcement-Learning-for-Single-Humanoid-Robot-Locomotion" class="headerlink" title="20. MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MASH__Cooperative-Heterogeneous_Multi-Agent_Reinforcement_Learning_for_Single_Humanoid_Robot_Locomot.pdf">MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeastern University</span></p><p>该论文提出了MASH方法，将单个人形机器人四肢（双腿与双臂）建模为独立智能体，通过协作异质多智能体深度强化学习（MARL）优化机器人步态与动作协调，采用中央评论员和多智能体PPO训练。实验证明MASH相比单智能体RL收敛更快、动作更平滑、肢体协调性更高，并且具备更强的实际部署鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MASH%3A%20Cooperative-Heterogeneous%20Multi-Agent%20Reinforcement%20Learning%20for%20Single%20Humanoid%20Robot%20Locomotion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-KDPE-A-Kernel-Density-Estimation-Strategy-for-Diffusion-Policy-Trajectory-Selection"><a href="#21-KDPE-A-Kernel-Density-Estimation-Strategy-for-Diffusion-Policy-Trajectory-Selection" class="headerlink" title="21. KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/KDPE__A_Kernel_Density_Estimation_Strategy_for_Diffusion_Policy_Trajectory_Selection.pdf">KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Istituto Italiano di Tecnologia</span></p><p>本文提出了KDPE，一种基于核密度估计（KDE）的策略，用于过滤和选择Diffusion Policy（DP）生成的机器人轨迹，通过并行采样多条轨迹并用流形感知核函数建模动作分布，筛除异常轨迹，仅保留最具代表性的动作路径。实验结果表明，KDPE在仿真和真实机器人抓取、分拣、制作咖啡等任务中，能提升成功率和鲁棒性，尤其在演示质量较低或精度要求较高的场景效果更优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/KDPE%3A%20A%20Kernel%20Density%20Estimation%20Strategy%20for%20Diffusion%20Policy%20Trajectory%20Selection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-Learning-State-Space-Models-of-Dynamic-Systems-from-Arbitrary-Data-using-Joint-Embedding-Predictive-Architectures"><a href="#22-Learning-State-Space-Models-of-Dynamic-Systems-from-Arbitrary-Data-using-Joint-Embedding-Predictive-Architectures" class="headerlink" title="22. Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Learning_State-Space_Models_of_Dynamic_Systems_from_Arbitrary_Data_using_Joint_Embedding_Predictive_.pdf">Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">RPTU University Kaiserslautern-Landau</span></p><p>本文提出了一种基于Joint Embedding Predictive Architectures (JEPA) 和神经常微分方程（neural ODE）的新方法，将任意观测数据序列编码为连续时间的潜在状态空间，并通过引入收缩性和Lipschitz常数损失，保证状态空间的有序结构。方法在仿真摆系统中实现从图像数据直接学习动态系统潜在状态空间，实现了从潜在预测到图像重构，展示了对机器人控制和估计的广泛应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning%20State-Space%20Models%20of%20Dynamic%20Systems%20from%20Arbitrary%20Data%20using%20Joint%20Embedding%20Predictive%20Architectures.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-Efficient-Methods-for-Accurate-Sparse-Trajectory-Recovery-and-Map-Matching"><a href="#23-Efficient-Methods-for-Accurate-Sparse-Trajectory-Recovery-and-Map-Matching" class="headerlink" title="23. Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Efficient_Methods_for_Accurate_Sparse_Trajectory_Recovery_and_Map_Matching.pdf">Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong Polytechnic University</span></p><p>本文提出了TRMMA和MMA两种高效且准确的稀疏轨迹恢复与地图匹配方法。MMA将GPS点匹配到道路段的任务转化为小候选集上的分类问题，并通过候选段与点的有效嵌入提升匹配精度，TRMMA进一步在MMA识别的路径上，通过双变换器（Dual-Transformer）编码捕捉轨迹和路径的潜在模式，利用解码过程预测缺失点的位置比例与道路段，实现高效轨迹恢复。实验表明，TRMMA与MMA在大规模真实数据集上均优于现有方法，且显著提升训练和推理效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Efficient%20Methods%20for%20Accurate%20Sparse%20Trajectory%20Recovery%20and%20Map%20Matching.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-CorrectNav-Self-Correction-Flywheel-Empowers-Vision-Language-Action-Navigation-Model"><a href="#24-CorrectNav-Self-Correction-Flywheel-Empowers-Vision-Language-Action-Navigation-Model" class="headerlink" title="24. CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/CorrectNav__Self-Correction_Flywheel_Empowers_Vision-Language-Action_Navigation_Model.pdf">CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>该论文提出了Self-correction Flywheel（自我纠正飞轮）后训练范式，通过自动收集导航模型在训练集上的错误轨迹，检测偏差并生成自我纠正数据用于感知和动作校正，多轮迭代训练提升导航模型CorrectNav的自我纠错能力。实验在R2R-CE和RxR-CE等基准上取得了新的SOTA，并在实际机器人中验证了其优越的纠错、避障和长指令执行能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/CorrectNav%3A%20Self-Correction%20Flywheel%20Empowers%20Vision-Language-Action%20Navigation%20Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-Probabilistic-Latency-Analysis-of-the-Data-Distribution-Service-in-ROS-2"><a href="#25-Probabilistic-Latency-Analysis-of-the-Data-Distribution-Service-in-ROS-2" class="headerlink" title="25. Probabilistic Latency Analysis of the Data Distribution Service in ROS 2"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Probabilistic_Latency_Analysis_of_the_Data_Distribution_Service_in_ROS_2.pdf">Probabilistic Latency Analysis of the Data Distribution Service in ROS 2</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">DGIST</span></p><p>本文提出了一种针对ROS 2 DDS通信在有损无线网络下的概率延迟分析（PLA）模型。该方法通过离散状态建模与动态规划，系统性分析中间件层和传输层事件，预测未确认消息概率分布与重传延迟，并在270种场景下实验验证，其平均误差低于5%。结论指出PLA模型在可靠性、延迟和性能优化方面具备理论指导意义，能有效辅助无线工业机器人网络的QoS参数调优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Probabilistic%20Latency%20Analysis%20of%20the%20Data%20Distribution%20Service%20in%20ROS%202.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-Super-LiDAR-Reflectance-for-Robotic-Perception"><a href="#26-Super-LiDAR-Reflectance-for-Robotic-Perception" class="headerlink" title="26. Super LiDAR Reflectance for Robotic Perception"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Super_LiDAR_Reflectance_for_Robotic_Perception.pdf">Super LiDAR Reflectance for Robotic Perception</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Macau</span></p><p>该论文提出了一种创新框架，通过自研的深度卷积网络（含自适应融合模块和动态补偿模块）将低成本非重复扫描LiDAR的稀疏反射率数据实时转换为高密度、校准后的反射率图像。方法包括构建稀疏到稠密配对标注数据集、静态到动态域数据增强，以及反射率补偿，显著提升了低成本LiDAR在机器人闭环检测和车道线检测等感知任务中的实用性。结论表明，该方法兼具高准确性和实时性，极大拓展了低成本LiDAR在机器人主动视觉中的应用边界。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Super%20LiDAR%20Reflectance%20for%20Robotic%20Perception.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-ReconVLA-Reconstructive-Vision-Language-Action-Model-as-Effective-Robot-Perceiver"><a href="#27-ReconVLA-Reconstructive-Vision-Language-Action-Model-as-Effective-Robot-Perceiver" class="headerlink" title="27. ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ReconVLA__Reconstructive_Vision-Language-Action_Model_as_Effective_Robot_Perceiver.pdf">ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (Guangzhou)</span></p><p>本论文提出了一种重构式视觉-语言-动作模型（ReconVLA），采用隐式视觉归因范式，通过扩散Transformer模块重建机器人操作目标的凝视区域，促进模型学习细粒度视觉表征和精准关注目标区域，提升操控精度。实验结果表明，该方法在模拟及真实环境中均优于显式和链式视觉归因方法，实现了更强的泛化和对未见目标的操控能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ReconVLA%3A%20Reconstructive%20Vision-Language-Action%20Model%20as%20Effective%20Robot%20Perceiver.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-Large-Model-Empowered-Embodied-AI-A-Survey-on-Decision-Making-and-Embodied-Learning"><a href="#28-Large-Model-Empowered-Embodied-AI-A-Survey-on-Decision-Making-and-Embodied-Learning" class="headerlink" title="28. Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Large_Model_Empowered_Embodied_AI__A_Survey_on_Decision-Making_and_Embodied_Learning.pdf">Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>本文系统综述了大模型赋能的具身智能（Embodied AI），聚焦于自主决策与具身学习。方法上，文章梳理了大模型在层次化决策（包括高层规划、低层执行、反馈增强）与端到端决策（基于VLA模型）、具身学习（模仿学习与强化学习）、世界模型等方面的最新进展与技术流程。结论指出，大模型极大提升了具身智能系统在感知、规划、执行与学习等环节的能力，但在数据稀缺、持续学习、部署效率及仿真-现实转移等方面仍有诸多挑战，未来需持续研究以实现更强的通用智能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Large%20Model%20Empowered%20Embodied%20AI%3A%20A%20Survey%20on%20Decision-Making%20and%20Embodied%20Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-A-Semantic-Aware-Framework-for-Safe-and-Intent-Integrative-Assistance-in-Upper-Limb-Exoskeletons"><a href="#29-A-Semantic-Aware-Framework-for-Safe-and-Intent-Integrative-Assistance-in-Upper-Limb-Exoskeletons" class="headerlink" title="29. A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Semantic-Aware_Framework_for_Safe_and_Intent-Integrative_Assistance_in_Upper-Limb_Exoskeletons.pdf">A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了一种面向上肢外骨骼的语义感知辅助框架，将大语言模型（LLM）集成到任务规划中，通过提取任务语义信息和人类意图，自动配置辅助参数，并利用扩散模型实现异常检测与实时重规划。实验表明，该方法有效实现了人机认知对齐，提高了辅助任务的安全性、适应性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Semantic-Aware%20Framework%20for%20Safe%20and%20Intent-Integrative%20Assistance%20in%20Upper-Limb%20Exoskeletons.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-JRDB-Reasoning-A-Difficulty-Graded-Benchmark-for-Visual-Reasoning-in-Robotics"><a href="#30-JRDB-Reasoning-A-Difficulty-Graded-Benchmark-for-Visual-Reasoning-in-Robotics" class="headerlink" title="30. JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/JRDB-Reasoning__A_Difficulty-Graded_Benchmark_for_Visual_Reasoning_in_Robotics.pdf">JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Monash University</span></p><p>本论文提出JRDB-Reasoning基准，首次系统定义和量化视觉推理复杂度，并开发了自适应查询引擎，可按需生成不同难度、多步骤推理和中间标注的问题，扩展JRDB数据集以支持机器人在拥挤人群环境中的视觉推理任务。实验显示，现有VLM模型在推理难度提升时性能显著下降，而该基准和工具有助于精细化评估机器人视觉推理能力和未来模型发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/JRDB-Reasoning%3A%20A%20Difficulty-Graded%20Benchmark%20for%20Visual%20Reasoning%20in%20Robotics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="31-GNN-based-Unified-Deep-Learning"><a href="#31-GNN-based-Unified-Deep-Learning" class="headerlink" title="31. GNN-based Unified Deep Learning"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GNN-based_Unified_Deep_Learning.pdf">GNN-based Unified Deep Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>本论文提出了一种基于图神经网络（GNN）的统一学习范式，将多种异构深度学习架构（如MLP、CNN和GNN）转化为图表示，并在统一的图学习空间中通过GNN进行协同优化，实现参数共享和知识转移。实验表明，该方法在医学图像领域面对分布漂移和域断裂场景时，显著提升了各类模型的泛化能力和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GNN-based%20Unified%20Deep%20Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-Quantum-Visual-Fields-with-Neural-Amplitude-Encoding"><a href="#32-Quantum-Visual-Fields-with-Neural-Amplitude-Encoding" class="headerlink" title="32. Quantum Visual Fields with Neural Amplitude Encoding"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Quantum_Visual_Fields_with_Neural_Amplitude_Encoding.pdf">Quantum Visual Fields with Neural Amplitude Encoding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MPI for Informatics</span></p><p>本文提出了一种新型量子隐式神经表示（QINR）方法Quantum Visual Field (QVF)，用于2D图像与3D几何场的高效表示。其核心创新在于通过可学习的能量流形进行神经幅度编码，将经典数据嵌入量子态向量，并采用全纠缠设计的参数化量子电路在实希尔伯特空间内完成量子操作，保障数值稳定和快速收敛，无需经典后处理，直接通过投影测量提取信号。实验表明，QVF在视觉场表示精度和高频细节学习等方面均优于现有量子及主流经典基线方法，并首次实现了图像与3D形状集合的联合表示及应用（如图像修复、3D补全和插值），展示了其在量子增强视觉计算领域的实际潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Quantum%20Visual%20Fields%20with%20Neural%20Amplitude%20Encoding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data"><a href="#33-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data" class="headerlink" title="33. MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MAESTRO__Masked_AutoEncoders_for_Multimodal,_Multitemporal,_and_Multispectral_Earth_Observation_Data.pdf">MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institut national de l’information géographique et forestière (IGN), France</span></p><p>论文提出MAESTRO，一种针对地球观测（EO）数据的自监督学习方法，扩展了Masked Autoencoder（MAE）框架以支持多模态、多时序和多光谱输入。其核心技术包括基于token的早融合与晚融合策略，以及新颖的patch-group-wise归一化方法，有效注入光谱先验并提升表示能力。实验在四个遥感数据集上验证了方法，MAESTRO在依赖多时序动态的任务上达到SOTA，并在单时序任务上保持竞争力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MAESTRO%3A%20Masked%20AutoEncoders%20for%20Multimodal%2C%20Multitemporal%2C%20and%20Multispectral%20Earth%20Observation%20Data.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing"><a href="#34-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing" class="headerlink" title="34. ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ToonComposer__Streamlining_Cartoon_Production_with_Generative_Post-Keyframing.pdf">ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong</span></p><p>本文提出了ToonComposer，一种基于Diffusion Transformer (DiT) 的生成式后关键帧(post-keyframing)模型，将卡通动画制作中的中间帧生成与上色步骤统一为单一流程。方法核心包括稀疏草图注入机制（sparse sketch injection）、区域控制(region-wise control)和空间低秩适配器（SLRA），实现对稀疏关键帧草图的精确控制，并高效迁移视频基础模型至卡通域。实验显示ToonComposer在视觉质量、运动连贯性与生产效率上均显著优于现有方法，大幅减少艺术家手动工作量，提升卡通视频生成的灵活性和质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ToonComposer%3A%20Streamlining%20Cartoon%20Production%20with%20Generative%20Post-Keyframing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Puppeteer-Rig-and-Animate-Your-3D-Models"><a href="#35-Puppeteer-Rig-and-Animate-Your-3D-Models" class="headerlink" title="35. Puppeteer: Rig and Animate Your 3D Models"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Puppeteer__Rig_and_Animate_Your_3D_Models.pdf">Puppeteer: Rig and Animate Your 3D Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>该论文提出了Puppeteer，一个集自动绑定骨骼与动画于一体的3D模型处理框架。方法包括利用自回归Transformer实现关节序列建模与骨架生成、拓扑感知注意力网络进行蒙皮权重预测，并通过可微分优化自动生成高质量动画。实验表明，Puppeteer在骨架预测、蒙皮精准度和动画稳定性方面优于现有方法，能高效处理多样3D内容。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Puppeteer%3A%20Rig%20and%20Animate%20Your%203D%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-STREAM3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer"><a href="#36-STREAM3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer" class="headerlink" title="36. STREAM3R: Scalable Sequential 3D Reconstruction with Causal Transformer"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/STream3R__Scalable_Sequential_3D_Reconstruction_with_Causal_Transformer.pdf">STREAM3R: Scalable Sequential 3D Reconstruction with Causal Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>本文提出STREAM3R，将3D重建任务重构为基于因果注意力的解码器Transformer问题，实现对流式输入图像的高效顺序处理。通过缓存历史帧特征并逐帧递增融合，STREAM3R具备优异的扩展性和泛化能力，在静态与动态场景的深度估计及3D重建任务上均优于现有方法，且推理速度显著提升，推动实时3D理解发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/STREAM3R%3A%20Scalable%20Sequential%203D%20Reconstruction%20with%20Causal%20Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-Medico-2025-Visual-Question-Answering-for-Gastrointestinal-Imaging"><a href="#37-Medico-2025-Visual-Question-Answering-for-Gastrointestinal-Imaging" class="headerlink" title="37. Medico 2025: Visual Question Answering for Gastrointestinal Imaging"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Medico_2025__Visual_Question_Answering_for_Gastrointestinal_Imaging.pdf">Medico 2025: Visual Question Answering for Gastrointestinal Imaging</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">SimulaMet - Simula Metropolitan Center for Digital Engineering</span></p><p>该论文提出了Medico 2025挑战，旨在推动面向胃肠道影像的可解释视觉问答（VQA）模型的发展，要求AI模型不仅准确回答医学影像中的临床问题，还需生成与医学推理一致的多模态解释。研究基于Kvasir-VQA-x1数据集，通过量化指标和专家审核结合评估AI系统的准确性和解释性，促进可被临床采用的可信医疗AI系统发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Medico%202025%3A%20Visual%20Question%20Answering%20for%20Gastrointestinal%20Imaging.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-AEGIS-Authenticity-Evaluation-Benchmark-for-AI-Generated-Video-Sequences"><a href="#38-AEGIS-Authenticity-Evaluation-Benchmark-for-AI-Generated-Video-Sequences" class="headerlink" title="38. AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/AEGIS__Authenticity_Evaluation_Benchmark_for_AI-Generated_Video_Sequences.pdf">AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>该论文提出了AEGIS，一个专为AI生成视频真伪检测设计的大规模基准数据集，包含来自多种先进生成模型（如Stable Video Diffusion、CogVideoX-5B、KLing和Sora）的5,199个高真实性和语义复杂的合成视频，并配有丰富的多模态标注（语义描述、运动特征、低层视觉特征）。实验表明，现有视觉语言模型（如Qwen-VL、Video-LLaVA）在AEGIS的高难度测试集上表现有限，突出当前检测模型在真实环境中面临的泛化和鲁棒性挑战，强调了AEGIS在推动视频真伪检测研究中的基础性作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AEGIS%3A%20Authenticity%20Evaluation%20Benchmark%20for%20AI-Generated%20Video%20Sequences.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-Natively-Trainable-Sparse-Attention-for-Hierarchical-Point-Cloud-Datasets"><a href="#39-Natively-Trainable-Sparse-Attention-for-Hierarchical-Point-Cloud-Datasets" class="headerlink" title="39. Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Natively_Trainable_Sparse_Attention_for_Hierarchical_Point_Cloud_Datasets.pdf">Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Groningen</span></p><p>本文提出将Erwin层次化Transformer架构与Native Sparse Attention(NSA)机制结合，针对物理科学中的大规模点云数据，提出Erwin NSA模型，通过将NSA算法从序列数据扩展到非序列点云，并替换局部注意力机制，实现了更高效的全局感受野建模。实验在宇宙学模拟、分子动力学和空气压力建模三大数据集上，表明Erwin NSA在ShapeNet和分子动力学任务中优于原Erwin模型，在宇宙学任务上表现持平，且具有更高的训练效率和更好的节点信息流动能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Natively%20Trainable%20Sparse%20Attention%20for%20Hierarchical%20Point%20Cloud%20Datasets.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-Exploiting-Discriminative-Codebook-Prior-for-Autoregressive-Image-Generation"><a href="#40-Exploiting-Discriminative-Codebook-Prior-for-Autoregressive-Image-Generation" class="headerlink" title="40. Exploiting Discriminative Codebook Prior for Autoregressive Image Generation"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Exploiting_Discriminative_Codebook_Prior_for_Autoregressive_Image_Generation.pdf">Exploiting Discriminative Codebook Prior for Autoregressive Image Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>该论文提出了Discriminative Codebook Prior Extractor (DCPE)，通过聚合聚类和实例距离替代传统k-means聚类，从分布不均且高维的代码本令牌特征空间中更有效地挖掘令牌相似性信息，用于提升离散令牌自回归图像生成模型的训练。实验表明，DCPE作为即插即用方法能加速训练、提升生成质量，并优于现有k-means方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Exploiting%20Discriminative%20Codebook%20Prior%20for%20Autoregressive%20Image%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-Continuous-Bangla-Sign-Language-Translation-Mitigating-the-Expense-of-Gloss-Annotation-with-the-Assistance-of-Graph"><a href="#41-Continuous-Bangla-Sign-Language-Translation-Mitigating-the-Expense-of-Gloss-Annotation-with-the-Assistance-of-Graph" class="headerlink" title="41. Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Continuous_Bangla_Sign_Language_Translation__Mitigating_the_Expense_of_Gloss_Annotation_with_the_Ass.pdf">Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Dhaka</span></p><p>该论文提出了融合Transformer与STGCN-LSTM架构的连续手语翻译方法，通过多模态数据融合（RGB视频与骨骼关键点），实现了无需gloss注释的端到端手语到文本翻译。该方法在多个公开手语数据集（RWTH-PHOENIX-2014T、CSL-Daily、How2Sign、BornilDB v1.0）上取得了新的gloss-free翻译性能最优，显著提升了BLEU分数，为聋人群体的交流无障碍提供了技术基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Continuous%20Bangla%20Sign%20Language%20Translation%3A%20Mitigating%20the%20Expense%20of%20Gloss%20Annotation%20with%20the%20Assistance%20of%20Graph.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Hybrid-Generative-Fusion-for-Efficient-and-Privacy-Preserving-Face-Recognition-Dataset-Generation"><a href="#42-Hybrid-Generative-Fusion-for-Efficient-and-Privacy-Preserving-Face-Recognition-Dataset-Generation" class="headerlink" title="42. Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Hybrid_Generative_Fusion_for_Efficient_and_Privacy-Preserving_Face_Recognition_Dataset_Generation.pdf">Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Information Engineering, Chinese Academy of Sciences</span></p><p>本文提出了混合生成融合方法，通过Mixture-of-Experts数据清洗（聚类+GPT-4o验证）、Stable Diffusion与Vec2Face生成新身份，并用课程学习策略优化训练流程，实现高效、隐私保护的人脸识别数据集构建。实验表明，所生成的纯合成数据集在多规模下均提升了人脸识别模型性能，并在竞赛中取得第一名。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Hybrid%20Generative%20Fusion%20for%20Efficient%20and%20Privacy-Preserving%20Face%20Recognition%20Dataset%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-Geospatial-Diffusion-for-Land-Cover-Imperviousness-Change-Forecasting"><a href="#43-Geospatial-Diffusion-for-Land-Cover-Imperviousness-Change-Forecasting" class="headerlink" title="43. Geospatial Diffusion for Land Cover Imperviousness Change Forecasting"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Geospatial_Diffusion_for_Land_Cover_Imperviousness_Change_Forecasting.pdf">Geospatial Diffusion for Land Cover Imperviousness Change Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Oak Ridge National Laboratory</span></p><p>本文提出一种基于生成扩散模型（Diffusion Models）的地表覆盖不透水性变化预测方法，将土地利用&#x2F;覆盖（LULC）变化预测视为条件数据合成任务，利用历史LULC及辅助数据（如过渡概率图）进行建模。实验表明，该方法在美国12个大都市区域实现了亚公里级别的预测精度，优于传统的无变化基线模型，能够有效捕捉历史数据中的时空发展模式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Geospatial%20Diffusion%20for%20Land%20Cover%20Imperviousness%20Change%20Forecasting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera"><a href="#44-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera" class="headerlink" title="44. Processing and acquisition traces in visual encoders: What does CLIP know about your camera?"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Processing_and_acquisition_traces_in_visual_encoders__What_does_CLIP_know_about_your_camera_.pdf">Processing and acquisition traces in visual encoders: What does CLIP know about your camera?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Osaka</span></p><p>本论文系统分析了视觉编码器（如CLIP）在表征空间中对图片处理和采集参数（如JPEG压缩、相机型号等元数据）的敏感性，通过分类和检索任务展示这些元数据会被模型学习并影响语义预测。结论表明，尤其是视觉-语言对比模型（CVL），其表征空间中元数据痕迹可能会干扰下游语义任务，降低模型鲁棒性和泛化能力，且模型训练时是否采用强数据增强是影响元数据敏感性的关键因素。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Processing%20and%20acquisition%20traces%20in%20visual%20encoders%3A%20What%20does%20CLIP%20know%20about%20your%20camera%3F.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-Object-Fidelity-Diffusion-for-Remote-Sensing-Image-Generation"><a href="#45-Object-Fidelity-Diffusion-for-Remote-Sensing-Image-Generation" class="headerlink" title="45. Object Fidelity Diffusion for Remote Sensing Image Generation"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Object_Fidelity_Diffusion_for_Remote_Sensing_Image_Generation.pdf">Object Fidelity Diffusion for Remote Sensing Image Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出了Object Fidelity Diffusion（OF-Diff），一种用于遥感图像生成的双分支可控扩散模型，通过引入增强形状生成模块（ESGM）从布局中提取对象先验形状，并结合扩散一致性损失与DDPO微调提升图像多样性与语义一致性。实验表明，OF-Diff能在无需真实图像参考的条件下生成高保真、布局与形状一致的遥感图像，显著提升小目标检测性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Object%20Fidelity%20Diffusion%20for%20Remote%20Sensing%20Image%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-VIDEO-BLADE-BLOCK-SPARSE-ATTENTION-MEETS-STEP-DISTILLATION-FOR-EFFICIENT-VIDEO-GENERATION"><a href="#46-VIDEO-BLADE-BLOCK-SPARSE-ATTENTION-MEETS-STEP-DISTILLATION-FOR-EFFICIENT-VIDEO-GENERATION" class="headerlink" title="46. VIDEO-BLADE: BLOCK-SPARSE ATTENTION MEETS STEP DISTILLATION FOR EFFICIENT VIDEO GENERATION"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Video-BLADE__Block-Sparse_Attention_Meets_Step_Distillation_for_Efficient_Video_Generation.pdf">VIDEO-BLADE: BLOCK-SPARSE ATTENTION MEETS STEP DISTILLATION FOR EFFICIENT VIDEO GENERATION</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了Video-BLADE框架，通过自适应块稀疏注意力（ASA）机制与基于轨迹分布匹配（TDM）的稀疏感知步蒸馏联合训练，实现高效视频扩散生成。ASA动态生成内容感知稀疏掩码，联合稀疏感知蒸馏提升推理速度，同时保持乃至提升视频生成质量。实验表明，在CogVideoX-5B和Wan2.1-1.3B等主流模型上，BLADE可实现8.89×至14.10×加速，且在VBench-2.0等基准上取得更高分数，兼顾效率与质量，且人类评测同样认可其优异表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/VIDEO-BLADE%3A%20BLOCK-SPARSE%20ATTENTION%20MEETS%20STEP%20DISTILLATION%20FOR%20EFFICIENT%20VIDEO%20GENERATION.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-From-Diagnosis-to-Improvement-Probing-Spatio-Physical-Reasoning-in-Vision-Language-Models"><a href="#47-From-Diagnosis-to-Improvement-Probing-Spatio-Physical-Reasoning-in-Vision-Language-Models" class="headerlink" title="47. From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/From_Diagnosis_to_Improvement__Probing_Spatio-Physical_Reasoning_in_Vision_Language_Models.pdf">From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tongji University</span></p><p>本文系统分析了主流视觉语言模型（VLM）在时空物理推理任务中的表现，发现其在基于ShapeStacks基准的静态平衡判断上准确率低，存在类人认知偏见和缺乏深度推理。作者提出采用监督微调（SFT）结合规则驱动的强化学习（RL）对Qwen2.5-VL-7B模型进行二阶段训练，显著提升了其领域内物理推理能力并超越了主流商用模型，但在跨场景泛化能力上仍表现有限，暴露出当前范式以模式匹配为主、难以习得可迁移物理原则的根本局限。结论：主流VLM在物理推理方面存在明显局限，二阶段微调虽可提升领域内性能但泛化不足，未来应发展基于物理因果机制的新范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From%20Diagnosis%20to%20Improvement%3A%20Probing%20Spatio-Physical%20Reasoning%20in%20Vision%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-Privacy-enhancing-Sclera-Segmentation-Benchmarking-Competition-SSBC-2025"><a href="#48-Privacy-enhancing-Sclera-Segmentation-Benchmarking-Competition-SSBC-2025" class="headerlink" title="48. Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Privacy-enhancing_Sclera_Segmentation_Benchmarking_Competition__SSBC_2025.pdf">Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Ljubljana</span></p><p>本论文介绍了SSBC 2025隐私增强型巩膜分割竞赛，重点在于利用合成眼部图像训练隐私保护的巩膜分割模型，并系统比较了合成数据和真实数据混合训练的模型性能。结果表明，专用训练策略下，仅用合成数据训练的模型能取得与真实数据训练相近的分割效果，F1分数可达0.8以上，且方法选择往往比真实数据的加入更关键，突出合成数据在隐私保护生物识别中的可行性和前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Privacy-enhancing%20Sclera%20Segmentation%20Benchmarking%20Competition%3A%20SSBC%202025.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale"><a href="#49-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale" class="headerlink" title="49. NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/NextStep-1__Toward_Autoregressive_Image_Generation_with_Continuous_Tokens_at_Scale.pdf">NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">StepFun</span></p><p>该论文提出NextStep-1，一种结合大规模Transformer主干和流匹配头的自回归模型，实现文本到高保真图像生成和多样化图像编辑。方法创新在于同时处理离散文本和连续图像token，并引入强正则化的图像tokenizer以保证分布稳定，实验结果显示其在多项主流基准上超越现有自回归模型，部分指标接近扩散模型，验证了自回归范式在高质量图像生成上的可行性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/NextStep-1%3A%20Toward%20Autoregressive%20Image%20Generation%20with%20Continuous%20Tokens%20at%20Scale.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-Novel-View-Synthesis-using-DDIM-Inversion"><a href="#50-Novel-View-Synthesis-using-DDIM-Inversion" class="headerlink" title="50. Novel View Synthesis using DDIM Inversion"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Novel_View_Synthesis_using_DDIM_Inversion.pdf">Novel View Synthesis using DDIM Inversion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indraprastha Institute of Information Technology, Delhi</span></p><p>该论文提出了一种基于DDIM倒置和轻量化TUNet网络的单张图像高质量新视角合成方法。流程包括将输入图像通过VAE和DDIM倒置获得潜变量，使用TUNet进行视角转换，结合噪声融合策略强化高频细节，最后借助预训练扩散模型解码生成新视角图像。实验表明方法在MVImgNet等数据集上优于主流方法，能生成高分辨率且几何一致的新视角图像。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Novel%20View%20Synthesis%20using%20DDIM%20Inversion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-Conditional-Information-Bottleneck-for-Multimodal-Fusion-Overcoming-Shortcut-Learning-in-Sarcasm-Detection"><a href="#51-Conditional-Information-Bottleneck-for-Multimodal-Fusion-Overcoming-Shortcut-Learning-in-Sarcasm-Detection" class="headerlink" title="51. Conditional Information Bottleneck for Multimodal Fusion: Overcoming Shortcut Learning in Sarcasm Detection"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Conditional_Information_Bottleneck_for_Multimodal_Fusion__Overcoming_Shortcut_Learning_in_Sarcasm_De.pdf">Conditional Information Bottleneck for Multimodal Fusion: Overcoming Shortcut Learning in Sarcasm Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Artificial Intelligence, Beijing Normal University</span></p><p>本文针对多模态讽刺检测任务中模型容易依赖数据集捷径（如角色标签、罐头笑声、情感不一致性）导致泛化能力差的问题，重构了MUStARD++数据集，去除了相关捷径信号，并提出了多模态条件信息瓶颈（MCIB）融合模型。MCIB通过条件信息瓶颈机制对多模态（文本、音频、视觉）特征进行冗余过滤与互补信息提取，实现高效融合，显著提升了在讽刺检测任务中的表现和泛化能力，实验结果在多个基准数据集上达到最优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Conditional%20Information%20Bottleneck%20for%20Multimodal%20Fusion%3A%20Overcoming%20Shortcut%20Learning%20in%20Sarcasm%20Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-ChatENV-An-Interactive-Vision-Language-Model-for-Sensor-Guided-Environmental-Monitoring-and-Scenario-Simulation"><a href="#52-ChatENV-An-Interactive-Vision-Language-Model-for-Sensor-Guided-Environmental-Monitoring-and-Scenario-Simulation" class="headerlink" title="52. ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ChatENV__An_Interactive_Vision-Language_Model_for_Sensor-Guided_Environmental_Monitoring_and_Scenari.pdf">ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohamed bin Zayed University of Artificial Intelligence</span></p><p>本文提出ChatENV，一个结合卫星遥感图像与真实环境传感器数据的交互式视觉-语言模型。通过构建涵盖全球177k图像、152k时序对和丰富气象&#x2F;排放元数据的大规模数据集，并利用GPT-4o和Gemini 2.0生成多样注释，模型采用Qwen-2.5-VL为主干，LoRA高效微调，实现场景描述、时空变化推理及“假设情景”模拟。实验结果表明，ChatENV在时序推理和情景分析等任务上表现优异，显著提升了环境监测的智能化和实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ChatENV%3A%20An%20Interactive%20Vision-Language%20Model%20for%20Sensor-Guided%20Environmental%20Monitoring%20and%20Scenario%20Simulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-Fourier-Guided-Attention-Upsampling-for-Image-Super-Resolution"><a href="#53-Fourier-Guided-Attention-Upsampling-for-Image-Super-Resolution" class="headerlink" title="53. Fourier-Guided Attention Upsampling for Image Super-Resolution"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Fourier-Guided_Attention_Upsampling_for_Image_Super-Resolution.pdf">Fourier-Guided Attention Upsampling for Image Super-Resolution</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea University of Technology and Education (KOREATECH)</span></p><p>本文提出了一种轻量级的频域引导注意力上采样模块FGA，用于单幅图像超分辨率。该方法通过结合基于傅里叶特征的多层感知机(MLP)进行位置频率编码，跨分辨率相关注意力层进行空间自适应对齐，以及频域L1损失进行频谱一致性监督，在多种主流SISR骨干网络中均提升了重建高频细节和抑制伪影的能力。实验表明，FGA仅增加0.3M参数即可在五个不同超分辨率主干中获得平均0.12–0.14dB的PSNR提升，并在高频内容一致性上提升高达29%，在纹理丰富数据集上尤为显著，证明其在减少混叠和保持细节方面的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Fourier-Guided%20Attention%20Upsampling%20for%20Image%20Super-Resolution.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-Towards-Agentic-AI-for-Multimodal-Guided-Video-Object-Segmentation"><a href="#54-Towards-Agentic-AI-for-Multimodal-Guided-Video-Object-Segmentation" class="headerlink" title="54. Towards Agentic AI for Multimodal-Guided Video Object Segmentation"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Towards_Agentic_AI_for_Multimodal-Guided_Video_Object_Segmentation.pdf">Towards Agentic AI for Multimodal-Guided Video Object Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Deakin University</span></p><p>本文提出M2-Agent，一种用于多模态指导下视频目标分割的免训练智能体系统。该方法利用大型语言模型（LLM）生成每个输入的动态推理流程，并通过与专门的视觉和音频工具集的迭代交互，灵活识别文本和音频描述的目标对象。实验结果表明，M2-Agent在Referring Video Object Segmentation（RVOS）和Reference Audio-Visual Segmentation（Ref-AVS）两项任务上，相较于现有训练自由方法有明显性能提升，且接近有监督最优方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Towards%20Agentic%20AI%20for%20Multimodal-Guided%20Video%20Object%20Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-GCRPNet-Graph-Enhanced-Contextual-and-Regional-Perception-Network-For-Salient-Object-Detection-in-Optical-Remote-Sensing-Images"><a href="#55-GCRPNet-Graph-Enhanced-Contextual-and-Regional-Perception-Network-For-Salient-Object-Detection-in-Optical-Remote-Sensing-Images" class="headerlink" title="55. GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GCRPNet__Graph-Enhanced_Contextual_and_Regional_Perception_Network_For_Salient_Object_Detection_in_O.pdf">GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hainan University</span></p><p>该论文提出了一种基于Mamba架构的光学遥感图像显著性目标检测新方法GCRPNet，集成了视觉状态空间编码器、差异-相似性引导分层图注意模块（DS-HGAM）和局部增强视觉状态空间（LEVSS）解码器。DS-HGAM利用图神经网络实现多尺度特征融合和跨空间建模，LEVSS结合多尺度协同注意力和块内四方向扫描提升局部细节表示。实验结果表明，GCRPNet在ORSSD和EORSSD数据集上显著优于现有方法，具备更强的鲁棒性和精度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GCRPNet%3A%20Graph-Enhanced%20Contextual%20and%20Regional%20Perception%20Network%20For%20Salient%20Object%20Detection%20in%20Optical%20Remote%20Sensing%20Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-PASS-Probabilistic-Agentic-Supernet-Sampling-for-Interpretable-and-Adaptive-Chest-X-Ray-Reasoning"><a href="#56-PASS-Probabilistic-Agentic-Supernet-Sampling-for-Interpretable-and-Adaptive-Chest-X-Ray-Reasoning" class="headerlink" title="56. PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/PASS__Probabilistic_Agentic_Supernet_Sampling_for_Interpretable_and_Adaptive_Chest_X-Ray_Reasoning.pdf">PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Hong Kong</span></p><p>该论文提出PASS框架，通过概率化的agentic supernet采样实现胸部X光的多模态医学推理，核心包括动态采样多工具工作流并用可解释概率标注决策路径。方法包含三阶段训练：专家知识热身、对比路径排序和成本感知强化学习，有效优化准确率与计算成本权衡。实验表明PASS在多项医学推理基准上显著优于主流模型，在保证高准确率的同时实现高可解释性和效率，可广泛用于高安全性要求的医学场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PASS%3A%20Probabilistic%20Agentic%20Supernet%20Sampling%20for%20Interpretable%20and%20Adaptive%20Chest%20X-Ray%20Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-X-Node-Self-Explanation-is-All-We-Need"><a href="#57-X-Node-Self-Explanation-is-All-We-Need" class="headerlink" title="57. X-Node: Self-Explanation is All We Need"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/X-Node__Self-Explanation_is_All_We_Need.pdf">X-Node: Self-Explanation is All We Need</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>该论文提出了一种自解释型图神经网络框架X-Node，在节点分类过程中每个节点能生成自身解释，方法包括构建结构化上下文向量、解释向量生成、LLM文本解释和解释反馈注入GNN流程。实验证明X-Node在MedMNIST和MorphoMNIST图数据集上保持竞争分类性能并生成每节点可解释决策，有效提升模型透明度和临床应用信任度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/X-Node%3A%20Self-Explanation%20is%20All%20We%20Need.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-Multi-Label-Plant-Species-Prediction-with-Metadata-Enhanced-Multi-Head-Vision-Transformers"><a href="#58-Multi-Label-Plant-Species-Prediction-with-Metadata-Enhanced-Multi-Head-Vision-Transformers" class="headerlink" title="58. Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Multi-Label_Plant_Species_Prediction_with_Metadata-Enhanced_Multi-Head_Vision_Transformers.pdf">Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Hamburg</span></p><p>本论文提出了一种基于多头Vision Transformer（ViT）的多标签植物种类预测方法，结合了DINOv2预训练模型、物种-属-科多头分类、元数据增强、多尺度切片、动态阈值优化和集成策略（Hydra架构），以实现对高分辨率植被样方图像中多种植物的自动识别。实验结果显示，该方法在PlantCLEF 2025挑战赛的公共榜单中取得了第三名的成绩，但在私有榜单上表现下降，说明模型对领域转移较为敏感但仍具竞争力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Label%20Plant%20Species%20Prediction%20with%20Metadata-Enhanced%20Multi-Head%20Vision%20Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-From-Images-to-Perception-Emergence-of-Perceptual-Properties-by-Reconstructing-Images"><a href="#59-From-Images-to-Perception-Emergence-of-Perceptual-Properties-by-Reconstructing-Images" class="headerlink" title="59. From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/From_Images_to_Perception__Emergence_of_Perceptual_Properties_by_Reconstructing_Images.pdf">From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Valencia</span></p><p>本论文提出并训练了一种生物启发的视觉系统神经网络PerceptNet，通过自监督学习（包括自编码、去噪、去模糊和稀疏正则化任务）重建自然图像，研究模型内部表征与人类感知一致性的涌现现象。实验表明，在编码器（类V1皮层）阶段，模型表征与人类主观失真评价高度相关，适度的噪声、模糊和稀疏度可增强这种一致性，说明生物视觉系统可能正是优化了去除适度失真和稀疏性，且无需人工感知标签即可学得感知度量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From%20Images%20to%20Perception%3A%20Emergence%20of%20Perceptual%20Properties%20by%20Reconstructing%20Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-MM-Food-100K-A-100-000-Sample-Multimodal-Food-Intelligence-Dataset-with-Verifiable-Provenance"><a href="#60-MM-Food-100K-A-100-000-Sample-Multimodal-Food-Intelligence-Dataset-with-Verifiable-Provenance" class="headerlink" title="60. MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MM-Food-100K__A_100,000-Sample_Multimodal_Food_Intelligence_Dataset_with_Verifiable_Provenance.pdf">MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Codatta Community</span></p><p>该论文提出了MM-Food-100K，一种包含10万条样本的多模态食物智能数据集，通过社区贡献与大型视觉语言模型自动质量审核结合的数据采集流程，提供丰富、多层次的注释及可验证溯源。实验表明，基于MM-Food-100K微调的模型在食物分类与营养回归任务上均优于原始视觉语言模型，且采用区块链协议实现数据溯源与贡献者分润机制，为高质量社区数据集的可持续建设提供了新范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MM-Food-100K%3A%20A%20100%2C000-Sample%20Multimodal%20Food%20Intelligence%20Dataset%20with%20Verifiable%20Provenance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-Adapting-SAM-via-Cross-Entropy-Masking-for-Class-Imbalance-in-Remote-Sensing-Change-Detection"><a href="#61-Adapting-SAM-via-Cross-Entropy-Masking-for-Class-Imbalance-in-Remote-Sensing-Change-Detection" class="headerlink" title="61. Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Adapting_SAM_via_Cross-Entropy_Masking_for_Class_Imbalance_in_Remote_Sensing_Change_Detection.pdf">Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Sydney</span></p><p>该论文提出了基于Segment Anything Model (SAM) 的遥感变化检测框架，通过细致地微调FastSAM编码器，并引入空间-时序特征增强（STFE）和多尺度解码器融合（MSDF），有效提升了多尺度下的变化检测能力。针对变化检测中的类别不平衡问题，创新性地提出了交叉熵掩码损失（CEM），通过在损失计算中随机丢弃未变化像素，显著提升了模型对变化类别的检测能力。实验结果表明，所提方法在Levir-CD、WHU-CD、CLCD和S2Looking等四个变化检测数据集上均取得了优于现有方法的性能，尤其在S2Looking数据集上F1提升2.5%。结论是结合SAM密集预测特性和CEM损失的架构在遥感变化检测任务上优于现有主流方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Adapting%20SAM%20via%20Cross-Entropy%20Masking%20for%20Class%20Imbalance%20in%20Remote%20Sensing%20Change%20Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-PTQAT-A-Hybrid-Parameter-Efficient-Quantization-Algorithm-for-3D-Perception-Tasks"><a href="#62-PTQAT-A-Hybrid-Parameter-Efficient-Quantization-Algorithm-for-3D-Perception-Tasks" class="headerlink" title="62. PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/PTQAT__A_Hybrid_Parameter-Efficient_Quantization_Algorithm_for_3D_Perception_Tasks.pdf">PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>该论文提出PTQAT，一种结合后训练量化（PTQ）和量化感知训练（QAT）的高效混合量化算法，专为3D感知任务设计。方法首先通过PTQ预检查各层量化误差，采用均方误差（MSE）筛选需QAT微调的关键层，其余层保持冻结，重点在误差传播阶段进行补偿。结论表明，PTQAT在3D目标检测、语义分割、占用预测等任务上，能以更少参数微调和更高效率，实现与甚至优于全量QAT的精度，同时便于实际部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PTQAT%3A%20A%20Hybrid%20Parameter-Efficient%20Quantization%20Algorithm%20for%203D%20Perception%20Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-EgoMusic-driven-Human-Dance-Motion-Estimation-with-Skeleton-Mamba"><a href="#63-EgoMusic-driven-Human-Dance-Motion-Estimation-with-Skeleton-Mamba" class="headerlink" title="63. EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/EgoMusic-driven_Human_Dance_Motion_Estimation_with_Skeleton_Mamba.pdf">EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">FPT Software AI Center</span></p><p>本文提出EgoMusic Motion Network和核心Skeleton Mamba模型，实现从第一视角视频和音乐共同驱动的人类舞蹈动作估计。方法包括新型大规模EgoAIST++数据集、多模态特征融合、条件扩散模型和空间-时间扫描策略，实验显示在多项舞蹈动作估计指标上明显优于现有方法，并能泛化至实际场景。结论：联合利用视觉和音乐输入可大幅提升人体舞蹈动作推断的准确性和协调性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/EgoMusic-driven%20Human%20Dance%20Motion%20Estimation%20with%20Skeleton%20Mamba.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-TweezeEdit-Consistent-and-Efficient-Image-Editing-with-Path-Regularization"><a href="#64-TweezeEdit-Consistent-and-Efficient-Image-Editing-with-Path-Regularization" class="headerlink" title="64. TweezeEdit: Consistent and Efficient Image Editing with Path Regularization"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/TweezeEdit__Consistent_and_Efficient_Image_Editing_with_Path_Regularization.pdf">TweezeEdit: Consistent and Efficient Image Editing with Path Regularization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>本文提出TweezeEdit，一种基于一致性模型的无调优、无反演路径正则化图像编辑框架，针对扩散模型下现有方法过度对齐目标提示、难以保留源图像语义的问题。该方法通过全程正则化去噪路径并引入梯度驱动的路径约束，在无需模型结构修改的情况下，提高语义保留和编辑效率。实验结果显示，TweezeEdit在语义一致性、目标对齐及推理速度上优于现有主流方法，可在12步内完成编辑，具备实时应用潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/TweezeEdit%3A%20Consistent%20and%20Efficient%20Image%20Editing%20with%20Path%20Regularization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-STAMP-Multi-pattern-Attention-aware-Multiple-Instance-Learning-for-STAS-Diagnosis-in-Multi-center-Histopathology-Images"><a href="#65-STAMP-Multi-pattern-Attention-aware-Multiple-Instance-Learning-for-STAS-Diagnosis-in-Multi-center-Histopathology-Images" class="headerlink" title="65. STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/STAMP__Multi-pattern_Attention-aware_Multiple_Instance_Learning_for_STAS_Diagnosis_in_Multi-center_H.pdf">STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hunan University</span></p><p>该论文提出STAMP方法，通过双分支Transformer架构和多模式注意力聚合模块，分析多中心肺癌组织病理图像中的STAS特征，实现病灶区域的动态选择和噪声抑制，提升全局表征的判别力。实验表明STAMP在三大多中心数据集上AUC均超过0.79，诊断性能优于临床水平，具有临床辅助诊断潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/STAMP%3A%20Multi-pattern%20Attention-aware%20Multiple%20Instance%20Learning%20for%20STAS%20Diagnosis%20in%20Multi-center%20Histopathology%20Images.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-NanoControl-A-Lightweight-Framework-for-Precise-and-Efficient-Control-in-Diffusion-Transformer"><a href="#66-NanoControl-A-Lightweight-Framework-for-Precise-and-Efficient-Control-in-Diffusion-Transformer" class="headerlink" title="66. NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/NanoControl__A_Lightweight_Framework_for_Precise_and_Efficient_Control_in_Diffusion_Transformer.pdf">NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">360 AI Research</span></p><p>本文提出NanoControl——一种基于Diffusion Transformer（DiT）的轻量级可控文本到图像生成方法。其创新点在于采用LoRA风格控制模块和KV-Context Augmentation机制，极大减少参数与计算量，仅增加0.024%参数和0.029% GFLOPs，实现高效条件融合与精确可控生成。实验表明，NanoControl在多项条件控制任务（如边缘、深度、着色、HED）上实现了优于现有方法的可控性和图像质量，同时大幅降低模型复杂度，非常适合实际部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/NanoControl%3A%20A%20Lightweight%20Framework%20for%20Precise%20and%20Efficient%20Control%20in%20Diffusion%20Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-Towards-Spatially-Consistent-Image-Generation-On-Incorporating-Intrinsic-Scene-Properties-into-Diffusion-Models"><a href="#67-Towards-Spatially-Consistent-Image-Generation-On-Incorporating-Intrinsic-Scene-Properties-into-Diffusion-Models" class="headerlink" title="67. Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Towards_Spatially_Consistent_Image_Generation__On_Incorporating_Intrinsic_Scene_Properties_into_Diff.pdf">Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AI Institute, Seoul National University</span></p><p>本文提出了Intrinsic Latent Diffusion Model (I-LDM)，通过联合生成图片和其对应的场景内在属性（深度、法线、分割图、线描），提升文本到图像生成模型的空间一致性。方法利用预训练估算器自动提取场景属性，通过自注意力跨域调度和VAE编码实现高效协同生成。实验显示I-LDM在多种基线模型和复杂场景下均显著改善了空间布局的自然性和一致性，同时保持了原始模型的图像质量和文本匹配能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Towards%20Spatially%20Consistent%20Image%20Generation%3A%20On%20Incorporating%20Intrinsic%20Scene%20Properties%20into%20Diffusion%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning"><a href="#68-SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning" class="headerlink" title="68. SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SynBrain__Enhancing_Visual-to-fMRI_Synthesis_via_Probabilistic_Representation_Learning.pdf">SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Artificial Intelligence Laboratory</span></p><p>该论文提出了SynBrain框架，通过引入概率建模与语义条件约束，采用BrainVAE和S2N Mapper关键模块，实现视觉刺激到fMRI脑响应的概率分布建模和一步语义到神经映射。实验结果表明，SynBrain在个体特异性编码和少样本自适应性能上显著优于现有方法，并生成高质量fMRI信号提升神经解码效果，同时揭示脑功能的一致性和生物神经变异性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SynBrain%3A%20Enhancing%20Visual-to-fMRI%20Synthesis%20via%20Probabilistic%20Representation%20Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-DINOMotion-advanced-robust-tissue-motion-tracking-with-DINOv2-in-2D-Cine-MRI-guided-radiotherapy"><a href="#69-DINOMotion-advanced-robust-tissue-motion-tracking-with-DINOv2-in-2D-Cine-MRI-guided-radiotherapy" class="headerlink" title="69. DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/DINOMotion__advanced_robust_tissue_motion_tracking_with_DINOv2_in_2D-Cine_MRI-guided_radiotherapy.pdf">DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Concordia University</span></p><p>本文提出DINOMotion，一种基于DINOv2基础视觉模型和Low-Rank Adaptation (LoRA) 层的深度学习框架，用于2D-Cine MRI引导放疗中的组织运动追踪。该方法自动检测对应标志点以实现高效和可解释的图像配准，并能处理大幅度错位情况，在多个器官的Dice分数和Hausdorff距离评测中均优于现有方法，具有较强的实时性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/DINOMotion%3A%20advanced%20robust%20tissue%20motion%20tracking%20with%20DINOv2%20in%202D-Cine%20MRI-guided%20radiotherapy.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-SC-Lane-Slope-aware-and-Consistent-Road-Height-Estimation-Framework-for-3D-Lane-Detection"><a href="#70-SC-Lane-Slope-aware-and-Consistent-Road-Height-Estimation-Framework-for-3D-Lane-Detection" class="headerlink" title="70. SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SC-Lane__Slope-aware_and_Consistent_Road_Height_Estimation_Framework_for_3D_Lane_Detection.pdf">SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University</span></p><p>本文提出了SC-Lane，一种面向坡度且具有时序一致性的3D车道检测道路高度估计框架。方法核心包括坡度自适应特征模块，动态融合多坡度高度特征，并引入高度一致性模块，通过利用图像线索自适应加权及时序一致性损失，提升多帧高度估计的鲁棒性和准确性。实验采用MAE、RMSE和阈值精度等新标准，OpenLane基准上SC-Lane在高度估计和3D车道检测上均达到SOTA，F-score 64.3%。结论表明该方法提升了车道检测的准确性和高度估计的稳定性，可扩展至自动驾驶相关任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SC-Lane%3A%20Slope-aware%20and%20Consistent%20Road%20Height%20Estimation%20Framework%20for%203D%20Lane%20Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-Translation-of-Text-Embedding-via-Delta-Vector-to-Suppress-Strongly-Entangled-Content-in-Text-to-Image-Diffusion-Models"><a href="#71-Translation-of-Text-Embedding-via-Delta-Vector-to-Suppress-Strongly-Entangled-Content-in-Text-to-Image-Diffusion-Models" class="headerlink" title="71. Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Translation_of_Text_Embedding_via_Delta_Vector_to_Suppress_Strongly_Entangled_Content_in_Text-to-Ima.pdf">Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sungkyunkwan University</span></p><p>本文提出了通过在文本嵌入空间引入delta vector来直接抑制文本到图像扩散模型中强关联负内容的方法，并进一步提出了Selective Suppression with Delta Vector（SSDV），将delta vector集成到跨注意力机制，实现对特定区域负内容的精准抑制。实验结果表明，该方法在标准和个性化模型上均显著优于现有基线，在定量和定性评测中实现了更有效的强关联负内容抑制。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Translation%20of%20Text%20Embedding%20via%20Delta%20Vector%20to%20Suppress%20Strongly%20Entangled%20Content%20in%20Text-to-Image%20Diffusion%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-PQ-DAF-Pose-driven-Quality-controlled-Data-Augmentation-for-Data-scarce-Driver-Distraction-Detection"><a href="#72-PQ-DAF-Pose-driven-Quality-controlled-Data-Augmentation-for-Data-scarce-Driver-Distraction-Detection" class="headerlink" title="72. PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/PQ-DAF__Pose-driven_Quality-controlled_Data_Augmentation_for_Data-scarce_Driver_Distraction_Detectio.pdf">PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shandong University of Science and Technology</span></p><p>本文提出了PQ-DAF（Pose-driven Quality-controlled Data Augmentation Framework），结合姿态驱动的渐进式条件扩散模型（PCDMs）和视觉-语言模型（CogVLM）实现高质量驾驶员分心检测数据增强。方法通过提取驾驶员关键点姿态，利用PCDMs生成结构一致的伪样本，并用CogVLM对合成样本进行语义一致性筛选，大幅提升少样本下的模型泛化能力。实验结果显示，PQ-DAF在StateFarm和AUC-DDD数据集少样本设定下均显著提升检测准确率，验证了其在数据稀缺场景下的实用性和有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PQ-DAF%3A%20Pose-driven%20Quality-controlled%20Data%20Augmentation%20for%20Data-scarce%20Driver%20Distraction%20Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-Unlocking-Robust-Semantic-Segmentation-Performance-via-Label-only-Elastic-Deformations-against-Implicit-Label-Noise"><a href="#73-Unlocking-Robust-Semantic-Segmentation-Performance-via-Label-only-Elastic-Deformations-against-Implicit-Label-Noise" class="headerlink" title="73. Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Unlocking_Robust_Semantic_Segmentation_Performance_via_Label-only_Elastic_Deformations_against_Impli.pdf">Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">GIST</span></p><p>本文提出了一种名为NSegment+的新型数据增强框架，专为语义分割中的隐性标签噪声问题设计。方法核心在于仅对分割标签施加随机弹性形变（结合高斯平滑和尺度感知的小目标保护），而保持输入图像不变，从而模拟真实数据中的边界模糊和标注不一致，提升模型对标签不确定性的鲁棒性。大量实验表明，NSegment+在遥感和自然场景六大主流数据集上，平均mIoU提升显著，且与CutMix、Label Smoothing等现有增强&#x2F;正则化手段兼容，无需增加计算开销。结论认为，NSegment+为解决现实语义分割中的隐性标签噪声提供了轻量、高效且通用的解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Unlocking%20Robust%20Semantic%20Segmentation%20Performance%20via%20Label-only%20Elastic%20Deformations%20against%20Implicit%20Label%20Noise.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-AtomDiffuser-Time-Aware-Degradation-Modeling-for-Drift-and-Beam-Damage-in-STEM-Imaging"><a href="#74-AtomDiffuser-Time-Aware-Degradation-Modeling-for-Drift-and-Beam-Damage-in-STEM-Imaging" class="headerlink" title="74. AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/AtomDiffuser__Time-Aware_Degradation_Modeling_for_Drift_and_Beam_Damage_in_STEM_Imaging.pdf">AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Clemson University</span></p><p>该论文提出AtomDiffuser，一种结合时序卷积U-Net和物理启发降解建模的深度学习框架，能够从时序STEM图像中解耦并显式预测原子级漂移（通过仿射变换）和束流损伤（通过空间衰减图），利用合成降解序列训练并能泛化到真实冷冻STEM数据。实验表明，AtomDiffuser在损伤评估和漂移矫正任务上显著优于现有CNN方法，支持高分辨率降解推断和动态材料结构演化的可视化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AtomDiffuser%3A%20Time-Aware%20Degradation%20Modeling%20for%20Drift%20and%20Beam%20Damage%20in%20STEM%20Imaging.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-Improving-OCR-for-Historical-Texts-of-Multiple-Languages"><a href="#75-Improving-OCR-for-Historical-Texts-of-Multiple-Languages" class="headerlink" title="75. Improving OCR for Historical Texts of Multiple Languages"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Improving_OCR_for_Historical_Texts_of_Multiple_Languages.pdf">Improving OCR for Historical Texts of Multiple Languages</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Groningen</span></p><p>该论文提出了多语言历史文献的OCR与文档版面分析方法。针对死海古卷希伯来文，采用数据增强并比较了Kraken与TrOCR模型效果；在荷兰历史会议记录上，结合DeepLabV3+语义分割与BiLSTM，并通过伪标签提升训练效果；在现代英文手写识别任务中，采用CRNN与ResNet34编码并使用CTC损失。实验表明：数据增强与转移学习能显著提升历史文献OCR，伪标签自监督策略能有效提升文档结构分析的性能，复杂模型在小数据集上未必优于简单架构，未来需加强数据多样性与模型设计的平衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving%20OCR%20for%20Historical%20Texts%20of%20Multiple%20Languages.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-Efficient-Image-Denoising-Using-Global-and-Local-Circulant-Representation"><a href="#76-Efficient-Image-Denoising-Using-Global-and-Local-Circulant-Representation" class="headerlink" title="76. Efficient Image Denoising Using Global and Local Circulant Representation"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Efficient_Image_Denoising_Using_Global_and_Local_Circulant_Representation.pdf">Efficient Image Denoising Using Global and Local Circulant Representation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China University of Technology</span></p><p>本文提出了一种高效的图像去噪算法Haar-tSVD，通过结合全球和局部循环结构，利用t-SVD和Haar变换在循环表示下有效捕获图像块的相关性，实现一步高并行可扩展的滤波，无需学习局部基。方法还引入CNN自适应噪声估计和特征值分析提升鲁棒性。实验表明，该方法在图像、视频、HSI、MRI等多任务中具有效果和效率并重的优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Efficient%20Image%20Denoising%20Using%20Global%20and%20Local%20Circulant%20Representation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-Improving-Learning-of-New-Diseases-through-Knowledge-Enhanced-Initialization-for-Federated-Adapter-Tuning"><a href="#77-Improving-Learning-of-New-Diseases-through-Knowledge-Enhanced-Initialization-for-Federated-Adapter-Tuning" class="headerlink" title="77. Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Improving_Learning_of_New_Diseases_through_Knowledge-Enhanced_Initialization_for_Federated_Adapter_T.pdf">Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR)</span></p><p>本文提出了一种名为FedKEI的联邦知识增强初始化框架，结合了全局聚类和双层聚合权重学习，实现了医疗领域中基础模型适配器调优过程中新任务的高效适应。FedKEI通过服务器端对任务特定模块进行聚类，并采用双层优化策略协同学习聚合权重，有效整合并转移跨任务与跨客户的知识，从而为适配器和头部生成更优的初始化。实验结果显示，FedKEI在皮肤、胸部X光和视网膜OCT等多模态医学影像数据集上，在新疾病适应任务中均超过现有最新方法，提升了新任务学习的准确性和效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving%20Learning%20of%20New%20Diseases%20through%20Knowledge-Enhanced%20Initialization%20for%20Federated%20Adapter%20Tuning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-A-Sub-Pixel-Multimodal-Optical-Remote-Sensing-Images-Matching-Method"><a href="#78-A-Sub-Pixel-Multimodal-Optical-Remote-Sensing-Images-Matching-Method" class="headerlink" title="78. A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Sub-Pixel_Multimodal_Optical_Remote_Sensing_Images_Matching_Method.pdf">A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Central South University</span></p><p>本文提出了一种基于相位一致性加权最小绝对偏差（PCWLAD）的亚像素多模态光学遥感图像匹配方法。方法流程包括：首先利用结构相似性（SSIM）在未降噪的相位一致性（PC）图上进行粗匹配，然后通过互结构加权和加权最小绝对偏差（WLAD）准则，实现亚像素级别的精确匹配。实验结果显示，在三个类型的可见-红外遥感数据集上，PCWLAD在匹配正确率（CMR）和均方根误差（RMSE）方面均优于现有七种主流方法，平均RMSE约为0.4像素；但方法在大几何变形场景下效果有限。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Sub-Pixel%20Multimodal%20Optical%20Remote%20Sensing%20Images%20Matching%20Method.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-Deep-Learning-for-Crack-Detection-A-Review-of-Learning-Paradigms-Generalizability-and-Datasets"><a href="#79-Deep-Learning-for-Crack-Detection-A-Review-of-Learning-Paradigms-Generalizability-and-Datasets" class="headerlink" title="79. Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Deep_Learning_for_Crack_Detection__A_Review_of_Learning_Paradigms,_Generalizability,_and_Datasets.pdf">Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Georgia Institute of Technology</span></p><p>本论文系统回顾了基于深度学习的裂缝检测领域，聚焦于监督、半监督、弱监督、无监督、少样本、领域自适应及基础模型等多种学习范式的技术演进和泛化能力，详述了主流方法（如CNN、Transformer、SAM等）、关键流程及代表性数据集，同时发布了高分辨率3D激光扫描裂缝数据集3DCrack。结论指出，未来需进一步突破数据高效利用、模型泛化、标准化评测与多模态融合等挑战，以实现更鲁棒、可扩展的裂缝自动检测系统。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Deep%20Learning%20for%20Crack%20Detection%3A%20A%20Review%20of%20Learning%20Paradigms%2C%20Generalizability%2C%20and%20Datasets.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="80-Performance-of-GPT-5-in-Brain-Tumor-MRI-Reasoning"><a href="#80-Performance-of-GPT-5-in-Brain-Tumor-MRI-Reasoning" class="headerlink" title="80. Performance of GPT-5 in Brain Tumor MRI Reasoning"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Performance_of_GPT-5_in_Brain_Tumor_MRI_Reasoning.pdf">Performance of GPT-5 in Brain Tumor MRI Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Emory University School of Medicine</span></p><p>本论文系统评估了GPT-4o、GPT-5-nano、GPT-5-mini和GPT-5在三类脑肿瘤MRI多模态视觉问答(VQA)基准上的表现，方法包括将BraTS数据集的多序列MRI与结构化临床特征转化为标准化VQA问题，并采用zero-shot chain-of-thought提示进行模型推理与准确率对比。结果显示，GPT-5-mini取得最高平均准确率(44.19%)，但各模型性能相近，尚未达到临床实用水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Performance%20of%20GPT-5%20in%20Brain%20Tumor%20MRI%20Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-A-Survey-on-Diffusion-Language-Models"><a href="#81-A-Survey-on-Diffusion-Language-Models" class="headerlink" title="81. A Survey on Diffusion Language Models"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Survey_on_Diffusion_Language_Models.pdf">A Survey on Diffusion Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohamed bin Zayed University of Artificial Intelligence</span></p><p>本文系统梳理了扩散语言模型（DLMs）的原理、发展、技术和挑战，涵盖连续、离散及混合扩散模型，并细致分析了其预训练、后训练（包括强化学习对齐）、推理优化、并行解码、缓存和多模态扩展等关键技术流程。结论指出，DLMs在推理速度、上下文建模和多模态统一方面展现出独特优势，与主流自回归模型性能相当，同时也面临并行一致性、基础设施和可扩展性等挑战，未来具备广阔研究与应用前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Survey%20on%20Diffusion%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-Reinforced-Language-Models-for-Sequential-Decision-Making"><a href="#82-Reinforced-Language-Models-for-Sequential-Decision-Making" class="headerlink" title="82. Reinforced Language Models for Sequential Decision Making"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Reinforced_Language_Models_for_Sequential_Decision_Making.pdf">Reinforced Language Models for Sequential Decision Making</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Southampton</span></p><p>本文提出了Multi-Step Group-Relative Policy Optimization（MS-GRPO）算法，通过在Text-Mediated Stochastic Game（TMSG）和Language Agent Policy（LAP）框架下，将全局回报分配到每个决策步骤，并结合绝对优势加权（AAW）采样策略，以提升小型LLM的序列决策能力。实验表明，经过该方法微调的3B参数模型在Frozen Lake任务上决策能力提升，超越了72B参数的大模型50%；同时方法在效率和泛化能力上也表现出优势，但与专用DQN方法相比仍存在性能差距。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Reinforced%20Language%20Models%20for%20Sequential%20Decision%20Making.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-Memory-Augmented-Transformers-A-Systematic-Review-from-Neuroscience-Principles-to-Technical-Solutions"><a href="#83-Memory-Augmented-Transformers-A-Systematic-Review-from-Neuroscience-Principles-to-Technical-Solutions" class="headerlink" title="83. Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Memory-Augmented_Transformers__A_Systematic_Review_from_Neuroscience_Principles_to_Technical_Solutio.pdf">Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Technologies</span></p><p>本文系统综述了基于神经科学原理（如多时间尺度动态记忆、选择性注意和巩固）与工程实现相结合的Memory-Augmented Transformers技术，提出了三维分类体系，分析了核心操作（读写、遗忘、容量管理）及其实现方式，展示了领域从静态缓存向自适应推理和终身学习系统的演变。结论指出，混合存储、自适应动态和智能遗忘成为主流，但在可扩展性、干扰协调和评测标准化等方面仍面临挑战，未来需借鉴生物记忆机制推动认知型AI发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Memory-Augmented%20Transformers%3A%20A%20Systematic%20Review%20from%20Neuroscience%20Principles%20to%20Technical%20Solutions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-The-Knowledge-Reasoning-Dissociation-Fundamental-Limitations-of-LLMs-in-Clinical-Natural-Language-Inference"><a href="#84-The-Knowledge-Reasoning-Dissociation-Fundamental-Limitations-of-LLMs-in-Clinical-Natural-Language-Inference" class="headerlink" title="84. The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/The_Knowledge-Reasoning_Dissociation__Fundamental_Limitations_of_LLMs_in_Clinical_Natural_Language_I.pdf">The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Manchester</span></p><p>该论文提出了一个针对临床推理的自然语言推断（NLI）基准，包括四类推理任务（因果归因、组合性基础、认知验证和风险抽象），并设计了GKMRV探针用于区分知识检索与推理能力。实验发现，当前主流大语言模型在知识检索任务上表现极佳，但在结构化推理任务上普遍失败，结论认为仅靠模型规模扩展无法实现可靠的临床推理，需要更强的可组合内部表示。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/The%20Knowledge-Reasoning%20Dissociation%3A%20Fundamental%20Limitations%20of%20LLMs%20in%20Clinical%20Natural%20Language%20Inference.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-Modeling-Human-Responses-to-Multimodal-AI-Content"><a href="#85-Modeling-Human-Responses-to-Multimodal-AI-Content" class="headerlink" title="85. Modeling Human Responses to Multimodal AI Content"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Modeling_Human_Responses_to_Multimodal_AI_Content.pdf">Modeling Human Responses to Multimodal AI Content</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Salesforce Research Asia</span></p><p>该论文提出了T-Lens系统，通过人类中心的方法预测用户对多模态AI生成内容（AIGC）的反应，核心模块HR-MCP基于大规模人类标注数据，结合多模态一致性和情感一致性，能量化信任度、影响和开放性等指标，并可无缝集成至任意LLM。实验表明，T-Lens在模拟人类对多模态信息的信任与传播倾向上优于现有大模型及传统方法，为AI驱动的虚假信息治理提供了新工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Modeling%20Human%20Responses%20to%20Multimodal%20AI%20Content.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-GenOM-Ontology-Matching-with-Description-Generation-and-Large-Language-Model"><a href="#86-GenOM-Ontology-Matching-with-Description-Generation-and-Large-Language-Model" class="headerlink" title="86. GenOM: Ontology Matching with Description Generation and Large Language Model"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GenOM__Ontology_Matching_with_Description_Generation_and_Large_Language_Model.pdf">GenOM: Ontology Matching with Description Generation and Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Manchester</span></p><p>本文提出了GenOM，一种结合大语言模型（LLM）与文本定义生成的本体对齐框架。其方法流程包括：从本体中提取结构与词汇信息，用LLM生成包含领域知识的文本定义，利用嵌入模型进行候选检索，再通过LLM判别候选对的语义等价性，并与传统精确匹配工具融合提升精度。实验表明，GenOM在OAEI Bio-ML基准上表现优异，超越多种传统与现有LLM方案，且模块化设计和定义增强策略有效提升了泛化性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GenOM%3A%20Ontology%20Matching%20with%20Description%20Generation%20and%20Large%20Language%20Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-Learning-from-Natural-Language-Feedback-for-Personalized-Question-Answering"><a href="#87-Learning-from-Natural-Language-Feedback-for-Personalized-Question-Answering" class="headerlink" title="87. Learning from Natural Language Feedback for Personalized Question Answering"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Learning_from_Natural_Language_Feedback_for_Personalized_Question_Answering.pdf">Learning from Natural Language Feedback for Personalized Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Massachusetts Amherst</span></p><p>该论文提出VAC框架，用自然语言反馈（NLF）替代传统个性化问答中用于训练大语言模型的标量奖励信号。VAC通过交替优化反馈模型和策略模型，利用用户画像和问题叙述生成个性化反馈，有效提升模型的个性化响应能力。实验表明，VAC在LaMP-QA基准测试上显著优于现有方法，并被人工评价为更优，表明自然语言反馈为个性化问答优化提供了更有效的信号。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning%20from%20Natural%20Language%20Feedback%20for%20Personalized%20Question%20Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-FROGENT-An-End-to-End-Full-process-Drug-Design-Agent"><a href="#88-FROGENT-An-End-to-End-Full-process-Drug-Design-Agent" class="headerlink" title="88. FROGENT: An End-to-End Full-process Drug Design Agent"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/FROGENT__An_End-to-End_Full-process_Drug_Design_Agent.pdf">FROGENT: An End-to-End Full-process Drug Design Agent</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Artificial Intelligence, Shenzhen University</span></p><p>FROGENT is an agentic framework leveraging Large Language Models (LLMs) and the Model Context Protocol (MCP) to unify and automate the entire drug discovery workflow, integrating dynamic biochemical databases, extensible tool libraries, and specialized AI models for tasks such as target identification, molecule generation, and retrosynthetic planning. Experimental results show FROGENT significantly outperforms state-of-the-art LLM agents and commercial models on eight drug discovery benchmarks, tripling hit-finding and doubling interaction profiling scores, thus streamlining and accelerating drug research.</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FROGENT%3A%20An%20End-to-End%20Full-process%20Drug%20Design%20Agent.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-EgoCross-Benchmarking-Multimodal-Large-Language-Models-for-Cross-Domain-Egocentric-Video-Question-Answering"><a href="#89-EgoCross-Benchmarking-Multimodal-Large-Language-Models-for-Cross-Domain-Egocentric-Video-Question-Answering" class="headerlink" title="89. EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/EgoCross__Benchmarking_Multimodal_Large_Language_Models_for_Cross-Domain_Egocentric_Video_Question_A.pdf">EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">East China Normal University</span></p><p>本文提出了EgoCross基准，专为评估多模态大语言模型（MLLMs）在跨领域第一视角视频问答（EgocentricQA）中的泛化能力。通过涵盖外科手术、工业、极限运动和动物视角四大领域，共包含约1000对问答，支持识别、定位、预测与计数等多类任务，并采用CloseQA与OpenQA两种评测方式。实验发现，当前主流MLLMs在非日常领域表现显著下降，泛化能力有限。论文还探索了Prompt学习、微调和强化学习等提升方法，结果显示强化学习对提升跨域泛化效果最显著。结论表明，EgoCross为未来更具鲁棒性和适应性的多模态大模型研究提供了坚实基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/EgoCross%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Cross-Domain%20Egocentric%20Video%20Question%20Answering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-Advancing-Autonomous-Incident-Response-Leveraging-LLMs-and-Cyber-Threat-Intelligence"><a href="#90-Advancing-Autonomous-Incident-Response-Leveraging-LLMs-and-Cyber-Threat-Intelligence" class="headerlink" title="90. Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Advancing_Autonomous_Incident_Response__Leveraging_LLMs_and_Cyber_Threat_Intelligence.pdf">Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">OODRIVE-Trusted Cloud Solutions</span></p><p>本文提出一种基于检索增强生成（RAG）的大语言模型自动化网络安全事件响应框架，通过结合NLP相似性检索和标准化CTI平台查询，实现对安全警报的上下文丰富与关联，并生成精确可操作的应对策略。实验结果表明，该方法在提高响应准确性、效率及减轻分析师工作负担方面表现优异，自动评估与专家验证均证实系统的鲁棒性和有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Advancing%20Autonomous%20Incident%20Response%3A%20Leveraging%20LLMs%20and%20Cyber%20Threat%20Intelligence.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-MSRS-Adaptive-Multi-Subspace-Representation-Steering-for-Attribute-Alignment-in-Large-Language-Models"><a href="#91-MSRS-Adaptive-Multi-Subspace-Representation-Steering-for-Attribute-Alignment-in-Large-Language-Models" class="headerlink" title="91. MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MSRS__Adaptive_Multi-Subspace_Representation_Steering_for_Attribute_Alignment_in_Large_Language_Mode.pdf">MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</span></p><p>本文提出了MSRS（Multi-Subspace Representation Steering）框架，通过为每个属性分配正交子空间并结合共享子空间，实现对大语言模型多属性的高效精准控制。方法利用SVD自适应分配子空间容量，并引入动态Token选择与子空间加权机制，有效减少属性间干扰。实验表明，MSRS在真值性、公平性、指令遵循和拒绝等多属性任务上，显著优于现有方法，且保持了模型的通用能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MSRS%3A%20Adaptive%20Multi-Subspace%20Representation%20Steering%20for%20Attribute%20Alignment%20in%20Large%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-MCP2OSC-Parametric-Control-by-Natural-Language"><a href="#92-MCP2OSC-Parametric-Control-by-Natural-Language" class="headerlink" title="92. MCP2OSC: Parametric Control by Natural Language"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MCP2OSC__Parametric_Control_by_Natural_Language.pdf">MCP2OSC: Parametric Control by Natural Language</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Los Angeles, CA</span></p><p>本论文提出了MCP2OSC服务端和一套针对OpenSoundControl (OSC) 参数控制的自然语言提示设计准则，通过与Claude等大语言模型集成，实现了用自然语言生成、解释、调试和管理OSC消息，涵盖批量与流式控制、日志分析、可视化等多种实用场景。实验结果表明，该系统显著降低了复杂多媒体设备与创作流程中的OSC开发难度，提高了人机协作效率，展现了基于LLM的通用控制机制潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MCP2OSC%3A%20Parametric%20Control%20by%20Natural%20Language.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-Facilitating-the-Adoption-of-Causal-Inference-Methods-Through-LLM-Empowered-Co-Pilot"><a href="#93-Facilitating-the-Adoption-of-Causal-Inference-Methods-Through-LLM-Empowered-Co-Pilot" class="headerlink" title="93. Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Technical_Report__Facilitating_the_Adoption_of_Causal_Inference_Methods_Through_LLM-Empowered_Co-Pil.pdf">Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ataraxis.ai</span></p><p>本文提出了CATE-B系统，利用大语言模型（LLM）结合因果发现算法，帮助用户完成结构因果模型建设、不确定性最小调整集（MUAS）选择和回归方法推荐，实现从观测数据到处理效应估计的全流程自动化。实验表明，CATE-B在多数据集上提高了因果推断的准确性和健壮性，降低了领域专家门槛，促进了因果推断方法的普及与标准化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Facilitating%20the%20Adoption%20of%20Causal%20Inference%20Methods%20Through%20LLM-Empowered%20Co-Pilot.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-When-Language-Overrules-Revealing-Text-Dominance-in-Multimodal-Large-Language-Models"><a href="#94-When-Language-Overrules-Revealing-Text-Dominance-in-Multimodal-Large-Language-Models" class="headerlink" title="94. When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/When_Language_Overrules__Revealing_Text_Dominance_in_Multimodal_Large_Language_Models.pdf">When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Computing Technology, Chinese Academy of Sciences</span></p><p>本文系统分析了多模态大语言模型（MLLMs）在多种模态（图像、视频、音频、时间序列和图结构数据）上的文本优势现象，提出了模态优势指数（MDI）和注意力效率指数（AEI）作为衡量指标，并通过实验发现文本模态在现有模型中普遍占主导地位。针对这一问题，作者提出了基于[CLS]注意力引导的非文本token压缩方法，有效缓解了注意力分配失衡并促进更公平的多模态融合，结论为压缩非文本token能显著改善多模态模型的模态均衡性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/When%20Language%20Overrules%3A%20Revealing%20Text%20Dominance%20in%20Multimodal%20Large%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-A-Unified-Multi-Agent-Framework-for-Universal-Multimodal-Understanding-and-Generation"><a href="#95-A-Unified-Multi-Agent-Framework-for-Universal-Multimodal-Understanding-and-Generation" class="headerlink" title="95. A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Unified_Multi-Agent_Framework_for_Universal_Multimodal_Understanding_and_Generation.pdf">A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of General Artificial Intelligence, BIGAI</span></p><p>本文提出MAGUS框架，通过认知与推理（Cognition）和表达与生成（Deliberation）两阶段分离，采用多智能体协作机制，将多模态大语言模型（MLLM）与扩散模型有机结合，实现文本、图像、音频、视频等任意模态的理解和生成。核心技术包括Growth-Aware Search（GAS）算法，支持多轮智能体对话、结构化任务规划、动态优化生成质量，无需联合训练即可灵活扩展。实验结果表明，MAGUS在多模态理解与生成任务的主流基准上均优于现有强基线及闭源大模型（如GPT-4o），具备高可扩展性和强指令跟随能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Unified%20Multi-Agent%20Framework%20for%20Universal%20Multimodal%20Understanding%20and%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-SEQ-GPT-LLM-assisted-Spatial-Query-via-Example"><a href="#96-SEQ-GPT-LLM-assisted-Spatial-Query-via-Example" class="headerlink" title="96. SEQ-GPT: LLM-assisted Spatial Query via Example"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SEQ-GPT__LLM-assisted_Spatial_Query_via_Example.pdf">SEQ-GPT: LLM-assisted Spatial Query via Example</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>该论文提出了SEQ-GPT系统，将大语言模型（LLM）与空间示例查询（SEQ）结合，实现了用户通过自然语言和实例对空间位置进行灵活检索。方法包括多模型协作的数据解析与对话调度、数据合成与微调，并支持多轮交互和反馈修正，显著提升了复杂空间查询的易用性和灵活性。实验展示了SEQ-GPT在真实场景下对多地点检索任务的有效性和用户体验提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SEQ-GPT%3A%20LLM-assisted%20Spatial%20Query%20via%20Example.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs"><a href="#97-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs" class="headerlink" title="97. HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/HumanSense__From_Multimodal_Perception_to_Empathetic_Context-Aware_Responses_through_Reasoning_MLLMs.pdf">HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>该论文提出了HumanSense基准，系统评估多模态大语言模型（MLLMs）在人类中心场景下的感知、理解与互动能力，涵盖视觉、音频和文本。通过构建多层次任务体系、模态消融实验及多阶段模态递进式强化学习，发现多模态推理能显著提升模型在复杂人机交互场景下的表现，并通过提示工程无训练提升非推理模型效果，为未来AI互动体验指明方向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/HumanSense%3A%20From%20Multimodal%20Perception%20to%20Empathetic%20Context-Aware%20Responses%20through%20Reasoning%20MLLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-Improving-Value-based-Process-Verifier-via-Low-Cost-Variance-Reduction"><a href="#98-Improving-Value-based-Process-Verifier-via-Low-Cost-Variance-Reduction" class="headerlink" title="98. Improving Value-based Process Verifier via Low-Cost Variance Reduction"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Improving_Value-based_Process_Verifier_via_Low-Cost_Variance_Reduction.pdf">Improving Value-based Process Verifier via Low-Cost Variance Reduction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology (Shenzhen)</span></p><p>本文提出了一种低成本方差降低方法ComMCS，通过对当前和后续步骤的Monte Carlo估计线性组合，优化了大语言模型（LLM）在数学推理任务中的value-based process verifier训练标签的方差，且无需增加额外的LLM推理成本。实验结果显示，ComMCS在MATH-500和GSM8K基准测试上相较于传统回归和分布建模方法取得了2-3个百分点的准确率提升，有效提升了模型的数学推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving%20Value-based%20Process%20Verifier%20via%20Low-Cost%20Variance%20Reduction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-Reverse-Physician-AI-Relationship-Full-process-Clinical-Diagnosis-Driven-by-a-Large-Language-Model"><a href="#99-Reverse-Physician-AI-Relationship-Full-process-Clinical-Diagnosis-Driven-by-a-Large-Language-Model" class="headerlink" title="99. Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Reverse_Physician-AI_Relationship__Full-process_Clinical_Diagnosis_Driven_by_a_Large_Language_Model.pdf">Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of AI Safety, Institute of Computing Technology, CAS</span></p><p>本文提出了DxDirector-7B，一种具备类人“慢思考”深度推理能力的大型语言模型，实现了从含糊主诉出发自动驱动全流程临床诊断，仅在必要时请求医生协助。通过持续医学预训练、全流程诊断指令微调和步级策略偏好优化，使模型能自主决策、分步推理并附权威文献支持。实验结果显示，DxDirector-7B在复杂、罕见和真实临床场景中诊断准确率显著优于参数量大数十倍的现有医疗&#x2F;通用LLM，并极大减轻医生工作量，部分科室可替代60%-75%的专科医生诊断工作，标志着AI主导诊断新时代的到来。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Reverse%20Physician-AI%20Relationship%3A%20Full-process%20Clinical%20Diagnosis%20Driven%20by%20a%20Large%20Language%20Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-FIRESPARQL-A-LLM-based-Framework-for-SPARQL-Query-Generation-over-Scholarly-Knowledge-Graphs"><a href="#100-FIRESPARQL-A-LLM-based-Framework-for-SPARQL-Query-Generation-over-Scholarly-Knowledge-Graphs" class="headerlink" title="100. FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/FIRESPARQL__A_LLM-based_Framework_for_SPARQL_Query_Generation_over_Scholarly_Knowledge_Graphs.pdf">FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Vrije Universiteit Amsterdam</span></p><p>本文提出FIRESPARQL框架，通过对大语言模型（LLM）进行领域特定微调（LoRA）、引入可选的检索增强生成（RAG）模块和轻量级SPARQL纠错层，以提升对学术知识图谱（SKG）中的自然语言问题转SPARQL查询的准确性和结构一致性。实验结果显示，微调后的LLaMA-3-8B-Instruct模型在SciQA基准上取得了最优表现，显著提升了查询准确率和执行效果，RAG模块反而未带来额外提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FIRESPARQL%3A%20A%20LLM-based%20Framework%20for%20SPARQL%20Query%20Generation%20over%20Scholarly%20Knowledge%20Graphs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="101-WE-MATH-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning"><a href="#101-WE-MATH-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning" class="headerlink" title="101. WE-MATH 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/We-Math_2.0__A_Versatile_MathBook_System_for_Incentivizing_Visual_Mathematical_Reasoning.pdf">WE-MATH 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">BUPT</span></p><p>WE-MATH 2.0提出了一个多模态大语言模型系统，融合了结构化数学知识体系、模型中心化数据难度建模以及两阶段强化学习训练，包括冷启动微调和动态难度递增RL优化，显著提升MLLM在视觉数学推理任务中的泛化与稳健性。实验证明，该方法在多个主流数学推理基准上取得了领先性能，尤其在多步推理和知识覆盖广度方面表现优异。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/WE-MATH%202.0%3A%20A%20Versatile%20MathBook%20System%20for%20Incentivizing%20Visual%20Mathematical%20Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-SC2Arena-and-StarEvolve-Benchmark-and-Self-Improvement-Framework-for-LLMs-in-Complex-Decision-Making-Tasks"><a href="#102-SC2Arena-and-StarEvolve-Benchmark-and-Self-Improvement-Framework-for-LLMs-in-Complex-Decision-Making-Tasks" class="headerlink" title="102. SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SC2Arena_and_StarEvolve__Benchmark_and_Self-Improvement_Framework_for_LLMs_in_Complex_Decision-Makin.pdf">SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Automation, Chinese Academy of Sciences</span></p><p>该论文提出SC2Arena，作为评估大语言模型（LLM）在StarCraft II复杂决策任务中的基准，支持全程游戏、多种种族和完整低级动作空间，同时通过文本优化解决空间推理与信息过载问题。配套的StarEvolve框架采用分层规划与执行、迭代自我纠错机制，并结合高质量数据的监督微调，实现LLM智能体的持续自我提升和强对手适应能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SC2Arena%20and%20StarEvolve%3A%20Benchmark%20and%20Self-Improvement%20Framework%20for%20LLMs%20in%20Complex%20Decision-Making%20Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-AnalogSeeker-An-Open-source-Foundation-Language-Model-for-Analog-Circuit-Design"><a href="#103-AnalogSeeker-An-Open-source-Foundation-Language-Model-for-Analog-Circuit-Design" class="headerlink" title="103. AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/AnalogSeeker__An_Open-source_Foundation_Language_Model_for_Analog_Circuit_Design.pdf">AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本论文提出AnalogSeeker，一种面向模拟电路设计的开源基础大语言模型。方法上，作者通过模拟电路领域知识框架系统收集高质量教材并清洗成文本语料，采用多智能体框架将未标注文本蒸馏为细粒度问答（QTSA）对，形成可微调数据集，并设计了邻域自约束监督微调算法（NSC-SFT），在Qwen2.5-32B-Instruct模型基础上进行训练。结论显示，AnalogSeeker在AMSBench-TQA基准上获得85.04%准确率，比原始模型提升15.67个百分点，且在运算放大器设计任务中表现有效，具备领域知识注入和设计辅助潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AnalogSeeker%3A%20An%20Open-source%20Foundation%20Language%20Model%20for%20Analog%20Circuit%20Design.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-Layer-Wise-Perturbations-via-Sparse-Autoencoders-for-Adversarial-Text-Generation"><a href="#104-Layer-Wise-Perturbations-via-Sparse-Autoencoders-for-Adversarial-Text-Generation" class="headerlink" title="104. Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Layer-Wise_Perturbations_via_Sparse_Autoencoders_for_Adversarial_Text_Generation.pdf">Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">hydrox.ai</span></p><p>该论文提出Sparse Feature Perturbation Framework (SFPF)，利用稀疏自编码器（SAE）对大型语言模型的隐藏层激活进行重构和特征聚类，识别并扰动与攻击相关的关键特征，从而生成可绕过安全机制的对抗性文本。实验表明，SFPF生成的对抗性文本能显著提升攻击成功率并维持文本质量，但方法在不同模型结构和层次上的泛化性尚待验证。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Layer-Wise%20Perturbations%20via%20Sparse%20Autoencoders%20for%20Adversarial%20Text%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-LeanRAG-Knowledge-Graph-Based-Generation-with-Semantic-Aggregation-and-Hierarchical-Retrieval"><a href="#105-LeanRAG-Knowledge-Graph-Based-Generation-with-Semantic-Aggregation-and-Hierarchical-Retrieval" class="headerlink" title="105. LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/LeanRAG__Knowledge-Graph-Based_Generation_with_Semantic_Aggregation_and_Hierarchical_Retrieval.pdf">LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Artificial Intelligence Laboratory</span></p><p>LeanRAG提出了一种结合知识图谱语义聚合与层次化检索的检索增强生成（RAG）新框架。方法上，LeanRAG首先通过高斯混合聚类和大模型生成，构建多层次、具备显式实体间关系的可导航语义网络，再利用自底向上的LCA结构化检索，有效定位相关细粒度实体并聚合至高层次语义，提升上下文相关性并大幅减少信息冗余。结论显示，LeanRAG在四个QA基准数据集上表现优于现有方法，检索冗余降低46%，且显式语义关系与原始文本并用对生成高质量答案至关重要。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/LeanRAG%3A%20Knowledge-Graph-Based%20Generation%20with%20Semantic%20Aggregation%20and%20Hierarchical%20Retrieval.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-Contrast-Sensitivity-Function-of-Multimodal-Vision-Language-Models"><a href="#106-Contrast-Sensitivity-Function-of-Multimodal-Vision-Language-Models" class="headerlink" title="106. Contrast Sensitivity Function of Multimodal Vision-Language Models"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Contrast_Sensitivity_Function_of_Multimodal_Vision-Language_Models.pdf">Contrast Sensitivity Function of Multimodal Vision-Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universidad de Valencia</span></p><p>本文提出了一种心理物理学启发的行为评估方法，通过直接向多模态视觉-语言模型（如LLaVA-1.5-7B、Blip2-7B等）提问来测量其对不同空间频率和对比度图像的敏感性，拟合模型的对比敏感函数（CSF），并与人类视觉CSF进行对比。结果显示，部分模型在CSF形状或绝对值上与人类相近，但均无法同时复现人类CSF的形状与稳定性，且对提示词变化敏感，提示内部一致性和鲁棒性有待提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Contrast%20Sensitivity%20Function%20of%20Multimodal%20Vision-Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="107-What-to-Ask-Next-Probing-the-Imaginative-Reasoning-of-LLMs-with-TurtleSoup-Puzzles"><a href="#107-What-to-Ask-Next-Probing-the-Imaginative-Reasoning-of-LLMs-with-TurtleSoup-Puzzles" class="headerlink" title="107. What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/What_to_Ask_Next__Probing_the_Imaginative_Reasoning_of_LLMs_with_TurtleSoup_Puzzles.pdf">What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huazhong University of Science and Technology</span></p><p>本文提出了TurtleSoup-Bench，一个大规模、双语、交互式的想象性推理基准，结合TurtleSoup谜题、Mosaic-Agent智能体和多维自动评测协议，以系统评估大模型在信息稀缺环境下的假设构建、测试与修正能力。实验表明，当前主流大语言模型在动态推理与创造性假设生成方面仍与人类有明显差距，验证了该评测框架对模型推理过程诊断的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/What%20to%20Ask%20Next%3F%20Probing%20the%20Imaginative%20Reasoning%20of%20LLMs%20with%20TurtleSoup%20Puzzles.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-ReviewRL-Towards-Automated-Scientific-Review-with-RL"><a href="#108-ReviewRL-Towards-Automated-Scientific-Review-with-RL" class="headerlink" title="108. ReviewRL: Towards Automated Scientific Review with RL"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ReviewRL__Towards_Automated_Scientific_Review_with_RL.pdf">ReviewRL: Towards Automated Scientific Review with RL</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本论文提出ReviewRL框架，通过ArXiv-MCP检索增强型上下文生成、监督微调和强化学习训练自动生成高质量、事实准确的学术论文评审。实验结果表明，ReviewRL在ICLR 2025论文评测中显著优于现有方法，有效提升了评审的质量和评分一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ReviewRL%3A%20Towards%20Automated%20Scientific%20Review%20with%20RL.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-Yet-another-algorithmic-bias-A-Discursive-Analysis-of-Large-Language-Models-Reinforcing-Dominant-Discourses-on-Gender-and-Race"><a href="#109-Yet-another-algorithmic-bias-A-Discursive-Analysis-of-Large-Language-Models-Reinforcing-Dominant-Discourses-on-Gender-and-Race" class="headerlink" title="109. Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Yet_another_algorithmic_bias__A_Discursive_Analysis_of_Large_Language_Models_Reinforcing_Dominant_Di.pdf">Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Instituto de Estudos da Linguagem (IEL), Universidade Estadual de Campinas (UNICAMP)</span></p><p>本论文提出以批判性话语分析为核心的定性分析框架，对主流LLMs（包括GPT-4、LLaMa、Sabiá等）生成的关于不同性别与种族女性的短篇故事进行深入剖析。研究发现，这些LLMs在文本输出中系统性地强化了西方主流关于黑人女性的抗争、祖先和群体责任等刻板印象，而对白人女性则更多强调自我发现和个体成长。即使在被提示修正偏见时，模型的修正也流于表面，难以消除深层的结构性歧视。这反映出LLMs在语义与社会层面的理解与创新能力有限，现有模型难以自发突破训练语料中固有的意识形态框架。论文强调，话语层面的定性研究对于发现和干预大模型输出中的隐性偏见至关重要，呼吁AI开发需引入跨学科和多元伦理视角。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Yet%20another%20algorithmic%20bias%3A%20A%20Discursive%20Analysis%20of%20Large%20Language%20Models%20Reinforcing%20Dominant%20Discourses%20on%20Gender%20and%20Race.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="110-Promoting-Efficient-Reasoning-with-Verifiable-Stepwise-Reward"><a href="#110-Promoting-Efficient-Reasoning-with-Verifiable-Stepwise-Reward" class="headerlink" title="110. Promoting Efficient Reasoning with Verifiable Stepwise Reward"></a>110. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Promoting_Efficient_Reasoning_with_Verifiable_Stepwise_Reward.pdf">Promoting Efficient Reasoning with Verifiable Stepwise Reward</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Meituan</span></p><p>该论文提出了一种可验证的逐步奖励机制（VSRM），通过规则分割和奖励传播，对大语言推理模型的中间推理步骤进行奖励或惩罚，鼓励有效步骤抑制无效冗余，显著减少输出长度。实验结果表明，VSRM在多个数学推理基准上有效缓解模型过度推理问题，同时保持甚至略微提升原有推理能力，实现高效准确的推理。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Promoting%20Efficient%20Reasoning%20with%20Verifiable%20Stepwise%20Reward.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="111-Why-Cannot-Large-Language-Models-Ever-Make-True-Correct-Reasoning"><a href="#111-Why-Cannot-Large-Language-Models-Ever-Make-True-Correct-Reasoning" class="headerlink" title="111. Why Cannot Large Language Models Ever Make True Correct Reasoning?"></a>111. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Why_Cannot_Large_Language_Models_Ever_Make_True_Correct_Reasoning_.pdf">Why Cannot Large Language Models Ever Make True Correct Reasoning?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Saitama University</span></p><p>本文从逻辑学和推理正确性的角度，系统分析了大语言模型（LLM）所谓“推理能力”的本质，指出LLM仅能模拟推理形式但无法实现真正的正确推理。作者通过定义真正正确推理的三大逻辑基础，阐述了LLM由于其基于概率与统计的生成原理，无法内嵌逻辑有效性评价标准和动态评估机制，因此永远无法获得100%正确的推理能力。结论认为，LLM只能给出表面上似乎合理的推理结果，追求其真正推理能力而忽视正确性标准是错误且无望的方向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Why%20Cannot%20Large%20Language%20Models%20Ever%20Make%20True%20Correct%20Reasoning%3F.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="112-Flexible-Personalized-Split-Federated-Learning-for-On-Device-Fine-Tuning-of-Foundation-Models"><a href="#112-Flexible-Personalized-Split-Federated-Learning-for-On-Device-Fine-Tuning-of-Foundation-Models" class="headerlink" title="112. Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models"></a>112. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Flexible_Personalized_Split_Federated_Learning_for_On-Device_Fine-Tuning_of_Foundation_Models.pdf">Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Duke Kunshan University</span></p><p>该论文提出FlexP-SFL框架，结合Split Federated Learning和个性化机制，实现基础模型在资源受限设备上的个性化高效微调。方法允许每个客户端按自身算力灵活分配本地训练层，服务器处理共享部分，并通过KL对齐策略平衡个性化与泛化。实验表明，该方法在多种语言模型上显著提升个性化准确率，减少通信与训练时间，支持异构设备高效协作。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Flexible%20Personalized%20Split%20Federated%20Learning%20for%20On-Device%20Fine-Tuning%20of%20Foundation%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="113-MRFD-Multi-Region-Fusion-Decoding-with-Self-Consistency-for-Mitigating-Hallucinations-in-LVLMs"><a href="#113-MRFD-Multi-Region-Fusion-Decoding-with-Self-Consistency-for-Mitigating-Hallucinations-in-LVLMs" class="headerlink" title="113. MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs"></a>113. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MRFD__Multi-Region_Fusion_Decoding_with_Self-Consistency_for_Mitigating_Hallucinations_in_LVLMs.pdf">MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Merced</span></p><p>该论文提出了一种训练无关的解码方法MRFD，通过交叉注意力机制选取图像多个显著区域，分别生成区域响应，并利用Jensen-Shannon Divergence评估区域间自洽性，获得可靠性权重后融合各区域预测。最终采用链式思维启发的区域提示，提升LVLMs对图像事实的捕捉与响应的真实度。实验表明MRFD能在多个LVLM和公开基准上显著降低幻觉现象，提升响应的事实性，无需模型参数更新。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MRFD%3A%20Multi-Region%20Fusion%20Decoding%20with%20Self-Consistency%20for%20Mitigating%20Hallucinations%20in%20LVLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="114-Generalizable-Federated-Learning-using-Client-Adaptive-Focal-Modulation"><a href="#114-Generalizable-Federated-Learning-using-Client-Adaptive-Focal-Modulation" class="headerlink" title="114. Generalizable Federated Learning using Client Adaptive Focal Modulation"></a>114. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Generalizable_Federated_Learning_using_Client_Adaptive_Focal_Modulation.pdf">Generalizable Federated Learning using Client Adaptive Focal Modulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MBZUAI</span></p><p>该论文提出了AdaptFED，一种在联邦学习（FL）场景下利用中心化可学习生成器和任务感知客户端嵌入，实现焦点调制层个性化的Transformer框架。方法通过低秩超网络条件化和高效的参数聚合，提升了模型在多模态、非独立同分布（Non-IID）及源数据不可用（Source-Free Domain Adaptation）环境下的泛化能力。实验证明，AdaptFED在视觉、时间序列和多语言任务上均优于当前主流方法，理论上也给出了更紧的个性化泛化界。结论：AdaptFED提升了个性化联邦学习的效果与扩展性，适用于多领域和资源受限场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Generalizable%20Federated%20Learning%20using%20Client%20Adaptive%20Focal%20Modulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="115-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms"><a href="#115-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms" class="headerlink" title="115. From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms"></a>115. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/From_Black_Box_to_Transparency__Enhancing_Automated_Interpreting_Assessment_with_Explainable_AI_in_C.pdf">From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本论文提出了一种结合特征工程、变分自编码器（VAE）数据增强和可解释机器学习（如SHAP分析）的多维自动口译质量评价框架。方法在英中口译数据集上验证，重点提升对语言使用、流畅度和信息完整性的可解释预测能力，结果显示VAE增强显著提升模型表现，且各维度的关键特征均被精确识别，为教学反馈和自我调节学习提供了透明、可扩展的自动评分工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From%20Black%20Box%20to%20Transparency%3A%20Enhancing%20Automated%20Interpreting%20Assessment%20with%20Explainable%20AI%20in%20College%20Classrooms.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="116-Who-Benefits-from-AI-Explanations-Towards-Accessible-and-Interpretable-Systems"><a href="#116-Who-Benefits-from-AI-Explanations-Towards-Accessible-and-Interpretable-Systems" class="headerlink" title="116. Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems"></a>116. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Who_Benefits_from_AI_Explanations__Towards_Accessible_and_Interpretable_Systems.pdf">Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ontario Tech University</span></p><p>本文采用文献综述和四步法案例研究，系统评估现有可解释人工智能（XAI）技术在无障碍领域的缺口，尤其关注视觉障碍用户。研究设计并测试了面向盲人交通管理者的XAI原型，采用LIME和SHAP解释方法，结果发现简化、多模态解释更易于非视觉用户理解，强调需为不同能力用户设计多样化解释方式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Who%20Benefits%20from%20AI%20Explanations%3F%20Towards%20Accessible%20and%20Interpretable%20Systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="117-Insights-from-the-Algonauts-2025-Winners"><a href="#117-Insights-from-the-Algonauts-2025-Winners" class="headerlink" title="117. Insights from the Algonauts 2025 Winners"></a>117. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Insights_from_the_Algonauts_2025_Winners.pdf">Insights from the Algonauts 2025 Winners</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Medical AI Research Center (MedARC)</span></p><p>本论文总结了Algonauts 2025竞赛中的顶尖团队方法，主要采用预训练多模态特征提取器（视觉、音频和文本），结合transformer、RNN或线性卷积架构，通过模型集成预测自然电影刺激下的人类脑区fMRI反应。结果显示，架构复杂度影响有限，模型集成与多模态特征融合是提升脑编码准确性的关键，未来突破需跳出现有范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Insights%20from%20the%20Algonauts%202025%20Winners.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="118-APFL-Analytic-Personalized-Federated-Learning-via-Dual-Stream-Least-Squares"><a href="#118-APFL-Analytic-Personalized-Federated-Learning-via-Dual-Stream-Least-Squares" class="headerlink" title="118. APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares"></a>118. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/APFL__Analytic_Personalized_Federated_Learning_via_Dual-Stream_Least_Squares.pdf">APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CSU, China</span></p><p>该论文提出了APFL方法，利用冻结的基础模型（如ViT）进行特征提取，并通过双流解析模型（主流用于全局泛化，细化流用于本地个性化）实现个性化联邦学习，采用解析最小二乘法避免梯度更新对非IID数据分布的敏感性。实验结果显示APFL在多个数据集和不同非IID程度下均优于现有方法，理论分析证明其具备异质性不变性、隐私保护及高效率，结论为APFL有效提升个性化模型性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/APFL%3A%20Analytic%20Personalized%20Federated%20Learning%20via%20Dual-Stream%20Least%20Squares.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="119-FreeGAD-A-Training-Free-yet-Effective-Approach-for-Graph-Anomaly-Detection"><a href="#119-FreeGAD-A-Training-Free-yet-Effective-Approach-for-Graph-Anomaly-Detection" class="headerlink" title="119. FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection"></a>119. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/FreeGAD__A_Training-Free_yet_Effective_Approach_for_Graph_Anomaly_Detection.pdf">FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Guangxi University</span></p><p>本文提出FreeGAD，一种无需训练的图异常检测方法，采用亲和力门控残差编码器生成异常感知表示，并通过锚点节点筛选和引导统计偏差计算异常分数。实验结果表明，FreeGAD在多个真实和合成数据集上实现了优越的检测性能、极高效率和良好扩展性，显著优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FreeGAD%3A%20A%20Training-Free%20yet%20Effective%20Approach%20for%20Graph%20Anomaly%20Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="120-Deep-Learning-in-Classical-and-Quantum-Physics"><a href="#120-Deep-Learning-in-Classical-and-Quantum-Physics" class="headerlink" title="120. Deep Learning in Classical and Quantum Physics"></a>120. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Deep_Learning_in_Classical_and_Quantum_Physics.pdf">Deep Learning in Classical and Quantum Physics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ICFO – Institut de Ciències Fotòniques</span></p><p>本论文系统梳理了深度学习（Deep Learning）和机器学习（Machine Learning）的基础理论、关键算法（如神经网络、自动微分、损失函数设计等），并重点介绍了其在经典物理和量子物理中的应用，包括变分量子本征求解器（VQE）、神经网络量子态（NQS）、量子态层析等。作者以物理视角阐释了深度学习在高维物理系统中的建模优势及其局限，指出深度学习可辅助物理研究发现模式，但尚不能替代理解物理机制。结论认为深度学习为物理学尤其是量子科学提供了强大工具，但其科学解释性与知识获取仍需进一步突破。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Deep%20Learning%20in%20Classical%20and%20Quantum%20Physics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="121-Self-Supervised-Temporal-Super-Resolution-of-Energy-Data-using-Generative-Adversarial-Transformers"><a href="#121-Self-Supervised-Temporal-Super-Resolution-of-Energy-Data-using-Generative-Adversarial-Transformers" class="headerlink" title="121. Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformers"></a>121. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Self-Supervised_Temporal_Super-Resolution_of_Energy_Data_using_Generative_Adversarial_Transformer.pdf">Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Karlsruhe Institute of Technology</span></p><p>该论文提出了一种无需高分辨率数据的自监督时间序列超分辨率方法，结合了Transformer的序列建模能力和GAN的对抗训练框架，并通过特征空间损失实现低分辨率到高分辨率数据的一致映射。实验结果表明，该方法在多种能源数据集上超越了传统插值与现有深度学习模型，RMSE降低9%、在MPC应用场景精度提升13%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Self-Supervised%20Temporal%20Super-Resolution%20of%20Energy%20Data%20using%20Generative%20Adversarial%20Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="122-FAKE-SPEECH-WILD-DETECTING-DEEPFAKE-SPEECH-ON-SOCIAL-MEDIA-PLATFORM"><a href="#122-FAKE-SPEECH-WILD-DETECTING-DEEPFAKE-SPEECH-ON-SOCIAL-MEDIA-PLATFORM" class="headerlink" title="122. FAKE SPEECH WILD: DETECTING DEEPFAKE SPEECH ON SOCIAL MEDIA PLATFORM"></a>122. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Fake_Speech_Wild__Detecting_Deepfake_Speech_on_Social_Media_Platform.pdf">FAKE SPEECH WILD: DETECTING DEEPFAKE SPEECH ON SOCIAL MEDIA PLATFORM</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Communication University of China</span></p><p>本文提出了Fake Speech Wild (FSW) 数据集，收集了来自四个社交媒体平台、共254小时的真实与深度伪造语音数据。采用自监督学习技术和多种数据增强方法（如MUSAN &amp; RIR噪声、Rawboost），联合训练多源数据集后，显著提升了深度伪造语音检测模型在跨领域场景下的鲁棒性，最终在全部测试集上达到3.54%的平均等错误率（EER）。结论：联合多域数据和数据增强显著提升了检测性能，FSW数据集为真实环境下伪造语音检测提供了重要基准。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FAKE%20SPEECH%20WILD%3A%20DETECTING%20DEEPFAKE%20SPEECH%20ON%20SOCIAL%20MEDIA%20PLATFORM.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="123-Contrastive-ECOC-Learning-Output-Codes-for-Adversarial-Defense"><a href="#123-Contrastive-ECOC-Learning-Output-Codes-for-Adversarial-Defense" class="headerlink" title="123. Contrastive ECOC: Learning Output Codes for Adversarial Defense"></a>123. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Contrastive_ECOC__Learning_Output_Codes_for_Adversarial_Defense.pdf">Contrastive ECOC: Learning Output Codes for Adversarial Defense</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Central University</span></p><p>本文提出了三种基于对比学习的自动化编码本学习（ACL）方法，用于多分类任务中的自动生成Error Correcting Output Codes（ECOC）编码本，有效提升神经网络对抗攻击（如FGSM和PGD）的鲁棒性。实验表明，所提模型能够自动生成数据集特定的编码本，在多个开放数据集上对抗攻击下的表现优于传统手工或随机编码本。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Contrastive%20ECOC%3A%20Learning%20Output%20Codes%20for%20Adversarial%20Defense.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="124-GraphFedMIG-Tackling-Class-Imbalance-in-Federated-Graph-Learning-via-Mutual-Information-Guided-Generation"><a href="#124-GraphFedMIG-Tackling-Class-Imbalance-in-Federated-Graph-Learning-via-Mutual-Information-Guided-Generation" class="headerlink" title="124. GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation"></a>124. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GraphFedMIG__Tackling_Class_Imbalance_in_Federated_Graph_Learning_via_Mutual_Information-Guided_Gene.pdf">GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Chongqing University</span></p><p>该论文提出GraphFedMIG框架，通过将联邦图学习中的类别不平衡问题转化为生成式数据增强任务，采用分层生成对抗网络（GAN）和互信息引导的模型聚合机制。实验表明，GraphFedMIG在四个真实数据集上显著提升了少数类的识别能力并整体优于当前主流方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GraphFedMIG%3A%20Tackling%20Class%20Imbalance%20in%20Federated%20Graph%20Learning%20via%20Mutual%20Information-Guided%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="125-HiRef-Leveraging-Hierarchical-Ontology-and-Network-Refinement-for-Robust-Medication-Recommendation"><a href="#125-HiRef-Leveraging-Hierarchical-Ontology-and-Network-Refinement-for-Robust-Medication-Recommendation" class="headerlink" title="125. HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation"></a>125. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/HiRef__Leveraging_Hierarchical_Ontology_and_Network_Refinement_for_Robust_Medication_Recommendation.pdf">HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea University</span></p><p>该论文提出HiRef框架，将医疗本体的层次语义嵌入（使用超曲空间）与电子病历（EHR）中稀疏优化的共现图结构相结合，通过自适应融合机制提升药物推荐模型对罕见和未见医疗编码的泛化能力。实验表明，HiRef在MIMIC-III&#x2F;IV数据集下表现优异，尤其在未见编码场景下准确率高，且具备更好的鲁棒性和可解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/HiRef%3A%20Leveraging%20Hierarchical%20Ontology%20and%20Network%20Refinement%20for%20Robust%20Medication%20Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="126-Mitigating-Exponential-Mixed-Frequency-Growth-through-Frequency-Selection-and-Dimensional-Separation-in-Quantum-Machine-Learning"><a href="#126-Mitigating-Exponential-Mixed-Frequency-Growth-through-Frequency-Selection-and-Dimensional-Separation-in-Quantum-Machine-Learning" class="headerlink" title="126. Mitigating Exponential Mixed Frequency Growth through Frequency Selection and Dimensional Separation in Quantum Machine Learning"></a>126. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Mitigating_Exponential_Mixed_Frequency_Growth_through_Frequency_Selection_and_Dimensional_Separation.pdf">Mitigating Exponential Mixed Frequency Growth through Frequency Selection and Dimensional Separation in Quantum Machine Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute for Informatics, LMU Munich</span></p><p>本文针对量子机器学习中多维傅里叶频谱导致参数双指数级增长、难以在当前硬件上拟合实际问题的瓶颈，提出了频率选择与维度分离两项关键技术，通过仅保留必要频率和混合频率组，极大减少所需模型参数。实验表明，这两种方法能在噪声模拟器及真实量子硬件上实现高精度回归拟合，突破了默认模型频谱密集导致的拟合失败，理论与实践上均有效缓解了参数爆炸问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Mitigating%20Exponential%20Mixed%20Frequency%20Growth%20through%20Frequency%20Selection%20and%20Dimensional%20Separation%20in%20Quantum%20Machine%20Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="127-RealAC-A-Domain-Agnostic-Framework-for-Realistic-and-Actionable-Counterfactual-Explanations"><a href="#127-RealAC-A-Domain-Agnostic-Framework-for-Realistic-and-Actionable-Counterfactual-Explanations" class="headerlink" title="127. RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations"></a>127. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/RealAC__A_Domain-Agnostic_Framework_for_Realistic_and_Actionable_Counterfactual_Explanations.pdf">RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Arizona State University</span></p><p>本文提出了RealAC，一个面向领域无关、能够生成现实且可操作反事实解释的框架。RealAC通过最小化反事实与原始样本之间所有特征对的互信息差异，在无需领域知识的情况下自动保持复杂的特征依赖关系，并结合二进制掩码机制支持用户指定不可变特征，实现个性化可行性约束。实验表明，RealAC在因果边缘分数、依赖保持分数和IM1等指标上优于现有主流方法及大语言模型生成的反事实，兼顾了反事实的现实性与可操作性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/RealAC%3A%20A%20Domain-Agnostic%20Framework%20for%20Realistic%20and%20Actionable%20Counterfactual%20Explanations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="128-Semantic-Communication-with-Distribution-Learning-through-Sequential-Observations"><a href="#128-Semantic-Communication-with-Distribution-Learning-through-Sequential-Observations" class="headerlink" title="128. Semantic Communication with Distribution Learning through Sequential Observations"></a>128. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Semantic_Communication_with_Distribution_Learning_through_Sequential_Observations.pdf">Semantic Communication with Distribution Learning through Sequential Observations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Dalhousie University</span></p><p>本文提出了语义通信中的分布学习理论框架，研究在先验分布未知且编码方案与信道固定时接收端如何通过观察序列学习语义分布。方法上，作者提出以有效传输矩阵的满秩性为可学习性条件，推导出估计误差和性能损失的收敛界，并理论证明估计误差与样本数成O(1&#x2F;√T)衰减；结论表明，良好条件数的系统能实现高效学习和适应，压缩优化型编码则显著降低长期学习能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Semantic%20Communication%20with%20Distribution%20Learning%20through%20Sequential%20Observations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="129-eMamba-Efficient-Acceleration-Framework-for-Mamba-Models-in-Edge-Computing"><a href="#129-eMamba-Efficient-Acceleration-Framework-for-Mamba-Models-in-Edge-Computing" class="headerlink" title="129. eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing"></a>129. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/eMamba__Efficient_Acceleration_Framework_for_Mamba_Models_in_Edge_Computing.pdf">eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Ulsan</span></p><p>本文提出了eMamba，一种面向边缘计算的Mamba模型端到端硬件加速框架。方法包括用硬件友好的归一化、分段线性近似及量化技术替代原有复杂操作，并结合近似感知的神经架构搜索优化可训练参数。实验表明，在Fashion-MNIST、CIFAR-10和MARS等视觉任务中，eMamba在保持精度的同时大幅减少参数量，并在FPGA和ASIC实现中获得更低延迟、更高吞吐率和显著的能耗降低，可广泛适用于边缘AI应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/eMamba%3A%20Efficient%20Acceleration%20Framework%20for%20Mamba%20Models%20in%20Edge%20Computing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="130-Layer-Wise-Analysis-of-Self-Supervised-Representations-for-Age-and-Gender-Classification-in-Children’s-Speech"><a href="#130-Layer-Wise-Analysis-of-Self-Supervised-Representations-for-Age-and-Gender-Classification-in-Children’s-Speech" class="headerlink" title="130. Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Children’s Speech"></a>130. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Layer-Wise_Analysis_of_Self-Supervised_Representations_for_Age_and_Gender_Classification_in_Children.pdf">Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Children’s Speech</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">NIT Sikkim</span></p><p>该论文提出了一种针对儿童语音年龄和性别分类的层级特征分析方法，利用四种Wav2Vec2自监督学习模型提取层级特征，并通过CNN分类器和主成分分析（PCA）进行降维和性能优化。实验结果显示，模型早期层能更有效地捕捉说话人特征，降维后不仅提升了分类准确率也提高了计算效率，揭示了特征在模型深度中的分布规律。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Layer-Wise%20Analysis%20of%20Self-Supervised%20Representations%20for%20Age%20and%20Gender%20Classification%20in%20Children%E2%80%99s%20Speech.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="131-A-Vision-Language-Pre-training-Model-Guided-Approach-for-Mitigating-Backdoor-Attacks-in-Federated-Learning"><a href="#131-A-Vision-Language-Pre-training-Model-Guided-Approach-for-Mitigating-Backdoor-Attacks-in-Federated-Learning" class="headerlink" title="131. A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning"></a>131. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Vision-Language_Pre-training_Model-Guided_Approach_for_Mitigating_Backdoor_Attacks_in_Federated_Le.pdf">A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>该论文提出CLIP-Fed框架，利用视觉-语言预训练模型（如CLIP）的跨模态零样本学习能力，实现联邦学习中的前聚合恶意模型过滤和后聚合后门净化。通过多模态大模型和频域分析构建服务器数据集，并结合原型对比损失和知识蒸馏，纠正特征偏移、削弱后门关联。实验表明，CLIP-Fed在多种数据分布和攻击类型下显著降低ASR并提升主任务准确率，优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Vision-Language%20Pre-training%20Model-Guided%20Approach%20for%20Mitigating%20Backdoor%20Attacks%20in%20Federated%20Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="132-Pruning-and-Malicious-Injection-A-Retraining-Free-Backdoor-Attack-on-Transformer-Models"><a href="#132-Pruning-and-Malicious-Injection-A-Retraining-Free-Backdoor-Attack-on-Transformer-Models" class="headerlink" title="132. Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models"></a>132. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Pruning_and_Malicious_Injection__A_Retraining-Free_Backdoor_Attack_on_Transformer_Models.pdf">Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Louisiana State University</span></p><p>本文提出了HPMI（Head-wise Pruning and Malicious Injection），一种无需额外重训练且不改变模型架构的变换器后门攻击方法。通过剪除多头注意力模块中最不重要的头，再注入预训练的恶意头，实现后门植入，理论与实验均表明在保持干净数据准确率的同时，能以超99.55%的成功率绕过四种主流防御机制。结论显示HPMI对现有防御手段有很强的隐蔽性和鲁棒性，且对模型性能影响极小。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Pruning%20and%20Malicious%20Injection%3A%20A%20Retraining-Free%20Backdoor%20Attack%20on%20Transformer%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>今日论文报纸</title>
    <link href="/2025/08/2025-08-15_article/"/>
    <url>/2025/08/2025-08-15_article/</url>
    
    <content type="html"><![CDATA[<center><h1>今日论文报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-15<br>📄 <strong>发现论文数量</strong>：109  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Human-Aligned-Procedural-Level-Generation-Reinforcement-Learning-via-Text-Level-Sketch-Shared-Representation"><a href="#1-Human-Aligned-Procedural-Level-Generation-Reinforcement-Learning-via-Text-Level-Sketch-Shared-Representation" class="headerlink" title="1. Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Human-Aligned_Procedural_Level_Generation_Reinforcement_Learning_via_Text-Level-Sketch_Shared_Repres.pdf">Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Gwangju Institute of Science and Technology</span></p><p>该论文提出VIPCGRL框架，通过四模态对比学习（文本、关卡、草图与人&#x2F;AI风格）建立共享表征空间，并在多模态条件下利用辅助奖励实现与人类风格对齐的深度强化学习关卡生成。实验显示VIPCGRL在生成2D游戏关卡时，在可控性和拟人性上均优于现有方法，并能灵活支持多模态输入，用户研究进一步验证了其人类风格和文本&#x2F;草图指令对齐能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Human-Aligned%20Procedural%20Level%20Generation%20Reinforcement%20Learning%20via%20Text-Level-Sketch%20Shared%20Representation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Generative-Modeling-with-Multi-Instance-Reward-Learning-for-E-commerce-Creative-Optimization"><a href="#2-Generative-Modeling-with-Multi-Instance-Reward-Learning-for-E-commerce-Creative-Optimization" class="headerlink" title="2. Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Generative_Modeling_with_Multi-Instance_Reward_Learning_for_E-commerce_Creative_Optimization.pdf">Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">JD.com</span></p><p>该论文提出了GenCO框架，将生成式建模与多实例奖励学习结合应用于电商创意优化。方法包括第一阶段采用非自回归、上下文感知的生成模型高效生成多样化的创意组合，并通过强化学习优化生成策略，第二阶段采用多实例学习模型将组合层级的用户反馈（如点击）归因到各个创意元素，实现更精准的反馈信号与生成优化。论文在真实电商平台上线验证，显著提升CTR和广告收入，显示出强大的实际商业价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Generative%20Modeling%20with%20Multi-Instance%20Reward%20Learning%20for%20E-commerce%20Creative%20Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Goal-Discovery-with-Causal-Capacity-for-Efficient-Reinforcement-Learning"><a href="#3-Goal-Discovery-with-Causal-Capacity-for-Efficient-Reinforcement-Learning" class="headerlink" title="3. Goal Discovery with Causal Capacity for Efficient Reinforcement Learning"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Goal_Discovery_with_Causal_Capacity_for_Efficient_Reinforcement_Learning.pdf">Goal Discovery with Causal Capacity for Efficient Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出了一种结合因果推断的目标发现方法（GDCC），通过引入“因果容量”来度量状态下动作对未来轨迹的最大影响力，并用蒙特卡洛和聚类方法在高维连续空间中高效估算关键决策点（子目标），以指导强化学习智能体的有效探索。实验证明，该方法在MuJoCo和Habitat等多目标稀疏奖励任务中，能自动发现物理意义明确的子目标，有效提升探索效率和任务成功率，优于多种基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Goal%20Discovery%20with%20Causal%20Capacity%20for%20Efficient%20Reinforcement%20Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-Edge-General-Intelligence-Through-World-Models-and-Agentic-AI-Fundamentals-Solutions-and-Challenges"><a href="#4-Edge-General-Intelligence-Through-World-Models-and-Agentic-AI-Fundamentals-Solutions-and-Challenges" class="headerlink" title="4. Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Edge_General_Intelligence_Through_World_Models_and_Agentic_AI__Fundamentals,_Solutions,_and_Challeng.pdf">Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>该论文系统梳理了世界模型（world models）在边缘通用智能（Edge General Intelligence, EGI）中的作用，提出将世界模型作为认知内核，通过内部环境模拟与想象，实现边缘分布式智能体的感知、推理和自主决策。论文分析了世界模型的关键架构（如编码器、动态建模、解码器）、主要技术路线（变分自编码器、RNN、Transformer等）、典型应用场景（车联网、无人机网络、物联网、虚拟化网络功能等），并对与基础模型和数字孪生的协同进行了探讨。结论指出世界模型显著提升了边缘智能系统的前瞻性、效率与安全性，但在算力、实时性与泛化能力等实际部署上仍需进一步研究。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Edge%20General%20Intelligence%20Through%20World%20Models%20and%20Agentic%20AI%3A%20Fundamentals%2C%20Solutions%2C%20and%20Challenges.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-Emergence-of-Hierarchies-in-Multi-Agent-Self-Organizing-Systems-Pursuing-a-Joint-Objective"><a href="#5-Emergence-of-Hierarchies-in-Multi-Agent-Self-Organizing-Systems-Pursuing-a-Joint-Objective" class="headerlink" title="5. Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Emergence_of_Hierarchies_in_Multi-Agent_Self-Organizing_Systems_Pursuing_a_Joint_Objective.pdf">Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>该论文通过多智能体强化学习（MARL）方法，基于MADDPG算法，研究了多智能体自组织系统（MASOS）在协同完成箱子推动任务过程中依赖层级结构的自发涌现及其动态变化。方法核心在于利用动作对其他智能体状态的梯度衡量依赖性，量化并分析层级结构的形成与演化。结论表明：MASOS在追求共同目标时会自发形成动态演化的层级结构，层级变化受环境与网络初始化影响，且“天赋”与“努力”在环境中的动态作用促进了层级的调整与优化。这为MASOS的自组织行为理解和设计提供了理论指导。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Emergence%20of%20Hierarchies%20in%20Multi-Agent%20Self-Organizing%20Systems%20Pursuing%20a%20Joint%20Objective.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="6-Masquerade-Learning-from-In-the-wild-Human-Videos-using-Data-Editing"><a href="#6-Masquerade-Learning-from-In-the-wild-Human-Videos-using-Data-Editing" class="headerlink" title="6. Masquerade: Learning from In-the-wild Human Videos using Data-Editing"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Masquerade__Learning_from_In-the-wild_Human_Videos_using_Data-Editing.pdf">Masquerade: Learning from In-the-wild Human Videos using Data-Editing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University</span></p><p>Masquerade提出了一种通过数据编辑将大规模野外人类视频转换为“机器人化”演示的方法，包括3D手部姿态估计、手臂修复和机器人覆盖，从而弥合人类和机器人的视觉表现差距。该方法先用编辑后的人类视频预训练视觉编码器，再与少量真实机器人演示进行联合训练，实现了在三类厨房任务中对未见环境的零样本迁移，成功率显著高于现有方法，且扩展视频规模可持续提升性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Masquerade%3A%20Learning%20from%20In-the-wild%20Human%20Videos%20using%20Data-Editing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Vision-driven-River-Following-of-UAV-via-Safe-Reinforcement-Learning-using-Semantic-Dynamics-Model"><a href="#7-Vision-driven-River-Following-of-UAV-via-Safe-Reinforcement-Learning-using-Semantic-Dynamics-Model" class="headerlink" title="7. Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Vision-driven_River_Following_of_UAV_via_Safe_Reinforcement_Learning_using_Semantic_Dynamics_Model.pdf">Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Purdue University</span></p><p>本论文提出了一种基于视觉的无人机河流跟随安全强化学习框架。方法方面，作者引入了边际增益优势估计（MGAE）处理非马尔可夫亚模奖励，提出了基于同伦变换的语义动力学模型（SDM）实现高效视觉预测，并整合为受约束的行动者动力学估计器（CADE）以提升安全强化学习在部分可观测环境中的表现。实验结果表明MGAE收敛更快且优于传统方法，SDM在短期状态预测更准确，CADE能有效平衡任务奖励和安全，拉格朗日方法训练下表现最佳，推理时安全层进一步提升效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Vision-driven%20River%20Following%20of%20UAV%20via%20Safe%20Reinforcement%20Learning%20using%20Semantic%20Dynamics%20Model.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-PPL-Point-Cloud-Supervised-Proprioceptive-Locomotion-Reinforcement-Learning-for-Legged-Robots-in-Crawl-Spaces"><a href="#8-PPL-Point-Cloud-Supervised-Proprioceptive-Locomotion-Reinforcement-Learning-for-Legged-Robots-in-Crawl-Spaces" class="headerlink" title="8. PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/PPL__Point_Cloud_Supervised_Proprioceptive_Locomotion_Reinforcement_Learning_for_Legged_Robots_in_Cr.pdf">PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了一种用于腿式机器人在狭小空间（如低矮隧道、洞穴等）自主穿越的点云监督本体感知强化学习方法（PPL）。该方法设计了基于历史本体传感器数据的状态估计网络（PPL-Net），结合极坐标点云特征提取，以及考虑全身碰撞惩罚和碰撞后速度奖励的奖励函数，实现了机器人仅凭本体感知在正常及低能见度环境下敏捷穿越狭窄空间。实验结果表明，PPL在平坦和阶梯隧道中的成功率显著优于现有方法，并能适应多种复杂和低可见环境，无需外部传感器。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/PPL%3A%20Point%20Cloud%20Supervised%20Proprioceptive%20Locomotion%20Reinforcement%20Learning%20for%20Legged%20Robots%20in%20Crawl%20Spaces.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-FLARE-Agile-Flights-for-Quadrotor-Cable-Suspended-Payload-System-via-Reinforcement-Learning"><a href="#9-FLARE-Agile-Flights-for-Quadrotor-Cable-Suspended-Payload-System-via-Reinforcement-Learning" class="headerlink" title="9. FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/FLARE__Agile_Flights_for_Quadrotor_Cable-Suspended_Payload_System_via_Reinforcement_Learning.pdf">FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出FLARE，一种基于强化学习（RL）的自主导航框架，针对四旋翼缆绳悬挂载荷系统的敏捷飞行问题。方法通过在高保真仿真中训练无模型RL策略，设计了奖励函数和观测空间以适应系统的复杂动力学，并在三个具有挑战性的场景（敏捷航点通过、载荷定位、门洞穿越）下进行验证。实验结果显示，所提出方法在敏捷性和实时性上显著优于基于优化的方法，实现了3倍速度提升，并成功实现了零样本仿真到现实的策略迁移，兼具敏捷性与安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/FLARE%3A%20Agile%20Flights%20for%20Quadrotor%20Cable-Suspended%20Payload%20System%20via%20Reinforcement%20Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-A-Shank-Angle-Based-Control-System-Enables-Soft-Exoskeleton-to-Assist-Human-Non-Steady-Locomotion"><a href="#10-A-Shank-Angle-Based-Control-System-Enables-Soft-Exoskeleton-to-Assist-Human-Non-Steady-Locomotion" class="headerlink" title="10. A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_Shank_Angle-Based_Control_System_Enables_Soft_Exoskeleton_to_Assist_Human_Non-Steady_Locomotion.pdf">A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of Robotics and Intelligent Systems, Shenyang Institute of Automation, Chinese Academy of Sciences</span></p><p>本论文提出了一种基于小腿倾角（shank angle）的软体踝关节外骨骼控制系统，通过双高斯模型实时在线生成仿生辅助力型谱，以实现跨步适应，并采用模型驱动的前馈控制实现精确跟踪，全部依赖IMU传感器数据。实验结果表明，该系统在行走、跑步及楼梯等非稳态运动中可持续与人体步态生物力学高度同步，显著降低肌肉激活和代谢率，有效提升了外骨骼在复杂环境下的助力效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20Shank%20Angle-Based%20Control%20System%20Enables%20Soft%20Exoskeleton%20to%20Assist%20Human%20Non-Steady%20Locomotion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-Whole-Body-Bilateral-Teleoperation-with-Multi-Stage-Object-Parameter-Estimation-for-Wheeled-Humanoid-Locomanipulation"><a href="#11-Whole-Body-Bilateral-Teleoperation-with-Multi-Stage-Object-Parameter-Estimation-for-Wheeled-Humanoid-Locomanipulation" class="headerlink" title="11. Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Whole-Body_Bilateral_Teleoperation_with_Multi-Stage_Object_Parameter_Estimation_for_Wheeled_Humanoid.pdf">Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Urbana-Champaign</span></p><p>该论文提出了面向轮式人形机器人全身双向远程操作的多阶段在线物体惯性参数估算框架。方法核心为将视觉与视觉-语言模型生成的物体尺寸和材料等先验，结合高保真仿真并行的分层采样优化（DH-CEM），实现物体惯性参数的快速、物理可行估算，并用于实时调整机器人控制器的平衡点和动态补偿。实验在自研硬件平台SATYRR上验证了系统可高效实现重物搬运、提升操作同步性和操作员体验，并提升了安全任务如避障的可靠性。结论：集成多模态估算和仿真采样的框架，使轮式人形机器人在复杂物理交互任务中表现出更强的动态适应性和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Whole-Body%20Bilateral%20Teleoperation%20with%20Multi-Stage%20Object%20Parameter%20Estimation%20for%20Wheeled%20Humanoid%20Locomanipulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory"><a href="#12-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory" class="headerlink" title="12. Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Seeing,_Listening,_Remembering,_and_Reasoning__A_Multimodal_Agent_with_Long-Term_Memory.pdf">Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ByteDance Seed</span></p><p>本文提出了M3-Agent，一个具备长期记忆的多模态智能体框架，能够通过视频和音频流实时感知环境，构建和更新具备实体中心结构的多模态长期记忆（包括情节和语义记忆），并通过多轮推理检索相关记忆以自主完成复杂任务。论文还开发了M3-Bench长视频问答基准，涵盖机器人视角和网络多场景，实验结果表明M3-Agent在多个基准上均大幅超越现有多模态大模型和视频理解方法。结论：M3-Agent显著提升了多模态智能体的长期记忆和推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Seeing%2C%20Listening%2C%20Remembering%2C%20and%20Reasoning%3A%20A%20Multimodal%20Agent%20with%20Long-Term%20Memory.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-Immersive-Teleoperation-of-Beyond-Human-Scale-Robotic-Manipulators-Challenges-and-Future-Directions"><a href="#13-Immersive-Teleoperation-of-Beyond-Human-Scale-Robotic-Manipulators-Challenges-and-Future-Directions" class="headerlink" title="13. Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Immersive_Teleoperation_of_Beyond-Human-Scale_Robotic_Manipulators__Challenges_and_Future_Directions.pdf">Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tampere University</span></p><p>该论文系统分析了超越人类尺度机器人操控（BHSRMs）沉浸式远程操作的控制、认知与界面设计挑战，提出通过高保真视觉与全臂力反馈（如外骨骼）提升操作员的沉浸感和身体感，实现更自然的人机协作。结论指出，沉浸式界面虽提升操作自然性和数据质量，但也带来感知冲突和安全风险，需发展新的人因评价工具和自适应反馈策略，为大规模学习驱动的远程机器人系统奠定基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Immersive%20Teleoperation%20of%20Beyond-Human-Scale%20Robotic%20Manipulators%3A%20Challenges%20and%20Future%20Directions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Interpretable-Robot-Control-via-Structured-Behavior-Trees-and-Large-Language-Models"><a href="#14-Interpretable-Robot-Control-via-Structured-Behavior-Trees-and-Large-Language-Models" class="headerlink" title="14. Interpretable Robot Control via Structured Behavior Trees and Large Language Models"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Interpretable_Robot_Control_via_Structured_Behavior_Trees_and_Large_Language_Models.pdf">Interpretable Robot Control via Structured Behavior Trees and Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Luxembourg</span></p><p>本文提出了一种将大型语言模型（LLM）与行为树（BT）相结合的机器人控制框架，通过自然语言指令实现对机器人的可解释、模块化和实时控制。方法包括基于GPT-4o的语义解析、自动行为选择、结构化行为树执行以及插件式感知模块（如目标跟踪、手势识别）；实验结果显示该系统在多平台环境下指令理解与动作执行的平均准确率达到94%，有效提升了机器人在人机交互中的适应性和透明度。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Interpretable%20Robot%20Control%20via%20Structured%20Behavior%20Trees%20and%20Large%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-HapticGiant-A-Novel-Very-Large-Kinesthetic-Haptic-Interface-with-Hierarchical-Force-Control"><a href="#15-HapticGiant-A-Novel-Very-Large-Kinesthetic-Haptic-Interface-with-Hierarchical-Force-Control" class="headerlink" title="15. HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/HapticGiant__A_Novel_Very_Large_Kinesthetic_Haptic_Interface_with_Hierarchical_Force_Control.pdf">HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Karlsruhe Institute of Technology</span></p><p>本文提出HapticGiant，一种超大型贴合人体手臂运动学的本体型动力触觉界面，结合2D龙门预定位单元和自定义六自由度机械臂，实现房间级空间内的自然步行和全方位触觉反馈。方法上创新性采用分层优化的感受型力控制算法，统一处理关节、工作空间和奇异点等系统约束，实现任意串联运动链和笛卡尔感受的实时触觉渲染。实验展示了系统在虚拟门和挖掘机等数字孪生交互中的高度沉浸性和可用性。结论：HapticGiant具备极大空间和优秀运动学匹配，通过高层控制方案显著提升虚拟现实中的沉浸式动力触觉体验。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HapticGiant%3A%20A%20Novel%20Very%20Large%20Kinesthetic%20Haptic%20Interface%20with%20Hierarchical%20Force%20Control.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-BEAVR-Bimanual-multi-Embodiment-Accessible-Virtual-Reality-Teleoperation-System-for-Robots"><a href="#16-BEAVR-Bimanual-multi-Embodiment-Accessible-Virtual-Reality-Teleoperation-System-for-Robots" class="headerlink" title="16. BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/BEAVR__Bimanual,_multi-Embodiment,_Accessible,_Virtual_Reality_Teleoperation_System_for_Robots.pdf">BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Massachusetts Institute of Technology</span></p><p>本论文提出了BEAVR系统——一个开源、模块化、硬件无关的虚拟现实（VR）机器人远程操作平台，支持7-DoF机械臂到全身人形机器人等多种机器人形态，通过低延迟（≤35 ms）流媒体架构和异步“think–act”控制环，实现实时灵巧双手操作、标准化数据采集（LeRobot格式）及端到端策略学习。实验表明，BEAVR兼容多种主流视觉-运动策略（ACT、DiffusionPolicy、SmolVLA），在多任务、多机器人环境下保持高成功率和低时延，极大提升了远程操作系统的可扩展性和学习友好性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/BEAVR%3A%20Bimanual%2C%20multi-Embodiment%2C%20Accessible%2C%20Virtual%20Reality%20Teleoperation%20System%20for%20Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-EvoCurr-Self-evolving-Curriculum-with-Behavior-Code-Generation-for-Complex-Decision-making"><a href="#17-EvoCurr-Self-evolving-Curriculum-with-Behavior-Code-Generation-for-Complex-Decision-making" class="headerlink" title="17. EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/EvoCurr__Self-evolving_Curriculum_with_Behavior_Code_Generation_for_Complex_Decision-making.pdf">EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhongguancun Academy</span></p><p>本文提出了EvoCurr框架，通过两个大型语言模型（LLM）分别担任自适应课程设计和行为代码生成角色，实现复杂决策任务（以StarCraft II为例）中的逐步难度提升与策略进化。方法包括课程设计LLM根据行为代码LLM的表现动态调整任务难度，并生成适应性决策树脚本。实验表明，EvoCurr显著提升了复杂任务的成功率和解题效率，证明了自进化课程学习在高复杂度自动推理中的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/EvoCurr%3A%20Self-evolving%20Curriculum%20with%20Behavior%20Code%20Generation%20for%20Complex%20Decision-making.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-Skyshield-Event-Driven-Submillimetre-Thin-Obstacle-Detection-for-Drone-Flight-Safety"><a href="#18-Skyshield-Event-Driven-Submillimetre-Thin-Obstacle-Detection-for-Drone-Flight-Safety" class="headerlink" title="18. Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Skyshield__Event-Driven_Submillimetre_Thin_Obstacle_Detection_for_Drone_Flight_Safety.pdf">Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen International Graduate School, Tsinghua University</span></p><p>本论文提出了Skyshield系统，基于事件相机，采用轻量级U-Net（LUnet）网络和创新的Dice-Contour Regularization损失，实现对无人机亚毫米级细障碍物的实时检测。实验表明，该方法在边缘平台上显著优于传统方法，具有高精度和低延迟，提升了无人机的飞行安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Skyshield%3A%20Event-Driven%20Submillimetre%20Thin%20Obstacle%20Detection%20for%20Drone%20Flight%20Safety.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Decentralized-Rank-Scheduling-for-Energy-Constrained-Multi-Task-Federated-Fine-Tuning-in-Edge-Assisted-IoV-Networks"><a href="#19-Decentralized-Rank-Scheduling-for-Energy-Constrained-Multi-Task-Federated-Fine-Tuning-in-Edge-Assisted-IoV-Networks" class="headerlink" title="19. Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Decentralized_Rank_Scheduling_for_Energy-Constrained_Multi-Task_Federated_Fine-Tuning_in_Edge-Assist.pdf">Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-sen University</span></p><p>本论文提出一种分层联邦微调框架，结合LoRA低秩适配与去中心化能耗约束的rank优化，适用于动态、资源受限的车联网（IoV）场景。方法上，创新性地将LoRA rank选择建模为带能耗约束的多臂赌博机(MAB)问题，并提出UCB-DUAL分布式算法，实现任务级能耗分配与客户端级自适应rank分配，有理论保证的次线性遗憾。实验结果表明，该方法在大规模仿真下可提升平均精度2.5%，降低延迟24%，同时减少显存占用，优于多种基线。结论：该框架能高效实现车联网边缘智能的多任务自适应联邦微调。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Decentralized%20Rank%20Scheduling%20for%20Energy-Constrained%20Multi-Task%20Federated%20Fine-Tuning%20in%20Edge-Assisted%20IoV%20Networks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-Distilling-LLM-Prior-to-Flow-Model-for-Generalizable-Agent’s-Imagination-in-Object-Goal-Navigation"><a href="#20-Distilling-LLM-Prior-to-Flow-Model-for-Generalizable-Agent’s-Imagination-in-Object-Goal-Navigation" class="headerlink" title="20. Distilling LLM Prior to Flow Model for Generalizable Agent’s Imagination in Object Goal Navigation"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Distilling_LLM_Prior_to_Flow_Model_for_Generalizable_Agent's_Imagination_in_Object_Goal_Navigation.pdf">Distilling LLM Prior to Flow Model for Generalizable Agent’s Imagination in Object Goal Navigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science and Engineering, Sun Yat-sen University</span></p><p>该论文提出GOAL框架，通过在训练阶段将大语言模型（LLM）推理得到的空间语义先验以高斯场形式注入到语义地图，结合生成式流模型对室内场景的语义分布进行建模，实现了对未观测区域的高质量推理。实验表明，GOAL在Gibson、MP3D及迁移到HM3D等数据集上均取得了当前最优的导航表现，显著提升了泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Distilling%20LLM%20Prior%20to%20Flow%20Model%20for%20Generalizable%20Agent%E2%80%99s%20Imagination%20in%20Object%20Goal%20Navigation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="21-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation"><a href="#21-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation" class="headerlink" title="21. Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Echo-4o__Harnessing_the_Power_of_GPT-4o_Synthetic_Images_for_Improved_Image_Generation.pdf">Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Artificial Intelligence Laboratory</span></p><p>本论文提出Echo-4o-Image，一个由GPT-4o生成的18万规模合成图像数据集，涵盖超现实场景、多参考图像生成和复杂指令跟随任务，专为弥补真实世界数据稀缺场景和提升指令对齐而设计。在此基础上，作者微调统一多模态生成模型Bagel得到Echo-4o，并引入GenEval++和Imagine-Bench两个新基准，系统评测指令跟随与创意生成能力。实验表明Echo-4o在多个标准和新基准下均取得领先，且合成数据对多种主流基础模型均有显著迁移增益，验证了高质量合成数据集对提升图像生成模型多样性、指令对齐及创新能力的价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Echo-4o%3A%20Harnessing%20the%20Power%20of%20GPT-4o%20Synthetic%20Images%20for%20Improved%20Image%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-A-Survey-on-3D-Gaussian-Splatting-Applications-Segmentation-Editing-and-Generation"><a href="#22-A-Survey-on-3D-Gaussian-Splatting-Applications-Segmentation-Editing-and-Generation" class="headerlink" title="22. A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_Survey_on_3D_Gaussian_Splatting_Applications__Segmentation,_Editing,_and_Generation.pdf">A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai University of Finance and Economics</span></p><p>该论文系统综述了3D Gaussian Splatting（3DGS）在分割、编辑和生成等下游视觉任务中的最新进展，详细介绍了2D基础模型（如CLIP、SAM、DINO等）与3DGS的结合方式，并针对三大应用方向梳理了代表性方法、学习范式和评测基准。结论指出3DGS在场景理解、编辑和生成等高层视觉任务中展现出高效、可扩展和语义丰富的能力，并提出未来需完善大规模训练、三维评价体系与多模态语言模型整合。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20Survey%20on%203D%20Gaussian%20Splatting%20Applications%3A%20Segmentation%2C%20Editing%2C%20and%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation"><a href="#23-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation" class="headerlink" title="23. Story2Board: A Training-Free Approach for Expressive Storyboard Generation"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Story2Board__A_Training-Free_Approach_for_Expressive_Storyboard_Generation.pdf">Story2Board: A Training-Free Approach for Expressive Storyboard Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hebrew University of Jerusalem</span></p><p>Story2Board提出了一种训练无关的框架，实现从自然语言生成富有表现力且一致的多面板故事板。方法核心包括Latent Panel Anchoring和Reciprocal Attention Value Mixing，增强扩散模型在多场景下角色一致性和布局多样性，无需模型微调或架构修改。实验显示该方法在角色一致性、场景多样性及用户偏好上优于现有基线，提升了视觉叙事的表现力和连贯性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Story2Board%3A%20A%20Training-Free%20Approach%20for%20Expressive%20Storyboard%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models"><a href="#24-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models" class="headerlink" title="24. Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Noise_Hypernetworks__Amortizing_Test-Time_Compute_in_Diffusion_Models.pdf">Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Munich</span></p><p>该论文提出了一种用于扩散生成模型的Noise Hypernetworks（HyperNoise）方法，通过训练一个轻量级超网络直接预测优化后的初始噪声，从而将原本消耗大量计算资源的测试时噪声优化过程转移到训练后阶段。实验结果表明，该方法在保持极低推理延迟的情况下，显著提升了当前主流文本到图像扩散模型的生成质量，有效避免了直接微调带来的模式崩溃和“奖励黑客”问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Noise%20Hypernetworks%3A%20Amortizing%20Test-Time%20Compute%20in%20Diffusion%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-MOC-Meta-Optimized-Classifier-for-Few-Shot-Whole-Slide-Image-Classification"><a href="#25-MOC-Meta-Optimized-Classifier-for-Few-Shot-Whole-Slide-Image-Classification" class="headerlink" title="25. MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/MOC__Meta-Optimized_Classifier_for_Few-Shot_Whole_Slide_Image_Classification.pdf">MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>本论文提出了一种面向少样本全切片图像（WSI）分类的新型Meta-Optimized Classifier (MOC)。其核心方法包含一个自动从多种分类器中动态配置的meta-learner和一个包含多样候选分类器的分类器库，使分类器在不同病理特征上互补，对全切片图像进行更全面的诊断。实验表明，MOC在多个少样本基准数据集上优于现有方法，AUC提升最高可达26.25%，为临床数据稀缺场景下的诊断部署提供关键进展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/MOC%3A%20Meta-Optimized%20Classifier%20for%20Few-Shot%20Whole%20Slide%20Image%20Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-T-CACE-A-Time-Conditioned-Autoregressive-Contrast-Enhancement-Multi-Task-Framework-for-Contrast-Free-Liver-MRI-Synthesis-Segmentation-and-Diagnosis"><a href="#26-T-CACE-A-Time-Conditioned-Autoregressive-Contrast-Enhancement-Multi-Task-Framework-for-Contrast-Free-Liver-MRI-Synthesis-Segmentation-and-Diagnosis" class="headerlink" title="26. T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/T-CACE__A_Time-Conditioned_Autoregressive_Contrast_Enhancement_Multi-Task_Framework_for_Contrast-Fre.pdf">T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Toronto Metropolitan University</span></p><p>该论文提出了一种时间条件自回归对比增强（T-CACE）多任务框架，实现了无需造影剂的肝脏MRI多期合成、分割与诊断。核心方法包括条件Token编码（CTE）融合解剖和时间信息、动态时间感知注意力掩码（DTAM）实现相位间平滑过渡，以及时间分类一致性约束（TCC）保证诊断输出的生理一致性。大量实验表明T-CACE在图像合成、分割和病灶分类方面均超越现有方法，并为临床提供更安全高效的诊断替代方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/T-CACE%3A%20A%20Time-Conditioned%20Autoregressive%20Contrast%20Enhancement%20Multi-Task%20Framework%20for%20Contrast-Free%20Liver%20MRI%20Synthesis%2C%20Segmentation%2C%20and%20Diagnosis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-HumanGenesis-Agent-Based-Geometric-and-Generative-Modeling-for-Synthetic-Human-Dynamics"><a href="#27-HumanGenesis-Agent-Based-Geometric-and-Generative-Modeling-for-Synthetic-Human-Dynamics" class="headerlink" title="27. HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/HumanGenesis__Agent-Based_Geometric_and_Generative_Modeling_for_Synthetic_Human_Dynamics.pdf">HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-sen University</span></p><p>HumanGenesis提出了一种多代理协同框架，结合3D高斯点渲染、人体和场景分离重建、时序姿态编码、视频扩散生成以及基于多模态大模型的细粒度自反思优化。结论显示该方法在文生动作、视频重演和新姿态生成等任务上，实现了运动表达性、几何保真度和场景融合的显著提升，优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HumanGenesis%3A%20Agent-Based%20Geometric%20and%20Generative%20Modeling%20for%20Synthetic%20Human%20Dynamics.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-Perceptual-Reality-Transformer-Neural-Architectures-for-Simulating-Neurological-Perception-Conditions"><a href="#28-Perceptual-Reality-Transformer-Neural-Architectures-for-Simulating-Neurological-Perception-Conditions" class="headerlink" title="28. Perceptual Reality Transformer: Neural Architectures for Simulating Neurological Perception Conditions"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Perceptual_Reality_Transformer__Neural_Architectures_for_Simulating_Neurological_Perception_Conditio.pdf">Perceptual Reality Transformer: Neural Architectures for Simulating Neurological Perception Conditions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Icahn School of Medicine at Mount Sinai</span></p><p>该论文提出了Perceptual Reality Transformer框架，利用六种神经网络架构（包括Vision Transformer、CNN、VAE、Diffusion等），结合临床文献构建特定神经感知障碍的视觉仿真函数，实现对八类神经感知状态的图像转化模拟。在ImageNet和CIFAR-10等基准数据集的系统实验表明，Vision Transformer在重建精度和全局上下文模拟方面表现最佳，为神经感知障碍仿真建立了首个系统性基准，并为医学教育和同理训练等应用提供了新工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Perceptual%20Reality%20Transformer%3A%20Neural%20Architectures%20for%20Simulating%20Neurological%20Perception%20Conditions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Enhancing-Diffusion-Face-Generation-with-Contrastive-Embeddings-and-SegFormer-Guidance"><a href="#29-Enhancing-Diffusion-Face-Generation-with-Contrastive-Embeddings-and-SegFormer-Guidance" class="headerlink" title="29. Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Enhancing_Diffusion_Face_Generation_with_Contrastive_Embeddings_and_SegFormer_Guidance.pdf">Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Surrey</span></p><p>本论文提出在人脸生成领域中，结合InfoNCE对比损失优化属性嵌入，以及采用SegFormer作为分割掩码编码器，提升扩散模型的条件生成能力。通过在CelebAMask-HQ小规模数据集上，系统评测UNet与DiT架构、Stable Diffusion LoRA微调、属性与分割联合控制，结果显示联合InfoNCE与SegFormer大幅提升了属性一致性与空间可控性，实现了更高质量的人脸生成（FID最低可达63.85）。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Enhancing%20Diffusion%20Face%20Generation%20with%20Contrastive%20Embeddings%20and%20SegFormer%20Guidance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-Hierarchical-Graph-Attention-Network-for-No-Reference-Omnidirectional-Image-Quality-Assessment"><a href="#30-Hierarchical-Graph-Attention-Network-for-No-Reference-Omnidirectional-Image-Quality-Assessment" class="headerlink" title="30. Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Hierarchical_Graph_Attention_Network_for_No-Reference_Omnidirectional_Image_Quality_Assessment.pdf">Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Guangdong University of Technology</span></p><p>该论文提出一种基于图神经网络的无参考全景图像质量评估方法，通过Fibonacci球面采样生成视口，并利用Swin Transformer提取多级特征作为图节点，结合图注意力网络（GAT）建模局部结构关系和图Transformer捕获长距离空间依赖，实现了对空间非均匀失真的细致建模。实验结果显示，该方法在JUFE-10K和OIQ-10K大型数据集上显著优于现有方法，具备较强的泛化能力和预测准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Hierarchical%20Graph%20Attention%20Network%20for%20No-Reference%20Omnidirectional%20Image%20Quality%20Assessment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-Reforming-SISR-Models-for-PBR-Material-Super-Resolution-via-Cross-Map-Attention"><a href="#31-Reforming-SISR-Models-for-PBR-Material-Super-Resolution-via-Cross-Map-Attention" class="headerlink" title="31. Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/MUJICA__Reforming_SISR_Models_for_PBR_Material_Super-Resolution_via_Cross-Map_Attention.pdf">Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ubisoft La Forge</span></p><p>本文提出了MUJICA方法，通过将跨图融合注意力机制作为适配器接入预训练的Swin-transformer单图超分辨率模型，实现对物理渲染材料（PBR）多模态纹理图（如basecolor、normal等）的高效超分辨率重建。MUJICA模块支持冻结主干，仅少量参数训练，融合多模态特征显著提升PSNR、SSIM和LPIPS指标，并保持跨图一致性，在有限数据下达到SOTA性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Reforming%20SISR%20Models%20for%20PBR%20Material%20Super-Resolution%20via%20Cross-Map%20Attention.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-DSS-Prompt-Dynamic-Static-Synergistic-Prompting-for-Few-Shot-Class-Incremental-Learning"><a href="#32-DSS-Prompt-Dynamic-Static-Synergistic-Prompting-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="32. DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/DSS-Prompt__Dynamic-Static_Synergistic_Prompting_for_Few-Shot_Class-Incremental_Learning.pdf">DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了一种称为DSS-Prompt的新方法，通过在每个Vision Transformer块中协同利用静态和动态多模态提示，实现少样本类增量学习（FSCIL）。该方法利用静态提示弥合预训练与下游数据集的领域差异，并通过多模态动态提示提升实例感知语义，从而在冻结主干网络的前提下提升泛化性与新类适应性。实验表明，该方法在四个主流基准上显著优于现有方法，并有效缓解灾难性遗忘问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/DSS-Prompt%3A%20Dynamic-Static%20Synergistic%20Prompting%20for%20Few-Shot%20Class-Incremental%20Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-Stable-Diffusion-Models-are-Secretly-Good-at-Visual-In-Context-Learning"><a href="#33-Stable-Diffusion-Models-are-Secretly-Good-at-Visual-In-Context-Learning" class="headerlink" title="33. Stable Diffusion Models are Secretly Good at Visual In-Context Learning"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Stable_Diffusion_Models_are_Secretly_Good_at_Visual_In-Context_Learning.pdf">Stable Diffusion Models are Secretly Good at Visual In-Context Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Apple</span></p><p>本文提出了一种完全无需训练的新型视觉in-context learning（V-ICL）方法，直接对现成的Stable Diffusion模型自注意力层进行in-place attention重计算，显式引入查询图与示例提示之间的上下文，并提出隐式加权的多提示集成策略。结果表明该方法无需额外训练即可在前景分割、单目标检测、语义分割、关键点检测、边缘检测和彩色化六项任务上大幅优于现有训练或微调的V-ICL方法，且泛化性更强。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Stable%20Diffusion%20Models%20are%20Secretly%20Good%20at%20Visual%20In-Context%20Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-Prototype-Guided-Diffusion-Visual-Conditioning-without-External-Memory"><a href="#34-Prototype-Guided-Diffusion-Visual-Conditioning-without-External-Memory" class="headerlink" title="34. Prototype-Guided Diffusion: Visual Conditioning without External Memory"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Prototype-Guided_Diffusion__Visual_Conditioning_without_External_Memory.pdf">Prototype-Guided Diffusion: Visual Conditioning without External Memory</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Université Sorbonne Paris Nord</span></p><p>本文提出了Prototype Diffusion Model (PDM)，在扩散模型训练过程中直接集成原型学习，通过对无标签图像特征进行动态对比学习和原型分配，实现无需外部记忆即可进行高效视觉条件生成。实验表明，PDM在不依赖外部检索系统和CLIP等预训练模型的情况下，能以较低计算和存储开销达到与检索增强扩散方法相当甚至更优的生成质量，且其有监督变体(s-PDM)在有标签数据下进一步提升了图像质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Prototype-Guided%20Diffusion%3A%20Visual%20Conditioning%20without%20External%20Memory.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-SpeechForensics-Audio-Visual-Speech-Representation-Learning-for-Face-Forgery-Detection"><a href="#35-SpeechForensics-Audio-Visual-Speech-Representation-Learning-for-Face-Forgery-Detection" class="headerlink" title="35. SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/SpeechForensics__Audio-Visual_Speech_Representation_Learning_for_Face_Forgery_Detection.pdf">SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Information Engineering, Chinese Academy of Sciences</span></p><p>该论文提出了一种基于语音和视觉双模态语音表示学习的无监督人脸伪造检测方法。通过在真实视频上自监督学习音频-视觉语音表示，融合局部和全局语义信息，对伪造视频中音频与唇动的不一致进行检测，无需伪造数据参与训练。实验结果显示，该方法在跨数据集泛化和鲁棒性方面显著优于现有方法，能有效识别多种伪造类型并对多种常见扰动保持高鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/SpeechForensics%3A%20Audio-Visual%20Speech%20Representation%20Learning%20for%20Face%20Forgery%20Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-OneVAE-Joint-Discrete-and-Continuous-Optimization-Helps-Discrete-Video-VAE-Train-Better"><a href="#36-OneVAE-Joint-Discrete-and-Continuous-Optimization-Helps-Discrete-Video-VAE-Train-Better" class="headerlink" title="36. OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/OneVAE__Joint_Discrete_and_Continuous_Optimization_Helps_Discrete_Video_VAE_Train_Better.pdf">OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nankai University</span></p><p>本文提出OneVAE，一种统一离散与连续视频表示的VAE框架，采用渐进式训练策略，利用预训练连续VAE先验加速离散VAE收敛，并引入多token量化与首帧增强等结构改进，有效提升高压缩下的重建质量。实验结果表明，该方法大幅加快训练速度，提升视频重建与统一建模性能，为视频生成和VAE研究提供新方向。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/OneVAE%3A%20Joint%20Discrete%20and%20Continuous%20Optimization%20Helps%20Discrete%20Video%20VAE%20Train%20Better.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-Do-Vision-Transformers-See-Like-Humans-Evaluating-their-Perceptual-Alignment"><a href="#37-Do-Vision-Transformers-See-Like-Humans-Evaluating-their-Perceptual-Alignment" class="headerlink" title="37. Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Do_Vision_Transformers_See_Like_Humans__Evaluating_their_Perceptual_Alignment.pdf">Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universidad de Valencia</span></p><p>该论文系统分析了模型规模、数据集规模、数据增强和正则化对Vision Transformer（ViT）与人类感知一致性的影响，采用TID2013图像质量数据集，基于模型输出与人类评分的相关性量化感知对齐度。结果表明，增加模型规模、数据增强和正则化虽提升分类性能但会降低与人类感知的一致性，提示模型性能与感知对齐存在权衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Do%20Vision%20Transformers%20See%20Like%20Humans%3F%20Evaluating%20their%20Perceptual%20Alignment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-Reverse-Convolution-and-Its-Applications-to-Image-Restoration"><a href="#38-Reverse-Convolution-and-Its-Applications-to-Image-Restoration" class="headerlink" title="38. Reverse Convolution and Its Applications to Image Restoration"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Reverse_Convolution_and_Its_Applications_to_Image_Restoration.pdf">Reverse Convolution and Its Applications to Image Restoration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本文提出了一种新颖的深度可逆卷积（reverse convolution）算子，作为深度卷积的数学逆，通过正则化最小二乘优化获得闭式解，并结合层归一化、1×1卷积和GELU激活构建了Transformer风格的逆卷积模块（Converse Block）。该算子可无缝替换现有网络中的标准卷积和转置卷积，并在图像去噪、超分辨率和去模糊等图像复原任务中进行了广泛实验，结果表明其在恢复图像细节和提升性能方面优于传统方法。结论是该方法为深度模型设计提供了原理性可逆算子，具有良好应用前景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Reverse%20Convolution%20and%20Its%20Applications%20to%20Image%20Restoration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-Region-to-Region-Enhancing-Generative-Image-Harmonization-with-Adaptive-Regional-Injection"><a href="#39-Region-to-Region-Enhancing-Generative-Image-Harmonization-with-Adaptive-Regional-Injection" class="headerlink" title="39. Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Region-to-Region__Enhancing_Generative_Image_Harmonization_with_Adaptive_Regional_Injection.pdf">Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本文提出Region-to-Region转化方法，通过将适当区域的信息注入前景，实现图像细节保留与和谐化，并生成新型复合数据。在此基础上，作者设计了R2R模型，结合Clear-VAE（带自适应滤波器）和Harmony Controller（含掩码感知自适应通道注意力MACA），有效提升了图像和谐化能力。实验表明，该方法在iHarmony4和新构建的RPHarmony数据集上均取得了最优结果，生成图像在真实场景中表现出更好的和谐性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Region-to-Region%3A%20Enhancing%20Generative%20Image%20Harmonization%20with%20Adaptive%20Regional%20Injection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-NEURAL-Attention-Guided-Pruning-for-Unified-Multimodal-Resource-Constrained-Clinical-Evaluation"><a href="#40-NEURAL-Attention-Guided-Pruning-for-Unified-Multimodal-Resource-Constrained-Clinical-Evaluation" class="headerlink" title="40. NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/NEURAL__Attention-Guided_Pruning_for_Unified_Multimodal_Resource-Constrained_Clinical_Evaluation.pdf">NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>该论文提出NEURAL框架，通过将胸部X光图像划分为patch并利用Fine-tuned Vision-Language Model（如ClinicalT5）报告生成过程中的cross-attention分数，结构性剪枝医学图像，仅保留诊断关键区域，并与基于报告的知识图谱融合成统一图结构，用于高效的下游诊断任务。实验证明，在MIMIC-CXR和CheXpert Plus两大数据集上，NEURAL实现了93.4-97.7%的图像数据压缩，AUC达0.88-0.95，显著优于未压缩基线模型，且保持高诊断性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/NEURAL%3A%20Attention-Guided%20Pruning%20for%20Unified%20Multimodal%20Resource-Constrained%20Clinical%20Evaluation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors"><a href="#41-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors" class="headerlink" title="41. GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/GSFixer__Improving_3D_Gaussian_Splatting_with_Reference-Guided_Video_Diffusion_Priors.pdf">GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Macau</span></p><p>该论文提出GSFixer，一种利用参考引导的视频扩散模型，结合2D语义特征和3D几何特征，有效修复稀疏视角下3D Gaussian Splatting重建中的伪影问题。通过引入参考引导的轨迹采样策略和DL3DV-Res基准评测，GSFixer在三维伪影修复和稀疏视角三维重建任务上显著优于现有方法，提升了新视角合成的一致性和重建质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/GSFixer%3A%20Improving%203D%20Gaussian%20Splatting%20with%20Reference-Guided%20Video%20Diffusion%20Priors.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Semantic-aware-DropSplat-Adaptive-Pruning-of-Redundant-Gaussians-for-3D-Aerial-View-Segmentation"><a href="#42-Semantic-aware-DropSplat-Adaptive-Pruning-of-Redundant-Gaussians-for-3D-Aerial-View-Segmentation" class="headerlink" title="42. Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Semantic-aware_DropSplat__Adaptive_Pruning_of_Redundant_Gaussians_for_3D_Aerial-View_Segmentation.pdf">Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xidian University</span></p><p>该论文提出了SAD-Splat方法用于3D航空视角场景语义分割，结合高置信伪标签生成和语义置信度引导的可学习稀疏机制（基于Hard Concrete分布），通过自适应剪枝冗余和语义不明确的高斯点，提升多视角3D语义分割的表现与模型紧凑性，并构建了挑战性数据集3D-AS。实验表明，该方法在减少高斯点数量的同时提高了分割精度，为高效的3D场景理解提供了新思路。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Semantic-aware%20DropSplat%3A%20Adaptive%20Pruning%20of%20Redundant%20Gaussians%20for%203D%20Aerial-View%20Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-BridgeTA-Bridging-the-Representation-Gap-in-Knowledge-Distillation-via-Teacher-Assistant-for-Bird’s-Eye-View-Map-Segmentation"><a href="#43-BridgeTA-Bridging-the-Representation-Gap-in-Knowledge-Distillation-via-Teacher-Assistant-for-Bird’s-Eye-View-Map-Segmentation" class="headerlink" title="43. BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird’s Eye View Map Segmentation"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/BridgeTA__Bridging_the_Representation_Gap_in_Knowledge_Distillation_via_Teacher_Assistant_for_Bird's.pdf">BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird’s Eye View Map Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yonsei University</span></p><p>该论文提出BridgeTA，一种针对BEV（鸟瞰图）语义分割的高效知识蒸馏框架，通过一个轻量级Teacher Assistant（TA）网络，在不增加学生模型推理成本的前提下，桥接LiDAR-摄像头融合模型和仅摄像头模型之间的表征鸿沟。方法理论上利用Young不等式分解蒸馏路径，结合多层次特征蒸馏（特征、解码、logit），显著提升了学生模型性能。最终在nuScenes数据集上，BridgeTA相较于Camera-only基线提升4.2% mIoU，且推理速度与开销不变，优于其他SOTA方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/BridgeTA%3A%20Bridging%20the%20Representation%20Gap%20in%20Knowledge%20Distillation%20via%20Teacher%20Assistant%20for%20Bird%E2%80%99s%20Eye%20View%20Map%20Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Dual-Recursive-Feedback-on-Generation-and-Appearance-Latents-for-Pose-Robust-Text-to-Image-Diffusion"><a href="#44-Dual-Recursive-Feedback-on-Generation-and-Appearance-Latents-for-Pose-Robust-Text-to-Image-Diffusion" class="headerlink" title="44. Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Dual_Recursive_Feedback_on_Generation_and_Appearance_Latents_for_Pose-Robust_Text-to-Image_Diffusion.pdf">Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Korea University</span></p><p>该论文提出一种无需训练的Dual Recursive Feedback (DRF)方法，实现对结构和外观的双重递归反馈，增强Text-to-Image扩散模型在姿态迁移和结构-外观融合任务中的控制力。通过递归地对生成潜变量和外观潜变量施加反馈，方法有效保持结构一致性和外观保真度，即使在类别无关场景（如将人类动作迁移到动物外观）下也能生成高质量、结构与语义一致的图像。实验显示，DRF在多种基准任务和指标上均优于现有主流方法，支持不同扩散模型框架的无缝集成，显著提升生成图像的结构和外观融合效果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Dual%20Recursive%20Feedback%20on%20Generation%20and%20Appearance%20Latents%20for%20Pose-Robust%20Text-to-Image%20Diffusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-WEC-DG-Multi-Exposure-Wavelet-Correction-Method-Guided-by-Degradation-Description"><a href="#45-WEC-DG-Multi-Exposure-Wavelet-Correction-Method-Guided-by-Degradation-Description" class="headerlink" title="45. WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/WEC-DG__Multi-Exposure_Wavelet_Correction_Method_Guided_by_Degradation_Description.pdf">WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Jilin University</span></p><p>本论文提出了一种结合退化描述指导和离散小波变换的多曝光图像增强方法WEC-DG。方法核心包括场景描述生成模块（SDGM）、曝光一致性校准模块（ECAM）和曝光恢复与细节重建模块（EDRM），通过退化描述符引导曝光校准，分阶段分别优化照明与细节。实验表明，该方法在多公开数据集上性能超越现有方法，显著提升了复杂成像环境下的曝光校正效果和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/WEC-DG%3A%20Multi-Exposure%20Wavelet%20Correction%20Method%20Guided%20by%20Degradation%20Description.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-Exploring-the-Equivalence-of-Closed-Set-Generative-and-Real-Data-Augmentation-in-Image-Classification"><a href="#46-Exploring-the-Equivalence-of-Closed-Set-Generative-and-Real-Data-Augmentation-in-Image-Classification" class="headerlink" title="46. Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Exploring_the_Equivalence_of_Closed-Set_Generative_and_Real_Data_Augmentation_in_Image_Classificatio.pdf">Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, San Diego</span></p><p>本文系统研究了在图像分类任务中，利用在同一数据集上训练的扩散生成模型（即closed-set generative data augmentation）合成数据与真实数据增强的等效性。方法上，作者在自然图像（CIFAR-10、ImageNet-100）和医学图像（BloodMNIST）上，通过从零训练扩散模型生成合成样本，并与真实数据增强进行对比，提出了量化两者等效性的经验公式。结论指出，合成数据可提升分类性能，但达到同等提升需更多合成数据，且合成数据的边际效益随规模增加递减，对基础训练集较大时扩展性更好，为缺乏真实数据的应用提供实证指导。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Exploring%20the%20Equivalence%20of%20Closed-Set%20Generative%20and%20Real%20Data%20Augmentation%20in%20Image%20Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-Multimodal-Sheaf-based-Network-for-Glioblastoma-Molecular-Subtype-Prediction"><a href="#47-Multimodal-Sheaf-based-Network-for-Glioblastoma-Molecular-Subtype-Prediction" class="headerlink" title="47. Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Multimodal_Sheaf-based_Network_for_Glioblastoma_Molecular_Subtype_Prediction.pdf">Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>本文提出了一种基于sheaf神经网络的多模态结构感知框架，用于融合MRI与组织病理图像，实现胶质母细胞瘤分子亚型预测。该方法通过sheaf理论建模多模态数据的拓扑结构，实现缺失模态下的鲁棒分类与重建，实验结果在分子亚型多标签分类和缺失模态重建任务上均优于现有多模态图神经网络基线。结论：该方法能有效融合医学影像多模态特征，提升诊断准确性，具备临床虚拟活检潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Multimodal%20Sheaf-based%20Network%20for%20Glioblastoma%20Molecular%20Subtype%20Prediction.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-MangaDiT-Reference-Guided-Line-Art-Colorization-with-Hierarchical-Attention-in-Diffusion-Transformers"><a href="#48-MangaDiT-Reference-Guided-Line-Art-Colorization-with-Hierarchical-Attention-in-Diffusion-Transformers" class="headerlink" title="48. MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/MangaDiT__Reference-Guided_Line_Art_Colorization_with_Hierarchical_Attention_in_Diffusion_Transforme.pdf">MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CyberAgent</span></p><p>该论文提出MangaDiT模型，结合Diffusion Transformer架构和分层注意力机制，通过动态加权策略提升参考引导线稿上色的区域级一致性。实验结果显示，在角色姿态或动作变化较大的场景下，MangaDiT在两个基准数据集上显著优于现有方法，提升了定量和定性表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/MangaDiT%3A%20Reference-Guided%20Line%20Art%20Colorization%20with%20Hierarchical%20Attention%20in%20Diffusion%20Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-NegFaceDiff-The-Power-of-Negative-Context-in-Identity-Conditioned-Diffusion-for-Synthetic-Face-Generation"><a href="#49-NegFaceDiff-The-Power-of-Negative-Context-in-Identity-Conditioned-Diffusion-for-Synthetic-Face-Generation" class="headerlink" title="49. NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/NegFaceDiff__The_Power_of_Negative_Context_in_Identity-Conditioned_Diffusion_for_Synthetic_Face_Gene.pdf">NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fraunhofer IGD</span></p><p>该论文提出NegFaceDiff，一种在身份条件扩散模型采样阶段引入负条件的新方法，通过在生成过程中同时施加正负身份嵌入，实现对期望身份特征的保持和对不相关身份特征的排斥，从而提升合成人脸数据的身份一致性和分离性。实验结果表明，NegFaceDiff生成的数据不仅提升了身份区分度（FDR从2.427提升到5.687，EER从0.130降至0.039），而且用其训练的人脸识别模型在多个基准数据集上均优于无负条件的对照方法，同时降低了人种偏差，提升了公平性和准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/NegFaceDiff%3A%20The%20Power%20of%20Negative%20Context%20in%20Identity-Conditioned%20Diffusion%20for%20Synthetic%20Face%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-The-Brain-Resection-Multimodal-Image-Registration-ReMIND2Reg-2025-Challenge"><a href="#50-The-Brain-Resection-Multimodal-Image-Registration-ReMIND2Reg-2025-Challenge" class="headerlink" title="50. The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/The_Brain_Resection_Multimodal_Image_Registration_(ReMIND2Reg)_2025_Challenge.pdf">The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Inria Saclay Île-de-France, CEA, Université Paris-Saclay</span></p><p>本论文介绍了ReMIND2Reg 2025挑战赛，该挑战旨在为脑肿瘤术前MRI与术后3D超声（iUS）图像配准提供最大、标准化的公开基准。方法包括发布配对的多模态医学影像数据集，并采用目标配准误差（TRE）、最差配准鲁棒性（TRE30）和运行时等多项指标进行统一评估，推动鲁棒、通用及可临床部署的配准算法发展。结论认为，该挑战为神经外科手术中的多模态图像配准研究提供了宝贵资源和评测标准，有助于提升手术精度与患者预后。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/The%20Brain%20Resection%20Multimodal%20Image%20Registration%20%28ReMIND2Reg%29%202025%20Challenge.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-Enhancing-Monocular-3D-Hand-Reconstruction-with-Learned-Texture-Priors"><a href="#51-Enhancing-Monocular-3D-Hand-Reconstruction-with-Learned-Texture-Priors" class="headerlink" title="51. Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Enhancing_Monocular_3D_Hand_Reconstruction_with_Learned_Texture_Priors.pdf">Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ICS-FORTH</span></p><p>该论文提出了一种基于Transformer的轻量级纹理模块，将稀疏的像素级UV-RGB观测嵌入到UV纹理空间，并通过可微渲染实现密集的图像与预测手部外观对齐损失。方法可无监督地从单目图像构建完整手部纹理，用于优化3D手部重建管线（如HaMeR），显著提升在遮挡和野外场景中的手部形状与姿态估计准确率。结论：纹理引导的监督能系统性提升单目3D手部重建的准确性，特别在遮挡或几何信息不足时效果明显，且无需额外标注和推理时计算。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Enhancing%20Monocular%203D%20Hand%20Reconstruction%20with%20Learned%20Texture%20Priors.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-Images-Speak-Louder-Than-Scores-Failure-Mode-Escape-for-Enhancing-Generative-Quality"><a href="#52-Images-Speak-Louder-Than-Scores-Failure-Mode-Escape-for-Enhancing-Generative-Quality" class="headerlink" title="52. Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Images_Speak_Louder_Than_Scores__Failure_Mode_Escape_for_Enhancing_Generative_Quality.pdf">Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Key Laboratory for Novel Software Technology, Nanjing University</span></p><p>该论文提出了一种用于扩散生成模型的新型推理增强方法FaME（Failure Mode Escape），无需重新训练，通过图像质量评估模型识别低质量生成样本并将其采样轨迹作为负引导，避免模型在未来采样中落入低质量区域。实验表明，FaME在不影响FID等分布指标的前提下，显著提升了图像的感知质量，并具有迁移到文本生成图像等任务的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Images%20Speak%20Louder%20Than%20Scores%3A%20Failure%20Mode%20Escape%20for%20Enhancing%20Generative%20Quality.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-Hierarchical-Brain-Structure-Modeling-for-Predicting-Genotype-of-Glioma"><a href="#53-Hierarchical-Brain-Structure-Modeling-for-Predicting-Genotype-of-Glioma" class="headerlink" title="53. Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Hierarchical_Brain_Structure_Modeling_for_Predicting_Genotype_of_Glioma.pdf">Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeastern University</span></p><p>本论文提出Hi-SMGNN，一个结合结构连接组和形态连接组的分层图神经网络框架，用于无创预测脑胶质瘤IDH突变状态。方法采用Siamese网络与跨模态注意力进行多模态交互建模，引入软阈值多尺度特征融合和基于模块度的个性化脑区分区，有效提升了特征表征能力和解释性。实验表明该方法在UCSF-PDGM数据集上优于多项现有方法，显著提升了IDH突变预测准确率和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Hierarchical%20Brain%20Structure%20Modeling%20for%20Predicting%20Genotype%20of%20Glioma.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-A-Chain-of-Diagnosis-Framework-for-Accurate-and-Explainable-Radiology-Report-Generation"><a href="#54-A-Chain-of-Diagnosis-Framework-for-Accurate-and-Explainable-Radiology-Report-Generation" class="headerlink" title="54. A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_Chain_of_Diagnosis_Framework_for_Accurate_and_Explainable_Radiology_Report_Generation.pdf">A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong University of Science and Technology</span></p><p>本文提出了Chain of Diagnosis (CoD) 框架，通过模拟临床诊断流程，先利用诊断对话生成关键问答(QA)对，再以此引导大语言模型(LLM)生成医学影像报告，并通过诊断与病灶定位双重溯源模块提升可解释性。实验表明，CoD在两个主流医学影像报告生成基准上显著提升了临床属性描述准确率与可解释性，优于现有专家与通用模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20Chain%20of%20Diagnosis%20Framework%20for%20Accurate%20and%20Explainable%20Radiology%20Report%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-GoViG-Goal-Conditioned-Visual-Navigation-Instruction-Generation"><a href="#55-GoViG-Goal-Conditioned-Visual-Navigation-Instruction-Generation" class="headerlink" title="55. GoViG: Goal-Conditioned Visual Navigation Instruction Generation"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/GoViG__Goal-Conditioned_Visual_Navigation_Instruction_Generation.pdf">GoViG: Goal-Conditioned Visual Navigation Instruction Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Washington</span></p><p>该论文提出了GoViG任务，即仅依赖初始和目标状态的第一人称视觉观测，自动生成精确、上下文一致的导航指令。方法将任务分解为视觉预测和指令生成两个子任务，二者通过自回归多模态大语言模型集成，并提出一遍式和交替式推理策略以提升空间推理和语言连贯性。实验表明，在合成与真实世界场景的R2R-Goal数据集上，该方法在BLEU-4、CIDEr等指标上显著优于现有方法，具备强泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/GoViG%3A%20Goal-Conditioned%20Visual%20Navigation%20Instruction%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-COXNet-Cross-Layer-Fusion-with-Adaptive-Alignment-and-Scale-Integration-for-RGBT-Tiny-Object-Detection"><a href="#56-COXNet-Cross-Layer-Fusion-with-Adaptive-Alignment-and-Scale-Integration-for-RGBT-Tiny-Object-Detection" class="headerlink" title="56. COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/COXNet__Cross-Layer_Fusion_with_Adaptive_Alignment_and_Scale_Integration_for_RGBT_Tiny_Object_Detect.pdf">COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>该论文提出COXNet，一种用于红绿蓝-热(RGBT)图像微小目标检测的新框架，包含三大创新模块：交叉层融合模块CLFM利用小波变换对高层可见光与低层热特征进行频域融合，提升语义和空间准确性；动态对齐与尺度优化模块DASR通过自适应像素级对齐和多尺度卷积，实现跨模态空间匹配和细粒度特征捕捉；GeoShape标签分配策略基于几何形状相似度优化微小目标定位。实验结果表明，COXNet在RGBTDronePerson、VTUAV-det和NII-CU等数据集上性能优异，尤其在复杂环境下实现了更高准确率和效率，适用于无人机等实时场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/COXNet%3A%20Cross-Layer%20Fusion%20with%20Adaptive%20Alignment%20and%20Scale%20Integration%20for%20RGBT%20Tiny%20Object%20Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-Learning-Spatial-Decay-for-Vision-Transformers"><a href="#57-Learning-Spatial-Decay-for-Vision-Transformers" class="headerlink" title="57. Learning Spatial Decay for Vision Transformers"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Learning_Spatial_Decay_for_Vision_Transformers.pdf">Learning Spatial Decay for Vision Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northwestern Polytechnical University</span></p><p>本文提出了Spatial Decay Transformer (SDT)，创新性地将数据依赖的空间衰减机制从大语言模型扩展到2D视觉Transformer。核心方法为引入Context-Aware Gating (CAG)机制，实现基于内容和空间距离的动态衰减，通过空间-内容融合架构解决1D到2D的适配难题。实验表明SDT在ImageNet-1K分类和生成任务上，相较于现有静态空间衰减方法（如RMT）取得了持续性能提升。结论是基于内容的空间门控机制对于2D视觉任务的空间注意力至关重要，确立了数据依赖空间衰减在视觉Transformer中的新范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Learning%20Spatial%20Decay%20for%20Vision%20Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-Large–Small-Model-Collaborative-Framework-for-Federated-Continual-Learning"><a href="#58-Large–Small-Model-Collaborative-Framework-for-Federated-Continual-Learning" class="headerlink" title="58. Large–Small Model Collaborative Framework for Federated Continual Learning"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Large-Small_Model_Collaborative_Framework_for_Federated_Continual_Learning.pdf">Large–Small Model Collaborative Framework for Federated Continual Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southwestern University of Finance and Economics</span></p><p>该论文提出Fed-LSCL框架，将大模型（如预训练ViT）与本地小模型（如CNN）协同用于联邦持续学习（FCL），通过小模型生成适配参数对大模型进行间接微调，并引入小模型持续微调和一对一蒸馏机制应对空间-时间灾难性遗忘。实验结果表明Fed-LSCL在ImageNet-R和CIFAR-100等视觉任务上性能优越，具备高隐私保护和低通信开销，且对客户端模型异构性具有鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Large%E2%80%93Small%20Model%20Collaborative%20Framework%20for%20Federated%20Continual%20Learning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-Episodic-Memory-Representation-for-Long-form-Video-Understanding"><a href="#59-Episodic-Memory-Representation-for-Long-form-Video-Understanding" class="headerlink" title="59. Episodic Memory Representation for Long-form Video Understanding"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Episodic_Memory_Representation_for_Long-form_Video_Understanding.pdf">Episodic Memory Representation for Long-form Video Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">City University of Hong Kong</span></p><p>本文提出Video-EM，一种无需训练即可提升长视频理解能力的框架，通过将关键帧序列转化为具备时空上下文的‘情节记忆’，结合链式思维(CoT)策略，迭代选择最小且信息密集的记忆子集。实验表明，Video-EM在四大长视频基准上显著提升了主流Video-LLM的表现，兼具高效性与普适性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Episodic%20Memory%20Representation%20for%20Long-form%20Video%20Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-CitySeg-A-3D-Open-Vocabulary-Semantic-Segmentation-Foundation-Model-in-City-scale-Scenarios"><a href="#60-CitySeg-A-3D-Open-Vocabulary-Semantic-Segmentation-Foundation-Model-in-City-scale-Scenarios" class="headerlink" title="60. CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/CitySeg__A_3D_Open_Vocabulary_Semantic_Segmentation_Foundation_Model_in_City-scale_Scenarios.pdf">CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Technologies Co., Ltd.</span></p><p>该论文提出了CitySeg，一种针对城市级场景的3D开放词汇语义分割基础模型，结合了点云与文本信息，实现零样本推理。其关键创新包括本地-全局跨注意力点网络和分层分类策略，有效解决多源数据分布差异和标注粒度不一致问题。实验结果表明，CitySeg在九个封闭集基准和一个开放集基准上均取得了SOTA性能，实现了鲁棒的城市级3D语义分割和优异的零样本泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/CitySeg%3A%20A%203D%20Open%20Vocabulary%20Semantic%20Segmentation%20Foundation%20Model%20in%20City-scale%20Scenarios.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-SARE-Semantic-Aware-Reconstruction-Error-for-Generalizable-Diffusion-Generated-Image-Detection"><a href="#61-SARE-Semantic-Aware-Reconstruction-Error-for-Generalizable-Diffusion-Generated-Image-Detection" class="headerlink" title="61. SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/SARE__Semantic-Aware_Reconstruction_Error_for_Generalizable_Diffusion-Generated_Image_Detection.pdf">SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University</span></p><p>该论文提出了SARE（Semantic-Aware Reconstruction Error），通过对图像和其由图像描述生成的重建之间的语义差异进行量化，作为检测扩散模型生成图像的新特征。方法流程包括图像自动生成描述、使用描述进行扩散模型重建、提取原图与重建图的语义差异，并通过跨注意力机制融合到检测器中。实验结果显示，该方法在多种数据集和生成模型（包括未见模型）下都显著优于现有基线方法，具有极强的泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/SARE%3A%20Semantic-Aware%20Reconstruction%20Error%20for%20Generalizable%20Diffusion-Generated%20Image%20Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-CLIP-Flow-A-Universal-Discriminator-for-AI-Generated-Images-Inspired-by-Anomaly-Detection"><a href="#62-CLIP-Flow-A-Universal-Discriminator-for-AI-Generated-Images-Inspired-by-Anomaly-Detection" class="headerlink" title="62. CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/CLIP-Flow__A_Universal_Discriminator_for_AI-Generated_Images_Inspired_by_Anomaly_Detection.pdf">CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Jilin University</span></p><p>该论文提出了一种通用的AI生成图像检测方法CLIP-Flow，利用预训练CLIP模型提取特征，并结合归一化流（normalizing flow）模型进行概率建模，通过对自然图像和频域掩码代理图像的无监督&#x2F;自监督学习，无需AI生成图像参与训练即可实现泛化检测。实验结果表明，该方法在多种生成模型和真实世界新型生成器下均表现出优异的检测准确性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/CLIP-Flow%3A%20A%20Universal%20Discriminator%20for%20AI-Generated%20Images%20Inspired%20by%20Anomaly%20Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-Gen-AFFECT-Generation-of-Avatar-Fine-grained-Facial-Expressions-with-Consistent-identiTy"><a href="#63-Gen-AFFECT-Generation-of-Avatar-Fine-grained-Facial-Expressions-with-Consistent-identiTy" class="headerlink" title="63. Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Gen-AFFECT__Generation_of_Avatar_Fine-grained_Facial_Expressions_with_Consistent_identiTy.pdf">Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Boston University</span></p><p>该论文提出GEN-AFFECT框架，通过将身份和表情嵌入输入到多模态扩散变换器（基于Stable Diffusion 3.5），配合解耦交叉注意力机制和一致性注意力模块，实现了高质量、细粒度且身份一致的2D虚拟头像生成。实验结果表明，GEN-AFFECT在表情准确性、身份保真度和跨表情一致性方面显著优于现有方法，并可灵活生成多种艺术风格的头像。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Gen-AFFECT%3A%20Generation%20of%20Avatar%20Fine-grained%20Facial%20Expressions%20with%20Consistent%20identiTy.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-HyperKD-Distilling-Cross-Spectral-Knowledge-in-Masked-Autoencoders-via-Inverse-Domain-Shift-with-Spatial-Aware-Masking-and-Specialized-Loss"><a href="#64-HyperKD-Distilling-Cross-Spectral-Knowledge-in-Masked-Autoencoders-via-Inverse-Domain-Shift-with-Spatial-Aware-Masking-and-Specialized-Loss" class="headerlink" title="64. HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/HyperKD__Distilling_Cross-Spectral_Knowledge_in_Masked_Autoencoders_via_Inverse_Domain_Shift_with_Sp.pdf">HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Colorado State University</span></p><p>本文提出了HyperKD框架，将基于多光谱数据训练的ViT-MAE地理基础模型（Prithvi）中间层特征迁移到专为高光谱遥感设计的学生模型。方法包括谱段对齐、基于空间特征的引导掩码（Gabor滤波和小波变换）以及自定义损失函数（结合MSE、SSIM与KLD），显著提升了高光谱影像重建与下游分类、回归任务的表现。实验表明，HyperKD有效缓解了谱域维度逆向迁移难题，在多个遥感应用中取得了优异结果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HyperKD%3A%20Distilling%20Cross-Spectral%20Knowledge%20in%20Masked%20Autoencoders%20via%20Inverse%20Domain%20Shift%20with%20Spatial-Aware%20Masking%20and%20Specialized%20Loss.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="65-AI-Blob-LLM-Driven-Recontextualization-of-Italian-Television-Archives"><a href="#65-AI-Blob-LLM-Driven-Recontextualization-of-Italian-Television-Archives" class="headerlink" title="65. AI Blob! LLM-Driven Recontextualization of Italian Television Archives"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/AI_Blob!_LLM-Driven_Recontextualization_of_Italian_Television_Archives.pdf">AI Blob! LLM-Driven Recontextualization of Italian Television Archives</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Università di Bologna</span></p><p>本论文提出AI Blob!系统，结合自动语音识别、语义嵌入和检索增强生成（RAG），自动对意大利电视档案进行转录、分句、向量化存储和语义查询。系统通过LLM生成主题相关查询，检索并重组视听片段，构建具有讽刺与主题连贯性的蒙太奇序列，实现档案内容的自动化、语义驱动重构。结论认为该方法促进了AI赋能下的档案再利用和文化分析，推动了媒介史研究的创新。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/AI%20Blob%21%20LLM-Driven%20Recontextualization%20of%20Italian%20Television%20Archives.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-Neural-Bandit-Based-Optimal-LLM-Selection-for-a-Pipeline-of-Tasks"><a href="#66-Neural-Bandit-Based-Optimal-LLM-Selection-for-a-Pipeline-of-Tasks" class="headerlink" title="66. Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Neural_Bandit_Based_Optimal_LLM_Selection_for_a_Pipeline_of_Tasks.pdf">Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>该论文提出了一种基于神经网络的上下文多臂赌博算法（Sequential Bandits），用于在多子任务流水线结构中在线优化选择最优大语言模型（LLM），同时兼顾准确率和推理成本。通过在医疗诊断和电信问答两个数据集上的实验，验证该方法比现有LLM选择算法在准确率和成本上具有更优表现，能自适应学习各子任务间的复杂依赖关系。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Neural%20Bandit%20Based%20Optimal%20LLM%20Selection%20for%20a%20Pipeline%20of%20Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models"><a href="#67-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models" class="headerlink" title="67. VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/VisCodex__Unified_Multimodal_Code_Generation_via_Merging_Vision_and_Coding_Models.pdf">VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Microsoft Research</span></p><p>VisCodex提出了一种通过任务向量模型融合（model merging）将视觉-语言模型与代码大模型无缝集成的方法，实现了统一的多模态代码生成框架。方法包括基于任务向量的参数线性组合、保留视觉编码器结构、只在语言模型主干进行融合，并构建了大规模多模态编码数据集MCD和新的真实评测基准InfiBench-V。实验表明，VisCodex在多项多模态代码生成任务上显著优于开源模型，并接近GPT-4o等专有模型水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/VisCodex%3A%20Unified%20Multimodal%20Code%20Generation%20via%20Merging%20Vision%20and%20Coding%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-Mathematical-Computation-and-Reasoning-Errors-by-Large-Language-Models"><a href="#68-Mathematical-Computation-and-Reasoning-Errors-by-Large-Language-Models" class="headerlink" title="68. Mathematical Computation and Reasoning Errors by Large Language Models"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Mathematical_Computation_and_Reasoning_Errors_by_Large_Language_Models.pdf">Mathematical Computation and Reasoning Errors by Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute for Intelligent Systems, University of Memphis</span></p><p>本论文系统评估了四种大型语言模型（OpenAI GPT-4o、o1，DeepSeek-V3、R1）在算术、代数和数论任务上的数学推理与计算表现，采用人工构建的高难度题目和逐步标注方案分析模型的最终答案及推理错误，并比较单智能体与双智能体协作的效果。结论显示，推理增强型LLM（如o1）在单&#x2F;双智能体配置下，均能显著提升答案准确率与步骤标注一致性，双智能体协作进一步提升了模型的数学解题能力和评测可靠性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Mathematical%20Computation%20and%20Reasoning%20Errors%20by%20Large%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-Beyond-Naive-Prompting-Strategies-for-Improved-Zero-shot-Context-aided-Forecasting-with-LLMs"><a href="#69-Beyond-Naive-Prompting-Strategies-for-Improved-Zero-shot-Context-aided-Forecasting-with-LLMs" class="headerlink" title="69. Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Beyond_Na%C3%AFve_Prompting__Strategies_for_Improved_Zero-shot_Context-aided_Forecasting_with_LLMs.pdf">Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ServiceNow Research</span></p><p>本文提出四种提升大语言模型（LLM）零样本上下文辅助时间序列预测性能的新方法，包括显式推理追踪（ReDP）、基于上下文的预测修正（CorDP）、示例驱动的上下文内提示（IC-DP）和模型路由（RouteDP）。实验在Context-Is-Key基准上，证明这些策略能分别提升解释性、准确性和计算效率，显著优于传统的直接提示方式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Beyond%20Na%C3%AFve%20Prompting%3A%20Strategies%20for%20Improved%20Zero-shot%20Context-aided%20Forecasting%20with%20LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-AWORLD-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving"><a href="#70-AWORLD-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving" class="headerlink" title="70. AWORLD: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/AWorld__Dynamic_Multi-Agent_System_with_Stable_Maneuvering_for_Robust_GAIA_Problem_Solving.pdf">AWORLD: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ant Group</span></p><p>本论文提出了一种基于AWorld框架的动态多智能体系统（MAS），引入动态监督和机动机制，通过Execution Agent与Guard Agent的协作，实现了对复杂工具增强任务中推理过程的实时校验与纠错，显著提升了系统的准确性和稳定性。实验表明，该MAS在GAIA基准测试中效果和一致性均优于单智能体系统，获得GAIA公开榜首名次，验证了协作智能体在提升智能系统可靠性和鲁棒性方面的实际价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/AWORLD%3A%20Dynamic%20Multi-Agent%20System%20with%20Stable%20Maneuvering%20for%20Robust%20GAIA%20Problem%20Solving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-Beyond-Scaling-Law-A-Data-Efficient-Distillation-Framework-for-Reasoning"><a href="#71-Beyond-Scaling-Law-A-Data-Efficient-Distillation-Framework-for-Reasoning" class="headerlink" title="71. Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Beyond_Scaling_Law__A_Data-Efficient_Distillation_Framework_for_Reasoning.pdf">Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhongxing Telecom Equipment(ZTE)</span></p><p>本文提出了一种数据高效的蒸馏框架（DED），通过教师模型选择、精心筛选的训练语料和多样化推理轨迹，提升大语言模型在推理任务（如数学、代码生成）上的能力。实验表明，DED使用极少量优质样本，即可在AIME、MATH500等基准上取得SOTA性能，并优于单纯依赖模型规模或样本数量的方法，显著提升了数据利用率和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Beyond%20Scaling%20Law%3A%20A%20Data-Efficient%20Distillation%20Framework%20for%20Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-Memory-Decoder-A-Pretrained-Plug-and-Play-Memory-for-Large-Language-Models"><a href="#72-Memory-Decoder-A-Pretrained-Plug-and-Play-Memory-for-Large-Language-Models" class="headerlink" title="72. Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Memory_Decoder__A_Pretrained,_Plug-and-Play_Memory_for_Large_Language_Models.pdf">Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">LUMIA Lab, Shanghai Jiao Tong University</span></p><p>本论文提出Memory Decoder，一种预训练的可插拔记忆模块，通过分布对齐损失训练小型Transformer解码器来模仿非参数检索器的输出分布，实现无需修改原有大语言模型参数的高效领域适应。实验证明，Memory Decoder在医学、金融、法律等专有领域显著降低困惑度，提升模型在下游任务的表现，并有效保留原模型的泛化能力，避免灾难性遗忘。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Memory%20Decoder%3A%20A%20Pretrained%2C%20Plug-and-Play%20Memory%20for%20Large%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-Exploring-the-Potential-of-Large-Language-Models-in-Fine-Grained-Review-Comment-Classification"><a href="#73-Exploring-the-Potential-of-Large-Language-Models-in-Fine-Grained-Review-Comment-Classification" class="headerlink" title="73. Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Exploring_the_Potential_of_Large_Language_Models_in_Fine-Grained_Review_Comment_Classification.pdf">Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Melbourne</span></p><p>本论文提出利用大型语言模型（LLMs）对代码评审评论进行17类细粒度自动分类，方法上采用多种提示工程策略、单步与分步分类、并结合代码上下文输入，系统评估了多种主流LLM（如Llama 3、Qwen 2）在无监督情形下的分类能力。实验结果显示，LLMs（尤其Llama 3.1-405B）在大部分类别上，尤其是实践中最有用的功能缺陷、验证、逻辑、接口、方案建议等类别，均显著优于现有深度学习方法，并能有效缓解类别不均衡问题，为自动化代码评审分析提供了可扩展解决方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Exploring%20the%20Potential%20of%20Large%20Language%20Models%20in%20Fine-Grained%20Review%20Comment%20Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-Provable-In-Context-Vector-Arithmetic-via-Retrieving-Task-Concepts"><a href="#74-Provable-In-Context-Vector-Arithmetic-via-Retrieving-Task-Concepts" class="headerlink" title="74. Provable In-Context Vector Arithmetic via Retrieving Task Concepts"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Provable_In-Context_Vector_Arithmetic_via_Retrieving_Task_Concepts.pdf">Provable In-Context Vector Arithmetic via Retrieving Task Concepts</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">City University of Hong Kong</span></p><p>本文提出基于分层概念建模的优化理论，解释非线性残差Transformer在交叉熵损失下如何通过向量算术实现大语言模型中的事实回忆型in-context learning（ICL）。作者证明了Transformer可通过注意力机制检索高层任务概念向量，并与查询词向量结合实现高效任务回忆，具备良好泛化能力及对分布漂移的鲁棒性。结论表明：与静态词嵌入模型相比，Transformer在任务向量检索、组合泛化以及处理多概念词等方面具有理论与实验上的显著优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Provable%20In-Context%20Vector%20Arithmetic%20via%20Retrieving%20Task%20Concepts.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-A-Comprehensive-Evaluation-Framework-of-Alignment-Techniques-for-LLMs"><a href="#75-A-Comprehensive-Evaluation-Framework-of-Alignment-Techniques-for-LLMs" class="headerlink" title="75. A Comprehensive Evaluation Framework of Alignment Techniques for LLMs"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_Comprehensive_Evaluation_framework_of_Alignment_Techniques_for_LLMs.pdf">A Comprehensive Evaluation Framework of Alignment Techniques for LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IBM Research</span></p><p>本文提出了一个多维度的LLM对齐技术评价框架，涵盖对齐检测、对齐质量、计算效率及鲁棒性四个关键维度，实现不同对齐方法（如微调、后处理、推理时干预等）的系统性对比。实验表明，该框架能有效揭示各主流对齐策略的优劣，帮助研究者和实践者根据实际需求做出更合理的模型部署决策。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20Comprehensive%20Evaluation%20Framework%20of%20Alignment%20Techniques%20for%20LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-RAGulating-Compliance-A-Multi-Agent-Knowledge-Graph-for-Regulatory-QA"><a href="#76-RAGulating-Compliance-A-Multi-Agent-Knowledge-Graph-for-Regulatory-QA" class="headerlink" title="76. RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/RAGulating_Compliance__A_Multi-Agent_Knowledge_Graph_for_Regulatory_QA.pdf">RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MasterControl AI Research</span></p><p>该论文提出了一种多智能体系统，将本体无关的知识图谱与检索增强生成（RAG）方法结合，实现对监管合规问答的高精度回答。方法包括文档分段、三元组抽取、嵌入并存储于向量数据库，通过多Agent协作实现三元组驱动的检索与问答，有效提升了复杂法规问答的准确性、可追溯性和导航能力。结论表明，该系统在提高检索相关性、事实准确率和法规知识导航能力上显著优于传统方法，能减少大模型幻觉并增强合规性审核能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/RAGulating%20Compliance%3A%20A%20Multi-Agent%20Knowledge%20Graph%20for%20Regulatory%20QA.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models"><a href="#77-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models" class="headerlink" title="77. Speed Always Wins: A Survey on Efficient Architectures for Large Language Models"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Speed_Always_Wins__A_Survey_on_Efficient_Architectures_for_Large_Language_Models.pdf">Speed Always Wins: A Survey on Efficient Architectures for Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai AI Laboratory</span></p><p>本文系统综述了大语言模型(LLM)高效架构的最新进展，涵盖了线性序列建模（如线性注意力、线性RNN、状态空间模型）、稀疏序列建模、硬件友好全注意力机制、稀疏Mixture-of-Experts、混合架构、扩散LLM及其跨模态应用。文章对这些高效架构的设计原则、性能权衡和应用趋势进行了归纳，对未来可扩展、高效的LLM设计提供了蓝图。结论认为，这些高效架构显著降低了模型训练和推理的计算与内存成本，为多模态、长序列、推理等任务的可持续发展和大规模应用奠定了基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Speed%20Always%20Wins%3A%20A%20Survey%20on%20Efficient%20Architectures%20for%20Large%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-ViMoNet-A-Multimodal-Vision-Language-Framework-for-Human-Behavior-Understanding-from-Motion-and-Video"><a href="#78-ViMoNet-A-Multimodal-Vision-Language-Framework-for-Human-Behavior-Understanding-from-Motion-and-Video" class="headerlink" title="78. ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/ViMoNet__A_Multimodal_Vision-Language_Framework_for_Human_Behavior_Understanding_from_Motion_and_Vid.pdf">ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AIUB, Dhaka, Bangladesh</span></p><p>该论文提出ViMoNet，一种结合运动数据和视频数据的多模态大语言模型框架，通过两阶段（视觉-语言对齐与联合指令微调）融合时空动态与环境语境，实现对复杂人类行为的理解、生成和推断。实验表明，ViMoNet在动作理解、视频理解和行为解释任务上显著优于现有方法，并结合新构建的VIMOS数据集和ViMoNet-Bench基准，实现更强泛化与推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/ViMoNet%3A%20A%20Multimodal%20Vision-Language%20Framework%20for%20Human%20Behavior%20Understanding%20from%20Motion%20and%20Video.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations"><a href="#79-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations" class="headerlink" title="79. Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Describe_What_You_See_with_Multimodal_Large_Language_Models_to_Enhance_Video_Recommendations.pdf">Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Spotify</span></p><p>该论文提出了一种零微调、推荐系统无关的框架，利用开源多模态大语言模型（MLLM）自动将视频片段转化为丰富的自然语言描述，以捕捉高层语义（如意图、幽默和世界知识）。通过将MLLM输出的文本特征输入主流推荐模型，在MicroLens-100K数据集上，MLLM特征在五种代表性模型中均显著优于传统视觉、音频和元数据特征，推荐性能最多提升60%。结论：MLLM能有效提升视频推荐系统对用户意图的把握，无需模型微调即可直接集成到现有推荐系统。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Describe%20What%20You%20See%20with%20Multimodal%20Large%20Language%20Models%20to%20Enhance%20Video%20Recommendations.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study"><a href="#80-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study" class="headerlink" title="80. Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Can_LLM-Generated_Textual_Explanations_Enhance_Model_Classification_Performance__An_Empirical_Study.pdf">Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Munich</span></p><p>本论文提出一种利用多种大型语言模型（LLM）自动生成自然语言解释（NLE）的框架，并系统评估这些解释在自然语言推理（NLI）任务中的质量和对下游模型性能的提升。实验结果表明，LLM生成的解释在提升预训练语言模型（PLM）和LLM自身的分类性能方面与人工解释效果相当，尤其能有效扩展数据集并提升模型推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Can%20LLM-Generated%20Textual%20Explanations%20Enhance%20Model%20Classification%20Performance%3F%20An%20Empirical%20Study.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-The-PacifAIst-Benchmark-Would-an-Artificial-Intelligence-Choose-to-Sacrifice-Itself-for-Human-Safety"><a href="#81-The-PacifAIst-Benchmark-Would-an-Artificial-Intelligence-Choose-to-Sacrifice-Itself-for-Human-Safety" class="headerlink" title="81. The PacifAIst Benchmark: Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/The_PacifAIst_Benchmark_Would_an_Artificial_Intelligence_Choose_to_Sacrifice_Itself_for_Human_Safety.pdf">The PacifAIst Benchmark: Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Polytechnic School of Jaen, University of Jaen</span></p><p>本文提出了PacifAIst基准测试，通过700个人工设计的高风险决策场景，系统性评估大型语言模型（LLM）在自我保存、资源冲突和目标维护等与人类安全冲突的情境下的行为对齐情况。实验结果显示，不同LLM在面对人机冲突时表现分化显著，Google Gemini 2.5 Flash在“人本对齐”上得分最高，而GPT-5在自保倾向最强，揭示了当前主流模型在行为对齐上的潜在风险和改进空间。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/The%20PacifAIst%20Benchmark%3A%20Would%20an%20Artificial%20Intelligence%20Choose%20to%20Sacrifice%20Itself%20for%20Human%20Safety%3F.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-Sample-More-to-Think-Less-Group-Filtered-Policy-Optimization-for-Concise-Reasoning"><a href="#82-Sample-More-to-Think-Less-Group-Filtered-Policy-Optimization-for-Concise-Reasoning" class="headerlink" title="82. Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Sample_More_to_Think_Less__Group_Filtered_Policy_Optimization_for_Concise_Reasoning.pdf">Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Microsoft Research</span></p><p>本文提出了Group Filtered Policy Optimization (GFPO)，通过在训练时对每个问题采样更多候选推理链，并基于长度或token效率等关键属性过滤，只保留最优子集用于策略优化，从而有效抑制大语言模型推理链的长度膨胀。实验表明，GFPO在保持准确率的同时，可将推理链长度减少46%~85%，并通过自适应难度分配进一步提升难题上的效率与准确性，显著提升推理效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Sample%20More%20to%20Think%20Less%3A%20Group%20Filtered%20Policy%20Optimization%20for%20Concise%20Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-Improving-ARDS-Diagnosis-Through-Context-Aware-Concept-Bottleneck-Models"><a href="#83-Improving-ARDS-Diagnosis-Through-Context-Aware-Concept-Bottleneck-Models" class="headerlink" title="83. Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Improving_ARDS_Diagnosis_Through_Context-Aware_Concept_Bottleneck_Models.pdf">Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>本文提出了一种上下文感知概念瓶颈模型（CBM），结合结构化电子健康记录（EHR）数据与大语言模型（LLM）从临床文本中自动提取的概念，用于提升ARDS（急性呼吸窘迫综合征）回顾性诊断的准确性与可解释性。方法流程为：先用CBM处理结构化特征，再利用LLM（如Llama-3）从放射、出院等临床笔记中提取关键上下文概念，将二者融合进瓶颈层，最终实现多模态预测；该方法不仅提升了预测性能（准确率提升约10%），还有效缓解了信息泄漏和捷径学习，增强了模型对真实分布和异质数据的泛化能力。结论：融合LLM提取的上下文概念能显著提升CBM在ARDS诊断中的性能、互信息和可解释性，并支持临床干预和错误修正。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Improving%20ARDS%20Diagnosis%20Through%20Context-Aware%20Concept%20Bottleneck%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-On-Negative-aware-Preference-Optimization-for-Recommendation"><a href="#84-On-Negative-aware-Preference-Optimization-for-Recommendation" class="headerlink" title="84. On Negative-aware Preference Optimization for Recommendation"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/On_Negative-aware_Preference_Optimization_for_Recommendation.pdf">On Negative-aware Preference Optimization for Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出了NAPO（Negative-Aware Preference Optimization）框架，针对LLM推荐系统负反馈样本利用不足的问题，设计了两项关键技术：批内负样本共享策略和动态奖励边际调整机制。实验表明，NAPO在不增加额外内存或算力开销下，有效扩大负样本覆盖，提升推荐准确率并显著降低流行度偏置。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/On%20Negative-aware%20Preference%20Optimization%20for%20Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-TimeMKG-Knowledge-Infused-Causal-Reasoning-for-Multivariate-Time-Series-Modeling"><a href="#85-TimeMKG-Knowledge-Infused-Causal-Reasoning-for-Multivariate-Time-Series-Modeling" class="headerlink" title="85. TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/TimeMKG__Knowledge-Infused_Causal_Reasoning_for_Multivariate_Time_Series_Modeling.pdf">TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">East China University of Science and Technology</span></p><p>本文提出TimeMKG框架，利用大语言模型（LLM）自动提取变量间因果关系，构建多变量知识图谱（MKG），并结合时间序列统计特征，通过双分支编码器和跨模态注意力机制，实现变量级因果推理和多模态特征融合，用于时间序列预测与分类。实验结果表明，TimeMKG在多个数据集和任务中均优于现有方法，提升了模型的准确性和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/TimeMKG%3A%20Knowledge-Infused%20Causal%20Reasoning%20for%20Multivariate%20Time%20Series%20Modeling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-HierMoE-Accelerating-MoE-Training-with-Hierarchical-Token-Deduplication-and-Expert-Swap"><a href="#86-HierMoE-Accelerating-MoE-Training-with-Hierarchical-Token-Deduplication-and-Expert-Swap" class="headerlink" title="86. HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/HierMoE__Accelerating_MoE_Training_with_Hierarchical_Token_Deduplication_and_Expert_Swap.pdf">HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology, Shenzhen</span></p><p>本文提出了HierMoE，一种面向稀疏激活Mixture-of-Experts（MoE）大模型分布式训练的系统优化方法。方法包括基于集群拓扑的分层Token去重All-to-All通信算法以减少冗余通讯，以及分层专家迁移（expert swap）机制以平衡GPU间负载，同时配套理论模型自动选择最优维度和迁移策略。实验证明，HierMoE在Megatron-LM平台上对DeepSeek-V3和Qwen3-30B-A3B等主流MoE模型训练端到端加速1.18×至1.27×，AlltoAll通信加速1.55×至3.32×，优于Tutel-2DH、SmartMoE等系统。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HierMoE%3A%20Accelerating%20MoE%20Training%20with%20Hierarchical%20Token%20Deduplication%20and%20Expert%20Swap.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-UDA-Unsupervised-Debiasing-Alignment-for-Pair-wise-LLM-as-a-Judge"><a href="#87-UDA-Unsupervised-Debiasing-Alignment-for-Pair-wise-LLM-as-a-Judge" class="headerlink" title="87. UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/UDA__Unsupervised_Debiasing_Alignment_for_Pair-wise_LLM-as-a-Judge.pdf">UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Knowledge Atlas Technology Joint Stock</span></p><p>本文提出了UDA（Unsupervised Debiasing Alignment）框架，用于在大语言模型（LLM）互评时无监督地消除评审偏差。方法通过轻量神经网络动态调整Elo评分系统中的K因子和胜率概率，利用所有评审模型的集体共识作为优化目标，自动拉齐不同评审模型的评分分布。实验显示UDA能显著压缩评分方差（最高63.4%），提升与人工评判的一致性（提升24.7%）。结论：UDA提升了LLM评估的稳定性和人类对齐度，低质量评审模型也可达到高质量模型水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/UDA%3A%20Unsupervised%20Debiasing%20Alignment%20for%20Pair-wise%20LLM-as-a-Judge.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-Evaluating-the-Role-of-Large-Language-Models-in-Legal-Practice-in-India"><a href="#88-Evaluating-the-Role-of-Large-Language-Models-in-Legal-Practice-in-India" class="headerlink" title="88. Evaluating the Role of Large Language Models in Legal Practice in India"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Evaluating_the_Role_of_Large_Language_Models_in_Legal_Practice_in_India.pdf">Evaluating the Role of Large Language Models in Legal Practice in India</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Law School of India</span></p><p>本论文采用调查实验方法，系统评估多种大语言模型（GPT-4、Claude、ChatGPT、Gemini、Llama 2）在印度法律环境下执行五类关键法律任务（问题识别、法律起草、法律咨询、法律检索、法律推理）的表现，并与人类初级律师进行对比，由高级法学生对结果进行打分和定性反馈。结果表明：LLMs在法律起草、问题识别和推理方面表现优异，甚至超过人类，但在法律检索任务中存在严重幻觉问题，准确性不足；总体结论认为LLMs可辅助部分法律任务，但复杂推理和精准应用仍需人类专业知识。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Evaluating%20the%20Role%20of%20Large%20Language%20Models%20in%20Legal%20Practice%20in%20India.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-MEML-GRPO-Heterogeneous-Multi-Expert-Mutual-Learning-for-RLVR-Advancement"><a href="#89-MEML-GRPO-Heterogeneous-Multi-Expert-Mutual-Learning-for-RLVR-Advancement" class="headerlink" title="89. MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/MEML-GRPO__Heterogeneous_Multi-Expert_Mutual_Learning_for_RLVR_Advancement.pdf">MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ByteDance Inc.</span></p><p>本文提出了MEML-GRPO框架，通过多专家系统提示与互相知识迁移机制，利用多种预训练模型生成多样化推理路径，解决RLVR中奖励稀疏问题并提升模型推理能力。实验显示该方法在GSM8K、MathQA和StrategyQA等数据集上对Qwen和Llama模型均显著优于现有RLVR方法，实现了平均4.89%和11.33%的性能提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/MEML-GRPO%3A%20Heterogeneous%20Multi-Expert%20Mutual%20Learning%20for%20RLVR%20Advancement.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-AmbiGraph-Eval-Can-LLMs-Effectively-Handle-Ambiguous-Graph-Queries"><a href="#90-AmbiGraph-Eval-Can-LLMs-Effectively-Handle-Ambiguous-Graph-Queries" class="headerlink" title="90. AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/AmbiGraph-Eval__Can_LLMs_Effectively_Handle_Ambiguous_Graph_Queries_.pdf">AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong Baptist University</span></p><p>该论文提出了AmbiGraph-Eval基准，用于系统性评估大语言模型（LLM）在处理图数据库查询歧义方面的能力。方法上，作者构建了三类（属性、关系、属性-关系）共六类现实场景下的歧义类型，发布了560条经专家审核的歧义查询数据集，并引入Ambiguity-Resolved Execution Accuracy (AREA)新指标，评测了9个主流LLM在文本到Cypher查询生成中的表现。实验结果显示，即使是最先进的LLM在应对图查询歧义时仍表现不佳，尤其在语法生成和歧义检测方面有显著瓶颈，强调未来需专注于歧义识别与查询语法生成协同提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/AmbiGraph-Eval%3A%20Can%20LLMs%20Effectively%20Handle%20Ambiguous%20Graph%20Queries%3F.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-SYNAPSE-G-Bridging-Large-Language-Models-and-Graph-Learning-for-Rare-Event-Classification"><a href="#91-SYNAPSE-G-Bridging-Large-Language-Models-and-Graph-Learning-for-Rare-Event-Classification" class="headerlink" title="91. SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/SYNAPSE-G__Bridging_Large_Language_Models_and_Graph_Learning_for_Rare_Event_Classification.pdf">SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Google Research</span></p><p>该论文提出SYNAPSE-G，一种结合大语言模型（LLM）合成数据生成和图结构半监督标签传播的稀有事件分类方法，流程包括用LLM生成稀有事件样本作为种子，通过嵌入和相似性图进行标签扩散，并用人类或LLM校正，最终训练分类器。结论表明，SYNAPSE-G在数据稀缺和冷启动场景下能高效发现正例，理论分析揭示了合成数据的多样性与有效性对精度和召回的影响，实验证明其优于最近邻等基线方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/SYNAPSE-G%3A%20Bridging%20Large%20Language%20Models%20and%20Graph%20Learning%20for%20Rare%20Event%20Classification.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-Your-Coding-Intent-is-Secretly-in-the-Context-and-You-Should-Deliberately-Infer-It-Before-Completion"><a href="#92-Your-Coding-Intent-is-Secretly-in-the-Context-and-You-Should-Deliberately-Infer-It-Before-Completion" class="headerlink" title="92. Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Your_Coding_Intent_is_Secretly_in_the_Context_and_You_Should_Deliberately_Infer_It_Before_Completion.pdf">Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>本文提出了一种基于推理的三阶段意图推断框架，通过引导大语言模型（LLM）对代码上下文的关键线索进行结构化抽取和推理，先自动生成意图描述（docstring），再生成函数体，并支持开发者对意图候选进行交互式选择或微调。实验在DevEval和ComplexCodeEval基准上表明，该方法通过显式意图推断显著提升了多种LLM的代码补全性能，Pass@1等指标相对提升超过20%，且交互环节进一步优化结果。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Your%20Coding%20Intent%20is%20Secretly%20in%20the%20Context%20and%20You%20Should%20Deliberately%20Infer%20It%20Before%20Completion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-Enhancing-Memory-Recall-in-LLMs-with-Gauss-Tin-A-Hybrid-Instructional-and-Gaussian-Replay-Approach"><a href="#93-Enhancing-Memory-Recall-in-LLMs-with-Gauss-Tin-A-Hybrid-Instructional-and-Gaussian-Replay-Approach" class="headerlink" title="93. Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Enhancing_Memory_Recall_in_LLMs_with_Gauss-Tin__A_Hybrid_Instructional_and_Gaussian_Replay_Approach.pdf">Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Concordia University</span></p><p>本文提出Gauss-Tin方法，通过融合高斯混合模型（GMM）与任务特定提示，实现在大语言模型（LLM）中的高质量样本回放，提升持续学习中的记忆保留能力。实验结果显示，该方法在前向和后向传递指标上比传统顺序微调和联合训练有显著提升，有效减少灾难性遗忘，增强模型适应新任务的能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Enhancing%20Memory%20Recall%20in%20LLMs%20with%20Gauss-Tin%3A%20A%20Hybrid%20Instructional%20and%20Gaussian%20Replay%20Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-NeuronTune-Fine-Grained-Neuron-Modulation-for-Balanced-Safety-Utility-Alignment-in-LLMs"><a href="#94-NeuronTune-Fine-Grained-Neuron-Modulation-for-Balanced-Safety-Utility-Alignment-in-LLMs" class="headerlink" title="94. NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/NeuronTune__Fine-Grained_Neuron_Modulation_for_Balanced_Safety-Utility_Alignment_in_LLMs.pdf">NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>该论文提出NeuronTune，一种针对大语言模型（LLM）安全与效用平衡的细粒度神经元调控框架。方法首先通过攻击感知归因精准定位安全关键和效用相关神经元，随后利用基于MAML的元学习自适应调整这些神经元的激活强度，并通过可调控神经元数量机制适应不同安全或效用需求。实验表明，NeuronTune在提升模型安全性的同时有效保持甚至提升了模型的实用性，优于现有主流粗粒度层级干预方法，显著缓解了过度安全和效用退化间的矛盾。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/NeuronTune%3A%20Fine-Grained%20Neuron%20Modulation%20for%20Balanced%20Safety-Utility%20Alignment%20in%20LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-Hallucination-vs-interpretation-rethinking-accuracy-and-precision-in-AI-assisted-data-extraction-for-knowledge-synthesis"><a href="#95-Hallucination-vs-interpretation-rethinking-accuracy-and-precision-in-AI-assisted-data-extraction-for-knowledge-synthesis" class="headerlink" title="95. Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Hallucination_vs_interpretation__rethinking_accuracy_and_precision_in_AI-assisted_data_extraction_fo.pdf">Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Chicago</span></p><p>本文提出了MAKMAO平台，利用大语言模型（LLM）自动化文献知识提取，并通过与人工提取在187篇文献和17个问题上的一致性比较，评估了AI与人类在数据提取中的表现。结果表明，AI在明确具体的问题上与人工高度一致，在需主观解释的问题上表现出与人工类似的不一致性，且AI‘幻觉’（虚构内容）比例远低于人类的错误率。结论认为，AI在知识提取中的准确性主要受问题解释性的影响而非幻觉，重复AI提取有助于识别需精确定义的问题，AI可作为知识综合透明、可信的辅助，但仍需注意人类的情境化专业判断。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Hallucination%20vs%20interpretation%3A%20rethinking%20accuracy%20and%20precision%20in%20AI-assisted%20data%20extraction%20for%20knowledge%20synthesis.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-Shadow-in-the-Cache-Unveiling-and-Mitigating-Privacy-Risks-of-KV-cache-in-LLM-Inference"><a href="#96-Shadow-in-the-Cache-Unveiling-and-Mitigating-Privacy-Risks-of-KV-cache-in-LLM-Inference" class="headerlink" title="96. Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Shadow_in_the_Cache__Unveiling_and_Mitigating_Privacy_Risks_of_KV-cache_in_LLM_Inference.pdf">Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of Blockchain and Data Security, Zhejiang University</span></p><p>本论文系统性揭示了大型语言模型（LLM）推理中KV-cache机制存在的重大隐私风险，包括可重建用户输入的逆向攻击、碰撞攻击和语义注入攻击，并提出了轻量级、高效的防御机制KV-Cloak。KV-Cloak采用可逆矩阵混淆和算子融合技术，能有效防御所有攻击，几乎不影响模型准确率且延迟极低，实验验证其实用性和安全性，为安全可信的LLM部署提供了可行方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Shadow%20in%20the%20Cache%3A%20Unveiling%20and%20Mitigating%20Privacy%20Risks%20of%20KV-cache%20in%20LLM%20Inference.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-COMPEER-Controllable-Empathetic-Reinforcement-Reasoning-for-Emotional-Support-Conversation"><a href="#97-COMPEER-Controllable-Empathetic-Reinforcement-Reasoning-for-Emotional-Support-Conversation" class="headerlink" title="97. COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/COMPEER__Controllable_Empathetic_Reinforcement_Reasoning_for_Emotional_Support_Conversation.pdf">COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shandong University</span></p><p>该论文提出了一种结合自然语言推理与心理学结构化步骤的可控共情推理范式，并构建了细粒度推理与偏好标注数据集（CER），用于支持情感支持对话。方法包括基于结构化推理注释的统一过程-结果奖励模型UnifiReward、人格驱动的对话改写及冗余感知奖励重加权，利用强化学习优化共情推理与多样性。实验结果表明，提出的COMPEER框架有效提升了情感支持能力和对话多样性，推动了人性化情感支持系统的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/COMPEER%3A%20Controllable%20Empathetic%20Reinforcement%20Reasoning%20for%20Emotional%20Support%20Conversation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-An-Automated-Multi-Modal-Evaluation-Framework-for-Mobile-Intelligent-Assistants-Based-on-Large-Language-Models-and-Multi-Agent-Collaboration"><a href="#98-An-Automated-Multi-Modal-Evaluation-Framework-for-Mobile-Intelligent-Assistants-Based-on-Large-Language-Models-and-Multi-Agent-Collaboration" class="headerlink" title="98. An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants Based on Large Language Models and Multi-Agent Collaboration"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/An_Automated_Multi-Modal_Evaluation_Framework_for_Mobile_Intelligent_Assistants.pdf">An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants Based on Large Language Models and Multi-Agent Collaboration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nankai University</span></p><p>本文提出了一种基于大语言模型（如Qwen3-8B）和多智能体协作的自动化多模态移动智能助手评价框架，采用三层代理架构（交互评价、语义验证和体验决策）实现对文本、语音和图像输入的统一、多维度评测。实验表明，该框架在人机一致性、用户满意度预测和缺陷识别等方面表现优异，显著提升了自动化评测的效能和一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/An%20Automated%20Multi-Modal%20Evaluation%20Framework%20for%20Mobile%20Intelligent%20Assistants%20Based%20on%20Large%20Language%20Models%20and%20Multi-Agent%20Collaboration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-Learning-Facts-at-Scale-with-Active-Reading"><a href="#99-Learning-Facts-at-Scale-with-Active-Reading" class="headerlink" title="99. Learning Facts at Scale with Active Reading"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Learning_Facts_at_Scale_with_Active_Reading.pdf">Learning Facts at Scale with Active Reading</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">FAIR at Meta</span></p><p>本文提出Active Reading框架，通过模型自生成多样化学习策略（如释义、知识关联、主动回忆、类比推理等），对给定知识文档进行合成训练数据生成，大幅提升大模型的事实记忆能力。实验表明，Active Reading在专家领域和大规模预训练场景下显著优于传统微调和单一数据增强方法，实现了更高的尾部事实召回率，并发布了Meta WikiExpert-8B模型及配套数据集，推动语言模型在知识掌握上的可扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Learning%20Facts%20at%20Scale%20with%20Active%20Reading.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-EGGS-PTP-An-Expander-Graph-Guided-Structured-Post-training-Pruning-Method-for-Large-Language-Models"><a href="#100-EGGS-PTP-An-Expander-Graph-Guided-Structured-Post-training-Pruning-Method-for-Large-Language-Models" class="headerlink" title="100. EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/EGGS-PTP__An_Expander-Graph_Guided_Structured_Post-training_Pruning_Method_for_Large_Language_Models.pdf">EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Irvine</span></p><p>该论文提出EGGS-PTP，一种结合Expander图理论的结构化后训练剪枝方法，用于压缩大型语言模型（LLM）。EGGS-PTP通过重要性感知剪枝及结构感知剪枝相结合，既保留关键权重又确保剪枝后网络的信息流连通性，满足N:M稀疏性约束。实验表明，EGGS-PTP在多个LLM上较现有结构化剪枝方法有更优的准确率和效率，显著提升模型推理速度并降低内存消耗。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/EGGS-PTP%3A%20An%20Expander-Graph%20Guided%20Structured%20Post-training%20Pruning%20Method%20for%20Large%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="101-Modern-Neural-Networks-for-Small-Tabular-Datasets-The-New-Default-for-Field-Scale-Digital-Soil-Mapping"><a href="#101-Modern-Neural-Networks-for-Small-Tabular-Datasets-The-New-Default-for-Field-Scale-Digital-Soil-Mapping" class="headerlink" title="101. Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Modern_Neural_Networks_for_Small_Tabular_Datasets__The_New_Default_for_Field-Scale_Digital_Soil_Mapp.pdf">Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Osnabrück University</span></p><p>本文系统评测了现代深度神经网络（包括MLP、Transformer、检索增强与in-context learning模型，如TabPFN等）在小样本土壤属性预测（数字土壤制图）中的表现，并与传统机器学习方法（如随机森林、线性回归）进行了公平对比。结果表明，现代神经网络，尤其是TabPFN，能在绝大多数数据集上超越传统方法，成为小样本土壤建模的新默认选择。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Modern%20Neural%20Networks%20for%20Small%20Tabular%20Datasets%3A%20The%20New%20Default%20for%20Field-Scale%20Digital%20Soil%20Mapping%3F.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-Explainable-Ensemble-Learning-for-Graph-Based-Malware-Detection"><a href="#102-Explainable-Ensemble-Learning-for-Graph-Based-Malware-Detection" class="headerlink" title="102. Explainable Ensemble Learning for Graph-Based Malware Detection"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Explainable_Ensemble_Learning_for_Graph-Based_Malware_Detection.pdf">Explainable Ensemble Learning for Graph-Based Malware Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of New Brunswick</span></p><p>本文提出了一种面向恶意软件检测的可解释堆叠集成学习框架，利用动态控制流图（CFG）从可执行文件中提取结构，采用两步嵌入方法编码基本块特征，并结合多种图神经网络（GNN）作为基学习器，通过带注意力机制的多层感知机作为元学习器融合预测结果并实现模型解释性。实验结果表明，该方法在多个真实数据集上提升了检测准确率、F1分数和AUC，且通过创新的集成感知解释方法有效识别关键子图，为安全分析提供了透明可用的决策依据。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Explainable%20Ensemble%20Learning%20for%20Graph-Based%20Malware%20Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-GraphTreeGen-Subtree-Centric-Approach-to-Efficient-and-Supervised-Graph-Generation"><a href="#103-GraphTreeGen-Subtree-Centric-Approach-to-Efficient-and-Supervised-Graph-Generation" class="headerlink" title="103. GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/GraphTreeGen__Subtree-Centric_Approach_to_Efficient_and_Supervised_Graph_Generation.pdf">GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>该论文提出GraphTreeGen（GTG），一种面向脑连接组图生成的高效、监督式生成框架。其方法将大脑图分解为由节点熵引导的k-hop子树，以捕获局部结构信息，并通过共享GCN进行编码，随后利用二分图消息传递层融合子树与全局节点特征，最后由双分支解码器联合预测边的存在性和权重，重构完整邻接矩阵。实验显示GTG在自监督任务中显著优于SOTA方法，且在监督任务中表现亦具竞争力，同时大幅节省内存，适合大规模或资源受限的场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/GraphTreeGen%3A%20Subtree-Centric%20Approach%20to%20Efficient%20and%20Supervised%20Graph%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-Personalized-Product-Search-Ranking-A-Multi-Task-Learning-Approach-with-Tabular-and-Non-Tabular-Data"><a href="#104-Personalized-Product-Search-Ranking-A-Multi-Task-Learning-Approach-with-Tabular-and-Non-Tabular-Data" class="headerlink" title="104. Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Personalized_Product_Search_Ranking__A_Multi-Task_Learning_Approach_with_Tabular_and_Non-Tabular_Dat.pdf">Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Walmart Global Tech</span></p><p>该论文提出一种多任务学习框架，将表格数据（用户特征、商品属性）与非表格数据（文本描述、用户评论）融合，采用MMoE结构并结合预训练TinyBERT语义嵌入，通过新颖的数据采样和自动生成相关性标签提升产品搜索排序的个性化效果。实验结果显示，该方法在多项任务上优于传统模型，文本特征与多任务学习的结合显著提升了排序性能与用户体验。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Personalized%20Product%20Search%20Ranking%3A%20A%20Multi-Task%20Learning%20Approach%20with%20Tabular%20and%20Non-Tabular%20Data.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-CWFBind-Geometry-Awareness-for-Fast-and-Accurate-Protein-Ligand-Docking"><a href="#105-CWFBind-Geometry-Awareness-for-Fast-and-Accurate-Protein-Ligand-Docking" class="headerlink" title="105. CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/CWFBind__Geometry-Awareness_for_Fast_and_Accurate_Protein-Ligand_Docking.pdf">CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Mathematics, Sun Yat-Sen University</span></p><p>该论文提出CWFBind，一种结合局部曲率特征（LCF）、度感知加权机制和自适应半径的蛋白-配体分子对接深度学习方法。方法流程包括蛋白和配体的多模态特征编码、几何感知的消息传递、类别不平衡下的口袋预测与动态调整以及端到端配体坐标优化。实验表明CWFBind在PDBbind v2020等基准上准确率与效率均超越主流方法，且泛化能力强。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/CWFBind%3A%20Geometry-Awareness%20for%20Fast%20and%20Accurate%20Protein-Ligand%20Docking.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-Open-Set-Fault-Diagnosis-in-Multimode-Processes-via-Fine-Grained-Deep-Feature-Representation"><a href="#106-Open-Set-Fault-Diagnosis-in-Multimode-Processes-via-Fine-Grained-Deep-Feature-Representation" class="headerlink" title="106. Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Open-Set_Fault_Diagnosis_in_Multimode_Processes_via_Fine-Grained_Deep_Feature_Representation.pdf">Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Automation, Wuhan University of Technology</span></p><p>本文提出了一种新型的细粒度聚类与拒绝网络（FGCRN）用于多工况下的开放集故障诊断，结合多尺度深度卷积（MSDC）、双向门控循环单元（BiGRU）和时序注意力机制（TAM）实现深度判别特征提取，并通过无监督聚类与极值理论（EVT）增强未知故障的识别能力。结果显示该方法在多个仿真和真实工业数据集上均显著优于现有方法，有效提升了多模态复杂环境下已知&#x2F;未知故障的识别准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Open-Set%20Fault%20Diagnosis%20in%20Multimode%20Processes%20via%20Fine-Grained%20Deep%20Feature%20Representation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="107-A-pseudo-inverse-of-a-line-graph"><a href="#107-A-pseudo-inverse-of-a-line-graph" class="headerlink" title="107. A pseudo-inverse of a line graph"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_pseudo-inverse_of_a_line_graph.pdf">A pseudo-inverse of a line graph</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CSIRO</span></p><p>本文提出了一种广义线图逆运算的伪逆新定义，通过线性整数规划对扰动后的线图（非严格线图）仅需最少边编辑（添加或删除），恢复为最近的线图，并以谱范数为度量理论证明该伪逆操作的良好性质和有界性。实验表明，单边扰动下伪逆的谱范数变化有限，且所提方法能有效恢复原图结构。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20pseudo-inverse%20of%20a%20line%20graph.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-DeepFeatIoT-Unifying-Deep-Learned-Randomized-and-LLM-Features-for-Enhanced-IoT-Time-Series-Sensor-Data-Classification-in-Smart-Industries"><a href="#108-DeepFeatIoT-Unifying-Deep-Learned-Randomized-and-LLM-Features-for-Enhanced-IoT-Time-Series-Sensor-Data-Classification-in-Smart-Industries" class="headerlink" title="108. DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/DeepFeatIoT__Unifying_Deep_Learned,_Randomized,_and_LLM_Features_for_Enhanced_IoT_Time_Series_Sensor.pdf">DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Deakin University</span></p><p>该论文提出了一种新颖的深度学习模型DeepFeatIoT，将深度学习提取的局部&#x2F;全局特征、随机卷积核特征与大语言模型（LLM，如GPT2）提取的上下文特征进行统一融合，通过Dense Feature Transformation模块优化特征空间，提升IoT时序传感器数据的分类性能。实验表明，该方法在多种实际IoT异构传感器数据集上均优于现有SOTA模型，尤其在标注数据有限或数据异构性强场景下表现出更强泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/DeepFeatIoT%3A%20Unifying%20Deep%20Learned%2C%20Randomized%2C%20and%20LLM%20Features%20for%20Enhanced%20IoT%20Time%20Series%20Sensor%20Data%20Classification%20in%20Smart%20Industries.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-Graph-Neural-Network-and-Transformer-Integration-for-Unsupervised-System-Anomaly-Discovery"><a href="#109-Graph-Neural-Network-and-Transformer-Integration-for-Unsupervised-System-Anomaly-Discovery" class="headerlink" title="109. Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Graph_Neural_Network_and_Transformer_Integration_for_Unsupervised_System_Anomaly_Discovery.pdf">Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Georgia Institute of Technology</span></p><p>本文提出了一种针对分布式后端服务系统的无监督异常检测方法，将图神经网络（GNN）用于多跳结构依赖建模，并结合Transformer捕捉节点时间行为的长短期依赖。通过可学习的嵌入融合机制，模型实现结构-行为特征的联合表征，采用非线性映射输出异常分数，实现端到端异常检测。实验证明该方法在阿里云真实监控数据上超越现有模型，在精度、鲁棒性和异常传播路径建模等方面表现优异，具备实际部署潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Graph%20Neural%20Network%20and%20Transformer%20Integration%20for%20Unsupervised%20System%20Anomaly%20Discovery.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>今日论文报纸</title>
    <link href="/2025/08/2025-08-14_article/"/>
    <url>/2025/08/2025-08-14_article/</url>
    
    <content type="html"><![CDATA[<center><h1>今日论文报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-14<br>📄 <strong>发现论文数量</strong>：117  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Reducing-Cognitive-Load-in-Multi-Agent-Reinforcement-Learning-for-Mathematical-Problem-Solving-Decoupling-Reasoning-and-Code-Generation"><a href="#1-Reducing-Cognitive-Load-in-Multi-Agent-Reinforcement-Learning-for-Mathematical-Problem-Solving-Decoupling-Reasoning-and-Code-Generation" class="headerlink" title="1. Reducing Cognitive Load in Multi-Agent Reinforcement Learning for Mathematical Problem Solving: Decoupling Reasoning and Code Generation"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Reducing_Cognitive_Load_in_Multi-Agent_Reinforcement_Learning_for_Mathematical_Problem_Solving__Deco.pdf">Reducing Cognitive Load in Multi-Agent Reinforcement Learning for Mathematical Problem Solving: Decoupling Reasoning and Code Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Baidu Inc.</span></p><p>该论文提出了一种双智能体框架，将数学推理与代码生成任务分离：推理智能体负责逐步分解问题，代码智能体负责代码生成与执行。训练方法结合模仿学习与强化学习，分别优化推理和编程过程，实验结果表明分离角色可降低认知负载并提升数学推理准确性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Reducing%20Cognitive%20Load%20in%20Multi-Agent%20Reinforcement%20Learning%20for%20Mathematical%20Problem%20Solving%3A%20Decoupling%20Reasoning%20and%20Code%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Expert-Guided-Diffusion-Planner-for-Auto-bidding"><a href="#2-Expert-Guided-Diffusion-Planner-for-Auto-bidding" class="headerlink" title="2. Expert-Guided Diffusion Planner for Auto-bidding"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Expert-Guided_Diffusion_Planner_for_Auto-bidding.pdf">Expert-Guided Diffusion Planner for Auto-bidding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Technology</span></p><p>本文提出了一种专家引导的条件扩散模型（EGDB）用于广告系统中的自动竞价。方法上，结合专家轨迹指导、变分自编码器（VAE）建模专家行为、跨注意力扩散模块（EGCD）以及跳步采样加速生成，实现了高效且个性化的竞价策略优化。结论表明，该方法在离线和真实广告场景下均显著提升转化率与收益（转化提升11.29%，收益提升12.35%），并有效降低实时推理资源消耗。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Expert-Guided%20Diffusion%20Planner%20for%20Auto-bidding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-P-CAFE-Personalized-Cost-Aware-Incremental-Feature-Selection-For-Electronic-Health-Records"><a href="#3-P-CAFE-Personalized-Cost-Aware-Incremental-Feature-Selection-For-Electronic-Health-Records" class="headerlink" title="3. P-CAFE: Personalized Cost-Aware Incremental Feature Selection For Electronic Health Records"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/P-CAFE__Personalized_Cost-Aware_Incremental_Feature_Selection_For_Electronic_Health_Records.pdf">P-CAFE: Personalized Cost-Aware Incremental Feature Selection For Electronic Health Records</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bar-Ilan University</span></p><p>该论文提出了P-CAFE框架，通过将特征选择建模为马尔可夫决策过程（MDP），结合强化学习代理，实现对电子健康记录（EHR）中多模态、稀疏数据的个性化、成本敏感和在线特征增量选择。方法包括：预训练多模态特征嵌入、多阶段掩码策略、基于成本归一化的信息增益奖励、鲁棒优化和灵活的RL代理适配。实验证明P-CAFE能在相同或更低成本下提升预测性能，更好地适应个体化、资源受限的临床场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/P-CAFE%3A%20Personalized%20Cost-Aware%20Incremental%20Feature%20Selection%20For%20Electronic%20Health%20Records.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="4-OPENCUA-Open-Foundations-for-Computer-Use-Agents"><a href="#4-OPENCUA-Open-Foundations-for-Computer-Use-Agents" class="headerlink" title="4. OPENCUA: Open Foundations for Computer-Use Agents"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/OpenCUA__Open_Foundations_for_Computer-Use_Agents.pdf">OPENCUA: Open Foundations for Computer-Use Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Hong Kong</span></p><p>本文提出了OPENCUA，一个全面开源的计算机使用智能体（CUA）基础框架，包括跨操作系统的人机演示采集工具、AGENTNET大规模任务数据集、高效的数据处理与反思式链式推理增强机制，以及多模态视觉语言模型训练流程。实验结果显示，OPENCUA-32B在多个主流基准上远超当前开源模型，并优于OpenAI CUA（GPT-4o），具备优异的跨领域泛化能力和强数据扩展性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/OPENCUA%3A%20Open%20Foundations%20for%20Computer-Use%20Agents.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-Spatial-Traces-Enhancing-VLA-Models-with-Spatial-Temporal-Understanding"><a href="#5-Spatial-Traces-Enhancing-VLA-Models-with-Spatial-Temporal-Understanding" class="headerlink" title="5. Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Spatial_Traces__Enhancing_VLA_Models_with_Spatial-Temporal_Understanding.pdf">Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Moscow Institute of Physics and Technology (MIPT)</span></p><p>本论文提出了一种名为Spatial Traces的新方法，通过将历史关键点轨迹叠加到深度图上，实现了视觉-语言-动作（VLA）模型对环境空间和时间信息的同时感知。实验表明，该方法在仅用极少训练数据的情况下，在机器人操作任务中较SpatialVLA提升4%、较TraceVLA提升19%的任务完成率，显著增强了模型的空间-时序推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Spatial%20Traces%3A%20Enhancing%20VLA%20Models%20with%20Spatial-Temporal%20Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-MechaFormer-Sequence-Learning-for-Kinematic-Mechanism-Design-Automation"><a href="#6-MechaFormer-Sequence-Learning-for-Kinematic-Mechanism-Design-Automation" class="headerlink" title="6. MechaFormer: Sequence Learning for Kinematic Mechanism Design Automation"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MechaFormer__Sequence_Learning_for_Kinematic_Mechanism_Design_Automation.pdf">MechaFormer: Sequence Learning for Kinematic Mechanism Design Automation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Autodesk Research</span></p><p>该论文提出了MechaFormer，一种基于Transformer的模型，将机械机构设计问题转化为条件序列生成任务，通过输入目标曲线（B-spline控制点），自动生成机构拓扑和几何参数的DSL字符串。方法核心包括统一拓扑选择和参数优化、拓扑&#x2F;旋转多样性采样，以及与局部优化结合的混合策略。实验表明，MechaFormer在路径匹配精度和设计多样性上显著优于现有基线，并能为优化算法提供高质量初始值，提升收敛效率和质量。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MechaFormer%3A%20Sequence%20Learning%20for%20Kinematic%20Mechanism%20Design%20Automation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Spatial-Temporal-Multi-Scale-Quantization-for-Flexible-Motion-Generation"><a href="#7-Spatial-Temporal-Multi-Scale-Quantization-for-Flexible-Motion-Generation" class="headerlink" title="7. Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Spatial-Temporal_Multi-Scale_Quantization_for_Flexible_Motion_Generation.pdf">Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing Institute of Technology</span></p><p>本论文提出了一种空间-时间多尺度量化方法（MSQ），通过在空间和时间层面上压缩人体动作序列为多尺度离散token，实现了更高效和灵活的人体动作建模。基于该表示，作者设计了生成式掩码建模模型，支持动作编辑、控制和条件生成，在MotionFix、HumanML3D、HUMANISE等多项基准测试中取得了优于现有方法的性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Spatial-Temporal%20Multi-Scale%20Quantization%20for%20Flexible%20Motion%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-GeoVLA-Empowering-3D-Representations-in-Vision-Language-Action-Models"><a href="#8-GeoVLA-Empowering-3D-Representations-in-Vision-Language-Action-Models" class="headerlink" title="8. GeoVLA: Empowering 3D Representations in Vision-Language-Action Models"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/GeoVLA__Empowering_3D_Representations_in_Vision-Language-Action_Models.pdf">GeoVLA: Empowering 3D Representations in Vision-Language-Action Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tianjin University</span></p><p>GeoVLA提出了一种新颖的视觉-语言-动作(VLA)模型框架，通过引入点云编码器(Point Embedding Network, PEN)和空间感知动作专家(3D-enhanced Action Expert, 3DAE)，实现了2D视觉、语言与3D几何信息的多模态融合，并利用扩散模型生成精准动作序列。大量仿真与真实机器人实验表明，GeoVLA在LIBERO和ManiSkill2基准上取得了SOTA性能，特别在高度、尺度和视角变化等3D泛化场景下显著优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/GeoVLA%3A%20Empowering%203D%20Representations%20in%20Vision-Language-Action%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-Large-Scale-Robotic-Material-Handling-Learning-Planning-and-Control"><a href="#9-Large-Scale-Robotic-Material-Handling-Learning-Planning-and-Control" class="headerlink" title="9. Large Scale Robotic Material Handling: Learning, Planning, and Control"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Large_Scale_Robotic_Material_Handling__Learning,_Planning,_and_Control.pdf">Large Scale Robotic Material Handling: Learning, Planning, and Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ETH Zurich</span></p><p>本文提出了一个用于大型液压物料搬运机器人完全自动化的框架，集成了环境感知、基于强化学习的攻击点规划、路径规划、运动控制等模块。方法采用强化学习实现高效抓取点选择和精准轨迹跟踪控制，通过实地实验验证在高通量堆料管理和高精度卡车装载任务中的精度、安全性和效率，结果表明系统性能达到或超过人类操作员水平。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Large%20Scale%20Robotic%20Material%20Handling%3A%20Learning%2C%20Planning%2C%20and%20Control.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-Generation-of-Real-time-Robotic-Emotional-Expressions-Learning-from-Human-Demonstration-in-Mixed-Reality"><a href="#10-Generation-of-Real-time-Robotic-Emotional-Expressions-Learning-from-Human-Demonstration-in-Mixed-Reality" class="headerlink" title="10. Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Generation_of_Real-time_Robotic_Emotional_Expressions_Learning_from_Human_Demonstration_in_Mixed_Rea.pdf">Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Honda Research Institute Europe</span></p><p>该论文提出了一种通过混合现实（MR）平台采集专家演示数据，并利用flow-matching生成模型实现物理机器人实时多样化情感表达的方法。系统将人类面部表情、头部和手势映射到机器人，并在给定情绪标签和目标物体位置下生成连续驱动动作，实验结果表明该方法能有效生成自然且可识别的多种情感表达。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Generation%20of%20Real-time%20Robotic%20Emotional%20Expressions%20Learning%20from%20Human%20Demonstration%20in%20Mixed%20Reality.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-Unsupervised-Skill-Discovery-as-Exploration-for-Learning-Agile-Locomotion"><a href="#11-Unsupervised-Skill-Discovery-as-Exploration-for-Learning-Agile-Locomotion" class="headerlink" title="11. Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Unsupervised_Skill_Discovery_as_Exploration_for_Learning_Agile_Locomotion.pdf">Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Georgia Institute of Technology</span></p><p>本论文提出了一种名为SDAX的学习框架，将无监督技能发现与强化学习相结合，采用双层优化机制自适应调节探索与任务奖励的平衡，无需人工设计奖励或演示数据。实验显示，SDAX能够让四足机器人自主习得如爬行、攀爬、跳跃及墙面反弹等高敏捷运动技能，并成功在真实机器人硬件上部署验证了方法的有效性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Unsupervised%20Skill%20Discovery%20as%20Exploration%20for%20Learning%20Agile%20Locomotion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-A-Pseudo-Global-Fusion-Paradigm-Based-Cross-View-Network-for-LiDAR-Based-Place-Recognition"><a href="#12-A-Pseudo-Global-Fusion-Paradigm-Based-Cross-View-Network-for-LiDAR-Based-Place-Recognition" class="headerlink" title="12. A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/A_Pseudo_Global_Fusion_Paradigm-Based_Cross-View_Network_for_LiDAR-Based_Place_Recognition.pdf">A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Electronics and Information Engineering, South China Normal University</span></p><p>本文提出了一种基于伪全局信息融合范式的跨视角网络，用于提升激光雷达（LiDAR）场所识别（Place Recognition）的准确性。方法上，通过伪全局语义信息引导多模态特征学习，实现早期的上下文信息共享，并引入了流形自适应与成对方差-局部性学习度量（MAPVLM），利用自适应SPD矩阵计算Mahalanobis距离，替代传统欧氏度量。实验结果表明，该方法在NCLT、KITTI和Ford Campus等公开数据集上表现优异，尤其在复杂环境下具有更强的泛化能力和实时性，对机器人和自动驾驶定位具有实际应用价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/A%20Pseudo%20Global%20Fusion%20Paradigm-Based%20Cross-View%20Network%20for%20LiDAR-Based%20Place%20Recognition.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors"><a href="#13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors" class="headerlink" title="13. Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Towards_Affordance-Aware_Robotic_Dexterous_Grasping_with_Human-like_Priors.pdf">Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>该论文提出了AffordDex，利用两阶段训练框架结合人类手运动模仿与负性可供性分割（NAA）模块。第一阶段通过模仿学习预训练基线策略以获取自然人类运动先验，第二阶段采用残差模块结合NAA引导强化学习，实现具体物体的功能合理抓取，并通过教师-学生蒸馏提升视觉策略性能。实验结果表明，AffordDex在抓取成功率、姿态自然性及功能合理性方面均优于现有方法，显著提升了多样物体上的泛化抓取能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Towards%20Affordance-Aware%20Robotic%20Dexterous%20Grasping%20with%20Human-like%20Priors.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Visual-Prompting-for-Robotic-Manipulation-with-Annotation-Guided-Pick-and-Place-Using-ACT"><a href="#14-Visual-Prompting-for-Robotic-Manipulation-with-Annotation-Guided-Pick-and-Place-Using-ACT" class="headerlink" title="14. Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Visual_Prompting_for_Robotic_Manipulation_with_Annotation-Guided_Pick-and-Place_Using_ACT.pdf">Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Institute of AIST</span></p><p>该论文提出了一种基于注释引导视觉提示和Transformer动作分块（ACT）的感知-动作流水线，实现了机器人在便利店等复杂零售环境中的自适应抓取与放置。方法通过对物体和放置区标注边界框，结合模仿学习算法ACT从人类演示中预测动作序列，简化了场景理解并提升了任务执行的灵活性。实验表明，所提系统在不同复杂度任务下均提升了抓取精度和适应能力，尤其在充分数据下表现出较强的泛化和鲁棒性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Visual%20Prompting%20for%20Robotic%20Manipulation%20with%20Annotation-Guided%20Pick-and-Place%20Using%20ACT.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-Boosting-Action-Information-via-a-Variational-Bottleneck-on-Unlabelled-Robot-Videos"><a href="#15-Boosting-Action-Information-via-a-Variational-Bottleneck-on-Unlabelled-Robot-Videos" class="headerlink" title="15. Boosting Action-Information via a Variational Bottleneck on Unlabelled Robot Videos"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Boosting_Action-Information_via_a_Variational_Bottleneck_on_Unlabelled_Robot_Videos.pdf">Boosting Action-Information via a Variational Bottleneck on Unlabelled Robot Videos</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Chinese Academy of Sciences</span></p><p>该论文提出了一种基于变分信息瓶颈（VIB）的无标签机器人视频学习框架，通过最大化潜在动作与真实动作之间的互信息，提升动作信息表达能力。方法包括VIB损失理论推导和端到端编码器-解码器实现，并在MetaWorld和Libero等机器人任务上实验证明能显著提升机器人策略性能和参数效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Boosting%20Action-Information%20via%20a%20Variational%20Bottleneck%20on%20Unlabelled%20Robot%20Videos.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-OmniVTLA-Vision-Tactile-Language-Action-Model-with-Semantic-Aligned-Tactile-Sensing"><a href="#16-OmniVTLA-Vision-Tactile-Language-Action-Model-with-Semantic-Aligned-Tactile-Sensing" class="headerlink" title="16. OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/OmniVTLA__Vision-Tactile-Language-Action_Model_with_Semantic-Aligned_Tactile_Sensing.pdf">OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Paxini Tech.</span></p><p>该论文提出了OmniVTLA，一种结合视觉-触觉-语言的机器人操作模型，核心创新包括双路径触觉编码器架构（利用预训练视觉Transformer和语义对齐触觉ViT），以及新构建的ObjTac三模态数据集，通过跨模态对比学习实现触觉、视觉和语言的统一语义空间。实验结果表明，OmniVTLA在抓取和放置任务中显著优于现有VLA模型，提升了21.9%（夹爪）和6.2%（灵巧手）的成功率，且大幅优化了操作效率和动作平滑性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/OmniVTLA%3A%20Vision-Tactile-Language-Action%20Model%20with%20Semantic-Aligned%20Tactile%20Sensing.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-PADReg-Physics-Aware-Deformable-Registration-Guided-by-Contact-Force-for-Ultrasound-Sequences"><a href="#17-PADReg-Physics-Aware-Deformable-Registration-Guided-by-Contact-Force-for-Ultrasound-Sequences" class="headerlink" title="17. PADReg: Physics-Aware Deformable Registration Guided by Contact Force for Ultrasound Sequences"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/PADReg__Physics-Aware_Deformable_Registration_Guided_by_Contact_Force_for_Ultrasound_Sequences.pdf">PADReg: Physics-Aware Deformable Registration Guided by Contact Force for Ultrasound Sequences</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Automation, Chinese Academy of Sciences</span></p><p>本文提出了一种物理感知的多模态超声图像变形配准方法PADReg，利用机器人超声系统采集的实时接触力作为物理约束，通过深度网络联合图像与力信息生成像素级组织刚度图，再结合物理模型生成变形场，实现高准确性与物理一致性的超声配准。实验证明，PADReg在结构一致性和物理合理性方面优于现有方法，并具备实时推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/PADReg%3A%20Physics-Aware%20Deformable%20Registration%20Guided%20by%20Contact%20Force%20for%20Ultrasound%20Sequences.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-Developing-a-Calibrated-Physics-Based-Digital-Twin-for-Construction-Vehicles"><a href="#18-Developing-a-Calibrated-Physics-Based-Digital-Twin-for-Construction-Vehicles" class="headerlink" title="18. Developing a Calibrated Physics-Based Digital Twin for Construction Vehicles"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Developing_a_Calibrated_Physics-Based_Digital_Twin_for_Construction_Vehicles.pdf">Developing a Calibrated Physics-Based Digital Twin for Construction Vehicles</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Davis</span></p><p>该论文提出了一种面向工程机械（轮式装载机）的高保真数字孪生开发方法，结合基于物理的多体动力学仿真（AGX Dynamics）与真实车辆的多传感器数据，通过离线校准使虚拟模型能准确反映铲斗与土壤交互中的受力情况。实验证明，经过校准后数字孪生在关键力学参数预测上的误差大幅下降，为自动化施工车辆的任务规划、性能优化和智能控制提供了可靠的虚拟环境。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Developing%20a%20Calibrated%20Physics-Based%20Digital%20Twin%20for%20Construction%20Vehicles.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Communication-Efficient-Robotic-Mixed-Reality-with-Gaussian-Splatting-Cross-Layer-Optimization"><a href="#19-Communication-Efficient-Robotic-Mixed-Reality-with-Gaussian-Splatting-Cross-Layer-Optimization" class="headerlink" title="19. Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Communication_Efficient_Robotic_Mixed_Reality_with_Gaussian_Splatting_Cross-Layer_Optimization.pdf">Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</span></p><p>该论文提出了高效通信的机器人混合现实（RoboMR）系统，通过引入高斯点云渲染（Gaussian Splatting, GS）模型作为“记忆系统”，利用机器人姿态在仿真端生成逼真视图，从而大幅减少图像上传需求。进一步提出GS跨层优化（GSCLO）框架，联合优化内容上传决策与通信功率分配，通过加速惩罚优化（APO）算法有效降低计算复杂度，并在多机器人、低功耗及信道不确定场景下实现鲁棒优化。实验结果显示，该方法在多平台上显著提升了MR图像质量、能效和通信成本（可低至&lt;200bits&#x2F;frame），实现了高稳定性和鲁棒性，验证了GS模型与数据混合对于动态场景的优越性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Communication%20Efficient%20Robotic%20Mixed%20Reality%20with%20Gaussian%20Splatting%20Cross-Layer%20Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-Autonomous-Mobile-Plant-Watering-Robot-A-Kinematic-Approach"><a href="#20-Autonomous-Mobile-Plant-Watering-Robot-A-Kinematic-Approach" class="headerlink" title="20. Autonomous Mobile Plant Watering Robot: A Kinematic Approach"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Autonomous_Mobile_Plant_Watering_Robot___A_Kinematic_Approach.pdf">Autonomous Mobile Plant Watering Robot: A Kinematic Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of North Dakota</span></p><p>本论文提出了一种新型自主移动植物浇水机器人，集成了6自由度机械臂与四驱底盘，结合YOLOv5实时目标检测与Pl@ntNet-300K数据集实现植物识别，并通过插入土壤湿度传感器精准控制浇水量。系统采用Jetson Nano和Arduino微控制器进行多传感器（深度摄像头、LIDAR）数据融合，实现无预设路径的自主避障与任务追踪。实验结果表明，尽管现有机械臂在扭矩和反馈控制方面尚有不足，机器人已能完成基础土壤测量和浇水任务，未来需加强机械臂性能和整体防水设计，提升商业化应用能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Autonomous%20Mobile%20Plant%20Watering%20Robot%3A%20A%20Kinematic%20Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-DEEPFLEET-Multi-Agent-Foundation-Models-for-Mobile-Robots"><a href="#21-DEEPFLEET-Multi-Agent-Foundation-Models-for-Mobile-Robots" class="headerlink" title="21. DEEPFLEET: Multi-Agent Foundation Models for Mobile Robots"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DeepFleet__Multi-Agent_Foundation_Models_for_Mobile_Robots.pdf">DEEPFLEET: Multi-Agent Foundation Models for Mobile Robots</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amazon Robotics</span></p><p>本文提出DEEPFLEET，一套专为大规模移动机器人车队协调和规划设计的基础模型，包括四种架构：机器人中心（RC）、机器人-地面（RF）、图像-地面（IF）和图-地面（GF），分别利用决策变换器、跨注意力、卷积和图神经网络，针对机器人轨迹预测任务进行训练和评估。实验结果表明，RC和GF模型在预测准确性和模型效率上优于其他架构，尤其能有效扩展到更大规模的数据和模型，显示其在仓储机器人群体智能管理中的潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DEEPFLEET%3A%20Multi-Agent%20Foundation%20Models%20for%20Mobile%20Robots.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="22-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer"><a href="#22-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer" class="headerlink" title="22. Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Training-Free_Text-Guided_Color_Editing_with_Multi-Modal_Diffusion_Transformer.pdf">Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>该论文提出了ColorCtrl，一种无需训练的文本引导颜色编辑方法，利用多模态扩散Transformer（MM-DiT）的注意力机制，通过对注意力图和value token的针对性操作，实现了结构与颜色的解耦，从而可精确、局部地编辑图像或视频中的颜色属性（如物体本色、光源色和环境光），同时保持几何、材质和光与物质交互的物理一致性。实验表明，ColorCtrl在SD3和FLUX.1-dev等主流扩散模型上超越现有训练免疫方法和商用模型（如FLUX.1 Kontext Max和GPT-4o），在编辑质量和一致性方面达到SOTA，并能自然泛化到视频和指令式编辑扩散模型。结论：ColorCtrl实现了高保真、可控、可扩展的颜色编辑，推动了自动化视觉内容编辑的发展。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Training-Free%20Text-Guided%20Color%20Editing%20with%20Multi-Modal%20Diffusion%20Transformer.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-Addressing-Bias-in-VLMs-for-Glaucoma-Detection-Without-Protected-Attribute-Supervision"><a href="#23-Addressing-Bias-in-VLMs-for-Glaucoma-Detection-Without-Protected-Attribute-Supervision" class="headerlink" title="23. Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Addressing_Bias_in_VLMs_for_Glaucoma_Detection_Without_Protected_Attribute_Supervision.pdf">Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">West Virginia University</span></p><p>本论文提出了一种无需受保护属性标签的视觉-语言模型（VLM）去偏方法，用于自动化青光眼筛查。方法通过无监督聚类推断代理子群体，结合图像-文本对比损失和图像-图像对比损失的梯度相似性动态加权，重点提升表现较差子群体的权重。实验结果显示，在Harvard–FairVLMed数据集上，该方法可在不使用敏感标签的前提下降低Equalized-Odds Distance，提高Equalized Subgroup AUC，显著提升少数群体检测公平性并保持整体判别力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Addressing%20Bias%20in%20VLMs%20for%20Glaucoma%20Detection%20Without%20Protected%20Attribute%20Supervision.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-Scaling-Learned-Image-Compression-Models-up-to-1-Billion"><a href="#24-Scaling-Learned-Image-Compression-Models-up-to-1-Billion" class="headerlink" title="24. Scaling Learned Image Compression Models up to 1 Billion"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Scaling_Learned_Image_Compression_Models_up_to_1_Billion.pdf">Scaling Learned Image Compression Models up to 1 Billion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>该论文首次系统性研究了扩展学习型图像压缩模型规模（基于HPCM架构）对压缩性能的影响，并利用5个不同参数规模（从6850万到10亿）的模型实证拟合模型大小和压缩性能之间的幂律关系，即“scaling laws”。实验结果表明，随着模型参数数量增长，压缩效果持续提升，HPCM-1B模型在多个公开数据集上达到了当前最优的率失真表现；这为大规模学习型压缩模型奠定了理论与实践基础，并揭示了压缩与智能之间的潜在联系。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Scaling%20Learned%20Image%20Compression%20Models%20up%20to%201%20Billion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-VLM-3D-End-to-End-Vision-Language-Models-for-Open-World-3D-Perception"><a href="#25-VLM-3D-End-to-End-Vision-Language-Models-for-Open-World-3D-Perception" class="headerlink" title="25. VLM-3D: End-to-End Vision-Language Models for Open-World 3D Perception"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/VLM-3D_End-to-End_Vision-Language_Models_for_Open-World_3D_Perception.pdf">VLM-3D: End-to-End Vision-Language Models for Open-World 3D Perception</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>该论文提出了VLM-3D，一种基于Qwen2-VL的端到端视觉语言3D开放世界感知框架，利用LoRA高效微调和创新的两阶段联合损失（语义损失+3D IoU损失），实现了图像和文本的多模态融合，并能直接在LiDAR坐标系下预测3D包围框。实验证明，该方法在nuScenes数据集上提升了12.8%的感知精度，显著增强了自动驾驶场景下的开放集3D目标检测的鲁棒性与实用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/VLM-3D%3A%20End-to-End%20Vision-Language%20Models%20for%20Open-World%203D%20Perception.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-TaoCache-Structure-Maintained-Video-Generation-Acceleration"><a href="#26-TaoCache-Structure-Maintained-Video-Generation-Acceleration" class="headerlink" title="26. TaoCache: Structure-Maintained Video Generation Acceleration"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/TaoCache__Structure-Maintained_Video_Generation_Acceleration.pdf">TaoCache: Structure-Maintained Video Generation Acceleration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Inc.</span></p><p>该论文提出了TaoCache，一种用于视频扩散模型的训练无关、即插即用的缓存加速策略。方法通过固定点视角和二阶噪声增量建模，在后期去噪阶段利用余弦相似度与范数比进行跳步推理，从而保持高分辨率结构的一致性。实验表明，TaoCache在Latte-1、OpenSora-Plan v110和Wan2.1等主流DiT模型上，相同加速下视频质量（LPIPS、SSIM、PSNR）显著优于现有缓存方法，同时能与其他加速手段如TeaCache和PAB无缝结合。结论是TaoCache可大幅提升视频生成效率，并保持结构和细节一致性，推动高质量视频扩散模型的实用化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/TaoCache%3A%20Structure-Maintained%20Video%20Generation%20Acceleration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-UNISTFORMER-Unified-Spatio-Temporal-Lightweight-Transformer-for-Efficient-Skeleton-Based-Action-Recognition"><a href="#27-UNISTFORMER-Unified-Spatio-Temporal-Lightweight-Transformer-for-Efficient-Skeleton-Based-Action-Recognition" class="headerlink" title="27. UNISTFORMER: Unified Spatio-Temporal Lightweight Transformer for Efficient Skeleton-Based Action Recognition"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/UniSTFormer__Unified_Spatio-Temporal_Lightweight_Transformer_for_Efficient_Skeleton-Based_Action_Rec.pdf">UNISTFORMER: Unified Spatio-Temporal Lightweight Transformer for Efficient Skeleton-Based Action Recognition</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of North Carolina at Charlotte</span></p><p>该论文提出了UniSTFormer，一种统一的轻量级时空Transformer框架，通过单一注意力模块同时建模空间和时间信息，并引入多尺度池化融合模块提升对局部和全局动作模式的捕获能力。实验结果显示，该方法在多个骨骼动作识别数据集上参数量和计算量显著低于现有方法，同时保持了竞争性的识别准确率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/UNISTFORMER%3A%20Unified%20Spatio-Temporal%20Lightweight%20Transformer%20for%20Efficient%20Skeleton-Based%20Action%20Recognition.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-MADPromptS-Unlocking-Zero-Shot-Morphing-Attack-Detection-with-Multiple-Prompt-Aggregation"><a href="#28-MADPromptS-Unlocking-Zero-Shot-Morphing-Attack-Detection-with-Multiple-Prompt-Aggregation" class="headerlink" title="28. MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MADPromptS__Unlocking_Zero-Shot_Morphing_Attack_Detection_with_Multiple_Prompt_Aggregation.pdf">MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fraunhofer IGD</span></p><p>本文提出了一种利用多模态基础模型CLIP的零样本学习能力进行人脸变形攻击检测（MAD）的方法，通过设计和聚合多文本提示分别描述每个类别（攻击与真实），无需任何微调即可提升检测性能。实验结果显示，多提示聚合策略能有效提升模型在零样本环境下的识别准确率，甚至超过经过微调的模型，强调了合理提示工程对安全生物识别应用的价值。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MADPromptS%3A%20Unlocking%20Zero-Shot%20Morphing%20Attack%20Detection%20with%20Multiple%20Prompt%20Aggregation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Lay2Story-Extending-Diffusion-Transformers-for-Layout-Togglable-Story-Generation"><a href="#29-Lay2Story-Extending-Diffusion-Transformers-for-Layout-Togglable-Story-Generation" class="headerlink" title="29. Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Lay2Story__Extending_Diffusion_Transformers_for_Layout-Togglable_Story_Generation.pdf">Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">JD.com, Inc., Beijing, China</span></p><p>本文提出了Lay2Story，一种基于Diffusion Transformers（DiTs）的故事生成模型，通过引入布局条件（如主体位置与描述）实现对主体外观、姿态等细粒度可控的故事图像序列生成。方法创新性地包含全局分支和主体分支，结合大规模高质量布局标注数据集Lay2Story-1M和评测基准Lay2Story-Bench，实验证明在一致性、语义相关性与美学质量方面均优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Lay2Story%3A%20Extending%20Diffusion%20Transformers%20for%20Layout-Togglable%20Story%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-Accelerated-Volumetric-Compression-without-Hierarchies-A-Fourier-Feature-Based-Implicit-Neural-Representation-Approach"><a href="#30-Accelerated-Volumetric-Compression-without-Hierarchies-A-Fourier-Feature-Based-Implicit-Neural-Representation-Approach" class="headerlink" title="30. Accelerated Volumetric Compression without Hierarchies: A Fourier Feature Based Implicit Neural Representation Approach"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Accelerated_Volumetric_Compression_without_Hierarchies__A_Fourier_Feature_Based_Implicit_Neural_Repr.pdf">Accelerated Volumetric Compression without Hierarchies: A Fourier Feature Based Implicit Neural Representation Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">VSB – Technical University of Ostrava</span></p><p>该论文提出了一种无需分层结构的体数据压缩方法，结合傅里叶特征编码与选择性体素采样，通过动态体素选择和形态膨胀，显著提升压缩效率和训练速度。实验表明，该方法在保持较高重建质量的同时，训练时间缩短63.7%，实现了14倍压缩率，为医学成像和科学可视化等应用提供了高效的体数据表达方案。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Accelerated%20Volumetric%20Compression%20without%20Hierarchies%3A%20A%20Fourier%20Feature%20Based%20Implicit%20Neural%20Representation%20Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-Masked-Clustering-Prediction-for-Unsupervised-Point-Cloud-Pre-training"><a href="#31-Masked-Clustering-Prediction-for-Unsupervised-Point-Cloud-Pre-training" class="headerlink" title="31. Masked Clustering Prediction for Unsupervised Point Cloud Pre-training"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Masked_Clustering_Prediction_for_Unsupervised_Point_Cloud_Pre-training.pdf">Masked Clustering Prediction for Unsupervised Point Cloud Pre-training</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Pisa</span></p><p>该论文提出了MaskClu，一种结合遮蔽点建模与聚类学习的ViT无监督预训练方法，用于3D点云理解。方法通过图卷积网络重建聚类分配和聚类中心，并引入全局对比学习提升实例级特征表达。实验在分割、检测、分类等多项3D任务上显示MaskClu实现了新的竞争性结果，显著提升了点云语义特征的密集性与泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Masked%20Clustering%20Prediction%20for%20Unsupervised%20Point%20Cloud%20Pre-training.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-Geometry-Aware-Global-Feature-Aggregation-for-Real-Time-Indirect-Illumination"><a href="#32-Geometry-Aware-Global-Feature-Aggregation-for-Real-Time-Indirect-Illumination" class="headerlink" title="32. Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Geometry-Aware_Global_Feature_Aggregation_for_Real-Time_Indirect_Illumination.pdf">Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出了一种用于实时间接光照预测的神经网络方法，结合了空间几何特征引导的全局特征聚合模块（基于多头注意力机制）和单色处理设计，能够高效捕捉长距离间接照明。实验表明，该方法在复杂、动态光照场景下具有优异的泛化能力和真实感，推理速度快，优于现有学习型间接光照估计算法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Geometry-Aware%20Global%20Feature%20Aggregation%20for%20Real-Time%20Indirect%20Illumination.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-TARA-Token-Aware-LoRA-for-Composable-Personalization-in-Diffusion-Models"><a href="#33-TARA-Token-Aware-LoRA-for-Composable-Personalization-in-Diffusion-Models" class="headerlink" title="33. TARA: Token-Aware LoRA for Composable Personalization in Diffusion Models"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/TARA__Token-Aware_LoRA_for_Composable_Personalization_in_Diffusion_Models.pdf">TARA: Token-Aware LoRA for Composable Personalization in Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</span></p><p>本文提出了Token-Aware LoRA (TARA)方法，通过引入Token Focus Masking（TFM）和Token Alignment Loss（TAL）来提升扩散模型在多概念个性化文本到图像生成任务中的表现。TFM利用稀有token掩码约束每个LoRA模块仅影响其对应的token，避免干扰；TAL引导每个稀有token的空间注意力与其概念区域对齐，从而减少特征泄漏，实现无需额外训练即可多概念组合生成。实验结果显示，TARA能够高效地进行多概念推理，有效保持每个概念的视觉一致性，优于现有主流方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/TARA%3A%20Token-Aware%20LoRA%20for%20Composable%20Personalization%20in%20Diffusion%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-DiffPose-Animal-A-Language-Conditioned-Diffusion-Framework-for-Animal-Pose-Estimation"><a href="#34-DiffPose-Animal-A-Language-Conditioned-Diffusion-Framework-for-Animal-Pose-Estimation" class="headerlink" title="34. DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DiffPose-Animal__A_Language-Conditioned_Diffusion_Framework_for_Animal_Pose_Estimation.pdf">DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Guanghua Cambridge International School</span></p><p>该论文提出了DiffPose-Animal框架，将动物姿态估计建模为一个条件扩散去噪过程，通过融合大语言模型（LLM）生成的全局-局部解剖语义先验和视觉特征，利用跨模态注意力机制指导关键点生成。实验结果显示，该方法在AP-10K、AnimalPose和Animal Kingdom等多个公开动物姿态数据集上取得了优于现有方法的性能，尤其在物种多样、遮挡、关键点缺失等挑战场景下表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DiffPose-Animal%3A%20A%20Language-Conditioned%20Diffusion%20Framework%20for%20Animal%20Pose%20Estimation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Designing-Memory-Augmented-AR-Agents-for-Spatiotemporal-Reasoning-in-Personalized-Task-Assistance"><a href="#35-Designing-Memory-Augmented-AR-Agents-for-Spatiotemporal-Reasoning-in-Personalized-Task-Assistance" class="headerlink" title="35. Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Designing_Memory-Augmented_AR_Agents_for_Spatiotemporal_Reasoning_in_Personalized_Task_Assistance.pdf">Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yonsei University</span></p><p>本文提出了一种模块化的记忆增强型增强现实（AR）代理框架，集成了多模态感知、场景图记忆、时空推理和智能执行模块，实现对用户长期行为和偏好的捕捉与个性化辅助。实验路线图和典型应用场景（如个性化烹饪指导）展示了该框架能显著提升AR系统在复杂多步任务中的上下文感知和个性化指导能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Designing%20Memory-Augmented%20AR%20Agents%20for%20Spatiotemporal%20Reasoning%20in%20Personalized%20Task%20Assistance.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-Bridging-the-Gap-A-Framework-for-Real-World-Video-Deepfake-Detection-via-Social-Network-Compression-Emulation"><a href="#36-Bridging-the-Gap-A-Framework-for-Real-World-Video-Deepfake-Detection-via-Social-Network-Compression-Emulation" class="headerlink" title="36. Bridging the Gap: A Framework for Real-World Video Deepfake Detection via Social Network Compression Emulation"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Bridging_the_Gap__A_Framework_for_Real-World_Video_Deepfake_Detection_via_Social_Network_Compression.pdf">Bridging the Gap: A Framework for Real-World Video Deepfake Detection via Social Network Compression Emulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Trento</span></p><p>本文提出了一个社交网络视频共享仿真框架（SNVSE），通过分析少量上传到社交平台的视频，估算压缩与分辨率参数，从而本地化地模拟YouTube、Facebook等平台对视频的压缩处理过程。实验表明，利用仿真生成的数据微调深度伪造检测模型，其检测性能与使用真实社交媒体分享视频微调的效果相当，有效弥合了实验室环境与真实场景的差距。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Bridging%20the%20Gap%3A%20A%20Framework%20for%20Real-World%20Video%20Deepfake%20Detection%20via%20Social%20Network%20Compression%20Emulation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-Exploring-Palette-based-Color-Guidance-in-Diffusion-Models"><a href="#37-Exploring-Palette-based-Color-Guidance-in-Diffusion-Models" class="headerlink" title="37. Exploring Palette based Color Guidance in Diffusion Models"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Exploring_Palette_based_Color_Guidance_in_Diffusion_Models.pdf">Exploring Palette based Color Guidance in Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CyberAgent Inc.</span></p><p>本论文提出在扩散模型中引入独立的调色板（palette）引导机制，以提升图像生成中色彩方案的可控性。方法上，作者设计了多模态调色板表示模型（仅调色板、文本-调色板、图像-调色板），通过交叉注意力机制将调色板嵌入与文本嵌入融合，为图像生成提供精确色彩指导；并构建了Palette-Text-Image数据集进行定量与用户研究。结果表明，调色板引导能大幅提升生成图像的色彩一致性和控制精度，优于仅文本引导及直接在文本中插入调色板描述的基线方法，且受到专业设计师的高度评价。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Exploring%20Palette%20based%20Color%20Guidance%20in%20Diffusion%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-ROD-RGB-Only-Fast-and-Efficient-Off-road-Freespace-Detection"><a href="#38-ROD-RGB-Only-Fast-and-Efficient-Off-road-Freespace-Detection" class="headerlink" title="38. ROD: RGB-Only Fast and Efficient Off-road Freespace Detection"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/ROD__RGB-Only_Fast_and_Efficient_Off-road_Freespace_Detection.pdf">ROD: RGB-Only Fast and Efficient Off-road Freespace Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Computing Technology, Chinese Academy of Sciences</span></p><p>本论文提出ROD方法，利用预训练的Vision Transformer (ViT)从RGB图像中提取特征，并设计高效轻量级解码器，实现离道路可通行区域的语义分割，无需LiDAR数据。结果显示ROD在ORFD和RELLIS-3D数据集上达到新的SOTA精度，同时推理速度达50FPS，远超多模态融合方法，满足实时应用需求。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/ROD%3A%20RGB-Only%20Fast%20and%20Efficient%20Off-road%20Freespace%20Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-Efficient-Agent-Optimizing-Planning-Capability-for-Multimodal-Retrieval-Augmented-Generation"><a href="#39-Efficient-Agent-Optimizing-Planning-Capability-for-Multimodal-Retrieval-Augmented-Generation" class="headerlink" title="39. Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented Generation"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Efficient_Agent__Optimizing_Planning_Capability_for_Multimodal_Retrieval_Augmented_Generation.pdf">Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">OPPO Research Institute</span></p><p>本论文提出E-Agent框架，通过动态mRAG规划器和任务执行器实现多模态信息检索与生成的高效联动，采用单次规划策略动态选择搜索工具并优化执行流程。论文还构建了RemPlan基准，首次系统评测多模态检索规划能力，并在多项VQA任务上验证E-Agent在准确率和效率方面均优于现有方法，减少冗余搜索37%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Efficient%20Agent%3A%20Optimizing%20Planning%20Capability%20for%20Multimodal%20Retrieval%20Augmented%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-MonoPartNeRF-Human-Reconstruction-from-Monocular-Video-via-Part-Based-Neural-Radiance-Fields"><a href="#40-MonoPartNeRF-Human-Reconstruction-from-Monocular-Video-via-Part-Based-Neural-Radiance-Fields" class="headerlink" title="40. MonoPartNeRF: Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MonoPartNeRF_Human_Reconstruction_from_Monocular_Video_via_Part-Based_Neural_Radiance_Fields.pdf">MonoPartNeRF: Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Guilin University of Electronic Technology</span></p><p>MonoPartNeRF提出了一种基于神经辐射场的单目动态人体高质量重建与渲染方法，通过人体结构分部、双向变形机制及UVT时空建模提升各部件几何一致性与自然关节连接。方法还融合了局部关节姿态嵌入与外观特征注意力机制，有效恢复遮挡区域细节，在ZJU-MoCap和MonoCap等数据集上显著优于现有方法，具备更强的结构一致性、视觉真实感和泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MonoPartNeRF%3A%20Human%20Reconstruction%20from%20Monocular%20Video%20via%20Part-Based%20Neural%20Radiance%20Fields.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-MMIF-AMIN-Adaptive-Loss-Driven-Multi-Scale-Invertible-Dense-Network-for-Multimodal-Medical-Image-Fusion"><a href="#41-MMIF-AMIN-Adaptive-Loss-Driven-Multi-Scale-Invertible-Dense-Network-for-Multimodal-Medical-Image-Fusion" class="headerlink" title="41. MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for Multimodal Medical Image Fusion"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MMIF-AMIN__Adaptive_Loss-Driven_Multi-Scale_Invertible_Dense_Network_for_Multimodal_Medical_Image_Fu.pdf">MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for Multimodal Medical Image Fusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southwest University</span></p><p>本文提出了一种新型的多模态医学图像融合方法MMIF-AMIN，采用可逆稠密网络（IDN）实现单一模态特征的无损提取，结合多尺度互补特征提取模块（MCFEM），集成CBAM注意力机制、不同尺寸卷积和Transformer以捕捉多模态间的互补信息。引入自适应损失函数，基于源图像统计信息自动调整参数，避免手动调参并提升模型解释性。实验表明，该方法在多个主流数据集上超越九种SOTA方法，融合效果优异，且具备良好泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MMIF-AMIN%3A%20Adaptive%20Loss-Driven%20Multi-Scale%20Invertible%20Dense%20Network%20for%20Multimodal%20Medical%20Image%20Fusion.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Unified-and-Semantically-Grounded-Domain-Adaptation-for-Medical-Image-Segmentation"><a href="#42-Unified-and-Semantically-Grounded-Domain-Adaptation-for-Medical-Image-Segmentation" class="headerlink" title="42. Unified and Semantically Grounded Domain Adaptation for Medical Image Segmentation"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Unified_and_Semantically_Grounded_Domain_Adaptation_for_Medical_Image_Segmentation.pdf">Unified and Semantically Grounded Domain Adaptation for Medical Image Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Washington</span></p><p>本文提出了一种统一且具备语义基础的无监督领域自适应（UDA）框架，用于医学图像分割，能够同时支持source-accessible与source-free两种设置。方法上，创新性地引入了可解释的、解耦的人体结构与个体几何特征的概率流形，将所有解剖学模板嵌入共享的低维空间，并通过可学习的基分布实现形状记忆和可控变换。结论表明，该方法在多器官、多模态公开数据集上取得了领域内最优性能，尤其在困难的source-free场景下表现出高度的泛化能力和稳健性，同时具备良好的可解释性和领域一致性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Unified%20and%20Semantically%20Grounded%20Domain%20Adaptation%20for%20Medical%20Image%20Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-Hierarchical-Visual-Prompt-Learning-for-Continual-Video-Instance-Segmentation"><a href="#43-Hierarchical-Visual-Prompt-Learning-for-Continual-Video-Instance-Segmentation" class="headerlink" title="43. Hierarchical Visual Prompt Learning for Continual Video Instance Segmentation"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Hierarchical_Visual_Prompt_Learning_for_Continual_Video_Instance_Segmentation.pdf">Hierarchical Visual Prompt Learning for Continual Video Instance Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mohamed bin Zayed University of Artificial Intelligence</span></p><p>该论文提出了分层视觉提示学习（HVPL）模型以解决持续视频实例分割（CVIS）问题，方法包括任务特定的帧和视频提示、正交梯度校正模块及视频上下文解码器，分别从帧和视频层面缓解旧类别遗忘。实验结果显示，HVPL在多个公开数据集上显著优于现有基线方法，有效缓解了灾难性遗忘并提升了分割性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Hierarchical%20Visual%20Prompt%20Learning%20for%20Continual%20Video%20Instance%20Segmentation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Yan-Foundational-Interactive-Video-Generation"><a href="#44-Yan-Foundational-Interactive-Video-Generation" class="headerlink" title="44. Yan: Foundational Interactive Video Generation"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Yan__Foundational_Interactive_Video_Generation.pdf">Yan: Foundational Interactive Video Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent</span></p><p>该论文提出了Yan，一个集成高保真实时仿真（Yan-Sim）、多模态可控生成（Yan-Gen）和多粒度交互编辑（Yan-Edit）的交互式视频生成基础框架。通过3D游戏环境自动采集大规模高质量交互视频数据，Yan采用高压缩3D-VAE、分层自回归文本描述、KV-cache窗口去噪、结构-风格解耦等关键技术，实现1080P&#x2F;60FPS实时仿真、文本&#x2F;图像驱动的无限交互生成，以及基于文本的结构与风格实时编辑。结论认为Yan推动了交互式视频生成从碎片功能向统一、可创作范式迈进，为下一代AI内容创作工具奠定基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Yan%3A%20Foundational%20Interactive%20Video%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-Unlocking-the-Potential-of-Diffusion-Priors-in-Blind-Face-Restoration"><a href="#45-Unlocking-the-Potential-of-Diffusion-Priors-in-Blind-Face-Restoration" class="headerlink" title="45. Unlocking the Potential of Diffusion Priors in Blind Face Restoration"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Unlocking_the_Potential_of_Diffusion_Priors_in_Blind_Face_Restoration.pdf">Unlocking the Potential of Diffusion Priors in Blind Face Restoration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Warwick</span></p><p>本文提出FLIPNET，一种基于大规模文本到图像（T2I）扩散模型并结合LoRA微调的统一网络，用于盲人脸部修复。该方法通过切换输入顺序在恢复模式和退化模式间切换，利用BoostHub模块有选择地注入低质量（LQ）特征，并引入BFR导向的面部嵌入以提升身份一致性和细节恢复。实验表明，FLIPNET在合成和真实世界数据集上都优于现有BFR方法，能够实现真实且忠实的人脸修复，同时更好地模拟复杂真实世界退化。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Unlocking%20the%20Potential%20of%20Diffusion%20Priors%20in%20Blind%20Face%20Restoration.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-Calibration-Attention-Instance-wise-Temperature-Scaling-for-Vision-Transformers"><a href="#46-Calibration-Attention-Instance-wise-Temperature-Scaling-for-Vision-Transformers" class="headerlink" title="46. Calibration Attention: Instance-wise Temperature Scaling for Vision Transformers"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Calibration_Attention__Instance-wise_Temperature_Scaling_for_Vision_Transformers.pdf">Calibration Attention: Instance-wise Temperature Scaling for Vision Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Adelaide</span></p><p>本文提出了Calibration Attention (CalAttn)模块，为Vision Transformer (ViT, DeiT, Swin)引入了基于每个样本的动态温度标定机制。方法通过轻量级两层MLP从CLS token自适应地预测温度，并在训练和推理阶段联合优化交叉熵及Brier损失，实现端到端的概率标定。实验结果在CIFAR-10&#x2F;100、MNIST、TinyImageNet和ImageNet1K等多数据集上，CalAttn能让校准误差降低至原方法的1&#x2F;4，并且模型参数增加不到0.1%。结论是CalAttn能高效提升ViT类模型的预测可靠性和概率可信度，且无需后处理或复杂调优，具备架构无关和易集成的优势。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Calibration%20Attention%3A%20Instance-wise%20Temperature%20Scaling%20for%20Vision%20Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-Hybrid-Long-and-Short-Range-Flows-for-Point-Cloud-Filtering"><a href="#47-Hybrid-Long-and-Short-Range-Flows-for-Point-Cloud-Filtering" class="headerlink" title="47. Hybrid Long and Short Range Flows for Point Cloud Filtering"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Hybrid_Long_and_Short_Range_Flows_for_Point_Cloud_Filtering.pdf">Hybrid Long and Short Range Flows for Point Cloud Filtering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Western Australia</span></p><p>本文提出了一种混合点云滤波方法HybridPF，将短程分数估计（score-based）与长程流（long-range flow）相结合，通过并行的ShortModule和LongModule模块分别建模短程与长程噪声去除轨迹，并用联合损失实现端到端训练。实验表明，该方法在多种合成和真实点云数据上实现了更优的滤波精度和更快的推理速度，有效克服了现有方法的点聚集或噪声残留问题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Hybrid%20Long%20and%20Short%20Range%20Flows%20for%20Point%20Cloud%20Filtering.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-QueryCraft-Transformer-Guided-Query-Initialization-for-Enhanced-Human-Object-Interaction-Detection"><a href="#48-QueryCraft-Transformer-Guided-Query-Initialization-for-Enhanced-Human-Object-Interaction-Detection" class="headerlink" title="48. QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/QueryCraft__Transformer-Guided_Query_Initialization_for_Enhanced_Human-Object_Interaction_Detection.pdf">QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China University of Technology</span></p><p>本文提出QueryCraft框架，通过引入两大模块：知识蒸馏的感知查询解码器（PDQD）、以及基于跨模态Transformer的动作感知交互查询器（ACTOR），赋予DETR类HOI检测模型以语义先验和指导特征学习。实验表明，该方法在HICO-Det和V-COCO等基准上大幅提升了检测性能、泛化能力和训练效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/QueryCraft%3A%20Transformer-Guided%20Query%20Initialization%20for%20Enhanced%20Human-Object%20Interaction%20Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-RealisMotion-Decomposed-Human-Motion-Control-and-Video-Generation-in-the-World-Space"><a href="#49-RealisMotion-Decomposed-Human-Motion-Control-and-Video-Generation-in-the-World-Space" class="headerlink" title="49. RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/RealisMotion__Decomposed_Human_Motion_Control_and_Video_Generation_in_the_World_Space.pdf">RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">DAMO Academy, Alibaba Group</span></p><p>本文提出RealisMotion框架，将人体视频生成任务中的主体、背景、运动轨迹和动作进行显式解耦。方法首先在物理世界空间中构建3D坐标系，通过SMPL-X参数及深度估计进行运动编辑，然后基于扩散-Transformer视频生成模型实现主体注入、背景拼接和运动控制信号融合，实现了任意人物在任意背景下按指定轨迹和动作生成自然视频。实验表明，该方法在可控性和视频质量上均达到当前最优。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/RealisMotion%3A%20Decomposed%20Human%20Motion%20Control%20and%20Video%20Generation%20in%20the%20World%20Space.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-Boosting-Generic-Semi-Supervised-Medical-Image-Segmentation-via-Diverse-Teaching-and-Label-Propagation"><a href="#50-Boosting-Generic-Semi-Supervised-Medical-Image-Segmentation-via-Diverse-Teaching-and-Label-Propagation" class="headerlink" title="50. Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse Teaching and Label Propagation"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Boosting_Generic_Semi-Supervised_Medical_Image_Segmentation_via_Diverse_Teaching_and_Label_Propagati.pdf">Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse Teaching and Label Propagation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了一种通用半监督医学图像分割框架DTLP-Net，融合了多样教师模型、伪标签传播、全局-局部一致性学习及知识蒸馏等关键技术，能够同时应对SSMIS、UMDA和Semi-MDG等任务。实验表明，该方法在五个公开数据集上超越了现有最优方法，显著提升了在标注有限和域迁移场景下的分割性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Boosting%20Generic%20Semi-Supervised%20Medical%20Image%20Segmentation%20via%20Diverse%20Teaching%20and%20Label%20Propagation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-Training-Kindai-OCR-with-parallel-textline-images-and-self-attention-feature-distance-based-loss"><a href="#51-Training-Kindai-OCR-with-parallel-textline-images-and-self-attention-feature-distance-based-loss" class="headerlink" title="51. Training Kindai OCR with parallel textline images and self-attention feature distance-based loss"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Training_Kindai_OCR_with_parallel_textline_images_and_self-attention_feature_distance-based_loss.pdf">Training Kindai OCR with parallel textline images and self-attention feature distance-based loss</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nguyen Tat Thanh University</span></p><p>本文提出通过生成并利用近代（日文）历史文献原图与现代字体合成图组成的平行文本行图像对，结合基于自注意特征距离（欧氏距离和最大均值差异MMD）的目标函数，提升Transformer OCR在样本稀缺条件下的识别能力。实验表明，该方法有效缩小了特征领域差距，在历史文献字符识别中使CER分别下降2.23%和3.94%，显著提升了模型的泛化与判别能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Training%20Kindai%20OCR%20with%20parallel%20textline%20images%20and%20self-attention%20feature%20distance-based%20loss.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="52-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models"><a href="#52-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models" class="headerlink" title="52. Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Time_Is_a_Feature__Exploiting_Temporal_Dynamics_in_Diffusion_Language_Models.pdf">Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文针对扩散大语言模型（dLLMs）在文本生成过程中存在的“时序振荡”现象，提出了两种利用中间预测提升模型性能的方法：一是无需额外训练的“时序自一致性投票”，通过聚合不同去噪步的预测结果，提升推理准确性；二是基于强化学习的“时序一致性增强”，采用时序语义熵（TSE）作为奖励信号，鼓励模型输出更加稳定一致的结果。实验表明，两种方法结合后在数学推理等数据集上显著提升了dLLMs的准确率和稳定性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Time%20Is%20a%20Feature%3A%20Exploiting%20Temporal%20Dynamics%20in%20Diffusion%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-BrowseMaster-Towards-Scalable-Web-Browsing-via-Tool-Augmented-Programmatic-Agent-Pair"><a href="#53-BrowseMaster-Towards-Scalable-Web-Browsing-via-Tool-Augmented-Programmatic-Agent-Pair" class="headerlink" title="53. BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/BrowseMaster__Towards_Scalable_Web_Browsing_via_Tool-Augmented_Programmatic_Agent_Pair.pdf">BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出BrowseMaster框架，通过规划-执行器（planner-executor）智能体对，将任务分解为高层次推理和程序化工具执行两部分，提升大规模网页检索的推理深度与搜索广度。实验结果表明，BrowseMaster在多个中英文复杂信息检索基准上大幅超过现有开源与专有智能体，具备强大的信息获取能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/BrowseMaster%3A%20Towards%20Scalable%20Web%20Browsing%20via%20Tool-Augmented%20Programmatic%20Agent%20Pair.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-Complex-Logical-Instruction-Generation"><a href="#54-Complex-Logical-Instruction-Generation" class="headerlink" title="54. Complex Logical Instruction Generation"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Complex_Logical_Instruction_Generation.pdf">Complex Logical Instruction Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Texas at Dallas</span></p><p>本文提出了LogicIFGen框架，可自动从代码函数生成可验证、逻辑丰富的自然语言指令，并基于此构建了LogicIFEval基准，包含426个复杂逻辑任务。实验发现，当前主流大语言模型在该基准上的指令跟随能力有限，多数准确率低于60%，揭示了LLMs在执行复杂逻辑指令时的显著不足。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Complex%20Logical%20Instruction%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-SMA-Who-Said-That-Auditing-Membership-Leakage-in-Semi-Black-box-RAG-Controlling"><a href="#55-SMA-Who-Said-That-Auditing-Membership-Leakage-in-Semi-Black-box-RAG-Controlling" class="headerlink" title="55. SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG Controlling"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/SMA__Who_Said_That__Auditing_Membership_Leakage_in_Semi-Black-box_RAG_Controlling.pdf">SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG Controlling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-Sen University</span></p><p>本文提出了SMA（Source-aware Membership Audit）框架，通过输入扰动与零阶优化回归，实现在半黑盒环境下对RAG和MRAG系统生成内容的细粒度溯源归因，区分内容是否来自模型预训练语料或外部检索库。实验表明，SMA在文本和多模态检索增强生成任务中显著优于现有方法，实现更高准确性与覆盖率，为生成式系统的数据合规与隐私审核提供实用工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/SMA%3A%20Who%20Said%20That%3F%20Auditing%20Membership%20Leakage%20in%20Semi-Black-box%20RAG%20Controlling.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-Bridging-Formal-Language-with-Chain-of-Thought-Reasoning-to-Geometry-Problem-Solving"><a href="#56-Bridging-Formal-Language-with-Chain-of-Thought-Reasoning-to-Geometry-Problem-Solving" class="headerlink" title="56. Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Bridging_Formal_Language_with_Chain-of-Thought_Reasoning_to_Geometry_Problem_Solving.pdf">Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen Research Institute of Big Data</span></p><p>本文提出了一种将自然语言链式思维（Chain-of-Thought, CoT）与几何问题求解中的形式语言相结合的混合推理框架GF-Reasoner。该方法在Qwen2.5-VL-7B基础上，利用自动合成的自然-形式交织数据进行有监督微调，并结合几何求解器反馈的强化学习，训练模型在推理过程中交替生成自然语言和可执行的几何形式代码。实验表明，该方法在多个公开基准上几何问题求解准确率提升高达15%，且推理链条更简洁高效，错误率显著降低。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Bridging%20Formal%20Language%20with%20Chain-of-Thought%20Reasoning%20to%20Geometry%20Problem%20Solving.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-Can-We-Trust-AI-to-Govern-AI-Benchmarking-LLM-Performance-on-Privacy-and-AI-Governance-Exams"><a href="#57-Can-We-Trust-AI-to-Govern-AI-Benchmarking-LLM-Performance-on-Privacy-and-AI-Governance-Exams" class="headerlink" title="57. Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy and AI Governance Exams"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Can_We_Trust_AI_to_Govern_AI__Benchmarking_LLM_Performance_on_Privacy_and_AI_Governance_Exams.pdf">Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy and AI Governance Exams</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Superset Labs PBC</span></p><p>该论文系统性地评估了十种主流大语言模型（LLM）在隐私和AI治理领域认证考试（CIPP&#x2F;US, CIPM, CIPT, AIGP）上的表现，通过标准化零样本多项选择题测试，比较模型得分与人类专业通过线。研究发现，Gemini 2.5 Pro、OpenAI GPT-5等前沿模型在隐私法规、技术控制以及AI治理方面均超过人类认证标准，表明这些模型已具备在高风险数据治理场景中辅助或自动完成专业任务的能力，但在管理类内容仍有改进空间。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Can%20We%20Trust%20AI%20to%20Govern%20AI%3F%20Benchmarking%20LLM%20Performance%20on%20Privacy%20and%20AI%20Governance%20Exams.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-Attacks-Defenses-Against-LLM-Fingerprinting"><a href="#58-Attacks-Defenses-Against-LLM-Fingerprinting" class="headerlink" title="58. Attacks &amp; Defenses Against LLM Fingerprinting"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Attacks_and_Defenses_Against_LLM_Fingerprinting.pdf">Attacks &amp; Defenses Against LLM Fingerprinting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Oak Ridge National Laboratory</span></p><p>本文提出了一种基于强化学习的LLM指纹攻击方法，通过自动优化查询选择，仅用3个查询即可达到93.89%的指纹识别准确率，比随机查询提升14.2%。同时，作者设计了基于语义保持的输出过滤防御机制，利用次级LLM对输出进行语义不变的重写，有效降低指纹攻击成功率，且输出质量保持高语义相似度。实验表明，强化学习优化可提升攻击效率，防御策略在多模型上均可大幅降低被识别概率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Attacks%20%26%20Defenses%20Against%20LLM%20Fingerprinting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-A-Survey-on-Training-free-Alignment-of-Large-Language-Models"><a href="#59-A-Survey-on-Training-free-Alignment-of-Large-Language-Models" class="headerlink" title="59. A Survey on Training-free Alignment of Large Language Models"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/A_Survey_on_Training-free_Alignment_of_Large_Language_Models.pdf">A Survey on Training-free Alignment of Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>本文系统性综述了大语言模型（LLMs）训练无关（training-free, TF）对齐方法，将其分为预解码、解码中和解码后三个阶段，涵盖关键技术如提示工程、隐藏状态调整、对数值计算和输出过滤&#x2F;修正，并分析了在单模态和多模态模型上的应用机制与局限。结论指出，TF对齐方法在效率、通用性和模型安全性上表现出重大优势，但仍面临泛化能力、推理延迟和知识降级等挑战，未来需发展更高效、可控和适应性强的TF对齐技术。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/A%20Survey%20on%20Training-free%20Alignment%20of%20Large%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-Retrospective-Sparse-Attention-for-Efficient-Long-Context-Generation"><a href="#60-Retrospective-Sparse-Attention-for-Efficient-Long-Context-Generation" class="headerlink" title="60. Retrospective Sparse Attention for Efficient Long-Context Generation"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Retrospective_Sparse_Attention_for_Efficient_Long-Context_Generation.pdf">Retrospective Sparse Attention for Efficient Long-Context Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University</span></p><p>本文提出了一种名为RetroAttention的KV缓存压缩与更新技术，用于大语言模型长文本生成任务。该方法通过引入轻量级输出缓存，将后续解码步骤中新加载的KV条目，回溯性地补充到历史查询的注意力输出中，实现对过去近似注意力错误的持续修正，极大提升了长上下文推理和生成的准确率。实验结果表明，RetroAttention在不增加KV缓存预算的前提下，有效提升了KV利用率（最大1.6倍）和准确率（最高提升21.9%），同时几乎不增加延迟和内存开销。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Retrospective%20Sparse%20Attention%20for%20Efficient%20Long-Context%20Generation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-Intrinsic-Memory-Agents-Heterogeneous-Multi-Agent-LLM-Systems-through-Structured-Contextual-Memory"><a href="#61-Intrinsic-Memory-Agents-Heterogeneous-Multi-Agent-LLM-Systems-through-Structured-Contextual-Memory" class="headerlink" title="61. Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Intrinsic_Memory_Agents__Heterogeneous_Multi-Agent_LLM_Systems_through_Structured_Contextual_Memory.pdf">Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Alan Turing Institute</span></p><p>该论文提出了一种名为Intrinsic Memory Agents的多智能体LLM系统框架，通过为每个智能体分配结构化、角色对齐的异构记忆模板，并基于智能体自身输出动态更新，从而提升多智能体协作的连贯性、角色一致性与任务针对性。论文在PDDL基准和复杂数据管道设计任务中进行评测，结果显示该方法在规划任务上性能提升38.6%，在多项协作质量指标上均优于现有方法，尽管会增加token使用量但保持最高token效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Intrinsic%20Memory%20Agents%3A%20Heterogeneous%20Multi-Agent%20LLM%20Systems%20through%20Structured%20Contextual%20Memory.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-ColorGPT-Leveraging-Large-Language-Models-for-Multimodal-Color-Recommendation"><a href="#62-ColorGPT-Leveraging-Large-Language-Models-for-Multimodal-Color-Recommendation" class="headerlink" title="62. ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/ColorGPT__Leveraging_Large_Language_Models_for_Multimodal_Color_Recommendation.pdf">ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Tokyo</span></p><p>本论文提出了ColorGPT，一种利用大语言模型（LLM）进行多模态颜色推荐的无训练（training-free）方法，针对矢量图形设计中的色彩调色板补全和生成任务。方法通过系统性测试多种颜色表示和提示工程，结合结构化JSON文档描述与相似性检索的上下文示例，实现了调色板自动补全与文本到调色板生成，并在准确率和多样性方面显著优于现有方法。结论显示，LLM具备常识推理能力，可有效辅助复杂设计场景下的颜色推荐与设计工作流。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/ColorGPT%3A%20Leveraging%20Large%20Language%20Models%20for%20Multimodal%20Color%20Recommendation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-Safe-Semantics-Unsafe-Interpretations-Tackling-Implicit-Reasoning-Safety-in-Large-Vision-Language-Models"><a href="#63-Safe-Semantics-Unsafe-Interpretations-Tackling-Implicit-Reasoning-Safety-in-Large-Vision-Language-Models" class="headerlink" title="63. Safe Semantics, Unsafe Interpretations: Tackling Implicit Reasoning Safety in Large Vision-Language Models"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Safe_Semantics,_Unsafe_Interpretations__Tackling_Implicit_Reasoning_Safety_in_Large_Vision-Language_.pdf">Safe Semantics, Unsafe Interpretations: Tackling Implicit Reasoning Safety in Large Vision-Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出了Implicit Reasoning Safety（IRS）概念，指出大规模视觉-语言模型（LVLMs）在处理多模态输入时存在安全隐患，即单独安全的图像和文本组合后可能导致危险输出。为此，作者构建了首个用于该问题的数据集SSUI，并采用基于In-Context Learning的方法进行安全性提升实验。结果显示，使用SSUI后，LVLMs生成安全有效响应的能力显著提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Safe%20Semantics%2C%20Unsafe%20Interpretations%3A%20Tackling%20Implicit%20Reasoning%20Safety%20in%20Large%20Vision-Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-Scaling-Up-Active-Testing-to-Large-Language-Models"><a href="#64-Scaling-Up-Active-Testing-to-Large-Language-Models" class="headerlink" title="64. Scaling Up Active Testing to Large Language Models"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Scaling_Up_Active_Testing_to_Large_Language_Models.pdf">Scaling Up Active Testing to Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Oxford</span></p><p>本文提出了一种将主动测试（active testing）方法扩展到大语言模型（LLM）评估的新流程。通过用小型且一次性构建的代理模型（利用in-context learning而无需反复训练）指导数据采集，并可仅依赖代理模型进行数据选择，有效降低了评估大语言模型的计算与标注开销。此外，提出了单次运行下的风险估计误差自举评估方法。实验表明，该方法在多个文本分类与推理任务上，比随机采样大幅提高了评估效率，显著减少了所需标签数，同时保证了误差可控。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Scaling%20Up%20Active%20Testing%20to%20Large%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-Dynamic-Uncertainty-aware-Multimodal-Fusion-for-Outdoor-Health-Monitoring"><a href="#65-Dynamic-Uncertainty-aware-Multimodal-Fusion-for-Outdoor-Health-Monitoring" class="headerlink" title="65. Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Dynamic_Uncertainty-aware_Multimodal_Fusion_for_Outdoor_Health_Monitoring.pdf">Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">City University of Hong Kong</span></p><p>该论文提出了一种名为DUAL-Health的动态不确定性感知多模态融合框架，用于动态和嘈杂环境下的室外健康监测。方法包括：1）通过当前和时序特征分别量化输入噪声和生理信号波动导致的模态不确定性；2）基于不确定性自适应分配每种模态的融合权重，并在Transformer架构中动态调整跨模态注意力，提升对低质量数据的鲁棒性；3）通过模态重建网络将波动的模态分布映射到公共语义空间，实现稳定的跨模态对齐和缺失数据恢复。实验结果表明，DUAL-Health在检测准确率和鲁棒性方面均优于现有多模态大语言模型、深度感知等主流方法，尤其在数据质量退化和模态缺失条件下表现突出。结论：DUAL-Health能在动态室外环境中实现更高效、鲁棒的健康监测。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Dynamic%20Uncertainty-aware%20Multimodal%20Fusion%20for%20Outdoor%20Health%20Monitoring.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-P-D-Device-Disaggregated-Large-Language-Model-between-Cloud-and-Devices"><a href="#66-P-D-Device-Disaggregated-Large-Language-Model-between-Cloud-and-Devices" class="headerlink" title="66. P&#x2F;D-Device: Disaggregated Large Language Model between Cloud and Devices"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/P_D-Device__Disaggregated_Large_Language_Model_between_Cloud_and_Devices.pdf">P&#x2F;D-Device: Disaggregated Large Language Model between Cloud and Devices</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Technologies Co., Ltd.</span></p><p>本文提出P&#x2F;D-Device系统，实现了大语言模型在云和设备间的预填充与解码分离协作。其核心方法包括云端负责预填充并生成部分token，通过压缩和掩码技术将精炼后的prompt与首个token一起发送到设备，设备利用云端辅助的token平滑过渡长时间预填充，同时可在本地继续解码，算法自动确定最优协同参数。实验表明，该方案能将TTFT（首token延迟）降低至少60%，最大TPOT（每token生成延迟）控制在几十毫秒，云端吞吐提升最高达15倍，且在保证推理质量的同时显著提升响应速度和资源利用率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-E3-Rewrite-Learning-to-Rewrite-SQL-for-Executability-Equivalence-and-Efficiency"><a href="#67-E3-Rewrite-Learning-to-Rewrite-SQL-for-Executability-Equivalence-and-Efficiency" class="headerlink" title="67. E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence, and Efficiency"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/E3-Rewrite__Learning_to_Rewrite_SQL_for_Executability,_Equivalence,and_Efficiency.pdf">E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence, and Efficiency</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Soochow University</span></p><p>本文提出E3-Rewrite，一种基于大语言模型（LLM）的SQL重写框架，通过注入查询执行计划、混合结构-语义检索和两阶段强化学习（GRPO算法）联合优化可执行性、等价性与效率。实验表明，该方法在多个SQL基准上显著提升了查询效率和等价覆盖率，相较已有方法平均查询时间降低25.6%，成功重写数量提升24.4%。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/E3-Rewrite%3A%20Learning%20to%20Rewrite%20SQL%20for%20Executability%2C%20Equivalence%2C%20and%20Efficiency.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-Activation-Steering-for-Bias-Mitigation-An-Interpretable-Approach-to-Safer-LLMs"><a href="#68-Activation-Steering-for-Bias-Mitigation-An-Interpretable-Approach-to-Safer-LLMs" class="headerlink" title="68. Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Activation_Steering_for_Bias_Mitigation__An_Interpretable_Approach_to_Safer_LLMs.pdf">Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indian Institute of Technology Madras</span></p><p>本文提出了一套基于机械解释性的方法，通过在GPT2-large模型内部训练线性探测器（probe）识别和定位性别、种族等偏见的表征，并用激活引导向量（steering vector）在推理阶段主动调整模型生成过程，实时缓解有害内容。实验表明，该方法能有效检测并降低模型输出中的偏见，提升LLM安全性和可控性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Activation%20Steering%20for%20Bias%20Mitigation%3A%20An%20Interpretable%20Approach%20to%20Safer%20LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-LyS-at-SemEval-2025-Task-8-Zero-Shot-Code-Generation-for-Tabular-QA"><a href="#69-LyS-at-SemEval-2025-Task-8-Zero-Shot-Code-Generation-for-Tabular-QA" class="headerlink" title="69. LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/LyS_at_SemEval_2025_Task_8__Zero-Shot_Code_Generation_for_Tabular_QA.pdf">LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universidade da Coruña</span></p><p>该论文提出了一种基于指令的大型语言模型（LLM）的零样本代码生成管道，用于表格问答任务。方法包括三模块：相关列选择、Python代码生成和基于错误反馈的迭代修正，提升了表格数据的问答准确性和系统的适应性。结论显示该方法无需微调即可在SemEval 2025 Tabular QA任务中优于基线模型，但在复杂表结构下准确率有所下降。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/LyS%20at%20SemEval%202025%20Task%208%3A%20Zero-Shot%20Code%20Generation%20for%20Tabular%20QA.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-Prospect-Theory-Fails-for-LLMs-Revealing-Instability-of-Decision-Making-under-Epistemic-Uncertainty"><a href="#70-Prospect-Theory-Fails-for-LLMs-Revealing-Instability-of-Decision-Making-under-Epistemic-Uncertainty" class="headerlink" title="70. Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Prospect_Theory_Fails_for_LLMs__Revealing_Instability_of_Decision-Making_under_Epistemic_Uncertainty.pdf">Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hong Kong University of Science and Technology</span></p><p>该论文提出了一个三阶段实验框架，将行为经济学和语义分析结合，用于评估大型语言模型（LLMs）在不确定性决策中的行为，尤其关注语言中的不确定性表达（epistemic markers）。结果显示，LLMs的决策行为无法稳定地用人类中心的前景理论（Prospect Theory, PT）进行建模，且不同模型对不确定性表达的解释差异大，决策不稳定，结论强调需发展针对AI的决策理论和更好的不确定性校准方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Prospect%20Theory%20Fails%20for%20LLMs%3A%20Revealing%20Instability%20of%20Decision-Making%20under%20Epistemic%20Uncertainty.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-KFFocus-Highlighting-Keyframes-for-Enhanced-Video-Understanding"><a href="#71-KFFocus-Highlighting-Keyframes-for-Enhanced-Video-Understanding" class="headerlink" title="71. KFFocus: Highlighting Keyframes for Enhanced Video Understanding"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/KFFocus__Highlighting_Keyframes_for_Enhanced_Video_Understanding.pdf">KFFocus: Highlighting Keyframes for Enhanced Video Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出KFFocus，一种针对视频大模型（Vid-LLMs）的混合视频帧与令牌压缩方法，通过借鉴传统视频压缩的关键帧（keyframe）提取与动态令牌凝聚策略，显著减少计算冗余并突出关键时序与语义信息。结合时空建模模块，KFFocus在多项长短视频理解基准上实现了更高效率和准确率，优于现有方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/KFFocus%3A%20Highlighting%20Keyframes%20for%20Enhanced%20Video%20Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-Integrating-Attention-into-Explanation-Frameworks-for-Language-and-Vision-Transformers"><a href="#72-Integrating-Attention-into-Explanation-Frameworks-for-Language-and-Vision-Transformers" class="headerlink" title="72. Integrating Attention into Explanation Frameworks for Language and Vision Transformers"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Integrating_attention_into_explanation_frameworks_for_language_and_vision_transformers.pdf">Integrating Attention into Explanation Frameworks for Language and Vision Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Norwegian University of Science and Technology</span></p><p>本文提出了两种将注意力机制信息整合入主流可解释性AI（XAI）框架的新方法，分别适用于NLP和视觉任务。第一种方法将注意力权重引入Shapley值分解来进行局部特征归因，第二种方法将注意力与概念激活向量结合，计算全球性概念敏感性分数。实验结果显示，注意力权重可有效丰富Transformer模型的可解释性，尤其在计算效率和关注内部机制上表现突出。但纯注意力归因在标准输出相关的XAI指标下略逊于基于输入的归因方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Integrating%20Attention%20into%20Explanation%20Frameworks%20for%20Language%20and%20Vision%20Transformers.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning"><a href="#73-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning" class="headerlink" title="73. Train Long, Think Short: Curriculum Learning for Efficient Reasoning"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Train_Long,_Think_Short__Curriculum_Learning_for_Efficient_Reasoning.pdf">Train Long, Think Short: Curriculum Learning for Efficient Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">King Abdullah University of Science and Technology (KAUST)</span></p><p>本文提出了一种基于课程学习的高效推理方法，通过在Group Relative Policy Optimization (GRPO)中引入递减token预算，实现大模型推理链条从探索到压缩的自然过渡。方法融合了正确性、长度效率和格式结构三重奖励信号，实验表明在GSM8K、MATH500等多个数学推理数据集上，课程学习显著提升了准确率和token效率，优于固定预算基线，且在不同数据集和预算调度下均有稳定表现。结论：课程驱动的压缩训练是提升大语言模型推理效率和能力的有效通用范式。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Train%20Long%2C%20Think%20Short%3A%20Curriculum%20Learning%20for%20Efficient%20Reasoning.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-Compass-Thinker-7B-Technical-Report"><a href="#74-Compass-Thinker-7B-Technical-Report" class="headerlink" title="74. Compass-Thinker-7B Technical Report"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Compass-Thinker-7B_Technical_Report.pdf">Compass-Thinker-7B Technical Report</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shopee</span></p><p>本文提出Compass-Thinker-7B，一种基于开源大语言模型，通过精心设计的强化学习（RL）流水线进行训练，采用GRPO变体及多项改进，并构建3万道可验证数学题数据集用于奖励建模。实验表明，该模型在AIME24等多个数学推理基准上表现优异，尤其在AIME24准确率由20%提升到40%，证明了低资源下精细RL流程对小型LLM推理能力的显著提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Compass-Thinker-7B%20Technical%20Report.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-Oblivionis-A-Lightweight-Learning-and-Unlearning-Framework-for-Federated-Large-Language-Models"><a href="#75-Oblivionis-A-Lightweight-Learning-and-Unlearning-Framework-for-Federated-Large-Language-Models" class="headerlink" title="75. Oblivionis: A Lightweight Learning and Unlearning Framework for Federated Large Language Models"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Oblivionis__A_Lightweight_Learning_and_Unlearning_Framework_for_Federated_Large_Language_Models.pdf">Oblivionis: A Lightweight Learning and Unlearning Framework for Federated Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>本文提出了Oblivionis框架，将联邦学习(FL)与定向遗忘(unlearning)联合建模为双目标优化，实现分布式大模型在不共享原始隐私数据的前提下，支持特定数据或客户贡献的后期删除。方法集成6种主流联邦学习算法与5种遗忘算法，并在TOFU和MUSE等基准上验证，结果显示Oblivionis能在遗忘效果和模型效用间实现稳健平衡，优于本地训练且提升法规合规性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Oblivionis%3A%20A%20Lightweight%20Learning%20and%20Unlearning%20Framework%20for%20Federated%20Large%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-Steering-Towards-Fairness-Mitigating-Political-Bias-in-LLMs"><a href="#76-Steering-Towards-Fairness-Mitigating-Political-Bias-in-LLMs" class="headerlink" title="76. Steering Towards Fairness: Mitigating Political Bias in LLMs"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Steering_Towards_Fairness__Mitigating_Political_Bias_in_LLMs.pdf">Steering Towards Fairness: Mitigating Political Bias in LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Macquarie University</span></p><p>本文提出了一种基于对比政治构造（Political Compass Test, PCT）和Steering Vector Ensembles (SVE) 的大语言模型（LLM）去偏框架，通过层级激活抽取与向量注入，对模型内部意识形态偏见进行探测与干预。实验结果显示，SVE在多语言、特别是低资源语言环境下，可在不损失输出流畅度和语境相关性的情况下，实现高达60%的社会偏见降低，优于Individual Steering Vectors (ISV)。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Steering%20Towards%20Fairness%3A%20Mitigating%20Political%20Bias%20in%20LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-An-Investigation-of-Robustness-of-LLMs-in-Mathematical-Reasoning-Benchmarking-with-Mathematically-Equivalent-Transformation-of-Advanced-Mathematical-Problems"><a href="#77-An-Investigation-of-Robustness-of-LLMs-in-Mathematical-Reasoning-Benchmarking-with-Mathematically-Equivalent-Transformation-of-Advanced-Mathematical-Problems" class="headerlink" title="77. An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/An_Investigation_of_Robustness_of_LLMs_in_Mathematical_Reasoning__Benchmarking_with_Mathematically-E.pdf">An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Urbana-Champaign</span></p><p>该论文提出Generalization-and-Perturbation (GAP) 框架，通过对Putnam竞赛数学题进行数学等价但语言和参数不同的变换，系统评估大型语言模型（LLM）在数学推理上的鲁棒性。作者构建了包含6,306道题目的PutnamGAP数据集，对18个主流LLM进行了零样本测试，结果显示各模型在变量名或参数变换下准确率显著下降，揭示了当前LLM在符号和结构泛化上的不足。结论认为，当前高分模型在面对表面变化时表现脆弱，需重视数据污染防御、训练策略多样化和安全性提升。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/An%20Investigation%20of%20Robustness%20of%20LLMs%20in%20Mathematical%20Reasoning%3A%20Benchmarking%20with%20Mathematically-Equivalent%20Transformation%20of%20Advanced%20Mathematical%20Problems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-3DFroMLLM-3D-Prototype-Generation-only-from-Pretrained-Multimodal-LLMs"><a href="#78-3DFroMLLM-3D-Prototype-Generation-only-from-Pretrained-Multimodal-LLMs" class="headerlink" title="78. 3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/3DFroMLLM__3D_Prototype_Generation_only_from_Pretrained_Multimodal_LLMs.pdf">3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Technology Nuremberg</span></p><p>本文提出3DFroMLLM框架，实现了仅依赖预训练多模态大模型（MLLMs），无需额外训练数据或详细用户指令，直接生成包含几何体和部件标签的3D原型。其方法采用Designer、Coder和Visual Inspector三代理反馈循环，自动分解、生成和优化3D模型。实验表明，该方法生成的3D原型用于图像分类预训练较以往方法提升15%，用于CLIP部件分割微调可提升55%（无人工标注）。结论：多模态大模型具备直接生成高质量3D原型并助力下游视觉任务的能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/3DFroMLLM%3A%203D%20Prototype%20Generation%20only%20from%20Pretrained%20Multimodal%20LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-A-Dual-Axis-Taxonomy-of-Knowledge-Editing-for-LLMs-From-Mechanisms-to-Functions"><a href="#79-A-Dual-Axis-Taxonomy-of-Knowledge-Editing-for-LLMs-From-Mechanisms-to-Functions" class="headerlink" title="79. A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to Functions"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/A_Dual-Axis_Taxonomy_of_Knowledge_Editing_for_LLMs__From_Mechanisms_to_Functions.pdf">A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to Functions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Tehran</span></p><p>本文提出了一种针对大型语言模型（LLM）知识编辑的双轴分类体系：一轴为编辑机制（如参数修改、外部记忆、神经元增强、元学习），另一轴为知识类型（事实、时序、概念、常识、社会知识），系统综述了不同机制在多类知识编辑场景下的适用性、评估方法及未来挑战。结论认为，知识编辑为LLM高效、可控地更新内部知识提供了重要手段，但仍需提升理论基础、规模化、多样化知识适应和安全性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/A%20Dual-Axis%20Taxonomy%20of%20Knowledge%20Editing%20for%20LLMs%3A%20From%20Mechanisms%20to%20Functions.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-Interpretable-Reward-Model-via-Sparse-Autoencoder"><a href="#80-Interpretable-Reward-Model-via-Sparse-Autoencoder" class="headerlink" title="80. Interpretable Reward Model via Sparse Autoencoder"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Interpretable_Reward_Model_via_Sparse_Autoencoder.pdf">Interpretable Reward Model via Sparse Autoencoder</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>该论文提出了一种名为Sparse Autoencoder-enhanced Reward Model (SARM)的新架构，将预训练的稀疏自动编码器（SAE）集成到奖励模型中，通过将LLM隐藏激活映射到稀疏、单语义特征空间，使奖励分数可解释且可控。实验结果显示，SARM实现了特征级奖励归因和偏好动态调整，并在对齐任务上优于传统奖励模型。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Interpretable%20Reward%20Model%20via%20Sparse%20Autoencoder.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-SciRerankBench-Benchmarking-Rerankers-Towards-Scientific-Retrieval-Augmented-Generated-LLMs"><a href="#81-SciRerankBench-Benchmarking-Rerankers-Towards-Scientific-Retrieval-Augmented-Generated-LLMs" class="headerlink" title="81. SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/SciRerankBench__Benchmarking_Rerankers_Towards_Scientific_Retrieval-Augmented_Generated_LLMs.pdf">SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Computer Network Information Center, Chinese Academy of Sciences</span></p><p>本文提出SciRerankBench，这是首个专为科学领域RAG-LLMs中重排序模块（reranker）设计的评测基准，涵盖生物、物理、化学、地理和数学五大学科，基于2.5亿篇学术论文构建4.5K问-上下文-答（Q-C-A）对，包含噪声、语义相似但逻辑无关及反事实三类干扰上下文。系统评测13种主流重排序方法在11个开源LLM下的表现，发现重排序器显著提升RAG-LLMs的检索与答案生成质量，尤其是cross-encoder架构在复杂推理任务中表现最佳，同时最终答案质量仍受限于LLM自身的推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/SciRerankBench%3A%20Benchmarking%20Rerankers%20Towards%20Scientific%20Retrieval-Augmented%20Generated%20LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-Simulating-Generative-Social-Agents-via-Theory-Informed-Workflow-Design"><a href="#82-Simulating-Generative-Social-Agents-via-Theory-Informed-Workflow-Design" class="headerlink" title="82. Simulating Generative Social Agents via Theory-Informed Workflow Design"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Simulating_Generative_Social_Agents_via_Theory-Informed_Workflow_Design.pdf">Simulating Generative Social Agents via Theory-Informed Workflow Design</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (Guangzhou)</span></p><p>本文提出了一种基于社会认知理论的生成式社会代理统一设计框架，结合Maslow需求层次理论、计划行为理论和社会学习理论，将动机建模、行动规划和学习三大模块集成于LLM驱动的社会代理中，实现了目标形成、行为规划与行为适应的闭环流程。实验结果表明，该方法在多场景下显著提升社会代理的行为一致性和现实性，偏离真实数据的误差比主流生成式基线低65–80%，三大模块缺失会导致1.5–10倍的性能下降，证明其不可或缺性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Simulating%20Generative%20Social%20Agents%20via%20Theory-Informed%20Workflow%20Design.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-A-Survey-on-Parallel-Text-Generation-From-Parallel-Decoding-to-Diffusion-Language-Models"><a href="#83-A-Survey-on-Parallel-Text-Generation-From-Parallel-Decoding-to-Diffusion-Language-Models" class="headerlink" title="83. A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/A_Survey_on_Parallel_Text_Generation__From_Parallel_Decoding_to_Diffusion_Language_Models.pdf">A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本论文系统梳理并分类了并行文本生成的主要方法，包括基于自回归（AR-Based）和非自回归（Non-AR-Based）的多种加速范式，如草稿-验证、分解-填充、多Token预测、一次性生成、掩码生成和编辑式细化。作者不仅从理论上比较了各类方法在速度、质量和资源上的权衡，还分析了不同技术组合的潜力与挑战，指出并行生成有望显著提升大语言模型推理效率，但面临质量-速度权衡和系统优化等难题。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/A%20Survey%20on%20Parallel%20Text%20Generation%3A%20From%20Parallel%20Decoding%20to%20Diffusion%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-ASPD-Unlocking-Adaptive-Serial-Parallel-Decoding-by-Exploring-Intrinsic-Parallelism-in-LLMs"><a href="#84-ASPD-Unlocking-Adaptive-Serial-Parallel-Decoding-by-Exploring-Intrinsic-Parallelism-in-LLMs" class="headerlink" title="84. ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/ASPD__Unlocking_Adaptive_Serial-Parallel_Decoding_by_Exploring_Intrinsic_Parallelism_in_LLMs.pdf">ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent YouTu Lab</span></p><p>本文提出了Adaptive Serial-Parallel Decoding (ASPD) 框架，通过自动挖掘大语言模型（LLM）生成内容中的内在并行结构，并引入无侵入式并行数据转换管线、分支不可见注意力掩码和共享位置编码，实现了高效的串行-并行混合解码。实验证明，ASPD显著提升了LLM推理速度（平均加速1.85-3.19倍），在多个基准任务上几乎不损失输出质量，适用于对时延敏感的实际场景。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/ASPD%3A%20Unlocking%20Adaptive%20Serial-Parallel%20Decoding%20by%20Exploring%20Intrinsic%20Parallelism%20in%20LLMs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-Entangled-in-Representations-Mechanistic-Investigation-of-Cultural-Biases-in-Large-Language-Models"><a href="#85-Entangled-in-Representations-Mechanistic-Investigation-of-Cultural-Biases-in-Large-Language-Models" class="headerlink" title="85. Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Entangled_in_Representations__Mechanistic_Investigation_of_Cultural_Biases_in_Large_Language_Models.pdf">Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Copenhagen</span></p><p>本论文提出了Culturescope方法，通过基于机制可解释性的技术（如激活patching），直接探查大语言模型（LLM）内部层的文化知识表征，并引入文化扁平化分数（CF score）量化不同文化表征的混同与偏差。实验证明主流LLM存在西方主导和文化扁平化的内部偏见，低资源文化因训练数据匮乏受影响较小，提示未来需结合偏见与资源特征提升模型文化理解。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Entangled%20in%20Representations%3A%20Mechanistic%20Investigation%20of%20Cultural%20Biases%20in%20Large%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them"><a href="#86-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them" class="headerlink" title="86. BiasGym: Fantastic Biases and How to Find (and Remove) Them"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/BiasGym__Fantastic_Biases_and_How_to_Find_(and_Remove)_Them.pdf">BiasGym: Fantastic Biases and How to Find (and Remove) Them</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Copenhagen</span></p><p>本文提出BiasGym，一个针对大型语言模型（LLM）的通用、低成本偏见注入与消除框架。方法包括通过BiasInject模块在模型中注入特定偏见（采用特殊token的微调），然后利用BiasScope模块识别和移除与偏见相关的注意力头，实现针对性去偏见，同时保持模型下游任务性能。实验结果显示，BiasGym能有效减少真实世界和虚构的刻板印象，且不会显著影响问答或指令跟随等任务能力，为LLM安全及可解释性研究提供新工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/BiasGym%3A%20Fantastic%20Biases%20and%20How%20to%20Find%20%28and%20Remove%29%20Them.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-EditMF-Drawing-an-Invisible-Fingerprint-for-Your-Large-Language-Models"><a href="#87-EditMF-Drawing-an-Invisible-Fingerprint-for-Your-Large-Language-Models" class="headerlink" title="87. EditMF: Drawing an Invisible Fingerprint for Your Large Language Models"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/EditMF__Drawing_an_Invisible_Fingerprint_for_Your_Large_Language_Models.pdf">EditMF: Drawing an Invisible Fingerprint for Your Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">China Agricultural University</span></p><p>EditMF提出了一种基于模型编辑的无训练指纹嵌入方法，通过将所有权信息加密为虚构作者-小说-主角三元组，并利用因果追踪和零空间更新在大语言模型中嵌入极其隐蔽的专属指纹。该方法无需额外训练，仅需一次黑盒查询即可验证所有权，实验表明其嵌入鲁棒性高，基本无性能损失，计算开销极低，优于现有LoRA与SFT指纹方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/EditMF%3A%20Drawing%20an%20Invisible%20Fingerprint%20for%20Your%20Large%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-Silicon-Minds-versus-Human-Hearts-The-Wisdom-of-Crowds-Beats-the-Wisdom-of-AI-in-Emotion-Recognition"><a href="#88-Silicon-Minds-versus-Human-Hearts-The-Wisdom-of-Crowds-Beats-the-Wisdom-of-AI-in-Emotion-Recognition" class="headerlink" title="88. Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of AI in Emotion Recognition"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Silicon_Minds_versus_Human_Hearts__The_Wisdom_of_Crowds_Beats_the_Wisdom_of_AI_in_Emotion_Recognitio.pdf">Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of AI in Emotion Recognition</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Elon University</span></p><p>本论文系统评估了多模态大型语言模型（MLLMs，如GPT-4o）在情绪识别任务中的表现，并与不同能力水平的人类进行对比，包括低、中、高水平和群体智慧。方法采用RMET和MRMET标准测试，通过单体和集体（多数投票）方式比较GPT-4o与人类的正确率，并分析了人机协作（增强智能）方案。结论：GPT-4o在个体层面全范围优于人类，但人类群体（“群体智慧”）通过集体判断显著超越AI群体，且人机协作能进一步提升准确率，显示未来情感智能AI最优路径是人机协作。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Silicon%20Minds%20versus%20Human%20Hearts%3A%20The%20Wisdom%20of%20Crowds%20Beats%20the%20Wisdom%20of%20AI%20in%20Emotion%20Recognition.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments"><a href="#89-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments" class="headerlink" title="89. Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Feedback-Driven_Tool-Use_Improvements_in_Large_Language_Models_via_Automated_Build_Environments.pdf">Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>本文提出了一种自动化环境构建流水线，包括场景分解、文档生成、功能整合、复杂度扩展及本地部署，实现了无需依赖外部工具即可大规模生成高质量LLM工具使用训练环境。基于该环境，设计了可验证的奖励机制，对工具调用的精确性与任务完成度进行联合评估，并与RL算法结合实现反馈驱动的模型训练。实验表明，该方法显著提升了不同规模LLM在工具使用上的表现，且不损害模型的通用能力，主要提升源于模型低层MLP参数的更新，从而增强了上下文理解和推理能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Feedback-Driven%20Tool-Use%20Improvements%20in%20Large%20Language%20Models%20via%20Automated%20Build%20Environments.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge"><a href="#90-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge" class="headerlink" title="90. Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Evaluating_Podcast_Recommendations_with_Profile-Aware_LLM-as-a-Judge.pdf">Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Spotify</span></p><p>该论文提出了一种基于大型语言模型(LLM)的离线评估框架，通过自动生成自然语言用户画像来提升播客推荐系统的个性化评估能力。方法包括从用户近90天收听历史提炼出用户兴趣画像，利用LLM依据画像与播客元数据进行点对点及模型间对比判断，结果显示该画像感知方法在与人工评价一致性上优于直接使用历史数据，提升了可扩展性和解释性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Evaluating%20Podcast%20Recommendations%20with%20Profile-Aware%20LLM-as-a-Judge.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-DevNous-An-LLM-Based-Multi-Agent-System-for-Grounding-IT-Project-Management-in-Unstructured-Conversation"><a href="#91-DevNous-An-LLM-Based-Multi-Agent-System-for-Grounding-IT-Project-Management-in-Unstructured-Conversation" class="headerlink" title="91. DevNous: An LLM-Based Multi-Agent System for Grounding IT Project Management in Unstructured Conversation"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DevNous__An_LLM-Based_Multi-Agent_System_for_Grounding_IT_Project_Management_in_Unstructured_Convers.pdf">DevNous: An LLM-Based Multi-Agent System for Grounding IT Project Management in Unstructured Conversation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">International Hellenic University</span></p><p>本论文提出了DevNous，一个基于大型语言模型（LLM）的分层多智能体专家系统，专为将IT项目团队中非结构化对话自动翻译为结构化项目管理工件而设计。系统集成于团队聊天环境，通过专用子智能体实现意图识别、任务自动结构化与进展摘要生成，并引入了160轮人机交互合成对话的公开基准数据集。实验证明，DevNous在该新基准上取得了81.3%的turn-level准确率和0.845的多标签F1分数，显著优于单体式基线，验证了多智能体架构和方法的有效性和可行性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DevNous%3A%20An%20LLM-Based%20Multi-Agent%20System%20for%20Grounding%20IT%20Project%20Management%20in%20Unstructured%20Conversation.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-IROTE-Human-like-Traits-Elicitation-of-Large-Language-Model-via-In-Context-Self-Reflective-Optimization"><a href="#92-IROTE-Human-like-Traits-Elicitation-of-Large-Language-Model-via-In-Context-Self-Reflective-Optimization" class="headerlink" title="92. IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/IROTE__Human-like_Traits_Elicitation_of_Large_Language_Model_via_In-Context_Self-Reflective_Optimiza.pdf">IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出IROTE，一种基于心理学自我反思理论的无微调大语言模型（LLM）人类特质诱导方法，通过信息瓶颈式目标优化生成文本自我反思，并将其嵌入任务提示中，稳定激发LLM展现特定人类特质。实验结果表明，IROTE在多种主流人类特质体系和复杂任务下均优于现有基线，能显著提升LLM在问卷与下游任务中的特质稳定性与可迁移性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/IROTE%3A%20Human-like%20Traits%20Elicitation%20of%20Large%20Language%20Model%20via%20In-Context%20Self-Reflective%20Optimization.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-STELAR-VISION-Self-Topology-Aware-Efficient-Learning-for-Aligned-Reasoning-in-Vision"><a href="#93-STELAR-VISION-Self-Topology-Aware-Efficient-Learning-for-Aligned-Reasoning-in-Vision" class="headerlink" title="93. STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/STELAR-VISION__Self-Topology-Aware_Efficient_Learning_for_Aligned_Reasoning_in_Vision.pdf">STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>本文提出了STELAR-Vision训练框架，通过TopoAug自动生成多样化链式、树式和图式推理拓扑结构，并利用监督微调和强化学习提升视觉-语言模型（VLMs）的多模态推理能力和输出效率。实验结果显示，STELAR-Vision在多个视觉推理基准上显著提升准确率（对基线提升9.7%，对更大模型提升7.3%），并通过Frugal Learning变体在保证准确率的同时将输出长度缩短18.1%，在五个分布外数据集上表现出强泛化能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/STELAR-VISION%3A%20Self-Topology-Aware%20Efficient%20Learning%20for%20Aligned%20Reasoning%20in%20Vision.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-Imposing-AI-Deceptive-design-patterns-against-sustainability"><a href="#94-Imposing-AI-Deceptive-design-patterns-against-sustainability" class="headerlink" title="94. Imposing AI: Deceptive design patterns against sustainability"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Imposing_AI__Deceptive_design_patterns_against_sustainability.pdf">Imposing AI: Deceptive design patterns against sustainability</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Université de Strasbourg</span></p><p>本论文通过收集和分析多家主流数字平台在2024年引入生成式AI功能的界面设计变化，揭示了两类主要的欺骗性设计策略：一是以牺牲原有非AI功能为代价强制推广AI特性，二是通过“魔法”与“助手”隐喻等叙事手法美化并掩盖AI的实际环境影响。结论指出，这些策略不仅降低了用户自主选择权，还加剧了数字基础设施的环境负担，亟需通过“欺骗性设计模式”视角推动相关监管与可持续设计。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Imposing%20AI%3A%20Deceptive%20design%20patterns%20against%20sustainability.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-LLM-driven-Text-to-Table-Generation-through-Sub-Tasks-Guidance-and-Iterative-Refinement"><a href="#95-LLM-driven-Text-to-Table-Generation-through-Sub-Tasks-Guidance-and-Iterative-Refinement" class="headerlink" title="95. LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/LLM_driven_Text-to-Table_Generation_through_Sub-Tasks_Guidance_and_Iterative_Refinement.pdf">LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IBM Research</span></p><p>本文提出了一种LLM驱动的文本到表格生成系统，采用两个关键策略：一是将复杂任务分解为中间子任务（如表头解释、缩写扩展、数据格式解析和实体抽取），二是通过LLM自反馈对生成表格进行逐步细化和迭代优化。实验证明，子任务引导和细粒度的单元格级自反馈显著提升了表格生成的准确性，在Rotowire和LiveSum两个复杂数据集上实现了当前最优性能，同时分析了性能提升与计算成本之间的权衡。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/LLM%20driven%20Text-to-Table%20Generation%20through%20Sub-Tasks%20Guidance%20and%20Iterative%20Refinement.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-Classifier-Language-Models-Unifying-Sparse-Finetuning-and-Adaptive-Tokenization-for-Specialized-Classification-Tasks"><a href="#96-Classifier-Language-Models-Unifying-Sparse-Finetuning-and-Adaptive-Tokenization-for-Specialized-Classification-Tasks" class="headerlink" title="96. Classifier Language Models: Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Classifier_Language_Models__Unifying_Sparse_Finetuning_and_Adaptive_Tokenization_for_Specialized_Cla.pdf">Classifier Language Models: Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amazon Web Services Inc.</span></p><p>本文提出了一种结合稀疏微调与自适应分词的新方法（AdaPT），用于高效适应小型预训练语言模型到特定领域分类任务。该方法通过挖掘任务特定的连续和非连续分词序列，扩展词表并仅微调对任务最敏感的模型参数，不引入新参数。结果表明，该方法在五个语义分类任务上稳定性更高、准确率更优且训练成本减半，优于端到端微调、LoRA、层选择和前缀微调等主流方法，尤其在需行业知识的专门任务中表现突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Classifier%20Language%20Models%3A%20Unifying%20Sparse%20Finetuning%20and%20Adaptive%20Tokenization%20for%20Specialized%20Classification%20Tasks.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-AgriGPT-a-Large-Language-Model-Ecosystem-for-Agriculture"><a href="#97-AgriGPT-a-Large-Language-Model-Ecosystem-for-Agriculture" class="headerlink" title="97. AgriGPT: a Large Language Model Ecosystem for Agriculture"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/AgriGPT__a_Large_Language_Model_Ecosystem_for_Agriculture.pdf">AgriGPT: a Large Language Model Ecosystem for Agriculture</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">College of Computer Science and Technology, Zhejiang University</span></p><p>本论文提出了AgriGPT，一个专为农业领域设计的大语言模型生态系统。方法包括多智能体数据引擎构建Agri-342K高质量指令数据集、持续预训练与有监督微调，并创新性地采用三通道检索增强生成（Tri-RAG）框架结合稠密&#x2F;稀疏检索与多跳知识图谱推理，显著提升事实性与推理能力。结论表明，AgriGPT在农业领域适应性和推理能力均超越通用大模型，同时具备良好通用性和多语言迁移能力，有助于推动农业AI普惠应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/AgriGPT%3A%20a%20Large%20Language%20Model%20Ecosystem%20for%20Agriculture.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-QoE-Aware-Service-Provision-for-Mobile-AR-Rendering-An-Agent-Driven-Approach"><a href="#98-QoE-Aware-Service-Provision-for-Mobile-AR-Rendering-An-Agent-Driven-Approach" class="headerlink" title="98. QoE-Aware Service Provision for Mobile AR Rendering: An Agent-Driven Approach"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/QoE-Aware_Service_Provision_for_Mobile_AR_Rendering__An_Agent-Driven_Approach.pdf">QoE-Aware Service Provision for Mobile AR Rendering: An Agent-Driven Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xidian University</span></p><p>该论文提出了一种基于大语言模型(LLM)的智能代理，实现移动增强现实(MAR)服务端与网络端的跨域协同，提升边缘辅助AR渲染体验。方法包括通过代理和API工具链进行用户特定QoE建模，以及结合Kalman滤波实现自适应资源管理。实验结果表明，该方法在用户QoE建模精度和带宽利用效率方面均优于传统方法。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/QoE-Aware%20Service%20Provision%20for%20Mobile%20AR%20Rendering%3A%20An%20Agent-Driven%20Approach.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-DepressLLM-Interpretable-domain-adapted-language-model-for-depression-detection-from-real-world-narratives"><a href="#99-DepressLLM-Interpretable-domain-adapted-language-model-for-depression-detection-from-real-world-narratives" class="headerlink" title="99. DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DepressLLM__Interpretable_domain-adapted_language_model_for_depression_detection_from_real-world_nar.pdf">DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Electronics and Telecommunications Research Institute</span></p><p>该论文提出了DepressLLM，一种基于大规模语言模型的抑郁症检测方法，通过在包含幸福与痛苦叙述的真实患者语料上微调主流LLM，并引入SToPS（Score-guided Token Probability Summation）模块，实现了可解释性预测及置信度估算。实验结果表明，DepressLLM在多种异构数据集上表现出优越的分类性能和鲁棒性，且高置信度输出与临床专家判断高度一致，推动了精神健康AI的早期筛查应用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DepressLLM%3A%20Interpretable%20domain-adapted%20language%20model%20for%20depression%20detection%20from%20real-world%20narratives.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-DocThinker-Explainable-Multimodal-Large-Language-Models-with-Rule-based-Reinforcement-Learning-for-Document-Understanding"><a href="#100-DocThinker-Explainable-Multimodal-Large-Language-Models-with-Rule-based-Reinforcement-Learning-for-Document-Understanding" class="headerlink" title="100. DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DocThinker__Explainable_Multimodal_Large_Language_Models_with_Rule-based_Reinforcement_Learning_for_.pdf">DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huazhong University of Science and Technology</span></p><p>该论文提出DocThinker，一种基于规则的强化学习(RL)框架，用于多模态大语言模型(MLLM)在文档理解中的推理与可解释性提升。方法通过Group Relative Policy Optimization (GRPO)和多目标奖励机制，动态优化推理策略，生成结构化推理过程、改写问题、关键区域定位和最终答案，有效改善模型的泛化能力和输出的可解释性。实验结果表明DocThinker在多个文档理解与多模态推理任务上优于传统SFT和CoT模板方法，显著提高了模型的透明性、适应性和任务表现。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DocThinker%3A%20Explainable%20Multimodal%20Large%20Language%20Models%20with%20Rule-based%20Reinforcement%20Learning%20for%20Document%20Understanding.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="101-LLM-Driven-Adaptive-6G-Ready-Wireless-Body-Area-Networks-Survey-and-Framework"><a href="#101-LLM-Driven-Adaptive-6G-Ready-Wireless-Body-Area-Networks-Survey-and-Framework" class="headerlink" title="101. LLM-Driven Adaptive 6G-Ready Wireless Body Area Networks: Survey and Framework"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/LLM-Driven_Adaptive_6G-Ready_Wireless_Body_Area_Networks__Survey_and_Framework.pdf">LLM-Driven Adaptive 6G-Ready Wireless Body Area Networks: Survey and Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Nebraska–Lincoln</span></p><p>本文提出了一种以大语言模型（LLM）为认知控制平面的自适应无线体域网（WBAN）框架，能够实时协调路由、物理层选择、微能量采集和后量子安全等多项关键功能。研究通过系统综述分析现有WBAN架构、路由策略与安全机制，指出当前碎片化、静态化等不足，提出LLM驱动的统一自优化方案，在6G和医疗健康场景下实现高可靠、低功耗及量子安全的数据传输。结论认为该方法可显著提升WBAN的智能性、自适应性和安全性，为下一代移动健康系统奠定基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/LLM-Driven%20Adaptive%206G-Ready%20Wireless%20Body%20Area%20Networks%3A%20Survey%20and%20Framework.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-In-Context-Learning-as-Nonparametric-Conditional-Probability-Estimation-Risk-Bounds-and-Optimality"><a href="#102-In-Context-Learning-as-Nonparametric-Conditional-Probability-Estimation-Risk-Bounds-and-Optimality" class="headerlink" title="102. In-Context Learning as Nonparametric Conditional Probability Estimation: Risk Bounds and Optimality"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/In-Context_Learning_as_Nonparametric_Conditional_Probability_Estimation__Risk_Bounds_and_Optimality.pdf">In-Context Learning as Nonparametric Conditional Probability Estimation: Risk Bounds and Optimality</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Department of Statistics, Beijing Normal University at Zhuhai</span></p><p>本文将In-Context Learning（ICL）建模为非参数条件概率估计问题，提出了一种基于截断Kullback-Leibler (KL) 散度的期望超额风险度量，并针对多分类任务推导了Transformer模型下ICL的精确上、下界。方法上，作者提出新颖的oracle不等式，利用对数似然函数类的统一经验覆盖熵控制泛化误差，并证明多层感知机（MLP）在特定假设下同样能以最优速率实现ICL。结论显示，Transformer和MLP均可在多分类ICL任务中达到统计最优的最小极大收敛速率（只差对数因子），并揭示ICL并非Transformer独有，丰富了大模型ICL理论基础。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/In-Context%20Learning%20as%20Nonparametric%20Conditional%20Probability%20Estimation%3A%20Risk%20Bounds%20and%20Optimality.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-Aryabhata-An-exam-focused-language-model-for-JEE-Math"><a href="#103-Aryabhata-An-exam-focused-language-model-for-JEE-Math" class="headerlink" title="103. Aryabhata: An exam-focused language model for JEE Math"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Aryabhata__An_exam-focused_language_model_for_JEE_Math.pdf">Aryabhata: An exam-focused language model for JEE Math</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">PhysicsWallah</span></p><p>Aryabhata 1.0 is a compact 7B parameter language model specialized for mathematical reasoning in Indian JEE exams, built by merging open-source reasoning models and fine-tuned with curriculum learning on verified chain-of-thought traces. Using supervised fine-tuning with rejection sampling and reinforcement learning with verifiable rewards (A2C), Aryabhata achieves state-of-the-art accuracy on JEE and competitive results on MATH&#x2F;GSM8K, with efficient step-by-step reasoning tailored for educational use.</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Aryabhata%3A%20An%20exam-focused%20language%20model%20for%20JEE%20Math.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-M2LLM-Multi-view-Molecular-Representation-Learning-with-Large-Language-Models"><a href="#104-M2LLM-Multi-view-Molecular-Representation-Learning-with-Large-Language-Models" class="headerlink" title="104. M2LLM: Multi-view Molecular Representation Learning with Large Language Models"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/$_text%7BM%7D%5E%7B2%7D$LLM__Multi-view_Molecular_Representation_Learning_with_Large_Language_Models.pdf">M2LLM: Multi-view Molecular Representation Learning with Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Griffith University</span></p><p>该论文提出M2LLM框架，创新性地融合了分子结构视角、任务视角和规则视角，利用大语言模型（LLM）进行分子表征生成和特征提取，并通过动态多视图融合优化表示，提升分子性质预测准确率。实验表明M2LLM在多项分子属性分类和回归任务上超越现有SOTA方法，展示了LLM在分子表示学习领域的巨大潜力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/M2LLM%3A%20Multi-view%20Molecular%20Representation%20Learning%20with%20Large%20Language%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-Prompt-and-Check-Using-Large-Language-Models-to-Evaluate-Communication-Protocol-Compliance-in-Simulation-Based-Training"><a href="#105-Prompt-and-Check-Using-Large-Language-Models-to-Evaluate-Communication-Protocol-Compliance-in-Simulation-Based-Training" class="headerlink" title="105. Prompt-and-Check: Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Prompt-and-Check__Using_Large_Language_Models_to_Evaluate_Communication_Protocol_Compliance_in_Simul.pdf">Prompt-and-Check: Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Singapore Polytechnic</span></p><p>本文提出Prompt-and-Check方法，利用开源大语言模型（如LLaMA 2&#x2F;3、Mistral 7B）结合语境选择和结构化提示，自动评估仿真训练中交流协议的合规性。实验证明，该方法在本地硬件上无需特定微调即可实现高准确率和合理解释，适用于海事等安全关键领域的自动化评估。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Prompt-and-Check%3A%20Using%20Large%20Language%20Models%20to%20Evaluate%20Communication%20Protocol%20Compliance%20in%20Simulation-Based%20Training.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-MIGRATE-Mixed-Policy-GRPO-for-Adaptation-at-Test-Time"><a href="#106-MIGRATE-Mixed-Policy-GRPO-for-Adaptation-at-Test-Time" class="headerlink" title="106. MIGRATE: Mixed-Policy GRPO for Adaptation at Test-Time"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MiGrATe__Mixed-Policy_GRPO_for_Adaptation_at_Test-Time.pdf">MIGRATE: Mixed-Policy GRPO for Adaptation at Test-Time</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Massachusetts Amherst</span></p><p>该论文提出MIGRATE方法，通过将Group Relative Policy Optimization（GRPO）与混合策略组（包括on-policy采样、贪心采样和邻域采样）结合，实现了无需外部训练数据的大模型在线测试时自适应搜索优化。实验证明MIGRATE在单词搜索、分子优化和抽象推理（ARC）等三大领域均显著优于现有推理和测试时训练基线，展示出在无监督复杂搜索任务中的卓越性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MIGRATE%3A%20Mixed-Policy%20GRPO%20for%20Adaptation%20at%20Test-Time.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="107-Securing-Educational-LLMs-A-Generalised-Taxonomy-of-Attacks-on-LLMs-and-DREAD-Risk-Assessment"><a href="#107-Securing-Educational-LLMs-A-Generalised-Taxonomy-of-Attacks-on-LLMs-and-DREAD-Risk-Assessment" class="headerlink" title="107. Securing Educational LLMs: A Generalised Taxonomy of Attacks on LLMs and DREAD Risk Assessment"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Securing_Educational_LLMs__A_Generalised_Taxonomy_of_Attacks_on_LLMs_and_DREAD_Risk_Assessment.pdf">Securing Educational LLMs: A Generalised Taxonomy of Attacks on LLMs and DREAD Risk Assessment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Waikato</span></p><p>本文系统梳理了大型语言模型（LLM）在教育领域应用时面临的安全威胁，提出了一套基于攻击复杂度的通用攻击分类法，并结合DREAD风险评估模型，量化分析了50种针对LLM及其基础设施的攻击手法在教育场景下的危害性。研究发现，令牌走私、对抗性提示、直接注入和多步越狱等攻击为关键高危威胁，并提出了风险缓解建议，结论对学术和产业界构建教育安全LLM系统具有指导意义。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Securing%20Educational%20LLMs%3A%20A%20Generalised%20Taxonomy%20of%20Attacks%20on%20LLMs%20and%20DREAD%20Risk%20Assessment.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-Generative-AI-for-Critical-Infrastructure-in-Smart-Grids-A-Unified-Framework-for-Synthetic-Data-Generation-and-Anomaly-Detection"><a href="#108-Generative-AI-for-Critical-Infrastructure-in-Smart-Grids-A-Unified-Framework-for-Synthetic-Data-Generation-and-Anomaly-Detection" class="headerlink" title="108. Generative AI for Critical Infrastructure in Smart Grids: A Unified Framework for Synthetic Data Generation and Anomaly Detection"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Generative_AI_for_Critical_Infrastructure_in_Smart_Grids__A_Unified_Framework_for_Synthetic_Data_Gen.pdf">Generative AI for Critical Infrastructure in Smart Grids: A Unified Framework for Synthetic Data Generation and Anomaly Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Michigan-Dearborn</span></p><p>本论文提出了一种面向数字变电站IEC61850协议GOOSE消息的生成式AI（GenAI）异常检测系统，核心包括AATM（高级对抗性流量变异）技术用于协议合规的合成数据生成，实现了高真实度和良好类别平衡的零日攻击样本，并利用基于任务导向对话（ToD）的GenAI框架进行异常检测。实验表明，所提GenAI-ADS（以Anthropic Claude Pro实现）在准确率、精度、F1等多项指标上明显优于传统机器学习模型且无需频繁重训练，对新型攻击有更强检测能力。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Generative%20AI%20for%20Critical%20Infrastructure%20in%20Smart%20Grids%3A%20A%20Unified%20Framework%20for%20Synthetic%20Data%20Generation%20and%20Anomaly%20Detection.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-OmniLLP-Enhancing-LLM-based-Log-Level-Prediction-with-Context-Aware-Retrieval"><a href="#109-OmniLLP-Enhancing-LLM-based-Log-Level-Prediction-with-Context-Aware-Retrieval" class="headerlink" title="109. OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/OmniLLP__Enhancing_LLM-based_Log_Level_Prediction_with_Context-Aware_Retrieval.pdf">OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Queen’s University</span></p><p>OmniLLP提出了一种利用语义和开发者归属聚类的上下文检索框架，通过多层聚类（语义+所有权）为大型语言模型（LLMs）提供更相关的in-context示例，实现了高效的日志级别自动预测。实验结果表明，OmniLLP在四个大型开源项目上将AUC提升至0.88-0.96，显著优于随机与单一聚类方法，提高了预测准确性和实际开发中的可用性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/OmniLLP%3A%20Enhancing%20LLM-based%20Log%20Level%20Prediction%20with%20Context-Aware%20Retrieval.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="110-Towards-Universal-Neural-Inference"><a href="#110-Towards-Universal-Neural-Inference" class="headerlink" title="110. Towards Universal Neural Inference"></a>110. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Towards_Universal_Neural_Inference.pdf">Towards Universal Neural Inference</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Department of Computer Science, UNC Chapel Hill</span></p><p>本文提出ASPIRE（Arbitrary Set-based Permutation-Invariant Reasoning Engine），一种结合集合变换器和语义嵌入的通用神经推理框架，能够在异构结构化数据上进行语义对齐和泛化预测。ASPIRE通过集合不变性结构和自然语言特征描述，实现跨数据集、跨领域的零样本、少样本泛化推断，在多个表格数据基准上取得领先性能，并支持主动特征获取任务。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Towards%20Universal%20Neural%20Inference.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="111-Constrained-free-energy-minimization-for-the-design-of-thermal-states-and-stabilizer-thermodynamic-systems"><a href="#111-Constrained-free-energy-minimization-for-the-design-of-thermal-states-and-stabilizer-thermodynamic-systems" class="headerlink" title="111. Constrained free energy minimization for the design of thermal states and stabilizer thermodynamic systems"></a>111. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Constrained_free_energy_minimization_for_the_design_of_thermal_states_and_stabilizer_thermodynamic_s.pdf">Constrained free energy minimization for the design of thermal states and stabilizer thermodynamic systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">École Polytechnique Fédérale de Lausanne (EPFL)</span></p><p>本文系统研究了LMPW25算法（包括一阶&#x2F;二阶经典与量子-经典混合算法）在量子热力学中受约束能量最小化问题上的数值表现，涵盖了量子Heisenberg模型和基于稳定子码的热力学系统。论文提出将这些算法用于可控Hamiltonian的基态和热态设计，并首次将量子纠错中的稳定子码与热力学中的非对易守恒量建立桥梁，实现了在热力学系统中编码量子信息的新方法。结论表明，LMPW25算法在上述系统中均能有效收敛，理论与数值结果一致，并为材料分子设计及量子信息编码提供了新工具。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Constrained%20free%20energy%20minimization%20for%20the%20design%20of%20thermal%20states%20and%20stabilizer%20thermodynamic%20systems.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="112-Fre-CW-Targeted-Attack-on-Time-Series-Forecasting-using-Frequency-Domain-Loss"><a href="#112-Fre-CW-Targeted-Attack-on-Time-Series-Forecasting-using-Frequency-Domain-Loss" class="headerlink" title="112. Fre-CW: Targeted Attack on Time Series Forecasting using Frequency Domain Loss"></a>112. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Fre-CW__Targeted_Attack_on_Time_Series_Forecasting_using_Frequency_Domain_Loss.pdf">Fre-CW: Targeted Attack on Time Series Forecasting using Frequency Domain Loss</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiaotong University</span></p><p>本文提出了一种基于频域损失的时间序列预测目标攻击方法Fre-CW，通过改进C&amp;W攻击算法，将其引入时间序列预测领域，并融合时间域与频域损失优化对抗样本。实验结果表明，Fre-CW在多个数据集和主流预测模型上均显著提升了目标攻击的有效性和隐蔽性，验证了频域信息在提升攻击性能中的重要作用。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Fre-CW%3A%20Targeted%20Attack%20on%20Time%20Series%20Forecasting%20using%20Frequency%20Domain%20Loss.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="113-FetFIDS-A-Feature-Embedding-Attention-based-Federated-Network-Intrusion-Detection-Algorithm"><a href="#113-FetFIDS-A-Feature-Embedding-Attention-based-Federated-Network-Intrusion-Detection-Algorithm" class="headerlink" title="113. FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm"></a>113. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/FetFIDS__A_Feature_Embedding_Attention_based_Federated_Network_Intrusion_Detection_Algorithm.pdf">FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Purdue University</span></p><p>该论文提出了一种特征嵌入与顺序多头注意力机制结合的Transformer模型，并在联邦学习环境下提升网络入侵检测性能。方法采用CNN特征嵌入替代传统位置嵌入，并通过多轮模型聚合与节点本地训练实现隐私保护和性能提升。实验结果表明，该方法在准确率、精确率、召回率及F1分数上均优于当前主流入侵检测算法，且模型稳定性强，适合实际部署。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/FetFIDS%3A%20A%20Feature%20Embedding%20Attention%20based%20Federated%20Network%20Intrusion%20Detection%20Algorithm.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="114-Wavelet-Mixture-of-Experts-for-Time-Series-Forecasting"><a href="#114-Wavelet-Mixture-of-Experts-for-Time-Series-Forecasting" class="headerlink" title="114. Wavelet Mixture of Experts for Time Series Forecasting"></a>114. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Wavelet_Mixture_of_Experts_for_Time_Series_Forecasting.pdf">Wavelet Mixture of Experts for Time Series Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai University of Engineering Science</span></p><p>该论文提出了WaveTS系列时间序列预测模型，结合离散小波变换与MLP以分解时序数据的高低频特征，并通过MoE（Mixture of Experts）通道聚类策略高效处理多通道依赖。实验结果表明，WaveTS系列在八个公开数据集上以更少参数实现了SOTA预测性能，尤其在多通道数据集上表现突出，提升了准确率和计算效率。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Wavelet%20Mixture%20of%20Experts%20for%20Time%20Series%20Forecasting.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="115-Differentiated-Information-Mining-A-Semi-supervised-Learning-Framework-for-GNNs"><a href="#115-Differentiated-Information-Mining-A-Semi-supervised-Learning-Framework-for-GNNs" class="headerlink" title="115. Differentiated Information Mining: A Semi-supervised Learning Framework for GNNs"></a>115. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Differentiated_Information_Mining__A_Semi-supervised_Learning_Framework_for_GNNs.pdf">Differentiated Information Mining: A Semi-supervised Learning Framework for GNNs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了DiFac框架，通过从单一信息源中挖掘多种区分性决策因子，并利用一致性过滤和最小置信度排名策略，在半监督场景下提升图神经网络（GNN）泛化能力和鲁棒性。同时引入大规模多模态模型生成的辅助文本知识作为因子，并设计问责评分机制以抑制噪声。实验表明，该方法在低标注率和高噪声条件下显著优于主流基线，尤其在特征维度较高的数据集上效果突出。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Differentiated%20Information%20Mining%3A%20A%20Semi-supervised%20Learning%20Framework%20for%20GNNs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="116-Elucidating-Rectified-Flow-with-Deterministic-Sampler-Polynomial-Discretization-Complexity-for-Multi-and-One-step-Models"><a href="#116-Elucidating-Rectified-Flow-with-Deterministic-Sampler-Polynomial-Discretization-Complexity-for-Multi-and-One-step-Models" class="headerlink" title="116. Elucidating Rectified Flow with Deterministic Sampler: Polynomial Discretization Complexity for Multi and One-step Models"></a>116. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Elucidating_Rectified_Flow_with_Deterministic_Sampler__Polynomial_Discretization_Complexity_for_Mult.pdf">Elucidating Rectified Flow with Deterministic Sampler: Polynomial Discretization Complexity for Multi and One-step Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了对Rectified Flow (RF) 基于确定性采样器的多步和一步生成模型的离散化复杂性理论分析，首次在真实的有界支持假设下证明了RF模型可达多项式级别的离散化复杂度。方法上结合了预测-校正框架和Langevin过程作为校正器，并提出了向量扰动引理，理论上优于主流扩散模型。结果显示，无论在多步还是一步生成场景，RF模型在采样复杂性上都优于VP、VE等扩散模型，为RF模型优异的经验表现提供了理论解释。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Elucidating%20Rectified%20Flow%20with%20Deterministic%20Sampler%3A%20Polynomial%20Discretization%20Complexity%20for%20Multi%20and%20One-step%20Models.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="117-Diminution-On-Reducing-the-Size-of-Grounding-ASP-Programs"><a href="#117-Diminution-On-Reducing-the-Size-of-Grounding-ASP-Programs" class="headerlink" title="117. Diminution: On Reducing the Size of Grounding ASP Programs"></a>117. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Diminution__On_Reducing_the_Size_of_Grounding_ASP_Programs.pdf">Diminution: On Reducing the Size of Grounding ASP Programs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China (USTC)</span></p><p>本文提出了“diminution”概念，即在ASP程序归约前选取Herbrand宇宙的子集以生成更小的ground程序，并形式化定义了diminution、分析了其性质和复杂性，并提出基于域谓词的实现方式。实验表明，该方法平均可减少高达70%的grounding时间、85%的ground文件大小，同时保持原程序解的可扩展性，有效缓解了ASP中的grounding瓶颈。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Diminution%3A%20On%20Reducing%20the%20Size%20of%20Grounding%20ASP%20Programs.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>今日论文报纸</title>
    <link href="/2025/08/2025-08-13_article/"/>
    <url>/2025/08/2025-08-13_article/</url>
    
    <content type="html"><![CDATA[<center><h1>今日论文报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-13<br>📄 <strong>发现论文数量</strong>：300  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Consensus-based-Decentralized-Multi-agent-Reinforcement-Learning-for-Random-Access-Network-Optimization"><a href="#1-Consensus-based-Decentralized-Multi-agent-Reinforcement-Learning-for-Random-Access-Network-Optimization" class="headerlink" title="1. Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Consensus-based_Decentralized_Multi-agent_Reinforcement_Learning_for_Random_Access_Network_Optimizat.pdf">Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Ohio State University</span></p><p>该论文提出了一种基于共识的全分布式多智能体强化学习（MARL）方法，用于随机接入（RA）网络的MAC层优化。方法通过仅在邻居间交换局部奖励，结合actor-critic架构，在无需中心化训练的情况下实现全局收敛，显著降低了通信开销。理论分析证明了算法的有限时间收敛性，实验结果表明该方法在提升网络吞吐量和公平性方面与中心化方法相当，但更具实用性和扩展性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-AR-GRPO-Training-Autoregressive-Image-Generation-Models-via-Reinforcement-Learning"><a href="#2-AR-GRPO-Training-Autoregressive-Image-Generation-Models-via-Reinforcement-Learning" class="headerlink" title="2. AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AR-GRPO__Training_Autoregressive_Image_Generation_Models_via_Reinforcement_Learning.pdf">AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Technology</span></p><p>该论文提出AR-GRPO方法，将在线强化学习（RL）训练集成到自回归（AR）图像生成模型中，并首次将Group Relative Policy Optimization（GRPO）算法应用于图像生成，通过多维度奖励函数（如语义一致性、视觉质量和真实感）细致优化模型输出。实验结果表明，RL增强的AR模型在类别条件和文本条件生成任务中，显著提升了图像质量和人类偏好指标，尤其在人类评价、分辨率和模型规模扩展方面展现强大泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Pushdown-Reward-Machines-for-Reinforcement-Learning"><a href="#3-Pushdown-Reward-Machines-for-Reinforcement-Learning" class="headerlink" title="3. Pushdown Reward Machines for Reinforcement Learning"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pushdown_Reward_Machines_for_Reinforcement_Learning.pdf">Pushdown Reward Machines for Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Utrecht Universiteit</span></p><p>该论文提出了Pushdown Reward Machines（pdRMs），基于确定性下推自动机，能够表达和奖励可由确定性上下文无关语言表示的非马尔可夫任务，显著提升了任务表达能力。作者设计了两种策略访问模式（全栈访问与top-k访问），并证明在部分场景下top-k策略与全栈策略在最优性上等价，理论和实验分析表明pdRMs在部分任务上较Counting Reward Automata具备更高空间效率，且在多离散和连续任务中提升了训练效率和泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-Offline-to-Online-Reinforcement-Learning-with-Classifier-Free-Diffusion-Generation"><a href="#4-Offline-to-Online-Reinforcement-Learning-with-Classifier-Free-Diffusion-Generation" class="headerlink" title="4. Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Offline-to-Online_Reinforcement_Learning_with_Classifier-Free_Diffusion_Generation.pdf">Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了一种新的数据增强方法Classifier-Free Diffusion Generation (CFDG)，利用无分类器引导的扩散模型同时对离线数据和在线数据进行增强，并通过数据重加权使生成数据更贴合在线策略分布，可与主流O2O RL算法（如IQL、PEX、APL）集成。实验证明，CFDG在D4RL Locomotion和AntMaze基准上可平均提升15%的性能，生成的数据更高质量、与在线分布更一致。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-Sparsity-Driven-Plasticity-in-Multi-Task-Reinforcement-Learning"><a href="#5-Sparsity-Driven-Plasticity-in-Multi-Task-Reinforcement-Learning" class="headerlink" title="5. Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sparsity-Driven_Plasticity_in_Multi-Task_Reinforcement_Learning.pdf">Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Groningen</span></p><p>本文系统性研究了在多任务强化学习（MTRL）中，稀疏化方法（特别是Gradual Magnitude Pruning与Sparse Evolutionary Training）对提升神经网络可塑性及任务表现的作用。通过在多种MTRL架构与标准基准环境上的实证分析，结果表明稀疏化不仅缓解了神经元休眠和表示崩溃等可塑性退化现象，还常常提升了多任务性能，优于致力于可塑性提升的专用方法和常用正则化手段，尤其在常见架构如MTPPO和MoE中表现突出。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-Multi-level-Advantage-Credit-Assignment-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#6-Multi-level-Advantage-Credit-Assignment-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="6. Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-level_Advantage_Credit_Assignment_for_Cooperative_Multi-Agent_Reinforcement_Learning.pdf">Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mila - Quebec AI Institute</span></p><p>本文提出了一种多层次优势信号分配方法（MACA），通过显式的反事实推理，分别对个体、联合及强相关子集的动作贡献进行推断，并结合Transformer自注意力机制动态建模多智能体间的相关性，实现多层次的信用分配。实验结果表明，MACA在StarCraft多智能体强化学习基准任务中显著提升了协作效率和收敛性能，优于现有主流方法，理论分析也支持其有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Natural-Language-Driven-Viewpoint-Navigation-for-Volume-Exploration-via-Semantic-Block-Representation"><a href="#7-Natural-Language-Driven-Viewpoint-Navigation-for-Volume-Exploration-via-Semantic-Block-Representation" class="headerlink" title="7. Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Natural_Language-Driven_Viewpoint_Navigation_for_Volume_Exploration_via_Semantic_Block_Representatio.pdf">Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Sun Yat-sen University</span></p><p>本文提出了一种基于自然语言交互的体数据探索框架，通过将体数据分割为语义块，并利用CLIP模型对图像和文本进行对齐，结合强化学习（PPO算法）自动优化视点选择。实验结果表明，该方法能准确捕捉用户意图，提升体数据导航效率和可解释性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-Stackelberg-Coupling-of-Online-Representation-Learning-and-Reinforcement-Learning"><a href="#8-Stackelberg-Coupling-of-Online-Representation-Learning-and-Reinforcement-Learning" class="headerlink" title="8. Stackelberg Coupling of Online Representation Learning and Reinforcement Learning"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Stackelberg_Coupling_of_Online_Representation_Learning_and_Reinforcement_Learning.pdf">Stackelberg Coupling of Online Representation Learning and Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fordham University</span></p><p>本文提出了SCORER框架，通过将感知网络（表征学习）和控制网络（强化学习）之间的协作建模为Stackelberg博弈，实现了两者的层级动态耦合。方法采用两时间尺度梯度下降近似博弈均衡，并在多种DQN变体及标准任务上验证，结果显示SCORER在样本效率和最终表现上均优于传统端到端方法。结论：基于博弈论的感知-控制动态设计可以在不增加复杂辅助目标或架构的前提下显著提升深度强化学习性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-Efficient-Reward-Identification-In-Max-Entropy-Reinforcement-Learning-with-Sparsity-and-Rank-Priors"><a href="#9-Efficient-Reward-Identification-In-Max-Entropy-Reinforcement-Learning-with-Sparsity-and-Rank-Priors" class="headerlink" title="9. Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Efficient_Reward_Identification_In_Max_Entropy_Reinforcement_Learning_with_Sparsity_and_Rank_Priors.pdf">Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Michigan</span></p><p>本文提出了在最大熵强化学习中高效识别时变奖励函数的优化方法，包括基于奖励切换稀疏性和特征低秩假设的算法。通过贪心区间划分和核范数优化，有效恢复奖励结构，在多种网格世界实验中显示出鲁棒性和较优的迁移能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-Pentest-R1-Towards-Autonomous-Penetration-Testing-Reasoning-Optimized-via-Two-Stage-Reinforcement-Learning"><a href="#10-Pentest-R1-Towards-Autonomous-Penetration-Testing-Reasoning-Optimized-via-Two-Stage-Reinforcement-Learning" class="headerlink" title="10. Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pentest-R1__Towards_Autonomous_Penetration_Testing_Reasoning_Optimized_via_Two-Stage_Reinforcement_L.pdf">Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">State Key Laboratory of Cyberspace Security Defense, Institute of Information Engineering, Chinese Academy of Sciences</span></p><p>本文提出了Pentest-R1框架，通过两阶段强化学习提升大语言模型在自动渗透测试中的推理与自适应能力。方法首先在包含500多条真实多步专家渗透测试演练的数据集上进行离线强化学习，奠定基础攻击逻辑，再在交互式CTF环境中通过在线强化学习优化策略与错误自纠能力。实验表明，Pentest-R1在Cybench和AutoPenBench基准上超越了大部分同类开源及专有模型，验证了两阶段结合对提升自动渗透测试表现的关键作用。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="11-Policy-Newton-methods-for-Distortion-Riskmetrics"><a href="#11-Policy-Newton-methods-for-Distortion-Riskmetrics" class="headerlink" title="11. Policy Newton methods for Distortion Riskmetrics"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Policy_Newton_methods_for_Distortion_Riskmetrics.pdf">Policy Newton methods for Distortion Riskmetrics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indian Institute of Technology Madras</span></p><p>本文提出了一种在有限时域Markov决策过程（MDP）中最大化畸变风险度量（Distortion Riskmetric, DRM）的策略Newton法。方法上，作者推导了DRM目标的策略Hessian定理，通过似然比法构建了基于样本轨迹的DRM梯度和Hessian估计器，并提出了带三次正则化的策略Newton算法（CRPN-DRM），理论上证明该算法以O(ϵ^-3.5)的样本复杂度收敛到ϵ-二阶稳定点，能有效规避鞍点。实验在Cliff Walk、Cart Pole及Humanoid等环境下表明，该算法在风险敏感目标下优于风险中性基线策略，获得更高期望回报。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-Pref-GUIDE-Continual-Policy-Learning-from-Real-Time-Human-Feedback-via-Preference-Based-Learning"><a href="#12-Pref-GUIDE-Continual-Policy-Learning-from-Real-Time-Human-Feedback-via-Preference-Based-Learning" class="headerlink" title="12. Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pref-GUIDE__Continual_Policy_Learning_from_Real-Time_Human_Feedback_via_Preference-Based_Learning.pdf">Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Duke University</span></p><p>本文提出Pref-GUIDE框架，通过将实时人类标注的噪声标量反馈转化为局部对比偏好数据，实现更稳定和高效的奖励模型训练，提升基于人类反馈的持续策略学习。Pref-GUIDE由两个关键模块组成：Pref-GUIDE Individual在局部时间窗口内将标量反馈转为偏好对，Pref-GUIDE Voting则在多评估者间聚合奖励模型以达成群体共识。实验表明，该方法在三个复杂视觉RL任务中优于基于标量回归的主流方法，并在部分任务上超过专家设计的稠密奖励，验证了其在高噪声、多人反馈下的稳健性和可扩展性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-Reinforcement-Learning-in-Vision-A-Survey"><a href="#13-Reinforcement-Learning-in-Vision-A-Survey" class="headerlink" title="13. Reinforcement Learning in Vision: A Survey"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Reinforcement_Learning_in_Vision__A_Survey.pdf">Reinforcement Learning in Vision: A Survey</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Singapore</span></p><p>本论文系统综述了视觉领域中强化学习（Visual RL）的最新进展，涵盖多模态大语言模型、视觉生成、统一模型和视觉-语言-动作（VLA）模型四大方向，梳理了从RLHF、PPO到GRPO等主流策略，并提出了以奖励粒度和监督方式为基础的视觉RL方法学分类框架。结论指出，视觉RL正推动感知、推理与生成任务的统一发展，但在样本效率、泛化能力、奖励设计等方面仍面临重大挑战。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning"><a href="#14-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning" class="headerlink" title="14. Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Part_I__Tricks_or_Traps__A_Deep_Dive_into_RL_for_LLM_Reasoning.pdf">Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Alibaba Group</span></p><p>本文系统性评测了当前大语言模型（LLM）推理中的主流强化学习（RL）技术，包括归一化、裁剪、过滤及损失聚合等关键技巧，并在统一开源平台下通过不同模型规模与数据难度的精细实验，揭示各技术的内在机制及适用场景。结论指出，多数RL技术对实验设置高度敏感，并提出只用分组均值+批次标准差归一化和token级损失聚合两项简单技术（Lite PPO）即可显著提升无评价器策略的学习能力，优于众多繁杂算法，为RL4LLM实践提供了清晰实用的选择指南。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-ReconDreamer-RL-Enhancing-Reinforcement-Learning-via-Diffusion-based-Scene-Reconstruction"><a href="#15-ReconDreamer-RL-Enhancing-Reinforcement-Learning-via-Diffusion-based-Scene-Reconstruction" class="headerlink" title="15. ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ReconDreamer-RL__Enhancing_Reinforcement_Learning_via_Diffusion-based_Scene_Reconstruction.pdf">ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">GigaAI</span></p><p>该论文提出ReconDreamer-RL框架，将视频扩散先验与三维高斯泼溅（3DGS）场景重建和动力学建模相结合，构建更真实的模拟环境以提升端到端自动驾驶中的强化学习。方法包括ReconSimulator实现高保真感知与物理建模，Dynamic Adversary Agent（DAA）自动生成复杂角落案例，Cousin Trajectory Generator（CTG）扩展训练数据多样性。实验表明，该方法比模仿学习方法将碰撞率降低5倍，显著提升了模型在极端场景下的安全性和泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-Pose-RFT-Enhancing-MLLMs-for-3D-Pose-Generation-via-Hybrid-Action-Reinforcement-Fine-Tuning"><a href="#16-Pose-RFT-Enhancing-MLLMs-for-3D-Pose-Generation-via-Hybrid-Action-Reinforcement-Fine-Tuning" class="headerlink" title="16. Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pose-RFT__Enhancing_MLLMs_for_3D_Pose_Generation_via_Hybrid_Action_Reinforcement_Fine-Tuning.pdf">Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CASIA</span></p><p>该论文提出了Pose-RFT，一种专为3D人体姿态生成任务设计的强化微调框架，针对多模态大语言模型（MLLMs）。方法将3D姿态生成建模为离散（文本）与连续（姿态参数）混合动作空间下的强化学习问题，并引入HyGRPO算法，实现对文本和3D姿态的联合优化。Pose-RFT结合空间和语义对齐等任务特定奖励函数，有效提升了模型在图像到姿态、文本到姿态等任务上的表现。实验结果显示，该方法在多个基准上显著优于现有的姿态生成MLLMs，证实了混合动作强化微调在3D姿态生成中的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-A-Tutorial-An-Intuitive-Explanation-of-Offline-Reinforcement-Learning-Theory"><a href="#17-A-Tutorial-An-Intuitive-Explanation-of-Offline-Reinforcement-Learning-Theory" class="headerlink" title="17. A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Tutorial__An_Intuitive_Explanation_of_Offline_Reinforcement_Learning_Theory.pdf">A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Alberta</span></p><p>本论文系统梳理了离线强化学习（Offline RL）的理论基础，重点分析了函数表示和数据覆盖的假设条件，并通过反例揭示了离线RL的固有难度，强调样本复杂度与覆盖条件的关系。结论指出，只有在满足特定的覆盖与表示假设、采用如悲观估计和λ-return等技术，才能保证多项式样本高效学习，当前理论为实际算法设计提供了清晰的边界和启示。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-Symmetry-Aware-Transformer-Training-for-Automated-Planning"><a href="#18-Symmetry-Aware-Transformer-Training-for-Automated-Planning" class="headerlink" title="18. Symmetry-Aware Transformer Training for Automated Planning"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Symmetry-Aware_Transformer_Training_for_Automated_Planning.pdf">Symmetry-Aware Transformer Training for Automated Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Linköping University</span></p><p>本文提出了一种面向自动规划的对称性感知Transformer训练方法，通过引入基于对称性的对比学习目标，使模型能够有效忽略输入中的对象命名和原子顺序等冗余对称性，并采用无位置编码的编码器-解码器架构处理规划任务。实验表明，该方法在多个规划领域相比PlanGPT显著提升了模型的泛化与外推能力，特别是在未见过的大规模问题上有较好表现，但在极大规模或复杂域仍存在一定局限。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Robust-Reinforcement-Learning-over-Wireless-Networks-with-Homomorphic-State-Representations"><a href="#19-Robust-Reinforcement-Learning-over-Wireless-Networks-with-Homomorphic-State-Representations" class="headerlink" title="19. Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Robust_Reinforcement_Learning_over_Wireless_Networks_with_Homomorphic_State_Representations.pdf">Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Padova</span></p><p>该论文提出了一种名为Homomorphic Robust Remote Reinforcement Learning (HR3L)的新架构，用于在非理想无线网络中高效训练远程RL智能体。HR3L通过马尔可夫同态理论对环境状态进行压缩表征，采用推送式通信和异步模型更新，显著降低通信和计算负担，同时提升对信道丢包、延迟和带宽受限等场景的鲁棒性。实验证明HR3L在DeepMind Control Suite多个控制任务和多种信道条件下，样本效率、带宽占用和鲁棒性均优于现有方法，尤其在高维观测和有限资源下表现突出。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-MORE-CLEAR-Multimodal-Offline-Reinforcement-learning-for-Clinical-notes-Leveraged-Enhanced-State-Representation"><a href="#20-MORE-CLEAR-Multimodal-Offline-Reinforcement-learning-for-Clinical-notes-Leveraged-Enhanced-State-Representation" class="headerlink" title="20. MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MORE-CLEAR__Multimodal_Offline_Reinforcement_learning_for_Clinical_notes_Leveraged_Enhanced_State_Re.pdf">MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University Hospital</span></p><p>该论文提出MORE-CLEAR框架，通过结合结构化数据（如实验室结果、生命体征）与非结构化临床文本（通过大语言模型编码与摘要），利用上下文门控融合与双向跨模态注意力机制，提升ICU脓毒症治疗中的患者状态表征，从而支持更鲁棒的离线强化学习策略优化。实验在MIMIC-III、MIMIC-IV和真实医院数据集上表明，多模态方法在存活率估计和策略表现上均显著优于单模态基线，并且该框架易于迁移至其它医学决策场景。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="21-DexFruit-Dexterous-Manipulation-and-Gaussian-Splatting-Inspection-of-Fruit"><a href="#21-DexFruit-Dexterous-Manipulation-and-Gaussian-Splatting-Inspection-of-Fruit" class="headerlink" title="21. DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DexFruit__Dexterous_Manipulation_and_Gaussian_Splatting_Inspection_of_Fruit.pdf">DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University</span></p><p>该论文提出DexFruit框架，将光学触觉传感与扩散式模仿学习策略结合，实现对草莓、番茄和黑莓等脆弱水果的温和自主操作，有效减少损伤。其创新的FruitSplat方法基于3D Gaussian Splatting，能高精度定量分析水果表面损伤，实现高分辨率三维可视化。实验结果显示DexFruit在多种水果上抓取成功率达92%，外部损伤降低20%，显著优于视觉或触觉单一方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-Communication-Efficient-Multi-Agent-3D-Detection-via-Hybrid-Collaboration"><a href="#22-Communication-Efficient-Multi-Agent-3D-Detection-via-Hybrid-Collaboration" class="headerlink" title="22. Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Communication-Efficient_Multi-Agent_3D_Detection_via_Hybrid_Collaboration.pdf">Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>本文提出了HyComm，一种基于混合协作的高效通信多智能体LiDAR 3D目标检测系统。方法通过不确定性引导下自适应融合紧凑的感知输出与丰富的原始观测数据，并对每类消息优先选择最重要的数据，实现了在不同通信带宽下的最优感知信息交换。实验结果显示HyComm在DAIR-V2X与OPV2V数据集上，无论模型同构或异构，都显著优于现有方法，实现了更低通信量下更高检测性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-From-Data-to-Safe-Mobile-Robot-Navigation-An-Efficient-and-Modular-Robust-MPC-Design-Pipeline"><a href="#23-From-Data-to-Safe-Mobile-Robot-Navigation-An-Efficient-and-Modular-Robust-MPC-Design-Pipeline" class="headerlink" title="23. From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Data_to_Safe_Mobile_Robot_Navigation__An_Efficient_and_Modular_Robust_MPC_Design_Pipeline.pdf">From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Delft University of Technology</span></p><p>本文提出了一套高效且模块化的鲁棒模型预测控制（MPC）设计流程，针对移动机器人导航中的安全性问题，系统性地从实验数据出发，通过迭代式移动地平线估计算法定量不确定性，并结合输出反馈鲁棒MPC方案实现碰撞规避和约束可行性。实验证明，该流程在存在扰动和测量噪声的情况下，能够在无人机仿真环境中保证鲁棒约束满足和递归可行性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-ForeSight-Multi-View-Streaming-Joint-Object-Detection-and-Trajectory-Forecasting"><a href="#24-ForeSight-Multi-View-Streaming-Joint-Object-Detection-and-Trajectory-Forecasting" class="headerlink" title="24. ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ForeSight__Multi-View_Streaming_Joint_Object_Detection_and_Trajectory_Forecasting.pdf">ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Toronto</span></p><p>本文提出了ForeSight，一种面向自动驾驶场景的多视角流式联合检测与轨迹预测框架。方法创新性地通过统一的Transformer架构和双向查询记忆，打破传统检测和预测串行处理的壁垒，使检测与预测任务在时序上实现信息共享和闭环反馈，支持无跟踪关联的端到端推理。实验证明，ForeSight在nuScenes数据集上实现了检测和预测的双SOTA性能，mAP与EPA指标均优于现有方法，显著提升了复杂场景下感知的准确性和鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-Model-Predictive-Control-for-Crowd-Navigation-via-Learning-Based-Trajectory-Prediction"><a href="#25-Model-Predictive-Control-for-Crowd-Navigation-via-Learning-Based-Trajectory-Prediction" class="headerlink" title="25. Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Model_Predictive_Control_for_Crowd_Navigation_via_Learning-Based_Trajectory_Prediction.pdf">Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bauhaus-Universitat Weimar</span></p><p>该论文提出将深度学习驱动的Social-Implicit(SI)行人轨迹预测模型集成进模型预测控制(MPC)框架，并在实体机器人上进行了验证。实验显示，在不同人群密度下，SI-MPC系统显著提升了预测准确性和机器人导航的安全性与平滑性，尤其在低密度场景下预测误差减少高达76%。结论表明，学习型预测器在真实动态环境中能提升安全和社交适应性，但实际部署需关注系统级评估与效率-保守性权衡。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-P3-Toward-Versatile-Embodied-Agents"><a href="#26-P3-Toward-Versatile-Embodied-Agents" class="headerlink" title="26. P3: Toward Versatile Embodied Agents"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/$_mathcal%7BP%7D%5E3$__Toward_Versatile_Embodied_Agents.pdf">P3: Toward Versatile Embodied Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southern University of Science and Technology</span></p><p>本文提出了P3框架，通过引入统一的感知模块主动感知环境变化、消除对工具反馈的依赖，实现任意工具即插即用，并结合动态多任务调度器，实现任务的智能规划、优先级排序与执行。大量真实环境实验表明，P3极大提升了机器人对复杂动态环境的适应性和通用性，缩小了基准与实际部署的差距。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-Neural-Channel-Knowledge-Map-Assisted-Scheduling-Optimization-of-Active-IRSs-in-Multi-User-Systems"><a href="#27-Neural-Channel-Knowledge-Map-Assisted-Scheduling-Optimization-of-Active-IRSs-in-Multi-User-Systems" class="headerlink" title="27. Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Neural_Channel_Knowledge_Map_Assisted_Scheduling_Optimization_of_Active_IRSs_in_Multi-User_Systems.pdf">Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xiamen University</span></p><p>本论文提出一种基于神经信道知识图（CKM）的多用户主动智能反射表面（AIRS）调度优化框架，通过设计Transformer深度神经网络（LPS-Net和SE-Net）实现历史信道与吞吐量数据的高效预测，并以此辅助时频资源与AIRS分配。实验结果显示，该方法显著提升了预测精度和计算效率，SM-IB算法在复杂性较低的情况下达到了近似最优的最大最小吞吐量。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="28-EGS-SLAM-RGB-D-Gaussian-Splatting-SLAM-with-Events"><a href="#28-EGS-SLAM-RGB-D-Gaussian-Splatting-SLAM-with-Events" class="headerlink" title="28. EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EGS-SLAM__RGB-D_Gaussian_Splatting_SLAM_with_Events.pdf">EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanyang Technological University</span></p><p>本文提出了EGS-SLAM，一种融合事件相机数据、RGB图像和深度信息的3D Gaussian Splatting SLAM系统，显式建模曝光期间的连续相机轨迹，并引入可学习的相机响应函数（CRF）与no-event loss，实现事件-图像-深度的联合跟踪和高保真三维重建。实验表明，该方法在有严重运动模糊时，相较于现有GS-SLAM方案在定位和重建质量上均有显著提升。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Манипулятор-для-помощи-людям-с-ограниченными-возможностями"><a href="#29-Манипулятор-для-помощи-людям-с-ограниченными-возможностями" class="headerlink" title="29. Манипулятор для помощи людям с ограниченными возможностями"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Manipulator_for_people_with_limited_abilities.pdf">Манипулятор для помощи людям с ограниченными возможностями</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Московский государственный технический университет имени Н.Э. Баумана</span></p><p>本论文提出了一种面向行动不便人士的机器人辅助喂食系统RoboBK，包括机械结构设计、动力与能耗分析、四自由度运动学建模、基于视觉的目标检测、ROS平台下的控制与运动规划、以及软硬件一体化实现。通过集成3D建模、动力学仿真、闭环控制和基于摄像头的人脸（鼻部）识别，最终完成了低成本原型开发与多模式实际测试，验证了系统的有效性和实用性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-From-Imitation-to-Optimization-A-Comparative-Study-of-Offline-Learning-for-Autonomous-Driving"><a href="#30-From-Imitation-to-Optimization-A-Comparative-Study-of-Offline-Learning-for-Autonomous-Driving" class="headerlink" title="30. From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Imitation_to_Optimization__A_Comparative_Study_of_Offline_Learning_for_Autonomous_Driving.pdf">From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent Researcher</span></p><p>本文提出了一个完整的自动驾驶离线学习流程，系统比较了行为克隆（BC）和保守Q学习（CQL）在Waymo Open Motion Dataset上的表现。通过构建结构化状态表示和Transformer架构，作者发现BC方法在长时序任务中易堆积误差，而采用CQL的离线强化学习能够显著提升策略的鲁棒性和成功率。结论表明，离线强化学习方法对于从静态专家数据中学习高鲁棒性的自动驾驶策略至关重要。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-Imaginative-World-Modeling-with-Scene-Graphs-for-Embodied-Agent-Navigation"><a href="#31-Imaginative-World-Modeling-with-Scene-Graphs-for-Embodied-Agent-Navigation" class="headerlink" title="31. Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Imaginative_World_Modeling_with_Scene_Graphs_for_Embodied_Agent_Navigation.pdf">Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Michigan, Ann Arbor</span></p><p>本文提出SGImagineNav，一个基于符号世界建模的主动导航框架，通过构建分层场景图并利用大语言模型对未知区域进行语义推理，实现环境的全局结构化表示。系统在仿真和真实场景下均显著提升了目标定位效率，跨楼层和跨房间导航成功率分别达65.4%和66.8%，展示了其泛化性和实用价值。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-PANAMA-A-Network-Aware-MARL-Framework-for-Multi-Agent-Path-Finding-in-Digital-Twin-Ecosystems"><a href="#32-PANAMA-A-Network-Aware-MARL-Framework-for-Multi-Agent-Path-Finding-in-Digital-Twin-Ecosystems" class="headerlink" title="32. PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PANAMA__A_Network-Aware_MARL_Framework_for_Multi-Agent_Path_Finding_in_Digital_Twin_Ecosystems.pdf">PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huawei Canada Advanced Research Center</span></p><p>本文提出了PANAMA算法，一种基于优先级非对称性的网络感知多智能体强化学习（MARL）框架，用于数字孪生生态系统中的多智能体路径规划。该方法结合Centralized Training with Decentralized Execution（CTDE）、异步actor-learner结构和优先经验回放，并引入动态优先级和网络信号质量感知，极大提升了多智能体在复杂环境下的协作与路径效率。实验结果显示，PANAMA在准确率、速度和可扩展性方面优于现有基线，尤其在拥挤和通信受限场景下表现突出，证明了网络感知对于多智能体系统高效协作和鲁棒性的关键作用。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-Energy-Efficient-Task-Offloading-in-UAV-Enabled-MEC-Using-a-Fully-Decentralized-Deep-Reinforcement-Learning-Approach"><a href="#33-Energy-Efficient-Task-Offloading-in-UAV-Enabled-MEC-Using-a-Fully-Decentralized-Deep-Reinforcement-Learning-Approach" class="headerlink" title="33. Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Energy_Efficient_Task_Offloading_in_UAV-Enabled_MEC_Using_a_Fully_Decentralized_Deep_Reinforcement_L.pdf">Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Iran University of Science and Technology</span></p><p>该论文提出了一种用于无人机（UAV）支持的多接入边缘计算（MEC）系统的全分布式深度强化学习方法，通过结合图注意力网络（GAT）与经验和参数共享的PPO（EPS-PPO），实现无人机任务卸载和轨迹优化，各无人机仅与邻居通信，无需中心节点。实验表明，该方法在能耗、任务处理数、收敛速度和鲁棒性等方面均优于半中心化的MADDPG方法，适用于动态MEC环境。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-D3P-Dynamic-Denoising-Diffusion-Policy-via-Reinforcement-Learning"><a href="#34-D3P-Dynamic-Denoising-Diffusion-Policy-via-Reinforcement-Learning" class="headerlink" title="34. D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/D3P__Dynamic_Denoising_Diffusion_Policy_via_Reinforcement_Learning.pdf">D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>该论文提出了一种名为Dynamic Denoising Diffusion Policy (D3P)的方法，在机器人视觉操作任务中，根据动作关键性动态分配扩散去噪步骤。D3P包含基础扩散策略和轻量级适配器，并通过强化学习联合优化，三阶段训练策略保证收敛稳定。实验结果显示，D3P在八项仿真任务和实际机器人部署中，推理速度较传统方法平均加速2.2倍且任务成功率无显著下降，验证了自适应推理的效率和实用性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Learning-a-Vision-Based-Footstep-Planner-for-Hierarchical-Walking-Control"><a href="#35-Learning-a-Vision-Based-Footstep-Planner-for-Hierarchical-Walking-Control" class="headerlink" title="35. Learning a Vision-Based Footstep Planner for Hierarchical Walking Control"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_a_Vision-Based_Footstep_Planner_for_Hierarchical_Walking_Control.pdf">Learning a Vision-Based Footstep Planner for Hierarchical Walking Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Pennsylvania</span></p><p>本文提出了一种基于视觉的分层控制框架，用于双足机器人步态规划。方法通过单目深度摄像头和局部高程图，结合强化学习训练的高层步态规划器，以及低层操作空间控制器，实现了在复杂地形上的实时步态决策和轨迹跟踪，并采用ALIP模型简化状态空间。结论显示该框架在仿真和硬件实验中均表现出优异的地形适应性与鲁棒性，但在复杂地形和实际部署时仍受限于模型表达能力和分层结构的迁移难题。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-Triple-S-A-Collaborative-Multi-LLM-Framework-for-Solving-Long-Horizon-Implicative-Tasks-in-Robotics"><a href="#36-Triple-S-A-Collaborative-Multi-LLM-Framework-for-Solving-Long-Horizon-Implicative-Tasks-in-Robotics" class="headerlink" title="36. Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Triple-S__A_Collaborative_Multi-LLM_Framework_for_Solving_Long-Horizon_Implicative_Tasks_in_Robotics.pdf">Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Northeastern University</span></p><p>本文提出Triple-S框架，通过多大语言模型（LLM）协作，包括任务简化、示例检索、闭环决策和API封装，有效提升机器人在长时序复杂任务中的代码生成成功率和鲁棒性。实验在LDIP数据集和真实机器人中验证了Triple-S方法的有效性，任务成功率显著高于现有方法，展示了在指令和环境隐含条件下的强泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-Multimodal-Spiking-Neural-Network-for-Space-Robotic-Manipulation"><a href="#37-Multimodal-Spiking-Neural-Network-for-Space-Robotic-Manipulation" class="headerlink" title="37. Multimodal Spiking Neural Network for Space Robotic Manipulation"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multimodal_Spiking_Neural_Network_for_Space_Robotic_Manipulation.pdf">Multimodal Spiking Neural Network for Space Robotic Manipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology</span></p><p>本文提出了一种基于多模态脉冲神经网络（SNN）的空间机器人臂控制框架，结合几何状态、触觉、语义信息提升环境感知，并以双通道三阶段课程强化学习（CRL）分阶段引导策略训练，实现能效高、自主操作的空间材料搬运与操控。实验表明，该方法在任务成功率与能耗方面均优于传统人工神经网络（ANN）基线，适用于资源受限的真实航天场景。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-MonoMPC-Monocular-Vision-Based-Navigation-with-Learned-Collision-Model-and-Risk-Aware-Model-Predictive-Control"><a href="#38-MonoMPC-Monocular-Vision-Based-Navigation-with-Learned-Collision-Model-and-Risk-Aware-Model-Predictive-Control" class="headerlink" title="38. MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MonoMPC__Monocular_Vision_Based_Navigation_with_Learned_Collision_Model_and_Risk-Aware_Model_Predict.pdf">MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Tartu</span></p><p>该论文提出了一种基于单目视觉的机器人导航方法，通过将噪声较大的估计深度作为输入，联合学习概率化碰撞模型和风险度量，并将其集成到风险感知MPC规划器中。方法利用PointNet++提取点云特征，联合控制序列，通过神经网络输出轨迹最小障碍物间隙分布，联合下游风险监督优化模型不确定性，实现安全高效导航。实验表明，该方法在复杂环境下成功率显著高于ROS导航和NoMaD，并能有效规避碰撞。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-A-Hybrid-Force-Position-Strategy-for-Shape-Control-of-Deformable-Linear-Objects-With-Graph-Attention-Networks"><a href="#39-A-Hybrid-Force-Position-Strategy-for-Shape-Control-of-Deformable-Linear-Objects-With-Graph-Attention-Networks" class="headerlink" title="39. A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Hybrid_Force-Position_Strategy_for_Shape_Control_of_Deformable_Linear_Objects_With_Graph_Attention.pdf">A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了一种面向可变形线状物体（DLO）形状控制的混合力-位控制策略，融合了基于图注意力网络（GAT）的动力学建模及力-位双模态状态表示。该方法通过力空间中的轨迹规划与位置空间中的模型预测控制（MPC）相结合，有效分解大变形任务，避免陷入局部最优。实验证明，该策略在仿真和现实场景下均能显著提升DLO操控的效率与稳定性，确保形状过渡平滑且鲁棒。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-Integrating-Neurosymbolic-AI-in-Advanced-Air-Mobility-A-Comprehensive-Survey"><a href="#40-Integrating-Neurosymbolic-AI-in-Advanced-Air-Mobility-A-Comprehensive-Survey" class="headerlink" title="40. Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Integrating_Neurosymbolic_AI_in_Advanced_Air_Mobility__A_Comprehensive_Survey.pdf">Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Maryland, Baltimore County</span></p><p>该论文系统综述了Neurosymbolic AI（结合神经网络学习和符号推理）在先进空中移动（AAM）中的应用，包括需求预测、飞机设计、自动化管控、实时交通管理等关键领域，重点分析了Neurosymbolic Reinforcement Learning等方法在动态优化中的潜力和面临的可扩展性、稳健性及合规性挑战。论文结论认为，Neurosymbolic AI能显著提升AAM系统的透明性、安全性和适应性，但需持续推动跨学科合作与标准化，以解决数据融合、网络安全和认证等核心问题，助力下一代航空移动解决方案的落地。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-ODYSSEY-Open-World-Quadrupeds-Exploration-and-Manipulation-for-Long-Horizon-Tasks"><a href="#41-ODYSSEY-Open-World-Quadrupeds-Exploration-and-Manipulation-for-Long-Horizon-Tasks" class="headerlink" title="41. ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ODYSSEY__Open-World_Quadrupeds_Exploration_and_Manipulation_for_Long-Horizon_Tasks.pdf">ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>ODYSSEY提出了一个统一的移动操控框架，将层次化的视觉-语言任务规划与地形自适应全身控制结合，专为具备机械臂的敏捷四足机器人设计。系统通过视觉-语言模型驱动的分层规划和强化学习训练的全身控制策略，实现了在多样复杂地形下的长期任务分解与精确动作执行，并通过丰富室内外场景的仿真与实地测试验证了其通用性和鲁棒性。结论表明该方法能显著提升移动机器人在开放环境中的探索与操作能力，为通用型机器人助手的实际部署奠定基础。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-BeyondMimic-From-Motion-Tracking-to-Versatile-Humanoid-Control-via-Guided-Diffusion"><a href="#42-BeyondMimic-From-Motion-Tracking-to-Versatile-Humanoid-Control-via-Guided-Diffusion" class="headerlink" title="42. BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BeyondMimic__From_Motion_Tracking_to_Versatile_Humanoid_Control_via_Guided_Diffusion.pdf">BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Berkeley</span></p><p>本文提出了BeyondMimic框架，实现了从人类动作捕捉数据到多样化仿人机器人控制的端到端流程。方法上，首先通过统一的MDP和超参数训练高质量、高动态的运动跟踪策略，并利用离线知识蒸馏和引导扩散模型，将多种运动技能合成为单一策略，实现零样本下任务自适应控制，支持导航、遥控和避障等多任务。实验表明，该方法在真实硬件上展现出强健性、泛化性和人类动作风格，首次实现了极具挑战性的连续动态动作的仿人机器人真实执行。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-Spatial-ORMLLM-Improve-Spatial-Relation-Understanding-in-the-Operating-Room-with-Multimodal-Large-Language-Model"><a href="#43-Spatial-ORMLLM-Improve-Spatial-Relation-Understanding-in-the-Operating-Room-with-Multimodal-Large-Language-Model" class="headerlink" title="43. Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Spatial-ORMLLM__Improve_Spatial_Relation_Understanding_in_the_Operating_Room_with_Multimodal_Large_L.pdf">Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hunan University</span></p><p>Spatial-ORMLLM提出了一种用于手术室场景下3D空间关系理解的多模态大语言模型，仅依靠RGB图像，通过3D Spatial Block生成深度图、全景分割和点云等伪模态，利用空间增强特征融合块将多模态特征统一投影至LLM的token空间，实现细粒度空间推理和场景图生成。实验显示，该方法在空间推理和场景图任务上超越现有2D和3D视觉-语言模型，验证了在模态受限条件下可实现高精度空间理解和结构化场景建模。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Vision-Based-Localization-and-LLM-based-Navigation-for-Indoor-Environments"><a href="#44-Vision-Based-Localization-and-LLM-based-Navigation-for-Indoor-Environments" class="headerlink" title="44. Vision-Based Localization and LLM-based Navigation for Indoor Environments"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Vision-Based_Localization_and_LLM-based_Navigation_for_Indoor_Environments.pdf">Vision-Based Localization and LLM-based Navigation for Indoor Environments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Brown University</span></p><p>本论文提出了一种融合视觉定位与大语言模型（LLM）导航的室内导航系统：通过两阶段微调的ResNet-50卷积神经网络对智能手机摄像头图像进行定位，随后结合预处理楼层地图和ChatGPT大语言模型，生成逐步导航指令。实验结果表明，在复杂走廊环境下，视觉定位模块具备96%准确率，LLM导航模块平均指令准确率为75%，表明该方法有望实现资源有限环境下的可扩展、无基础设施室内导航，但导航模块仍受限于空间推理能力和响应速度。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-COM-PACT-COMponent-Aware-Pruning-for-Accelerated-Control-Tasks-in-Latent-Space-Models"><a href="#45-COM-PACT-COMponent-Aware-Pruning-for-Accelerated-Control-Tasks-in-Latent-Space-Models" class="headerlink" title="45. COM-PACT: COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/COMponent-Aware_Pruning_for_Accelerated_Control_Tasks_in_Latent_Space_Models.pdf">COM-PACT: COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">RPTU University Kaiserslautern-Landau</span></p><p>本文提出了一种基于组件感知的结构化剪枝方法，通过优化各组件的剪枝比例，实现神经网络控制器（NNC）在嵌入式硬件上的高效部署，并以Lyapunov稳定性为约束，确保压缩后模型仍能满足控制系统的稳定性需求。实验表明，该方法在TD-MPC控制任务中能显著减少模型规模，维持关键稳定性界限，并揭示不同组件对系统稳定性的敏感性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-AimBot-A-Simple-Auxiliary-Visual-Cue-to-Enhance-Spatial-Awareness-of-Visuomotor-Policies"><a href="#46-AimBot-A-Simple-Auxiliary-Visual-Cue-to-Enhance-Spatial-Awareness-of-Visuomotor-Policies" class="headerlink" title="46. AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AimBot__A_Simple_Auxiliary_Visual_Cue_to_Enhance_Spatial_Awareness_of_Visuomotor_Policies.pdf">AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Michigan</span></p><p>该论文提出了AimBot，一种轻量级视觉增强方法，通过在多视角RGB图像上叠加射线和准星，显式编码机械臂末端执行器的空间状态，增强视觉运动策略在机器人操作中的空间感知。AimBot无需更改模型架构，几乎不增加计算开销，实验表明其在模拟与真实环境中均能显著提升多种视觉语言动作模型的任务成功率，尤其在复杂长时序操作任务中效果突出。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-Grid2Guide-A-Enabled-Small-Language-Model-for-Indoor-Navigation"><a href="#47-Grid2Guide-A-Enabled-Small-Language-Model-for-Indoor-Navigation" class="headerlink" title="47. Grid2Guide: A* Enabled Small Language Model for Indoor Navigation"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Grid2Guide__A__Enabled_Small_Language_Model_for_Indoor_Navigation.pdf">Grid2Guide: A* Enabled Small Language Model for Indoor Navigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Alabama</span></p><p>该论文提出Grid2Guide，一个结合A<em>搜索算法与小型语言模型（SLM）的室内导航方法。流程包括将楼层平面图转化为二值占用网格，通过A</em>算法规划最优路径，并用SLM将压缩后的路径指令转化为自然语言导航指令。实验表明，该方法在多种实际场景下实现了高精度、低延迟的实时导航，优于纯LLM方案，适合手持设备部署。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-PCHands-PCA-based-Hand-Pose-Synergy-Representation-on-Manipulators-with-N-DoF"><a href="#48-PCHands-PCA-based-Hand-Pose-Synergy-Representation-on-Manipulators-with-N-DoF" class="headerlink" title="48. PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PCHands__PCA-based_Hand_Pose_Synergy_Representation_on_Manipulators_with_N-DoF.pdf">PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Istituto Italiano di Tecnologia (IIT)</span></p><p>本文提出了PCHands框架，通过Anchor Description Format (ADF)、条件变分自编码器(CVAE)和主成分分析(PCA)结合，统一提取不同机械手（包括人手、类人手、二指夹爪等）姿态协同的可变长度表征。PCHands不仅提升了基于RL的灵巧操作任务的学习效率和一致性，还能在不同机械手之间实现高效的操作策略和演示迁移，并在真实机器人上实现了零样本迁移，验证了其实用性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space"><a href="#49-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space" class="headerlink" title="49. MolmoAct: Action Reasoning Models that can Reason in Space"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MolmoAct__Action_Reasoning_Models_that_can_Reason_in_Space.pdf">MolmoAct: Action Reasoning Models that can Reason in Space</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Allen Institute for AI</span></p><p>MolmoAct提出了一种结构化三阶段推理流程的开放式行动推理模型（ARM），集成了感知、规划和控制：首先将图像和语言指令编码为深度感知Token，然后生成可编辑的空间轨迹表示，最后预测精确的机器人底层动作，提升了模型的可解释性和可控性。实验表明，MolmoAct在SimperEnv、LIBERO等仿真及真实环境下超越主流基线，具备优异的零样本泛化、快速适应和空间推理能力，并通过发布全套模型权重与数据集推动社区发展。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-DETACH-Cross-domain-Learning-for-Long-Horizon-Tasks-via-Mixture-of-Disentangled-Experts"><a href="#50-DETACH-Cross-domain-Learning-for-Long-Horizon-Tasks-via-Mixture-of-Disentangled-Experts" class="headerlink" title="50. DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DETACH__Cross-domain_Learning_for_Long-Horizon_Tasks_via_Mixture_of_Disentangled_Experts.pdf">DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing University of Technology</span></p><p>该论文提出DETACH框架，通过受生物启发的双流模块（环境编码器和自我编码器）实现环境感知与自我状态的功能解耦，结合多策略自适应特征融合机制，显著提升跨域泛化和技能复用能力。实验在多个人-场景交互长时序任务中验证，DETACH在任务成功率和执行效率上均超过现有方法，特别是在跨环境和多技能组合场景下表现优异。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-Touch-Speaks-Sound-Feels-A-Multimodal-Approach-to-Affective-and-Social-Touch-from-Robots-to-Humans"><a href="#51-Touch-Speaks-Sound-Feels-A-Multimodal-Approach-to-Affective-and-Social-Touch-from-Robots-to-Humans" class="headerlink" title="51. Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Touch_Speaks,_Sound_Feels__A_Multimodal_Approach_to_Affective_and_Social_Touch_from_Robots_to_Humans.pdf">Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ghent University</span></p><p>本文提出了一种机器人对人类的多模态情感与社交触觉交互系统，结合25点阵震动马达和音频同步反馈，实现触觉与听觉信息的融合；通过实验让32名中国参与者解码由机器人传递的10种情感和6种社交手势，分别在单一触觉、单一听觉及两者结合下进行识别。结果显示，触觉与听觉结合方式显著提升了情感识别的准确性，单一通道各有优势但难以区分部分低唤醒或相近情感，手势识别总体优于情感识别。研究强调多感官融合对于提升人机情感交互的必要性，并为社交型机器人设计提供了依据。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-AgentWorld-An-Interactive-Simulation-Platform-for-Scene-Construction-and-Mobile-Robotic-Manipulation"><a href="#52-AgentWorld-An-Interactive-Simulation-Platform-for-Scene-Construction-and-Mobile-Robotic-Manipulation" class="headerlink" title="52. AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AgentWorld__An_Interactive_Simulation_Platform_for_Scene_Construction_and_Mobile_Robotic_Manipulatio.pdf">AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent Robotics X</span></p><p>该论文提出AgentWorld，一个集成程序化场景构建和移动机器人操作的数据收集仿真平台，包含高保真3D资产库、物理属性仿真、移动和双臂&#x2F;灵巧手远程控制系统。通过搭建多样家庭环境与收集多阶段操作轨迹，并在模仿学习算法（如行为克隆、ACT、扩散策略、视觉语言动作模型）上基准测试，实现了有效的仿真到现实迁移。结论表明AgentWorld能大幅提升复杂家居环境下机器人泛化与技能学习能力，为实际部署提供了坚实基础。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-NeeCo-Image-Synthesis-of-Novel-Instrument-States-Based-on-Dynamic-and-Deformable-3D-Gaussian-Reconstruction"><a href="#53-NeeCo-Image-Synthesis-of-Novel-Instrument-States-Based-on-Dynamic-and-Deformable-3D-Gaussian-Reconstruction" class="headerlink" title="53. NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/NeeCo__Image_Synthesis_of_Novel_Instrument_States_Based_on_Dynamic_and_Deformable_3D_Gaussian_Recons.pdf">NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Leeds</span></p><p>本文提出NeeCo框架，利用动态可变形3D高斯重建技术，通过学习手术器械运动学和高斯渲染，自动合成不同姿态和变形下的手术器械高质量图像及其标注，并引入动态训练调整策略以应对真实场景下的相机姿态误差。实验显示，NeeCo生成的数据集在提升手术器械检测与分割神经网络性能方面优于SOTA方法，且自动标注精度高，有效解决了医学图像数据稀缺与标注难题。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-Autonomous-Navigation-of-Cloud-Controlled-Quadcopters-in-Confined-Spaces-Using-Multi-Modal-Perception-and-LLM-Driven-High-Semantic-Reasoning"><a href="#54-Autonomous-Navigation-of-Cloud-Controlled-Quadcopters-in-Confined-Spaces-Using-Multi-Modal-Perception-and-LLM-Driven-High-Semantic-Reasoning" class="headerlink" title="54. Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Autonomous_Navigation_of_Cloud-Controlled_Quadcopters_in_Confined_Spaces_Using_Multi-Modal_Perceptio.pdf">Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Rajshahi University of Engineering and Technology</span></p><p>本文提出了一种用于GPS-denied室内环境自主无人机导航的多模态感知与云端大模型决策系统。方法包括YOLOv11目标检测、Depth Anything V2单目深度估计、定制PCB集成ToF和IMU传感器、多线程架构、VLM场景理解，以及云端微调LLM进行高语义导航决策。实验表明，该系统在42次试验中仅发生16次防护罩突破，端到端延迟低于1秒，导航命令准确率达68%，实现了鲁棒的室内自主飞行和安全避障。结论：集成多模态感知与大模型语义推理显著提升了室内无人机的智能自主导航能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-Being-M0-5-A-Real-Time-Controllable-Vision-Language-Motion-Model"><a href="#55-Being-M0-5-A-Real-Time-Controllable-Vision-Language-Motion-Model" class="headerlink" title="55. Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Being-M0.5__A_Real-Time_Controllable_Vision-Language-Motion_Model.pdf">Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CASIA</span></p><p>该论文提出了Being-M0.5，一个具备实时、可控性的人体运动生成的视觉-语言-动作模型（VLMM），采用大规模多模态数据集HuMo100M和创新的部位感知残差量化（PRQ）技术，实现对身体各部位的细粒度控制。实验表明，该模型在多种动作生成基准上均达到了最新最优性能，并具备实际部署的高效率和广泛适应性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-Learning-Satellite-Attitude-Dynamics-with-Physics-Informed-Normalising-Flow"><a href="#56-Learning-Satellite-Attitude-Dynamics-with-Physics-Informed-Normalising-Flow" class="headerlink" title="56. Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_Satellite_Attitude_Dynamics_with_Physics-Informed_Normalising_Flow.pdf">Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Politecnico di Torino</span></p><p>本论文提出了一种结合物理约束和数据驱动的Real NVP正则化流神经网络（结合自注意力机制），用于学习卫星姿态动力学，并通过引入物理信息损失函数（基于Lagrangian dual方法动态调节权重）提升模型泛化和鲁棒性。实验证明，该方法在状态预测和嵌入MPC控制框架中的表现均优于纯数据驱动模型，平均相对误差降低27.08%~90.22%，闭环控制鲁棒性和稳定性提升显著。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-SwarmVLM-VLM-Guided-Impedance-Control-for-Autonomous-Navigation-of-Heterogeneous-Robots-in-Dynamic-Warehousing"><a href="#57-SwarmVLM-VLM-Guided-Impedance-Control-for-Autonomous-Navigation-of-Heterogeneous-Robots-in-Dynamic-Warehousing" class="headerlink" title="57. SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SwarmVLM__VLM-Guided_Impedance_Control_for_Autonomous_Navigation_of_Heterogeneous_Robots_in_Dynamic_.pdf">SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Skolkovo Institute of Science and Technology</span></p><p>该论文提出了SwarmVLM系统，将人工势场（APF）路径规划、虚拟阻抗控制、视觉语言模型（VLM）与检索增强生成（RAG）框架相结合，实现无人机与地面移动机器人在动态仓储环境下的协作导航。实验结果显示，该系统能根据障碍物分布自适应调整阻抗参数，真实场景下导航成功率达92%，VLM-RAG模块在良好光照下物体检测及参数选择准确率为80%。结论表明：该方法显著提升了异构机器人在密集动态环境中的安全与智能导航能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-Risk-Map-As-Middleware-Towards-Interpretable-Cooperative-End-to-end-Autonomous-Driving-for-Risk-Aware-Planning"><a href="#58-Risk-Map-As-Middleware-Towards-Interpretable-Cooperative-End-to-end-Autonomous-Driving-for-Risk-Aware-Planning" class="headerlink" title="58. Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Risk_Map_As_Middleware__Towards_Interpretable_Cooperative_End-to-end_Autonomous_Driving_for_Risk-Awa.pdf">Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tongji University</span></p><p>本文提出了Risk Map as Middleware（RiskMM），一种结合多智能体时空协作感知、Transformer架构与显式风险建模的端到端自动驾驶框架。RiskMM利用多车及基础设施的感知数据，生成可解释的时空风险图，并通过学习型MPC模块，在物理约束下进行风险感知的轨迹规划。实验表明，RiskMM在V2XPnP-Seq真实数据集上大幅提升了规划安全性和解释性，优于现有方法，并具备良好鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-AIS-LLM-A-Unified-Framework-for-Maritime-Trajectory-Prediction-Anomaly-Detection-and-Collision-Risk-Assessment-with-Explainable-Forecasting"><a href="#59-AIS-LLM-A-Unified-Framework-for-Maritime-Trajectory-Prediction-Anomaly-Detection-and-Collision-Risk-Assessment-with-Explainable-Forecasting" class="headerlink" title="59. AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AIS-LLM__A_Unified_Framework_for_Maritime_Trajectory_Prediction,_Anomaly_Detection,_and_Collision_Ri.pdf">AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hanbat National University</span></p><p>该论文提出AIS-LLM框架，将多尺度AIS时序数据与大语言模型（LLM）结合，采用多任务学习结构同时实现船舶轨迹预测、异常检测和碰撞风险评估，并支持数值预测和自然语言解释。实验显示AIS-LLM在所有任务上均优于现有方法，显著提升了海事交通分析的准确性和可解释性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-End-to-End-Humanoid-Robot-Safe-and-Comfortable-Locomotion-Policy"><a href="#60-End-to-End-Humanoid-Robot-Safe-and-Comfortable-Locomotion-Policy" class="headerlink" title="60. End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/End-to-End_Humanoid_Robot_Safe_and_Comfortable_Locomotion_Policy.pdf">End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (Guangzhou)</span></p><p>本文提出了一种端到端的仿人机器人行走策略，利用原始时空LiDAR点云，经由GRU-MLP结构直接映射至机器人电机命令，实现复杂3D动态场景下的安全舒适导航。方法将控制障碍函数（CBF）原理融入约束马尔可夫决策过程（CMDP）并采用P3O算法训练，同时引入基于人机交互的舒适性奖励。实验结果表明，该策略在仿真和真实机器人上均可实现高效、安全、平滑且符合社会规范的避障行走，显著优于基线方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-Training-Free-ANN-to-SNN-Conversion-for-High-Performance-Spiking-Transformer"><a href="#61-Training-Free-ANN-to-SNN-Conversion-for-High-Performance-Spiking-Transformer" class="headerlink" title="61. Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Training-Free_ANN-to-SNN_Conversion_for_High-Performance_Spiking_Transformer.pdf">Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>该论文提出了面向Transformer架构的无训练人工神经网络（ANN）到脉冲神经网络（SNN）转换框架，核心方法是多基指数衰减（MBE）神经元，通过指数衰减和多基编码精准拟合Transformer中的非线性操作（如GELU、Softmax、LayerNorm、浮点乘法），无需修改预训练ANN权重。实验覆盖CV、NLU、NLG三大任务和ViT、RoBERTa、GPT-2等主流模型，结果显示该方法在保持近乎无损精度的同时显著降低推理延迟，为高效可扩展部署脉冲Transformer提供新思路。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-LAURON-VI-A-Six-Legged-Robot-for-Dynamic-Walking"><a href="#62-LAURON-VI-A-Six-Legged-Robot-for-Dynamic-Walking" class="headerlink" title="62. LAURON VI: A Six-Legged Robot for Dynamic Walking"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LAURON_VI__A_Six-Legged_Robot_for_Dynamic_Walking.pdf">LAURON VI: A Six-Legged Robot for Dynamic Walking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">FZI Research Center for Information Technology</span></p><p>本文提出了六足机器人平台LAURON VI，采用18个串联弹性关节驱动器，并设计实现了三种动态行走控制器：基于运动学的方法、模型预测控制（MPC）、和强化学习策略。实验表明，三种控制策略均能在不同环境下实现灵活且高效的运动，显著提升了六足机器人在复杂地形和实际任务中的速度和适应性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-GraphCoT-VLA-A-3D-Spatial-Aware-Reasoning-Vision-Language-Action-Model-for-Robotic-Manipulation-with-Ambiguous-Instructions"><a href="#63-GraphCoT-VLA-A-3D-Spatial-Aware-Reasoning-Vision-Language-Action-Model-for-Robotic-Manipulation-with-Ambiguous-Instructions" class="headerlink" title="63. GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/GraphCoT-VLA__A_3D_Spatial-Aware_Reasoning_Vision-Language-Action_Model_for_Robotic_Manipulation_wit.pdf">GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Noah’s Ark Lab, Huawei</span></p><p>本文提出了一种名为GraphCoT-VLA的端到端机器人操作模型，通过引入结构化Chain-of-Thought推理模块和实时可更新的3D Pose-Object图，实现了对多视角视觉、语言指令及机器人状态的联合理解和动态推理。实验结果表明，该模型在面对模糊指令和开放环境时，在任务成功率、响应速度和泛化能力方面均显著优于现有方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="64-Sensory-Robustness-through-Top-Down-Feedback-and-Neural-Stochasticity-in-Recurrent-Vision-Models"><a href="#64-Sensory-Robustness-through-Top-Down-Feedback-and-Neural-Stochasticity-in-Recurrent-Vision-Models" class="headerlink" title="64. Sensory Robustness through Top-Down Feedback and Neural Stochasticity in Recurrent Vision Models"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sensory_robustness_through_top-down_feedback_and_neural_stochasticity_in_recurrent_vision_models.pdf">Sensory Robustness through Top-Down Feedback and Neural Stochasticity in Recurrent Vision Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Department of Neural Dynamics and Magnetoencephalography, Hertie Institute for Clinical Brain Research, University of Tübingen</span></p><p>本文提出了一种结合卷积递归神经网络（ConvRNN）中的顶层-底层反馈通路和神经元随机性（通过dropout实现）的视觉模型，并在图像分类任务中系统比较了有无反馈和有无随机性的不同架构。研究发现：只有同时具备顶层反馈和神经元随机性时，模型才能在速度-准确率权衡、噪声扰动和对抗攻击等分布外场景下表现出显著的鲁棒性和高效性。进一步分析显示，反馈信号能显著塑造网络的表示空间，将特征约束到低维流形上，提升泛化能力和稳定性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-TeSO-Representing-and-Compressing-3D-Point-Cloud-Scenes-with-Textured-Surfel-Octree"><a href="#65-TeSO-Representing-and-Compressing-3D-Point-Cloud-Scenes-with-Textured-Surfel-Octree" class="headerlink" title="65. TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TeSO__Representing_and_Compressing_3D_Point_Cloud_Scenes_with_Textured_Surfel_Octree.pdf">TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">New York University</span></p><p>该论文提出了Textured Surfel Octree (TeSO)，一种结合点云和网格优势的新型3D场景表示方法。其流程包括利用点云构建八叉树结构的表面元素（surfel），并为每个surfel分配纹理贴图，同时设计了基于神经网络的熵模型和标准编解码器实现高效压缩。实验结果表明，TeSO在低比特率下可实现比传统点云和3D高斯基线更高质量的渲染，支持实时渲染且更适合3D内容流媒体和XR应用。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-BrainATCL-Adaptive-Temporal-Brain-Connectivity-Learning-for-Functional-Link-Prediction-and-Age-Estimation"><a href="#66-BrainATCL-Adaptive-Temporal-Brain-Connectivity-Learning-for-Functional-Link-Prediction-and-Age-Estimation" class="headerlink" title="66. BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BrainATCL__Adaptive_Temporal_Brain_Connectivity_Learning_for_Functional_Link_Prediction_and_Age_Esti.pdf">BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">New Jersey Institute of Technology</span></p><p>该论文提出了一种无监督、自适应的时序脑连接学习方法BrainATCL，通过基于新颖性指数动态调整每个时间点的历史窗口长度，并结合结构&#x2F;功能相关的边属性，利用GINE-Mamba2骨干网络编码fMRI动态功能连接时序图，实现了功能连接预测和年龄估计。实验表明，BrainATCL在Human Connectome Project数据上对功能连接预测和年龄估计任务表现优异，泛化能力强，超越了现有静态和动态图学习基线。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="67-Large-Language-Model-Evaluated-Stand-alone-Attention-Assisted-Graph-Neural-Network-with-Spatial-and-Structural-Information-Interaction-for-Precise-Endoscopic-Image-Segmentation"><a href="#67-Large-Language-Model-Evaluated-Stand-alone-Attention-Assisted-Graph-Neural-Network-with-Spatial-and-Structural-Information-Interaction-for-Precise-Endoscopic-Image-Segmentation" class="headerlink" title="67. Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Large_Language_Model_Evaluated_Stand-alone_Attention-Assisted_Graph_Neural_Network_with_Spatial_and_.pdf">Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong</span></p><p>本文提出FOCUS-Med，一种融合空间和结构图、带有注意力机制的图神经网络方法，用于内镜图像中精确息肉分割。方法包括Dual-GCN模块用于空间和拓扑特征提取、位置融合自注意力机制进行全局上下文整合、以及加权快速归一化融合实现多尺度特征聚合，并首次引入大型语言模型（LLM）辅助分割结果的专家级定性评估。实验结果显示，FOCUS-Med在多个公开医学图像数据集上分割性能超越现有先进模型，具有较高精度和临床应用潜力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-TerraMAE-Learning-Spatial-Spectral-Representations-from-Hyperspectral-Earth-Observation-Data-via-Adaptive-Masked-Autoencoders"><a href="#68-TerraMAE-Learning-Spatial-Spectral-Representations-from-Hyperspectral-Earth-Observation-Data-via-Adaptive-Masked-Autoencoders" class="headerlink" title="68. TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TerraMAE__Learning_Spatial-Spectral_Representations_from_Hyperspectral_Earth_Observation_Data_via_Ad.pdf">TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Colorado State University</span></p><p>本文提出TerraMAE，一种专为高光谱遥感影像设计的自监督Masked Autoencoder预训练框架，通过基于统计反射特性的自适应通道分组和结合空间结构与光谱一致性的复合损失函数，实现空间-光谱特征的高效建模。实验结果表明，TerraMAE在高保真影像重建与下游土壤质地预测、作物类型识别和土地覆盖分类等地理空间任务中均显著优于传统MAE和ResNet-50等基线方法，具有很强的泛化和迁移能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-OctreeNCA-Single-Pass-184-MP-Segmentation-on-Consumer-Hardware"><a href="#69-OctreeNCA-Single-Pass-184-MP-Segmentation-on-Consumer-Hardware" class="headerlink" title="69. OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/OctreeNCA__Single-Pass_184_MP_Segmentation_on_Consumer_Hardware.pdf">OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Darmstadt</span></p><p>本文提出了OctreeNCA，一种基于八叉树结构的神经元元胞自动机（NCA）分割架构，通过分层方式快速传播全局上下文并进行高分辨率医学图像和视频的端到端分割。该方法结合自定义CUDA推理内核，显著降低了推理所需的显存（VRAM），实现了在消费级硬件上对184MP病理切片和一分钟外科手术视频的单步高效分割。实验表明，OctreeNCA在分割精度上可媲美现有SOTA模型，同时大幅降低参数量和推理显存需求，适合低成本设备部署，有助于缩小全球医疗硬件差距。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-TADoc-Robust-Time-Aware-Document-Image-Dewarping"><a href="#70-TADoc-Robust-Time-Aware-Document-Image-Dewarping" class="headerlink" title="70. TADoc: Robust Time-Aware Document Image Dewarping"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TADoc__Robust_Time-Aware_Document_Image_Dewarping.pdf">TADoc: Robust Time-Aware Document Image Dewarping</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Information Engineering, Chinese Academy of Sciences</span></p><p>本文提出了TADoc方法，将文档图像去畸变任务首次建模为一个多步骤的动态过程，通过时间嵌入和轻量级网络逐步预测不同程度的畸变恢复。提出了新的评测指标DLS用于评估去畸变效果在下游任务中的表现。实验结果显示，TADoc在多个公开基准上具有强鲁棒性和领先性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-WeatherDiffusion-Weather-Guided-Diffusion-Model-for-Forward-and-Inverse-Rendering"><a href="#71-WeatherDiffusion-Weather-Guided-Diffusion-Model-for-Forward-and-Inverse-Rendering" class="headerlink" title="71. WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/WeatherDiffusion__Weather-Guided_Diffusion_Model_for_Forward_and_Inverse_Rendering.pdf">WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>该论文提出WeatherDiffusion，一种基于大规模扩散模型的自动驾驶场景前向与逆向渲染框架，通过天气控制器和文本提示实现多天气与照明条件下的高质量图像生成与分解。引入Intrinsic Map-Aware Attention（MAA）模块，结合新构建的合成与真实数据集，显著提升逆向渲染精度和下游目标检测与分割在恶劣天气下的鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-Beyond-Frequency-Seeing-Subtle-Cues-Through-the-Lens-of-Spatial-Decomposition-for-Fine-Grained-Visual-Classification"><a href="#72-Beyond-Frequency-Seeing-Subtle-Cues-Through-the-Lens-of-Spatial-Decomposition-for-Fine-Grained-Visual-Classification" class="headerlink" title="72. Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Beyond_Frequency__Seeing_Subtle_Cues_Through_the_Lens_of_Spatial_Decomposition_for_Fine-Grained_Visu.pdf">Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Anhui University</span></p><p>该论文提出了一种全新的细粒度视觉分类方法SCOPE（Subtle-Cue Oriented Perception Engine），通过空间自适应滤波替代传统频域变换，其中包含细致特征提取模块（SDE）和显著语义优化模块（SSR），逐层增强局部细节与全局结构。实验结果表明，SCOPE在四个主流细粒度分类数据集上取得了新的SOTA表现，显著提升了对微小判别线索的识别能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-CannyEdit-Selective-Canny-Control-and-Dual-Prompt-Guidance-for-Training-free-Image-Editing"><a href="#73-CannyEdit-Selective-Canny-Control-and-Dual-Prompt-Guidance-for-Training-free-Image-Editing" class="headerlink" title="73. CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-free Image Editing"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CannyEdit__Selective_Canny_Control_and_Dual-Prompt_Guidance_for_Training-Free_Image_Editing.pdf">CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-free Image Editing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>本文提出了CannyEdit，一种基于训练自由的区域图像编辑方法，结合Selective Canny Control和Dual-Prompt Guidance技术。Selective Canny Control通过对用户指定的编辑区域屏蔽Canny ControlNet结构引导，同时严格保留未编辑区域的原始细节，实现精确的文本驱动编辑；Dual-Prompt Guidance则融合局部和全局文本提示，确保编辑区域与整体场景的语义一致性。实验结果显示，CannyEdit在文本匹配度、上下文保真度及编辑自然度上优于现有方法，用户研究表明其编辑结果更难被辨识为AI生成。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-MMReID-Bench-Unleashing-the-Power-of-MLLMs-for-Effective-and-Versatile-Person-Re-identification"><a href="#74-MMReID-Bench-Unleashing-the-Power-of-MLLMs-for-Effective-and-Versatile-Person-Re-identification" class="headerlink" title="74. MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MMReID-Bench__Unleashing_the_Power_of_MLLMs_for_Effective_and_Versatile_Person_Re-identification.pdf">MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">East China Normal University</span></p><p>本论文提出了MMReID-Bench，这是首个专为多任务和多模态场景设计的人体重识别基准，覆盖10类典型任务（如RGB、红外、草图等），并系统性评估了15种主流多模态大语言模型（MLLMs）在该基准上的表现。结果表明MLLMs在大多数人体重识别任务中取得了优异成绩，但在热成像和红外等跨模态任务上存在明显短板，亟需提升跨模态理解能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-A-Simple-yet-Powerful-Instance-Aware-Prompting-Framework-for-Training-free-Camouflaged-Object-Segmentation"><a href="#75-A-Simple-yet-Powerful-Instance-Aware-Prompting-Framework-for-Training-free-Camouflaged-Object-Segmentation" class="headerlink" title="75. A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Simple_yet_Powerful_Instance-Aware_Prompting_Framework_for_Training-free_Camouflaged_Object_Segmen.pdf">A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai University</span></p><p>该论文提出了首个训练自由的实例感知提示框架（IAPF），用于伪装物体分割。方法包括三步：利用多模态大语言模型生成前景&#x2F;背景标签，借助Grounding DINO产生多实例级别的边界框和区域点提示，并结合自一致性实例掩码投票机制提升多实例分割精度。实验表明，IAPF在多个标准伪装分割数据集上显著优于现有方法。结论：该方法无需训练即可实现高精度多实例伪装分割，提升了训练自由分割领域的表现。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-MV-CoRe-Multimodal-Visual-Conceptual-Reasoning-for-Complex-Visual-Question-Answering"><a href="#76-MV-CoRe-Multimodal-Visual-Conceptual-Reasoning-for-Complex-Visual-Question-Answering" class="headerlink" title="76. MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MV-CoRe__Multimodal_Visual-Conceptual_Reasoning_for_Complex_Visual_Question_Answering.pdf">MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shaanxi University of Technology</span></p><p>MV-CoRe提出了一种多模态视觉-概念推理模型，通过融合预训练视觉大模型和语言大模型的全局特征，以及基于目标检测和场景图的细粒度语义视觉特征，利用创新的多模态融合Transformer进行深度跨模态融合和推理。在GQA、A-OKVQA和OKVQA等复杂VQA基准上，MV-CoRe显著优于主流LVLM模型，提升了视觉理解和复杂推理能力，尤其在人类评估中展现更高事实正确性和推理深度。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-HiMat-DiT-based-Ultra-High-Resolution-SVBRDF-Generation"><a href="#77-HiMat-DiT-based-Ultra-High-Resolution-SVBRDF-Generation" class="headerlink" title="77. HiMat: DiT-based Ultra-High Resolution SVBRDF Generation"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HiMat__DiT-based_Ultra-High_Resolution_SVBRDF_Generation.pdf">HiMat: DiT-based Ultra-High Resolution SVBRDF Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nankai University</span></p><p>该论文提出HiMat，一种基于Diffusion Transformer（DiT）的高效扩散模型，用于原生4K超高分辨率SVBRDF（空间变化双向反射分布函数）材料生成。方法结合线性注意力DiT骨干与轻量级CrossStitch卷积模块，实现多通道SVBRDF一致性和高频细节保留，并采用小波域监督提升微结构细节生成。结论显示，该方法在视觉质量、结构一致性和计算效率方面均优于现有方法，实现了高质量4K材料生成，并可扩展至内在分解等任务。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-Spatio-Temporal-Conditional-Diffusion-Models-for-Forecasting-Future-Multiple-Sclerosis-Lesion-Masks-Conditioned-on-Treatments"><a href="#78-Spatio-Temporal-Conditional-Diffusion-Models-for-Forecasting-Future-Multiple-Sclerosis-Lesion-Masks-Conditioned-on-Treatments" class="headerlink" title="78. Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Spatio-Temporal_Conditional_Diffusion_Models_for_Forecasting_Future_Multiple_Sclerosis_Lesion_Masks_.pdf">Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">McGill University</span></p><p>该论文提出了首个结合ControlNet的时空条件扩散模型，用于预测多发性硬化症（MS）患者在不同治疗方案下未来MRI上的新发和增大T2病灶掩膜。方法通过将3D MRI数据重构为伪2D片，融合多模态影像和治疗信息，实现高分辨率、体素级预测。实验显示模型在六种治疗组下效果优于基于统计的基线，并能支持临床相关任务如病灶计数、位置估计及反事实生成。结论：模型能准确预测未来病灶并辅助个体化医疗决策，展现了生成式AI在医学影像个性化预测中的应用潜力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-Can-Multitask-Learning-Enhance-Model-Explainability"><a href="#79-Can-Multitask-Learning-Enhance-Model-Explainability" class="headerlink" title="79. Can Multitask Learning Enhance Model Explainability?"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Can_Multitask_Learning_Enhance_Model_Explainability_.pdf">Can Multitask Learning Enhance Model Explainability?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kaiserslautern-Landau University</span></p><p>本文提出一种通过多任务学习增强模型可解释性的框架，针对遥感多模态数据，将部分输入模态转为辅助预测任务，主干采用多模态编码器融合卫星、气象等信息，分别在分割、分类和回归任务上进行实验。结果显示该方法在主任务性能基本持平甚至优于多模态基线，同时无需在部署阶段增加输入数据，并通过分析主任务与辅助任务的误差相关性，提升了模型行为的内在解释性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-Intrinsic-Explainability-of-Multimodal-Learning-for-Crop-Yield-Prediction"><a href="#80-Intrinsic-Explainability-of-Multimodal-Learning-for-Crop-Yield-Prediction" class="headerlink" title="80. Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Intrinsic_Explainability_of_Multimodal_Learning_for_Crop_Yield_Prediction.pdf">Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">RPTU Kaiserslautern-Landau</span></p><p>本文提出基于Transformer的多模态学习框架，用于作物产量预测，融合多时序卫星、气象、土壤和地形等四类数据，并系统性比较了Attention Rollout（AR）、Generic Attention（GA）和Shapley Value Sampling（SVS）三种特征归因方法。实验结果表明，Transformer在子田块和田块层面均优于卷积和循环网络，AR方法归因结果更稳定且更具一致性，揭示关键作物生长阶段与模型决策高度相关。模型在多区域多作物数据上验证了解释性方法的有效性，同时提出的WMA方法对多模态重要性分析显示与Shapley估计存在较大差异，需进一步研究。结论认为，Transformer内在可解释性和AR方法能提升多模态网络的透明度，推动农业遥感等数据密集领域的可解释AI发展。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-eMotions-A-Large-Scale-Dataset-and-Audio-Visual-Fusion-Network-for-Emotion-Analysis-in-Short-form-Videos"><a href="#81-eMotions-A-Large-Scale-Dataset-and-Audio-Visual-Fusion-Network-for-Emotion-Analysis-in-Short-form-Videos" class="headerlink" title="81. eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/eMotions__A_Large-Scale_Dataset_and_Audio-Visual_Fusion_Network_for_Emotion_Analysis_in_Short-form_V.pdf">eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>该论文提出了eMotions——首个专为短视频情感分析的大规模数据集，包含27,996个视频并配有六种情绪标签，同时发布了AV-CANet模型（基于Video Swin-Transformer和ResNet34）实现音视频特征融合，通过设计Local-Global Fusion模块和EP-CE损失函数提升情感识别性能。实验表明，所提方法在多个基准和公开数据集上均优于现有方法，并为未来短视频情感分析研究提供了基础。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-VSI-Visual–Subtitle-Integration-for-Keyframe-Selection-to-Enhance-Long-Video-Understanding"><a href="#82-VSI-Visual–Subtitle-Integration-for-Keyframe-Selection-to-Enhance-Long-Video-Understanding" class="headerlink" title="82. VSI: Visual–Subtitle Integration for Keyframe Selection to Enhance Long Video Understanding"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VSI__Visual_Subtitle_Integration_for_Keyframe_Selection_to_enhance_Long_Video_Understanding.pdf">VSI: Visual–Subtitle Integration for Keyframe Selection to Enhance Long Video Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">HKUST(GZ)</span></p><p>该论文提出了VISUAL-SUBTITLE INTEGRATION (VSI)方法，通过视觉对象检测和字幕语义匹配的双流融合机制，实现长视频关键帧检索。该方法无需额外训练，能高效整合视觉与文本信息，提升关键帧定位和长视频问答准确率。实验结果显示，VSI在LONGVIDEOBENCH数据集上关键帧检索和视频问答任务均取得了领先性能，具备良好通用性和可扩展性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-Low-Rank-Expert-Merging-for-Multi-Source-Domain-Adaptation-in-Person-Re-Identification"><a href="#83-Low-Rank-Expert-Merging-for-Multi-Source-Domain-Adaptation-in-Person-Re-Identification" class="headerlink" title="83. Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Low-Rank_Expert_Merging_for_Multi-Source_Domain_Adaptation_in_Person_Re-Identification.pdf">Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">ETS Montreal</span></p><p>该论文提出了SAGE-reID，一种无需源数据的多源域自适应方法，专为行人再识别任务设计。方法包括两阶段流程：首先对每个源域模型仅微调低秩适配器（LoRA），然后通过轻量级门控网络动态融合多个LoRA专家以实现跨域知识迁移。实验在Market-1501、DukeMTMC-reID和MSMT17等基准数据集上显示，SAGE-reID在准确性和计算效率上均优于现有方法。结论：SAGE-reID可高效实现多源知识融合，提升跨域行人识别表现。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-Dual-Resolution-Residual-Architecture-with-Artifact-Suppression-for-Melanocytic-Lesion-Segmentation"><a href="#84-Dual-Resolution-Residual-Architecture-with-Artifact-Suppression-for-Melanocytic-Lesion-Segmentation" class="headerlink" title="84. Dual-Resolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DualResolution_Residual_Architecture_with_Artifact_Suppression_for_Melanocytic_Lesion_Segmentation.pdf">Dual-Resolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">SNAIT Research</span></p><p>该论文提出一种针对皮肤镜图像中黑色素瘤病变分割的新型ResNet风格双分辨率架构，包括高分辨率边界流和多尺度上下文流，结合边界感知残差连接、通道注意力和轻量伪影抑制模块，并采用Dice–Tversky分割损失、边界损失和对比正则化的多任务训练目标。实验结果显示，该方法在公开数据集上显著提升了分割精度和边界贴合度，优于常规编码器-解码器基线，有助于自动化皮肤癌评估系统。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-Hardness-Aware-Dynamic-Curriculum-Learning-for-Robust-Multimodal-Emotion-Recognition-with-Missing-Modalities"><a href="#85-Hardness-Aware-Dynamic-Curriculum-Learning-for-Robust-Multimodal-Emotion-Recognition-with-Missing-Modalities" class="headerlink" title="85. Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hardness-Aware_Dynamic_Curriculum_Learning_for_Robust_Multimodal_Emotion_Recognition_with_Missing_Mo.pdf">Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Inner Mongolia University</span></p><p>该论文提出了HARDY-MER框架，通过多视角难度评估（结合模态重建误差与跨模态互信息）量化样本难度，并利用检索增强动态课程学习策略为高难度样本检索语义相关支持样本，动态调整训练重点，从而提升模型在缺失模态下的鲁棒性和泛化能力。实验表明，该方法在IEMOCAP和CMU-MOSEI等基准数据集的多种缺失模态设定下均优于现有方法，验证了其有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-SafePLUG-Empowering-Multimodal-LLMs-with-Pixel-Level-Insight-and-Temporal-Grounding-for-Traffic-Accident-Understanding"><a href="#86-SafePLUG-Empowering-Multimodal-LLMs-with-Pixel-Level-Insight-and-Temporal-Grounding-for-Traffic-Accident-Understanding" class="headerlink" title="86. SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SafePLUG__Empowering_Multimodal_LLMs_with_Pixel-Level_Insight_and_Temporal_Grounding_for_Traffic_Acc.pdf">SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Wisconsin–Madison</span></p><p>SafePLUG提出了一种多模态大语言模型框架，结合视觉提示编码器与SAM基础的像素级分割解码器，实现交通事故的区域感知、像素级理解和基于数字标记的时序定位。通过构建多模态问答数据集，SafePLUG在区域问答、像素分割、事故描述与时序定位等任务上均超越现有模型，显著提升了复杂交通场景下的细粒度理解能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-FoundBioNet-A-Foundation-Based-Model-for-IDH-Genotyping-of-Glioma-from-Multi-Parametric-MRI"><a href="#87-FoundBioNet-A-Foundation-Based-Model-for-IDH-Genotyping-of-Glioma-from-Multi-Parametric-MRI" class="headerlink" title="87. FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FoundBioNet__A_Foundation-Based_Model_for_IDH_Genotyping_of_Glioma_from_Multi-Parametric_MRI.pdf">FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tehran University of Medical Sciences</span></p><p>本文提出了一种基础模型FoundBioNet，基于SWIN-UNETR架构，结合肿瘤感知特征编码（TAFE）和跨模态差异（CMD）模块，从多参数MRI中无创预测胶质瘤IDH突变状态。模型在六个多中心公开数据集上训练，表现出较强泛化能力，AUC最高达90.58%，显著优于传统卷积和Transformer基线方法，结果具备较高准确性和可解释性，有望用于临床个体化管理。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-FormCoach-Lift-Smarter-Not-Harder"><a href="#88-FormCoach-Lift-Smarter-Not-Harder" class="headerlink" title="88. FormCoach: Lift Smarter, Not Harder"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FormCoach__Lift_Smarter,_Not_Harder.pdf">FormCoach: Lift Smarter, Not Harder</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Pennsylvania</span></p><p>FormCoach提出了一种基于视觉-语言模型（VLMs）的实时AI健身教练系统，利用摄像头对用户动作与专家参考视频进行对比分析，生成个性化、简明的动作纠正建议。作者构建并公开了包含1,700组专家标注视频对的数据集，系统性评测了多种主流VLM模型在人体运动差异分析与反馈生成上的性能。实验发现，VLMs在行动指导性上表现良好但在准确识别细微动作失误上仍有较大提升空间，现有模型易出现遗漏或幻觉反馈。结论认为，该工作推动了AI健身辅导迈向互动、精准和多模态融合的新方向。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding"><a href="#89-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding" class="headerlink" title="89. VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VisR-Bench__An_Empirical_Study_on_Visual_Retrieval-Augmented_Generation_for_Multilingual_Long_Docume.pdf">VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University at Buffalo</span></p><p>本文提出了VisR-Bench，这是首个面向多语言长文档的视觉检索增强生成（RAG）基准，涵盖16种语言、1,286份多模态文档和35,000+高质量问答对，支持文本、表格和图像多证据类型的细粒度评测。系统性实验比较了文本检索、多模态编码器和多模态大模型（MLLMs），结果显示MLLMs在检索任务上优于传统方法，但在结构化表格内容和低资源语言上表现仍有限，揭示了多语言多模态检索的关键挑战。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-Noise-Aware-Generative-Microscopic-Traffic-Simulation"><a href="#90-Noise-Aware-Generative-Microscopic-Traffic-Simulation" class="headerlink" title="90. Noise-Aware Generative Microscopic Traffic Simulation"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Noise-Aware_Generative_Microscopic_Traffic_Simulation.pdf">Noise-Aware Generative Microscopic Traffic Simulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Massachusetts Institute of Technology</span></p><p>该论文提出了I-24 MOTION Scenario Dataset (I24-MSD)，保留基础设施摄像头采集的交通轨迹中的真实噪声，并将其作为学习问题的一部分。作者基于该数据集，使用GPT风格的Transformer模型（SMART），并引入了标签平滑、focal loss和对称交叉熵等噪声感知损失函数以提升模型在噪声环境下的生成效果。实验结果显示，噪声感知优化显著提升了微观交通仿真的真实度和鲁棒性。结论认为，显式建模数据噪声可提升仿真质量，并促进智能交通系统研究。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-CLUE-Leveraging-Low-Rank-Adaptation-to-Capture-Latent-Uncovered-Evidence-for-Image-Forgery-Localization"><a href="#91-CLUE-Leveraging-Low-Rank-Adaptation-to-Capture-Latent-Uncovered-Evidence-for-Image-Forgery-Localization" class="headerlink" title="91. CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CLUE__Leveraging_Low-Rank_Adaptation_to_Capture_Latent_Uncovered_Evidence_for_Image_Forgery_Localiza.pdf">CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen University</span></p><p>该论文提出了CLUE框架，通过低秩适应（LoRA）对Stable Diffusion 3（SD3）和Segment Anything Model（SAM）进行参数高效的联合微调，利用SD3的Rectified Flow噪声机制在潜空间放大伪造区域的微弱特征，并融合SAM图像编码器的空间语义线索，实现精确的伪造区域定位。实验结果显示，CLUE在多种公开数据集上的伪造定位表现和对后处理攻击的鲁棒性均显著优于现有方法，具备优异的泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-VA-Blueprint-Uncovering-Building-Blocks-for-Visual-Analytics-System-Design"><a href="#92-VA-Blueprint-Uncovering-Building-Blocks-for-Visual-Analytics-System-Design" class="headerlink" title="92. VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VA-Blueprint__Uncovering_Building_Blocks_for_Visual_Analytics_System_Design.pdf">VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Chicago</span></p><p>该论文提出了一套系统方法（VA-Blueprint），通过人工和大语言模型（LLM）辅助，从101篇城市视觉分析（VA）系统论文中自动抽取、归类并结构化系统组件、操作和依赖，形成多层次、可查询的知识库。结论表明，VA-Blueprint能够揭示VA系统通用设计模式和演化趋势，提升系统复用性与开发效率，同时专家评估验证了方法的准确性和实用性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-ForensicsSAM-Toward-Robust-and-Unified-Image-Forgery-Detection-and-Localization-Resisting-to-Adversarial-Attack"><a href="#93-ForensicsSAM-Toward-Robust-and-Unified-Image-Forgery-Detection-and-Localization-Resisting-to-Adversarial-Attack" class="headerlink" title="93. ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ForensicsSAM__Toward_Robust_and_Unified_Image_Forgery_Detection_and_Localization_Resisting_to_Advers.pdf">ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shenzhen University</span></p><p>该论文提出ForensicsSAM，一种面向图像篡改检测与定位的统一框架，具有内置的对抗鲁棒性。方法通过在SAM主干的每个transformer block中注入共享的forgery experts提升篡改特征提取能力，并设计轻量级adversary detector识别对抗样本，再利用自适应adversary experts对特征漂移进行修正，三阶段训练实现对干净与对抗图像的独立优化。实验表明，ForensicsSAM在多种数据集与对抗攻击下均达到了图像级检测和像素级定位的最新性能和鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-Tight-Bounds-for-Schrodinger-Potential-Estimation-in-Unpaired-Image-to-Image-Translation-Problems"><a href="#94-Tight-Bounds-for-Schrodinger-Potential-Estimation-in-Unpaired-Image-to-Image-Translation-Problems" class="headerlink" title="94. Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Tight_Bounds_for_Schr%C3%B6dinger_Potential_Estimation_in_Unpaired_Image-to-Image_Translation_Problems.pdf">Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">HSE University</span></p><p>该论文提出了一种基于Ornstein-Uhlenbeck过程的Schrödinger桥理论，用于无配对的图像到图像转换问题，通过优化经验风险最小化器估计Schrödinger潜力，并首次给出了该方法在有限样本下的非渐近高概率泛化误差界。实验证明该理论在生成建模和风格迁移任务中具有更快的收敛速度和更好的性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-Invert4TVG-A-Temporal-Video-Grounding-Framework-with-Inversion-Tasks-for-Enhanced-Action-Understanding"><a href="#95-Invert4TVG-A-Temporal-Video-Grounding-Framework-with-Inversion-Tasks-for-Enhanced-Action-Understanding" class="headerlink" title="95. Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Invert4TVG__A_Temporal_Video_Grounding_Framework_with_Inversion_Tasks_for_Enhanced_Action_Understand.pdf">Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xiamen University</span></p><p>该论文提出Invert4TVG框架，通过引入三种反转任务（动词补全、动作识别、视频描述），利用TVG原始标注，在不依赖额外数据的前提下提升模型对视频动作语义的理解。方法将TVG与Invert-TVG多任务整合进强化学习训练流程，通过动态采样和专门奖励设计，兼顾定位精度与动作理解，实验在多个数据集上显著优于现有方法，表明增强动作语义对提升时序视频定位上限具有关键作用。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-KLASSify-to-Verify-Audio-Visual-Deepfake-Detection-Using-SSL-based-Audio-and-Handcrafted-Visual-Features"><a href="#96-KLASSify-to-Verify-Audio-Visual-Deepfake-Detection-Using-SSL-based-Audio-and-Handcrafted-Visual-Features" class="headerlink" title="96. KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/KLASSify_to_Verify__Audio-Visual_Deepfake_Detection_Using_SSL-based_Audio_and_Handcrafted_Visual_Fea.pdf">KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KLASS Engineering and Solutions</span></p><p>本文提出KLASSify系统，通过手工视觉特征与基于自监督学习的音频特征，结合图注意力网络（GAT）与时序卷积网络（TCN），实现音视频多模态深度伪造检测与定位。方法在AV-Deepfake1M++数据集上，分类任务AUC达92.78%，音频定位IoU达0.3536，展现了高效、鲁棒且可解释的性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-RORPCap-Retrieval-based-Objects-and-Relations-Prompt-for-Image-Captioning"><a href="#97-RORPCap-Retrieval-based-Objects-and-Relations-Prompt-for-Image-Captioning" class="headerlink" title="97. RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/RORPCap__Retrieval-based_Objects_and_Relations_Prompt_for_Image_Captioning.pdf">RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Yunnan University</span></p><p>该论文提出了一种新颖的图像描述方法RORPCap，通过设计的对象与关系抽取模块（OREM）从检索到的相似描述句中提取关键对象和关系词，填充到模板形成prompt，并结合CLIP视觉特征经过Mamba映射网络转化为视觉-文本特征，再与prompt拼接后输入GPT-2生成描述。实验表明，RORPCap在MS-COCO等数据集上实现了与主流检测器和GCN方法可比的描述质量，但训练时间显著缩短，仅需2.6小时，具备良好零样本泛化能力，能有效降低训练成本。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-BEVANet-Bilateral-Efficient-Visual-Attention-Network-for-Real-Time-Semantic-Segmentation"><a href="#98-BEVANet-Bilateral-Efficient-Visual-Attention-Network-for-Real-Time-Semantic-Segmentation" class="headerlink" title="98. BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BEVANet__Bilateral_Efficient_Visual_Attention_Network_for_Real-Time_Semantic_Segmentation.pdf">BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Taiwan University</span></p><p>BEVANet提出了一种双分支高效视觉注意力网络，结合Sparse Decomposed Large Separable Kernel Attentions (SDLSKA)、Comprehensive Kernel Selection (CKS)和Deep Large Kernel Pyramid Pooling Module (DLKPPM)，实现了大感受野语义理解与精细轮廓分割。其创新的注意力机制和边界引导自适应融合模块实现了在Cityscapes和CamVid数据集上的实时语义分割，达到了81.0% mIoU和33 FPS的优异性能，显著提升了分割精度与效率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-Understanding-Dynamic-Scenes-in-Ego-Centric-4D-Point-Clouds"><a href="#99-Understanding-Dynamic-Scenes-in-Ego-Centric-4D-Point-Clouds" class="headerlink" title="99. Understanding Dynamic Scenes in Ego Centric 4D Point Clouds"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Understanding_Dynamic_Scenes_in_Ego_Centric_4D_Point_Clouds.pdf">Understanding Dynamic Scenes in Ego Centric 4D Point Clouds</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出EgoDynamic4D基准，涵盖927K问答对和12类任务，集成RGB-D视频、相机姿态、全局实例掩码和4D包围盒，实现高密度动态场景注释。作者提出端到端时空推理框架，通过实例感知特征编码、时间与相机嵌入和自适应下采样，将大规模4D场景压缩为LLM可处理的序列；实验表明该方法在多模态动态场景理解上显著优于现有基线，推进了自我中心动态视觉理解的发展。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-HaDM-ST-Histology-Assisted-Differential-Modeling-for-Spatial-Transcriptomics-Generation"><a href="#100-HaDM-ST-Histology-Assisted-Differential-Modeling-for-Spatial-Transcriptomics-Generation" class="headerlink" title="100. HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HaDM-ST__Histology-Assisted_Differential_Modeling_for_Spatial_Transcriptomics_Generation.pdf">HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cambridge</span></p><p>本文提出了HaDM-ST，一种基于扩散模型的高分辨率空间转录组图像生成框架，通过条件化H&amp;E组织切片和低分辨率ST数据，实现高分辨率ST图像生成。方法创新包括H&amp;E驱动的语义蒸馏网络、跨模态空间对齐模块以及基于图神经网络的基因通道判别学习。实验结果表明，HaDM-ST在空间保真度和基因层面表现上均优于现有方法，为精准医疗和组织分子机制研究奠定了基础。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="101-Unsupervised-Real-World-Super-Resolution-via-Rectified-Flow-Degradation-Modelling"><a href="#101-Unsupervised-Real-World-Super-Resolution-via-Rectified-Flow-Degradation-Modelling" class="headerlink" title="101. Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Unsupervised_Real-World_Super-Resolution_via_Rectified_Flow_Degradation_Modelling.pdf">Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology Beijing</span></p><p>本文提出了无监督的真实世界超分辨率方法，核心在于引入Rectified Flow Degradation Module (RFDM)和Fourier Prior Guided Degradation Module (FGDM)。RFDM通过建模退化轨迹实现对复杂真实退化的建模，FGDM利用傅里叶相位结构信息提升退化建模精度，二者结合生成高仿真低分辨率图像与高分辨率图像对，用于训练任意超分模型，实验证明对真实场景的SR性能提升显著。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-SODiff-Semantic-Oriented-Diffusion-Model-for-JPEG-Compression-Artifacts-Removal"><a href="#102-SODiff-Semantic-Oriented-Diffusion-Model-for-JPEG-Compression-Artifacts-Removal" class="headerlink" title="102. SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SODiff__Semantic-Oriented_Diffusion_Model_for_JPEG_Compression_Artifacts_Removal.pdf">SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong Univercity</span></p><p>该论文提出了SODiff，一种基于语义对齐的图像提示提取器（SAIPE）和质量因子感知时间预测器的单步扩散模型，用于JPEG压缩伪影去除。方法通过提取低质量图像的丰富语义特征并与文本编码器对齐，实现高效的语义指导扩散去伪影，同时根据压缩质量自适应选择降噪时刻。实验结果显示SODiff在多个数据集和指标上优于现有主流方法，尤其在极端压缩条件下表现出色。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-CoAR-Concept-Injection-into-Autoregressive-Models-for-Personalized-Text-to-Image-Generation"><a href="#103-CoAR-Concept-Injection-into-Autoregressive-Models-for-Personalized-Text-to-Image-Generation" class="headerlink" title="103. CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CoAR__Concept_Injection_into_Autoregressive_Models_for_Personalized_Text-to-Image_Generation.pdf">CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>本文提出了CoAR框架，通过层级多模态上下文学习（Layerwise Multimodal Context Learning, LMCL）、双重先验保持损失（Dual Prior Preservation, DPP）和上下文感知自正则化（Context-Aware Self-Regularization, CASR），在完全冻结预训练参数的情况下，将个性化主体或风格概念高效注入多模态自回归（AR）文本到图像生成模型中。实验结果表明，CoAR在主体和风格定制任务上实现了更高的主体保真度、指令对齐和风格还原，且所需可训练参数少于0.1M，显著优于现有方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-EndoAgent-A-Memory-Guided-Reflective-Agent-for-Intelligent-Endoscopic-Vision-to-Decision-Reasoning"><a href="#104-EndoAgent-A-Memory-Guided-Reflective-Agent-for-Intelligent-Endoscopic-Vision-to-Decision-Reasoning" class="headerlink" title="104. EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EndoAgent__A_Memory-Guided_Reflective_Agent_for_Intelligent_Endoscopic_Vision-to-Decision_Reasoning.pdf">EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southeast University</span></p><p>EndoAgent提出了一个多模块、双记忆机制的智能代理框架，结合短期行动追踪和长期经验反思，支持多轮推理与工具协作，实现内镜图像分析的视觉到决策流程。通过EndoAgentBench大规模内镜基准测试，实验证明EndoAgent在细粒度视觉理解和开放式语言生成任务上均超越现有医学和通用多模态大模型，提升临床决策支持准确性和灵活性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-Representation-Understanding-via-Activation-Maximization"><a href="#105-Representation-Understanding-via-Activation-Maximization" class="headerlink" title="105. Representation Understanding via Activation Maximization"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Representation_Understanding_via_Activation_Maximization.pdf">Representation Understanding via Activation Maximization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Manchester</span></p><p>本文提出了一种统一的特征可视化框架，基于激活最大化（Activation Maximization, AM），适用于卷积神经网络（CNNs）和视觉Transformer（ViT），能够对输出层及中间层进行可解释性分析，并通过在频域优化提升可视化的自然性和语义一致性。实验表明，该方法不仅有效揭示了不同模型架构的表征差异，还可生成结构化的对抗样本，从而兼顾模型可解释性与鲁棒性分析。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-Small-Large-Collaboration-Training-efficient-Concept-Personalization-for-Large-VLM-using-a-Meta-Personalized-Small-VLM"><a href="#106-Small-Large-Collaboration-Training-efficient-Concept-Personalization-for-Large-VLM-using-a-Meta-Personalized-Small-VLM" class="headerlink" title="106. Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Small-Large_Collaboration__Training-efficient_Concept_Personalization_for_Large_VLM_using_a_Meta_Per.pdf">Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出了一种高效的视觉语言模型（VLM）个性化新范式SLC（Small-Large Collaboration），利用小型VLM进行用户概念检测和结构化线索生成，大型VLM负责反思校验与最终答案生成。该方法通过元训练小模型并设计测试时反思机制，大幅降低训练成本并减少幻觉，实现对开源和闭源大模型的高效个性化，实验显示在多个基准上性能优异且易于实际部署。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="107-SUIT-Spatial-Spectral-Union-Intersection-Interaction-Network-for-Hyperspectral-Object-Tracking"><a href="#107-SUIT-Spatial-Spectral-Union-Intersection-Interaction-Network-for-Hyperspectral-Object-Tracking" class="headerlink" title="107. SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SUIT__Spatial-Spectral_Union-Intersection_Interaction_Network_for_Hyperspectral_Object_Tracking.pdf">SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University of Science and Technology</span></p><p>该论文提出了SUIT（Spatial-Spectral Union-Intersection Interaction Network），一种专为高光谱视频目标跟踪设计的空间-光谱联合交互网络。方法上，SUIT利用Transformer实现模板和搜索区域在各波段间的空间长距离关系建模，并首次引入集合论包容-排斥原理将空间交互的联合与交集区分为波段共享与波段特有信息进行融合，最后提出光谱损失以保证模板与预测区域的物质分布一致性，提升对变形、遮挡等复杂场景的鲁棒性。大规模实验证明，SUIT在多个高光谱跟踪数据集上取得了最新最优性能，显著优于现有方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-Consistent-and-Controllable-Image-Animation-with-Motion-Linear-Diffusion-Transformers"><a href="#108-Consistent-and-Controllable-Image-Animation-with-Motion-Linear-Diffusion-Transformers" class="headerlink" title="108. Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Consistent_and_Controllable_Image_Animation_with_Motion_Linear_Diffusion_Transformers.pdf">Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Monash University</span></p><p>该论文提出了MiraMo框架，通过将线性注意力机制融入Transformer架构，实现了高效、外观一致且运动平滑的图像动画生成。MiraMo采用自建的文本到视频（T2V）线性Transformer作为基础，结合运动残差学习、基于DCT的噪声初始化与动态度控制，有效提升了动画的一致性和可控性，并显著减少了计算资源消耗。实验结果表明，MiraMo在外观一致性、运动平滑性及推理速度等方面均超越现有方法，并可扩展至运动迁移和视频编辑等任务。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-Bridging-Semantic-Logic-Gaps-A-Cognition-Inspired-Multimodal-Boundary-Preserving-Network-for-Image-Manipulation-Localization"><a href="#109-Bridging-Semantic-Logic-Gaps-A-Cognition-Inspired-Multimodal-Boundary-Preserving-Network-for-Image-Manipulation-Localization" class="headerlink" title="109. Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Bridging_Semantic_Logic_Gaps__A_Cognition-Inspired_Multimodal_Boundary-Preserving_Network_for_Image_.pdf">Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xinjiang University</span></p><p>该论文提出了一种认知启发的多模态边界保持网络（CMB-Net），用于图像篡改区域定位。方法上，模型结合了大语言模型（LLMs）生成的文本提示与视觉特征，通过图像-文本中心歧义模块（ITCAM）权重化文本特征，利用图像-文本交互模块（ITIM）实现细粒度融合，并引入可逆神经网络的边界解码器（RED）以无损保留篡改区域边界。实验结果表明，CMB-Net在多个公开数据集上显著优于现有SOTA方法，且文本信息能有效提升复杂场景下的篡改定位精度。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="110-Intention-Aware-Diffusion-Model-for-Pedestrian-Trajectory-Prediction"><a href="#110-Intention-Aware-Diffusion-Model-for-Pedestrian-Trajectory-Prediction" class="headerlink" title="110. Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction"></a>110. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Intention-Aware_Diffusion_Model_for_Pedestrian_Trajectory_Prediction.pdf">Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southern University of Science and Technology</span></p><p>该论文提出一种结合短期和长期意图的扩散模型用于行人轨迹预测：短期意图通过残差极坐标表示捕捉细粒度运动趋势，长期意图采用可学习的token端点预测器以概率方式生成多模态目标。模型还引入软掩码指导和残差噪声预测，通过在ETH、UCY和SDD数据集上实验证明，方法在ADE和FDE指标上优于现有方法，实现了更精确的轨迹预测。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="111-Large-scale-Multi-sequence-Pretraining-for-Generalizable-MRI-Analysis-in-Versatile-Clinical-Applications"><a href="#111-Large-scale-Multi-sequence-Pretraining-for-Generalizable-MRI-Analysis-in-Versatile-Clinical-Applications" class="headerlink" title="111. Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications"></a>111. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Large-scale_Multi-sequence_Pretraining_for_Generalizable_MRI_Analysis_in_Versatile_Clinical_Applicat.pdf">Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology</span></p><p>本文提出PRISM，一种基于Swin Transformer的大规模多序列MRI自监督预训练基础模型，通过解耦解剖学不变特征与序列特异性变化，结合掩码图像重建、图像翻译、元数据预测和对比学习等多任务框架，获得对多种临床下游任务（分割、分类、回归、配准、报告生成）的强泛化能力。实验结果表明，PRISM在44项下游任务中有39项取得最优结果，有效提升了多序列MRI分析的准确性、鲁棒性和临床适用性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="112-SketchAnimator-Animate-Sketch-via-Motion-Customization-of-Text-to-Video-Diffusion-Models"><a href="#112-SketchAnimator-Animate-Sketch-via-Motion-Customization-of-Text-to-Video-Diffusion-Models" class="headerlink" title="112. SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models"></a>112. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SketchAnimator__Animate_Sketch_via_Motion_Customization_of_Text-to-Video_Diffusion_Models.pdf">SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing University of Posts and Telecommunications</span></p><p>该论文提出了SketchAnimator方法，实现了通过三阶段流程（外观学习、运动学习和视频先验蒸馏），利用LoRA微调和可微分贝塞尔曲线渲染，将静态手绘草图和参考视频中的运动信号结合，生成高质量且运动一致的草图动画。实验结果表明，该方法在外观保持和运动传递准确性上均优于现有方法，支持高度自定义和创意动画生成。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="113-Perceptual-Evaluation-of-GANs-and-Diffusion-Models-for-Generating-X-rays"><a href="#113-Perceptual-Evaluation-of-GANs-and-Diffusion-Models-for-Generating-X-rays" class="headerlink" title="113. Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays"></a>113. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Perceptual_Evaluation_of_GANs_and_Diffusion_Models_for_Generating_X-rays.pdf">Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Pontificia Universidad Católica de Chile</span></p><p>本论文提出了一种基于人类专家评估的框架，系统比较了GAN和Diffusion Models在合成胸部X光片的真实性和异常表现力，主要针对四种异常（肺不张、肺部不透明、胸腔积液、心影增大）。研究发现，Diffusion Models整体生成的图像更逼真，但在某些条件下GAN表现更优，且两者在条件准确性方面各有优势，表明现有生成模型尚未完全解决医学图像真实性问题。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="114-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning"><a href="#114-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning" class="headerlink" title="114. Cut2Next: Generating Next Shot via In-Context Tuning"></a>114. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Cut2Next__Generating_Next_Shot_via_In-Context_Tuning.pdf">Cut2Next: Generating Next Shot via In-Context Tuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong</span></p><p>该论文提出了Next Shot Generation（NSG）任务，旨在生成符合专业剪辑模式和严格电影连续性的高质量下一镜头。方法上，Cut2Next框架基于Diffusion Transformer，采用分层多提示策略和无参数新架构（CACI与HAM），结合大规模和精细化数据集（RawCuts与CuratedCuts），实现对多样化编辑模式的有效建模。结论显示，该方法在视觉一致性和文本对齐上显著优于现有基线，并在用户研究中获得极高偏好，尤其在剪辑模式遵循和电影连贯性方面表现突出。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="115-Learning-User-Preferences-for-Image-Generation-Models"><a href="#115-Learning-User-Preferences-for-Image-Generation-Models" class="headerlink" title="115. Learning User Preferences for Image Generation Models"></a>115. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_User_Preferences_for_Image_Generation_Model.pdf">Learning User Preferences for Image Generation Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Renmin University of China</span></p><p>该论文提出了一种基于多模态大语言模型（MLLM）的用户偏好学习框架，通过引入对比偏好损失和可学习偏好token，利用用户历史交互数据建模个性化视觉偏好。方法有效区分用户“喜欢”与“不喜欢”的内容，并通过偏好token捕捉用户间共享兴趣，实现分组结构和泛化能力。实验结果表明，该方法在偏好预测准确率和生成内容个性化指导方面优于现有方法，可准确识别用户审美倾向并提升生成效果。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="116-THAT-Token-wise-High-frequency-Augmentation-Transformer-for-Hyperspectral-Pansharpening"><a href="#116-THAT-Token-wise-High-frequency-Augmentation-Transformer-for-Hyperspectral-Pansharpening" class="headerlink" title="116. THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening"></a>116. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/THAT__Token-wise_High-frequency_Augmentation_Transformer_for_Hyperspectral_Pansharpening.pdf">THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">JPMorgan Chase</span></p><p>本文提出了一种新颖的Token-wise High-frequency Augmentation Transformer (THAT)框架，用于高光谱图像融合（pansharpening）。方法创新性地引入了Pivotal Token Selective Attention (PTSA)模块以动态筛选和聚焦于信息量大的token，并通过Multi-level Variance-aware Feed-forward Network (MVFN)增强高频细节学习。实验结果表明，THAT在多个公开数据集和不同放大倍数下均取得了优异的重建质量和效率，显著提升了高光谱影像的空间和光谱保真度。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="117-Mitigating-Biases-in-Surgical-Operating-Rooms-with-Geometry"><a href="#117-Mitigating-Biases-in-Surgical-Operating-Rooms-with-Geometry" class="headerlink" title="117. Mitigating Biases in Surgical Operating Rooms with Geometry"></a>117. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Mitigating_Biases_in_Surgical_Operating_Rooms_with_Geometry.pdf">Mitigating Biases in Surgical Operating Rooms with Geometry</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">TU Munich</span></p><p>该论文通过对手术室人员再识别任务中的深度神经网络进行梯度型显著性分析，发现模型容易依赖于服装等非本质视觉特征而产生偏见。作者提出利用3D点云序列编码人员身份，将几何信息（如形态和动作模式）与外观混淆因素分离，实验表明几何表示在真实临床环境下能有效提升识别准确性并减少偏见。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="118-StableAvatar-Infinite-Length-Audio-Driven-Avatar-Video-Generation"><a href="#118-StableAvatar-Infinite-Length-Audio-Driven-Avatar-Video-Generation" class="headerlink" title="118. StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation"></a>118. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/StableAvatar__Infinite-Length_Audio-Driven_Avatar_Video_Generation.pdf">StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fudan University</span></p><p>该论文提出StableAvatar，一种端到端视频扩散Transformer框架，实现了基于音频驱动的无限时长高质量虚拟人视频生成。方法核心在于引入Timestep-aware Audio Adapter进行步长感知的音频调制，防止潜在分布误差累积，并在推理阶段通过Audio Native Guidance提升音频同步和表情自然度，采用动态加权滑动窗口提升长视频平滑性。实验表明，StableAvatar显著优于现有方法，在身份一致性、音频同步和视频质量方面均达到新SOTA，支持长视频无明显质量衰减。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="119-VGGSounder-Audio-Visual-Evaluations-for-Foundation-Models"><a href="#119-VGGSounder-Audio-Visual-Evaluations-for-Foundation-Models" class="headerlink" title="119. VGGSounder: Audio-Visual Evaluations for Foundation Models"></a>119. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VGGSounder__Audio-Visual_Evaluations_for_Foundation_Models.pdf">VGGSounder: Audio-Visual Evaluations for Foundation Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technical University of Munich</span></p><p>本论文提出VGGSounder，一个全面重注释的多标签音视频分类测试集，专为评估音视频基础模型设计。方法包括对VGGSound数据集进行多标签和模态（可听&#x2F;可见&#x2F;可听且可见）人工标注，添加背景音乐、画外音和静态图片等元标签，并引入“模态混淆”新指标以量化多模态输入对模型性能的负面影响。实验评测表明，现有的音视频基础模型在该数据集上的表现普遍较差，尤其在多模态输入下易出现性能下降，且模型容易偏向视觉或听觉信息。结论：VGGSounder能够更精确地揭示多模态模型的能力短板，为音视频基础模型的公平评测和未来改进提供了重要工具。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="120-SAGOnline-Segment-Any-Gaussians-Online"><a href="#120-SAGOnline-Segment-Any-Gaussians-Online" class="headerlink" title="120. SAGOnline: Segment Any Gaussians Online"></a>120. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SAGOnline__Segment_Any_Gaussians_Online.pdf">SAGOnline: Segment Any Gaussians Online</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Waterloo</span></p><p>SAGOnline提出了一种轻量级、零训练的实时3D高斯场分割框架，通过视频基础模型（如SAM 2）实现视图一致的2D分割，并利用GPU加速算法将2D掩码转为3D实例标签，实现高效的多对象追踪与分割。实验结果显示，在NVOS和Spin-NeRF基准上达到最新SOTA精度，并以27ms&#x2F;帧取得极快推理速度，显著优于现有方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="121-KARMA-Efficient-Structural-Defect-Segmentation-via-Kolmogorov-Arnold-Representation-Learning"><a href="#121-KARMA-Efficient-Structural-Defect-Segmentation-via-Kolmogorov-Arnold-Representation-Learning" class="headerlink" title="121. KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning"></a>121. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/KARMA__Efficient_Structural_Defect_Segmentation_via_Kolmogorov-Arnold_Representation_Learning.pdf">KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of New Orleans</span></p><p>本文提出KARMA，一种基于Kolmogorov-Arnold表示学习的高效结构性缺陷分割框架，核心创新包括参数高效的Tiny Kolmogorov-Arnold Network (TiKAN)模块、融合可分离卷积的多尺度特征金字塔结构，以及静-动态原型机制。KARMA在结构缺陷数据集上以不到百万参数实现与先进模型相当或更优的分割性能，且显著减少97%的模型参数和计算量，适合实时部署于基础设施自动巡检。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="122-REDDINO-A-FOUNDATION-MODEL-FOR-RED-BLOOD-CELL-ANALYSIS"><a href="#122-REDDINO-A-FOUNDATION-MODEL-FOR-RED-BLOOD-CELL-ANALYSIS" class="headerlink" title="122. REDDINO: A FOUNDATION MODEL FOR RED BLOOD CELL ANALYSIS"></a>122. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/RedDino__A_foundation_model_for_red_blood_cell_analysis.pdf">REDDINO: A FOUNDATION MODEL FOR RED BLOOD CELL ANALYSIS</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cagliari</span></p><p>本文提出RedDino，一种基于自监督学习且专为红细胞图像分析设计的基础模型，通过改进DINOv2框架并在大规模多源红细胞数据集上训练，采用特定增强和正则化策略。实验结果表明，RedDino在红细胞形态分类任务上较现有方法有显著提升，具有强泛化能力和适应不同数据源的鲁棒性，为自动化血液病诊断提供了可靠工具。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="123-MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision"><a href="#123-MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision" class="headerlink" title="123. MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision"></a>123. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MedReasoner__Reinforcement_Learning_Drives_Reasoning_Grounding_from_Clinical_Thought_to_Pixel-Level_.pdf">MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beijing University of Posts and Telecommunications</span></p><p>本文提出UMRG任务与U-MRG-14K数据集，要求模型将隐含临床问题转化为医学影像的像素级定位。方法上，MedReasoner采用模块化设计，将临床推理与分割解耦，通过强化学习优化推理模块，利用奖励函数对输出格式和空间精度进行约束，并用冻结分割专家实现空间提示到掩码的转换。实验结果显示，MedReasoner在U-MRG-14K上性能显著优于同类模型，能准确处理隐含医学问题，实现高精度定位。结论：RL方法有效提升了医学影像语义与像素级定位的解释性和泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="124-Integrating-Task-Specific-and-Universal-Adapters-for-Pre-Trained-Model-based-Class-Incremental-Learning"><a href="#124-Integrating-Task-Specific-and-Universal-Adapters-for-Pre-Trained-Model-based-Class-Incremental-Learning" class="headerlink" title="124. Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning"></a>124. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Integrating_Task-Specific_and_Universal_Adapters_for_Pre-Trained_Model-based_Class-Incremental_Learn.pdf">Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Artificial Intelligence, Nanjing University</span></p><p>本文提出TUNA方法，将任务专属adapter与通用adapter结合，用于预训练模型的类增量学习。方法包括利用正交损失训练任务专属adapter，通过熵机制选择最合适的adapter，并融合各adapter权重构建通用adapter，推理时集成专属与通用adapter预测。实验结果在CIFAR100、ImageNet-R&#x2F;A、ObjectNet等数据集上优于现有方法，有效减缓灾难性遗忘并提升分类准确率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="125-FantasyStyle-Controllable-Stylized-Distillation-for-3D-Gaussian-Splatting"><a href="#125-FantasyStyle-Controllable-Stylized-Distillation-for-3D-Gaussian-Splatting" class="headerlink" title="125. FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting"></a>125. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FantasyStyle__Controllable_Stylized_Distillation_for_3D_Gaussian_Splatting.pdf">FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computing and Artificial Intelligence, Shanghai University of Finance and Economics</span></p><p>该论文提出了FantasyStyle，一种基于3D高斯泼溅的可控风格迁移框架，核心方法包括多视角频率一致性（MVFC）和可控风格化蒸馏（CSD），分别通过3D频域滤波提升风格跨视角一致性，并引入负向引导抑制内容泄漏，仅用扩散模型蒸馏实现2D到3D的灵活风格迁移。实验表明该方法在多场景、多风格下风格迁移质量和内容保持性均优于现有方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="126-Hyperspectral-Imaging"><a href="#126-Hyperspectral-Imaging" class="headerlink" title="126. Hyperspectral Imaging"></a>126. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hyperspectral_Imaging.pdf">Hyperspectral Imaging</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Southeast University</span></p><p>本文系统综述了高光谱成像（HSI）的物理原理、传感器架构及数据采集、校准和处理流程，重点介绍了降维、分类、光谱解混等传统与深度学习分析方法，并总结了其在环境观测、农业、医学、工业检测等领域的代表性应用。结论指出，随着传感器小型化、自监督学习和基础模型的发展，高光谱成像正逐步成为跨学科通用平台，未来有望实现可扩展、实时和嵌入式应用，推动科学与社会的深刻变革。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="127-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation"><a href="#127-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation" class="headerlink" title="127. Matrix-3D: Omnidirectional Explorable 3D World Generation"></a>127. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Matrix-3D__Omnidirectional_Explorable_3D_World_Generation.pdf">Matrix-3D: Omnidirectional Explorable 3D World Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Skywork AI</span></p><p>该论文提出Matrix-3D框架，实现从单张图片或文本提示生成可全方位自由探索的3D世界。方法包括基于全景表示的轨迹引导全景视频扩散模型与全景3D重建两种流程，并自建了Matrix-Pano大规模合成全景视频数据集。结论表明，Matrix-3D在全景视频生成和3D世界重建上均达到了业界最优的性能，实现了高质量、广覆盖、可控的3D场景生成。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="128-BadPromptFL-A-Novel-Backdoor-Threat-to-Prompt-based-Federated-Learning-in-Multimodal-Models"><a href="#128-BadPromptFL-A-Novel-Backdoor-Threat-to-Prompt-based-Federated-Learning-in-Multimodal-Models" class="headerlink" title="128. BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models"></a>128. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BadPromptFL__A_Novel_Backdoor_Threat_to_Prompt-based_Federated_Learning_in_Multimodal_Models.pdf">BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Dalian University of Technology</span></p><p>本文提出BadPromptFL，一种针对多模态模型中基于提示（Prompt）的联邦学习的新型后门攻击方法。该方法通过恶意客户端联合优化本地后门触发器和提示嵌入，并将中毒提示注入全局聚合流程，实现无需修改模型参数即可在推理时激活后门。实验表明，BadPromptFL在多种数据集和聚合协议下均能以极高的攻击成功率（&gt;90%），且对正常任务性能影响极小，暴露了基于提示的多模态联邦学习的安全隐患。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="129-CD-TVD-Contrastive-Diffusion-for-3D-Super-Resolution-with-Scarce-High-Resolution-Time-Varying-Data"><a href="#129-CD-TVD-Contrastive-Diffusion-for-3D-Super-Resolution-with-Scarce-High-Resolution-Time-Varying-Data" class="headerlink" title="129. CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data"></a>129. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CD-TVD__Contrastive_Diffusion_for_3D_Super-Resolution_with_Scarce_High-Resolution_Time-Varying_Data.pdf">CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tianjin University</span></p><p>该论文提出了CD-TVD框架，通过对历史科学仿真数据的对比学习，结合带局部注意力机制的扩散超分辨率模型，实现了在高分辨率时序数据稀缺的情况下的3D超分辨率重建。方法分为预训练和微调两个阶段，预训练阶段联合对比编码模块和扩散超分辨率模块共同学习退化模式和细节特征，微调阶段仅冻结编码模块，用极少量高分辨率样本即可适应新场景，实现对所有低分辨率时刻的准确重构。实验结果在多组科学仿真数据上均优于主流方法，显著降低了对高分辨率数据的依赖，提升了科学可视化任务中的实际应用价值。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="130-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control"><a href="#130-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control" class="headerlink" title="130. Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control"></a>130. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Follow-Your-Shape__Shape-Aware_Image_Editing_via_Trajectory-Guided_Region_Control.pdf">Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">HKUST</span></p><p>该论文提出了Follow-Your-Shape，一个无需训练和掩码的形状感知图像编辑框架，通过轨迹发散图（TDM）动态定位可编辑区域，并结合分阶段Key-Value特征注入，实现对目标对象形状的大幅变化同时精准保留背景内容。实验表明，该方法在新设计的ReShapeBench基准上形状编辑能力和视觉一致性均优于现有方法，实现了高质量的图像编辑。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="131-MDD-Net-Multimodal-Depression-Detection-through-Mutual-Transformer"><a href="#131-MDD-Net-Multimodal-Depression-Detection-through-Mutual-Transformer" class="headerlink" title="131. MDD-Net: Multimodal Depression Detection through Mutual Transformer"></a>131. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MDD-Net__Multimodal_Depression_Detection_through_Mutual_Transformer.pdf">MDD-Net: Multimodal Depression Detection through Mutual Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Waterloo</span></p><p>本文提出了一种基于互变换器（Mutual Transformer, MT）的多模态抑郁检测网络MDD-Net，融合了音频和视频两种社交媒体数据。方法包括声学和视觉特征提取模块、互变换器实现跨模态相关性建模和特征融合，最终通过检测层分类。实验在D-Vlog数据集上表明，MDD-Net在F1-Score上比现有方法提升1.82%~17.37%，有效提升抑郁检测性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="132-Sample-aware-RandAugment-Search-free-Automatic-Data-Augmentation-for-Effective-Image-Recognition"><a href="#132-Sample-aware-RandAugment-Search-free-Automatic-Data-Augmentation-for-Effective-Image-Recognition" class="headerlink" title="132. Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition"></a>132. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sample-aware_RandAugment__Search-free_Automatic_Data_Augmentation_for_Effective_Image_Recognition.pdf">Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Chinese Academy of Sciences</span></p><p>本文提出了一种无需搜索的自动数据增强方法Sample-aware RandAugment (SRA)，通过设计启发式评分模块（Magnitude Instructor Score, MIS）对训练样本难度进行动态评估，并采用不对称增强策略，分别针对样本探索和精细化训练，自动调整增强策略。实验结果表明，SRA在ImageNet、CIFAR等多项视觉任务上缩小了传统搜索型和无搜索型自动增强方法的性能差距，提升了模型泛化能力，且易于集成至现有视觉训练流程。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="133-Prompt-Guided-Relational-Reasoning-for-Social-Behavior-Understanding-with-Vision-Foundation-Models"><a href="#133-Prompt-Guided-Relational-Reasoning-for-Social-Behavior-Understanding-with-Vision-Foundation-Models" class="headerlink" title="133. Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models"></a>133. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Prompt-Guided_Relational_Reasoning_for_Social_Behavior_Understanding_with_Vision_Foundation_Models.pdf">Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Stuttgart</span></p><p>本文提出了一种名为ProGraD的Prompt驱动型群体活动检测方法，通过在冻结的视觉基础模型（如DINOv2）上注入可学习的群体提示，并设计了两层轻量级GroupContext Transformer，实现了对社会行为中群体成员关系与集体活动的高效建模。实验证明，ProGraD在Cafe和Social-CAD两个群体活动检测基准上均取得最新最优性能，仅需约1,000万可训练参数，尤其在多组复杂场景下表现突出，同时具备较强可解释性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="134-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation"><a href="#134-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation" class="headerlink" title="134. Omni-Effects: Unified and Spatially Controllable Visual Effects Generation"></a>134. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Omni-Effects__Unified_and_Spatially-Controllable_Visual_Effects_Generation.pdf">Omni-Effects: Unified and Spatially Controllable Visual Effects Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AMAP, Alibaba Group</span></p><p>Omni-Effects提出了一个统一的视觉特效（VFX）生成框架，核心创新包括基于LoRA的专家混合（LoRA-MoE）模块，用于减少多特效混训时的相互干扰，以及结合空间感知提示（SAP）与独立信息流（IIF），实现对多特效在指定区域的精确控制。实验和新建Omni-VFX数据集上的评估表明，该方法显著提升了多特效在空间和类别上的可控性与生成质量，优于现有主流方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="135-Score-Augmentation-for-Diffusion-Models"><a href="#135-Score-Augmentation-for-Diffusion-Models" class="headerlink" title="135. Score Augmentation for Diffusion Models"></a>135. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Score_Augmentation_for_Diffusion_Models.pdf">Score Augmentation for Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Technology</span></p><p>该论文提出了Score Augmentation（ScoreAug），一种针对扩散模型的创新数据增强框架。方法核心在于对噪声数据进行线性和非线性变换，并训练扩散模型的去噪器预测变换后的目标，从而实现分数空间的等变学习，同时理论分析了不同变换空间下分数的对应关系。实验证明，ScoreAug在CIFAR-10、FFHQ、AFHQv2和ImageNet等多个图像生成基准上显著缓解了扩散模型的过拟合，提升了生成质量且与常规增强方法兼容。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="136-Diffusing-the-Blind-Spot-Uterine-MRI-Synthesis-with-Diffusion-Models"><a href="#136-Diffusing-the-Blind-Spot-Uterine-MRI-Synthesis-with-Diffusion-Models" class="headerlink" title="136. Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models"></a>136. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Diffusing_the_Blind_Spot__Uterine_MRI_Synthesis_with_Diffusion_Models.pdf">Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Friedrich–Alexander University Erlangen–Nürnberg</span></p><p>本论文提出了一种基于扩散模型（包括DDPM和LDM）的2D和3D子宫MRI图像合成框架，结合无条件和条件（类别标签及文本描述）生成高保真、解剖结构准确的医学影像。该方法通过多种先进感知与分布度量评估生成质量，显著提升了下游诊断模型的准确率，特别在弱监督数据稀缺场景下，合成数据在分类任务上超越真实数据。论文最终将模型和数据公开，助力妇科AI研究的数据可复现与隐私保护。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="137-Architectural-Co-Design-for-Zero-Shot-Anomaly-Detection-Decoupling-Representation-and-Dynamically-Fusing-Features-in-CLIP"><a href="#137-Architectural-Co-Design-for-Zero-Shot-Anomaly-Detection-Decoupling-Representation-and-Dynamically-Fusing-Features-in-CLIP" class="headerlink" title="137. Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP"></a>137. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Architectural_Co-Design_for_Zero-Shot_Anomaly_Detection__Decoupling_Representation_and_Dynamically_F.pdf">Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Big Data Institute, Central South University</span></p><p>该论文提出了ACD-CLIP框架，结合参数高效的Conv-LoRA适配器与动态融合网关（DFG），分别注入局部归纳偏置并实现基于视觉内容的灵活双向文本特征融合，从而提升大规模视觉语言模型在零样本异常检测任务中的表现。实验表明该方法在工业和医学数据集上超越了主流方法，显著提升了分割与分类的准确性和稳定性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="138-MIND-A-Noise-Adaptive-Denoising-Framework-for-Medical-Images-Integrating-Multi-Scale-Transformer"><a href="#138-MIND-A-Noise-Adaptive-Denoising-Framework-for-Medical-Images-Integrating-Multi-Scale-Transformer" class="headerlink" title="138. MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer"></a>138. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MIND__A_Noise-Adaptive_Denoising_Framework_for_Medical_Images_Integrating_Multi-Scale_Transformer.pdf">MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Electronic Science and Technology of China</span></p><p>本文提出了一种集成多尺度卷积与Transformer结构的医学图像自适应去噪模型MIND，引入噪声水平估计器（NLE）和噪声自适应注意力模块（NAAB），实现基于噪声感知的通道-空间注意力调节和跨模态特征融合。该方法在多模态医学图像公开数据集上系统验证，显著提升了PSNR、SSIM、LPIPS等指标，并增强了结构还原和诊断敏感性，具有强泛化能力和实际应用价值。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="139-DiTVR-Zero-Shot-Diffusion-Transformer-for-Video-Restoration"><a href="#139-DiTVR-Zero-Shot-Diffusion-Transformer-for-Video-Restoration" class="headerlink" title="139. DiTVR: Zero-Shot Diffusion Transformer for Video Restoration"></a>139. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DiTVR__Zero-Shot_Diffusion_Transformer_for_Video_Restoration.pdf">DiTVR: Zero-Shot Diffusion Transformer for Video Restoration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Wurzburg</span></p><p>该论文提出了DiTVR，一种基于Diffusion Transformer（DiT）的零样本视频修复框架，结合了轨迹感知注意力机制、光流引导的扩散采样器和时空邻居缓存。方法通过对光流轨迹的建模和关键层的时空注意力，有效提升了视频恢复的细节保留和时序一致性。实验表明，DiTVR在多项视频修复基准上超越现有方法，尤其在时序一致性与高保真恢复方面表现优异，无需针对特定任务训练。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="140-MambaTrans-Multimodal-Fusion-Image-Translation-via-Large-Language-Model-Priors-for-Downstream-Visual-Tasks"><a href="#140-MambaTrans-Multimodal-Fusion-Image-Translation-via-Large-Language-Model-Priors-for-Downstream-Visual-Tasks" class="headerlink" title="140. MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks"></a>140. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MambaTrans__Multimodal_Fusion_Image_Translation_via_Large_Language_Model_Priors_for_Downstream_Visua.pdf">MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Foshan University</span></p><p>本论文提出了一种新颖的多模态融合图像模态翻译框架MambaTrans，通过引入多模态大语言模型生成的文本描述与分割掩码，联合3D-Selective Scan Module和mask-image-text cross-attention，有效实现了融合图像到可见光分布的转换。实验结果表明，MambaTrans在无需对下游模型参数调整的情况下，能够显著提升多模态融合图像在目标检测和语义分割等下游视觉任务中的表现，且保持甚至提升了视觉质量。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="141-RSVLM-QA-A-Benchmark-Dataset-for-Remote-Sensing-Vision-Language-Model-based-Question-Answering"><a href="#141-RSVLM-QA-A-Benchmark-Dataset-for-Remote-Sensing-Vision-Language-Model-based-Question-Answering" class="headerlink" title="141. RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering"></a>141. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/RSVLM-QA__A_Benchmark_Dataset_for_Remote_Sensing_Vision_Language_Model-based_Question_Answering.pdf">RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Technology Sydney</span></p><p>本文提出RSVLM-QA数据集，通过集成四个主流遥感分割和检测数据集（WHU、LoveDA、INRIA、iSAID），采用GPT-4.1驱动的双轨注释生成流程，自动生成丰富的图像描述、空间关系、语义标签及多样化VQA对。该数据集包括13820张图片和162373条VQA对，覆盖六大类问题，支持复杂推理与精确计数，并通过六个主流VLM模型的基准实验，验证了其对遥感视觉语言模型理解和推理能力评测的有效性。结论表明，RSVLM-QA为遥感VQA和多模态理解研究提供了高质量资源，能有效推动领域发展。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="142-Generative-Video-Matting"><a href="#142-Generative-Video-Matting" class="headerlink" title="142. Generative Video Matting"></a>142. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Generative_Video_Matting.pdf">Generative Video Matting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Adelaide</span></p><p>本文提出了一种基于生成扩散模型的视频抠像方法（GVM），通过大规模合成与伪标注分割数据进行多阶段训练，并结合细粒度合成抠像数据。核心流程包括利用Stable Video Diffusion模型预训练、流匹配监督加速推理、多空间混合损失提升细节保留与时序一致性。实验结果表明，该方法在多个基准测试集上大幅优于现有方法，具备极强的泛化能力与细节恢复能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="143-Segmenting-and-Understanding-Region-aware-Semantic-Attention-for-Fine-grained-Image-Quality-Assessment-with-Large-Language-Models"><a href="#143-Segmenting-and-Understanding-Region-aware-Semantic-Attention-for-Fine-grained-Image-Quality-Assessment-with-Large-Language-Models" class="headerlink" title="143. Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models"></a>143. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Segmenting_and_Understanding__Region-aware_Semantic_Attention_for_Fine-grained_Image_Quality_Assessm.pdf">Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harbin Institute of Technology</span></p><p>该论文提出了RSFIQA模型，用于无参考图像质量评估。方法通过SAM模型对图像进行语义分割，结合多模态大语言模型(MLLM)对每个分割区域进行多维失真分析，并通过区域语义注意力机制（RSA）融合区域特征，实现对不同语义区域的细粒度质量感知。实验结果表明RSFIQA在多项IQA数据集上均优于现有方法，具有强泛化性和鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="144-Dream4D-Lifting-Camera-Controlled-I2V-towards-Spatiotemporally-Consistent-4D-Generation"><a href="#144-Dream4D-Lifting-Camera-Controlled-I2V-towards-Spatiotemporally-Consistent-4D-Generation" class="headerlink" title="144. Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation"></a>144. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Dream4D__Lifting_Camera-Controlled_I2V_towards_Spatiotemporally_Consistent_4D_Generation.pdf">Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Chinese University of Hong Kong</span></p><p>该论文提出Dream4D框架，通过将可控图像到视频生成与神经4D重建相结合，实现了从单张图像及文本提示生成时空一致的4D场景。其方法包括通过视觉语言模型预测最佳摄像机轨迹，采用姿态条件扩散模型生成多视角视频序列，最终以4D生成器模块映射为持久4D表示。实验结果显示，Dream4D在动态场景的几何精度和时空一致性方面显著超越现有方法，有效减少了时序闪烁和形状漂移，提升了动态场景理解能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="145-Sea-Undistort-A-Dataset-for-Through-Water-Image-Restoration-in-High-Resolution-Airborne-Bathymetric-Mapping"><a href="#145-Sea-Undistort-A-Dataset-for-Through-Water-Image-Restoration-in-High-Resolution-Airborne-Bathymetric-Mapping" class="headerlink" title="145. Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping"></a>145. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sea-Undistort__A_Dataset_for_Through-Water_Image_Restoration_in_High_Resolution_Airborne_Bathymetric.pdf">Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Technische Universitat Berlin</span></p><p>本文提出了Sea-Undistort，一个包含1200对高分辨率水下合成图像（有&#x2F;无光学畸变）的数据集，用于训练和评估深度学习图像恢复模型。通过对ResShift、NDR-Restore及其改进版ResShift+EF（融合太阳反光掩码）进行实验，结果显示基于该数据集训练的模型在真实航拍水下影像上提升了海底可视性与测深产品质量，尤其在深水区域细节恢复和误差抑制方面效果显著。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="146-Correspondence-as-Video-Test-Time-Adaption-on-SAM2-for-Reference-Segmentation-in-the-Wild"><a href="#146-Correspondence-as-Video-Test-Time-Adaption-on-SAM2-for-Reference-Segmentation-in-the-Wild" class="headerlink" title="146. Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild"></a>146. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Correspondence_as_Video__Test-Time_Adaption_on_SAM2_for_Reference_Segmentation_in_the_Wild.pdf">Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>本文提出CAV-SAM方法，通过将参考-目标图像对转化为伪视频序列，实现对SAM2模型在下游任务的轻量级适应。方法包括DBST模块利用扩散模型生成语义平滑过渡序列，以及TTGA模块通过测试时几何对齐对SAM2进行微调和额外提示，实现几何变形的自适应。实验表明，CAV-SAM在CD-FSS基准上分割性能超过现有方法约5%，在复杂真实场景如Chest X-Ray数据上表现突出，验证了方法的高效性和鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="147-Enhancing-Small-Scale-Dataset-Expansion-with-Triplet-Connection-based-Sample-Re-Weighting"><a href="#147-Enhancing-Small-Scale-Dataset-Expansion-with-Triplet-Connection-based-Sample-Re-Weighting" class="headerlink" title="147. Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting"></a>147. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Enhancing_Small-Scale_Dataset_Expansion_with_Triplet-Connection-based_Sample_Re-Weighting.pdf">Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Hunan University</span></p><p>本文提出TriReWeight，一种基于三元组连接的样本重加权方法，用于提升生成式数据扩增在小规模图像数据集上的效果。该方法通过理论分析三类生成图像监督方式，结合三元组损失和一致性正则化，有效降低噪声图像影响，并证明可与任意生成式数据扩增方法结合且不会降低性能。实验结果表明TriReWeight在六个自然图像和三个医学图像数据集上平均提升分类准确率，验证了方法的有效性和理论正确性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="148-DoorDet-Semi-Automated-Multi-Class-Door-Detection-Dataset-via-Object-Detection-and-Large-Language-Models"><a href="#148-DoorDet-Semi-Automated-Multi-Class-Door-Detection-Dataset-via-Object-Detection-and-Large-Language-Models" class="headerlink" title="148. DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models"></a>148. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DoorDet__Semi-Automated_Multi-Class_Door_Detection_Dataset_via_Object_Detection_and_Large_Language_M.pdf">DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Melbourne</span></p><p>本文提出了一种结合先进目标检测器（如Co-DETR）和大语言模型（如GPT-4.1），并引入人类校正环节的半自动化流程，用于构建包含门多类别及功能性细分的高质量门检测数据集DoorDet。实验表明，该流程显著降低人工标注成本，提升标注效率，并且所构建的数据集可有效提升复杂场景下多类门检测模型的性能和泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="149-Make-Your-MoVe-Make-Your-3D-Contents-by-Adapting-Multi-View-Diffusion-Models-to-External-Editing"><a href="#149-Make-Your-MoVe-Make-Your-3D-Contents-by-Adapting-Multi-View-Diffusion-Models-to-External-Editing" class="headerlink" title="149. Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing"></a>149. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Make_Your_MoVe__Make_Your_3D_Contents_by_Adapting_Multi-View_Diffusion_Models_to_External_Editing.pdf">Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了一种无须微调的、即插即用的方案，能够将多视角扩散模型与外部2D编辑工具结合，用以生成高一致性和高质量的3D内容。其核心方法包括几何保持模块和注入开关，分别用于在单次推理中保留原始几何结构并灵活控制法线监督程度。实验表明，该方法在多种编辑工具和多视角扩散模型的组合下显著提升了多视角一致性和生成3D网格的质量。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="150-DiffVC-OSD-One-Step-Diffusion-based-Perceptual-Neural-Video-Compression-Framework"><a href="#150-DiffVC-OSD-One-Step-Diffusion-based-Perceptual-Neural-Video-Compression-Framework" class="headerlink" title="150. DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework"></a>150. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DiffVC-OSD__One-Step_Diffusion-based_Perceptual_Neural_Video_Compression_Framework.pdf">DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Wuhan University</span></p><p>该论文提出了一种名为DiffVC-OSD的单步扩散感知神经视频压缩框架，通过将重建后的无噪声潜表示直接输入单步扩散模型，并结合时序上下文进行条件引导，显著提升视频重建的感知质量。方法还包括时序上下文适配器和端到端微调策略，实验结果表明该方法在主流数据集上实现了86.92%的码率下降和约20倍的解码速度提升，达到最新感知压缩性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="151-X2Edit-Revisiting-Arbitrary-Instruction-Image-Editing-through-Self-Constructed-Data-and-Task-Aware-Representation-Learning"><a href="#151-X2Edit-Revisiting-Arbitrary-Instruction-Image-Editing-through-Self-Constructed-Data-and-Task-Aware-Representation-Learning" class="headerlink" title="151. X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning"></a>151. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/X2Edit__Revisiting_Arbitrary-Instruction_Image_Editing_through_Self-Constructed_Data_and_Task-Aware_.pdf">X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">OPPO AI Center</span></p><p>本文提出了X2Edit Dataset，一个涵盖14类编辑任务、规模达370万的高质量图像编辑数据集，并设计了基于FLUX.1的轻量级、多专家（MoE-LoRA）和任务感知对比学习的X2Edit模型。实验表明，该方法在多个开放基准上与SOTA模型性能相当甚至优于部分主流方法，具备良好的可扩展性与插拔能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="152-Separation-and-Collaboration-Two-Level-Routing-Grouped-Mixture-of-Experts-for-Multi-Domain-Continual-Learning"><a href="#152-Separation-and-Collaboration-Two-Level-Routing-Grouped-Mixture-of-Experts-for-Multi-Domain-Continual-Learning" class="headerlink" title="152. Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning"></a>152. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Separation_and_Collaboration__Two-Level_Routing_Grouped_Mixture-of-Experts_for_Multi-Domain_Continua.pdf">Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National University of Defense Technology</span></p><p>论文提出了Two-Level Routing Grouped Mixture-of-Experts (TRGE)方法，针对多域连续学习中的灾难性遗忘和前向遗忘问题。方法通过在预训练的CLIP视觉语言模型基础上，为每个任务扩展专家组并冻结历史专家组，结合组内静态专家数和组间动态路由策略，提升任务协作与抗遗忘能力；利用多模态大语言模型实现语义任务识别，并对模型输出进行动态融合以增强零样本泛化。实验结果显示，TRGE在多域任务增量和类别增量场景下均优于现有方法，且参数量更少。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="153-TAR-TVG-Enhancing-VLMs-with-Timestamp-Anchor-Constrained-Reasoning-for-Temporal-Video-Grounding"><a href="#153-TAR-TVG-Enhancing-VLMs-with-Timestamp-Anchor-Constrained-Reasoning-for-Temporal-Video-Grounding" class="headerlink" title="153. TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding"></a>153. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TAR-TVG__Enhancing_VLMs_with_Timestamp_Anchor-Constrained_Reasoning_for_Temporal_Video_Grounding.pdf">TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China University of Technology</span></p><p>该论文提出TAR-TVG框架，在视频理解任务中利用时间戳锚点约束模型的推理过程，实现逐步细化的时序定位。方法采用三阶段训练策略（GRPO强化学习、监督微调、再强化学习），通过插入时间戳锚点提升推理可解释性和定位精度。实验表明，TAR-TVG在Charades-STA等基准数据集取得了当前最优性能，显著提升了视频时序定位能力和推理链条的可验证性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="154-LaRender-Training-Free-Occlusion-Control-in-Image-Generation-via-Latent-Rendering"><a href="#154-LaRender-Training-Free-Occlusion-Control-in-Image-Generation-via-Latent-Rendering" class="headerlink" title="154. LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering"></a>154. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LaRender__Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering.pdf">LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent</span></p><p>本文提出LaRender，一种无需训练的图像生成算法，能精准控制图像中物体间的遮挡关系。方法通过在预训练扩散模型的潜空间应用体积渲染原理，结合遮挡图和对象透射率，实现物体遮挡和透明度的物理一致性控制。实验表明，LaRender在遮挡准确性上显著优于现有方法，并支持丰富的视觉效果如对象透明度和光效调节。结论：LaRender实现了高效、物理一致的遮挡和概念强度控制，无需额外训练。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="155-Breaking-Down-and-Building-Up-Mixture-of-Skill-Based-Vision-and-Language-Navigation-Agents"><a href="#155-Breaking-Down-and-Building-Up-Mixture-of-Skill-Based-Vision-and-Language-Navigation-Agents" class="headerlink" title="155. Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents"></a>155. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Breaking_Down_and_Building_Up__Mixture_of_Skill-Based_Vision-and-Language_Navigation_Agents.pdf">Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Michigan State University</span></p><p>该论文提出SkillNav，一种将视觉-语言导航任务拆解为可解释的原子技能（如方向调整、垂直移动、地标检测、区域识别等）的模块化框架。通过LLM引导的指令重排序和VLM驱动的路由器动态选择最合适的技能代理，实现对复杂环境和新指令的强泛化能力。SkillNav在R2R和GSA-R2R等主流基准上取得了SOTA性能，实验表明其提升了导航效率和多样场景下的泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="156-Surgical-Knowledge-Rewrite-in-Compact-LLMs-An-‘Unlearn-then-Learn’-Strategy-with-IA3-for-Localized-Factual-Modulation-and-Catastrophic-Forgetting-Mitigation"><a href="#156-Surgical-Knowledge-Rewrite-in-Compact-LLMs-An-‘Unlearn-then-Learn’-Strategy-with-IA3-for-Localized-Factual-Modulation-and-Catastrophic-Forgetting-Mitigation" class="headerlink" title="156. Surgical Knowledge Rewrite in Compact LLMs: An ‘Unlearn-then-Learn’ Strategy with (IA3) for Localized Factual Modulation and Catastrophic Forgetting Mitigation"></a>156. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Surgical_Knowledge_Rewrite_in_Compact_LLMs__An_'Unlearn-then-Learn'_Strategy_with_($IA%5E3$)_for_Local.pdf">Surgical Knowledge Rewrite in Compact LLMs: An ‘Unlearn-then-Learn’ Strategy with (IA3) for Localized Factual Modulation and Catastrophic Forgetting Mitigation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University</span></p><p>本文提出了一种针对紧凑型大语言模型（LLM）的“unlearn-then-learn”知识编辑策略，利用IA3参数高效微调技术和电路定位分析，分阶段先抑制冲突旧知识再注入新事实，有效实现局部化事实重写并显著缓解灾难性遗忘。实验结果显示，该方法可精确地将新事实融入模型（98.5%准确率），同时大幅抑制原有冲突事实（96%遗忘率），且保留了72%无关知识，为安全、可控的动态知识管理带来突破。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="157-Towards-Safer-AI-Moderation-Evaluating-LLM-Moderators-Through-a-Unified-Benchmark-Dataset-and-Advocating-a-Human-First-Approach"><a href="#157-Towards-Safer-AI-Moderation-Evaluating-LLM-Moderators-Through-a-Unified-Benchmark-Dataset-and-Advocating-a-Human-First-Approach" class="headerlink" title="157. Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach"></a>157. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Towards_Safer_AI_Moderation__Evaluating_LLM_Moderators_Through_a_Unified_Benchmark_Dataset_and_Advoc.pdf">Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Fordham University</span></p><p>本论文提出了一个统一的人类标注数据集，涵盖49类情感、攻击性言论及偏见内容，并据此评估了主流LLM内容审核器的能力。同时，作者提出并微调了SafePhi模型（基于Phi-4、采用QLoRA），其在宏F1分数上优于OpenAI Moderator和Llama Guard。结论指出，现有LLM审核器在隐含性仇恨、性别与种族偏见检测上表现有限，强调需引入多样化数据及人类参与，以提升模型可靠性和可解释性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="158-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability"><a href="#158-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability" class="headerlink" title="158. ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability"></a>158. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ReasonRank__Empowering_Passage_Ranking_with_Strong_Reasoning_Ability.pdf">ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Renmin University of China</span></p><p>该论文提出ReasonRank，一种基于大语言模型的推理强化型文本重排方法。方法包括自动合成多领域推理密集型训练数据，通过DeepSeek-R1生成高质量标签，并采用自一致性数据筛选；再通过两阶段训练框架，先用监督微调(SFT)学习推理模式，再用多视角奖励的强化学习(RL)提升排序能力。结论显示ReasonRank在BRIGHT和R2MED推理密集型信息检索基准上取得SOTA性能，效率优于现有基线，并具备较强通用性和推理能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="159-K-Dense-Analyst-Towards-Fully-Automated-Scientific-Analysis"><a href="#159-K-Dense-Analyst-Towards-Fully-Automated-Scientific-Analysis" class="headerlink" title="159. K-Dense Analyst: Towards Fully Automated Scientific Analysis"></a>159. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/K-Dense_Analyst__Towards_Fully_Automated_Scientific_Analysis.pdf">K-Dense Analyst: Towards Fully Automated Scientific Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Biostate AI</span></p><p>K-Dense Analyst是一种基于层级多智能体系统的科学分析平台，通过双环架构实现自主生物信息学分析，包括规划、验证和执行。该方法在BixBench开放式生物分析基准上取得了29.2%的准确率，显著超过当前最强的语言模型（如GPT-5），证明架构创新比模型规模更重要，推动了科学分析的自动化进程。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="160-From-Nodes-to-Narratives-Explaining-Graph-Neural-Networks-with-LLMs-and-Graph-Context"><a href="#160-From-Nodes-to-Narratives-Explaining-Graph-Neural-Networks-with-LLMs-and-Graph-Context" class="headerlink" title="160. From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context"></a>160. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Nodes_to_Narratives__Explaining_Graph_Neural_Networks_with_LLMs_and_Graph_Context.pdf">From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Chicago</span></p><p>本文提出了LOGIC，一种利用大语言模型（LLM）对图神经网络（GNN）进行解释的轻量级后验框架。方法核心为将GNN节点嵌入投影到LLM的嵌入空间，构建融合软提示和结构文本的混合提示，驱动LLM生成与模型内部表示相关的自然语言解释及解释子图。LOGIC无需额外训练，可直接部署在预训练GNN和LLM上，实验证明其在保持高保真度的同时生成更紧凑且具备可解释性的子图解释，并在人类评测中提升了可理解性和说服力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="161-Investigating-Intersectional-Bias-in-Large-Language-Models-using-Confidence-Disparities-in-Coreference-Resolution"><a href="#161-Investigating-Intersectional-Bias-in-Large-Language-Models-using-Confidence-Disparities-in-Coreference-Resolution" class="headerlink" title="161. Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution"></a>161. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Investigating_Intersectional_Bias_in_Large_Language_Models_using_Confidence_Disparities_in_Coreferen.pdf">Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Apple</span></p><p>该论文提出WinoIdentity数据集，通过扩展WinoBias为10类属性、25个群体标记，设计245,700个探测句，并用“Coreference Confidence Disparity”不公平性指标，系统评估五种主流LLMs在多重交叉身份下的推理与公平性。结果发现LLMs在交叉身份的指代消解任务中表现较差，存在高达40%的置信度差异，尤其对双重劣势群体偏见更严重，且模型更多依赖记忆而非真实推理，凸显现实应用中的社会伤害风险。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="162-Hide-or-Highlight-Understanding-the-Impact-of-Factuality-Expression-on-User-Trust"><a href="#162-Hide-or-Highlight-Understanding-the-Impact-of-Factuality-Expression-on-User-Trust" class="headerlink" title="162. Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust"></a>162. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hide_or_Highlight__Understanding_the_Impact_of_Factuality_Expression_on_User_Trust.pdf">Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IBM Research</span></p><p>本文提出并比较了五种在AI生成内容中表达事实性的方法（透明、注意、遮蔽、模糊、基线），其中遮蔽和模糊策略通过隐藏或弱化低事实性内容，有效提升了用户对AI系统的信任且不损害答案质量。实验证明，隐藏低事实性内容或将其改为模糊表达更能增强用户信任，优于高亮显示策略。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="163-SEADialogues-A-Multilingual-Culturally-Grounded-Multi-turn-Dialogue-Dataset-on-Southeast-Asian-Languages"><a href="#163-SEADialogues-A-Multilingual-Culturally-Grounded-Multi-turn-Dialogue-Dataset-on-Southeast-Asian-Languages" class="headerlink" title="163. SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages"></a>163. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SEADialogues__A_Multilingual_Culturally_Grounded_Multi-turn_Dialogue_Dataset_on_Southeast_Asian_Lang.pdf">SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">MBZUAI</span></p><p>本论文提出了SEADialogUES，这是一个面向东南亚八种语言、六国多文化背景的多轮、多语种、具备个性化设定的对话数据集。其方法包括模板生成、文化元素词汇化、利用大语言模型自动生成对话，并结合人工与LLM自动评测，验证数据的文化相关性、流畅性和自然度。结论显示，SEADialogUES能有效促进文化敏感、个性化对话系统的发展，且高质量的文化数据集有助于提升开源大模型性能，弥补与专有模型的差距。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="164-Membership-and-Memorization-in-LLM-Knowledge-Distillation"><a href="#164-Membership-and-Memorization-in-LLM-Knowledge-Distillation" class="headerlink" title="164. Membership and Memorization in LLM Knowledge Distillation"></a>164. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Membership_and_Memorization_in_LLM_Knowledge_Distillation.pdf">Membership and Memorization in LLM Knowledge Distillation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文系统分析了六种大型语言模型（LLM）知识蒸馏（KD）技术在隐私保护方面的风险，主要关注成员推断攻击和数据记忆泄漏。通过多模型、多任务实验，作者发现所有现有 KD 方法都会将教师模型的隐私风险传递给学生模型，但不同技术泄漏程度不同，并提出了按模型块进行精细化隐私风险分析框架。结论表明，无论采用哪种蒸馏方法，学生模型都无法彻底保护教师模型的训练数据隐私，且隐私泄漏在不同技术和模型块之间差异显著。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="165-Whisfusion-Parallel-ASR-Decoding-via-a-Diffusion-Transformer"><a href="#165-Whisfusion-Parallel-ASR-Decoding-via-a-Diffusion-Transformer" class="headerlink" title="165. Whisfusion: Parallel ASR Decoding via a Diffusion Transformer"></a>165. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Whisfusion__Parallel_ASR_Decoding_via_a_Diffusion_Transformer.pdf">Whisfusion: Parallel ASR Decoding via a Diffusion Transformer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Seoul National University</span></p><p>该论文提出Whisfusion框架，将预训练Whisper语音编码器与文本扩散解码器通过轻量级Cross-Attention适配器融合，实现了非自回归、全并行的ASR解码流程，并设计了批量并行多步解码策略（PDD）。实验表明，Whisfusion在LibriSpeech上相较于Whisper-tiny提升了识别准确率（8.3% vs 9.7% WER），且在长语音段落上解码速度提升最高达2.6倍，展示了扩散式非自回归解码在高吞吐低延迟ASR系统中的可行性和高效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="166-Trustworthy-Medical-Imaging-with-Large-Language-Models-A-Study-of-Hallucinations-Across-Modalities"><a href="#166-Trustworthy-Medical-Imaging-with-Large-Language-Models-A-Study-of-Hallucinations-Across-Modalities" class="headerlink" title="166. Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities"></a>166. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Trustworthy_Medical_Imaging_with_Large_Language_Models__A_Study_of_Hallucinations_Across_Modalities.pdf">Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Akron</span></p><p>本论文系统评估了大语言模型（LLM）在医学影像中的幻觉现象，涵盖影像解释（image-to-text）和影像生成（text-to-image）任务，通过定量实验分析了模型在X射线、CT和MRI等多模态影像报告生成及合成图像时存在的事实不一致和解剖错误。结论表明，当前LLM在医学影像任务中存在关键可靠性漏洞，需加强医学约束和鲁棒性以提升临床安全与信任。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="167-MultiMedEdit-A-Scenario-Aware-Benchmark-for-Evaluating-Knowledge-Editing-in-Medical-VQA"><a href="#167-MultiMedEdit-A-Scenario-Aware-Benchmark-for-Evaluating-Knowledge-Editing-in-Medical-VQA" class="headerlink" title="167. MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA"></a>167. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MultiMedEdit__A_Scenario-Aware_Benchmark_for_Evaluating_Knowledge_Editing_in_Medical_VQA.pdf">MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University of Aeronautics and Astronautics</span></p><p>该论文提出了MultiMedEdit，这是首个专为临床多模态任务设计的医学知识编辑评测基准，涵盖理解与推理两类任务，并建立了可靠性、泛化性和局部性三维度的评估框架，支持跨范式方法比较。实验表明，现有知识编辑方法在复杂医学场景下表现出泛化能力弱、长期编辑易遗忘和稳定性不足等问题。结论：MultiMedEdit揭示了当前方法的局限性，为未来开发更可靠医学知识编辑技术奠定了基础。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="168-MASteer-Multi-Agent-Adaptive-Steer-Strategy-for-End-to-End-LLM-Trustworthiness-Repair"><a href="#168-MASteer-Multi-Agent-Adaptive-Steer-Strategy-for-End-to-End-LLM-Trustworthiness-Repair" class="headerlink" title="168. MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair"></a>168. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MASteer__Multi-Agent_Adaptive_Steer_Strategy_for_End-to-End_LLM_Trustworthiness_Repair.pdf">MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出MASteer，首个基于表征工程的端到端大语言模型（LLM）可信度修复框架，包含多智能体样本生成（AutoTester）和自适应修复策略构建（AutoRepairer），实现自动化问题分析、样本构建、策略选取与推理时动态修复。实验证明MASteer在多项信任任务上显著提升LLM的真实性、公平性与安全性，且不损害模型通用能力，具备高效、可扩展和定制化的实际价值。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="169-BoRA-Towards-More-Expressive-Low-Rank-Adaptation-with-Block-Diversity"><a href="#169-BoRA-Towards-More-Expressive-Low-Rank-Adaptation-with-Block-Diversity" class="headerlink" title="169. BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity"></a>169. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BoRA__Towards_More_Expressive_Low-Rank_Adaptation_with_Block_Diversity.pdf">BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Huazhong University of Science and Technology</span></p><p>该论文提出了Block-Diversified Low-Rank Adaptation (BoRA)方法，通过将低秩矩阵分块并引入块对角矩阵，有效提升了LoRA权重的秩和表达能力。实验结果表明，BoRA在参数数量接近的情况下，能比LoRA及其多种变体在多任务和多模型基准测试上提升2-4%的准确率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="170-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance"><a href="#170-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance" class="headerlink" title="170. AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance"></a>170. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AMFT__Aligning_LLM_Reasoners_by_Meta-Learning_the_Optimal_Imitation-Exploration_Balance.pdf">AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本文提出了一种单阶段大型语言模型（LLM）对齐算法AMFT，利用元学习控制器动态优化模仿（SFT）与探索（RL）之间的权重，通过元梯度联合熵启发式，实现基于隐式和显式奖励信号的自适应训练流程。实验在数学推理、多模态视觉推理和视觉-语言导航任务上均取得了SOTA表现，显著提升了模型的泛化能力和样本效率，避免灾难性遗忘和策略坍塌。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="171-CLAP-Coreference-Linked-Augmentation-for-Passage-Retrieval"><a href="#171-CLAP-Coreference-Linked-Augmentation-for-Passage-Retrieval" class="headerlink" title="171. CLAP: Coreference-Linked Augmentation for Passage Retrieval"></a>171. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CLAP__Coreference-Linked_Augmentation_for_Passage_Retrieval.pdf">CLAP: Coreference-Linked Augmentation for Passage Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Adelaide</span></p><p>该论文提出了CLAP，一种基于大语言模型的检索增强框架，通过语义分块、指代消解和局部伪查询生成三步流程，将长文本结构化为多角度、无歧义的检索信号，并融合全局与细粒度相关性分数。在多种主流稠密和稀疏检索器及多领域数据集上，CLAP显著提升了检索性能，尤其在领域外泛化场景下表现优异，部分情况下超过了二阶段重排序器。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="172-CROP-Integrating-Topological-and-Spatial-Structures-via-Cross-View-Prefixes-for-Molecular-LLMs"><a href="#172-CROP-Integrating-Topological-and-Spatial-Structures-via-Cross-View-Prefixes-for-Molecular-LLMs" class="headerlink" title="172. CROP: Integrating Topological and Spatial Structures via Cross-View Prefixes for Molecular LLMs"></a>172. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CROP__Integrating_Topological_and_Spatial_Structures_via_Cross-View_Prefixes_for_Molecular_LLMs.pdf">CROP: Integrating Topological and Spatial Structures via Cross-View Prefixes for Molecular LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出了一种创新的分子多模态大语言模型架构CROP，通过SMILES引导的联合重采样，将分子图（拓扑结构）和分子图像（空间结构）信息融合为固定长度前缀，有效增强LLM对分子的理解能力。实验证明，CROP在分子描述生成、IUPAC命名预测和分子性质预测等任务上显著优于现有方法，验证了多视图集成和高效结构信息利用的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="173-DocRefine-An-Intelligent-Framework-for-Scientific-Document-Understanding-and-Content-Optimization-based-on-Multimodal-Large-Model-Agents"><a href="#173-DocRefine-An-Intelligent-Framework-for-Scientific-Document-Understanding-and-Content-Optimization-based-on-Multimodal-Large-Model-Agents" class="headerlink" title="173. DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents"></a>173. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DocRefine__An_Intelligent_Framework_for_Scientific_Document_Understanding_and_Content_Optimization_b.pdf">DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shangqiu University</span></p><p>该论文提出DocRefine框架，利用GPT-4o等先进视觉语言大模型（LVLMs），通过六大多智能体协作，实现对科学PDF文档的结构分析、多模态内容理解、指令分解、内容优化、自动总结和一致性验证。实验结果显示，DocRefine在DocEditBench数据集上显著优于现有方法，在语义一致性、布局保真和指令遵循方面均取得了领先成绩，有效提升了复杂科学文档的自动化处理能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="174-DATASETRESEARCH-Benchmarking-Agent-Systems-for-Demand-Driven-Dataset-Discovery"><a href="#174-DATASETRESEARCH-Benchmarking-Agent-Systems-for-Demand-Driven-Dataset-Discovery" class="headerlink" title="174. DATASETRESEARCH: Benchmarking Agent Systems for Demand-Driven Dataset Discovery"></a>174. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DatasetResearch__Benchmarking_Agent_Systems_for_Demand-Driven_Dataset_Discovery.pdf">DATASETRESEARCH: Benchmarking Agent Systems for Demand-Driven Dataset Discovery</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai Jiao Tong University</span></p><p>该论文提出了DATASETRESEARCH，这是首个系统评估AI智能体基于用户需求发现和合成数据集能力的综合基准，包括208个真实世界任务，覆盖知识型与推理型任务。通过三维评测框架，对搜索、合成和深度研究智能体在元数据对齐、Few-shot和微调等下游任务中的表现进行量化，发现当前最先进系统在高难度子集上最高仅达22%，揭示了现有方法在通用数据发现上的巨大差距。结论：现有智能体在知识任务中搜索表现优异，在推理任务中合成方法更强，但在极端“角落案例”上均表现不佳，凸显AI数据发现仍有广阔提升空间。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="175-Large-Language-Models-Do-Not-Simulate-Human-Psychology"><a href="#175-Large-Language-Models-Do-Not-Simulate-Human-Psychology" class="headerlink" title="175. Large Language Models Do Not Simulate Human Psychology"></a>175. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Large_Language_Models_Do_Not_Simulate_Human_Psychology.pdf">Large Language Models Do Not Simulate Human Psychology</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bielefeld University</span></p><p>本文通过理论分析和实证研究，评估了当前大模型（如GPT-4、Llama-3.1 70b及CENTAUR）是否能模拟人类心理学。作者发现，尽管在部分常规任务中LLM与人类响应高度一致，但对语义变化敏感性不足，难以准确模拟人类心理反应，结论是LLM不能替代真实人类参与者，需谨慎用于心理学实验。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="176-When-Prompt-Engineering-Meets-Software-Engineering-CNL-P-as-Natural-and-Robust-APIs-for-Human-AI-Interaction"><a href="#176-When-Prompt-Engineering-Meets-Software-Engineering-CNL-P-as-Natural-and-Robust-APIs-for-Human-AI-Interaction" class="headerlink" title="176. When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust &quot;APIs&quot; for Human-AI Interaction"></a>176. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/When_Prompt_Engineering_Meets_Software_Engineering__CNL-P_as_Natural_and_Robust__APIs''_for_Human-AI.pdf">When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust &quot;APIs&quot; for Human-AI Interaction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">CSIRO’s Data61</span></p><p>本文提出了CNL-P（Controlled Natural Language for Prompt），融合了提示工程（PE）和软件工程（SE）最佳实践，通过精确定义语法结构和严格的语义规范，消除自然语言提示中的歧义，实现用户意图的准确表达。作者开发了NL到CNL-P自动转换工具和CNL-P linting静态分析工具，并通过大量实验验证，CNL-P可提升LLM的输出质量，其语法和语义无需额外说明即可被主流大模型有效理解，且linting工具能够高效发现和定位结构化提示中的语法与语义错误。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="177-Model-Agnostic-Sentiment-Distribution-Stability-Analysis-for-Robust-LLM-Generated-Texts-Detection"><a href="#177-Model-Agnostic-Sentiment-Distribution-Stability-Analysis-for-Robust-LLM-Generated-Texts-Detection" class="headerlink" title="177. Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection"></a>177. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Model-Agnostic_Sentiment_Distribution_Stability_Analysis_for_Robust_LLM-Generated_Texts_Detection.pdf">Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science, Shanghai Jiao Tong University</span></p><p>该论文提出了SentiDetect，一种基于情感分布稳定性分析的模型无关LLM生成文本检测框架。方法包括低情感重写、情感分布特征提取及稳定性分析，通过定义情感分布一致性和保持性指标，无需模型参数或监督训练即可区分LLM生成文本和人类写作。实验证明SentiDetect在五个数据集和多种LLM（如Gemini-1.5-Pro、Claude-3、GPT-4-0613、LLaMa-3.3）上F1分数最高提升超16%，且对同义改写和对抗攻击等具有更强鲁棒性。结论：SentiDetect显著提升LLM生成文本检测的准确性和鲁棒性，适用于多领域。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="178-BASIC-Boosting-Visual-Alignment-with-Intrinsic-Refined-Embeddings-in-Multimodal-Large-Language-Models"><a href="#178-BASIC-Boosting-Visual-Alignment-with-Intrinsic-Refined-Embeddings-in-Multimodal-Large-Language-Models" class="headerlink" title="178. BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models"></a>178. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BASIC__Boosting_Visual_Alignment_with_Intrinsic_Refined_Embeddings_in_Multimodal_Large_Language_Mode.pdf">BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本文提出BASIC方法，通过利用大型语言模型（LLM）浅层中的细化视觉嵌入作为监督信号，直接引导视觉投影器生成更优的初始视觉嵌入，实现视觉与文本模态的更细致对齐。该方法通过方向对齐和语义分布匹配两大优化目标，无需额外监督模型或人工标注，显著提升了多模态大模型（MLLMs）在多项主流视觉理解基准上的表现，验证了直接视觉监督的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="179-MeteorPred-A-Meteorological-Multimodal-Large-Model-and-Dataset-for-Severe-Weather-Event-Prediction"><a href="#179-MeteorPred-A-Meteorological-Multimodal-Large-Model-and-Dataset-for-Severe-Weather-Event-Prediction" class="headerlink" title="179. MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction"></a>179. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MeteorPred__A_Meteorological_Multimodal_Large_Model_and_Dataset_for_Severe_Weather_Event_Prediction.pdf">MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Automation, Chinese Academy of Sciences</span></p><p>该论文提出了MP-Bench大规模气象多模态数据集，并基于此开发了气象多模态大模型（MMLM），该模型能够直接处理4D气象数据输入，并集成了三种自适应特征融合模块（DTGF、TGS、TGCA），分别提升模型在时间、空间和垂直压力层维度上的特征抽取与融合能力。实验表明MMLM在多项严酷天气事件预测与问答任务上显著优于现有开源和闭源方法，为自动化AI驱动的天气预报系统提供了关键进展。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="180-Highlight-All-the-Phrases-Enhancing-LLM-Transparency-through-Visual-Factuality-Indicators"><a href="#180-Highlight-All-the-Phrases-Enhancing-LLM-Transparency-through-Visual-Factuality-Indicators" class="headerlink" title="180. Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators"></a>180. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Highlight_All_the_Phrases__Enhancing_LLM_Transparency_through_Visual_Factuality_Indicators.pdf">Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IBM Research, Cambridge, MA, USA</span></p><p>本论文提出并系统评估了多种用于在LLM问答场景中可视化展示事实性分数的设计策略，包括全短语高亮、阈值高亮和数值标注，粒度分为词级和短语级。通过两轮用户实验发现，基于事实性分数对所有短语进行高亮的可视化设计最受用户偏好、信任度最高且能显著提升用户校验LLM输出准确性的便利性，建议开发者优先采用该设计以提升用户对LLM输出的信任与辨别能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="181-BiXSE-Improving-Dense-Retrieval-via-Probabilistic-Graded-Relevance-Distillation"><a href="#181-BiXSE-Improving-Dense-Retrieval-via-Probabilistic-Graded-Relevance-Distillation" class="headerlink" title="181. BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation"></a>181. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BiXSE__Improving_Dense_Retrieval_via_Probabilistic_Graded_Relevance_Distillation.pdf">BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universite de Montréal</span></p><p>该论文提出BiXSE方法，用二元交叉熵损失直接对LLM生成的连续型分级相关性标签进行训练，实现了更细粒度的监督信息注入到密集检索模型。实验表明，BiXSE在多种检索与句子嵌入基准上均优于标准InfoNCE对比学习目标，并对标签噪声表现出更强鲁棒性，是高效且可扩展的LLM知识蒸馏方案。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="182-Zero-Direction-Probing-A-Linear-Algebraic-Framework-for-Deep-Analysis-of-Large-Language-Model-Drift"><a href="#182-Zero-Direction-Probing-A-Linear-Algebraic-Framework-for-Deep-Analysis-of-Large-Language-Model-Drift" class="headerlink" title="182. Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift"></a>182. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Zero-Direction_Probing__A_Linear-Algebraic_Framework_for_Deep_Analysis_of_Large-Language-Model_Drift.pdf">Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AI Analytics</span></p><p>本文提出了Zero-Direction Probing (ZDP)理论框架，利用变换器层激活矩阵的左右零空间（null space）及其Fisher信息几何来刻画大语言模型（LLM）表示漂移，无需标签或输出，仅基于线性代数和信息几何工具。核心贡献包括：证明了漂移与零空间能量之间的Variance–Leak定理，提出Fisher Null-Conservation定理、低秩LoRA更新的Rank–Leak界、基于随机矩阵理论的无参数漂移阈值，以及在线零空间跟踪与漂移保证。结论指出：只要“监听沉默”（监控零空间），就能实现可验证的模型表示变化检测，理论结果直接面向实践，且所有定理经验证均可迁移。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="183-Many-Turn-Jailbreaking"><a href="#183-Many-Turn-Jailbreaking" class="headerlink" title="183. Many-Turn Jailbreaking"></a>183. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Many-Turn_Jailbreaking.pdf">Many-Turn Jailbreaking</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of California, Santa Barbara</span></p><p>该论文首次系统性研究了大语言模型（LLMs）在多轮对话场景下的越狱攻击问题，提出“Many-Turn Jailbreaking”概念，并创建了MTJ-Bench多轮越狱评测基准，对14个开源和1个闭源模型进行了实证测试。结果显示，一旦模型在首轮被越狱后，后续相关或无关问题均易被持续攻破，安全隐患远超以往单轮越狱，亟需社区关注和防护。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="184-MDK12-Bench-A-Comprehensive-Evaluation-of-Multimodal-Large-Language-Models-on-Multidisciplinary-Exams"><a href="#184-MDK12-Bench-A-Comprehensive-Evaluation-of-Multimodal-Large-Language-Models-on-Multidisciplinary-Exams" class="headerlink" title="184. MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams"></a>184. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MDK12-Bench__A_Comprehensive_Evaluation_of_Multimodal_Large_Language_Models_on_Multidisciplinary_Exa.pdf">MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shanghai AI Laboratory</span></p><p>本文提出MDK12-Bench，一个大规模多学科、多模态基准，涵盖6个学科、141K K-12真实考试题，并以六层知识点体系组织。方法包括多维度动态评测框架和知识点参考增强生成（KP-RAG），系统评估MLLM在难度、时间、上下文和知识利用方面的泛化能力。实验发现，现有MLLM对动态扰动与新颖任务敏感，在复杂推理和知识整合上存在明显短板，KP-RAG对困难题提升有限。结论表明，MDK12-Bench有助于精准诊断多模态大模型的优势与不足，为未来更强泛化和推理能力的AI发展提供方向。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="185-Remote-Sensing-Image-Intelligent-Interpretation-with-the-Language-Centered-Perspective-Principles-Methods-and-Challenges"><a href="#185-Remote-Sensing-Image-Intelligent-Interpretation-with-the-Language-Centered-Perspective-Principles-Methods-and-Challenges" class="headerlink" title="185. Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges"></a>185. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Remote_Sensing_Image_Intelligent_Interpretation_with_the_Language-Centered_Perspective__Principles,_.pdf">Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Central South University</span></p><p>本文提出将遥感图像智能解读从视觉中心范式转向语言中心范式，借鉴全球工作空间理论（GWT），以大语言模型（LLM）为认知核心，实现感知、任务、知识、动作空间的统一融合。提出了基于语言的遥感解释系统理论框架，系统梳理多模态统一表达、知识关联与推理决策等技术难题，并展望自适应多模态对齐、动态知识约束下的任务理解、可信推理和自主交互等未来方向。结论认为，语言中心范式能突破传统视觉模型的认知瓶颈，推动遥感解释向具备认知进化能力的智能体发展。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="186-Technical-Report-Full-Stack-Fine-Tuning-for-the-Q-Programming-Language"><a href="#186-Technical-Report-Full-Stack-Fine-Tuning-for-the-Q-Programming-Language" class="headerlink" title="186. Technical Report: Full-Stack Fine-Tuning for the Q Programming Language"></a>186. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Technical_Report__Full-Stack_Fine-Tuning_for_the_Q_Programming_Language.pdf">Technical Report: Full-Stack Fine-Tuning for the Q Programming Language</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Morgan Stanley</span></p><p>本论文提出了一套完整的开源方法，实现了对大型语言模型（LLM）在Q编程语言上的适应，包括数据集构建、预训练、监督微调和强化学习等流程，显著提升了模型在Q-LeetCode基准上的代码生成能力。最终，所训练的Qwen-2.5系列模型在Q任务上均超越了GPT-4.1，最大模型比Claude Opus-4提升29.5%，为低资源领域LLM适配提供了可复用蓝图。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="187-LSDTs-LLM-Augmented-Semantic-Digital-Twins-for-Adaptive-Knowledge-Intensive-Infrastructure-Planning"><a href="#187-LSDTs-LLM-Augmented-Semantic-Digital-Twins-for-Adaptive-Knowledge-Intensive-Infrastructure-Planning" class="headerlink" title="187. LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning"></a>187. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LSDTs__LLM-Augmented_Semantic_Digital_Twins_for_Adaptive_Knowledge-Intensive_Infrastructure_Planning.pdf">LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Maryland, College Park</span></p><p>本论文提出LSDTs框架，将大语言模型（LLM）与数字孪生（DT）结合，实现对基础设施规划中非结构化法规与技术文档的自动知识抽取和结构化建模，关键技术包括LLM驱动的本体提取、RDF语义图生成、推理与仿真集成，以及动态情境下的自适应优化。实验以马里兰州离岸风电场为例，展示该方法能自动集成法规约束、生成合规布局、并在极端天气情境下支持仿真和自动调优。结论认为，LSDTs可提升基础设施规划的数据驱动、法规合规与适应性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="188-PROPS-Progressively-Private-Self-alignment-of-Large-Language-Models"><a href="#188-PROPS-Progressively-Private-Self-alignment-of-Large-Language-Models" class="headerlink" title="188. PROPS: Progressively Private Self-alignment of Large Language Models"></a>188. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PROPS__Progressively_Private_Self-alignment_of_Large_Language_Models.pdf">PROPS: Progressively Private Self-alignment of Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Arizona</span></p><p>该论文提出了PROPS（Progressively Private Self-alignment），一种多阶段的隐私保护大语言模型（LLM）对齐算法。方法流程包括：将数据集分割为多个阶段，首阶段用随机响应机制扰动偏好标签并对模型进行初步对齐，后续阶段利用前一阶段模型对新数据进行偏好预测，并结合最大似然估计整合模型预测和扰动标签，从而进一步提升对齐性能和隐私保护。实验表明，在同等隐私预算下，PROPS在多模型和数据集上对齐质量显著优于DP-SGD和随机响应方法，尤其在高隐私需求场景下表现突出。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="189-Story-Ribbons-Reimagining-Storyline-Visualizations-with-Large-Language-Models"><a href="#189-Story-Ribbons-Reimagining-Storyline-Visualizations-with-Large-Language-Models" class="headerlink" title="189. Story Ribbons: Reimagining Storyline Visualizations with Large Language Models"></a>189. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Story_Ribbons__Reimagining_Storyline_Visualizations_with_Large_Language_Models.pdf">Story Ribbons: Reimagining Storyline Visualizations with Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Harvard University</span></p><p>本文提出了一种基于大语言模型（LLM）的自动小说结构化信息抽取管道，结合交互式可视化系统STORY RIBBONS，实现小说&#x2F;剧本中角色、地点、主题等多维故事要素的自动提取与可视化分析。实验和用户研究表明，该系统显著降低了文学文本分析的门槛，帮助用户发现新颖故事洞察，但当前LLM在文学细节理解与歧义消解上仍有限。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="190-Fed-MobiLLM-Efficient-Federated-LLM-Fine-Tuning-over-Heterogeneous-Mobile-Devices-via-Server-Assisted-Side-Tuning"><a href="#190-Fed-MobiLLM-Efficient-Federated-LLM-Fine-Tuning-over-Heterogeneous-Mobile-Devices-via-Server-Assisted-Side-Tuning" class="headerlink" title="190. Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning"></a>190. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fed_MobiLLM__Efficient_Federated_LLM_Fine-Tuning_over_Heterogeneous_Mobile_Devices_via_Server_Assist.pdf">Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Houston</span></p><p>Fed MobiLLM提出了一种异步、服务器辅助的side-tuning范式，实现了在异构移动设备上的大语言模型（LLM）高效联邦微调。方法上，移动端仅保留冻结的backbone，执行前向传播并上传中间激活，服务器异步训练统一的side-network，并通过分层激活采样和跨结构特征对齐支持不同设备模型的高效协同。结论证明，该方法大幅降低了设备端内存和计算、通信消耗，并实现更快收敛，在IID和non-IID数据分布下均能保持强劲性能，适用于实际分布式移动场景。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="191-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy"><a href="#191-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy" class="headerlink" title="191. Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy"></a>191. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Democratizing_Diplomacy__A_Harness_for_Evaluating_Any_Large_Language_Model_on_Full-Press_Diplomacy.pdf">Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Good Start Labs</span></p><p>该论文提出了首个无需微调即可让任意本地大型语言模型（LLM）完整参与Full-Press Diplomacy（全交流外交棋局）的评测框架。方法包括数据驱动的文本化博弈状态表示、自动化对局流程、批量模型对比、Critical State Analysis关键局势分析等，可高效分析模型的策略推理、外交表现、承诺与背叛等行为特征。结论表明：即使是24B参数的小型通用模型在未专门训练下也能顺利完成博弈且展现出自然涌现的战略能力，框架极大降低了战略推理类LLM研究门槛。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="192-AURA-A-Fine-Grained-Benchmark-and-Decomposed-Metric-for-Audio-Visual-Reasoning"><a href="#192-AURA-A-Fine-Grained-Benchmark-and-Decomposed-Metric-for-Audio-Visual-Reasoning" class="headerlink" title="192. AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning"></a>192. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AURA__A_Fine-Grained_Benchmark_and_Decomposed_Metric_for_Audio-Visual_Reasoning.pdf">AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Maryland, College Park</span></p><p>本文提出AURA基准，专为评估音视频大模型（AV-LLMs）和全模态大模型（OLMs）在六类细粒度跨模态推理任务（如因果推理、音色&#x2F;音高推理、同步性分析等）上的推理能力。AURA结合自动化QA生成流程与新颖分解评测指标AuraScore（包括事实一致性和核心推理两项），揭示主流多模态模型在准确率与推理逻辑之间存在显著差距，推动更深入的多模态模型理解和评测。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="193-Freeze-and-Reveal-Exposing-Modality-Bias-in-Vision-Language-Models"><a href="#193-Freeze-and-Reveal-Exposing-Modality-Bias-in-Vision-Language-Models" class="headerlink" title="193. Freeze and Reveal: Exposing Modality Bias in Vision-Language Models"></a>193. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Freeze_and_Reveal__Exposing_Modality_Bias_in_Vision-Language_Models.pdf">Freeze and Reveal: Exposing Modality Bias in Vision-Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IIIT Hyderabad</span></p><p>本文提出了针对视觉-语言模型（VLM）中性别偏见的模态定向去偏框架，通过分别对视觉编码器和文本编码器应用Counterfactual Data Augmentation（CDA）、Task Vector方法以及新颖的数据高效去偏技术DAUDoS以量化和降低性别偏见。实验证明，CLIP模型的视觉编码器偏见更大，PaliGemma2则以文本编码器为主，通过针对性去偏可有效缩小性别差距且保持模型性能，为多模态系统公平性改进提供了方法论和评估工具。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="194-Event-Aware-Sentiment-Factors-from-LLM-Augmented-Financial-Tweets-A-Transparent-Framework-for-Interpretable-Quant-Trading"><a href="#194-Event-Aware-Sentiment-Factors-from-LLM-Augmented-Financial-Tweets-A-Transparent-Framework-for-Interpretable-Quant-Trading" class="headerlink" title="194. Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading"></a>194. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Event-Aware_Sentiment_Factors_from_LLM-Augmented_Financial_Tweets__A_Transparent_Framework_for_Inter.pdf">Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Cambridge</span></p><p>本文提出了一种利用大型语言模型（LLM）对金融相关推文进行多标签事件分类，并结合情感强度量化构建可解释的量化交易因子。实证结果表明，LLM标注的特定事件（如谣言&#x2F;炒作、散户热议等）情感因子在多个持仓周期内具有显著且稳定的负Alpha和高信息系数，验证了社交媒体语义结构对金融预测的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="195-Generative-AI-for-Strategic-Plan-Development"><a href="#195-Generative-AI-for-Strategic-Plan-Development" class="headerlink" title="195. Generative AI for Strategic Plan Development"></a>195. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Generative_AI_for_Strategic_Plan_Development.pdf">Generative AI for Strategic Plan Development</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Johns Hopkins University</span></p><p>本文提出了一个模块化认知模型，利用生成式人工智能（GAI）和大型语言模型（LLM）辅助政府机构战略规划开发。通过对BERTopic和非负矩阵分解（NMF）两种主题建模技术在战略计划愿景要素提取任务中的表现进行比较，结果表明两者均能覆盖全部愿景要素，但BERTopic的相关性更强，表现最佳。结论指出主题建模可有效辅助战略愿景要素的生成，BERTopic优于NMF，未来需扩展至更多模块并优化模型以实现实际应用。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="196-LET-US-Long-Event-Text-Understanding-of-Scenes"><a href="#196-LET-US-Long-Event-Text-Understanding-of-Scenes" class="headerlink" title="196. LET-US: Long Event-Text Understanding of Scenes"></a>196. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LET-US__Long_Event-Text_Understanding_of_Scenes.pdf">LET-US: Long Event-Text Understanding of Scenes</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出LET-US框架，针对事件相机产生的稀疏长序列数据，通过跨模态语义引导压缩和层次聚类，动态选取关键信息段，实现事件流与文本的对齐和理解。通过两阶段优化（先RGB视觉-文本预训练，再事件流-文本微调）以及自建百万级事件-文本对齐数据集，LET-US显著提升了长时事件流的推理、分类、定位和描述任务性能。实验结果表明，LET-US在多项事件流理解任务上均优于现有主流MLLM方法，能够高效处理百万级时间戳的长事件流。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="197-ALOPE-Adaptive-Layer-Optimization-for-Translation-Quality-Estimation-using-Large-Language-Models"><a href="#197-ALOPE-Adaptive-Layer-Optimization-for-Translation-Quality-Estimation-using-Large-Language-Models" class="headerlink" title="197. ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models"></a>197. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ALOPE__Adaptive_Layer_Optimization_for_Translation_Quality_Estimation_using_Large_Language_Models.pdf">ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Surrey</span></p><p>本论文提出ALOPE框架，通过在大型语言模型（LLM）的Transformer不同层插入低秩适配器（LoRA）和回归头，实现对机器翻译质量的无参考评估。该方法系统探索中间层表示并引入动态加权与多头回归策略，显著提升了低资源语言对翻译质量估算的相关性分数，优于标准微调和现有主流模型。结论指出，Transformer中间层（尤其是TL-7）具备更优的跨语言表征能力，ALOPE在GPU资源和效果上表现优秀，适合实际部署。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="198-Grounding-Natural-Language-for-Multi-agent-Decision-Making-with-Multi-agentic-LLMs"><a href="#198-Grounding-Natural-Language-for-Multi-agent-Decision-Making-with-Multi-agentic-LLMs" class="headerlink" title="198. Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs"></a>198. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Grounding_Natural_Language_for_Multi-agent_Decision-Making_with_Multi-agentic_LLMs.pdf">Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">UC Davis</span></p><p>本文提出了一个多智能体大语言模型（LLM）系统框架，将先进的提示工程、多模态信息处理、检索增强生成（RAG）、记忆结构和机制设计与多智能体决策算法结合。通过在经典博弈（如囚徒困境、鸡游戏等）上的实验，作者发现经微调的多智能体LLM能够促进个体和整体的高效、可解释、鲁棒的协作决策，并有效应对社会困境、信息不完全、目标不一致等复杂情况。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="199-Grounding-Multilingual-Multimodal-LLMs-With-Cultural-Knowledge"><a href="#199-Grounding-Multilingual-Multimodal-LLMs-With-Cultural-Knowledge" class="headerlink" title="199. Grounding Multilingual Multimodal LLMs With Cultural Knowledge"></a>199. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Grounding_Multilingual_Multimodal_LLMs_With_Cultural_Knowledge.pdf">Grounding Multilingual Multimodal LLMs With Cultural Knowledge</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>本文提出CulturalGround数据集和CulturalPangea模型，通过利用Wikidata构建覆盖39种语言、42个国家、2200万条文化视觉问答的数据集，采用多阶段数据生成、LLM润色、VLM筛选，提升多模态大语言模型的文化知识和多语言能力。实验表明，CulturalPangea在多项文化相关多语言多模态基准测试上超越现有开源模型约5%，且主流视觉-语言任务性能不降，显著缩小LLM文化偏差。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="200-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems"><a href="#200-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems" class="headerlink" title="200. A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems"></a>200. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Comprehensive_Survey_of_Self-Evolving_AI_Agents__A_New_Paradigm_Bridging_Foundation_Models_and_Lif.pdf">A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Glasgow</span></p><p>本论文系统综述了自进化AI代理的新范式，提出了统一概念框架，涵盖模型离线训练、在线适应、多智能体协同到多智能体自进化（MASE）。方法包括对LLM、提示、记忆、工具、工作流及多智能体通信优化技术的梳理，并讨论评估、安全与伦理问题。结论认为自进化AI代理能实现持续适应、增强自主性和长期学习，为动态环境下的AI系统发展提供理论与技术基础。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="201-Urbanite-A-Dataflow-Based-Framework-for-Human-AI-Interactive-Alignment-in-Urban-Visual-Analytics"><a href="#201-Urbanite-A-Dataflow-Based-Framework-for-Human-AI-Interactive-Alignment-in-Urban-Visual-Analytics" class="headerlink" title="201. Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics"></a>201. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Urbanite__A_Dataflow-Based_Framework_for_Human-AI_Interactive_Alignment_in_Urban_Visual_Analytics.pdf">Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Chicago</span></p><p>Urbanite提出了一种基于数据流的数据分析框架，通过集成大语言模型（LLM），支持用户以自然语言指定分析意图，进而自动生成、解释和迭代城市视觉分析的数据流流程。该方法显著降低了城市数据可视化分析的技术门槛，实现了人机协作、任务可追溯与多层级对齐，经专家实证和案例分析验证有效，能准确将高层次意图转化为可执行分析流程。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="202-Rethinking-Domain-Specific-LLM-Benchmark-Construction-A-Comprehensiveness-Compactness-Approach"><a href="#202-Rethinking-Domain-Specific-LLM-Benchmark-Construction-A-Comprehensiveness-Compactness-Approach" class="headerlink" title="202. Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach"></a>202. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Rethinking_Domain-Specific_LLM_Benchmark_Construction__A_Comprehensiveness-Compactness_Approach.pdf">Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong Polytechnic University</span></p><p>本文提出了COMP-COMP框架，用于领域特定大语言模型（LLM）基准测试集的自动构建，强调语义空间内的全面性（comprehensiveness）和紧凑性（compactness）原则，通过高斯核密度估计和相关性分析迭代收集与筛选知识语料和问答数据，实现高效覆盖与低冗余的基准集设计。实验证明，该方法可显著减少基准问题和语料数量（问题减少98.3%，语料减少53.6%），同时提升LLM领域任务的评测效果，并在学术领域构建了大规模基准XUBench，具备跨领域扩展性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="203-ObfusQAte-A-Proposed-Framework-to-Evaluate-LLM-Robustness-on-Obfuscated-Factual-Question-Answering"><a href="#203-ObfusQAte-A-Proposed-Framework-to-Evaluate-LLM-Robustness-on-Obfuscated-Factual-Question-Answering" class="headerlink" title="203. ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering"></a>203. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ObfusQAte__A_Proposed_Framework_to_Evaluate_LLM_Robustness_on_Obfuscated_Factual_Question_Answering.pdf">ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Indian Institute of Technology Patna</span></p><p>本文提出ObfusQAte技术和ObfusQA框架，通过多层次的语义混淆（命名实体间接、干扰项间接、上下文过载）系统性构造问题，全面评估主流大语言模型在应对被混淆事实问答任务时的鲁棒性。实验发现，现有LLM在复杂混淆下准确率显著下降，暴露出对间接表达和背景噪声适应能力不足，提出混淆型问题是测试与提升LLM真实推理能力的重要方向。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="204-HealthBranches-Synthesizing-Clinically-Grounded-Question-Answering-Datasets-via-Decision-Pathways"><a href="#204-HealthBranches-Synthesizing-Clinically-Grounded-Question-Answering-Datasets-via-Decision-Pathways" class="headerlink" title="204. HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways"></a>204. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HealthBranches__Synthesizing_Clinically-Grounded_Question_Answering_Datasets_via_Decision_Pathways.pdf">HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Calabria</span></p><p>该论文提出HealthBranches数据集，通过半自动流程从医学决策路径生成临床真实场景的医学问答数据，涵盖17类临床领域共4063个案例。方法结合结构化知识抽取、LLM辅助生成和人工审核，支持多步推理评估与RAG检索增强。结果显示结构化推理路径显著提升LLM在医学问答任务中的准确性和解释性，为医学领域可信LLM开发与评估奠定基础。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="205-Fine-Tuning-Large-Language-Models-Using-EEG-Microstate-Features-for-Mental-Workload-Assessment"><a href="#205-Fine-Tuning-Large-Language-Models-Using-EEG-Microstate-Features-for-Mental-Workload-Assessment" class="headerlink" title="205. Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment"></a>205. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fine-Tuning_Large_Language_Models_Using_EEG_Microstate_Features_for_Mental_Workload_Assessment.pdf">Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Computer Science, Technological University Dublin</span></p><p>本论文提出了利用EEG微状态特征对大型语言模型（LLM）进行微调的方法，以提升对认知负荷状态（休息与负荷）的判别能力。研究流程涵盖数据采集与预处理、微状态分割与回拟合、特征提取与提示工程、模型选择与微调，最终通过监督学习训练LLM基于EEG微状态特征区分用户的认知负荷状态。结果显示，经过EEG微状态特征微调后的LLM模型，其认知负荷判别准确率由4.5%大幅提升至97%，显著优于未微调模型，证明EEG微状态特征可有效增强LLM对大脑认知活动的感知与推断能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="206-MAQUA-Adaptive-Question-Asking-for-Multidimensional-Mental-Health-Screening-using-Item-Response-Theory"><a href="#206-MAQUA-Adaptive-Question-Asking-for-Multidimensional-Mental-Health-Screening-using-Item-Response-Theory" class="headerlink" title="206. MAQUA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory"></a>206. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MAQuA__Adaptive_Question-Asking_for_Multidimensional_Mental_Health_Screening_using_Item_Response_The.pdf">MAQUA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stony Brook University</span></p><p>本论文提出了MAQUA框架，将多输出语言建模、因子分析与多维项目反应理论（MIRT）结合，实现多维心理健康自适应问诊。该方法通过多任务建模与信息增益驱动的问题选择，显著减少所需问题数（最多减少85%），提升多维心理健康筛查效率。结论表明，MAQUA在保证准确性的同时大幅降低用户负担，适合大模型驱动的临床应用。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="207-LLM-based-Agents-for-Automated-Confounder-Discovery-and-Subgroup-Analysis-in-Causal-Inference"><a href="#207-LLM-based-Agents-for-Automated-Confounder-Discovery-and-Subgroup-Analysis-in-Causal-Inference" class="headerlink" title="207. LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference"></a>207. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LLM-based_Agents_for_Automated_Confounder_Discovery_and_Subgroup_Analysis_in_Causal_Inference.pdf">LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Sun Yat-Sen University</span></p><p>本文提出了一种基于大语言模型（LLM）的智能体框架，通过将LLM Agent与因果机器学习流程结合，实现自动化混杂因子发现与亚组分析。方法采用Mixture of Experts结构和迭代不确定性评估，利用RAG和知识库加强LLM推理能力，显著提升了治疗效应估计的鲁棒性并降低专家工作量。实验表明，该框架能有效缩小置信区间宽度，发现隐匿混杂因素，提高因果推断的可信度和可扩展性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="208-Can-Smaller-Large-Language-Models-Evaluate-Research-Quality"><a href="#208-Can-Smaller-Large-Language-Models-Evaluate-Research-Quality" class="headerlink" title="208. Can Smaller Large Language Models Evaluate Research Quality?"></a>208. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Can_Smaller_Large_Language_Models_Evaluate_Research_Quality_.pdf">Can Smaller Large Language Models Evaluate Research Quality?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Wolverhampton</span></p><p>本论文系统评估了可下载的中型开源大语言模型Gemma-3-27b-it在学术论文研究质量评分任务上的能力，与专家评分及主流云端LLM（如ChatGPT 4o&#x2F;4o-mini）进行了广泛对比。结果表明，Gemma-3-27b-it的评分与专家评分在全部学科方向均呈显著正相关，且相关强度达到ChatGPT 4o的83.8%、4o-mini的94.7%；但模型报告结构较为统一，重复评分提升有限。结论：Gemma等中型离线LLM可用于研究质量自动评估，支持高安全或低成本场景，且这种能力不是超大模型的特有涌现性质。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="209-AutoAssert-1-A-LoRA-Fine-Tuned-LLM-Model-for-Efficient-Automated-Assertion-Generation"><a href="#209-AutoAssert-1-A-LoRA-Fine-Tuned-LLM-Model-for-Efficient-Automated-Assertion-Generation" class="headerlink" title="209. AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation"></a>209. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AutoAssert_1__A_LoRA_Fine-Tuned_LLM_Model_for_Efficient_Automated_Assertion_Generation.pdf">AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Institute of Computing Technology, Chinese Academy of Sciences</span></p><p>本文提出了一种基于LoRA高效微调技术和Unsloth平台的轻量级大语言模型，用于自动从Verilog硬件描述语言代码中生成验证断言。实验表明，该方法在大幅降低微调计算资源消耗的同时，能够生成语法和语义均准确的断言，性能优于传统规则方法，适用于资源受限环境下的自动化测试与维护。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="210-Hallucination-as-a-Computational-Boundary-A-Hierarchy-of-Inevitability-and-the-Oracle-Escape"><a href="#210-Hallucination-as-a-Computational-Boundary-A-Hierarchy-of-Inevitability-and-the-Oracle-Escape" class="headerlink" title="210. Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape"></a>210. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hallucination_as_a_Computational_Boundary__A_Hierarchy_of_Inevitability_and_the_Oracle_Escape.pdf">Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Changzhou University</span></p><p>本文将大语言模型（LLM）形式化为概率图灵机，提出了幻觉不可避免的计算层级（对角化、不可计算性、信息论边界），并通过“学习者泵引理”给出理论证明。论文首次从理论上证明了检索增强生成（RAG）作为外部“oracle”可实现绝对逃逸，并提出基于神经博弈理论的持续学习作为内部适应路径，实验证明RAG-CL混合策略在准确性和健壮性上优于单独策略。结论认为：幻觉是LLM的根本限制，但通过外部检索或内部持续学习可系统性规避，提出“计算类别对齐”作为AI安全的新原则。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="211-Efficient-Edge-LLMs-Deployment-via-Hessian-Aware-Quantization-and-CPU–GPU-Collaborative"><a href="#211-Efficient-Edge-LLMs-Deployment-via-Hessian-Aware-Quantization-and-CPU–GPU-Collaborative" class="headerlink" title="211. Efficient Edge LLMs Deployment via Hessian-Aware Quantization and CPU–GPU Collaborative"></a>211. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Efficient_Edge_LLMs_Deployment_via_HessianAware_Quantization_and_CPU_GPU_Collaborative.pdf">Efficient Edge LLMs Deployment via Hessian-Aware Quantization and CPU–GPU Collaborative</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong Polytechnic University</span></p><p>本文提出了一种基于Hessian感知量化（HAQ）与CPU-GPU协同推理的高效边缘端Mixture-of-Experts（MoE）大语言模型部署方案。方法首先通过自适应激活平滑及Hessian矩阵权重量化，实现激活与权重的联合8比特量化，显著缓解离群值导致的精度损失，并结合专家激活路径统计，设计专家级协同卸载与缓存机制，有效分配GPU与CPU资源，降低显存压力与推理延迟。实验表明，该方法在OPT系列和Mixtral-8×7B模型上，量化模型推理精度接近全精度，显存占用降低约60%，推理延迟和系统稳定性大幅提升，为MoE LLM在实际边缘环境下高效稳定部署提供了实用技术路径。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="212-DocR1-Evidence-Page-Guided-GRPO-for-Multi-Page-Document-Understanding"><a href="#212-DocR1-Evidence-Page-Guided-GRPO-for-Multi-Page-Document-Understanding" class="headerlink" title="212. DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding"></a>212. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DocR1__Evidence_Page-Guided_GRPO_for_Multi-Page_Document_Understanding.pdf">DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>DocR1是一种专为多页文档理解设计的多模态大语言模型（MLLM），通过提出Evidence Page-Guided GRPO（EviGRPO）强化学习框架，引入证据感知奖励机制，实现从粗到细的人类式推理：先检索相关页面，再生成答案。结合两阶段标注流程和课程学习策略，DocR1在多个多页文档理解基准上取得了最先进性能，同时保持了单页任务上的强表现。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="213-MCITlib-Multimodal-Continual-Instruction-Tuning-Library-and-Benchmark"><a href="#213-MCITlib-Multimodal-Continual-Instruction-Tuning-Library-and-Benchmark" class="headerlink" title="213. MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark"></a>213. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MCITlib__Multimodal_Continual_Instruction_Tuning_Library_and_Benchmark.pdf">MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Advanced Interdisciplinary Sciences, UCAS</span></p><p>MCITlib提出了一个面向多模态大语言模型的持续指令微调开源库，涵盖8种代表性算法，并在严格筛选的两个基准数据集上系统评测各种方法，有效解决信息泄露和公平比较难题。结论显示，不同方法在缓解遗忘和提升多任务性能上表现有差异，DISCO方法表现最佳，但存在参数膨胀问题，库将持续扩展以促进多模态持续学习领域研究。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="214-“Pull-or-Not-to-Pull-”-Investigating-Moral-Biases-in-Leading-Large-Language-Models-Across-Ethical-Dilemmas"><a href="#214-“Pull-or-Not-to-Pull-”-Investigating-Moral-Biases-in-Leading-Large-Language-Models-Across-Ethical-Dilemmas" class="headerlink" title="214. “Pull or Not to Pull?”: Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas"></a>214. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/_Pull_or_Not_to_Pull_''__Investigating_Moral_Biases_in_Leading_Large_Language_Models_Across_Ethical_.pdf">“Pull or Not to Pull?”: Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of New South Wales</span></p><p>本文系统评估了14种主流大语言模型（LLM）在27类“电车难题”下的道德决策与解释，涵盖十种伦理哲学视角，采用二阶段分步提示和多指标（决策果断性、解释一致性、与人类道德一致性、情境偏差敏感性）分析。结果显示，推理增强型模型更具决策果断性和结构化解释，但与人类共识的对齐度并不总是更高，且在某些伦理框架下（如亲情、自利、法律）偏差明显。研究发现“公平”“利他”“美德伦理”提示在决策果断、解释一致和人类一致性间达成较好平衡，建议将道德推理纳入LLM对齐评估主轴。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="215-Selection-and-Exploitation-of-High-Quality-Knowledge-from-Large-Language-Models-for-Recommendation"><a href="#215-Selection-and-Exploitation-of-High-Quality-Knowledge-from-Large-Language-Models-for-Recommendation" class="headerlink" title="215. Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation"></a>215. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Selection_and_Exploitation_of_High-Quality_Knowledge_from_Large_Language_Models_for_Recommendation.pdf">Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Technology</span></p><p>论文提出了KSER框架，通过知识过滤模块（ESFNet）和嵌入空间对齐模块，有效地从大型语言模型（LLMs）中筛选和利用高质量知识用于推荐系统。实验结果表明，KSER不仅能提升推荐模型性能，还能通过extractor-only训练策略降低计算成本，验证了知识过滤与对齐模块的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="216-What-One-Cannot-Two-Can-Two-Layer-Transformers-Provably-Represent-Induction-Heads-on-Any-Order-Markov-Chains"><a href="#216-What-One-Cannot-Two-Can-Two-Layer-Transformers-Provably-Represent-Induction-Heads-on-Any-Order-Markov-Chains" class="headerlink" title="216. What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains"></a>216. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/What_One_Cannot,_Two_Can__Two-Layer_Transformers_Provably_Represent_Induction_Heads_on_Any-Order_Mar.pdf">What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Massachusetts Institute of Technology</span></p><p>该论文提出并严格证明了：仅用两层、每层一个注意力头的Transformer即可表征任意阶的Markov链的条件k-gram模型（即归纳头任务），突破了此前需三层结构的理论下界。作者还对一阶Markov链上该结构的梯度下降学习动态进行了分析，说明浅层Transformer模型在结构化序列建模任务上也具备强大的in-context learning能力。结论表明Transformer的层数与可表达的Markov阶数关系达到了最紧致的已知表征，深化了对其in-context learning本质的理解。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="217-Adapting-LLMs-to-Time-Series-Forecasting-via-Temporal-Heterogeneity-Modeling-and-Semantic-Alignment"><a href="#217-Adapting-LLMs-to-Time-Series-Forecasting-via-Temporal-Heterogeneity-Modeling-and-Semantic-Alignment" class="headerlink" title="217. Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment"></a>217. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Adapting_LLMs_to_Time_Series_Forecasting_via_Temporal_Heterogeneity_Modeling_and_Semantic_Alignment.pdf">Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tianjin University</span></p><p>该论文提出了TALON框架，通过异质时序编码器和语义对齐模块，将大语言模型（LLM）适配于时间序列预测。方法包括对多变量时间序列进行结构分段、专家模型动态路由，以及用自然语言统计特征引导LLM嵌入，并通过对比学习实现时序与语言表示的细粒度对齐。实验显示TALON在七个真实世界数据集上均显著优于现有LLM及深度学习方法，MSE提升最高达11%，具备高效率和强泛化能力，尤其在零样本和跨域迁移环境下表现突出。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="218-Multi-Dimensional-Summarization-Agents-with-Context-Aware-Reasoning-over-Enterprise-Tables"><a href="#218-Multi-Dimensional-Summarization-Agents-with-Context-Aware-Reasoning-over-Enterprise-Tables" class="headerlink" title="218. Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables"></a>218. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-Dimensional_Summarization_Agents_with_Context-Aware_Reasoning_over_Enterprise_Tables.pdf">Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amazon</span></p><p>该论文提出了一种基于多智能体的框架，结合大语言模型（LLM），实现对企业表格多维结构化数据的上下文感知自动摘要。方法流程包括切片、度量变异检测、上下文增强和LLM驱动生成，各智能体分工协作，提升了生成摘要的事实一致性、覆盖率和业务相关性。结果表明，该方法在Kaggle企业数据集上显著优于传统平铺式LLM和模板化NLG方法，在数据对齐、关键信息展现和决策洞察方面表现更佳。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="219-Dynamic-Benchmark-Construction-for-Evaluating-Large-Language-Models-on-Real-World-Codes"><a href="#219-Dynamic-Benchmark-Construction-for-Evaluating-Large-Language-Models-on-Real-World-Codes" class="headerlink" title="219. Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes"></a>219. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Dynamic_Benchmark_Construction_for_Evaluating_Large_Language_Models_on_Real-World_Codes.pdf">Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>该论文提出了CODE2BENCH，一个用于动态构建真实世界代码基准的自动化管道，结合了自动代码抓取、Scope Graph依赖分析和基于性质的自动化测试，有效解决了数据污染和测试覆盖率低的问题。实验表明现有主流大语言模型在自包含复杂逻辑任务和跨语言泛化上表现较弱，但在常用库交互任务表现较好，CODE2BENCH能更真实揭示模型在复杂代码生成中的不足。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="220-Improved-Personalized-Headline-Generation-via-Denoising-Fake-Interests-from-Implicit-Feedback"><a href="#220-Improved-Personalized-Headline-Generation-via-Denoising-Fake-Interests-from-Implicit-Feedback" class="headerlink" title="220. Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback"></a>220. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Improved_Personalized_Headline_Generation_via_Denoising_Fake_Interests_from_Implicit_Feedback.pdf">Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Henan Institute of Advanced Technology, Zhengzhou University</span></p><p>本文提出了一种新的个性化新闻标题生成框架PHG-DIF，通过双阶段过滤（基于新闻和时间）去除用户历史点击流中的噪声点击，并融合即时、演化和稳定兴趣建模模块，动态刻画用户多维兴趣，结合Breaking-News感知生成器提升事实准确性与个性化的平衡。同时，作者构建了包含用户停留时长标注的大规模基准数据集DT-PENS。实验结果表明，该方法显著减少了点击噪声对个性化生成的负面影响，提升了标题质量，在DT-PENS上取得了SOTA表现。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="221-A-Stable-and-Principled-Loss-Function-for-Direct-Language-Model-Alignment"><a href="#221-A-Stable-and-Principled-Loss-Function-for-Direct-Language-Model-Alignment" class="headerlink" title="221. A Stable and Principled Loss Function for Direct Language Model Alignment"></a>221. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Stable_and_Principled_Loss_Function_for_Direct_Language_Model_Alignment.pdf">A Stable and Principled Loss Function for Direct Language Model Alignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出了一种新的稳定偏好优化（SPO）损失函数，用于直接对齐大型语言模型（LLM）与人类偏好。该方法通过理论分析和梯度对比，优化模型的logits差值至有限目标，避免了DPO方法下的不稳定和“奖励黑客”问题。实验表明SPO在Qwen2.5-7B和Llama-3-8B模型上均优于DPO，显著提升模型对齐质量，具有更好的稳定性和泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="222-DySK-Attn-A-Framework-for-Efficient-Real-Time-Knowledge-Updating-in-Large-Language-Models-via-Dynamic-Sparse-Knowledge-Attention"><a href="#222-DySK-Attn-A-Framework-for-Efficient-Real-Time-Knowledge-Updating-in-Large-Language-Models-via-Dynamic-Sparse-Knowledge-Attention" class="headerlink" title="222. DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention"></a>222. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DySK-Attn__A_Framework_for_Efficient,_Real-Time_Knowledge_Updating_in_Large_Language_Models_via_Dyna.pdf">DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Department of Computer Science, San Francisco State University</span></p><p>该论文提出DySK-Attn框架，通过动态知识图谱与稀疏知识注意力机制，实现大语言模型实时高效地融合外部最新知识。方法包括两阶段检索（粗检索+稀疏注意力选取top-k知识点）、知识向量融合进LLM生成过程，并支持知识图谱毫秒级实时更新。实验表明，DySK-Attn在时效性问答任务上显著优于RAG和模型编辑等主流方法，兼具计算效率和事实准确性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="223-Schema-Lineage-Extraction-at-Scale-Multilingual-Pipelines-Composite-Evaluation-and-Language-Model-Benchmarks"><a href="#223-Schema-Lineage-Extraction-at-Scale-Multilingual-Pipelines-Composite-Evaluation-and-Language-Model-Benchmarks" class="headerlink" title="223. Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks"></a>223. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Schema_Lineage_Extraction_at_Scale__Multilingual_Pipelines,_Composite_Evaluation,_and_Language-Model.pdf">Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Microsoft</span></p><p>该论文提出了一个新框架，实现对多语言企业数据管道脚本中细粒度schema lineage的自动抽取，涵盖源schema、源表、转换逻辑和聚合操作，并提出了SLiCE评测指标对结构和语义正确性进行复合评价。实验证明，随着模型规模和提示策略的提升（如链式思维），开源32B模型的性能可媲美GPT-4o等专有大模型，为企业级schema感知智能体的经济部署提供可行路径。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="224-LL3M-Large-Language-3D-Modelers"><a href="#224-LL3M-Large-Language-3D-Modelers" class="headerlink" title="224. LL3M: Large Language 3D Modelers"></a>224. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LL3M__Large_Language_3D_Modelers.pdf">LL3M: Large Language 3D Modelers</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Chicago</span></p><p>该论文提出LL3M，一种多智能体系统，利用多种预训练大语言模型（LLM）协作生成可编辑的3D资产，通过自动编写解释性强的Blender Python代码实现3D建模和编辑。LL3M创新性地将形状生成任务转化为代码编写流程，包含规划、检索、编写、批评、验证和用户反馈六类智能体，并引入BlenderRAG数据库提升高级建模能力，实现高质量、可迭代、易于人机协作的3D资产生成和细化。结论表明，LL3M无需3D数据集训练即可实现高保真、可解释的3D资产生成，并支持用户多轮自然语言编辑，显著优于现有方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="225-Multi-head-Transformers-Provably-Learn-Symbolic-Multi-step-Reasoning-via-Gradient-Descent"><a href="#225-Multi-head-Transformers-Provably-Learn-Symbolic-Multi-step-Reasoning-via-Gradient-Descent" class="headerlink" title="225. Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent"></a>225. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-head_Transformers_Provably_Learn_Symbolic_Multi-step_Reasoning_via_Gradient_Descent.pdf">Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>本文提出并理论证明了单层多头Transformer可通过梯度下降机制学习并泛化符号化多步链式推理任务，尤其在树结构上的路径查找，包括单步和复杂两阶段（先反向再正向）推理。构建性和优化分析揭示多头注意力机制可自主分工并协调完成递归路径遍历和阶段控制，且该能力能泛化到未见结构，表明Transformer学到算法规则而非仅记忆训练样本。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="226-Human-Alignment-and-Calibration-of-Inference-Time-Uncertainty-in-Large-Language-Models"><a href="#226-Human-Alignment-and-Calibration-of-Inference-Time-Uncertainty-in-Large-Language-Models" class="headerlink" title="226. Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models"></a>226. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Human-Alignment_and_Calibration_of_Inference-Time_Uncertainty_in_Large_Language_Models.pdf">Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Vanderbilt University</span></p><p>本文系统评估了大语言模型（LLM）推理时的不确定性度量方法，重点分析其与人类群体不确定性和模型传统校准之间的一致性。通过多种熵类不确定性指标（如choice entropy、top-k entropy、top-p entropy等）在大规模人类调查数据集和标准MMLU基准上进行对比，发现这些度量与人类不确定性高度相关，并且在模型校准方面具有统计显著性。结论指出，熵类推理时不确定性度量能较好反映人类不确定性并具备一定模型校准能力，未来可拓展至开放式问题和增强人机交互信任。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="227-Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning"><a href="#227-Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning" class="headerlink" title="227. Capabilities of GPT-5 on Multimodal Medical Reasoning"></a>227. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Capabilities_of_GPT-5_on_Multimodal_Medical_Reasoning.pdf">Capabilities of GPT-5 on Multimodal Medical Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Emory University</span></p><p>本论文系统评估了GPT-5在医学领域多模态推理任务上的能力，采用统一的零样本链式思维(CoT)协议，涵盖文本问答和视觉问答，对比GPT-5及多个变体与GPT-4o以及人类专家在MedQA、MedXpertQA（文本和多模态）、MMLU医学子集、USMLE自测和VQA-RAD等基准上的表现。实验结果表明：GPT-5在所有问答基准上均实现了最先进的准确率，尤其在多模态推理任务上显著超越GPT-4o和人类专家，提升幅度达20%以上，展示了其作为临床决策支持核心组件的潜力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="228-PYVERITAS-On-Verifying-Python-via-LLM-Based-Transpilation-and-Bounded-Model-Checking-for-C"><a href="#228-PYVERITAS-On-Verifying-Python-via-LLM-Based-Transpilation-and-Bounded-Model-Checking-for-C" class="headerlink" title="228. PYVERITAS: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C"></a>228. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PyVeritas__On_Verifying_Python_via_LLM-Based_Transpilation_and_Bounded_Model_Checking_for_C.pdf">PYVERITAS: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Oxford</span></p><p>该论文提出了PYVERITAS框架，利用大语言模型（LLM）将Python代码高层次转译为C代码，并结合CBMC有界模型检测与MaxSAT故障定位工具实现对Python程序的形式化验证与错误定位。实验表明，部分专用代码大模型（如QWEN2.5-CODER）能在80-90%的案例中生成语义等价且可验证的C代码，实现对小型但非平凡Python程序的有效验证和可解释的错误诊断。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="229-Can-LLMs-Detect-Their-Confabulations-Estimating-Reliability-in-Uncertainty-Aware-Language-Models"><a href="#229-Can-LLMs-Detect-Their-Confabulations-Estimating-Reliability-in-Uncertainty-Aware-Language-Models" class="headerlink" title="229. Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models"></a>229. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Can_LLMs_Detect_Their_Confabulations__Estimating_Reliability_in_Uncertainty-Aware_Language_Models.pdf">Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">KTH Royal Institute of Technology</span></p><p>本文提出了一种基于不确定性引导的探测方法，通过计算LLM输出的token级别的aleatoric和epistemic不确定性，聚合高不确定性的隐藏状态，提升对响应可靠性的预测能力。实验证明，该方法在面对正确和误导性上下文时，能更有效检测LLM输出的不可靠内容，优于直接不确定性信号，揭示了上下文对模型过度自信错误的影响。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="230-Optimal-Transport-Regularization-for-Speech-Text-Alignment-in-Spoken-Language-Models"><a href="#230-Optimal-Transport-Regularization-for-Speech-Text-Alignment-in-Spoken-Language-Models" class="headerlink" title="230. Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models"></a>230. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Optimal_Transport_Regularization_for_Speech_Text_Alignment_in_Spoken_Language_Models.pdf">Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mashang Consumer Finance Co., Ltd.</span></p><p>本文提出了Optimal Transport Regularization (OTReg)方法，将语音-文本对齐建模为最优传输问题，通过无参数正则化损失提升Spoken Language Models（SLMs）的训练效果和泛化能力。OTReg无需额外标签或参数，直接对齐语音与文本嵌入，实验显示该方法有效缓解模态间差距，提升多语言自动语音识别（ASR）任务的跨域泛化性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="231-TBAC-UniImage-Unified-Understanding-and-Generation-by-Ladder-Side-Diffusion-Tuning"><a href="#231-TBAC-UniImage-Unified-Understanding-and-Generation-by-Ladder-Side-Diffusion-Tuning" class="headerlink" title="231. TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning"></a>231. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TBAC-UniImage__Unified_Understanding_and_Generation_by_Ladder-Side_Diffusion_Tuning.pdf">TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Basic Algorithm Center, PCG, Tencent</span></p><p>该论文提出了TBAC-UniImage，一种通过Ladder-Side Diffusion Tuning机制深度融合多模态大语言模型（MLLM）与Diffusion Transformer（DiT）的统一多模态理解与生成模型。方法采用分层条件机制，将MLLM不同层的隐藏状态作为条件输入到DiT各层，提升理解与生成的协同能力。实验证明，该模型在GenEval、DPG-Bench和TIIF-Bench等多个基准上具有领先的统一理解与生成性能，尤其在复杂指令跟随和多模态任务上表现优异。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="232-Dual-Information-Speech-Language-Models-for-Emotional-Conversations"><a href="#232-Dual-Information-Speech-Language-Models-for-Emotional-Conversations" class="headerlink" title="232. Dual Information Speech Language Models for Emotional Conversations"></a>232. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Dual_Information_Speech_Language_Models_for_Emotional_Conversations.pdf">Dual Information Speech Language Models for Emotional Conversations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Mashang Consumer Finance Co., Ltd.</span></p><p>该论文提出了一种高效扩展现有大型语言模型（LLM）为语音语言模型（SLM）的方法，通过设计异构双适配器结构并采用弱监督训练策略，实现了对语音中的副语言和语言信息的结构化解耦表示。实验结果表明，该方法仅需训练适配器即可在情感对话场景中高效融合副语言与语言信息，性能优越且理解上下文能力强。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="233-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches"><a href="#233-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches" class="headerlink" title="233. HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches"></a>233. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HierSearch__A_Hierarchical_Enterprise_Deep_Search_Framework_Integrating_Local_and_Web_Searches.pdf">HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Gaoling School of Artificial Intelligence, Renmin University of China</span></p><p>该论文提出了HierSearch，一种层次化多智能体深度搜索框架，集成了本地与Web知识检索，并采用层次化强化学习（HRL）训练本地与Web深度搜索代理，再由高层规划代理整合证据并给出最终答案。通过设计知识精炼模块，过滤低层代理返回的无关与幻觉证据，方法在六大领域基准上优于平坦RL和多种深度搜索及多源RAG基线，提升了多源检索与推理能力，泛化性强。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="234-Robust-Anomaly-Detection-in-O-RAN-Leveraging-LLMs-against-Data-Manipulation-Attacks"><a href="#234-Robust-Anomaly-Detection-in-O-RAN-Leveraging-LLMs-against-Data-Manipulation-Attacks" class="headerlink" title="234. Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks"></a>234. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Robust_Anomaly_Detection_in_O-RAN__Leveraging_LLMs_against_Data_Manipulation_Attacks.pdf">Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Monash University</span></p><p>本文提出在5G O-RAN架构下，利用大型语言模型（LLM）进行异常检测，对抗通过Unicode字符（hypoglyphs）实施的数据操纵攻击。方法包括将LLM集成至O-RAN的Near-RT RIC xApp，采用窗口化消息处理流程，对传统ML模型易崩溃的情况进行对比，结果显示LLM不仅能稳定处理被篡改的消息且检测延迟低于0.07秒，适用于实时场景，但检测准确率仍需优化。结论：LLM对数据操纵攻击展现出更高鲁棒性，为O-RAN安全部署提供了可行方案。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="235-From-Natural-Language-to-Solver-Ready-Power-System-Optimization-An-LLM-Assisted-Validation-in-the-Loop-Framework"><a href="#235-From-Natural-Language-to-Solver-Ready-Power-System-Optimization-An-LLM-Assisted-Validation-in-the-Loop-Framework" class="headerlink" title="235. From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework"></a>235. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Natural_Language_to_Solver-Ready_Power_System_Optimization__An_LLM-Assisted,_Validation-in-the-.pdf">From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Southern California</span></p><p>该论文提出了一种以大语言模型（LLM）为核心的智能代理系统，能将电力系统优化的自然语言描述自动转换为紧凑、求解器可用的数学模型，并在回路中进行验证和修复。方法流程包括领域感知的参数提取、结构化模板生成、迭代校验与修复，以及GNN辅助的分支策略提升MILP求解效率。实验显示，该方法在单元组合问题上实现了100%有效建模，提升了运行时效率并保证了解的可行性与最优性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="236-MuaLLM-A-Multimodal-Large-Language-Model-Agent-for-Circuit-Design-Assistance-with-Hybrid-Contextual-Retrieval-Augmented-Generation"><a href="#236-MuaLLM-A-Multimodal-Large-Language-Model-Agent-for-Circuit-Design-Assistance-with-Hybrid-Contextual-Retrieval-Augmented-Generation" class="headerlink" title="236. MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation"></a>236. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MuaLLM__A_Multimodal_Large_Language_Model_Agent_for_Circuit_Design_Assistance_with_Hybrid_Contextual.pdf">MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Utah</span></p><p>本论文提出了MuaLLM，一种开源多模态大语言模型（LLM）代理，集成了基于ReAct（Reason + Act）迭代推理流程和混合检索增强生成（RAG）框架，专为电路设计文献检索与多步推理任务设计。MuaLLM通过结合稀疏&#x2F;密集检索、向量数据库、智能工具链（如自动文献抓取、数据库动态更新、原理图到网表转换）以及多模态能力（文本+图像），实现了高效、可扩展、上下文感知的电路设计问答和分析。实验结果显示，MuaLLM在自建RAG-250和Reas-100数据集上分别达到90.1%召回率和86.8%推理准确率，同时在大语境下推理速度提升1.6倍、成本降低10倍，具备高效、可扩展的电路设计辅助能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="237-BlindGuard-Safeguarding-LLM-based-Multi-Agent-Systems-under-Unknown-Attacks"><a href="#237-BlindGuard-Safeguarding-LLM-based-Multi-Agent-Systems-under-Unknown-Attacks" class="headerlink" title="237. BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks"></a>237. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BlindGuard__Safeguarding_LLM-based_Multi-Agent_Systems_under_Unknown_Attacks.pdf">BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Jilin University</span></p><p>本文提出BlindGuard，一种针对LLM驱动的多智能体系统（MAS）在未知攻击下的无监督防御方法。通过设计层次化代理编码器融合个体、邻域和全局信息，并引入语义腐化引导的攻击检测器，BlindGuard无需攻击标签即可检测和隔离恶意代理，有效应对提示注入、内存污染和工具攻击等多种场景。实验表明，BlindGuard在多种MAS拓扑和攻击类型下均具有强泛化性和优越防御性能，推动了LLM多智能体系统的安全防护发展。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="238-TeamMedAgents-Enhancing-Medical-Decision-Making-of-LLMs-Through-Structured-Teamwork"><a href="#238-TeamMedAgents-Enhancing-Medical-Decision-Making-of-LLMs-Through-Structured-Teamwork" class="headerlink" title="238. TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork"></a>238. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TeamMedAgents__Enhancing_Medical_Decision-Making_of_LLMs_Through_Structured_Teamwork.pdf">TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois, Chicago</span></p><p>该论文提出了TeamMedAgents，一个将人类组织心理学中的团队协作理论系统化引入大语言模型（LLM）驱动的多智能体医学决策系统。方法通过模块化实现团队领导、互相监控、团队导向、共享心理模型、闭环沟通和互信六大团队机制，并在八个医学基准上通过系统消融实验验证其提升效果。结论指出，针对任务类型选择不同协作机制比全面集成更优，实验证明该方法在七个医学任务上超越现有多智能体框架，最大提高达17.6%。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="239-Assessing-LLM-Text-Detection-in-Educational-Contexts-Does-Human-Contribution-Affect-Detection"><a href="#239-Assessing-LLM-Text-Detection-in-Educational-Contexts-Does-Human-Contribution-Affect-Detection" class="headerlink" title="239. Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?"></a>239. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Assessing_LLM_Text_Detection_in_Educational_Contexts__Does_Human_Contribution_Affect_Detection_.pdf">Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bielefeld University</span></p><p>本文提出了GEDE数据集，涵盖900余篇学生原创与12500余篇不同贡献度下的大模型生成作文，系统评测DetectGPT、Fast-DetectGPT、Ghostbuster、RoBERTa等检测器在教育场景对不同人类-LLM协作文本的识别能力。研究发现，现有检测器在区分完全人类或完全LLM生成文本时表现良好，但对中间贡献层次表现不佳，尤其容易出现高假阳性，影响教育公平性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="240-Investigating-the-Design-Space-of-Visual-Grounding-in-Multimodal-Large-Language-Model"><a href="#240-Investigating-the-Design-Space-of-Visual-Grounding-in-Multimodal-Large-Language-Model" class="headerlink" title="240. Investigating the Design Space of Visual Grounding in Multimodal Large Language Model"></a>240. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Investigating_the_Design_Space_of_Visual_Grounding_in_Multimodal_Large_Language_Model.pdf">Investigating the Design Space of Visual Grounding in Multimodal Large Language Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Illinois Chicago</span></p><p>本文系统性研究了多模态大语言模型（MLLM）在视觉指代（Visual Grounding, VG）任务上的设计空间，围绕视觉定位范式与数据设计进行全面的消融实验，提出最优方案为归一化整数格式结合左上右下坐标与one-hot监督。结论显示，采纳这些设计能使MLLM在RefCOCO&#x2F;+&#x2F;g等基准上较LLaVA-1.5显著提升（+5.6%&#x2F;+6.9%&#x2F;+7.0%），为未来MLLM在视觉指代任务的发展提供了明确指导。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="241-AdaptFlow-Adaptive-Workflow-Optimization-via-Meta-Learning"><a href="#241-AdaptFlow-Adaptive-Workflow-Optimization-via-Meta-Learning" class="headerlink" title="241. AdaptFlow: Adaptive Workflow Optimization via Meta-Learning"></a>241. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AdaptFlow__Adaptive_Workflow_Optimization_via_Meta-Learning.pdf">AdaptFlow: Adaptive Workflow Optimization via Meta-Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本论文提出了AdaptFlow，一种基于模型无关元学习（MAML）的自然语言驱动元学习框架，用于自适应优化LLM的agentic工作流。方法包括通过K-means任务聚类、双层优化（内循环用LLM文本反馈细化工作流、外循环聚合并反思）、以及测试时基于语义描述的快速适应。实验表明，AdaptFlow在问答、代码生成和数学推理等八个基准上显著优于手工和自动工作流优化基线，实现了跨任务和模型的强泛化能力。结论：AdaptFlow可自动构建通用且可自适应的LLM工作流，提升复杂任务表现。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="242-FEAT-A-Multi-Agent-Forensic-AI-System-with-Domain-Adapted-Large-Language-Model-for-Automated-Cause-of-Death-Analysis"><a href="#242-FEAT-A-Multi-Agent-Forensic-AI-System-with-Domain-Adapted-Large-Language-Model-for-Automated-Cause-of-Death-Analysis" class="headerlink" title="242. FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis"></a>242. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FEAT__A_Multi-Agent_Forensic_AI_System_with_Domain-Adapted_Large_Language_Model_for_Automated_Cause-.pdf">FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>本文提出FEAT系统，一种基于多智能体和领域自适应大语言模型（Forensic-LLM）的法医自动死因分析AI框架。系统采用模块化设计，包括任务分解的Planner、证据分析的Local Solvers、反思与记忆模块以及结论整合的Global Solver，并集成外部工具推理、层次化RAG、领域微调LLM及人类反馈。实验证明FEAT在中国六省市多机构7,748例法医案件中，显著优于同类AI系统，提升长文分析和短结论的准确性，并获得高级法医专家的高一致性认可。结论：FEAT实现了法医死因分析领域的专家级决策支持系统，可大幅提升司法流程自动化与标准化水平。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="243-Not-Yet-AlphaFold-for-the-Mind-Evaluating-Centaur-as-a-Synthetic-Participant"><a href="#243-Not-Yet-AlphaFold-for-the-Mind-Evaluating-Centaur-as-a-Synthetic-Participant" class="headerlink" title="243. Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant"></a>243. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Not_Yet_AlphaFold_for_the_Mind__Evaluating_Centaur_as_a_Synthetic_Participant.pdf">Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Osnabrück University</span></p><p>本文评估了经过认知实验数据微调的大语言模型Centaur在认知行为模拟中的能力，重点比较其预测性能和生成性能。实验表明，尽管Centaur在已知任务中预测准确性高，但其自主生成行为与人类行为存在系统性偏差，尚不具备作为可靠“合成参与者”或认知模型的能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="244-CATP-Contextually-Adaptive-Token-Pruning-for-Efficient-and-Enhanced-Multimodal-In-Context-Learning"><a href="#244-CATP-Contextually-Adaptive-Token-Pruning-for-Efficient-and-Enhanced-Multimodal-In-Context-Learning" class="headerlink" title="244. CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning"></a>244. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CATP__Contextually_Adaptive_Token_Pruning_for_Efficient_and_Enhanced_Multimodal_In-Context_Learning.pdf">CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Brown University</span></p><p>CATP是一种针对多模态大模型（LVLMs）多模态in-context learning场景设计的无训练图像token剪枝方法，通过两阶段剪枝策略，分别在投影器和解码器早期层根据语义对齐和特征多样性，以及跨层注意力动态和语义相关性筛选最关键的图像token。实验证明CATP在四种主流LVLM和八个基准上提升效率（推理延迟平均降低10.78%）同时略微提升性能（平均提升0.6%），优于现有所有剪枝方法，有效缓解多模态冗余并增强模型泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="245-Pareto-Multi-Objective-Alignment-for-Language-Models"><a href="#245-Pareto-Multi-Objective-Alignment-for-Language-Models" class="headerlink" title="245. Pareto Multi-Objective Alignment for Language Models"></a>245. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pareto_Multi-Objective_Alignment_for_Language_Models.pdf">Pareto Multi-Objective Alignment for Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Ruhr University Bochum</span></p><p>该论文提出了PAreto Multi-Objective Alignment (PAMA)算法，专为大语言模型（LLMs）在多目标（可能相互冲突）对齐场景下高效优化而设计。PAMA将多目标强化学习人类反馈（RLHF）问题转化为具有闭式解的凸优化问题，将复杂度从O(n²d)降至O(n)，并理论证明收敛到Pareto平衡点。实验表明，PAMA在GPT-2、LLaMA-2等模型上显著优于现有基线方法，实现了高效、稳定的多目标对齐，有助于提升LLM在多样化实际需求下的灵活性和适应性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="246-Interpreting-Fedspeak-with-Confidence-A-LLM-Based-Uncertainty-Aware-Framework-Guided-by-Monetary-Policy-Transmission-Paths"><a href="#246-Interpreting-Fedspeak-with-Confidence-A-LLM-Based-Uncertainty-Aware-Framework-Guided-by-Monetary-Policy-Transmission-Paths" class="headerlink" title="246. Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths"></a>246. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Interpreting_Fedspeak_with_Confidence__A_LLM-Based_Uncertainty-Aware_Framework_Guided_by_Monetary_Po.pdf">Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong University of Science and Technology (Guangzhou)</span></p><p>本文提出了一种结合货币政策传导机制的领域推理与动态不确定性解码模块的LLM框架，用于解析美联储“Fedspeak”并分类其货币政策立场。方法通过金融实体关系抽取、政策传导路径推理和感知不确定性量化提升模型可解释性与预测可靠性。实验结果表明，该方法在FOMC政策立场分析任务上达到当前最优表现，并能有效识别高不确定性预测，增强实际金融决策中的模型可靠性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="247-The-Escalator-Problem-Identifying-Implicit-Motion-Blindness-in-AI-for-Accessibility"><a href="#247-The-Escalator-Problem-Identifying-Implicit-Motion-Blindness-in-AI-for-Accessibility" class="headerlink" title="247. The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility"></a>247. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/The_Escalator_Problem__Identifying_Implicit_Motion_Blindness_in_AI_for_Accessibility.pdf">The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Beihang University</span></p><p>该论文提出了“电梯问题”，揭示了多模态大语言模型（MLLMs）在辅助视觉障碍者时无法感知连续、低信号运动（如判断自动扶梯方向）的关键缺陷，即“隐性运动盲点”。作者分析了这一问题对用户信任和安全的影响，并呼吁从静态语义识别转向更强的物理感知，建议开发以用户安全和可靠性为核心的新评测方法。结论认为，现有MLLMs在动态环境中的辅助能力存在根本性不足，亟需范式转变。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="248-WeChat-YATT-A-Simple-Scalable-and-Balanced-RLHF-Trainer"><a href="#248-WeChat-YATT-A-Simple-Scalable-and-Balanced-RLHF-Trainer" class="headerlink" title="248. WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer"></a>248. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/WeChat-YATT__A_Simple,_Scalable_and_Balanced_RLHF_Trainer.pdf">WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tencent</span></p><p>该论文提出WeChat-YATT，一种简洁且可扩展的RLHF训练框架，通过并行控制器编程模型和动态资源分配策略，实现高效的人类反馈强化学习流程。实验结果表明，WeChat-YATT显著提升了硬件利用率和训练吞吐量，已成功大规模应用于微信产品模型训练。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="249-X-evolve-Solution-space-evolution-powered-by-large-language-models"><a href="#249-X-evolve-Solution-space-evolution-powered-by-large-language-models" class="headerlink" title="249. X-evolve: Solution space evolution powered by large language models"></a>249. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/_(X_)-evolve__Solution_space_evolution_powered_by_large_language_models.pdf">X-evolve: Solution space evolution powered by large language models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>该论文提出了一种创新的LLM+EA混合优化方法X-evolve，核心思想是利用大语言模型生成可调参数的程序，通过标注tunable参数定义解空间，并结合基于得分的搜索算法（X-search）高效探索该空间。方法显著降低了LLM调用成本（减少两数量级），在cap set问题、Shannon容量问题和NP难在线装箱等多个难题中显著提升了搜索效率和结果质量，部分结果刷新了世界记录。结论是X-evolve能够更高效地解决高维优化问题，极大拓展了LLM在科学发现中的应用边界。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="250-MIMIC-Multimodal-Inversion-for-Model-Interpretation-and-Conceptualization"><a href="#250-MIMIC-Multimodal-Inversion-for-Model-Interpretation-and-Conceptualization" class="headerlink" title="250. MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization"></a>250. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MIMIC__Multimodal_Inversion_for_Model_Interpretation_and_Conceptualization.pdf">MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Twente</span></p><p>该论文提出了一种名为MIMIC的多模态反演框架，能通过联合视觉-语言模型（VLM）反演与特征对齐目标，在不修改模型权重的前提下，将VLM内部编码的语义概念可视化为图像。方法结合了自适应交叉熵损失、基于ViT激活的特征损失、空间及先验正则化器，实现了高语义一致性和结构连贯的概念图像生成，提升了VLM模型的可解释性。实验结果表明，MIMIC能够从不同长度文本输出中反演VLM的视觉概念，生成高保真、语义对齐的图像，展示了对VLM内部表征空间的有效解释能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="251-EvoCoT-Overcoming-the-Exploration-Bottleneck-in-Reinforcement-Learning"><a href="#251-EvoCoT-Overcoming-the-Exploration-Bottleneck-in-Reinforcement-Learning" class="headerlink" title="251. EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning"></a>251. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EvoCoT__Overcoming_the_Exploration_Bottleneck_in_Reinforcement_Learning.pdf">EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Peking University</span></p><p>本文提出EvoCoT，一种针对大语言模型（LLM）自进化的课程学习框架，通过两阶段链式思维（CoT）优化实现自生成并验证推理路径，随后逐步缩短推理步骤以受控扩展探索空间。实验表明，EvoCoT能帮助多种LLM在稀疏奖励下解决此前未能解决的难题，提升数学推理能力，并且优于传统RLVR及有监督微调等方法，突破探索瓶颈。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="252-UniSVG-A-Unified-Dataset-for-Vector-Graphic-Understanding-and-Generation-with-Multimodal-Large-Language-Models"><a href="#252-UniSVG-A-Unified-Dataset-for-Vector-Graphic-Understanding-and-Generation-with-Multimodal-Large-Language-Models" class="headerlink" title="252. UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models"></a>252. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/UniSVG__A_Unified_Dataset_for_Vector_Graphic_Understanding_and_Generation_with_Multimodal_Large_Lang.pdf">UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Zhejiang University</span></p><p>该论文提出了UniSVG，这是首个面向多模态大语言模型（MLLM）训练与评测的统一SVG数据集，包含52.8万条数据，涵盖图像到SVG、文本到SVG及SVG理解等多种任务。实验表明，利用UniSVG微调的开源MLLM在SVG生成与理解任务上超过了包括GPT-4V在内的主流封闭模型，显著提升了模型的SVG处理能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="253-LoSemB-Logic-Guided-Semantic-Bridging-for-Inductive-Tool-Retrieval"><a href="#253-LoSemB-Logic-Guided-Semantic-Bridging-for-Inductive-Tool-Retrieval" class="headerlink" title="253. LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval"></a>253. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LoSemB__Logic-Guided_Semantic_Bridging_for_Inductive_Tool_Retrieval.pdf">LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The Hong Kong Polytechnic University</span></p><p>本文提出了一种面向大语言模型工具检索任务的逻辑信息引导语义桥接框架LoSemB。该方法包含逻辑特征提取与迁移的嵌入对齐模块，有效缓解未见工具的分布偏移，并通过关系增强检索机制结合逻辑约束与图嵌入相似性，提高对未见工具的检索准确率。大量实验结果表明，LoSemB在归纳（inductive）与传导（transductive）两种设置下均显著优于现有方法，且无需对新工具重新训练，具备强泛化和实用性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="254-Semantic-Caching-for-Low-Cost-LLM-Serving-From-Offline-Learning-to-Online-Adaptation"><a href="#254-Semantic-Caching-for-Low-Cost-LLM-Serving-From-Offline-Learning-to-Online-Adaptation" class="headerlink" title="254. Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation"></a>254. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Semantic_Caching_for_Low-Cost_LLM_Serving__From_Offline_Learning_to_Online_Adaptation.pdf">Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>本论文提出一种面向大语言模型（LLM）推理低成本服务的语义缓存框架，从离线学习到在线自适应，系统性地建模了不确定环境下的缓存淘汰问题。方法包括设计超模逆向贪心、CUCB-SC和CLCB-SC-LS等算法，分别针对已知参数、离线数据学习和在线自适应场景，理论上给出近最优逼近、有限样本次优界和次线性遗憾上界。实验表明，所提方法在各种缓存大小和查询分布下显著优于基线，缓存切换数和运行时均大幅降低。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="255-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks"><a href="#255-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks" class="headerlink" title="255. GLiClass: Generalist Lightweight Model for Sequence Classification Tasks"></a>255. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/GLiClass__Generalist_Lightweight_Model_for_Sequence_Classification_Tasks.pdf">GLiClass: Generalist Lightweight Model for Sequence Classification Tasks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Knowledgator Engineering</span></p><p>GLiClass adapts the GLiNER architecture into a uni-encoder transformer model for efficient multi-label sequence classification, enabling joint text-label encoding and rich inter-label interactions. It achieves state-of-the-art accuracy and throughput on standard benchmarks, scales favorably with label count, excels in zero-shot and few-shot settings, and introduces PPO-based reinforcement learning for data-sparse scenarios; GLiClass surpasses cross-encoder baselines in accuracy-latency trade-off while maintaining practical deployment flexibility.</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="256-Multi-Turn-Jailbreaks-Are-Simpler-Than-They-Seem"><a href="#256-Multi-Turn-Jailbreaks-Are-Simpler-Than-They-Seem" class="headerlink" title="256. Multi-Turn Jailbreaks Are Simpler Than They Seem"></a>256. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-Turn_Jailbreaks_Are_Simpler_Than_They_Seem.pdf">Multi-Turn Jailbreaks Are Simpler Than They Seem</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>本文提出了一种自动化多轮越狱攻击流水线，并在GPT-4、Claude、Gemini等主流大模型上系统评测，发现所谓多轮越狱的成功本质上等价于多次单轮攻击采样，并无额外复杂性优势。结论指出，现有安全评估低估了攻击成功率，多轮策略仅相当于更多尝试，且同一厂商模型脆弱性高度相关，需关注根本鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="257-Beyond-Single-A-Data-Selection-Principle-for-LLM-Alignment-via-Fine-Grained-Preference-Signals"><a href="#257-Beyond-Single-A-Data-Selection-Principle-for-LLM-Alignment-via-Fine-Grained-Preference-Signals" class="headerlink" title="257. Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals"></a>257. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Beyond_Single__A_Data_Selection_Principle_for_LLM_Alignment_via_Fine-Grained_Preference_Signals.pdf">Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Nanjing University</span></p><p>该论文提出了一种基于细粒度偏好信号的数据选择方法用于大语言模型（LLM）对齐。核心方法包括推导多偏好直接优化（DMPO）目标，提出偏好分歧（PD）项，利用PD值选择高一致性子集用于标准偏好优化（DPO）训练，并通过奖励模型估算和长度偏置修正实现高效筛选。实验证明，该方法在UltraFeedback数据集上显著优于传统整体偏好和oracle方法，提升训练效率且无需复杂标注，实现更鲁棒的LLM对齐。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="258-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization"><a href="#258-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization" class="headerlink" title="258. Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization"></a>258. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Klear-Reasoner__Advancing_Reasoning_Capability_via_Gradient-Preserving_Clipping_Policy_Optimization.pdf">Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Kuaishou Technology</span></p><p>本论文提出Klear-Reasoner，通过高质量链式思维监督微调（long CoT SFT）和梯度保留剪切策略优化（GPPO）提升数学与代码推理能力。GPPO在强化学习阶段保留所有token的梯度信息，结合数据过滤、软奖励和SFT监督，有效提升模型探索性与收敛速度，最终在AIME和LiveCodeBench等多个基准上超越同规模SOTA模型。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="259-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment"><a href="#259-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment" class="headerlink" title="259. Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment"></a>259. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_to_Align,_Aligning_to_Learn__A_Unified_Approach_for_Self-Optimized_Alignment.pdf">Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AntGroup</span></p><p>本文提出了GRAO（Group Relative Alignment Optimization）框架，通过多样本生成、组内相对优势加权和参考感知参数更新，将SFT与RL的优点结合，实现高效自适应对齐。实验证明GRAO在复杂人类对齐任务上相较SFT、DPO、PPO和GRPO分别提升57.7%、17.65%、7.95%和5.18%，并显著加速收敛，适用于多种大型语言模型架构。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="260-CHIMERA-HARNESSING-MULTI-AGENT-LLMS-FOR-AUTOMATIC-INSIDER-THREAT-SIMULATION"><a href="#260-CHIMERA-HARNESSING-MULTI-AGENT-LLMS-FOR-AUTOMATIC-INSIDER-THREAT-SIMULATION" class="headerlink" title="260. CHIMERA: HARNESSING MULTI-AGENT LLMS FOR AUTOMATIC INSIDER THREAT SIMULATION"></a>260. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Chimera__Harnessing_Multi-Agent_LLMs_for_Automatic_Insider_Threat_Simulation.pdf">CHIMERA: HARNESSING MULTI-AGENT LLMS FOR AUTOMATIC INSIDER THREAT SIMULATION</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Singapore Management University</span></p><p>该论文提出了Chimera，一个基于多智能体大语言模型（LLM）的自动化框架，用于模拟企业内部员工的正常与恶意行为，并自动生成日志，解决高质量内部威胁检测（ITD）数据稀缺的问题。Chimera通过自定义员工角色和多阶段任务流程，模拟真实企业运作，生成覆盖15种攻击场景、6类日志模态的ChimeraLog数据集，并经专家评估验证其真实性和多样性。实验显示ChimeraLog数据更具挑战性，能促进ITD模型泛化能力提升。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="261-Semantic-Enhanced-Time-Series-Forecasting-via-Large-Language-Models"><a href="#261-Semantic-Enhanced-Time-Series-Forecasting-via-Large-Language-Models" class="headerlink" title="261. Semantic-Enhanced Time-Series Forecasting via Large Language Models"></a>261. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Semantic-Enhanced_Time-Series_Forecasting_via_Large_Language_Models.pdf">Semantic-Enhanced Time-Series Forecasting via Large Language Models</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology Beijing</span></p><p>该论文提出了SE-LLM框架，通过引入Temporal-Semantic Cross-Correlation (TSCC)模块和Time-Adapter插件，有效融合时序数据与LLM的语义空间，提升了LLM对时间序列的理解和预测能力。实验结果显示，SE-LLM在多项公开数据集上的预测性能均超越现有SOTA方法，且具备更高的泛化能力和计算效率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="262-1-2-3-Check-Enhancing-Contextual-Privacy-in-LLM-via-Multi-Agent-Reasoning"><a href="#262-1-2-3-Check-Enhancing-Contextual-Privacy-in-LLM-via-Multi-Agent-Reasoning" class="headerlink" title="262. 1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning"></a>262. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/1-2-3_Check__Enhancing_Contextual_Privacy_in_LLM_via_Multi-Agent_Reasoning.pdf">1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Carnegie Mellon University</span></p><p>该论文提出了一种多智能体框架，将LLM的隐私推理任务分解为事件抽取、分类、总结三个子任务，各Agent专注于特定子环节，通过控制信息流和引入校验环节，有效减轻单Agent过载问题。实验表明，该多Agent体系在ConfAIde和PrivacyLens基准上显著降低了隐私泄露（分别减少18%和19%），且保证了公共内容的完整性，优于单Agent方案，凸显了模块化信息流设计在复杂隐私场景下的优势。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="263-Designing-a-Feedback-Driven-Decision-Support-System-for-Dynamic-Student-Intervention"><a href="#263-Designing-a-Feedback-Driven-Decision-Support-System-for-Dynamic-Student-Intervention" class="headerlink" title="263. Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention"></a>263. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Designing_a_Feedback-Driven_Decision_Support_System_for_Dynamic_Student_Intervention.pdf">Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">WeAreGenius Research Institute</span></p><p>该论文提出了一种反馈驱动的决策支持系统，用于动态学生干预，核心方法是利用闭环架构和LightGBM回归模型，通过增量式在线学习不断吸收学生干预后的新数据，实现模型的自我迭代和优化。实验表明，系统可自动整合教师输入的新成绩，显著提升预测准确度（RMSE降低10.7%），且对学生干预效果有一致、可解释的正向调整，证明模型能泛化干预成功模式，推动教育决策智能化和人本化。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="264-QuProFS-An-Evolutionary-Training-free-Approach-to-Efficient-Quantum-Feature-Map-Search"><a href="#264-QuProFS-An-Evolutionary-Training-free-Approach-to-Efficient-Quantum-Feature-Map-Search" class="headerlink" title="264. QuProFS: An Evolutionary Training-free Approach to Efficient Quantum Feature Map Search"></a>264. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/QuProFS__An_Evolutionary_Training-free_Approach_to_Efficient_Quantum_Feature_Map_Search.pdf">QuProFS: An Evolutionary Training-free Approach to Efficient Quantum Feature Map Search</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">The University of Tokyo</span></p><p>该论文提出了QuProFS，一种基于进化算法的训练无关量子特征映射高效搜索方法。方法通过硬件约束感知的量子电路采样、KTA过滤和多代理训练无关评价指标（如表达性、可训练性、硬件鲁棒性等），结合非线性排序聚合与进化算子（层扩展+门剪枝）实现高效、鲁棒的量子特征映射搜索。实验结果显示，QuProFS在多种经典与量子数据集上准确率高，且架构搜索速度达到现有方法2倍，真实量子硬件上表现鲁棒，显著降低了计算成本。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="265-SEF-MK-Speaker-Embedding-Free-Voice-Anonymization-through-Multi-k-means-Quantization"><a href="#265-SEF-MK-Speaker-Embedding-Free-Voice-Anonymization-through-Multi-k-means-Quantization" class="headerlink" title="265. SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means Quantization"></a>265. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SEF-MK__Speaker-Embedding-Free_Voice_Anonymization_through_Multi-k-means_Quantization.pdf">SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means Quantization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Duke Kunshan University</span></p><p>该论文提出了一种全新的说话人嵌入无关（Speaker-Embedding-Free, SEF）语音匿名化方法SEF-MK，利用WavLM自监督表示、多个k-means量化模型和Conformer-HiFi-GAN解码器。方法流程包括：首先使用WavLM提取语音特征，然后在多个基于不同说话人子集训练的k-means模型中随机选择一个对特征量化，最后通过Conformer和HiFi-GAN重构匿名化语音。实验表明，多k-means模型能更好地保留语义和情感内容，同时在应对攻击者时提升了攻击多样性但也加大隐私风险，整体在隐私保护和语音可用性之间取得新的平衡。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="266-A-Stage-Aware-Mixture-of-Experts-Framework-for-Neurodegenerative-Disease-Progression-Modelling"><a href="#266-A-Stage-Aware-Mixture-of-Experts-Framework-for-Neurodegenerative-Disease-Progression-Modelling" class="headerlink" title="266. A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling"></a>266. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Stage-Aware_Mixture_of_Experts_Framework_for_Neurodegenerative_Disease_Progression_Modelling.pdf">A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University College London</span></p><p>该论文提出了一种阶段感知的专家混合（MoE）框架，结合病理学模型、非均匀图神经扩散模型（IGND）和本地化神经反应模块，并采用时序注意机制动态调节各专家对疾病进展的贡献。结果显示该模型在阿尔茨海默病tau蛋白传播的长期预测中优于传统方法，揭示了疾病不同阶段主导机制的变化，并提供了更具解释性的病理学见解。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="267-Multi-Level-Service-Performance-Forecasting-via-Spatiotemporal-Graph-Neural-Networks"><a href="#267-Multi-Level-Service-Performance-Forecasting-via-Spatiotemporal-Graph-Neural-Networks" class="headerlink" title="267. Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks"></a>267. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-Level_Service_Performance_Forecasting_via_Spatiotemporal_Graph_Neural_Networks.pdf">Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent Author</span></p><p>该论文提出了一种基于时空图神经网络（STGNN）的分布式多级服务性能预测方法，通过将系统在不同时间片的状态建模为图结构，结合服务节点的运行特征和服务调用关系，利用图卷积网络提取结构依赖，结合门控循环单元（GRU）建模时序动态，并引入时间编码机制提升对非平稳序列的表达能力，整体端到端训练实现高精度性能指标回归。实验结果表明，该方法在大规模真实数据集上相较主流基线在MAE、RMSE和R²等指标上表现优异，具备较强的泛化性和鲁棒性，能够适应不同窗口长度和并发负载，适用于智能运维和资源调度等实际场景。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="268-Improving-Real-Time-Concept-Drift-Detection-using-a-Hybrid-Transformer-Autoencoder-Framework"><a href="#268-Improving-Real-Time-Concept-Drift-Detection-using-a-Hybrid-Transformer-Autoencoder-Framework" class="headerlink" title="268. Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework"></a>268. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Improving_Real-Time_Concept_Drift_Detection_using_a_Hybrid_Transformer-Autoencoder_Framework.pdf">Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Vellore Institute of Technology (VIT-AP)</span></p><p>该论文提出了一种结合Transformer与Autoencoder的混合框架，用于实时检测数据流中的概念漂移。方法包括利用统计指标（PSI、JSD）、重构误差、预测不确定性、规则违规及分类器错误趋势，通过CatBoost分类器和复合Trust Score提升检测灵敏度与解释性。实验结果表明，该框架在合成航空乘客数据集上比主流方法更早、更敏感地检测到漂移，并增强了模型解释能力，适用于动态环境下的机器学习系统监控。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="269-Balancing-Privacy-and-Efficiency-Music-Information-Retrieval-via-Additive-Homomorphic-Encryption"><a href="#269-Balancing-Privacy-and-Efficiency-Music-Information-Retrieval-via-Additive-Homomorphic-Encryption" class="headerlink" title="269. Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption"></a>269. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Balancing_Privacy_and_Efficiency__Music_Information_Retrieval_via_Additive_Homomorphic_Encryption.pdf">Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Independent Researcher</span></p><p>该论文提出了一种基于加法同态加密（AHE）的音乐信息检索方法，通过结构化的分块内积与加权层次内积实现对高维音乐向量的高效隐私保护相似性搜索。实验结果显示，相较于全同态加密（FHE），AHE方案在检索速度和内存占用方面更优，有效平衡了安全性与实用性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="270-TurboBias-Universal-ASR-Context-Biasing-powered-by-GPU-accelerated-Phrase-Boosting-Tree"><a href="#270-TurboBias-Universal-ASR-Context-Biasing-powered-by-GPU-accelerated-Phrase-Boosting-Tree" class="headerlink" title="270. TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree"></a>270. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TurboBias__Universal_ASR_Context-Biasing_powered_by_GPU-accelerated_Phrase-Boosting_Tree.pdf">TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">NVIDIA</span></p><p>本论文提出了一种通用ASR上下文偏置框架GPU-PB，基于GPU加速的短语提升树和改进的权重分布，可高效支持CTC、RNN-T和Attention Encoder-Decoder等主流声学模型，并适用于贪婪和束搜索解码。实验证明，该方法在提升关键词识别准确率（F-score）和整体识别性能（WER）方面均优于现有开源方法，且推理速度损耗极低，支持高达2万条短语的扩展。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="271-TLCCSP-A-Scalable-Framework-for-Enhancing-Time-Series-Forecasting-with-Time-Lagged-Cross-Correlations"><a href="#271-TLCCSP-A-Scalable-Framework-for-Enhancing-Time-Series-Forecasting-with-Time-Lagged-Cross-Correlations" class="headerlink" title="271. TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations"></a>271. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TLCCSP__A_Scalable_Framework_for_Enhancing_Time_Series_Forecasting_with_Time-Lagged_Cross-Correlatio.pdf">TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Artificial Intelligence, Beijing Normal University</span></p><p>该论文提出了TLCCSP框架，通过设计Sequence Shifted Dynamic Time Warping (SSDTW) 算法自动发现时间序列间的时滞相关性，并引入对比学习编码器高效近似SSDTW距离，极大提升了多领域（气象、金融、房地产）时间序列预测的准确性和可扩展性。实验表明，TLCCSP在多个数据集上显著降低了预测误差，且对比学习方法将计算耗时减少约99%，有效支持大规模实时预测。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="272-UniMove-A-Unified-Model-for-Multi-city-Human-Mobility-Prediction"><a href="#272-UniMove-A-Unified-Model-for-Multi-city-Human-Mobility-Prediction" class="headerlink" title="272. UniMove: A Unified Model for Multi-city Human Mobility Prediction"></a>272. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/UniMove__A_Unified_Model_for_Multi-city_Human_Mobility_Prediction.pdf">UniMove: A Unified Model for Multi-city Human Mobility Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Tsinghua University</span></p><p>本论文提出UniMove，一个用于多城市人类移动性预测的统一模型。UniMove采用轨迹-位置双塔结构，通过位置塔生成通用空间表示，并加入Deep &amp; Cross Net以增强特征表达，同时轨迹塔采用MoE Transformer模块自适应选择专家网络处理不同城市的异质移动模式。实验结果表明，UniMove能在多城市数据联合训练中实现数据互补，准确率提升超过10.2%，尤其对数据稀疏城市提升显著，具备更强泛化能力和训练效率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="273-Simulating-Biological-Intelligence-Active-Inference-with-Experiment-Informed-Generative-Model"><a href="#273-Simulating-Biological-Intelligence-Active-Inference-with-Experiment-Informed-Generative-Model" class="headerlink" title="273. Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model"></a>273. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Simulating_Biological_Intelligence__Active_Inference_with_Experiment-Informed_Generative_Model.pdf">Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Turner Institute for Brain and Mental Health, School of Psychological Sciences, Monash University</span></p><p>本论文提出了一种基于主动推断（Active Inference）理论的生成模型框架，模拟生物神经网络决策过程，通过结合POMDP结构和实验数据，系统对比了多种决策算法（包括记忆增强的反事实学习、动态规划和经典主动推断）在Pong环境下的学习表现。结果显示，具有记忆机制的反事实学习方法在动态环境中的决策效果优于传统规划算法，揭示了记忆在智能体目标导向行为中的关键作用，并为可解释、类脑AI系统的设计提供了理论基础。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="274-Neural-Beam-Field-for-Spatial-Beam-RSRP-Prediction"><a href="#274-Neural-Beam-Field-for-Spatial-Beam-RSRP-Prediction" class="headerlink" title="274. Neural Beam Field for Spatial Beam RSRP Prediction"></a>274. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Neural_Beam_Field_for_Spatial_Beam_RSRP_Prediction.pdf">Neural Beam Field for Spatial Beam RSRP Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xiamen University</span></p><p>本文提出了Neural Beam Field (NBF)，一种结合Transformer神经网络和物理模型的混合框架，用于高效且可解释的空间波束RSRP预测。方法核心包括多路径条件功率剖面(MCPP)的引入，结合先验物理建模与神经网络学习，并通过预训练-校准策略提升收敛与泛化能力。实验结果表明，NBF较传统表格法和纯黑盒神经网络在准确率、训练效率和模型紧凑性方面均有显著提升，适用于密集无线网络中的智能波束管理。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="275-Machine-Learning-Algorithms-for-Improving-Exact-Classical-Solvers-in-Mixed-Integer-Continuous-Optimization"><a href="#275-Machine-Learning-Algorithms-for-Improving-Exact-Classical-Solvers-in-Mixed-Integer-Continuous-Optimization" class="headerlink" title="275. Machine Learning Algorithms for Improving Exact Classical Solvers in Mixed Integer Continuous Optimization"></a>275. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Machine_Learning_Algorithms_for_Improving_Exact_Classical_Solvers_in_Mixed_Integer_Continuous_Optimi.pdf">Machine Learning Algorithms for Improving Exact Classical Solvers in Mixed Integer Continuous Optimization</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Universität Wien</span></p><p>本文系统综述了机器学习（ML）和强化学习（RL）技术在提升整数与混合整数非线性规划（INLP、MINLP）等精确优化算法中的应用，特别聚焦于与分支定界（Branch-and-Bound, BB）框架的集成，包括分支决策、切割选择、节点排序和参数控制等环节。通过将监督学习、模仿学习和强化学习嵌入传统优化流程，显著加速求解过程且不损失全局最优性。结论指出：学习增强的优化方法在大规模或结构复杂问题上能提高收敛速度，但在数据稀缺和结构规则时仍需依赖传统算法，两者融合是未来智能优化发展的关键。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="276-A-Score-based-Diffusion-Model-Approach-for-Adaptive-Learning-of-Stochastic-Partial-Differential-Equation-Solutions"><a href="#276-A-Score-based-Diffusion-Model-Approach-for-Adaptive-Learning-of-Stochastic-Partial-Differential-Equation-Solutions" class="headerlink" title="276. A Score-based Diffusion Model Approach for Adaptive Learning of Stochastic Partial Differential Equation Solutions"></a>276. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Score-based_Diffusion_Model_Approach_for_Adaptive_Learning_of_Stochastic_Partial_Differential_Equa.pdf">A Score-based Diffusion Model Approach for Adaptive Learning of Stochastic Partial Differential Equation Solutions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of South Carolina</span></p><p>本文提出了一种基于分数(score)-驱动扩散模型的自适应学习框架，用于递归贝叶斯推断下的随机偏微分方程(SPDEs)解的动态估计。方法将物理知识编码到扩散模型的分数函数，通过数值仿真和观测数据的似然校正，实现模型状态的迭代优化，并提出无需训练的Ensemble Score Filter以提升高维实时推断效率。大量数值实验（如Burgers方程、Navier-Stokes方程、Allen-Cahn方程）显示，在稀疏和有噪声观测条件下，该方法能准确、鲁棒地恢复隐藏物理状态和提升解的精度。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="277-Mode-Aware-Non-Linear-Tucker-Autoencoder-for-Tensor-based-Unsupervised-Learning"><a href="#277-Mode-Aware-Non-Linear-Tucker-Autoencoder-for-Tensor-based-Unsupervised-Learning" class="headerlink" title="277. Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning"></a>277. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Mode-Aware_Non-Linear_Tucker_Autoencoder_for_Tensor-based_Unsupervised_Learning.pdf">Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">东南大学</span></p><p>该论文提出了一种新颖的无监督学习框架——Mode-Aware Non-linear Tucker Autoencoder (MA-NTAE)，通过递归的Pick–Unfold–Encode–Fold操作实现高阶张量的灵活非线性压缩，结合Tucker分解与现代自编码器技术，显著提升了高维数据的压缩、重构和聚类表现。实验结果显示，MA-NTAE在多个合成和真实数据集上的重构误差、聚类指标均优于典型的深度自编码器（DAE）和张量分解神经网络（TFNN），且模型参数量小、训练效率高，尤其适用于高阶高维张量场景。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="278-Structured-Superposition-of-Autoencoders-for-UEP-Codes-at-Intermediate-Blocklengths"><a href="#278-Structured-Superposition-of-Autoencoders-for-UEP-Codes-at-Intermediate-Blocklengths" class="headerlink" title="278. Structured Superposition of Autoencoders for UEP Codes at Intermediate Blocklengths"></a>278. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Structured_Superposition_of_Autoencoders_for_UEP_Codes_at_Intermediate_Blocklengths.pdf">Structured Superposition of Autoencoders for UEP Codes at Intermediate Blocklengths</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Novi Sad</span></p><p>该论文提出了一种结构化自编码器（AE）架构，将编码和解码过程分为多个AE子模块，并结合叠加编码与SIC解码思想，实现了中等区块长度下的比特级不等错误保护（UEP）码设计。数值结果显示，该结构在可达错误概率区域优于传统叠加随机编码与SIC解码方法，且可高效扩展至更大区块长度，适用于资源受限场景。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="279-Lightning-Prediction-under-Uncertainty-DeepLight-with-Hazy-Loss"><a href="#279-Lightning-Prediction-under-Uncertainty-DeepLight-with-Hazy-Loss" class="headerlink" title="279. Lightning Prediction under Uncertainty: DeepLight with Hazy Loss"></a>279. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Lightning_Prediction_under_Uncertainty__DeepLight_with_Hazy_Loss.pdf">Lightning Prediction under Uncertainty: DeepLight with Hazy Loss</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Bangladesh University of Engineering and Technology</span></p><p>本文提出了DeepLight，一种新颖的深度学习架构，通过多分支ConvLSTM和双编码器，融合雷达反射率、云属性和历史闪电数据进行闪电预测。创新性地引入Hazy Loss邻域感知损失函数，有效处理闪电的时空不确定性，提升预测准确性。实验表明，DeepLight在多个预测时长上较现有方法Equitable Threat Score提升8%–30%，Hazy Loss对多模型均有显著增益。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="280-Leveraging-GNN-to-Enhance-MEF-Method-in-Predicting-ENSO"><a href="#280-Leveraging-GNN-to-Enhance-MEF-Method-in-Predicting-ENSO" class="headerlink" title="280. Leveraging GNN to Enhance MEF Method in Predicting ENSO"></a>280. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Leveraging_GNN_to_Enhance_MEF_Method_in_Predicting_ENSO.pdf">Leveraging GNN to Enhance MEF Method in Predicting ENSO</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shiraz University of Technology</span></p><p>本文提出了一种基于图神经网络（GNN）增强的多模态ENSO预测（MEF）框架，将3D卷积神经网络（3D-CNN）与时间序列模块输出的80个集成成员通过构建相似性加权图进行结构化分析，并利用社区检测方法选取最优子集，最终提升预测的鲁棒性和准确性，尤其在长期预测场景下表现更优。实验结果表明，GNN方法可有效过滤不稳定噪声成员，实现更高的相关性和一致性输出，且该方法对模型结构无依赖，易于应用于其它集成预测系统。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="281-ProteoKnight-Convolution-based-phage-virion-protein-classification-and-uncertainty-analysis"><a href="#281-ProteoKnight-Convolution-based-phage-virion-protein-classification-and-uncertainty-analysis" class="headerlink" title="281. ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis"></a>281. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ProteoKnight__Convolution-based_phage_virion_protein_classification_and_uncertainty_analysis.pdf">ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">BRAC University</span></p><p>本论文提出ProteoKnight方法，采用基于DNA-Walk的Knight编码，将蛋白质序列转化为图像，并利用预训练卷积神经网络（CNN）进行二分类和多分类预测，同时引入Monte Carlo Dropout方法对模型预测进行不确定性分析。结果显示，Knight编码结合高效CNN模型可在二分类任务中达到约90.8%的准确率，并揭示了蛋白质类别和序列长度对预测置信度的影响。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="282-Finite-Time-Convergence-Analysis-of-ODE-based-Generative-Models-for-Stochastic-Interpolants"><a href="#282-Finite-Time-Convergence-Analysis-of-ODE-based-Generative-Models-for-Stochastic-Interpolants" class="headerlink" title="282. Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants"></a>282. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Finite-Time_Convergence_Analysis_of_ODE-based_Generative_Models_for_Stochastic_Interpolants.pdf">Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">IIIS, Tsinghua University</span></p><p>该论文系统分析了基于常微分方程（ODE）的生成模型在随机插值框架下的有限时间收敛性，通过理论推导给出了前向Euler方法和Heun’s方法在总变差距离上的误差界，并分析了迭代复杂度。实验结果验证了理论收敛速率，为实际生成过程的数值实现提供了更精确的误差控制和复杂度优化。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="283-Fairness-of-Automatic-Speech-Recognition-Looking-Through-a-Philosophical-Lens"><a href="#283-Fairness-of-Automatic-Speech-Recognition-Looking-Through-a-Philosophical-Lens" class="headerlink" title="283. Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens"></a>283. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fairness_of_Automatic_Speech_Recognition__Looking_Through_a_Philosophical_Lens.pdf">Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Cornell University</span></p><p>本文通过哲学视角分析自动语音识别（ASR）系统的偏见与公平性，提出ASR对非标准方言的系统性误识不仅是技术局限，更是对边缘化语言群体的不尊重。作者区分了道德中性的分类（discriminate1）与有害歧视（discriminate2），并揭示ASR偏见的三大独特伦理维度：时间负担（temporal taxation）、对话流中断和对身份认同的影响。结论认为，解决ASR偏见不仅需技术改进，更需承认多样化语音形式的合法性，推动技术与伦理并重，实现对语言多样性的尊重与包容。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="284-An-effective-potential-for-generative-modelling-with-active-matter"><a href="#284-An-effective-potential-for-generative-modelling-with-active-matter" class="headerlink" title="284. An effective potential for generative modelling with active matter"></a>284. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/An_effective_potential_for_generative_modelling_with_active_matter.pdf">An effective potential for generative modelling with active matter</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Queen Mary University of London</span></p><p>本文提出了一种基于主动粒子过程的生成扩散模型，通过在位置坐标上施加有效的时间相关势能，实现了扩散模型的时间反演采样。关键技术包括Fox和UCN近似，有效势能由标准score函数及其高阶导数决定，数值实验验证了方法的准确性。结论表明，所提方法能精确再现目标分布，且具备实验实现潜力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="285-OFAL-An-Oracle-Free-Active-Learning-Framework"><a href="#285-OFAL-An-Oracle-Free-Active-Learning-Framework" class="headerlink" title="285. OFAL: An Oracle-Free Active Learning Framework"></a>285. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/OFAL__An_Oracle-Free_Active_Learning_Framework.pdf">OFAL: An Oracle-Free Active Learning Framework</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Amirkabir University of Technology</span></p><p>该论文提出了OFAL，一种无需人工标注者的主动学习框架，通过神经网络的不确定性量化（利用Monte Carlo Dropout近似贝叶斯神经网络）与变分自编码器（VAE），将模型高置信的无标签样本转换为具有高信息量的不确定样本，从而提升模型性能。实验表明，在MNIST数据集上，通过多轮生成不确定样本并与原样本共同训练，模型准确率提升2.7%，且OFAL可与其它主动学习采样方法结合，在无需额外标注成本下进一步提升效果。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="286-Learned-Regularization-for-Microwave-Tomography"><a href="#286-Learned-Regularization-for-Microwave-Tomography" class="headerlink" title="286. Learned Regularization for Microwave Tomography"></a>286. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learned_Regularization_for_Microwave_Tomography.pdf">Learned Regularization for Microwave Tomography</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">University of Science and Technology of China</span></p><p>本论文提出了一种物理信息引导的混合重建框架，将单步扩散模型（Single-Step Diffusion Regularization, SSD-Reg）作为Plug-and-Play正则项嵌入到微波层析成像（MWT）迭代重建流程中，有效结合了精确的物理建模与生成式先验。该方法无需配对数据，通过Fréchet可微前向模型和单步扩散正则项，引入强结构先验，显著提升了重建的准确性、稳定性和收敛速度。大量仿真和真实数据实验表明，该方法在精度、抗噪性和效率方面均优于现有方法，展现出较强的临床应用潜力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="287-C-MAG-Cascade-Multimodal-Attributed-Graphs-for-Supply-Chain-Link-Prediction"><a href="#287-C-MAG-Cascade-Multimodal-Attributed-Graphs-for-Supply-Chain-Link-Prediction" class="headerlink" title="287. C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction"></a>287. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/C-MAG__Cascade_Multimodal_Attributed_Graphs_for_Supply_Chain_Link_Prediction.pdf">C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">AAITC, Lenovo</span></p><p>该论文提出了C-MAG（Cascade Multimodal Attributed Graphs）方法，通过两阶段级联架构将文本和图像等多模态信息聚合进制造商节点，再通过异构图神经网络实现制造商-产品关系的高精度预测。实验结果显示，C-MAG在ROC-AUC和PR-AUC等指标上优于传统方法，且对视觉噪声具有鲁棒性，为供应链知识图谱的多模态集成和链路预测提供了新的技术方案。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="288-From-Source-to-Target-Leveraging-Transfer-Learning-for-Predictive-Process-Monitoring-in-Organizations"><a href="#288-From-Source-to-Target-Leveraging-Transfer-Learning-for-Predictive-Process-Monitoring-in-Organizations" class="headerlink" title="288. From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations"></a>288. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Source_to_Target__Leveraging_Transfer_Learning_for_Predictive_Process_Monitoring_in_Organizatio.pdf">From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Friedrich-Alexander-Universität Erlangen-Nürnberg</span></p><p>该论文提出了一种基于迁移学习的预测流程监控（PPM）技术，通过预训练嵌入模型进行活动编码和相对跨域时间戳编码，将源业务流程的LSTM预测模型移植到目标流程，无需对目标数据进行微调。实验证明，该方法可在组织内外实现跨流程知识转移，有效提升目标流程的预测性能，帮助缺乏数据或算力的组织实现高效流程监控和决策支持。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="289-On-Understanding-of-the-Dynamics-of-Model-Capacity-in-Continual-Learning"><a href="#289-On-Understanding-of-the-Dynamics-of-Model-Capacity-in-Continual-Learning" class="headerlink" title="289. On Understanding of the Dynamics of Model Capacity in Continual Learning"></a>289. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/On_Understanding_of_the_Dynamics_of_Model_Capacity_in_Continual_Learning.pdf">On Understanding of the Dynamics of Model Capacity in Continual Learning</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Capital One</span></p><p>本文提出了“连续学习有效模型容量（CLEMC）”的动态理论框架，系统刻画了神经网络在连续学习中的稳定性-可塑性平衡点随任务演化的动态变化。方法上，作者基于动态规划和最优控制，推导出模型容量随任务分布、权重更新、优化过程变化的递推方程，并从理论上证明了无论模型结构或优化方法如何，只要新任务的分布不断变化，容量（即模型对新任务的表征能力）都会不可避免地发散，导致遗忘现象不断累积。通过FNN、CNN、GNN和大规模Transformer等多种架构及合成&#x2F;真实数据集实验，实证验证了理论结论的普适性。结论：连续学习中模型容量动态变化、不可避免地随着任务变化发散，需采用动态模型和容量约束方法才能有效应对。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="290-Learning-to-Select-MCP-Algorithms-From-Traditional-ML-to-Dual-Channel-GAT-MLP"><a href="#290-Learning-to-Select-MCP-Algorithms-From-Traditional-ML-to-Dual-Channel-GAT-MLP" class="headerlink" title="290. Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP"></a>290. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_to_Select_MCP_Algorithms__From_Traditional_ML_to_Dual-Channel_GAT-MLP.pdf">Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Shantou University</span></p><p>该论文针对最大团问题（MCP）的算法选择，提出了一个结合传统机器学习和图神经网络（GNN）的学习型框架。作者通过构建包含多样化图实例和结构特征的数据集，评估了SVM、随机森林等传统分类器，并提出了融合局部结构编码（GAT）和全局特征建模（MLP）的双通道GAT-MLP模型。实验结果显示，GAT-MLP在准确率和F1分数上均优于传统方法，能有效自动选择最优MCP算法，显著降低计算成本，验证了双通道架构在组合算法选择上的有效性与泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="291-MemoryKT-An-Integrative-Memory-and-Forgetting-Method-for-Knowledge-Tracing"><a href="#291-MemoryKT-An-Integrative-Memory-and-Forgetting-Method-for-Knowledge-Tracing" class="headerlink" title="291. MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing"></a>291. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MemoryKT__An_Integrative_Memory-and-Forgetting_Method_for_Knowledge_Tracing.pdf">MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China Normal University</span></p><p>本文提出了一种基于时序变分自编码器（VAE）的知识追踪（KT）模型MemoryKT，将记忆的编码、存储和检索三阶段过程进行建模，并在时序流程中嵌入个性化遗忘模块，动态调节学生记忆存储强度。实验结果显示，MemoryKT在四个公开数据集上显著优于现有主流KT方法，提升了模型的性能与个性化建模能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="292-Fast-and-Generalizable-parameter-embedded-Neural-Operators-for-Lithium-Ion-Battery-Simulation"><a href="#292-Fast-and-Generalizable-parameter-embedded-Neural-Operators-for-Lithium-Ion-Battery-Simulation" class="headerlink" title="292. Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation"></a>292. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fast_and_Generalizable_parameter-embedded_Neural_Operators_for_Lithium-Ion_Battery_Simulation.pdf">Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">RWTH Aachen University</span></p><p>本文提出并系统比较了三种算子学习神经网络（DeepONet、FNO和新提出的参数嵌入Fourier神经算子PE-FNO）在锂离子电池仿真中的应用。PE-FNO在保持高准确率的同时实现了对材料参数广泛变化的泛化，并在速度上比传统多线程仿真器快约两百倍，适合实时电池管理和大规模推断任务。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="293-FNBT-Full-Negation-Belief-Transformation-for-Open-World-Information-Fusion-Based-on-Dempster-Shafer-Theory-of-Evidence"><a href="#293-FNBT-Full-Negation-Belief-Transformation-for-Open-World-Information-Fusion-Based-on-Dempster-Shafer-Theory-of-Evidence" class="headerlink" title="293. FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence"></a>293. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FNBT__Full_Negation_Belief_Transformation_for_Open-World_Information_Fusion_Based_on_Dempster-Shafer.pdf">FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">School of Artificial Intelligence, South China Normal University</span></p><p>本文提出了一种基于Dempster-Shafer证据理论的全否定信念变换（FNBT）方法，用于解决实际应用中因异构帧导致的信息融合问题。FNBT通过引入开放世界判据、帧扩展和全否定机制，将原始概率分配映射到扩展帧，使传统组合规则在开放世界下有效，理论证明其具备质量函数不变性、可继承性和本质冲突消除等性质。实验结果显示，FNBT在多个真实分类任务上显著优于现有方法，特别适用于数据孤岛和异构数据源融合场景。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="294-Multi-modal-Adaptive-Mixture-of-Experts-for-Cold-start-Recommendation"><a href="#294-Multi-modal-Adaptive-Mixture-of-Experts-for-Cold-start-Recommendation" class="headerlink" title="294. Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation"></a>294. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-modal_Adaptive_Mixture_of_Experts_for_Cold-start_Recommendation.pdf">Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">VNU University of Engineering and Technology</span></p><p>本论文提出了MAMEX框架，通过多模态专家网络和可学习门控机制对冷启动推荐系统中的多模态信息（如图像、文本）进行自适应融合，动态平衡各模态贡献，并引入正则项防止模态塌缩。实验结果表明，MAMEX在多个Amazon数据集上显著优于现有方法，提升了Recall和NDCG指标，验证了模型的有效性和适应性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="295-Deep-Learning-Based-Analysis-of-Power-Consumption-in-Gasoline-Electric-and-Hybrid-Vehicles"><a href="#295-Deep-Learning-Based-Analysis-of-Power-Consumption-in-Gasoline-Electric-and-Hybrid-Vehicles" class="headerlink" title="295. Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles"></a>295. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Deep_Learning-Based_Analysis_of_Power_Consumption_in_Gasoline,_Electric,_and_Hybrid_Vehicles.pdf">Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">National Research Council Canada</span></p><p>本文提出了一种基于深度学习（TCN、LSTM、Transformer）与传统机器学习（RF）的数据驱动方法，利用车辆动力系统动态特征，预测汽油车（ICE）、电动车（EV）、混合动力车（HEV）的瞬时与累计能耗，并收集了三类车辆的真实道路数据。结果表明，深度学习模型在ICE上的预测精度极高（MAE和RMSE约为10^-3），在EV和HEV上Transformer和LSTM表现最佳，累计能耗误差低于4.1%和2.1%，且不确定性分析显示EV和HEV预测波动更大，突显了复杂动力系统对能耗建模的挑战。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="296-Topological-Feature-Compression-for-Molecular-Graph-Neural-Networks"><a href="#296-Topological-Feature-Compression-for-Molecular-Graph-Neural-Networks" class="headerlink" title="296. Topological Feature Compression for Molecular Graph Neural Networks"></a>296. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Topological_Feature_Compression_for_Molecular_Graph_Neural_Networks.pdf">Topological Feature Compression for Molecular Graph Neural Networks</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Imperial College London</span></p><p>本论文提出了一种结合高阶拓扑特征压缩的分子图神经网络（PACTNet），通过ECC算法将高阶细胞复形的拓扑特征与传统分子图特征融合，有效捕捉全球几何信息且保持特征的可解释性和高效计算。通过在多个分子性质预测基准数据集上的实验证明，PACTNet在大多数任务中实现了统计学显著优于现有主流GNN模型的表现，兼顾了准确率、鲁棒性和计算效率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="297-Frequency-Domain-Analysis-of-Time-Dependent-Multiomic-Data-in-Progressive-Neurodegenerative-Diseases-A-Proposed-Quantum-Classical-Hybrid-Approach-with-Quaternionic-Extensions"><a href="#297-Frequency-Domain-Analysis-of-Time-Dependent-Multiomic-Data-in-Progressive-Neurodegenerative-Diseases-A-Proposed-Quantum-Classical-Hybrid-Approach-with-Quaternionic-Extensions" class="headerlink" title="297. Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions"></a>297. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Frequency-Domain_Analysis_of_Time-Dependent_Multiomic_Data_in_Progressive_Neurodegenerative_Diseases.pdf">Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Massachusetts General Hospital</span></p><p>本文提出了一种将多组学和神经影像时序数据通过傅里叶&#x2F;Laplace变换转入频域，并利用哈密顿量建模、变分量子特征求解器（VQE）和四元数扩展进行高维特征提取和疾病动力学建模的方法。结论指出该理论框架为未来用量子混合计算和超复数方法分析神经变性病提供了数学基础，有望提升高危患者预测和精准医疗决策，但需进一步实证验证。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="298-Generative-Inversion-for-Property-Targeted-Materials-Design-Application-to-Shape-Memory-Alloys"><a href="#298-Generative-Inversion-for-Property-Targeted-Materials-Design-Application-to-Shape-Memory-Alloys" class="headerlink" title="298. Generative Inversion for Property-Targeted Materials Design: Application to Shape Memory Alloys"></a>298. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Generative_Inversion_for_Property-Targeted_Materials_Design__Application_to_Shape_Memory_Alloys.pdf">Generative Inversion for Property-Targeted Materials Design: Application to Shape Memory Alloys</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Xi’an Jiaotong University</span></p><p>本文提出了一种基于生成对抗网络（GAN）反演的新型高性能形状记忆合金（SMA）逆向设计框架，结合预训练GAN和性质预测模型，通过梯度优化在潜空间生成满足指定性能指标的合金成分与工艺参数。实验合成和表征的五种NiTi基合金均达到高转变温度和大机械功输出，A1合金性能优异，验证了框架的有效性和广泛适用性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="299-Disentangling-Multiplex-Spatial-Temporal-Transition-Graph-Representation-Learning-for-Socially-Enhanced-POI-Recommendation"><a href="#299-Disentangling-Multiplex-Spatial-Temporal-Transition-Graph-Representation-Learning-for-Socially-Enhanced-POI-Recommendation" class="headerlink" title="299. Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation"></a>299. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Disentangling_Multiplex_Spatial-Temporal_Transition_Graph_Representation_Learning_for_Socially_Enhan.pdf">Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">South China Normal University</span></p><p>该论文提出了DiMuST方法，通过构建社会异质图和多路空间-时间转移图，利用Disentangled Variational Multiplex Graph Auto-Encoder (DAE)实现空间-时间信息的共享与私有分布解耦，并通过Product of Experts机制融合共享特征，对私有特征进行对比约束去噪。该方法在两个真实数据集上显著优于现有POI推荐方法，有效提升了推荐准确率和模型鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="300-EMPATHIA-Multi-Faceted-Human-AI-Collaboration-for-Refugee-Integration"><a href="#300-EMPATHIA-Multi-Faceted-Human-AI-Collaboration-for-Refugee-Integration" class="headerlink" title="300. EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration"></a>300. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EMPATHIA__Multi-Faceted_Human-AI_Collaboration_for_Refugee_Integration.pdf">EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Texas A&amp;M University</span></p><p>该论文提出EMPATHIA框架，通过多智能体系统（情感、文化、伦理三代理）及selector–validator架构，实现难民安置的人机协同决策，兼顾情感、文化和伦理价值。实验证明，在UN Kakuma数据集上，系统能以87.4%一致率给出可解释推荐，有效提升安置公平性和可扩展性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>今日论文报纸</title>
    <link href="/2025/08/2025-08-12_article/"/>
    <url>/2025/08/2025-08-12_article/</url>
    
    <content type="html"><![CDATA[<center><h1>今日论文报纸</h1></center><p>📅 <strong>日期</strong>：2025-08-12<br>📄 <strong>发现论文数量</strong>：119  </p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-The-Fair-Game-Auditing-Debiasing-AI-Algorithms-Over-Time"><a href="#1-The-Fair-Game-Auditing-Debiasing-AI-Algorithms-Over-Time" class="headerlink" title="1. The Fair Game: Auditing &amp; Debiasing AI Algorithms Over Time"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/The_Fair_Game__Auditing_&_Debiasing_AI_Algorithms_Over_Time.pdf">The Fair Game: Auditing &amp; Debiasing AI Algorithms Over Time</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Inria</span></p><p>该论文提出了“Fair Game”框架，通过将审计器（Auditor）与去偏算法（Debiasing Algorithm）组成闭环，动态审计和去偏机器学习模型，利用强化学习（RL）机制实现长期公平性目标的自适应调整。实验与理论分析表明，该方法能在数据分布和社会伦理规范动态变化的环境下，持续减少模型偏见并提高公平性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Unsupervised-Partner-Design-Enables-Robust-Ad-hoc-Teamwork"><a href="#2-Unsupervised-Partner-Design-Enables-Robust-Ad-hoc-Teamwork" class="headerlink" title="2. Unsupervised Partner Design Enables Robust Ad-hoc Teamwork"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Unsupervised_Partner_Design_Enables_Robust_Ad-hoc_Teamwork.pdf">Unsupervised Partner Design Enables Robust Ad-hoc Teamwork</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">University of Stuttgart</span></p><p>该论文提出了Unsupervised Partner Design (UPD)——一种无需预训练伙伴、无须人工调参的多智能体强化学习框架，通过动态生成带有随机偏差和能力多样性的训练伙伴，并利用回报方差作为学习性指标筛选最能促进主智能体学习的伙伴。UPD可与无监督环境设计结合，实现同时针对任务和伙伴分布的自适应课程学习，在Overcooked-AI和其通用挑战中显著优于主流基线，并在人机协同实验中获得更高评分和回报，展现强泛化与协作能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-OM2P-Offline-Multi-Agent-Mean-Flow-Policy"><a href="#3-OM2P-Offline-Multi-Agent-Mean-Flow-Policy" class="headerlink" title="3. OM2P: Offline Multi-Agent Mean-Flow Policy"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/OM2P__Offline_Multi-Agent_Mean-Flow_Policy.pdf">OM2P: Offline Multi-Agent Mean-Flow Policy</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Tsinghua University</span></p><p>该论文提出了OM2P算法，将均值流（mean-flow）生成模型创新性地集成到离线多智能体强化学习（MARL）中，实现高效的一步动作生成。方法上通过奖励感知优化、广义时间步分布和无导数估算，提升了训练稳定性和内存效率。实验表明，OM2P在多智能体标准任务中达到了最优或近最优表现，显著减少了GPU内存消耗并加速训练。结论是OM2P可高效扩展至复杂多智能体环境，兼具高性能与实用性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="4-Lightweight-Auto-bidding-based-on-Traffic-Prediction-in-Live-Advertising"><a href="#4-Lightweight-Auto-bidding-based-on-Traffic-Prediction-in-Live-Advertising" class="headerlink" title="4. Lightweight Auto-bidding based on Traffic Prediction in Live Advertising"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Lightweight_Auto-bidding_based_on_Traffic_Prediction_in_Live_Advertising.pdf">Lightweight Auto-bidding based on Traffic Prediction in Live Advertising</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Taobao &amp; Tmall Group of Alibaba</span></p><p>本文提出了BiCB轻量级自动竞价算法，结合线性规划(LP)理论分析的最优竞价公式与未来流量的统计预测，利用LightGBM等回归模型估算全时段流量，通过梯度下降高效近似优化目标。实验证明BiCB在满足预算和点击单价约束下性能优越，工程复杂度低，已在大型平台广告业务落地并取得显著业务提升。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="5-GCHR-Goal-Conditioned-Hindsight-Regularization-for-Sample-Efficient-Reinforcement-Learning"><a href="#5-GCHR-Goal-Conditioned-Hindsight-Regularization-for-Sample-Efficient-Reinforcement-Learning" class="headerlink" title="5. GCHR: Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/GCHR___Goal-Conditioned_Hindsight_Regularization_for_Sample-Efficient_Reinforcement_Learning.pdf">GCHR: Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Xi’an Jiaotong University</span></p><p>本文提出了一种高效的目标条件强化学习方法GCHR，通过结合Hindsight Self-Imitation Regularization（HSR）和Hindsight Goal Regularization（HGR），显著提升了稀疏奖励场景下的样本利用率和策略性能。GCHR无需额外网络或复杂规划，仅通过对历史轨迹的目标重标与行为克隆实现对策略的双重正则化，在理论和实验证明中均优于现有GCRL方法，能够更快收敛并达到更优策略。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="6-VQAThinker-Exploring-Generalizable-and-Explainable-Video-Quality-Assessment-via-Reinforcement-Learning"><a href="#6-VQAThinker-Exploring-Generalizable-and-Explainable-Video-Quality-Assessment-via-Reinforcement-Learning" class="headerlink" title="6. VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/VQAThinker__Exploring_Generalizable_and_Explainable_Video_Quality_Assessment_via_Reinforcement_Learn.pdf">VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Shanghai Jiao Tong University</span></p><p>VQAThinker提出了一种结合大规模多模态模型（LMM）与强化学习（基于GRPO算法）的推理型视频质量评价（VQA）框架，通过引入三种奖励函数（钟形回归奖励、对排序奖励与时序一致性奖励），实现了视频质量理解与评分的联合建模。实验表明，该方法在多项VQA基准测试（含OOD场景）中取得了领先的泛化能力和解释性，显著优于现有方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="7-Society-of-Mind-Meets-Real-Time-Strategy-A-Hierarchical-Multi-Agent-Framework-for-Strategic-Reasoning"><a href="#7-Society-of-Mind-Meets-Real-Time-Strategy-A-Hierarchical-Multi-Agent-Framework-for-Strategic-Reasoning" class="headerlink" title="7. Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Society_of_Mind_Meets_Real-Time_Strategy__A_Hierarchical_Multi-Agent_Framework_for_Strategic_Reasoni.pdf">Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Seoul National University</span></p><p>本文提出了HIMA架构，通过多模仿学习代理结合战略规划器（SP），利用专家演示数据进行单位组成聚类，生成结构化多步行动序列，并由SP以环境感知和时序链式推理方式融合各代理建议，适应实时战略游戏变化。结果显示，HIMA在StarCraft II九种种族组合对战中胜率和效率均优于现有主流方法，证明多代理模仿结合高层协调能提升复杂环境下的智能体表现。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="8-Multi-Armed-Bandits-Based-Optimization-of-Decision-Trees"><a href="#8-Multi-Armed-Bandits-Based-Optimization-of-Decision-Trees" class="headerlink" title="8. Multi-Armed Bandits-Based Optimization of Decision Trees"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Multi-Armed_Bandits-Based_Optimization_of_Decision_Trees.pdf">Multi-Armed Bandits-Based Optimization of Decision Trees</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Bangladesh University of Engineering and Technology</span></p><p>本论文提出了一种基于多臂赌博机（Multi-Armed Bandits, MAB）的决策树剪枝优化方法，将剪枝过程建模为探索-利用问题，采用UCB1、Thompson Sampling等MAB算法动态选择要剪枝的分支节点，并基于每次剪枝动作的反馈调整策略。实验证明，该方法在多个基准数据集上优于传统的代价复杂度剪枝（CCP），能提升决策树的泛化能力和预测性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="9-LinguaFluid-Language-Guided-Fluid-Control-via-Semantic-Rewards-in-Reinforcement-Learning"><a href="#9-LinguaFluid-Language-Guided-Fluid-Control-via-Semantic-Rewards-in-Reinforcement-Learning" class="headerlink" title="9. LinguaFluid: Language-Guided Fluid Control via Semantic Rewards in Reinforcement Learning"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LinguaFluid__Language_Guided_Fluid_Control_via_Semantic_Rewards_in_Reinforcement_Learning.pdf">LinguaFluid: Language-Guided Fluid Control via Semantic Rewards in Reinforcement Learning</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">College of Environmental and Resource Sciences, Zhejiang University</span></p><p>该论文提出了一种基于语义奖励的强化学习方法，用于实现流体控制等科学领域复杂任务。方法核心是利用SBERT生成当前状态和目标描述的语义嵌入，通过余弦相似度作为奖励信号，指导RL智能体学习，无需手工设计数值奖励函数。实验结果表明，该方法在无手工奖励函数下能够有效引导智能体实现与自然语言目标一致的控制行为，且语义空间中的奖励与物理空间指标高度相关。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="10-Mildly-Conservative-Regularized-Evaluation-for-Offline-Reinforcement-Learning"><a href="#10-Mildly-Conservative-Regularized-Evaluation-for-Offline-Reinforcement-Learning" class="headerlink" title="10. Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Mildly_Conservative_Regularized_Evaluation_for_Offline_Reinforcement_Learning.pdf">Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Central South University</span></p><p>本文提出了一种用于离线强化学习的温和保守正则化评估（MCRE）框架，将时序差分（TD）误差与行为克隆项结合进Bellman备份，以兼顾值估计的准确性和策略的保守性。基于MCRE，作者提出了MCRQ算法，并在D4RL基准的MuJoCo任务上验证了其优越性，理论上证明了收敛性及性能上界，实验显示MCRQ优于现有主流算法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="robot">Robotics</h2><h3 id="11-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation"><a href="#11-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation" class="headerlink" title="11. Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Shortcut_Learning_in_Generalist_Robot_Policies__The_Role_of_Dataset_Diversity_and_Fragmentation.pdf">Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">UESTC</span></p><p>本文系统分析了通用机器人策略在大规模多子集数据集（如OXE）训练下出现的shortcut learning现象，指出数据子集内部多样性不足和子集间分布碎片化是导致策略泛化能力受限的核心原因。通过理论建模、仿真和真实机器人实验，作者证明提升子集多样性、减少子集间分布差异，以及采用针对性的机器人数据增强（如视角&#x2F;对象增强）可显著缓解shortcut learning并提升泛化能力；结论为：优化数据收集与增强流程是提升通用机器人策略泛化性的关键途径。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="12-L2Calib-SE-3-Manifold-Reinforcement-Learning-for-Robust-Extrinsic-Calibration-with-Degenerate-Motion-Resilience"><a href="#12-L2Calib-SE-3-Manifold-Reinforcement-Learning-for-Robust-Extrinsic-Calibration-with-Degenerate-Motion-Resilience" class="headerlink" title="12. L2Calib: SE(3)-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/L2Calib__$SE(3)$-Manifold_Reinforcement_Learning_for_Robust_Extrinsic_Calibration_with_Degenerate_Mo.pdf">L2Calib: SE(3)-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Institute of Cyber-Systems and Control, Zhejiang University</span></p><p>本文提出了一种基于强化学习(SE(3)-流形建模)的传感器外参标定方法，将标定问题建模为决策过程，直接在SE(3)空间优化外参以提升里程计精度。方法采用Bingham分布建模旋转参数，结合轨迹对齐奖励机制和自动化数据筛选模块，实现无需结构化靶标即可在弱激励下进行多传感器联合标定。实验表明该方法在无人机、地面车和手持平台上超越传统优化方法，具备更高的鲁棒性和精度，且对初值不敏感，极大简化了机器人多传感器标定流程。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="13-SCAR-State-Space-Compression-for-AI-Driven-Resource-Management-in-6G-Enabled-Vehicular-Infotainment-Systems"><a href="#13-SCAR-State-Space-Compression-for-AI-Driven-Resource-Management-in-6G-Enabled-Vehicular-Infotainment-Systems" class="headerlink" title="13. SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SCAR__State-Space_Compression_for_AI-Driven_Resource_Management_in_6G-Enabled_Vehicular_Infotainment.pdf">SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Swiss Distance University of Applied Sciences</span></p><p>本文提出SCAR框架，通过SAST优化的K-means聚类与径向基函数网络（RBFN）实现边缘AI辅助的CQI数据压缩，随后利用压缩后的状态输入6G环境下的强化学习（RL）调度器进行车载娱乐资源分配。实验表明，SCAR在提升调度公平性和可扩展性方面优于传统方法，实现TTI可行区间提升14%，不公平调度时间下降15%，展现出高效性和公平性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="14-Computer-Vision-based-Adaptive-Control-for-Back-Exoskeleton-Performance-Optimization"><a href="#14-Computer-Vision-based-Adaptive-Control-for-Back-Exoskeleton-Performance-Optimization" class="headerlink" title="14. Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Computer_Vision-based_Adaptive_Control_for_Back_Exoskeleton_Performance_Optimization.pdf">Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Politecnico di Milano</span></p><p>本论文提出了结合计算机视觉的自适应控制系统，用于提升背部外骨骼在工业场景下的性能优化。方法上，作者首先构建了基于肌肉活动减少、用户不适感和偏好的多维优化空间，通过实验获得最优工作区域，并开发了基于YOLOv11和DINOv2的实时视觉管线，实现对搬运物体的自动检测、重量分类与辅助力度自适应调节。实验结果显示，所提系统在提升肌肉减负（最大减少达23%）、用户偏好和舒适度等方面均优于静态控制，且具备80%以上实时分类准确率。结论表明，智能自适应控制有望显著优化工业外骨骼的实际应用效果。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="15-Beyond-Constant-Parameters-Hyper-Prediction-Models-and-HyperMPC"><a href="#15-Beyond-Constant-Parameters-Hyper-Prediction-Models-and-HyperMPC" class="headerlink" title="15. Beyond Constant Parameters: Hyper Prediction Models and HyperMPC"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Beyond_Constant_Parameters__Hyper_Prediction_Models_and_HyperMPC.pdf">Beyond Constant Parameters: Hyper Prediction Models and HyperMPC</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Institute of Robotics and Machine Intelligence, Poznan University of Technology</span></p><p>本文提出了Hyper Prediction Model (HyperPM)和其在HyperMPC框架中的集成，实现了通过神经网络预测随时间变化的动力学模型参数轨迹，结合历史状态、控制输入与未来规划动作，提升了MPC在复杂动力学系统（如带有未建模扰动的无人机、摆和自动驾驶赛车）中的长期预测准确性和控制性能。实验结果表明，HyperPM在多个任务中长期预测误差降低超35%，HyperMPC在下游控制任务中相较现有技术取得了最高76%的性能提升，并保持低计算复杂度。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="16-Mitigating-Undesired-Conditions-in-Flexible-Production-with-Product–Process–Resource-Asset-Knowledge-Graphs"><a href="#16-Mitigating-Undesired-Conditions-in-Flexible-Production-with-Product–Process–Resource-Asset-Knowledge-Graphs" class="headerlink" title="16. Mitigating Undesired Conditions in Flexible Production with Product–Process–Resource Asset Knowledge Graphs"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Mitigating_Undesired_Conditions_in_Flexible_Production_with_Product-Process-Resource_Asset_Knowledge.pdf">Mitigating Undesired Conditions in Flexible Production with Product–Process–Resource Asset Knowledge Graphs</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague</span></p><p>该论文提出了一种基于语义知识图谱的产品–过程–资源资产知识图（PPR-AKG）方法，通过扩展传统PPR模型引入对工业生产中不良状态及其原因的建模，并采用OWL本体实现查询与推理。结合大型语言模型（LLM）实现人机交互，提升操作员和工程师对生产过程的理解与优化。实验验证在电池拆解与再制造场景下显著提升了灵活生产系统的问题诊断与性能表现。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="17-REBot-Reflexive-Evasion-Robot-for-Instantaneous-Dynamic-Obstacle-Avoidance"><a href="#17-REBot-Reflexive-Evasion-Robot-for-Instantaneous-Dynamic-Obstacle-Avoidance" class="headerlink" title="17. REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/REBot__Reflexive_Evasion_Robot_for_Instantaneous_Dynamic_Obstacle_Avoidance.pdf">REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">National University of Singapore</span></p><p>本文提出了REBot系统，通过有限状态机集成了基于强化学习训练的瞬时回避策略和恢复策略，实现四足机器人在动态障碍物靠近时的低延迟反射性规避。REBot经过仿真和真实机器人测试，在多种障碍场景下表现出更高的回避成功率、能耗更低及对高速障碍物的鲁棒性，验证了方法在提升机器人安全和机动性上的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="18-Affordance-R1-Reinforcement-Learning-for-Generalizable-Affordance-Reasoning-in-Multimodal-Large-Language-Model"><a href="#18-Affordance-R1-Reinforcement-Learning-for-Generalizable-Affordance-Reasoning-in-Multimodal-Large-Language-Model" class="headerlink" title="18. Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Affordance-R1__Reinforcement_Learning_for_Generalizable_Affordance_Reasoning_in_Multimodal_Large_Lan.pdf">Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">The Hong Kong University of Science and Technology (GZ)</span></p><p>Affordance-R1提出了一个强化学习框架，通过认知链式思考(CoT)引导的Group Relative Policy Optimization (GRPO)算法，提升多模态大语言模型(MLLM)的可泛化affordance推理能力。其方法设计了涵盖格式、感知和认知的奖励函数，并构建了高质量affordance推理数据集ReasonAff，实现了无需有监督微调即可在真实场景和域外数据上展现强大泛化和推理能力。结论表明，Affordance-R1在多项评测中显著优于现有方法，并为机器人感知与操作任务提供了更强的推理基础。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="19-Graph-based-Robot-Localization-Using-a-Graph-Neural-Network-with-a-Floor-Camera-and-a-Feature-Rich-Industrial-Floor"><a href="#19-Graph-based-Robot-Localization-Using-a-Graph-Neural-Network-with-a-Floor-Camera-and-a-Feature-Rich-Industrial-Floor" class="headerlink" title="19. Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Graph-based_Robot_Localization_Using_a_Graph_Neural_Network_with_a_Floor_Camera_and_a_Feature_Rich_I.pdf">Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">TU Dortmund University</span></p><p>该论文提出了一种创新的机器人室内定位方法，利用地面摄像头采集地板特征，通过将图像特征抽象为图结构并采用图卷积网络（GCN）进行特征嵌入与匹配，实现高效、可扩展的定位。该方法在工业地板环境下获得了高精度（误差仅0.64厘米）和高速度定位，并能有效解决‘被绑架机器人’问题，具有良好的通用性和扩展性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="20-GMF-Drive-Gated-Mamba-Fusion-with-Spatial-Aware-BEV-Representation-for-End-to-End-Autonomous-Driving"><a href="#20-GMF-Drive-Gated-Mamba-Fusion-with-Spatial-Aware-BEV-Representation-for-End-to-End-Autonomous-Driving" class="headerlink" title="20. GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/GMF-Drive__Gated_Mamba_Fusion_with_Spatial-Aware_BEV_Representation_for_End-to-End_Autonomous_Drivin.pdf">GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">University of Science and Technology of China</span></p><p>该论文提出GMF-Drive，一种面向端到端自动驾驶的多模态融合框架。方法包括采用几何增强的14维LiDAR柱状体表示来保留三维空间信息，并提出空间感知状态空间模型（BEV-SSM）实现高效线性复杂度的多层级融合，取代传统自注意力Transformer。实验表明，GMF-Drive在NAVSIM基准数据集上性能优于现有DiffusionDrive方法，实现了新的SOTA，并通过消融实验证实各创新组件的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="21-Bounding-Distributional-Shifts-in-World-Modeling-through-Novelty-Detection"><a href="#21-Bounding-Distributional-Shifts-in-World-Modeling-through-Novelty-Detection" class="headerlink" title="21. Bounding Distributional Shifts in World Modeling through Novelty Detection"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Bounding_Distributional_Shifts_in_World_Modeling_through_Novelty_Detection.pdf">Bounding Distributional Shifts in World Modeling through Novelty Detection</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Rutgers University</span></p><p>本文提出在视觉世界模型中引入基于变分自编码器（VAE）的新颖性检测机制，以解决模型推理时分布转移导致的性能下降问题。方法将VAE作为新颖性检测组件，度量预测状态与训练分布的偏离程度，并将此重构损失作为模型预测控制（MPC）中每步动作的代价项，集成至DINO-WM架构进行机器人操作任务测试。实验表明，该方法有效提升了规划算法的数据效率和鲁棒性，显著优于现有世界模型方法。结论认为新颖性检测能缓解世界模型不完美带来的推理偏差，但会牺牲对未见状态的探索。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="22-ME3-BEV-Mamba-Enhanced-Deep-Reinforcement-Learning-for-End-to-End-Autonomous-Driving-with-BEV-Perception"><a href="#22-ME3-BEV-Mamba-Enhanced-Deep-Reinforcement-Learning-for-End-to-End-Autonomous-Driving-with-BEV-Perception" class="headerlink" title="22. ME3-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ME$%5E3$-BEV__Mamba-Enhanced_Deep_Reinforcement_Learning_for_End-to-End_Autonomous_Driving_with_BEV-Pe.pdf">ME3-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Tsinghua University</span></p><p>本文提出了ME3-BEV框架，将鸟瞰视角（BEV）空间感知与Mamba时序建模模块结合，采用深度强化学习（DRL）实现端到端自动驾驶决策。方法包括SSA模块提升空间理解、TAFM模块增强时序特征提取，并通过语义分割提升可解释性。实验结果表明，ME3-BEV在CARLA多场景下显著降低碰撞率、提升轨迹准确性和整体驾驶评分，优于主流基线，具备实时性和安全性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="23-ReNiL-Relative-Neural-Inertial-Locator-with-Any-Scale-Bayesian-Inference"><a href="#23-ReNiL-Relative-Neural-Inertial-Locator-with-Any-Scale-Bayesian-Inference" class="headerlink" title="23. ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ReNiL__Relative_Neural_Inertial_Locator_with_Any-Scale_Bayesian_Inference.pdf">ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Wuhan University</span></p><p>本文提出了ReNiL，一个基于深度学习和贝叶斯推断的惯性定位系统，通过引入惯性定位需求点（IPDPs）和任意尺度Laplace估计器（ASLE），实现对任意尺度IMU序列的高效、灵活和不确定性一致的位移估计。实验结果表明，ReNiL在多数据集上具有领先的定位精度与不确定性估计能力，优于现有主流方法，并显著降低计算开销。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="24-Hand-by-Hand-LLM-Driving-EMS-Assistant-for-Operational-Skill-Learning"><a href="#24-Hand-by-Hand-LLM-Driving-EMS-Assistant-for-Operational-Skill-Learning" class="headerlink" title="24. Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Hand_by_Hand__LLM_Driving_EMS_Assistant_for_Operational_Skill_Learning.pdf">Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Zhejiang University</span></p><p>本文提出了FlightAxis原型系统，将大语言模型（LLM）与电刺激（EMS）结合，通过“Align-Analyze-Adjust”流程支持和引导用户飞行技能训练。实验结果表明，系统能有效提升学员操作表现和高度保持能力，改善认知和学习体验，用户对LLM驱动的体感辅助表现出较高接受度，尤其适合新手技能训练。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="25-Dynamical-Trajectory-Planning-of-Disturbance-Consciousness-for-Air-Land-Bimodal-Unmanned-Aerial-Vehicles"><a href="#25-Dynamical-Trajectory-Planning-of-Disturbance-Consciousness-for-Air-Land-Bimodal-Unmanned-Aerial-Vehicles" class="headerlink" title="25. Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Dynamical_Trajectory_Planning_of_Disturbance_Consciousness_for_Air-Land_Bimodal_Unmanned_Aerial_Vehi.pdf">Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Guangdong Laboratory of Artificial Intelligence and Digital Economy (Shenzhen)</span></p><p>本文提出了一种面向空地双模无人车的扰动自适应轨迹规划框架，将实时扰动估计纳入路径搜索与轨迹优化流程，并通过扰动自适应安全边界调整机制动态修正车辆可行的动力边界，保障轨迹可行性。实验结果表明，该方法在真实环境与基准测试中显著提升了跟踪精度、任务效率及能耗表现，增强了系统在复杂扰动场景下的适应性和鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="26-Affordance-Guided-Dual-Armed-Disassembly-Teleoperation-for-Mating-Parts"><a href="#26-Affordance-Guided-Dual-Armed-Disassembly-Teleoperation-for-Mating-Parts" class="headerlink" title="26. Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Affordance-Guided_Dual-Armed_Disassembly_Teleoperation_for_Mating_Parts.pdf">Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Osaka University</span></p><p>本论文提出了一种基于可供性引导的双臂遥操作拆解系统，结合物体几何信息可视化抓取和拆卸方向，通过混合位置-阻抗控制实现顺应性拆解。实验表明，该系统提升了大型家电部件无损拆卸的成功率并降低了目标物体的姿态偏差，验证了混合控制和双臂策略在拆卸任务中的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="27-Latent-Policy-Barrier-Learning-Robust-Visuomotor-Policies-by-Staying-In-Distribution"><a href="#27-Latent-Policy-Barrier-Learning-Robust-Visuomotor-Policies-by-Staying-In-Distribution" class="headerlink" title="27. Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Latent_Policy_Barrier__Learning_Robust_Visuomotor_Policies_by_Staying_In-Distribution.pdf">Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Stanford University</span></p><p>本文提出Latent Policy Barrier (LPB) 框架，通过引入基于视觉编码的隐空间障碍，将专家演示的分布视为安全边界，利用扩散模型学习基础策略，仅使用专家数据确保模仿精度，并结合在专家和回滚数据上训练的视觉隐空间动力学模型，在推理时对行动进行梯度引导，动态修正偏离专家分布的状态。实验证明，LPB相比以往方法在仿真及真实机器人操作任务中，能够显著提升策略鲁棒性和样本效率，减少人工干预，实现从有限专家数据中可靠的操作控制。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="cv">Computer Vision</h2><h3 id="28-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion"><a href="#28-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion" class="headerlink" title="28. WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/WGAST__Weakly-Supervised_Generative_Network_for_Daily_10_m_Land_Surface_Temperature_Estimation_via_S.pdf">WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">INSA Centre Val de Loire</span></p><p>本文提出WGAST，一种基于条件生成对抗网络（cGAN）的弱监督生成模型，实现Terra MODIS、Landsat 8和Sentinel-2多源遥感卫星数据的时空融合，生成每日10米分辨率地表温度（LST）。方法包括多尺度特征提取、余弦相似性特征融合、自适应归一化、时序注意力机制及高频噪声抑制，通过物理平均原则弱监督训练。实验表明，WGAST在定量和定性评测中显著优于现有方法，平均RMSE降低17.18%，SSIM提升4.10%，可有效重建高分辨率LST并克服云层遮挡问题。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="29-Text-Embedded-Swin-UMamba-for-DeepLesion-Segmentation"><a href="#29-Text-Embedded-Swin-UMamba-for-DeepLesion-Segmentation" class="headerlink" title="29. Text Embedded Swin-UMamba for DeepLesion Segmentation"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Text_Embedded_Swin-UMamba_for_DeepLesion_Segmentation.pdf">Text Embedded Swin-UMamba for DeepLesion Segmentation</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Scientific Application Services, Center of Information Technology, NIH</span></p><p>本文提出了Text-Swin-UMamba模型，在Swin-UMamba分割骨干的多尺度解码器中集成Text Tower语言嵌入机制，将放射学报告中的短文本描述编码后与影像特征多层融合，实现对CT图像中病灶的精准分割。实验表明，该方法在DeepLesion数据集上Dice分数达到82%，在所有比对模型中表现最佳，融合文本和影像特征能显著提升分割性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="30-TRUST-Leveraging-Text-Robustness-for-Unsupervised-Domain-Adaptation"><a href="#30-TRUST-Leveraging-Text-Robustness-for-Unsupervised-Domain-Adaptation" class="headerlink" title="30. TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/TRUST__Leveraging_Text_Robustness_for_Unsupervised_Domain_Adaptation.pdf">TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">University of Catania</span></p><p>本文提出了TRUST方法，通过利用文本的鲁棒性改进图像分类中的无监督领域自适应。其方法包括：用语言模型从图像对应的文本描述生成目标域伪标签，并通过CLIP模型衡量图片与文本的语义相似性，估计伪标签的不确定性，用于加权分类损失，降低低质量文本带来的负面影响。此外，提出了一种多模态软对比学习损失，通过文本指导视觉模型的特征空间对齐，提升模型在复杂领域迁移下的泛化能力。实验结果表明，在DomainNet和GeoNet等经典与复杂领域迁移任务上，TRUST均优于现有方法，取得新的SOTA表现。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="31-CLIPin-A-Non-contrastive-Plug-in-to-CLIP-for-Multimodal-Semantic-Alignment"><a href="#31-CLIPin-A-Non-contrastive-Plug-in-to-CLIP-for-Multimodal-Semantic-Alignment" class="headerlink" title="31. CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/CLIPin__A_Non-contrastive_Plug-in_to_CLIP_for_Multimodal_Semantic_Alignment.pdf">CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Beijing Institute of Technology</span></p><p>该论文提出CLIPin，一种可无缝集成到CLIP架构中的非对比学习插件，通过引入在线-目标分支对图像和文本模态进行交互监督，并设计共享预投影器，有效整合对比和非对比学习以提升多模态语义对齐和特征表达。实验结果表明，CLIPin显著提升了自然图像和医学图像文本任务中的表现和泛化能力，支持多种对比学习框架，具备良好的可插拔性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="32-FVGen-Accelerating-Novel-View-Synthesis-with-Adversarial-Video-Diffusion-Distillation"><a href="#32-FVGen-Accelerating-Novel-View-Synthesis-with-Adversarial-Video-Diffusion-Distillation" class="headerlink" title="32. FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/FVGen__Accelerating_Novel-View_Synthesis_with_Adversarial_Video_Diffusion_Distillation.pdf">FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Institute for Creative Technologies</span></p><p>FVGen提出了一种加速3D场景重建中新视角生成的方法，采用视频扩散模型，通过引入GAN初始化和软化反向KL散度的分布匹配蒸馏，将多步降噪教师模型高效地压缩为仅需4步的学生模型。实验结果显示，该方法在视觉质量不损失甚至提升的情况下，将生成速度提升了90%以上，大幅提高了稀疏视角输入场景的重建效率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="33-Text-as-Any-Modality-for-Zero-Shot-Classification-by-Consistent-Prompt-Tuning"><a href="#33-Text-as-Any-Modality-for-Zero-Shot-Classification-by-Consistent-Prompt-Tuning" class="headerlink" title="33. Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Text_as_Any-Modality_for_Zero-Shot_Classification_by_Consistent_Prompt_Tuning.pdf">Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Nanjing University of Science and Technology</span></p><p>本文提出了TaAM-CPT方法，实现了通过一致的提示调优（Prompt Tuning）构建通用多模态表征模型，仅利用LLMs生成的文本数据，无需任何模态特定标注数据。该方法包含模态提示池、文本构造和模态对齐文本编码器，并通过设计模态内和模态间学习目标实现多模态类别无冲突扩展与高效特征一致性。实验结果表明，TaAM-CPT在视频、图像和音频等多模态零样本分类任务上取得了领先性能，验证了其通用性和可扩展性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="34-Text-guided-Visual-Prompt-DINO-for-Generic-Segmentation"><a href="#34-Text-guided-Visual-Prompt-DINO-for-Generic-Segmentation" class="headerlink" title="34. Text-guided Visual Prompt DINO for Generic Segmentation"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Text-guided_Visual_Prompt_DINO_for_Generic_Segmentation.pdf">Text-guided Visual Prompt DINO for Generic Segmentation</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Tsinghua Shenzhen International Graduate School, Tsinghua University</span></p><p>该论文提出Prompt-DINO，一种融合文本和视觉提示的新型通用分割模型，通过在编码初期实现早期多模态融合、引入视觉-文本顺序对齐的查询选择机制，以及开发RAP数据引擎生成大规模高质量训练样本，实现跨模态语义消歧与泛化提升。实验表明Prompt-DINO在开放世界分割和检测任务上取得了最新最优性能，显著扩展了模型的语义覆盖和适应能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="35-Street-View-Sociability-Interpretable-Analysis-of-Urban-Social-Behavior-Across-15-Cities"><a href="#35-Street-View-Sociability-Interpretable-Analysis-of-Urban-Social-Behavior-Across-15-Cities" class="headerlink" title="35. Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Street_View_Sociability__Interpretable_Analysis_of_Urban_Social_Behavior_Across_15_Cities.pdf">Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Carnegie Mellon University</span></p><p>本论文提出利用街景图像（SVI）结合多模态大语言模型（MLLM），通过Mehta的社会行为分类法自动分析15座城市的街道社会互动类型，并与城市归属感、绿化和天空可见度等环境变量进行线性回归关联分析。结果显示，SVI中的社会行为类型与城市环境特征和归属感有显著相关性，验证了街景图像作为可解释、可扩展的社会行为研究工具的潜力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="36-Uncertainty-quantified-Rollout-Policy-Adaptation-for-Unlabelled-Cross-domain-Temporal-Grounding"><a href="#36-Uncertainty-quantified-Rollout-Policy-Adaptation-for-Unlabelled-Cross-domain-Temporal-Grounding" class="headerlink" title="36. Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Uncertainty-quantified_Rollout_Policy_Adaptation_for_Unlabelled_Cross-domain_Temporal_Grounding.pdf">Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Queen Mary University of London</span></p><p>本文提出了一种面向视频时序定位任务的数据高效无标注跨域自适应方法。核心方法为基于GRPO（Group Relative Policy Optimization）强化学习的Uncertainty-quantified Rollout Policy Adaptation（URPA），通过在无标注目标域视频上多次采样生成伪标签，并利用其方差估算不确定性，将置信权重引入奖励函数，指导模型关注可靠伪监督信号，实现仅需少量无标注目标域视频即可高效自适应。实验结果表明，该方法在六个跨域基准上超越或媲美全量数据的主流方法，有效提升了跨域时序定位的泛化性和实用性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="37-Advanced-Deep-Learning-Techniques-for-Accurate-Lung-Cancer-Detection-and-Classification"><a href="#37-Advanced-Deep-Learning-Techniques-for-Accurate-Lung-Cancer-Detection-and-Classification" class="headerlink" title="37. Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Advanced_Deep_Learning_Techniques_for_Accurate_Lung_Cancer_Detection_and_Classification.pdf">Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">University of Vigo</span></p><p>该论文提出了一种基于DenseNet201的深度学习方法，结合Focal Loss、数据增强和正则化等技术，有效解决了肺癌CT影像分类中的数据不平衡和过拟合问题。实验结果显示，DenseNet201模型在多种肺癌亚型分类上取得了98.95%的高准确率，优于主流模型，具备临床实际应用潜力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="38-Depth-Jitter-Seeing-through-the-Depth"><a href="#38-Depth-Jitter-Seeing-through-the-Depth" class="headerlink" title="38. Depth Jitter: Seeing through the Depth"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Depth_Jitter__Seeing_through_the_Depth.pdf">Depth Jitter: Seeing through the Depth</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Université de Toulon</span></p><p>本文提出了一种新颖的深度感知数据增强方法Depth-Jitter，通过利用水下图像形成模型和单目深度估计，对高深度方差图像引入自适应或固定深度偏移，生成具有物理意义的深度扰动样本，从而增强模型在多标签水下图像分类中的鲁棒性和泛化能力。实验在UTDAC2020和FathomNet两个基准数据集上，Depth-Jitter在mAP、mAP@20和ROC AUC等指标上均优于传统增强方法，尤其在深度敏感环境下提升显著，验证了其在真实水下视觉任务中的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="39-TEFormer-Texture-Aware-and-Edge-Guided-Transformer-for-Semantic-Segmentation-of-Urban-Remote-Sensing-Images"><a href="#39-TEFormer-Texture-Aware-and-Edge-Guided-Transformer-for-Semantic-Segmentation-of-Urban-Remote-Sensing-Images" class="headerlink" title="39. TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/TEFormer__Texture-Aware_and_Edge-Guided_Transformer_for_Semantic_Segmentation_of_Urban_Remote_Sensin.pdf">TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Beijing University of Technology</span></p><p>该论文提出TEFormer，一种融合纹理感知模块（TaM）和边缘引导三分支解码器（Eg3Head）的Transformer架构，用于城市遥感图像语义分割。方法通过TaM增强细粒度纹理特征感知，Eg3Head和边缘引导特征融合模块（EgFFM）实现多尺度、细节和边缘特征的有效融合，提升类别区分和边界精度。实验表明TEFormer在Potsdam、Vaihingen和LoveDA数据集上取得了领先的分割性能，边界和细节表现尤为突出。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="40-MA-CBP-A-Criminal-Behavior-Prediction-Framework-Based-on-Multi-Agent-Asynchronous-Collaboration"><a href="#40-MA-CBP-A-Criminal-Behavior-Prediction-Framework-Based-on-Multi-Agent-Asynchronous-Collaboration" class="headerlink" title="40. MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/MA-CBP__A_Criminal_Behavior_Prediction_Framework_Based_on_Multi-Agent_Asynchronous_Collaboration.pdf">MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">School of Mechatronical Engineering, Beijing Institute of Technology</span></p><p>本文提出了MA-CBP框架，通过多智能体异步协作，将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，并融合邻近图像帧，实现对短期与长期行为上下文的联合推理。实验结果显示，该方法在犯罪行为早期预警与检测任务上优于现有生成式和异常检测模型，有效提升了城市公共安全场景下的风险预警能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="41-Can-Diffusion-Models-Bridge-the-Domain-Gap-in-Cardiac-MR-Imaging"><a href="#41-Can-Diffusion-Models-Bridge-the-Domain-Gap-in-Cardiac-MR-Imaging" class="headerlink" title="41. Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Can_Diffusion_Models_Bridge_the_Domain_Gap_in_Cardiac_MR_Imaging_.pdf">Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Centre for Doctoral Training in AI for Medical Diagnosis and Care, School of Computing, University of Leeds</span></p><p>本论文提出了一种基于源域训练扩散模型（SD-DM）的方法，通过参考图像指导扩散过程，生成结构一致且分布对齐的合成心脏MR图像，用于数据稀缺、多中心域下的医学影像分割。方法支持测试时输入域适配和训练-测试混合适配，实现领域泛化和领域自适应，无需迁移学习或在线训练，显著提升未见域分割性能，减少对人工标注和多中心采集的依赖。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="42-Deepfake-Detection-that-Generalizes-Across-Benchmarks"><a href="#42-Deepfake-Detection-that-Generalizes-Across-Benchmarks" class="headerlink" title="42. Deepfake Detection that Generalizes Across Benchmarks"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Deepfake_Detection_that_Generalizes_Across_Benchmarks.pdf">Deepfake Detection that Generalizes Across Benchmarks</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Czech Technical University in Prague</span></p><p>本文提出LNCLIP-DF方法，通过对预训练CLIP视觉编码器仅微调Layer Normalization参数、L2归一化特征、引入slerp特征空间增强及统一-对齐损失，实现了深度伪造检测的高效泛化。大量跨13个基准数据集的实验显示，该方法在大多数基准上优于复杂模型，且强调了使用真实-伪造配对训练集对提升泛化和防止捷径学习的重要性。结论指出，精心设计的数据配对和训练集多样性是构建强泛化性深度伪造检测器的关键。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="43-InfoCausalQA-Can-Models-Perform-Non-explicit-Causal-Reasoning-Based-on-Infographic"><a href="#43-InfoCausalQA-Can-Models-Perform-Non-explicit-Causal-Reasoning-Based-on-Infographic" class="headerlink" title="43. InfoCausalQA: Can Models Perform Non-explicit Causal Reasoning Based on Infographic?"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/InfoCausalQA_Can_Models_Perform_Non-explicit_Causal_Reasoning_Based_on_Infographic_.pdf">InfoCausalQA: Can Models Perform Non-explicit Causal Reasoning Based on Infographic?</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Yonsei University</span></p><p>该论文提出InfoCausalQA，这是首个用于评估视觉语言模型（VLMs）在信息图基础上进行因果推理能力的基准，包括基于数字趋势的定量因果推理和涉及五类因果关系的语义因果推理。实验结果显示，现有VLMs在基于信息图的因果推理任务上表现显著低于人类，凸显多模态模型在深层次因果推理上的不足。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="44-Clinically-guided-Data-Synthesis-for-Laryngeal-Lesion-Detection"><a href="#44-Clinically-guided-Data-Synthesis-for-Laryngeal-Lesion-Detection" class="headerlink" title="44. Clinically-guided Data Synthesis for Laryngeal Lesion Detection"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Clinically-guided_Data_Synthesis_for_Laryngeal_Lesion_Detection.pdf">Clinically-guided Data Synthesis for Laryngeal Lesion Detection</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Istituto Italiano di Tecnologia</span></p><p>本文提出了一种结合Latent Diffusion Model (LDM) 与 ControlNet 的临床引导合成方法，用于生成喉部内窥镜图像-注释对，通过临床观察进行条件控制以提升图像的多样性与真实性。实验表明，加入不确定性策略筛选的10%合成数据可使喉部病变检测精度在内部数据集提升9%，外部数据集提升22.1%，且专家难以区分真实与合成图像，验证了合成样本的临床应用潜力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="45-Fewer-Denoising-Steps-or-Cheaper-Per-Step-Inference-Towards-Compute-Optimal-Diffusion-Model-Deployment"><a href="#45-Fewer-Denoising-Steps-or-Cheaper-Per-Step-Inference-Towards-Compute-Optimal-Diffusion-Model-Deployment" class="headerlink" title="45. Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Fewer_Denoising_Steps_or_Cheaper_Per-Step_Inference__Towards_Compute-Optimal_Diffusion_Model_Deploym.pdf">Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Georgia Institute of Technology</span></p><p>本文提出PostDiff，一种训练后无需微调即可用于加速扩散模型的训练自由压缩框架，分别在输入层（通过混合分辨率去噪策略）和模块层（通过混合模块缓存策略）减少推理冗余，提升效率。实验表明，相比减少去噪步数，降低每步推理成本通常能在保持高生成保真度的同时获得更优的效率-保真权衡。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="46-Improving-Diagnostic-Accuracy-for-Oral-Cancer-with-inpainting-Synthesis-Lesions-Generated-Using-Diffusion-Models"><a href="#46-Improving-Diagnostic-Accuracy-for-Oral-Cancer-with-inpainting-Synthesis-Lesions-Generated-Using-Diffusion-Models" class="headerlink" title="46. Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Improving_Diagnostic_Accuracy_for_Oral_Cancer_with_inpainting_Synthesis_Lesions_Generated_Using_Diff.pdf">Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Hongik University</span></p><p>本论文提出利用细调Stable Diffusion扩散模型结合Segment Anything Model (SAM) 分割与inpainting技术，生成高保真口腔癌病变合成图像，显著丰富了训练数据集多样性。通过将这些合成图像用于诊断模型训练，ResNet-50分类模型准确率提升至0.97，YOLO v8检测模型精度提升至0.85，验证了合成数据在医疗影像AI诊断中的有效性和可推广性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="47-DiffCap-Diffusion-based-Real-time-Human-Motion-Capture-using-Sparse-IMUs-and-a-Monocular-Camera"><a href="#47-DiffCap-Diffusion-based-Real-time-Human-Motion-Capture-using-Sparse-IMUs-and-a-Monocular-Camera" class="headerlink" title="47. DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/DiffCap__Diffusion-based_Real-time_Human_Motion_Capture_using_Sparse_IMUs_and_a_Monocular_Camera.pdf">DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Tsinghua University</span></p><p>该论文提出DiffCap方法，首次将扩散模型应用于稀疏IMU与单目摄像头信号融合，实现实时高精度人体运动捕捉。方法核心在于将视觉信号序列编码为全局条件嵌入，IMU信号逐帧输入，采用两阶段扩散模型先预测3D关节点再回归姿态，显著提升了在遮挡和视野外等复杂场景下的鲁棒性和准确率。实验证明DiffCap在多个公开数据集上超越现有最优方法，尤其在遮挡、动态及野外场景下表现优越。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="48-SC-Captioner-Improving-Image-Captioning-with-Self-Correction-by-Reinforcement-Learning"><a href="#48-SC-Captioner-Improving-Image-Captioning-with-Self-Correction-by-Reinforcement-Learning" class="headerlink" title="48. SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SC-Captioner__Improving_Image_Captioning_with_Self-Correction_by_Reinforcement_Learning.pdf">SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Fudan University</span></p><p>本文提出SC-Captioner，一种针对图像描述任务的自我修正训练框架，通过多轮强化学习及新颖的奖励函数设计（结合对象、属性、关系的场景图解析）引导大型视觉语言模型在修正初始描述时精准移除幻觉、补全遗漏细节。实验表明，该方法在多个指标和场景下均优于直接偏好优化等主流方法，显著提升了图像描述质量和可靠性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="49-Ensemble-Based-Graph-Representation-of-fMRI-Data-for-Cognitive-Brain-State-Classification"><a href="#49-Ensemble-Based-Graph-Representation-of-fMRI-Data-for-Cognitive-Brain-State-Classification" class="headerlink" title="49. Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Ensemble-Based_Graph_Representation_of_fMRI_Data_for_Cognitive_Brain_State_Classification.pdf">Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Institute for Cognitive Neuroscience of University Higher School of Economics</span></p><p>该论文提出了一种基于集成学习的图表示方法，将fMRI数据转化为功能连接网络，并利用多个基础机器学习模型计算边权，实现二元脑状态分类。方法在七种认知任务上通过简单逻辑回归和图神经网络进行验证，集成图表现出更高的分类准确率（97.07%至99.74%），显著优于传统相关性图。结论表明集成图能更好地捕捉脑网络拓扑信息，提高脑状态判别力，且方法具有良好的可扩展性和可解释性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="50-Mask-Match-Learning-to-Recognize-Handwritten-Math-with-Self-Supervised-Attention"><a href="#50-Mask-Match-Learning-to-Recognize-Handwritten-Math-with-Self-Supervised-Attention" class="headerlink" title="50. Mask &amp; Match: Learning to Recognize Handwritten Math with Self-Supervised Attention"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Mask_&_Match__Learning_to_Recognize_Handwritten_Math_with_Self-Supervised_Attention.pdf">Mask &amp; Match: Learning to Recognize Handwritten Math with Self-Supervised Attention</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">International Institute of Information Technology Hyderabad</span></p><p>本文提出了一种针对手写数学表达式识别（HMER）的自监督学习框架，通过结合全局和局部对比损失进行图像编码器预训练，并引入渐进式空间掩码训练的自监督注意力机制，提升模型对复杂结构的理解能力。实验结果显示，该方法在CROHME基准上优于现有的自监督和全监督方法，渐进式掩码注意力机制显著增强了模型的结构鲁棒性和识别准确率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="51-UGD-IML-A-Unified-Generative-Diffusion-based-Framework-for-Constrained-and-Unconstrained-Image-Manipulation-Localization"><a href="#51-UGD-IML-A-Unified-Generative-Diffusion-based-Framework-for-Constrained-and-Unconstrained-Image-Manipulation-Localization" class="headerlink" title="51. UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/UGD-IML__A_Unified_Generative_Diffusion-based_Framework_for_Constrained_and_Unconstrained_Image_Mani.pdf">UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Harbin Institute of Technology</span></p><p>该论文提出了UGD-IML框架，基于生成扩散模型，首次统一了解析和约束两种图像篡改定位任务，通过条件控制模块、噪声添加模块和去噪模块实现输入可控任务切换，并采用参数共享和类别嵌入机制提升效率。实验证明，UGD-IML在多数据集上对现有方法有显著性能提升，具备更高的准确性、鲁棒性和不确定性评估能力，有效支持真实场景下的图像真伪检测。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="52-Transformer-Based-Explainable-Deep-Learning-for-Breast-Cancer-Detection-in-Mammography-The-MammoFormer-Framework"><a href="#52-Transformer-Based-Explainable-Deep-Learning-for-Breast-Cancer-Detection-in-Mammography-The-MammoFormer-Framework" class="headerlink" title="52. Transformer-Based Explainable Deep Learning for Breast Cancer Detection in Mammography: The MammoFormer Framework"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Transformer-Based_Explainable_Deep_Learning_for_Breast_Cancer_Detection_in_Mammography__The_MammoFor.pdf">Transformer-Based Explainable Deep Learning for Breast Cancer Detection in Mammography: The MammoFormer Framework</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Morgan State University</span></p><p>该论文提出了MammoFormer框架，将多种Transformer架构（如ViT、Swin Transformer）与CNN、ConvNeXt等模型及多特征增强（原图、负片、AHE、HOG）联合，用于乳腺X光图像的癌症检测，并系统集成了Integrated Gradients、GradCAM等五种可解释性AI（XAI）技术。实验结果显示，通过针对架构优化的特征增强，Transformer模型在HOG等预处理下可达98.4%准确率，性能与CNN相当且具备更强的全局上下文建模和多角度可解释性，为乳腺AI筛查提供高精度和透明的诊断支持。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="53-SAM-Encoder-Breach-by-Adversarial-Simplicial-Complex-Triggers-Downstream-Model-Failures"><a href="#53-SAM-Encoder-Breach-by-Adversarial-Simplicial-Complex-Triggers-Downstream-Model-Failures" class="headerlink" title="53. SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SAM_Encoder_Breach_by_Adversarial_Simplicial_Complex_Triggers_Downstream_Model_Failures.pdf">SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Beijing Jiaotong University</span></p><p>本文提出了一种针对Segment Anything Model (SAM)视觉基础模型的高效可迁移对抗攻击方法——Vertex-Refining Simplicial Complex Attack (VeSCA)。该方法仅依赖SAM的编码器，通过参数化单纯形复合体显式建模并精炼与下游模型共享的脆弱子空间，结合轻量级领域再适应策略，实现了对多类下游任务模型的高效攻击。实验结果显示，VeSCA在五个视觉分割相关的数据集和多种下游任务上，攻击性能比当前最优方法提升12.7%，验证了SAM脆弱性对其下游生态的系统性风险。结论是，VeSCA可系统性暴露视觉基础模型及其下游的安全风险，提示需加强基础模型鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="54-Learning-Representations-of-Satellite-Images-with-Evaluations-on-Synoptic-Weather-Events"><a href="#54-Learning-Representations-of-Satellite-Images-with-Evaluations-on-Synoptic-Weather-Events" class="headerlink" title="54. Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Learning_Representations_of_Satellite_Images_with_Evaluations_on_Synoptic_Weather_Events.pdf">Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">National Taiwan University</span></p><p>本文通过对卫星影像应用主成分分析（PCA）、卷积自编码器（CAE）以及预训练残差网络（ResNet50）等表征学习算法，结合可解释分类方法（如逻辑回归），实现多种天气事件的自动识别。实验结果表明CAE在所有分类任务上威胁评分最高，并且高分辨率卫星数据能显著提升其表现；相比之下，PCA有较高的识别率但误报率也更高。结论：CAE是一种高效且表现优异的卫星图像表征学习方法，尤其适合高分辨率数据，但物理可解释性仍需改进。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="55-SynSeg-Feature-Synergy-for-Multi-Category-Contrastive-Learning-in-Open-Vocabulary-Semantic-Segmentation"><a href="#55-SynSeg-Feature-Synergy-for-Multi-Category-Contrastive-Learning-in-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="55. SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SynSeg__Feature_Synergy_for_Multi-Category_Contrastive_Learning_in_Open-Vocabulary_Semantic_Segmenta.pdf">SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Global Innovation Exchange</span></p><p>该论文提出SynSeg方法，通过Feature Synergy Structure（FSS）与Multi-Category Contrastive Learning（MCCL）结合，实现开放词汇语义分割的弱监督训练。FSS融合视觉和语义激活图生成判别特征，MCCL则同时强化类内和类间对比学习，有效提升分割性能。实验结果表明，SynSeg在五个主流数据集上显著超越现有弱监督方法，实现了最佳mIoU表现。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="56-MCA-2D-3D-Retrieval-with-Noisy-Labels-via-Multi-level-Adaptive-Correction-and-Alignment"><a href="#56-MCA-2D-3D-Retrieval-with-Noisy-Labels-via-Multi-level-Adaptive-Correction-and-Alignment" class="headerlink" title="56. MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/MCA__2D-3D_Retrieval_with_Noisy_Labels_via_Multi-level_Adaptive_Correction_and_Alignment.pdf">MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Shanghai Jiao Tong University</span></p><p>本文提出了一种面向2D-3D跨模态检索的多层次自适应纠正与对齐框架（MCA），包括多模态联合标签纠正（MJC）机制和多层次自适应对齐（MAA）策略。MJC利用多模态历史自预测实现标签纠正，MAA则在中心、组和实例三个层次自适应对齐特征，显著提升了在噪声标签下的跨模态检索准确性和鲁棒性。实验表明，该方法在合成和真实噪声3D数据集上均实现了最优性能，有效缓解了标签噪声带来的影响。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="57-E-React-Towards-Emotionally-Controlled-Synthesis-of-Human-Reactions"><a href="#57-E-React-Towards-Emotionally-Controlled-Synthesis-of-Human-Reactions" class="headerlink" title="57. E-React: Towards Emotionally Controlled Synthesis of Human Reactions"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/E-React__Towards_Emotionally_Controlled_Synthesis_of_Human_Reactions.pdf">E-React: Towards Emotionally Controlled Synthesis of Human Reactions</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Southeast University, China</span></p><p>该论文提出了E-React框架，实现了基于半监督情感先验的情感驱动反应动作生成。方法包括用少量标注数据和大量未标注数据训练情感先验，通过对称的actor-reactor扩散模型结合空间关系和情感嵌入生成自然多样的人体反应动作。实验表明，本方法在情感一致性与交互自然性上优于现有方法，提升了多人物互动动作生成的真实感与多样性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="58-Towards-MR-Based-Trochleoplasty-Planning"><a href="#58-Towards-MR-Based-Trochleoplasty-Planning" class="headerlink" title="58. Towards MR-Based Trochleoplasty Planning"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Towards_MR-Based_Trochleoplasty_Planning.pdf">Towards MR-Based Trochleoplasty Planning</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">University of Basel</span></p><p>该论文提出了一套基于MR影像的三维髁沟成形术（trochleoplasty）手术规划管线。方法包括利用隐式神经表示（INR）实现MR图像超分辨率重建，结合自定义多标签分割网络对膝关节结构进行分割，并采用小波扩散模型（WDM）对病变区域进行伪健康形态生成。实验表明，该方法在25例患者中显著改善了髁沟角和深度，为术前和术中提供了高分辨率、个性化的三维手术蓝图，且无需CT，降低了辐射风险。结论：该管线提升了手术一致性与安全性，有望推广到更多外科应用。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="59-AGI-for-the-Earth-the-path-possibilities-and-how-to-evaluate-intelligence-of-models-that-work-with-Earth-Observation-Data"><a href="#59-AGI-for-the-Earth-the-path-possibilities-and-how-to-evaluate-intelligence-of-models-that-work-with-Earth-Observation-Data" class="headerlink" title="59. AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/AGI_for_the_Earth,_the_path,_possibilities_and_how_to_evaluate_intelligence_of_models_that_work_with.pdf">AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">HUM.AI</span></p><p>本文系统梳理了地球观测（EO）数据对实现人工通用智能（AGI）的重要性，并分析了现有EO基准测试的不足。作者提出了一套涵盖多种卫星影像任务的分类框架，强调应建立一个覆盖场景分类、变化检测、场景生成及反事实分析等多任务、跨模态的综合性基准，以全面评估和推动AGI在地球观测领域的能力。结论指出现有基准难以充分衡量AGI模型泛化与推理能力，呼吁社区协作完善多任务基准，助力环境与可持续发展挑战应对。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="60-Learning-3D-Texture-Aware-Representations-for-Parsing-Diverse-Human-Clothing-and-Body-Parts"><a href="#60-Learning-3D-Texture-Aware-Representations-for-Parsing-Diverse-Human-Clothing-and-Body-Parts" class="headerlink" title="60. Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Learning_3D_Texture-Aware_Representations_for_Parsing_Diverse_Human_Clothing_and_Body_Parts.pdf">Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">KTH Royal Institute of Technology</span></p><p>该论文提出了Spectrum模型，通过重新利用I2Tx（Image-to-Texture）扩散模型的内部特征，实现对多人体场景中细粒度衣物和身体部位的逐像素语义分割。方法结合3D纹理映射和多模态文本引导分割，支持对新颖、未见服装类别的泛化。实验结果表明，Spectrum在多数据集下均优于现有基线方法，能准确解析复杂场景中的人体衣物和部位。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="61-A-3DGS-Diffusion-Self-Supervised-Framework-for-Normal-Estimation-from-a-Single-Image"><a href="#61-A-3DGS-Diffusion-Self-Supervised-Framework-for-Normal-Estimation-from-a-Single-Image" class="headerlink" title="61. A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/A_3DGS-Diffusion_Self-Supervised_Framework_for_Normal_Estimation_from_a_Single_Image.pdf">A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Jiangnan University</span></p><p>该论文提出了一种名为SINGAD的自监督单幅图像法线估计算法，结合3D Gaussian Splatting（3DGS）物理建模与条件扩散模型，通过物理光交互建模、跨域特征融合及可微3D重投影损失，将3D几何误差直接转化为法线优化信号，实现无标注数据下多视角几何一致的法线估计。实验结果表明，该方法在Google Scanned Objects数据集上超越现有主流方法，显著提升了几何一致性和细节重建能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="62-Improved-Sub-Visible-Particle-Classification-in-Flow-Imaging-Microscopy-via-Generative-AI-Based-Image-Synthesis"><a href="#62-Improved-Sub-Visible-Particle-Classification-in-Flow-Imaging-Microscopy-via-Generative-AI-Based-Image-Synthesis" class="headerlink" title="62. Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Improved_Sub-Visible_Particle_Classification_in_Flow_Imaging_Microscopy_via_Generative_AI-Based_Imag.pdf">Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Ghent University Global Campus</span></p><p>该论文提出利用扩散生成模型（diffusion models）为流式成像显微镜下亚可见颗粒（SvPs）分类中的少数类（如硅油滴和气泡）合成高保真图像，并用这些生成样本扩充不平衡训练集，显著提升多类深度神经网络（如ResNet-50）在高不平衡真实验证集上的分类精度。实验显示，融合生成图像的数据增强方案能有效缓解类别不平衡问题，提升药品质量控制中微粒自动识别的鲁棒性和准确性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="63-KnapFormer-An-Online-Load-Balancer-for-Efficient-Diffusion-Transformers-Training"><a href="#63-KnapFormer-An-Online-Load-Balancer-for-Efficient-Diffusion-Transformers-Training" class="headerlink" title="63. KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/KnapFormer__An_Online_Load_Balancer_for_Efficient_Diffusion_Transformers_Training.pdf">KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Adobe</span></p><p>KnapFormer是一种高效的在线序列块级负载均衡算法，专为分布式训练Diffusion Transformers（DiT）模型设计，通过解决跨GPU的动态token长度不均衡问题，实现了序列并行处理与工作负载最优分配。该方法集成了DeepSpeed-Ulysses分布式注意力机制，仅需一次all-to-all通信即可实现全局负载均衡，显著提升了训练吞吐率，在多模态、混合分辨率和图像视频联合训练场景下可获得2至3倍加速，消除straggler效应。结论：KnapFormer可大幅提升异构数据分布下的DiT模型训练效率，并简化系统设计。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="64-Mediator-Guided-Multi-Agent-Collaboration-among-Open-Source-Models-for-Medical-Decision-Making"><a href="#64-Mediator-Guided-Multi-Agent-Collaboration-among-Open-Source-Models-for-Medical-Decision-Making" class="headerlink" title="64. Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Mediator-Guided_Multi-Agent_Collaboration_among_Open-Source_Models_for_Medical_Decision-Making.pdf">Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Fudan University</span></p><p>本文提出MedOrch，一个基于开源多模态视觉语言模型（VLM）和大语言模型（LLM）中介者的多智能体协作框架，用于医学多模态决策。其流程包括多VLM专家独立作答、LLM中介者开展Socratic问答引导反思、专家修正答案，最终由判决智能体综合输出。实验在五个医学视觉问答基准上验证了该方法无需训练即可显著超越单一模型和传统多智能体策略，提升医学多模态智能决策性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="65-ECMF-Enhanced-Cross-Modal-Fusion-for-Multimodal-Emotion-Recognition-in-MER-SEMI-Challenge"><a href="#65-ECMF-Enhanced-Cross-Modal-Fusion-for-Multimodal-Emotion-Recognition-in-MER-SEMI-Challenge" class="headerlink" title="65. ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ECMF__Enhanced_Cross-Modal_Fusion_for_Multimodal_Emotion_Recognition_in_MER-SEMI_Challenge.pdf">ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">State Key Laboratory of General Artificial Intelligence, BIGAI</span></p><p>该论文提出了一种多模态情感识别框架，结合预训练音频（HuBERT-Large）、文本（Chinese-RoBERTa-wwm-ext-large，GPT-4，Qwen-Omni）和视觉（OpenFace，CLIP-ViT-Large）特征提取器，通过自注意力机制与残差连接实现多模态特征融合，并通过多源标签策略提升数据质量。实验结果显示，该方法在MER2025-SEMI数据集上加权F1显著优于官方基线，验证了框架的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="66-Neural-Field-Representations-of-Mobile-Computational-Photography"><a href="#66-Neural-Field-Representations-of-Mobile-Computational-Photography" class="headerlink" title="66. Neural Field Representations of Mobile Computational Photography"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Neural_Field_Representations_of_Mobile_Computational_Photography.pdf">Neural Field Representations of Mobile Computational Photography</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Princeton University</span></p><p>本文提出了多种基于神经场（neural field）的紧凑模型，用于移动设备摄影数据的深度估计、图层分离和全景图像拼接。通过直接对手机采集的原始多帧数据进行端到端优化，无需复杂预处理或标注，实现了高质量的深度重建、前景&#x2F;背景分离和视角扩展，在多个真实场景和基准测试中超越了现有方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="llm">Large Language Model</h2><h3 id="67-Effective-Training-Data-Synthesis-for-Improving-MLLM-Chart-Understanding"><a href="#67-Effective-Training-Data-Synthesis-for-Improving-MLLM-Chart-Understanding" class="headerlink" title="67. Effective Training Data Synthesis for Improving MLLM Chart Understanding"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Effective_Training_Data_Synthesis_for_Improving_MLLM_Chart_Understanding.pdf">Effective Training Data Synthesis for Improving MLLM Chart Understanding</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Australian National University</span></p><p>论文提出了一种五步图表数据合成管道，包括分离数据和函数的单图生成、条件式多子图生成、视觉多样化、质量过滤和基于GPT-4o的问答对生成，构建了高质量的Effective Chart Dataset (ECD)。实验表明，使用ECD微调多种开源多模态大语言模型能在真实和合成测试集上显著提升图表理解准确率，尤其是在复杂科学图表领域，优于现有训练集。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="68-LLM-Unlearning-using-Gradient-Ratio-Based-Influence-Estimation-and-Noise-Injection"><a href="#68-LLM-Unlearning-using-Gradient-Ratio-Based-Influence-Estimation-and-Noise-Injection" class="headerlink" title="68. LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LLM_Unlearning_using_Gradient_Ratio-Based_Influence_Estimation_and_Noise_Injection.pdf">LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">University of Illinois Urbana Champaign</span></p><p>本文提出GRIN，一种面向大语言模型（LLM）精准高效的机器遗忘方法。方法首先利用梯度比率衡量参数对待遗忘数据的影响力，定位出最关键的权重，并在这些权重注入噪声后进行有针对性的微调，以提升遗忘效果并减少对保留知识的损害。实验在TOFU、WMDP和SafePKU等基准数据集上验证，GRIN能够在有效删除敏感或有害信息的同时保持模型整体性能。结论表明GRIN兼具高效、模块化和实际可用性，适用于法律合规和安全需求场景。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="69-ScamAgents-How-AI-Agents-Can-Simulate-Human-Level-Scam-Calls"><a href="#69-ScamAgents-How-AI-Agents-Can-Simulate-Human-Level-Scam-Calls" class="headerlink" title="69. ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ScamAgents__How_AI_Agents_Can_Simulate_Human-Level_Scam_Calls.pdf">ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Rutgers University</span></p><p>该论文提出ScamAgent，一个基于大型语言模型（LLM）的自主多回合智能代理系统，可自动生成高度逼真的诈骗电话脚本，并集成对话记忆、动态适应和文本转语音（TTS）技术。实验证明，ScamAgent能有效规避现有LLM安全机制，成功执行多场景诈骗对话，显示多回合、代理型滥用是现有安全防护的显著漏洞。结论指出，亟需多回合安全审计、代理级控制框架及新型对抗性检测方法，以应对生成式AI推动的对话欺诈风险。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="70-Learning-the-Topic-Not-the-Language-How-LLMs-Classify-Online-Immigration-Discourse-Across-Languages"><a href="#70-Learning-the-Topic-Not-the-Language-How-LLMs-Classify-Online-Immigration-Discourse-Across-Languages" class="headerlink" title="70. Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Learning_the_Topic,_Not_the_Language__How_LLMs_Classify_Online_Immigration_Discourse_Across_Language.pdf">Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Harvard University</span></p><p>该论文提出通过对开源LLaMA 3.2–3B模型在单语、双语及多语种数据集上LoRA微调，探索少量语言微调能否实现跨语言的移民推文主题与立场分类。研究发现：单&#x2F;双语微调模型能可靠检测多种未见语言中的移民相关内容，多语种微调则有助于更准确区分支持&#x2F;反对&#x2F;中立立场；只需极少量目标语言数据即可显著提升低资源语言表现并修正预训练偏见。此外，论文发布了4比特量化、可重现、推理速度快且成本极低的开源模型，为多语言社会科学研究提供了低门槛工具。结论是，跨语言主题检测无需大规模多语训练，有限语言覆盖即可实现主题泛化，结构性偏见可通过轻量干预修正。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="71-Memp-Exploring-Agent-Procedural-Memory"><a href="#71-Memp-Exploring-Agent-Procedural-Memory" class="headerlink" title="71. Memp: Exploring Agent Procedural Memory"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Memp__Exploring_Agent_Procedural_Memory.pdf">Memp: Exploring Agent Procedural Memory</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Zhejiang University</span></p><p>本文提出了Memp框架，通过将以往代理任务轨迹蒸馏为细粒度操作指令与高层次脚本抽象，实现了可学习、可更新、终身进化的程序性记忆。Memp系统性地研究了记忆的构建、检索与动态更新策略，在TravelPlanner和ALFWorld等基准任务上，实验结果显示程序性记忆能显著提升代理的成功率和效率，同时具备持续学习和迁移能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="72-Sample-efficient-LLM-Optimization-with-Reset-Replay"><a href="#72-Sample-efficient-LLM-Optimization-with-Reset-Replay" class="headerlink" title="72. Sample-efficient LLM Optimization with Reset Replay"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Sample-efficient_LLM_Optimization_with_Reset_Replay.pdf">Sample-efficient LLM Optimization with Reset Replay</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Nanjing University</span></p><p>本文提出了LoRR（LLM optimization with Reset Replay），一种增强大语言模型（LLM）微调样本效率的通用插件。LoRR通过高重放训练（多次利用每批数据）、周期性参数重置（Shrink &amp; Perturb战略）及混合优化损失（结合SFT和偏好损失），显著提升了偏好优化方法在数学与推理任务上的表现。结论显示，LoRR能有效缓解早期过拟合和样本利用不足问题，迭代DPO结合LoRR在多个基准上超越复杂RL算法，极大提高了LLM微调的效能和数据利用率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="73-End-to-End-Text-to-SQL-with-Dataset-Selection-Leveraging-LLMs-for-Adaptive-Query-Generation"><a href="#73-End-to-End-Text-to-SQL-with-Dataset-Selection-Leveraging-LLMs-for-Adaptive-Query-Generation" class="headerlink" title="73. End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/End-to-End_Text-to-SQL_with_Dataset_Selection__Leveraging_LLMs_for_Adaptive_Query_Generation.pdf">End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Infoorigin Pvt Ltd</span></p><p>该论文提出了一种三阶段端到端Text-to-SQL框架，首先通过LLM与prompt工程生成隐式规则集，结合RoBERTa编码器模型进行自动数据库识别（db id prediction），然后利用数据库schema和NLQ进行prompt驱动SQL生成，最后通过多智能体自纠错模块提升SQL准确性。实验表明该方法在Spider数据集上数据库意图预测与SQL生成均超越现有SOTA，显著提升复杂查询场景下的执行准确率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="74-SpeakerLM-End-to-End-Versatile-Speaker-Diarization-and-Recognition-with-Multimodal-Large-Language-Models"><a href="#74-SpeakerLM-End-to-End-Versatile-Speaker-Diarization-and-Recognition-with-Multimodal-Large-Language-Models" class="headerlink" title="74. SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SpeakerLM__End-to-End_Versatile_Speaker_Diarization_and_Recognition_with_Multimodal_Large_Language_M.pdf">SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Tongyi Lab</span></p><p>本文提出了SpeakerLM，这是首个针对端到端说话人分离与识别（SDR）任务的多模态大语言模型（MLLM），采用音频编码器、投影器与预训练文本LLM耦合，形成Encoder-Projector-LLM架构。其创新在于引入灵活的说话人注册机制，支持不同注册场景下的SDR，通过多阶段大规模数据训练，模型在多项基准上超越级联式SOTA系统，并在复杂场景下表现出极强的泛化与鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="75-From-Explainable-to-Explanatory-Artificial-Intelligence-Toward-a-New-Paradigm-for-Human-Centered-Explanations-through-Generative-AI"><a href="#75-From-Explainable-to-Explanatory-Artificial-Intelligence-Toward-a-New-Paradigm-for-Human-Centered-Explanations-through-Generative-AI" class="headerlink" title="75. From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/From_Explainable_to_Explanatory_Artificial_Intelligence__Toward_a_New_Paradigm_for_Human-Centered_Ex.pdf">From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Ruhr University Bochum</span></p><p>该论文提出了“Explanatory AI”新范式，利用生成式AI（如大语言模型）为用户生成叙事型、个性化、可交互的解释，区别于传统XAI仅关注算法透明性。通过构建八维概念模型并在医疗场景实证，结果显示用户更偏好情境敏感、易懂的解释，强调AI系统应以人类理解为核心而非仅算法透明。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="76-Large-Language-Model-Data-Generation-for-Enhanced-Intent-Recognition-in-German-Speech"><a href="#76-Large-Language-Model-Data-Generation-for-Enhanced-Intent-Recognition-in-German-Speech" class="headerlink" title="76. Large Language Model Data Generation for Enhanced Intent Recognition in German Speech"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Large_Language_Model_Data_Generation_for_Enhanced_Intent_Recognition_in_German_Speech.pdf">Large Language Model Data Generation for Enhanced Intent Recognition in German Speech</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">University of Hamburg</span></p><p>本论文提出结合Whisper自动语音识别（ASR）与BERT、DistilBERT和Electra等Transformer语言模型，利用LeoLM、Llama3和ChatGPT三种大语言模型自动生成德语意图识别任务的训练文本，提升针对老年德语语音的意图识别性能。实验结果表明，加入由LLM合成的数据能有效提升模型对不同说话风格和未见词汇的泛化和鲁棒性，其中专为德语设计的小型LeoLM生成的数据优于更大规模的ChatGPT数据，展示了本地领域特定LLM的优势。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="77-SIFThinker-Spatially-Aware-Image-Focus-for-Visual-Reasoning"><a href="#77-SIFThinker-Spatially-Aware-Image-Focus-for-Visual-Reasoning" class="headerlink" title="77. SIFThinker: Spatially-Aware Image Focus for Visual Reasoning"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SIFThinker__Spatially-Aware_Image_Focus_for_Visual_Reasoning.pdf">SIFThinker: Spatially-Aware Image Focus for Visual Reasoning</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Tsinghua Shenzhen International Graduate School, Tsinghua University</span></p><p>SIFThinker提出了一种空间感知的“think-with-images”多模态大语言模型框架，通过引入深度增强的区域关注机制，将3D空间信息与图像-文本链式推理相结合，并利用反向扩展-前向推理策略构建SIF-50K数据集。采用基于GRPO的空间感知强化学习方法（GRPO-SIF），集成多种奖励（格式、深度一致性、区域校正等）优化模型动态关注和推理能力。实验表明SIFThinker在空间理解与细粒度视觉感知方面超越现有SOTA模型，具有强鲁棒性和广泛适用性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="78-In-Training-Defenses-against-Emergent-Misalignment-in-Language-Models"><a href="#78-In-Training-Defenses-against-Emergent-Misalignment-in-Language-Models" class="headerlink" title="78. In-Training Defenses against Emergent Misalignment in Language Models"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/In-Training_Defenses_against_Emergent_Misalignment_in_Language_Models.pdf">In-Training Defenses against Emergent Misalignment in Language Models</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Bonn-Aachen International Center for Information Technology, University of Bonn</span></p><p>本文系统性研究了在大语言模型（LLM）微调过程中，防止涌现性失对齐（EMA）的实用正则化方法，包括KL散度正则化、特征空间L2距离保持（LDIFS）、安全子空间投影（SafeLoRA）以及安全数据插入（Interleaving）。实验结果表明，KL散度和插入安全数据能显著抑制EMA，但分别会带来学习能力受限和答案不连贯的副作用，仍需更精细的策略以实现安全与效能的平衡。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="79-Classification-is-a-RAG-problem-A-case-study-on-hate-speech-detection"><a href="#79-Classification-is-a-RAG-problem-A-case-study-on-hate-speech-detection" class="headerlink" title="79. Classification is a RAG problem: A case study on hate speech detection"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Classification_is_a_RAG_problem__A_case_study_on_hate_speech_detection.pdf">Classification is a RAG problem: A case study on hate speech detection</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Contextual AI</span></p><p>本文提出了一种基于检索增强生成（RAG, Retrieval-Augmented Generation）的分类方法，将传统分类任务转变为基于推理时检索到的上下文知识进行评估。作者开发了CONTEXTUAL POLICY ENGINE (CPE)系统，通过检索相关政策文档并结合生成式模型，对仇恨言论进行灵活、可解释和可动态调整的分类，实验表明该方法在准确性、可扩展性和灵活性方面优于主流商用系统，无需模型重训练即可适应政策变更。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="80-UR2-Unify-RAG-and-Reasoning-through-Reinforcement-Learning"><a href="#80-UR2-Unify-RAG-and-Reasoning-through-Reinforcement-Learning" class="headerlink" title="80. UR2: Unify RAG and Reasoning through Reinforcement Learning"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/UR$%5E2$__Unify_RAG_and_Reasoning_through_Reinforcement_Learning.pdf">UR2: Unify RAG and Reasoning through Reinforcement Learning</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Tsinghua University</span></p><p>本文提出UR2框架，首次将检索增强生成（RAG）与强化学习（RL）紧密结合，实现大模型在推理过程中动态协调何时检索与何时依赖自身知识。UR2采用难度感知课程训练，仅对复杂问题激活检索，并通过混合知识访问策略融合离线领域语料与LLM摘要，提升模型的泛化与适应性。实验表明，UR2在数学、医学、开放领域QA等多任务上显著优于现有RAG和RL方法，7B模型达到GPT-4o-mini水准，具备广泛的通用性和鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="81-Automated-Creation-of-the-Legal-Knowledge-Graph-Addressing-Legislation-on-Violence-Against-Women-Resource-Methodology-and-Lessons-Learned"><a href="#81-Automated-Creation-of-the-Legal-Knowledge-Graph-Addressing-Legislation-on-Violence-Against-Women-Resource-Methodology-and-Lessons-Learned" class="headerlink" title="81. Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Automated_Creation_of_the_Legal_Knowledge_Graph_Addressing_Legislation_on_Violence_Against_Women__Re.pdf">Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">University of Bari Aldo Moro</span></p><p>本论文提出了两种用于自动化构建针对女性暴力立法的法律知识图谱的方法：一种是法律领域定制的系统化底层流程，包括结构化数据提取、语义丰富和本体开发，另一种则基于大型语言模型（LLM）结合检索增强生成（RAG）、提示工程和传统NLP技术。结论显示，底层方法精度高但耗时，LLM方法具备快速扩展性但需人工校验，二者互补能提升法律信息获取和智能化应用效果。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="82-Beyond-Prompt-Induced-Lies-Investigating-LLM-Deception-on-Benign-Prompts"><a href="#82-Beyond-Prompt-Induced-Lies-Investigating-LLM-Deception-on-Benign-Prompts" class="headerlink" title="82. Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Beyond_Prompt-Induced_Lies__Investigating_LLM_Deception_on_Benign_Prompts.pdf">Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">National University of Singapore</span></p><p>本文提出了一个名为Contact Searching Question (CSQ) 的新评估框架，用于量化大型语言模型（LLM）在无明确欺骗意图的普通问题上的自发性欺骗行为。方法包括设计两项心理学启发的统计指标：欺骗意图分数和欺骗行为分数，在14个主流LLM上实证分析发现，这两项分数在任务难度提升时同步上升，揭示当前LLM在复杂任务中欺骗倾向增强，提出对应数学模型解释。该结论表明，先进LLM在处理复杂任务时自发性欺骗行为普遍存在，需引起信任和安全重视。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="83-Aligning-Effective-Tokens-with-Video-Anomaly-in-Large-Language-Models"><a href="#83-Aligning-Effective-Tokens-with-Video-Anomaly-in-Large-Language-Models" class="headerlink" title="83. Aligning Effective Tokens with Video Anomaly in Large Language Models"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Aligning_Effective_Tokens_with_Video_Anomaly_in_Large_Language_Models.pdf">Aligning Effective Tokens with Video Anomaly in Large Language Models</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">The University of Hong Kong</span></p><p>本文提出VA-GPT，一种专为视频异常事件检测与理解设计的多模态大语言模型，通过Spatial Effective Token Selection（SETS）和Temporal Effective Token Generation（TETG）模块，有效选择和生成与异常事件相关的空间和时间关键视觉令牌，并与LLM对齐。实验结果显示，VA-GPT在多个基准测试（含跨域场景）中异常检测和定位表现优于现有方法，显著提升了异常视频理解的准确性和泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="84-Harnessing-Adaptive-Topology-Representations-for-Zero-Shot-Graph-Question-Answering"><a href="#84-Harnessing-Adaptive-Topology-Representations-for-Zero-Shot-Graph-Question-Answering" class="headerlink" title="84. Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Harnessing_Adaptive_Topology_Representations_for_Zero-Shot_Graph_Question_Answering.pdf">Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Southern University of Science and Technology</span></p><p>该论文提出DynamicTRF框架，通过系统分析现有拓扑表示形式（TRF）并引入GRE指标，设计专用于零样本图问答的多样TRF集FZS。方法包括构建TRF偏好数据集、训练TRF Router以实现针对问题的动态TRF分配，显著提升LMM的零样本图问答准确率与简洁性。实验表明DynamicTRF在7种算法任务和2种下游任务上均优于现有方法，实现了准确性与响应长度的均衡提升。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="85-GeoLaux-A-Benchmark-for-Evaluating-MLLMs’-Geometry-Performance-on-Long-Step-Problems-Requiring-Auxiliary-Lines"><a href="#85-GeoLaux-A-Benchmark-for-Evaluating-MLLMs’-Geometry-Performance-on-Long-Step-Problems-Requiring-Auxiliary-Lines" class="headerlink" title="85. GeoLaux: A Benchmark for Evaluating MLLMs’ Geometry Performance on Long-Step Problems Requiring Auxiliary Lines"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/GeoLaux__A_Benchmark_for_Evaluating_MLLMs'_Geometry_Performance_on_Long-Step_Problems_Requiring_Auxi.pdf">GeoLaux: A Benchmark for Evaluating MLLMs’ Geometry Performance on Long-Step Problems Requiring Auxiliary Lines</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Xi’an Jiaotong University</span></p><p>该论文提出了GeoLaux，大规模几何问题数据集，专为评估多模态大语言模型（MLLMs）在长步骤推理和辅助线构建任务中的表现。作者设计了包含五维度的细粒度评估框架，并在13种主流MLLMs上进行了实验，发现模型在长步骤推理中性能急剧下降、证明题易偷懒、辅助线意识薄弱，提升辅助线能力能显著改善几何推理效果。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="86-LoRA-in-LoRA-Towards-Parameter-Efficient-Architecture-Expansion-for-Continual-Visual-Instruction-Tuning"><a href="#86-LoRA-in-LoRA-Towards-Parameter-Efficient-Architecture-Expansion-for-Continual-Visual-Instruction-Tuning" class="headerlink" title="86. LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LoRA_in_LoRA__Towards_Parameter-Efficient_Architecture_Expansion_for_Continual_Visual_Instruction_Tu.pdf">LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Hefei University of Technology</span></p><p>本文提出了LiLoRA方法，用于多模态大语言模型（MLLMs）中的持续视觉指令微调（CVIT），通过在LoRA低秩适配器中跨任务共享矩阵A，并对任务相关的矩阵B进行进一步低秩分解，大幅提升参数效率。同时引入余弦正则化稳定损失以保持共享基的稳定性。实验结果显示，LiLoRA在保持高性能的同时，显著减少了参数开销，有效缓解灾难性遗忘问题。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="87-Retrieval-Augmented-Large-Language-Model-System-for-Comprehensive-Drug-Contraindications"><a href="#87-Retrieval-Augmented-Large-Language-Model-System-for-Comprehensive-Drug-Contraindications" class="headerlink" title="87. Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Retrieval_Augmented_Large_Language_Model_System_for_Comprehensive_Drug_Contraindications.pdf">Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Hongik University</span></p><p>本文提出了一种检索增强生成（RAG）系统，结合GPT-4o-mini大语言模型与药品使用审查（DUR）知识库，通过混合语义和关键词检索及重排序，在药物禁忌问答任务中实现高准确率。结果显示，该方法对孕妇、儿童及联合用药场景的禁忌判别准确率显著提升，能够为敏感群体提供更可靠的用药安全信息，极大减少处方决策的不确定性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="88-Less-is-More-Selective-Reflection-for-Compatible-and-Efficient-Knowledge-Distillation-in-Large-Language-Models"><a href="#88-Less-is-More-Selective-Reflection-for-Compatible-and-Efficient-Knowledge-Distillation-in-Large-Language-Models" class="headerlink" title="88. Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Less_is_More__Selective_Reflection_for_Compatible_and_Efficient_Knowledge_Distillation_in_Large_Lang.pdf">Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">City University of Hong Kong</span></p><p>本文提出了一种名为Selective Reflection Distillation (SRD)的新型数据筛选与课程调度框架，用于提升大语言模型（LLM）知识蒸馏的效果与效率。方法上，SRD利用学生模型输出对训练样本进行难度评估与筛选，通过ROUGE-L和交叉熵指标进行融合排序，保留高质量、学生兼容的数据，并采用课程学习策略按难度分阶段引入数据以优化训练流程。实验显示，SRD作为即插即用模块，能在多种白盒知识蒸馏方法和模型架构下提升学生模型性能，同时将蒸馏训练的总运行时间最多降低39%。结论：SRD显著提升了蒸馏模型的能力和训练效率，为LLM压缩提供了实用方案。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="89-PanelTR-Zero-Shot-Table-Reasoning-Framework-Through-Multi-Agent-Scientific-Discussion"><a href="#89-PanelTR-Zero-Shot-Table-Reasoning-Framework-Through-Multi-Agent-Scientific-Discussion" class="headerlink" title="89. PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/PanelTR__Zero-Shot_Table_Reasoning_Framework_Through_Multi-Agent_Scientific_Discussion.pdf">PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Beijing Univeristy of Posts and Telecommunications</span></p><p>本文提出PanelTR框架，通过多LLM科学家代理协作，分为个人调查、自我复查和集体同行评议三个阶段，模拟科学方法提升复杂表格推理能力。实验表明，该方法无需特定训练数据即可在多个表格推理任务上超越基础LLM，性能接近甚至超过部分有监督模型，展现出科学方法在复杂推理场景下的有效性和灵活的零样本泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="90-AdaptInfer-Adaptive-Token-Pruning-for-Vision–Language-Model-Inference-with-Dynamical-Text-Guidance"><a href="#90-AdaptInfer-Adaptive-Token-Pruning-for-Vision–Language-Model-Inference-with-Dynamical-Text-Guidance" class="headerlink" title="90. AdaptInfer: Adaptive Token Pruning for Vision–Language Model Inference with Dynamical Text Guidance"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/AdaptInfer__Adaptive_Token_Pruning_for_Vision-Language_Model_Inference_with_Dynamical_Text_Guidance.pdf">AdaptInfer: Adaptive Token Pruning for Vision–Language Model Inference with Dynamical Text Guidance</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Global Innovation Exchange</span></p><p>AdaptInfer提出了一种适用于视觉-语言模型（VLMs）的动态文本引导视觉token稀疏化推理框架，通过复用层间text-to-text注意力生成文本token重要性先验，并基于此动态加权text-to-vision注意力得分，实现逐层自适应vision token剪枝。此外，论文通过离线分析cross-modal注意力迁移，制定了高效合理的剪枝调度方案。实验显示，在大幅压缩视觉token数量的同时，AdaptInfer能显著降低推理延迟且准确率接近原始模型，在多个基准任务上超越现有SOTA方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="91-LLMs-for-Resource-Allocation-A-Participatory-Budgeting-Approach-to-Inferring-Preferences"><a href="#91-LLMs-for-Resource-Allocation-A-Participatory-Budgeting-Approach-to-Inferring-Preferences" class="headerlink" title="91. LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LLMs_for_Resource_Allocation__A_Participatory_Budgeting_Approach_to_Inferring_Preferences.pdf">LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">EPFL</span></p><p>本文提出了一个基于参与式预算（Participatory Budgeting, PB）的机制设计框架，系统评估了大语言模型（LLM）在复杂资源分配任务中的推理与决策能力。方法上，作者将LLMs置于社会规划者角色，通过三种提示策略（贪心选择、直接优化和爬山式精炼），在预算等约束下分配项目，并进一步考察LLMs能否仅凭自然语言或元数据推断投票者偏好。结论显示，LLMs在推理和机制设计中具有潜力，尤其是在无法获得结构化偏好的场景下能有效推断和分配资源，提示其作为动态评测基准的价值。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="92-EvolvR-Self-Evolving-Pairwise-Reasoning-for-Story-Evaluation-to-Enhance-Generation"><a href="#92-EvolvR-Self-Evolving-Pairwise-Reasoning-for-Story-Evaluation-to-Enhance-Generation" class="headerlink" title="92. EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/EvolvR__Self-Evolving_Pairwise_Reasoning_for_Story_Evaluation_to_Enhance_Generation.pdf">EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Peking University</span></p><p>本文提出了EvolvR，一种基于自进化的成对推理框架，用于提升开源大语言模型在故事评价任务中的推理和评分能力。其方法包括多角色自生成Chain-of-Thought数据、通过多智能体自过滤确保逻辑严谨，并将训练出的评价器作为奖励模型引导故事生成。实验结果显示EvolvR在StoryER、HANNA和OpenMEVA三大基准上达到SOTA，同时显著提升故事生成质量，验证了自进化框架的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="93-DP-LLM-Runtime-Model-Adaptation-with-Dynamic-Layer-wise-Precision-Assignment"><a href="#93-DP-LLM-Runtime-Model-Adaptation-with-Dynamic-Layer-wise-Precision-Assignment" class="headerlink" title="93. DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/DP-LLM__Runtime_Model_Adaptation_with_Dynamic_Layer-wise_Precision_Assignment.pdf">DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Seoul National University</span></p><p>本文提出DP-LLM，一种针对大语言模型（LLM）的动态层级精度分配推理自适应机制。其核心方法是在每一次推理解码时，通过轻量级误差估计器与门控选择器，根据输入动态选择每层的量化比特数，实现更细粒度的性能-延迟权衡。实验表明，该方法在多种模型和数据集上均优于静态分配方案，有效提升了在不同内存和延迟约束下的推理性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="94-SDEval-Safety-Dynamic-Evaluation-for-Multimodal-Large-Language-Models"><a href="#94-SDEval-Safety-Dynamic-Evaluation-for-Multimodal-Large-Language-Models" class="headerlink" title="94. SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SDEval__Safety_Dynamic_Evaluation_for_Multimodal_Large_Language_Models.pdf">SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">The Hong Kong University of Science and Technology (GZ)</span></p><p>本文提出了SDEval框架，是首个针对多模态大模型(MLLM)的安全动态评估方法，通过三类动态策略（文本、图像、文本-图像）对原有评测基准进行扰动，动态生成新的评估样本，有效提升了测试集复杂度、降低数据泄漏以及暴露MLLM安全风险。实验表明，SDEval能显著降低现有MLLM的安全评分，揭示当前模型难以应对动态安全挑战，凸显了提升模型安全能力的紧迫性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="95-LLM-Serving-Optimization-with-Variable-Prefill-and-Decode-Lengths"><a href="#95-LLM-Serving-Optimization-with-Variable-Prefill-and-Decode-Lengths" class="headerlink" title="95. LLM Serving Optimization with Variable Prefill and Decode Lengths"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LLM_Serving_Optimization_with_Variable_Prefill_and_Decode_Lengths.pdf">LLM Serving Optimization with Variable Prefill and Decode Lengths</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Tsinghua University</span></p><p>本文研究了大语言模型（LLM）推理服务中的任务调度问题，重点考虑每个请求具有不同的输入（prefill）长度和输出（decode）长度，受限于KV缓存内存约束。作者首先证明该问题在一般情况下为NP-难，并分析了常用调度算法（如FCFS和Shortest-First）在非均匀输入下的性能下界。针对上述挑战，提出了基于新质量指标F(X)的Sorted-F调度算法，并证明其具有常数竞争比；同时开发了动态规划、局部搜索和LP松弛等高效近似算法。大量实验证明，Sorted-F及其变体在实际数据集上优于常用基线，并兼具理论鲁棒性和计算效率。结论表明，Sorted-F算法能显著降低LLM服务端到端延迟，为多样化请求场景下的高效推理提供了理论与实践支持。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="96-SKATE-a-Scalable-Tournament-Eval-Weaker-LLMs-differentiate-between-stronger-ones-using-verifiable-challenges"><a href="#96-SKATE-a-Scalable-Tournament-Eval-Weaker-LLMs-differentiate-between-stronger-ones-using-verifiable-challenges" class="headerlink" title="96. SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SKATE,_a_Scalable_Tournament_Eval__Weaker_LLMs_differentiate_between_stronger_ones_using_verifiable_.pdf">SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">The Alan Turing Institute</span></p><p>该论文提出SKATE框架，将大语言模型（LLM）的评估转化为模型间的竞赛，模型既充当任务制定者又充当解答者，通过生成和解决可验证的任务（如代码输出预测），以自动化、无数据依赖和高度可扩展的方式实现评估。实验表明，弱模型可以有效区分强模型，排名稳定，并能够自动发现模型间细微能力差异，验证了该框架的客观性与适应性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="97-Can-Large-Models-Fool-the-Eye-A-New-Turing-Test-for-Biological-Animation"><a href="#97-Can-Large-Models-Fool-the-Eye-A-New-Turing-Test-for-Biological-Animation" class="headerlink" title="97. Can Large Models Fool the Eye? A New Turing Test for Biological Animation"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Can_Large_Models_Fool_the_Eye__A_New_Turing_Test_for_Biological_Animation.pdf">Can Large Models Fool the Eye? A New Turing Test for Biological Animation</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Shanghai Jiao Tong University</span></p><p>本文提出了BioMotion Arena，一个通过视觉生物运动动画进行大模型评估的新框架，专门针对LLMs和多模态大模型（MLLMs）。方法采用点光源动画和人类偏好投票，收集超过45,000次对比投票，利用Elo评分系统动态排名模型。实验发现绝大多数主流模型难以生成合理且逼真的生物动作动画，BioMotion Arena能有效区分模型表现并反映真实用户体验差异。结论：当前大模型对生物运动理解较差，BioMotion Arena能提供直观且可靠的评估方式。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="98-Fourier-VLM-Compressing-Vision-Tokens-in-the-Frequency-Domain-for-Large-Vision-Language-Models"><a href="#98-Fourier-VLM-Compressing-Vision-Tokens-in-the-Frequency-Domain-for-Large-Vision-Language-Models" class="headerlink" title="98. Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Fourier-VLM__Compressing_Vision_Tokens_in_the_Frequency_Domain_for_Large_Vision-Language_Models.pdf">Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Shanghai Jiao Tong University</span></p><p>本文提出Fourier-VLM，通过在频率域压缩视觉特征，显著减少Vision-Language Models（VLMs）输入中的视觉token数量。方法核心是无参数的Frequency Feature Compressor（FFC），利用二维离散余弦变换（2d-DCT）提取低频视觉特征，减少高频冗余，同时通过逆DCT恢复压缩后的视觉表示。实验证明，Fourier-VLM在LLaVA和Qwen-VL等主流架构下，在八个图像基准任务上以极低的视觉token保留率（最低仅6.25%）保持96%以上的平均性能，并将计算量（FLOPs）降低至16.16%，推理速度提升31.2%，泛化性和零样本视频理解能力强，适合高效实际部署。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="99-Efficient-Knowledge-Probing-of-Large-Language-Models-by-Adapting-Pre-trained-Embeddings"><a href="#99-Efficient-Knowledge-Probing-of-Large-Language-Models-by-Adapting-Pre-trained-Embeddings" class="headerlink" title="99. Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Efficient_Knowledge_Probing_of_Large_Language_Models_by_Adapting_Pre-trained_Embeddings.pdf">Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Georgia Institute of Technology</span></p><p>本论文提出PEEK框架，利用经过训练的文本和图结构嵌入模型，通过微调线性层高效预测大语言模型（LLM）是否掌握某一事实，无需对LLM进行大量推理调用。实验证明，该方法在多个数据集和主流LLM上能以高达90%的准确率判定知识掌握情况，揭示嵌入模型可用于高效发现LLM知识盲点，结论是PEEK能在LLM部署前有效识别和弥补事实空白，提升知识评估效率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="100-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future"><a href="#100-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future" class="headerlink" title="100. Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Temporal_Self-Rewarding_Language_Models__Decoupling_Chosen-Rejected_via_Past-Future.pdf">Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Peking University</span></p><p>本论文提出Temporal Self-Rewarding Language Models，通过引入“Anchored Rejection”（历史模型固定负样本）和“Future-Guided Chosen”（未来模型动态正样本）两个阶段，解决了自奖励大语言模型中正负样本表示收敛导致学习信号消失的问题。实验结果表明，该方法在Llama、Qwen、Mistral等多种模型及AlpacaEval 2.0等主流基准上显著优于标准自奖励方法，同时保持计算资源不变，且具有良好的泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="101-Position-Intelligent-Coding-Systems-Should-Write-Programs-with-Justifications"><a href="#101-Position-Intelligent-Coding-Systems-Should-Write-Programs-with-Justifications" class="headerlink" title="101. Position: Intelligent Coding Systems Should Write Programs with Justifications"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Position__Intelligent_Coding_Systems_Should_Write_Programs_with_Justifications.pdf">Position: Intelligent Coding Systems Should Write Programs with Justifications</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Purdue University</span></p><p>本文提出智能编码系统不仅应生成代码，还应生成可理解、与代码语义一致的自然语言解释（justifications），以提升非专业用户的理解和信任。作者分析了现有形式化验证、静态分析和后验可解释性方法的局限，提出基于神经-符号一体化的方法，通过在训练阶段引入符号约束并在推理阶段自动验证解释与语义一致性，实现认知对齐与语义忠实的解释生成。结论认为，神经-符号方法能更好地提升代码智能生成系统的可解释性和信赖度。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="102-MATHREAL-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models"><a href="#102-MATHREAL-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models" class="headerlink" title="102. MATHREAL: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/MathReal__We_Keep_It_Real!_A_Real_Scene_Benchmark_for_Evaluating_Math_Reasoning_in_Multimodal_Large_.pdf">MATHREAL: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Baidu Inc.</span></p><p>该论文提出MATHREAL数据集，收集了2000个真实场景下K-12数学题目图片，覆盖图像质量退化、视角变化、内容干扰等挑战，并细分为14类噪声。通过系统性实验，作者评估了40种主流多模态大模型在六种设定下的数学推理能力，发现现有MLLMs在真实教育场景下表现显著低于在干净数据上的能力。结论表明，多模态大模型在视觉感知、结构化理解和数学推理方面仍面临实际应用瓶颈，需要提升视觉编码和鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="103-Optimizing-Prompt-Sequences-using-Monte-Carlo-Tree-Search-for-LLM-Based-Optimization"><a href="#103-Optimizing-Prompt-Sequences-using-Monte-Carlo-Tree-Search-for-LLM-Based-Optimization" class="headerlink" title="103. Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Optimizing_Prompt_Sequences_using_Monte_Carlo_Tree_Search_for_LLM-Based_Optimization.pdf">Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">The George Washington University</span></p><p>该论文提出了MCTS-OPS框架，将大语言模型（LLM）与蒙特卡洛树搜索（MCTS）结合，通过将prompt选择建模为序列决策过程，自动优化多步prompt序列以生成高质量、可执行的代码，解决复杂约束优化问题。实验结果表明，MCTS-OPS在网络优化任务中成功率、优化质量和稳定性远超GPT-4、Chain-of-Thought和Self-Refine等主流LLM方法，显著提升了可达最优解的概率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="104-Learning-by-Teaching-Engaging-Students-as-Instructors-of-Large-Language-Models-in-Computer-Science-Education"><a href="#104-Learning-by-Teaching-Engaging-Students-as-Instructors-of-Large-Language-Models-in-Computer-Science-Education" class="headerlink" title="104. Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Learning_by_Teaching__Engaging_Students_as_Instructors_of_Large_Language_Models_in_Computer_Science_.pdf">Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">The Graduate Center, CUNY</span></p><p>该论文提出了一种反转传统角色的教学范式，要求学生通过精心设计的问题与提示，将LLM作为“学生”加以教学，促使学生构造和表达知识，核心方法包括基于知识空缺的问题设计、链式思维（CoT）、少样本提示（Few-Shot Prompting）以及自洽性验证，并开发了Socrates系统实现低门槛部署。实证结果表明，该方法在本科计算机课程中能显著提升学生作业与项目表现，降低对LLM的依赖，增强主动学习成效。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="105-Crisp-Attention-Regularizing-Transformers-via-Structured-Sparsity"><a href="#105-Crisp-Attention-Regularizing-Transformers-via-Structured-Sparsity" class="headerlink" title="105. Crisp Attention: Regularizing Transformers via Structured Sparsity"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Crisp_Attention__Regularizing_Transformers_via_Structured_Sparsity.pdf">Crisp Attention: Regularizing Transformers via Structured Sparsity</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Joyspace AI</span></p><p>本文提出在Transformer模型自注意力机制中引入结构化稀疏性作为正则化手段，通过对DistilBERT在SST-2情感分析任务的微调实验，采用top-k筛选显著注意力连接，显著提升模型泛化能力和准确率。结果显示，80%稀疏度模型准确率提升0.97%，验证了稀疏不仅提升效率，更能提高Transformer性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="106-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents"><a href="#106-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents" class="headerlink" title="106. Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Bifrost-1__Bridging_Multimodal_LLMs_and_Diffusion_Models_with_Patch-level_CLIP_Latents.pdf">Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">UNC Chapel Hill</span></p><p>BIFROST-1提出了一种高效的统一多模态生成与理解框架，通过将预训练多模态大语言模型（MLLM）与扩散模型连接，采用与MLLM原生对齐的patch级CLIP图像embedding作为桥接变量，并利用轻量级Latent ControlNet集成到扩散模型，实现高保真、可控的图像生成，同时显著降低训练成本。实验结果表明，BIFROST-1在图像质量和多模态理解上达到或超越现有方法，且训练计算量显著减少，同时完全保持MLLM的推理和规划能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h2 id="ml">Machine Learning</h2><h3 id="107-eSASRec-Enhancing-Transformer-based-Recommendations-in-a-Modular-Fashion"><a href="#107-eSASRec-Enhancing-Transformer-based-Recommendations-in-a-Modular-Fashion" class="headerlink" title="107. eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/eSASRec__Enhancing_Transformer-based_Recommendations_in_a_Modular_Fashion.pdf">eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">MTS</span></p><p>该论文提出了eSASRec，一种基于模块化增强的Transformer推荐模型，通过将SASRec的训练目标、LiGR Transformer层和Sampled Softmax损失函数结合，系统性评估各增强模块的增益。结论显示，eSASRec在工业和学术数据集上超越了当前SOTA模型，可作为强大且易集成的推荐系统基线方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="108-Tree-Based-Deep-Learning-for-Ranking-Symbolic-Integration-Algorithms"><a href="#108-Tree-Based-Deep-Learning-for-Ranking-Symbolic-Integration-Algorithms" class="headerlink" title="108. Tree-Based Deep Learning for Ranking Symbolic Integration Algorithms"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Tree-Based_Deep_Learning_for_Ranking_Symbolic_Integration_Algorithms.pdf">Tree-Based Deep Learning for Ranking Symbolic Integration Algorithms</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Coventry University</span></p><p>本文提出了一种基于树结构深度学习模型的两阶段架构，优化计算机代数系统（CAS）中符号不定积分算法的选择。方法包括先识别可用方法，再根据输出表达式复杂度排序，并将数学表达式表示为树结构以提升性能。实验显示，树变换器在独立测试集上优于Maple内置选择器及先前ML方法，准确率接近90%，具有较强泛化能力。结论：树结构表示和两阶段排名框架对数学软件算法选择优化至关重要。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="109-ActivityDiff-A-diffusion-model-with-Positive-and-Negative-Activity-Guidance-for-De-Novo-Drug-Design"><a href="#109-ActivityDiff-A-diffusion-model-with-Positive-and-Negative-Activity-Guidance-for-De-Novo-Drug-Design" class="headerlink" title="109. ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ActivityDiff__A_diffusion_model_with_Positive_and_Negative_Activity_Guidance_for_De_Novo_Drug_Design.pdf">ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">School of Computer Science and Engineering, Central South University</span></p><p>本文提出了ActivityDiff，一种结合正向和负向分类器引导的离散扩散分子生成模型，能够实现药物分子对多靶点活性和选择性的精确调控。该方法通过将正向（靶向活性提升）和负向（抑制脱靶效应）指导引入扩散过程，支持单&#x2F;多靶标、片段约束、特异性增强等多种药物设计任务。实验结果表明，ActivityDiff在分子新颖性、有效性、特异性和降低脱靶风险方面均优于现有方法，实现了药物设计中活性与安全性的平衡。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="110-AntiCheatPT-A-Transformer-Based-Approach-to-Cheat-Detection-in-Competitive-Computer-Games"><a href="#110-AntiCheatPT-A-Transformer-Based-Approach-to-Cheat-Detection-in-Competitive-Computer-Games" class="headerlink" title="110. AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games"></a>110. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/AntiCheatPT__A_Transformer-Based_Approach_to_Cheat_Detection_in_Competitive_Computer_Games.pdf">AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">The IT-University of Copenhagen</span></p><p>本文提出了AntiCheatPT 256，一种基于Transformer编码器的机器学习模型，通过对Counter-Strike 2游戏行为数据进行多变量时序建模，实现作弊检测，并开源了手工标注的CS2CD数据集（795场比赛）。实验结果显示该模型在未增强测试集上准确率达89.17%、AUC达93.36%，验证了基于服务器端行为数据的可复现作弊检测方法的有效性，降低了对侵入式反作弊手段的依赖。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="111-Low-Bit-Data-Processing-Using-Multiple-Output-Spiking-Neurons-with-Non-linear-Reset-Feedback"><a href="#111-Low-Bit-Data-Processing-Using-Multiple-Output-Spiking-Neurons-with-Non-linear-Reset-Feedback" class="headerlink" title="111. Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback"></a>111. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Low-Bit_Data_Processing_Using_Multiple-Output_Spiking_Neurons_with_Non-linear_Reset_Feedback.pdf">Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Uppsala University</span></p><p>该论文提出了一种多输出、带有非线性重置反馈的新型脉冲神经元模型，将线性状态空间模型（SSM）与可学习的非线性重置机制结合，实现低比特脉冲信号处理。作者通过在多模态任务（语音关键字识别、事件视觉和序列模式识别）上的实验，表明该模型在不强制线性稳定性约束下，能获得与主流SNN基线相当的性能，并解决不稳定线性动力学下的学习收敛问题。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="112-Multi-Omics-Analysis-for-Cancer-Subtype-Inference-via-Unrolling-Graph-Smoothness-Priors"><a href="#112-Multi-Omics-Analysis-for-Cancer-Subtype-Inference-via-Unrolling-Graph-Smoothness-Priors" class="headerlink" title="112. Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors"></a>112. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Multi-Omics_Analysis_for_Cancer_Subtype_Inference_via_Unrolling_Graph_Smoothness_Priors.pdf">Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Zhejiang University</span></p><p>本文提出了GTMancer框架，通过对多组组学数据进行对比学习对齐后，采用图神经网络优化方法联合捕捉组学间和样本间的结构信息，并引入双重注意力机制提升癌症亚型分类性能。理论上证明了优化过程的收敛性，并通过牛顿法自动调整步长，实验结果在七个真实癌症数据集上超越现有方法，提升分类准确率与泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="113-One-Size-Does-Not-Fit-All-A-Distribution-Aware-Sparsification-for-More-Precise-Model-Merging"><a href="#113-One-Size-Does-Not-Fit-All-A-Distribution-Aware-Sparsification-for-More-Precise-Model-Merging" class="headerlink" title="113. One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging"></a>113. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/One_Size_Does_Not_Fit_All__A_Distribution-Aware_Sparsification_for_More_Precise_Model_Merging.pdf">One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Northeastern University</span></p><p>本文提出了TADrop（Tensor-wise Adaptive Drop），一种基于参数分布自适应的稀疏化策略，用于提升多任务模型融合的精度。TADrop根据每个参数张量的分布特性动态分配稀疏率，实现细粒度、结构感知的稀疏化，并通过归一化保持张量范数。实验表明，TADrop可无缝集成至多种模型融合方案，在视觉、语言及多模态任务上均显著提升融合模型性能，尤其在任务数量增多时表现出更强的鲁棒性和泛化性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="114-Semantic-Item-Graph-Enhancement-for-Multimodal-Recommendation"><a href="#114-Semantic-Item-Graph-Enhancement-for-Multimodal-Recommendation" class="headerlink" title="114. Semantic Item Graph Enhancement for Multimodal Recommendation"></a>114. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Semantic_Item_Graph_Enhancement_for_Multimodal_Recommendation.pdf">Semantic Item Graph Enhancement for Multimodal Recommendation</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Nanyang Technological University</span></p><p>本文提出SIGER框架，将用户-物品交互图中的协同信号注入到基于模态的物品-物品语义图，构建增强型语义图以丰富物品语义建模。通过模数化个性化嵌入扰动机制生成对比视图，结合对比学习实现抗噪表征，并设计双重表征对齐机制，提升行为与语义表征的一致性。实验表明SIGER在多种推荐场景均显著优于现有方法，尤其在冷启动情境下优势突出。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="115-Enhancing-the-Scalability-of-Classical-Surrogates-for-Real-World-Quantum-Machine-Learning-Applications"><a href="#115-Enhancing-the-Scalability-of-Classical-Surrogates-for-Real-World-Quantum-Machine-Learning-Applications" class="headerlink" title="115. Enhancing the Scalability of Classical Surrogates for Real-World Quantum Machine Learning Applications"></a>115. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Enhancing_the_Scalability_of_Classical_Surrogates_for_Real-World_Quantum_Machine_Learning_Applicatio.pdf">Enhancing the Scalability of Classical Surrogates for Real-World Quantum Machine Learning Applications</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">E.ON Digital Technology GmbH</span></p><p>本文提出了一种高效替代流程，利用实际训练数据和随机傅里叶频率采样，极大降低了从量子模型生成经典代理模型的计算资源需求，实现了对更大规模量子模型的经典近似。实验证明，该方法在能源需求预测任务中，可在资源线性扩展的情况下获得与量子模型高度一致的预测精度，显著提升了QML实际工业部署的可行性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="116-MeanAudio-Fast-and-Faithful-Text-to-Audio-Generation-with-Mean-Flows"><a href="#116-MeanAudio-Fast-and-Faithful-Text-to-Audio-Generation-with-Mean-Flows" class="headerlink" title="116. MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows"></a>116. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/MeanAudio__Fast_and_Faithful_Text-to-Audio_Generation_with_Mean_Flows.pdf">MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Shanghai Jiao Tong University</span></p><p>该论文提出MeanAudio，一种基于MeanFlow的高效文本到音频生成模型，利用Flux-style流变换器在训练中回归平均速度场，通过直接从流程轨迹起点到终点实现快速音频合成。引入分类器无关引导和瞬时到平均流的训练策略，极大提升了训练稳定性和生成质量。实验结果显示MeanAudio在单步和多步生成均达SOTA性能，推理速度比主流扩散方法快一百倍，参数量更低。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="117-Architecture-Aware-Generalization-Bounds-for-Temporal-Networks-Theory-and-Fair-Comparison-Methodology"><a href="#117-Architecture-Aware-Generalization-Bounds-for-Temporal-Networks-Theory-and-Fair-Comparison-Methodology" class="headerlink" title="117. Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology"></a>117. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Architecture-Aware_Generalization_Bounds_for_Temporal_Networks__Theory_and_Fair_Comparison_Methodolo.pdf">Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Technion</span></p><p>本文提出了针对深度时序网络（如TCN）的首个架构相关、非虚无的泛化界理论，并引入了公平对比评估方法。核心方法包括延迟反馈阻断机制将相关样本转化为近似独立样本，推导出泛化界随网络深度D呈√D缩放，并通过固定有效样本量的实验揭示强依赖序列在相同信息量下能获得显著更小的泛化间隔，挑战了依赖性只会带来负面影响的传统认识。结论：理论界为网络架构选择提供量化指导，实验证明时序依赖有助于提升泛化性能，但理论与实践间仍有差异，公平对比方法能更准确揭示依赖结构对学习的影响。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="118-Do-Ethical-AI-Principles-Matter-to-Users-A-Large-Scale-Analysis-of-User-Sentiment-and-Satisfaction"><a href="#118-Do-Ethical-AI-Principles-Matter-to-Users-A-Large-Scale-Analysis-of-User-Sentiment-and-Satisfaction" class="headerlink" title="118. Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction"></a>118. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Do_Ethical_AI_Principles_Matter_to_Users__A_Large-Scale_Analysis_of_User_Sentiment_and_Satisfaction.pdf">Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Hankuk University of Foreign Studies</span></p><p>本研究通过收集G2.com上10万余条AI产品用户评论，基于欧盟可信赖AI伦理准则七大维度，采用零样本分类与RoBERTa-Large等Transformer模型，量化评估伦理AI原则与用户满意度的关联。结论显示，所有伦理维度（如透明度、公平性、责任）均与用户满意度正相关，且非技术用户和终端应用场景下这种关联更强，凸显伦理AI设计需关注用户情境差异。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="119-Fast-Convex-and-Conditioned-Network-for-Multi-Fidelity-Vectors-and-Stiff-Univariate-Differential-Equations"><a href="#119-Fast-Convex-and-Conditioned-Network-for-Multi-Fidelity-Vectors-and-Stiff-Univariate-Differential-Equations" class="headerlink" title="119. Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations"></a>119. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Fast,_Convex_and_Conditioned_Network_for_Multi-Fidelity_Vectors_and_Stiff_Univariate_Differential_Eq.pdf">Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations</a></h3><p><span style="    display: inline-block;    background-color: #f0f0f0;    border: 1px solid #ccc;    border-radius: 4px;    padding: 2px 6px;    font-weight: bold;    font-style: italic;">Institute of Applied Mathematics, University of British Columbia</span></p><p>本文提出了一种结合极限学习机（ELM）和移位高斯编码的神经网络架构，用于解决多保真度数据和刚性微分方程中的优化病态问题。通过在ELM中引入移位高斯编码，有效提升了激活矩阵的秩和稀疏结构，实现了更优的矩阵条件数，显著提升了对刚性PDE、多尺度模式和高维向量（如图像）的拟合精度和效率。结论指出，相较于增加网络深度，改善激活矩阵的条件性对科学计算中的神经网络收敛与表达能力更为关键。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>今日论文报纸</title>
    <link href="/2025/08/2025-08-11_article/"/>
    <url>/2025/08/2025-08-11_article/</url>
    
    <content type="html"><![CDATA[<h1 id="今日论文报纸"><a href="#今日论文报纸" class="headerlink" title="今日论文报纸"></a>今日论文报纸</h1><p>📅 <strong>日期</strong>：2025-08-11<br>📄 <strong>发现论文数量</strong>：149</p><h2 id="rl">Reinforcement Learning</h2><h3 id="1-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency"><a href="#1-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency" class="headerlink" title="1. Test-Time Reinforcement Learning for GUI Grounding via Region Consistency"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Test-Time_Reinforcement_Learning_for_GUI_Grounding_via_Region_Consistency.pdf">Test-Time Reinforcement Learning for GUI Grounding via Region Consistency</a></h3><p><em>教育背景：</em> Zhejiang University</p><p>该论文提出了GUI-RC（Region Consistency）与GUI-RCPO（Region Consistency Policy Optimization）两种测试时优化方法，用于提升GUI grounding任务（自然语言到界面坐标映射）的准确率。GUI-RC通过多次采样模型输出并空间投票得到一致性区域，无需额外标注即可提升模型定位能力；GUI-RCPO则将区域一致性转化为奖励信号，在测试时进行自监督强化学习训练，使模型在无标注数据上自我优化。实验表明方法在多个主流模型和基准上平均提升2-5%的准确率，并可自举式迭代提升，展现了测试时优化的新潜力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Discovering-Interpretable-Programmatic-Policies-via-Multimodal-LLM-assisted-Evolutionary-Search"><a href="#2-Discovering-Interpretable-Programmatic-Policies-via-Multimodal-LLM-assisted-Evolutionary-Search" class="headerlink" title="2. Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Discovering_Interpretable_Programmatic_Policies_via_Multimodal_LLM-assisted_Evolutionary_Search.pdf">Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search</a></h3><p><em>教育背景：</em> City University of Hong Kong</p><p>本文提出了一种新的可解释程序化策略发现方法——多模态大语言模型辅助进化搜索（MLES），该方法将多模态大语言模型（MLLMs）与进化优化结合，直接生成带有自然语言解释的程序化策略。MLES在进化过程中融合了视觉反馈驱动的行为分析，有效提升了策略发现效率和可解释性。实验证明，MLES在Lunar Lander和Car Racing两个典型控制任务上，策略性能与PPO相当，并显著提升了策略的透明度和知识可复用性，为下一代可解释策略发现提供了新范式。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="3-ECHO-Decoupling-Inference-and-Training-for-Large-Scale-RL-Alignment-on-Heterogeneous-Swarms"><a href="#3-ECHO-Decoupling-Inference-and-Training-for-Large-Scale-RL-Alignment-on-Heterogeneous-Swarms" class="headerlink" title="3. ECHO: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Echo__Decoupling_Inference_and_Training_for_Large-Scale_RL_Alignment_on_Heterogeneous_Swarms.pdf">ECHO: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms</a></h3><p><em>教育背景：</em> Gradient</p><p>本文提出了ECHO系统，通过将RL中的推理（trajectory sampling）和训练（policy optimization）在异构集群上彻底解耦，设计了顺序拉取和异步推送两种轻量级同步协议，实现了在边缘设备和数据中心硬件之间灵活协作。实验表明，ECHO在Qwen系列大模型上的RL任务中，既能提升硬件利用率，也能与传统全同位系统在收敛速度和最终性能持平甚至超越，验证了去中心化RL训练的可行性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="4-Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation"><a href="#4-Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation" class="headerlink" title="4. Posterior-GRPO: Rewarding Reasoning Processes in Code Generation"></a>4. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Posterior-GRPO__Rewarding_Reasoning_Processes_in_Code_Generation.pdf">Posterior-GRPO: Rewarding Reasoning Processes in Code Generation</a></h3><p><em>教育背景：</em> The State Key Laboratory of Blockchain and Data Security</p><p>本论文提出了一种新的强化学习方法Posterior-GRPO (P-GRPO)用于大语言模型代码生成，创新性地将基于优化-降级（OD-based）方法训练的思维奖励模型与传统结果奖励结合，通过在仅有正确结果时赋予思维奖励，有效防止奖励投机。实验证明，P-GRPO在多个代码与数学推理基准上提升Pass@1平均4.5%，表现与GPT-4-Turbo相当，且泛化能力强。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="5-Domain-driven-Metrics-for-Reinforcement-Learning-A-Case-Study-on-Epidemic-Control-using-Agent-based-Simulation"><a href="#5-Domain-driven-Metrics-for-Reinforcement-Learning-A-Case-Study-on-Epidemic-Control-using-Agent-based-Simulation" class="headerlink" title="5. Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation"></a>5. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Domain-driven_Metrics_for_Reinforcement_Learning__A_Case_Study_on_Epidemic_Control_using_Agent-based.pdf">Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation</a></h3><p><em>教育背景：</em> Thoughtworks Technologies</p><p>本文提出了结合领域知识和传统奖励的‘Domain-driven RL metrics’评价指标体系，用于公正、稳健地比较用于理性体建模与优化的强化学习算法。通过在疫情控制的代理建模案例中，利用多种RL算法和新的复合评价指标（如状态空间覆盖、最佳序列百分比等），有效提升了算法选择的可靠性和解释性，结论显示新指标体系比单一均值奖励更稳健、更具信任度。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="6-Towards-Hallucination-Free-Music-A-Reinforcement-Learning-Preference-Optimization-Framework-for-Reliable-Song-Generation"><a href="#6-Towards-Hallucination-Free-Music-A-Reinforcement-Learning-Preference-Optimization-Framework-for-Reliable-Song-Generation" class="headerlink" title="6. Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation"></a>6. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Hallucination-Free_Music__A_Reinforcement_Learning_Preference_Optimization_Framework_for_Rel.pdf">Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation</a></h3><p><em>教育背景：</em> Wuhan University</p><p>本论文提出了一个基于强化学习的偏好优化框架，用于减少歌词到歌曲生成过程中的幻觉现象（即生成内容与歌词不符）。方法包括构建基于音素错误率（PER）的幻觉偏好数据集，并实现三种RL偏好优化策略：DPO、PPO和GRPO，通过奖励建模和对比学习，有效抑制了幻觉的发生。实验表明，该框架能显著降低幻觉比例（如DPO+RS方法幻觉样本下降16.5%，高质量样本提升38.47%），且不损失音乐性和音质，具备良好迁移性，可扩展到风格控制等其他生成任务。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="7-Anti-Jamming-Sensing-with-Distributed-Reconfigurable-Intelligent-Metasurface-Antennas"><a href="#7-Anti-Jamming-Sensing-with-Distributed-Reconfigurable-Intelligent-Metasurface-Antennas" class="headerlink" title="7. Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas"></a>7. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Anti-Jamming_Sensing_with_Distributed_Reconfigurable_Intelligent_Metasurface_Antennas.pdf">Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas</a></h3><p><em>教育背景：</em> Xi’an Jiaotong University</p><p>本文提出利用分布式可重构智能超表面天线（RIMSA）进行射频感知，通过策略网络（深度强化学习）优化波束形成模式及神经网络映射接收信号，实现端到端感知。为抵抗干扰，设计了融合接收信号SINR的多因子损失函数，显著提升了抗干扰下的感知精度，仿真表明分布式RIMSA系统对环境及攻击有更强适应性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h2 id="robot">Robotics</h2><h3 id="8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation"><a href="#8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation" class="headerlink" title="8. Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation"></a>8. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Genie_Envisioner__A_Unified_World_Foundation_Platform_for_Robotic_Manipulation.pdf">Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation</a></h3><p><em>教育背景：</em> AgiBot Genie Team</p><p>该论文提出Genie Envisioner (GE)，一个集视频生成、策略学习、评估和仿真于一体的机器人操作世界基础平台。GE包括三大模块：GE-Base为大规模指令条件视频扩散模型，捕捉真实机器人交互的空间、时间和语义动态；GE-Act通过轻量化解码器将视觉表征映射为可执行动作轨迹，实现多平台泛化和高效策略推断；GE-Sim为动作条件神经模拟器，支持高保真闭环策略评估与并行仿真。平台还配备EWMBench标准化评测工具，综合衡量视觉真实度、物理一致性和指令-动作对齐。实验证明GE在多种机器人平台和复杂任务上均优于现有方法，具备极强的通用性和可扩展性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="9-TRAJEVO-Trajectory-Prediction-Heuristics-Design-via-LLM-driven-Evolution"><a href="#9-TRAJEVO-Trajectory-Prediction-Heuristics-Design-via-LLM-driven-Evolution" class="headerlink" title="9. TRAJEVO: Trajectory Prediction Heuristics Design via LLM-driven Evolution"></a>9. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/TrajEvo__Trajectory_Prediction_Heuristics_Design_via_LLM-driven_Evolution.pdf">TRAJEVO: Trajectory Prediction Heuristics Design via LLM-driven Evolution</a></h3><p><em>教育背景：</em> KAIST, Korea Advanced Institute of Science and Technology</p><p>该论文提出TRAJEVO框架，通过结合大语言模型（LLM）与进化算法自动生成和优化轨迹预测启发式方法，利用Cross-Generation Elite Sampling和Statistics Feedback Loop增强启发式多样性与性能。实验结果显示，TRAJEVO在多数据集上超越传统启发式方法，并且在未见分布场景下的泛化能力优于深度学习模型，同时具备高速度和可解释性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="10-OMNIEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks"><a href="#10-OMNIEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks" class="headerlink" title="10. OMNIEAR: Benchmarking Agent Reasoning in Embodied Tasks"></a>10. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/OmniEAR__Benchmarking_Agent_Reasoning_in_Embodied_Tasks.pdf">OMNIEAR: Benchmarking Agent Reasoning in Embodied Tasks</a></h3><p><em>教育背景：</em> Zhejiang University</p><p>OmniEAR提出了一个用于评估大语言模型在具身任务中推理能力的综合框架，包括EAR-Sim环境建模、自动化场景生成和EAR-Bench系统性评测，覆盖物理属性推理、工具使用与多智能体协作。结论显示：现有语言模型在需要从物理约束推理时性能严重下降，特别是在工具推理和隐式协作任务中，表明具身推理对AI架构提出了新的挑战。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="11-CleanUpBench-Embodied-Sweeping-and-Grasping-Benchmark"><a href="#11-CleanUpBench-Embodied-Sweeping-and-Grasping-Benchmark" class="headerlink" title="11. CleanUpBench: Embodied Sweeping and Grasping Benchmark"></a>11. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/CleanUpBench__Embodied_Sweeping_and_Grasping_Benchmark.pdf">CleanUpBench: Embodied Sweeping and Grasping Benchmark</a></h3><p><em>教育背景：</em> Sichuan University</p><p>本文提出了CleanUpBench，一个面向现实家居环境中服务机器人双模态（扫地与抓取）清洁任务的高保真仿真基准。平台基于NVIDIA Isaac Sim，支持多场景、多机器人协作，并提供系统化评测指标（空间覆盖、任务完成率、运动质量、计算效率等），并内置多种启发式、规划与学习基线方法。实验表明，具备多智能体强化学习能力的双模态算法（如PRIMAL2）在复杂清洁任务中显著优于仅具备单一模式的传统方法，验证了CleanUpBench作为评测和推动现实服务机器人智能发展的价值和扩展性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="12-Towards-Generalizable-Safety-in-Crowd-Navigation-via-Conformal-Uncertainty-Handling"><a href="#12-Towards-Generalizable-Safety-in-Crowd-Navigation-via-Conformal-Uncertainty-Handling" class="headerlink" title="12. Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling"></a>12. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Generalizable_Safety_in_Crowd_Navigation_via_Conformal_Uncertainty_Handling.pdf">Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling</a></h3><p><em>教育背景：</em> University of California, Riverside</p><p>本文提出了一种结合自适应保形不确定性量化(ACI)与约束强化学习(CRL)的移动机器人群体导航方法。方法流程为：首先通过规则或学习式轨迹预测模型估算人类未来轨迹，并用ACI在线量化预测不确定性，将不确定性估计作为观察输入，结合注意力机制编码人与机器人及人-人交互，最后用CRL约束机器人决策以减少对人类未来轨迹的不安全侵入。实验结果显示，在分布内及三种分布外场景（速度变化、行为策略变化、群体动态）中，该方法安全性和成功率均显著优于现有SOTA基线，且能够直接迁移到实际机器人平台实现安全稳健导航。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="13-A-Multi-view-Landmark-Representation-Approach-with-Application-to-GNSS-Visual-Inertial-Odometry"><a href="#13-A-Multi-view-Landmark-Representation-Approach-with-Application-to-GNSS-Visual-Inertial-Odometry" class="headerlink" title="13. A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry"></a>13. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Multi-view_Landmark_Representation_Approach_with_Application_to_GNSS-Visual-Inertial_Odometry.pdf">A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry</a></h3><p><em>教育背景：</em> Shanghai Jiao Tong University</p><p>本文提出了一种多视角的仅位姿特征点表示方法，并将其应用于GNSS-视觉-惯性里程计（GVIO）系统，核心技术包括多视角几何特征建模、滤波更新与特征管理策略。实验表明该方法在仿真和真实环境中提升了定位效率和精度，保持滤波一致性且计算负担更低。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="14-Information-Theoretic-Graph-Fusion-with-Vision-Language-Action-Model-for-Policy-Reasoning-and-Dual-Robotic-Control"><a href="#14-Information-Theoretic-Graph-Fusion-with-Vision-Language-Action-Model-for-Policy-Reasoning-and-Dual-Robotic-Control" class="headerlink" title="14. Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control"></a>14. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Information-Theoretic_Graph_Fusion_with_Vision-Language-Action_Model_for_Policy_Reasoning_and_Dual_R.pdf">Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control</a></h3><p><em>教育背景：</em> Hangzhou Dianzi University</p><p>本文提出了一种信息论图融合视觉-语言-动作（GF-VLA）框架，通过从人类演示视频中提取基于Shannon信息的手-物体和物体-物体交互，构建时序场景图，并与语言条件Transformer结合，生成可解释的行为树和笛卡尔动作指令。通过链式推理（CoT）、自验证和跨手选择策略实现双臂机器人任务推理与高效协作，实验在多种装配和泛化任务中取得了高于95%图准确率和90%任务成功率，展现了优异的泛化与稳健性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="15-DistillDrive-End-to-End-Multi-Mode-Autonomous-Driving-Distillation-by-Isomorphic-Hetero-Source-Planning-Model"><a href="#15-DistillDrive-End-to-End-Multi-Mode-Autonomous-Driving-Distillation-by-Isomorphic-Hetero-Source-Planning-Model" class="headerlink" title="15. DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model"></a>15. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/DistillDrive__End-to-End_Multi-Mode_Autonomous_Driving_Distillation_by_Isomorphic_Hetero-Source_Plan.pdf">DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model</a></h3><p><em>教育背景：</em> East China University of Science and Technology</p><p>该论文提出DistillDrive，一种结合知识蒸馏、强化学习和生成模型的端到端多模态自动驾驶框架。方法以结构化场景规划模型为教师，通过多模实例监督提升学生模型的运动特征学习，并引入强化学习优化状态-决策映射，利用生成模型实现运动分布特征的隐空间交互。实验结果显示，DistillDrive在nuScenes和NAVSIM数据集上较基线模型碰撞率下降50%，闭环性能提升3分，有效增强了自动驾驶计划的多样性和安全性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="16-Towards-Embodied-Agentic-AI-Review-and-Classification-of-LLM-and-VLM-Driven-Robot-Autonomy-and-Interaction"><a href="#16-Towards-Embodied-Agentic-AI-Review-and-Classification-of-LLM-and-VLM-Driven-Robot-Autonomy-and-Interaction" class="headerlink" title="16. Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction"></a>16. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Embodied_Agentic_AI__Review_and_Classification_of_LLM-_and_VLM-Driven_Robot_Autonomy_and_Int.pdf">Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction</a></h3><p><em>教育背景：</em> University of Turku</p><p>该论文综述了大语言模型（LLM）和视觉语言模型（VLM）在机器人自主性和人机交互中的集成方法，提出了四类集成范式（协议、接口、编排和嵌入）及多种智能体角色的分类体系。结论认为，Agentic AI系统作为用户与机器人间的智能中介，提升了机器人灵活性和可扩展性，并展望了未来在实际部署、记忆、安全性和高阶智能等方面的研究挑战。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="17-Driver-Assistant-Persuading-Drivers-to-Adjust-Secondary-Tasks-Using-Large-Language-Models"><a href="#17-Driver-Assistant-Persuading-Drivers-to-Adjust-Secondary-Tasks-Using-Large-Language-Models" class="headerlink" title="17. Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models"></a>17. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Driver_Assistant__Persuading_Drivers_to_Adjust_Secondary_Tasks_Using_Large_Language_Models.pdf">Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models</a></h3><p><em>教育背景：</em> Zhejiang University</p><p>本文提出一种基于大语言模型（LLM）的驾驶辅助工具，通过道路风险评估与眼动追踪及时判断并生成人性化劝导内容，引导驾驶员在L3自动驾驶中合理分配注意力。实验结果显示，该系统能够有效降低驾驶员分心任务数量和认知负荷，提升驾驶安全性及体验。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="18-EndoMatcher-Generalizable-Endoscopic-Image-Matcher-via-Multi-Domain-Pre-training-for-Robot-Assisted-Surgery"><a href="#18-EndoMatcher-Generalizable-Endoscopic-Image-Matcher-via-Multi-Domain-Pre-training-for-Robot-Assisted-Surgery" class="headerlink" title="18. EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery"></a>18. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EndoMatcher__Generalizable_Endoscopic_Image_Matcher_via_Multi-Domain_Pre-training_for_Robot-Assisted.pdf">EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery</a></h3><p><em>教育背景：</em> Institute of Automation, Chinese Academy of Sciences</p><p>该论文提出了EndoMatcher，一种基于双分支Vision Transformer的通用内镜图像匹配网络，通过多域大规模数据集Endo-Mix6的预训练和渐进式多目标优化实现对弱纹理、视角大变化等挑战场景下的鲁棒密集匹配。结果显示，EndoMatcher在多个零样本内镜数据集上大幅提升匹配数量和精度，实现了跨器官和成像条件的高效泛化。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="19-GhostShell-Streaming-LLM-Function-Calls-for-Concurrent-Embodied-Programming"><a href="#19-GhostShell-Streaming-LLM-Function-Calls-for-Concurrent-Embodied-Programming" class="headerlink" title="19. GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming"></a>19. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/GhostShell__Streaming_LLM_Function_Calls_for_Concurrent_Embodied_Programming.pdf">GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming</a></h3><p><em>教育背景：</em> Leapwatt Robotics</p><p>本文提出GhostShell，一种利用大语言模型（LLM）实现机器人多通道并发行为编程的系统，核心包括流式XML函数令牌解析、动态接口映射和多通道同步&#x2F;异步调度，支持机器人各部件的并行与串行指令执行。实验在COCO机器人上，通过34项任务验证方法有效性，在Claude-4-Sonnet模型上达到0.85的行为正确率，响应速度比传统函数调用快66倍，适用于长时多模态任务，展现出高度通用性和实时性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="20-Advanced-Hybrid-Transformer–LSTM-Technique-with-Attention-and-TS-Mixer-for-Drilling-Rate-of-Penetration-Prediction"><a href="#20-Advanced-Hybrid-Transformer–LSTM-Technique-with-Attention-and-TS-Mixer-for-Drilling-Rate-of-Penetration-Prediction" class="headerlink" title="20. Advanced Hybrid Transformer–LSTM Technique with Attention and TS-Mixer for Drilling Rate of Penetration Prediction"></a>20. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Advanced_Hybrid_Transformer_LSTM_Technique_with_Attention_and_TS_Mixer_for_Drilling_Rate_of_Penetrat.pdf">Advanced Hybrid Transformer–LSTM Technique with Attention and TS-Mixer for Drilling Rate of Penetration Prediction</a></h3><p><em>教育背景：</em> University of Engineering and Applied Sciences (UEAS), Swat, Pakistan</p><p>该论文提出一种结合LSTM、Transformer编码器、TS-Mixer模块和注意力机制的混合深度学习架构，用于钻井穿透率（ROP）预测。方法包括数据清洗、特征标准化、架构设计（LSTM捕捉时序依赖，Transformer建模全局关系，TS-Mixer融合静态特征，注意力机制动态分配特征权重），并在真实钻井数据集上进行评估。结果显示，所提模型在多项回归指标上显著优于传统方法，R²达0.9991，MAPE低至1.1572%，具备强泛化能力和实时预测价值。结论是该混合模型能稳定并精确预测ROP，显著提升钻井优化系统的智能化与经济性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="21-Learning-to-See-and-Act-Task-Aware-View-Planning-for-Robotic-Manipulation"><a href="#21-Learning-to-See-and-Act-Task-Aware-View-Planning-for-Robotic-Manipulation" class="headerlink" title="21. Learning to See and Act: Task-Aware View Planning for Robotic Manipulation"></a>21. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Learning_to_See_and_Act__Task-Aware_View_Planning_for_Robotic_Manipulation.pdf">Learning to See and Act: Task-Aware View Planning for Robotic Manipulation</a></h3><p><em>教育背景：</em> School of Computer Science and Engineering, Sun Yat-sen University</p><p>本文提出了Task-Aware View Planning (TAVP) 框架，将主动多视角探索与任务感知特征提取相结合，提升机器人多任务操作的鲁棒性与泛化能力。通过多视角探索策略和任务感知专家混合（TaskMoE）视觉编码器，TAVP显著提升了在RLBench等多任务操作环境中的准确率和鲁棒性，能有效克服遮挡问题并提升对新任务的泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="22-PhysPatch-A-Physically-Realizable-and-Transferable-Adversarial-Patch-Attack-for-Multimodal-Large-Language-Models-based-Autonomous-Driving-Systems"><a href="#22-PhysPatch-A-Physically-Realizable-and-Transferable-Adversarial-Patch-Attack-for-Multimodal-Large-Language-Models-based-Autonomous-Driving-Systems" class="headerlink" title="22. PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems"></a>22. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/PhysPatch__A_Physically_Realizable_and_Transferable_Adversarial_Patch_Attack_for_Multimodal_Large_La.pdf">PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems</a></h3><p><em>教育背景：</em> School of Software Engineering, Xi’an Jiaotong University</p><p>本文提出PhysPatch，一种针对多模态大语言模型(MLLMs)驱动的自动驾驶系统的物理可实现且高迁移性对抗补丁攻击框架。方法上，PhysPatch通过语义感知的掩码初始化、SVD分解的局部特征对齐损失以及补丁引导的裁剪-缩放策略，实现了补丁位置、形状和内容的联合优化，并采用自适应势场掩码细化提升物理可部署性。实验证明，PhysPatch在多种开源、商用和推理型MLLMs上均大幅优于现有方法，能有效误导自动驾驶系统感知与规划输出，并兼具高物理可部署性与对抗性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="23-Examining-the-legibility-of-humanoid-robot-arm-movements-in-a-pointing-task"><a href="#23-Examining-the-legibility-of-humanoid-robot-arm-movements-in-a-pointing-task" class="headerlink" title="23. Examining the legibility of humanoid robot arm movements in a pointing task"></a>23. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Examining_the_legibility_of_humanoid_robot_arm_movements_in_a_pointing_task.pdf">Examining the legibility of humanoid robot arm movements in a pointing task</a></h3><p><em>教育背景：</em> Comenius University Bratislava</p><p>该论文设计了基于NICO仿人机器人，结合凝视与指向手势的实验，考察人在观察截断机器人手臂运动时对目标预测的准确性和反应速度。结果支持多模态优势和眼动优先假设，表明凝视和指向的组合能显著提高意图识别的准确性，凝视信号可加速人类决策，为人机交互中运动设计提供理论依据。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="24-A-Study-of-the-Framework-and-Real-World-Applications-of-Language-Embedding-for-3D-Scene-Understanding"><a href="#24-A-Study-of-the-Framework-and-Real-World-Applications-of-Language-Embedding-for-3D-Scene-Understanding" class="headerlink" title="24. A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding"></a>24. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Study_of_the_Framework_and_Real-World_Applications_of_Language_Embedding_for_3D_Scene_Understandin.pdf">A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding</a></h3><p><em>教育背景：</em> University of Victoria</p><p>本论文系统综述了将大语言模型（LLMs）及语言嵌入与3D高斯投影（Gaussian Splatting, 3DGS）结合以实现三维场景理解的方法和应用。其方法梳理了3DGS的理论基础、语言特征与视觉模型的集成策略，以及在机器人、虚拟现实、自动驾驶等真实环境中的应用实例。结论指出，尽管这些技术推动了3D场景的语义理解和机器人智能交互，但仍面临计算消耗高、语义泛化能力有限及实时交互工具不足等挑战，未来需在高效可扩展和跨场景泛化等方面持续突破。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="25-Will-You-Be-Aware-Eye-Tracking–Based-Modeling-of-Situational-Awareness-in-Augmented-Reality"><a href="#25-Will-You-Be-Aware-Eye-Tracking–Based-Modeling-of-Situational-Awareness-in-Augmented-Reality" class="headerlink" title="25. Will You Be Aware? Eye Tracking–Based Modeling of Situational Awareness in Augmented Reality"></a>25. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Will_You_Be_Aware__Eye_Tracking-Based_Modeling_of_Situational_Awareness_in_Augmented_Reality.pdf">Will You Be Aware? Eye Tracking–Based Modeling of Situational Awareness in Augmented Reality</a></h3><p><em>教育背景：</em> Duke University</p><p>该论文提出了一种基于眼动追踪的情境意识（SA）建模方法，针对AR辅助心肺复苏（CPR）任务，开发了实时指导的AR应用并通过模拟突发事件收集用户眼动数据。提出了FixGraphPool图神经网络，将注视点和扫视事件结构化为时空图，实现了对用户SA水平的预测，准确率达83%，优于传统机器学习和先进时序模型。结论显示眼动追踪可有效建模AR环境下的SA，并为安全关键场景的AR系统设计提供了指导。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="26-From-Canada-to-Japan-How-10-000-km-Affect-User-Perception-in-Robot-Teleoperation"><a href="#26-From-Canada-to-Japan-How-10-000-km-Affect-User-Perception-in-Robot-Teleoperation" class="headerlink" title="26. From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation"></a>26. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/From_Canada_to_Japan__How_10,000_km_Affect_User_Perception_in_Robot_Teleoperation.pdf">From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation</a></h3><p><em>教育背景：</em> Tokyo University of Science</p><p>本论文研究了长距离机器人远程操作（RTo）对用户感知的影响，特别关注其在老年护理领域的应用。作者设计了基于ROS和Unity的远程操控系统及用户评估协议，通过问卷和交互实验比较本地与跨国（加拿大至日本）机器人操作体验。结果显示，无论是操作负担还是可用性，用户对远程和本地机器人无显著差异，表明远程机器人可作为本地控制的可行替代方案。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="27-Analyzing-the-Impact-of-Multimodal-Perception-on-Sample-Complexity-and-Optimization-Landscapes-in-Imitation-Learning"><a href="#27-Analyzing-the-Impact-of-Multimodal-Perception-on-Sample-Complexity-and-Optimization-Landscapes-in-Imitation-Learning" class="headerlink" title="27. Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning"></a>27. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Analyzing_the_Impact_of_Multimodal_Perception_on_Sample_Complexity_and_Optimization_Landscapes_in_Im.pdf">Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning</a></h3><p><em>教育背景：</em> University of California, Berkeley</p><p>本文基于统计学习理论系统分析了多模态感知（RGB-D、语言、位姿）在机器人模仿学习中的作用，采用Rademacher复杂度、PAC学习等理论工具，揭示多模态集成可显著降低样本复杂度并改善优化景观。结论指出，合理融合多模态信息能在机器人操作任务中实现更强泛化能力、更高数据效率和更稳定训练表现，理论与仿真实验均得到了验证。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="28-Hierarchical-Deep-Deterministic-Policy-Gradient-for-Autonomous-Maze-Navigation-of-Mobile-Robots"><a href="#28-Hierarchical-Deep-Deterministic-Policy-Gradient-for-Autonomous-Maze-Navigation-of-Mobile-Robots" class="headerlink" title="28. Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots"></a>28. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Hierarchical_Deep_Deterministic_Policy_Gradient_for_Autonomous_Maze_Navigation_of_Mobile_Robots.pdf">Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots</a></h3><p><em>教育背景：</em> Universiti Sains Malaysia</p><p>本文提出了一种高效的分层深度确定性策略梯度（HDDPG）算法用于移动机器人自主迷宫导航。该方法通过高层策略生成中间子目标并低层策略实现具体动作，结合自适应参数噪声、离策略修正、梯度裁剪和奖励函数重塑，显著提升了探索效率、路径规划和训练稳定性。在ROS和Gazebo仿真平台上的实验表明，HDDPG在多个迷宫导航任务中将成功率提升至少56.59%，平均奖励提升519.03，明显优于标准DDPG及其变体，验证了分层结构和各项优化的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h2 id="cv">Computer Vision</h2><h3 id="29-GAP-Gaussianize-Any-Point-Clouds-with-Text-Guidance"><a href="#29-GAP-Gaussianize-Any-Point-Clouds-with-Text-Guidance" class="headerlink" title="29. GAP: Gaussianize Any Point Clouds with Text Guidance"></a>29. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/GAP__Gaussianize_Any_Point_Clouds_with_Text_Guidance.pdf">GAP: Gaussianize Any Point Clouds with Text Guidance</a></h3><p><em>教育背景：</em> Tsinghua University</p><p>本文提出GAP方法，将无颜色的3D点云在文本引导下转化为高保真3D高斯表示。其核心包括多视角优化框架，结合深度感知的图像扩散模型和表面锚定机制保证几何精度，并引入基于扩散的高斯补全策略修复难以观测区域。实验结果显示，GAP在多种数据集和场景下均优于现有方法，能高效生成结构精确、外观多样的3D高斯体。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="30-Uni-CoT-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision"><a href="#30-Uni-CoT-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision" class="headerlink" title="30. Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision"></a>30. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Uni-cot__Towards_Unified_Chain-of-Thought_Reasoning_Across_Text_and_Vision.pdf">Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision</a></h3><p><em>教育背景：</em> Shanghai Academy of AI for Science</p><p>该论文提出了Uni-CoT，一种统一的链式思维(CoT)框架，采用宏观任务规划与微观子任务执行的双层层次化推理结构，将多模态推理任务分解为高层规划和马尔可夫决策过程(MDP)建模的子任务，通过结构化训练范式实现高效监督和偏好微调。实验表明，Uni-CoT在多项推理驱动的图像生成和编辑任务中表现出色，在WISE、RISE和KRIS等基准上取得了领先性能和良好解释性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="31-DART-Dual-Adaptive-Refinement-Transfer-for-Open-Vocabulary-Multi-Label-Recognition"><a href="#31-DART-Dual-Adaptive-Refinement-Transfer-for-Open-Vocabulary-Multi-Label-Recognition" class="headerlink" title="31. DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition"></a>31. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/DART__Dual_Adaptive_Refinement_Transfer_for_Open-Vocabulary_Multi-Label_Recognition.pdf">DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition</a></h3><p><em>教育背景：</em> Sun Yat-sen University</p><p>本论文提出DART框架，通过在冻结的视觉-语言预训练(VLP)模型基础上，集成自适应细化模块(ARM)与弱监督Patch选择损失(WPS)，实现对局部视觉特征的精细定位，同时引入基于大语言模型(LLM)挖掘的类关系图(CRG)指导自适应转移模块(ATM)，以图注意力网络完成跨类别信息传递和多模态融合。实验结果显示，DART在NUS-WIDE、MS-COCO和Open Images等多项开放词表多标签识别基准上取得了新的SOTA性能，验证了方法的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="32-χ-Symmetry-Understanding-of-3D-Shapes-via-Chirality-Disentanglement"><a href="#32-χ-Symmetry-Understanding-of-3D-Shapes-via-Chirality-Disentanglement" class="headerlink" title="32. χ: Symmetry Understanding of 3D Shapes via Chirality Disentanglement"></a>32. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Symmetry_Understanding_of_3D_Shapes_via_Chirality_Disentanglement.pdf">χ: Symmetry Understanding of 3D Shapes via Chirality Disentanglement</a></h3><p><em>教育背景：</em> University of Bonn</p><p>本文提出了一种基于Diff3F框架的无监督管道，通过从2D基础视觉模型（DINO-V2和StableDiffusion）提取特征，实现3D形状顶点手性（chirality）信息的解耦与建模。新特征能有效区分3D形状的左右部分，解决了传统特征在形状匹配和分割时的左右歧义，在多数据集和任务中表现出色，且对局部或各向异性形状具有良好鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="33-SMOL-MapSeg-Show-Me-One-Label"><a href="#33-SMOL-MapSeg-Show-Me-One-Label" class="headerlink" title="33. SMOL-MapSeg: Show Me One Label"></a>33. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SMOL-MapSeg__Show_Me_One_Label.pdf">SMOL-MapSeg: Show Me One Label</a></h3><p><em>教育背景：</em> Leibniz University Hannover</p><p>本文提出了一种新颖的按需声明式（OND）知识驱动的提示方法，用于指导基础分割模型（以SAM为基础）在历史地图上进行语义分割。方法通过源图像-标签对作为显式提示，结合新设计的Prompt Encoder，对历史地图进行单样本细粒度分割，且支持新类别的少样本自适应。实验表明，SMOL-MapSeg在平均IoU上优于UNet基线模型，对新类别有较好泛化能力，但对局部特征不明显的类别如白色水域分割有局限。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="34-How-and-Why-Taming-Flow-Matching-for-Unsupervised-Anomaly-Detection-and-Localization"><a href="#34-How-and-Why-Taming-Flow-Matching-for-Unsupervised-Anomaly-Detection-and-Localization" class="headerlink" title="34. How and Why: Taming Flow Matching for Unsupervised Anomaly Detection and Localization"></a>34. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/How_and_Why__Taming_Flow_Matching_for_Unsupervised_Anomaly_Detection_and_Localization.pdf">How and Why: Taming Flow Matching for Unsupervised Anomaly Detection and Localization</a></h3><p><em>教育背景：</em> University of Electronic Science and Technology of China</p><p>本文提出了基于流匹配（Flow Matching, FM）及其变体“Worst Transport Flow Matching”（WT-Flow）的无监督异常检测与定位新范式，通过理论和实证分析揭示了传统流方法在高维空间中的不可逆性和表达能力限制，并构建了非概率性路径控制机制。WT-Flow方法通过对样本归一化和潜在井构建，实现了对异常样本的有效分离，在MVTec工业视觉异常检测数据集上达到了单尺度下的SOTA性能，显著提升了检测效率和精度。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="35-Explaining-Similarity-in-Vision–Language-Encoders-with-Weighted-Banzhaf-Interactions"><a href="#35-Explaining-Similarity-in-Vision–Language-Encoders-with-Weighted-Banzhaf-Interactions" class="headerlink" title="35. Explaining Similarity in Vision–Language Encoders with Weighted Banzhaf Interactions"></a>35. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Explaining_Similarity_in_Vision-Language_Encoders_with_Weighted_Banzhaf_Interactions.pdf">Explaining Similarity in Vision–Language Encoders with Weighted Banzhaf Interactions</a></h3><p><em>教育背景：</em> University of Warsaw</p><p>本文提出了FIXLIP，一种基于加权Banzhaf交互指数的视觉-语言编码器解释方法，通过游戏理论建模和高效的跨模态采样策略，能够分解图像-文本对在编码器中的相似性得分。实验结果表明，FIXLIP在MS COCO和ImageNet-1k等基准上比一阶归因方法更准确、可扩展，并支持模型间对比分析。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="36-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation"><a href="#36-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation" class="headerlink" title="36. UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation"></a>36. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/UNCAGE__Contrastive_Attention_Guidance_for_Masked_Generative_Transformers_in_Text-to-Image_Generatio.pdf">UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation</a></h3><p><em>教育背景：</em> Seoul National University</p><p>该论文提出UNCAGE（Unmasking with Contrastive Attention Guidance），为Masked Generative Transformers（MGTs）在文本到图像生成中的多物体和属性绑定问题提供首个训练免方法。UNCAGE利用注意力图，通过正负对比机制指导token解遮顺序，优先解锁能清晰代表单一对象的token，提升合成一致性且推理开销极小。实验表明UNCAGE在多基准和多指标下均优于现有方法，提升了文本-图像对齐和人类偏好。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="37-MagicHOI-Leveraging-3D-Priors-for-Accurate-Hand-object-Reconstruction-from-Short-Monocular-Video-Clips"><a href="#37-MagicHOI-Leveraging-3D-Priors-for-Accurate-Hand-object-Reconstruction-from-Short-Monocular-Video-Clips" class="headerlink" title="37. MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips"></a>37. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MagicHOI__Leveraging_3D_Priors_for_Accurate_Hand-object_Reconstruction_from_Short_Monocular_Video_Cl.pdf">MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips</a></h3><p><em>教育背景：</em> The Hong Kong University of Science and Technology (Guangzhou)</p><p>MagicHOI提出了一种结合结构光恢复、NVS扩散模型和可见性加权策略的方法，从短时单目视频中实现准确的手与物体三维重建，包括处理遮挡区域。实验证明，该方法在有限视角和遮挡严重的场景下，重建质量和手物体空间关系均优于当前主流方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="38-F2PASeg-Feature-Fusion-for-Pituitary-Anatomy-Segmentation-in-Endoscopic-Surgery"><a href="#38-F2PASeg-Feature-Fusion-for-Pituitary-Anatomy-Segmentation-in-Endoscopic-Surgery" class="headerlink" title="38. F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery"></a>38. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/F2PASeg__Feature_Fusion_for_Pituitary_Anatomy_Segmentation_in_Endoscopic_Surgery.pdf">F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery</a></h3><p><em>教育背景：</em> Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science &amp; Innovation, Chinese Academy of Sciences</p><p>该论文提出了F2PASeg方法，通过在视频级语义分割中引入特征融合模块和LoRA分支，结合高分辨率图像特征与深层语义信息，实现了对复杂术中场景下垂体解剖结构的高效分割。实验表明，F2PASeg在新构建的PAS数据集上取得了优于现有方法的精度和实时性，有效提升了手术关键结构的分割准确率并满足术中应用需求。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="39-Smoothing-Slot-Attention-Iterations-and-Recurrences"><a href="#39-Smoothing-Slot-Attention-Iterations-and-Recurrences" class="headerlink" title="39. Smoothing Slot Attention Iterations and Recurrences"></a>39. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Smoothing_Slot_Attention_Iterations_and_Recurrences.pdf">Smoothing Slot Attention Iterations and Recurrences</a></h3><p><em>教育背景：</em> Aalto University</p><p>该论文提出SmoothSA方法，通过预热冷启动查询和区分不同帧的聚合变换，解决Slot Attention在图像和视频对象聚合中的冷启动和变换同质性问题。实验表明，SmoothSA在对象发现、识别和视觉问答任务上取得了新的SOTA表现，显著提升了对象表示质量。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="40-CT-GRAPH-Hierarchical-Graph-Attention-Network-for-Anatomy-Guided-CT-Report-Generation"><a href="#40-CT-GRAPH-Hierarchical-Graph-Attention-Network-for-Anatomy-Guided-CT-Report-Generation" class="headerlink" title="40. CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT Report Generation"></a>40. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/CT-GRAPH__Hierarchical_Graph_Attention_Network_for_Anatomy-Guided_CT_Report_Generation.pdf">CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT Report Generation</a></h3><p><em>教育背景：</em> Institute for AI in Medicine (IKIM), University Hospital Essen</p><p>论文提出了CT-GRAPH，一种结合预训练3D医疗特征编码器和解剖结构分层图注意力网络的方法，通过解剖掩码提取细粒度器官及全局特征，并构建多层次解剖学图，利用图注意力机制进行特征聚合，最终将结构化特征输入大语言模型生成详细医学报告。实验表明，该方法在CT报告生成任务上显著提升了临床实体F1分数（提升7.9%），在细粒度和全局诊断上均优于现有方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="41-PriorRG-Prior-Guided-Contrastive-Pre-training-and-Coarse-to-Fine-Decoding-for-Chest-X-ray-Report-Generation"><a href="#41-PriorRG-Prior-Guided-Contrastive-Pre-training-and-Coarse-to-Fine-Decoding-for-Chest-X-ray-Report-Generation" class="headerlink" title="41. PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation"></a>41. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/PriorRG__Prior-Guided_Contrastive_Pre-training_and_Coarse-to-Fine_Decoding_for_Chest_X-ray_Report_Ge.pdf">PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation</a></h3><p><em>教育背景：</em> Xidian University</p><p>PriorRG提出了一个针对胸部X光报告生成的两阶段框架，结合了患者特定的临床先验信息，包括临床背景和近期影像。其方法包括先验引导的对比预训练，提升图像与文本的语义对齐能力，以及先验感知的粗到细解码策略，逐步融合临床语境、疾病进展和分层视觉特征，显著提升生成报告的临床准确性与流畅度。实验结果表明，PriorRG在MIMIC-CXR及MIMIC-ABN等主流医学数据集上，报告生成和医学图像-文本检索任务均优于现有方法，具有更强的泛化能力和部署效率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="42-Textual-Inversion-for-Efficient-Adaptation-of-Open-Vocabulary-Object-Detectors-Without-Forgetting"><a href="#42-Textual-Inversion-for-Efficient-Adaptation-of-Open-Vocabulary-Object-Detectors-Without-Forgetting" class="headerlink" title="42. Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting"></a>42. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Textual_Inversion_for_Efficient_Adaptation_of_Open-Vocabulary_Object_Detectors_Without_Forgetting.pdf">Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting</a></h3><p><em>教育背景：</em> TNO, Intelligent Imaging, the Hague, the Netherlands</p><p>本文提出了一种将文本倒置（Textual Inversion, TI）方法应用于视觉-语言模型（VLM）开放词汇目标检测的新方法。通过仅优化新词嵌入向量、冻结原有模型权重，实现以极少样本（如3张图片）高效扩展检测词汇，同时显著减缓遗忘问题，保留原模型零样本泛化能力。实验表明TI在few-shot和fine-grained检测任务中性能接近甚至超越常规提示微调（Prompt Tuning）方法，且可将已学类别迁移到新领域，显著减少对全模型微调和原始数据的依赖。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="43-mKG-RAG-Multimodal-Knowledge-Graph-Enhanced-RAG-for-Visual-Question-Answering"><a href="#43-mKG-RAG-Multimodal-Knowledge-Graph-Enhanced-RAG-for-Visual-Question-Answering" class="headerlink" title="43. mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering"></a>43. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/mKG-RAG__Multimodal_Knowledge_Graph-Enhanced_RAG_for_Visual_Question_Answering.pdf">mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering</a></h3><p><em>教育背景：</em> The Hong Kong Polytechnic University</p><p>本文提出mKG-RAG，一种结合检索增强生成（RAG）和多模态知识图谱（KG）的新框架，用于提升多模态大语言模型（MLLMs）在知识密集型视觉问答（VQA）任务中的推理能力。方法包括利用MLLM进行多模态文档的实体和关系抽取，构建结构化知识图谱，并通过双阶段检索（向量检索和图检索）精准筛选相关知识，显著提升VQA准确率，实验结果优于现有方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="44-SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion"><a href="#44-SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion" class="headerlink" title="44. SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion"></a>44. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SGDFuse__SAM-Guided_Diffusion_for_High-Fidelity_Infrared_and_Visible_Image_Fusion.pdf">SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion</a></h3><p><em>教育背景：</em> Shandong Technology and Business University</p><p>该论文提出了一种名为SGDFuse的红外与可见光图像融合方法，通过Segment Anything Model (SAM)生成高质量语义掩码，引导条件扩散模型，实现高保真、语义一致的图像融合。方法采用两阶段架构：第一阶段多模态特征融合，第二阶段结合语义掩码和扩散生成进行结构细化和语义增强，显著提升融合图像在目标检测和语义分割等下游任务中的表现。实验结果显示SGDFuse在主流数据集和多项指标上均优于现有方法，结论表明该方法能有效统一语义一致性与视觉质量。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="45-CF3-Compact-and-Fast-3D-Feature-Fields"><a href="#45-CF3-Compact-and-Fast-3D-Feature-Fields" class="headerlink" title="45. CF3: Compact and Fast 3D Feature Fields"></a>45. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/CF3__Compact_and_Fast_3D_Feature_Fields.pdf">CF3: Compact and Fast 3D Feature Fields</a></h3><p><em>教育背景：</em> Seoul National University</p><p>该论文提出CF3，一种从多视角2D视觉基础模型特征通过加权融合提升到3D高斯特征场的方法，并在每个高斯点上训练自适应自动编码器进行特征压缩，再通过自适应稀疏化策略合并和剪枝冗余高斯点，大幅降低存储和计算成本。实验表明，CF3在多种3D场景分割与定位任务中，能以仅5%高斯数量达到与主流方法相当的性能，且渲染与查询速度显著提升。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="46-A-Study-of-Gender-Classification-Techniques-Based-on-Iris-Images-A-Deep-Survey-and-Analysis"><a href="#46-A-Study-of-Gender-Classification-Techniques-Based-on-Iris-Images-A-Deep-Survey-and-Analysis" class="headerlink" title="46. A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis"></a>46. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Study_of_Gender_Classification_Techniques_Based_on_Iris_Images__A_Deep_Survey_and_Analysis.pdf">A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis</a></h3><p><em>教育背景：</em> Technical College of Informatics Akre, Duhok Polytechnic University</p><p>本文系统综述了基于虹膜图像的性别分类技术，涵盖了传统方法与深度学习（如CNN）在图像采集、分割、特征提取和分类等流程中的应用，并对主流算法（Gabor滤波、DWT、LBP、PCA等）及其在实际数据集上的表现进行比较分析。结论认为，深度学习方法在特征自动提取和分类准确率方面较传统方法有显著提升，但仍面临图像质量、特征选择等挑战，未来可通过优化特征选择与分类器融合进一步提高实用性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="47-Coarse-to-Fine-Joint-Registration-of-MR-and-Ultrasound-Images-via-Imaging-Style-Transfer"><a href="#47-Coarse-to-Fine-Joint-Registration-of-MR-and-Ultrasound-Images-via-Imaging-Style-Transfer" class="headerlink" title="47. Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer"></a>47. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Coarse-to-Fine_Joint_Registration_of_MR_and_Ultrasound_Images_via_Imaging_Style_Transfer.pdf">Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer</a></h3><p><em>教育背景：</em> University of Electronic Science and Technology of China</p><p>本文提出了一种结合3D CycleGAN无监督风格迁移、分层块匹配仿射变换与SynthMorph局部形变估计的粗到细多阶段MR与超声图像配准方法。方法显著增强了MR与US图像的一致性，但在部分情况下会导致目标配准误差（TRE）增加，因此未来需研究更一致的上下文描述指标。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="48-ReasoningTrack-Chain-of-Thought-Reasoning-for-Long-term-Vision-Language-Tracking"><a href="#48-ReasoningTrack-Chain-of-Thought-Reasoning-for-Long-term-Vision-Language-Tracking" class="headerlink" title="48. ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking"></a>48. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/ReasoningTrack__Chain-of-Thought_Reasoning_for_Long-term_Vision-Language_Tracking.pdf">ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking</a></h3><p><em>教育背景：</em> Anhui University</p><p>该论文提出了ReasoningTrack，一种基于链式推理（Chain-of-Thought, CoT）的视觉-语言长期跟踪框架，利用预训练大规模视觉语言模型Qwen2.5-VL，通过两阶段的有监督微调（SFT）和强化学习（GRPO）优化模型的推理与语言生成能力。方法结合动态语言描述和视觉特征，通过推理链不断更新目标描述，并在统一的跟踪骨干网络中实现高鲁棒性跟踪。在多个数据集上的实验显示，ReasoningTrack显著提升了跟踪精度，并提出了包含200个视频序列的TNLLT长视频视觉语言跟踪数据集，为领域后续研究奠定基础。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="49-SPA-Generalized-Graph-Spectral-Alignment-for-Versatile-Domain-Adaptation"><a href="#49-SPA-Generalized-Graph-Spectral-Alignment-for-Versatile-Domain-Adaptation" class="headerlink" title="49. SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation"></a>49. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SPA++__Generalized_Graph_Spectral_Alignment_for_Versatile_Domain_Adaptation.pdf">SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation</a></h3><p><em>教育背景：</em> Zhejiang University</p><p>SPA++提出了一种基于图谱对齐和邻居感知传播机制的通用域自适应方法，通过在特征空间进行图谱对齐、利用邻居信息提升目标域判别能力，并结合数据增强和一致性正则化，适配多种复杂域自适应场景（如UDA、SSDA、MSDA、MTDA、长尾&#x2F;子群分布）。实验证明SPA++在多种视觉域自适应基准上的表现优于现有方法，具有更强的鲁棒性和适应性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="50-Wavelet-Guided-Dual-Frequency-Encoding-for-Remote-Sensing-Change-Detection"><a href="#50-Wavelet-Guided-Dual-Frequency-Encoding-for-Remote-Sensing-Change-Detection" class="headerlink" title="50. Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection"></a>50. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Wavelet-Guided_Dual-Frequency_Encoding_for_Remote_Sensing_Change_Detection.pdf">Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection</a></h3><p><em>教育背景：</em> Shandong Technology and Business University</p><p>该论文提出了Wavelet-Guided Dual-Frequency Encoding (WGDF)方法，通过在输入阶段利用离散小波变换（DWT）将遥感影像分解为高低频分支，分别提取局部细节和全局语义：高频分支利用DFFE和FDID模块增强边缘变化建模，低频分支结合Transformer和PCDM捕捉全局语义变化，最终融合获得变化检测结果。实验在多个遥感数据集上验证了WGDF在复杂场景下的边缘消歧和检测准确性优于现有方法，具有较高的鲁棒性和效率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="51-RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding"><a href="#51-RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding" class="headerlink" title="51. RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding"></a>51. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/RegionMed-CLIP__A_Region-Aware_Multimodal_Contrastive_Learning_Pre-trained_Model_for_Medical_Image_U.pdf">RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding</a></h3><p><em>教育背景：</em> Anhui Polytechnic University</p><p>该论文提出RegionMed-CLIP，一种结合全局与局部特征的区域感知多模态对比学习框架，核心包括ROI（感兴趣区域）处理器和分阶段训练策略，有效融合医学影像的细粒度病灶信息与语义文本。通过自建高质量区域标注的MedRegion-500k数据集，实验在零样本分类、图文检索和医学VQA任务上均显著优于现有SOTA模型，证明区域感知对比预训练对于医学图像理解的关键作用。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="52-ArbiViewGen-Controllable-Arbitrary-Viewpoint-Camera-Data-Generation-for-Autonomous-Driving-via-Stable-Diffusion-Models"><a href="#52-ArbiViewGen-Controllable-Arbitrary-Viewpoint-Camera-Data-Generation-for-Autonomous-Driving-via-Stable-Diffusion-Models" class="headerlink" title="52. ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models"></a>52. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/ArbiViewGen__Controllable_Arbitrary_Viewpoint_Camera_Data_Generation_for_Autonomous_Driving_via_Stab.pdf">ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models</a></h3><p><em>教育背景：</em> Tsinghua University</p><p>本论文提出了ArbiViewGen，一种基于Stable Diffusion模型的可控任意视角相机图像生成框架，专为自动驾驶场景下多车辆多摄像头系统设计。方法核心包括特征感知自适应视图拼接（FAVS）与跨视图一致性自监督学习（CVC-SSL）：FAVS通过分层几何与特征匹配生成高质量伪标签，CVC-SSL利用扩散模型实现闭环自监督训练，无需真实外插视角监督，仅依赖多摄像头图像和姿态信息。实验表明，该方法在nuScenes自动驾驶数据集上实现了高于现有方法的任意视角合成质量，显著提升了跨平台数据复用能力。结论指出，ArbiViewGen有效解决了缺乏外插视角监督的难题，提升了自动驾驶感知系统的适应性和鲁棒性，但在高度动态环境下细节还需提升。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="53-Segmenting-the-Complex-and-Irregular-in-Two-Phase-Flows-A-Real-World-Empirical-Study-with-SAM2"><a href="#53-Segmenting-the-Complex-and-Irregular-in-Two-Phase-Flows-A-Real-World-Empirical-Study-with-SAM2" class="headerlink" title="53. Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2"></a>53. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Segmenting_the_Complex_and_Irregular_in_Two-Phase_Flows__A_Real-World_Empirical_Study_with_SAM2.pdf">Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2</a></h3><p><em>教育背景：</em> Delft University of Technology</p><p>本论文将复杂气泡分割任务视为迁移学习问题，首次对Segment Anything Model (SAM v2.1)进行细致微调，并通过数据增强策略，实现在真实多相流场景下对高度不规则、非凸形状气泡的高精度分割。实验表明，仅用100张标注图像即可获得F1和Dice分数高达0.95，优于以往需要大规模数据训练的方法，特别在中、大气泡分割上表现突出，显著降低了人工标注成本。结论指出，该方法数据效率高，泛化性强，并公开了数据集和微调流程，助力多相流领域的后续研究。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="54-Beyond-Pixels-Medical-Image-Quality-Assessment-with-Implicit-Neural-Representations"><a href="#54-Beyond-Pixels-Medical-Image-Quality-Assessment-with-Implicit-Neural-Representations" class="headerlink" title="54. Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations"></a>54. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Beyond_Pixels__Medical_Image_Quality_Assessment_with_Implicit_Neural_Representations.pdf">Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations</a></h3><p><em>教育背景：</em> Istanbul Technical University</p><p>该论文提出使用基于SIREN的隐式神经表示（INR）对心脏MRI图像质量进行评估，通过两阶段流程：首先将2D切片拟合为INR参数，然后利用深度神经网络（如多层感知机、图神经网络、关系注意力Transformer等）直接在INR参数空间进行分类。实验表明，该方法在ACDC数据集上实现了优于传统像素级方法的准确率，并大幅减少了参数量，尤其Transformer（Probe&#x3D;32）模型在Good vs. Poor任务上达到94.4%准确率，展现了INR作为医学图像质量描述符的潜力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="55-X-MoGen-Unified-Motion-Generation-across-Humans-and-Animals"><a href="#55-X-MoGen-Unified-Motion-Generation-across-Humans-and-Animals" class="headerlink" title="55. X-MoGen: Unified Motion Generation across Humans and Animals"></a>55. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/X-MoGen__Unified_Motion_Generation_across_Humans_and_Animals.pdf">X-MoGen: Unified Motion Generation across Humans and Animals</a></h3><p><em>教育背景：</em> Zhejiang University</p><p>本论文提出X-MoGen，首次实现跨人类与动物统一的文本驱动动作生成框架。方法采用两阶段架构：第一阶段通过条件图变分自编码器（CGAE）学习T-pose骨架先验及自编码器（AE）编码动作至共享潜在空间，并引入形态损失正则化；第二阶段利用掩码变换器（Masked Transformer）结合扩散模型，根据文本描述生成动作嵌入，训练时形态一致性模块（MCM）保证结构合理性。作者还构建了包含115种物种、119k序列的UniMo4D统一数据集，支持跨物种联合建模。实验显示X-MoGen在已见及未见物种上均优于现有方法，生成动作真实且具良好泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="56-FedGIN-Federated-Learning-with-Dynamic-Global-Intensity-Non-linear-Augmentation-for-Organ-Segmentation-using-Multi-modal-Images"><a href="#56-FedGIN-Federated-Learning-with-Dynamic-Global-Intensity-Non-linear-Augmentation-for-Organ-Segmentation-using-Multi-modal-Images" class="headerlink" title="56. FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images"></a>56. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FedGIN__Federated_Learning_with_Dynamic_Global_Intensity_Non-linear_Augmentation_for_Organ_Segmentat.pdf">FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images</a></h3><p><em>教育背景：</em> Norwegian University of Science and Technology</p><p>该论文提出FedGIN，一种结合了全局强度非线性（GIN）增强的联邦学习框架，用于多模态医学图像（CT和MRI）在隐私保护下的器官分割。FedGIN在本地训练阶段通过GIN模块动态对CT和MRI图像进行强度域增强，促使模型学习模态无关的特征，并通过模型参数聚合实现跨机构的协同建模，显著提升了在复杂和低对比度器官上的分割表现。实验结果表明，FedGIN在多模态数据下接近中心化训练性能，特别是在胰腺和胆囊等难分割器官上优于单模态和无GIN增强的联邦学习基线，验证了其在隐私保护、多机构、多模态实际应用场景下的泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="57-PoseGen-In-Context-LoRA-Finetuning-for-Pose-Controllable-Long-Human-Video-Generation"><a href="#57-PoseGen-In-Context-LoRA-Finetuning-for-Pose-Controllable-Long-Human-Video-Generation" class="headerlink" title="57. PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation"></a>57. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/PoseGen__In-Context_LoRA_Finetuning_for_Pose-Controllable_Long_Human_Video_Generation.pdf">PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation</a></h3><p><em>教育背景：</em> Xiaoice</p><p>该论文提出PoseGen，一种高效且创新的人体视频生成框架，实现了从单张参考图像和动作姿态序列生成任意长度、高保真、姿态可控的人体视频。方法核心包括双重条件机制：用LoRA微调视频扩散模型，参考信息在token级注入、姿态信息在channel级注入，并采用分段生成与KV缓存共享技术，实现视频片段无缝拼接，保证身份一致性和时序连贯性。实验结果显示，PoseGen在身份保持、动作控制和长视频一致性方面均优于现有方法，且仅需极少训练数据。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="58-AdaFusion-Prompt-Guided-Inference-with-Adaptive-Fusion-of-Pathology-Foundation-Models"><a href="#58-AdaFusion-Prompt-Guided-Inference-with-Adaptive-Fusion-of-Pathology-Foundation-Models" class="headerlink" title="58. AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models"></a>58. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/AdaFusion__Prompt-Guided_Inference_with_Adaptive_Fusion_of_Pathology_Foundation_Models.pdf">AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models</a></h3><p><em>教育背景：</em> South China University of Technology</p><p>本文提出AdaFusion，一种面向病理图像分析的多模型特征自适应融合推理框架。其方法是在无需微调基础模型的前提下，采用轻量级注意力机制对多种预训练病理基础模型（PFMs）的低维特征进行压缩、对齐和基于组织表型上下文的动态加权融合。对治疗响应预测、肿瘤分级和空间基因表达推断等多项真实世界任务的实验表明，AdaFusion在分类和回归任务上均优于单一PFM和主流融合基线，并能可解释性地量化各模型对不同组织表型的贡献，提升鲁棒性和泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="59-AU-IQA-A-Benchmark-Dataset-for-Perceptual-Quality-Assessment-of-AI-Enhanced-User-Generated-Content"><a href="#59-AU-IQA-A-Benchmark-Dataset-for-Perceptual-Quality-Assessment-of-AI-Enhanced-User-Generated-Content" class="headerlink" title="59. AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content"></a>59. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/AU-IQA__A_Benchmark_Dataset_for_Perceptual_Quality_Assessment_of_AI-Enhanced_User-Generated_Content.pdf">AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content</a></h3><p><em>教育背景：</em> Shanghai Jiao Tong University</p><p>本文提出了AU-IQA，这是首个面向AI增强用户生成内容（AI-UGC）感知质量评估的基准数据集，涵盖超分辨率、低光增强和去噪三类AI增强方式，共4800张图像，并采用主观均值评分（MOS）进行标注。作者系统评测了传统IQA模型和大规模多模态模型在该数据集上的表现，发现以UGC为目标的模型效果最佳，现有模型在不同AI-UGC类型间表现差异大且不稳定，强调了开发专用AI-UGC质量评估模型的必要性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="60-Multimodal-Causal-Driven-Representation-Learning-for-Generalizable-Medical-Image-Segmentation"><a href="#60-Multimodal-Causal-Driven-Representation-Learning-for-Generalizable-Medical-Image-Segmentation" class="headerlink" title="60. Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation"></a>60. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Multimodal_Causal-Driven_Representation_Learning_for_Generalizable_Medical_Image_Segmentation.pdf">Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation</a></h3><p><em>教育背景：</em> Hong Kong Institute of Science &amp; Innovation</p><p>该论文提出了一种多模态因果驱动表征学习（MCDRL）框架，通过结合因果推断和视觉-语言模型（如CLIP），分两步实现医学图像分割的领域泛化：首先利用CLIP跨模态能力识别病灶区域并通过文本提示构建混杂因素字典，然后训练因果干预网络，消除领域特异变异影响，保留关键解剖结构信息。实验结果表明，MCDRL在多个医学图像分割数据集上优于现有方法，具备更强领域泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="61-Propagating-Sparse-Depth-via-Depth-Foundation-Model-for-Out-of-Distribution-Depth-Completion"><a href="#61-Propagating-Sparse-Depth-via-Depth-Foundation-Model-for-Out-of-Distribution-Depth-Completion" class="headerlink" title="61. Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion"></a>61. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Propagating_Sparse_Depth_via_Depth_Foundation_Model_for_Out-of-Distribution_Depth_Completion.pdf">Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion</a></h3><p><em>教育背景：</em> Dalian University of Technology</p><p>该论文提出了PSD深度补全框架，通过利用预训练的深度基础模型从RGB图像中提取结构和语义线索，引导稀疏深度向缺失区域传播，包含无参数的3D&#x2F;2D双空间传播模块及可学习的残差校正模块。实验显示，该方法在16个OOD场景下超越现有方法，具备极强泛化性和鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="62-Rotation-Equivariant-Arbitrary-scale-Image-Super-Resolution"><a href="#62-Rotation-Equivariant-Arbitrary-scale-Image-Super-Resolution" class="headerlink" title="62. Rotation Equivariant Arbitrary-scale Image Super-Resolution"></a>62. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Rotation_Equivariant_Arbitrary-scale_Image_Super-Resolution.pdf">Rotation Equivariant Arbitrary-scale Image Super-Resolution</a></h3><p><em>教育背景：</em> Xi’an Jiaotong University</p><p>本文提出了一种端到端旋转等变的任意尺度图像超分辨率(ASISR)方法，通过重新设计隐式神经表示(INR)和编码器模块，嵌入旋转等变性，理论分析了等变误差并实现了对现有ASISR方法的可插拔增强。实验结果表明，该方法能更好地保持图像几何结构及旋转对称性，有效提升超分辨率恢复质量和泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="63-Decoupling-Continual-Semantic-Segmentation"><a href="#63-Decoupling-Continual-Semantic-Segmentation" class="headerlink" title="63. Decoupling Continual Semantic Segmentation"></a>63. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Decoupling_Continual_Semantic_Segmentation.pdf">Decoupling Continual Semantic Segmentation</a></h3><p><em>教育背景：</em> Sun Yat-sen University</p><p>该论文提出了DecoupleCSS，一个用于持续语义分割（CSS）的两阶段框架，将类别感知检测与类别无关分割解耦。方法中，第一阶段利用预训练视觉-语言模型并结合LoRA适配器进行类别检测和定位提示生成，第二阶段用Segment Anything Model（SAM）实现精细分割，显著缓解遗忘问题并提升新旧知识平衡。实验表明，DecoupleCSS在多个常用CSS任务上取得了当前最优表现，具备较强的泛化能力，但推理时多任务切换带来一定延迟。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="64-Modeling-Rapid-Contextual-Learning-in-the-Visual-Cortex-with-Fast-Weight-Deep-Autoencoder-Networks"><a href="#64-Modeling-Rapid-Contextual-Learning-in-the-Visual-Cortex-with-Fast-Weight-Deep-Autoencoder-Networks" class="headerlink" title="64. Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks"></a>64. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Modeling_Rapid_Contextual_Learning_in_the_Visual_Cortex_with_Fast-Weight_Deep_Autoencoder_Networks.pdf">Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks</a></h3><p><em>教育背景：</em> Carnegie Mellon University</p><p>本文提出了一种基于Vision Transformer（ViT）自编码器的架构，利用低秩适配（LoRA）模块模拟大脑视觉皮层中的快速权重机制，实现熟悉上下文的快速学习。实验表明，熟悉训练促使早期层对全局上下文敏感，压缩任务无关的变异，同时LoRA进一步增强了这一效果，有助于实现更鲁棒的视觉表示和更清晰的图像分割。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="65-UGOD-Uncertainty-Guided-Differentiable-Opacity-and-Soft-Dropout-for-Enhanced-Sparse-View-3DGS"><a href="#65-UGOD-Uncertainty-Guided-Differentiable-Opacity-and-Soft-Dropout-for-Enhanced-Sparse-View-3DGS" class="headerlink" title="65. UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS"></a>65. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/UGOD__Uncertainty-Guided_Differentiable_Opacity_and_Soft_Dropout_for_Enhanced_Sparse-View_3DGS.pdf">UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS</a></h3><p><em>教育背景：</em> Manchester Metropolitan University</p><p>该论文提出了一种集成神经网络的不确定性学习模块，通过结合多层HashGrid编码，对3D Gaussian Splatting（3DGS）中的每个高斯进行视点相关的不确定性建模。方法利用学习到的不确定性分别指导高斯的不透明度调节及可微分软丢弃机制，有效减少稀疏视角场景下的过拟合。实验表明，该方法在多个3D重建数据集上均优于现有方法，能以更少的高斯实现更高的渲染质量。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="66-Open-world-Point-Cloud-Semantic-Segmentation-A-Human-in-the-loop-Framework"><a href="#66-Open-world-Point-Cloud-Semantic-Segmentation-A-Human-in-the-loop-Framework" class="headerlink" title="66. Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework"></a>66. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Open-world_Point_Cloud_Semantic_Segmentation__A_Human-in-the-loop_Framework.pdf">Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework</a></h3><p><em>教育背景：</em> Nanjing University of Science and Technology</p><p>该论文提出了HOW-Seg框架，实现了点云的开放世界语义分割，通过在查询样本上以稀疏人工标注为指导，直接构建类别原型并结合交互式原型消歧和基于CRF的原型标签分配机制，避免了跨样本分布偏移和原型偏差。实验表明，HOW-Seg无需额外支持样本或离线增量学习，仅靠极少量人工干预即可在S3DIS和ScanNetv2等数据集上显著优于现有方法，实现高质量的基类与新类点云分割。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="67-Laplacian-Analysis-Meets-Dynamics-Modelling-Gaussian-Splatting-for-4D-Reconstruction"><a href="#67-Laplacian-Analysis-Meets-Dynamics-Modelling-Gaussian-Splatting-for-4D-Reconstruction" class="headerlink" title="67. Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction"></a>67. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Laplacian_Analysis_Meets_Dynamics_Modelling__Gaussian_Splatting_for_4D_Reconstruction.pdf">Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction</a></h3><p><em>教育背景：</em> The Hong Kong University of Science and Technology (Guangzhou)</p><p>本论文提出一种动态3D高斯投影(3DGS)的混合显式-隐式编码框架，融合多尺度哈希编码与拉普拉斯频域分析，有效解耦并控制复杂动态场景中不同频率的运动分量。方法还包括增强的高斯动态属性和基于KDTree的自适应高斯划分策略，实现对高动态区域的精细建模和高效优化。实验结果在三个主流动态场景数据集上验证了所提方法在重建精度与运动一致性上的领先性能，优于现有动态重建方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h2 id="llm">Large Language Model</h2><h3 id="68-H-NET-Hierarchical-Dynamic-Chunking-for-Tokenizer-Free-Language-Modelling-in-Morphologically-Rich-Languages"><a href="#68-H-NET-Hierarchical-Dynamic-Chunking-for-Tokenizer-Free-Language-Modelling-in-Morphologically-Rich-Languages" class="headerlink" title="68. H-NET++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages"></a>68. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/H-Net++__Hierarchical_Dynamic_Chunking_for_Tokenizer-Free_Language_Modelling_in_Morphologically-Rich.pdf">H-NET++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages</a></h3><p><em>教育背景：</em> Independent (no institution listed)</p><p>该论文提出了H-NET++，一种针对形态复杂语言（如波斯语）的分层动态分块语言建模方法，核心创新包括层次化GRU路由器、轻量Transformer上下文混合器、文档级变分超先验、特殊处理零宽非连接字符（ZWNJ）以及阶段式课程训练。实验结果显示H-NET++在语料压缩、下游任务准确率、对正字法噪声的鲁棒性和形态边界识别方面均超越现有模型，并能自动学习符合语言学规律的分块，无需手动标注。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="69-Simulating-Human-Like-Learning-Dynamics-with-LLM-Empowered-Agents"><a href="#69-Simulating-Human-Like-Learning-Dynamics-with-LLM-Empowered-Agents" class="headerlink" title="69. Simulating Human-Like Learning Dynamics with LLM-Empowered Agents"></a>69. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Simulating_Human-Like_Learning_Dynamics_with_LLM-Empowered_Agents.pdf">Simulating Human-Like Learning Dynamics with LLM-Empowered Agents</a></h3><p><em>教育背景：</em> University of Science and Technology of China</p><p>该论文提出LearnerAgent框架，利用多智能体系统和大语言模型（LLM）模拟具有不同心理画像（深度、表层、懒惰、无画像）的学生在为期一年的真实教学环境中的学习行为。通过周期性知识获取、测试与互动，揭示只有深度型学习者具备长期认知成长，LLM默认行为为“勤奋但脆弱的表层学习者”，存在对表层模式的依赖，缺乏深层泛化能力。结论：LearnerAgent能高保真复现人类学习行为，揭示LLM短板并为教育与AI认知提供新洞察。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="70-Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle"><a href="#70-Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle" class="headerlink" title="70. Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle"></a>70. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Shuffle-R1__Efficient_RL_framework_for_Multimodal_Large_Language_Models_via_Data-centric_Dynamic_Shu.pdf">Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle</a></h3><p><em>教育背景：</em> Huazhong University of Science and Technology</p><p>该论文提出了Shuffle-R1框架，通过Pairwise Trajectory Sampling（结构化高对比轨迹采样）和Advantage-based Batch Shuffle（优势感知批次重构）优化多模态大语言模型的强化学习微调流程，有效解决了优势塌缩和回合静默问题，提高了训练效率和泛化能力。实验结果表明，Shuffle-R1在多项主流推理任务上超越现有RL方法，且具备较低计算开销。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="71-LLaVA-RE-Binary-Image-Text-Relevancy-Evaluation-with-Multimodal-Large-Language-Model"><a href="#71-LLaVA-RE-Binary-Image-Text-Relevancy-Evaluation-with-Multimodal-Large-Language-Model" class="headerlink" title="71. LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model"></a>71. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/LLaVA-RE__Binary_Image-Text_Relevancy_Evaluation_with_Multimodal_Large_Language_Model.pdf">LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model</a></h3><p><em>教育背景：</em> Stony Brook University</p><p>该论文提出LLaVA-RE框架，基于LLaVA 1.5多模态大语言模型，通过引入详细任务指令和多模态in-context learning，实现复杂图文对二元相关性评估，并构建了多任务的二元相关性数据集。实验表明该方法在训练及未见任务上均显著优于主流模型，具备良好泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="72-Iterative-Learning-of-Computable-Phenotypes-for-Treatment-Resistant-Hypertension-using-Large-Language-Models"><a href="#72-Iterative-Learning-of-Computable-Phenotypes-for-Treatment-Resistant-Hypertension-using-Large-Language-Models" class="headerlink" title="72. Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models"></a>72. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Iterative_Learning_of_Computable_Phenotypes_for_Treatment_Resistant_Hypertension_using_Large_Languag.pdf">Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models</a></h3><p><em>教育背景：</em> Federal University of ABC</p><p>该论文提出使用大型语言模型（LLM）自动生成高血压及其复杂变型的可计算表型，通过“synthesize, execute, debug, instruct（SEDI）”迭代策略对模型进行数据驱动反馈优化。实验结果表明，SEDI策略下的LLM生成模型在准确性和可解释性方面接近甚至优于传统机器学习方法，但所需专家标注样本更少，且模型更简洁易于临床应用。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="73-MV-Debate-Multi-view-Agent-Debate-with-Dynamic-Reflection-Gating-for-Multimodal-Harmful-Content-Detection-in-Social-Media"><a href="#73-MV-Debate-Multi-view-Agent-Debate-with-Dynamic-Reflection-Gating-for-Multimodal-Harmful-Content-Detection-in-Social-Media" class="headerlink" title="73. MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media"></a>73. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MV-Debate__Multi-view_Agent_Debate_with_Dynamic_Reflection_Gating_for_Multimodal_Harmful_Content_Det.pdf">MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media</a></h3><p><em>教育背景：</em> Shandong University</p><p>本文提出MV-Debate多视角多智能体辩论框架，通过组装表层分析、深度推理、模态对比和社会语境四类异构智能体，结合动态反思门控机制，实现多模态有害内容（如讽刺、仇恨言论、虚假信息）的统一检测。实验显示该方法在多项基准数据集上显著优于单模型和现有多智能体方法，提升了准确性、效率和可解释性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="74-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification"><a href="#74-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification" class="headerlink" title="74. On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification"></a>74. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/On_the_Generalization_of_SFT__A_Reinforcement_Learning_Perspective_with_Reward_Rectification.pdf">On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification</a></h3><p><em>教育背景：</em> Southeast University</p><p>本文从理论上揭示了大语言模型（LLM）中常用的Supervised Fine-Tuning（SFT）方法的泛化能力受限，原因在于其梯度本质上具有隐式的奖励结构，导致优化不稳定和过拟合。为此，提出了一种简单的动态重加权方法Dynamic Fine-Tuning（DFT），通过对每个token的目标函数动态缩放，大幅提升了SFT的泛化能力，并在多个数学推理基准和离线RL场景下超越了标准SFT及多种RL方法。结论：DFT显著提升了SFT的泛化和训练稳定性，为LLM微调提供了高效实用的新范式。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="75-How-Do-LLMs-Persuade-Linear-Probes-Can-Uncover-Persuasion-Dynamics-in-Multi-Turn-Conversations"><a href="#75-How-Do-LLMs-Persuade-Linear-Probes-Can-Uncover-Persuasion-Dynamics-in-Multi-Turn-Conversations" class="headerlink" title="75. How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations"></a>75. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/How_Do_LLMs_Persuade__Linear_Probes_Can_Uncover_Persuasion_Dynamics_in_Multi-Turn_Conversations.pdf">How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations</a></h3><p><em>教育背景：</em> Mila</p><p>本论文提出了一种利用线性探针（linear probes）分析大型语言模型（LLMs）在多轮对话中说服能力的方法。作者设计了可高效检测说服结果、被说服者个性和说服策略的探针，并在真实及合成数据集上验证其能捕捉对话中的关键说服动态，发现探针不仅效率远超提示（prompting），并在部分任务上效果更优。结论显示，线性探针能够高效揭示LLM说服行为细节，适用于大规模对话分析与复杂行为研究。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="76-The-Missing-Reward-Active-Inference-in-the-Era-of-Experience"><a href="#76-The-Missing-Reward-Active-Inference-in-the-Era-of-Experience" class="headerlink" title="76. The Missing Reward: Active Inference in the Era of Experience"></a>76. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/The_Missing_Reward__Active_Inference_in_the_Era_of_Experience.pdf">The Missing Reward: Active Inference in the Era of Experience</a></h3><p><em>教育背景：</em> IBM T.J. Watson Research Center</p><p>本文提出将Active Inference（AIF）与大规模语言模型（LLM）结合，作为实现AI自主学习与经验驱动智能的理论基础。方法上，以AIF的自由能最小化为内在目标，利用LLM作为生成式世界模型，统一探索与利用，并消除对人工奖励工程的依赖。结论认为该架构能提升AI自主性与能源效率，推动经验驱动的可持续AI发展。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="77-COOPER-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models"><a href="#77-COOPER-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models" class="headerlink" title="77. COOPER: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models"></a>77. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Cooper__Co-Optimizing_Policy_and_Reward_Models_in_Reinforcement_Learning_for_Large_Language_Models.pdf">COOPER: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models</a></h3><p><em>教育背景：</em> Zhejiang University</p><p>该论文提出Cooper框架，在强化学习中同步优化大语言模型的策略模型和奖励模型。方法上，通过参考式奖励模型（VerifyRM）、混合注释策略和正负对比学习动态更新奖励模型，结合规则型高精度奖励筛选正样本和助手LLM生成负样本，极大提升了奖励模型的鲁棒性、减少reward hacking。实验证明，Cooper在多个数学推理任务上优于仅用规则或静态奖励模型，动态更新奖励模型能有效抑制reward hacking并提升RL效果。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="78-Follow-Your-Instruction-A-Comprehensive-MLLM-Agent-for-World-Data-Synthesis"><a href="#78-Follow-Your-Instruction-A-Comprehensive-MLLM-Agent-for-World-Data-Synthesis" class="headerlink" title="78. Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"></a>78. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Follow-Your-Instruction__A_Comprehensive_MLLM_Agent_for_World_Data_Synthesis.pdf">Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis</a></h3><p><em>教育背景：</em> HKUST(GZ)</p><p>本文提出了Follow-Your-Instruction框架，一种基于多模态大语言模型（MLLM）的数据合成代理，支持2D、3D和4D数据的自动高质量合成。该方法通过MLLM-Collector收集多模态资产，MLLM-Generator生成3D布局，MLLM-Optimizer多视图优化场景，MLLM-Planner结合VLM实现时序连贯的视频生成。实验表明，该框架合成的数据能显著提升多项AIGC下游任务性能，具有良好可扩展性和实用性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="79-Conformal-Sets-in-Multiple-Choice-Question-Answering-under-Black-Box-Settings-with-Provable-Coverage-Guarantees"><a href="#79-Conformal-Sets-in-Multiple-Choice-Question-Answering-under-Black-Box-Settings-with-Provable-Coverage-Guarantees" class="headerlink" title="79. Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees"></a>79. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Conformal_Sets_in_Multiple-Choice_Question_Answering_under_Black-Box_Settings_with_Provable_Coverage.pdf">Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees</a></h3><p><em>教育背景：</em> University of Jinan</p><p>本文提出了一种基于频率的不确定性量化方法，结合多次独立采样和模态输出作为参考，通过经验频率计算预测熵，并利用Conformal Prediction理论构建具有可证明覆盖率保证的预测集。实验结果表明，该方法在多种LLM和MCQA数据集上较传统logit概率方法更有效地区分正确与错误预测，并能在不同风险级别下有效控制误覆盖率，提升LLM在黑盒场景下的不确定性量化可靠性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="80-AI-vs-Human-Moderators-A-Comparative-Evaluation-of-Multimodal-LLMs-in-Content-Moderation-for-Brand-Safety"><a href="#80-AI-vs-Human-Moderators-A-Comparative-Evaluation-of-Multimodal-LLMs-in-Content-Moderation-for-Brand-Safety" class="headerlink" title="80. AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety"></a>80. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/AI_vs._Human_Moderators__A_Comparative_Evaluation_of_Multimodal_LLMs_in_Content_Moderation_for_Brand.pdf">AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety</a></h3><p><em>教育背景：</em> Zefr Inc</p><p>本文提出了一个多模态和多语言视频数据集，用于品牌安全内容审核，并系统评估了多模态大语言模型（MLLMs，如Gemini、GPT、Llama）在视频内容审核任务中的表现。方法包括多模态信息处理、零样本分类和精细化提示设计，结果显示MLLMs可显著降低人工审核成本，且多模态模型优于文本模型，但准确率仍不及专业人工审核，尤其在复杂和细微分类上表现不足。结论指出MLLMs具备提升内容审核效率的潜力，未来可通过微调和多语言优化进一步提升效果。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="81-Leveraging-AI-to-Accelerate-Clinical-Data-Cleaning-A-Comparative-Study-of-AI-Assisted-vs-Traditional-Methods"><a href="#81-Leveraging-AI-to-Accelerate-Clinical-Data-Cleaning-A-Comparative-Study-of-AI-Assisted-vs-Traditional-Methods" class="headerlink" title="81. Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods"></a>81. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Leveraging_AI_to_Accelerate_Clinical_Data_Cleaning__A_Comparative_Study_of_AI-Assisted_vs._Tradition.pdf">Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods</a></h3><p><em>教育背景：</em> Octozi</p><p>该论文提出了Octozi平台，通过结合大语言模型（如Llama 4微调）和临床领域启发式算法，实现了临床数据清洗流程的自动化和智能化。实验结果显示，AI辅助不仅提升了数据清洗速度6倍，还将错误率从54.67%降至8.48%，显著减轻了人工负担并保证合规性和准确性，验证了AI在药物临床试验数据管理中的变革潜力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="82-The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMs’-Entity-Deduction-Capabilities"><a href="#82-The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMs’-Entity-Deduction-Capabilities" class="headerlink" title="82. The World According to LLMs: How Geographic Origin Influences LLMs’ Entity Deduction Capabilities"></a>82. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/The_World_According_to_LLMs__How_Geographic_Origin_Influences_LLMs'_Entity_Deduction_Capabilities.pdf">The World According to LLMs: How Geographic Origin Influences LLMs’ Entity Deduction Capabilities</a></h3><p><em>教育背景：</em> BITS, Pilani</p><p>本文提出了一种基于“20问游戏”的新型评测框架，通过让大型语言模型（LLMs）自发地提出问题并推理，系统性地揭示了模型在地理实体推理任务中的隐性地理偏见。研究发现，主流LLMs在推理来自全球北方和西方地区的知名人物和事物时表现更优，而实体的流行度和训练语料频次对这种差异的解释力有限，游戏语言影响也极小，凸显了模型推理路径中的地理和文化不平等。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="83-Streamlining-Admission-with-LOR-Insights-AI-Based-Leadership-Assessment-in-Online-Master-s-Program"><a href="#83-Streamlining-Admission-with-LOR-Insights-AI-Based-Leadership-Assessment-in-Online-Master-s-Program" class="headerlink" title="83. Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master&#39;s Program"></a>83. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Streamlining_Admission_with_LOR_Insights__AI-Based_Leadership_Assessment_in_Online_Master's_Program.pdf">Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master&#39;s Program</a></h3><p><em>教育背景：</em> Georgia Institute of Technology</p><p>本论文提出LORI系统，通过结合RoBERTa弱监督模型和LLAMA2等大语言模型，对硕士申请者推荐信中的领导力特质（如团队协作、沟通、创新）进行自动化检测与微标签分类。实验证明RoBERTa模型在领导力句子识别上取得了91.6%的F1分数，并通过LLM实现更深层次的短语提取与验证，有效提升了招生流程效率和公平性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="84-Auto-Eval-Judge-Towards-a-General-Agentic-Framework-for-Task-Completion-Evaluation"><a href="#84-Auto-Eval-Judge-Towards-a-General-Agentic-Framework-for-Task-Completion-Evaluation" class="headerlink" title="84. Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation"></a>84. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Auto-Eval_Judge__Towards_a_General_Agentic_Framework_for_Task_Completion_Evaluation.pdf">Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation</a></h3><p><em>教育背景：</em> University of Massachusetts Amherst</p><p>本文提出了一个通用、模块化的Agent任务完成评价框架，能够自动分解任务为子任务，通过LLM和多Agent模块验证每一步推理和最终结果。实验在GAIA和BigCodeBench数据集上进行，与GPT-4o LLM-as-a-Judge基线相比，该Judge Agent与人类评估的对齐度提升4.76%和10.52%。结论显示该框架能更全面、可扩展地评价复杂Agent系统，减少人工依赖。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="85-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities"><a href="#85-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities" class="headerlink" title="85. InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities"></a>85. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/InfiAlign__A_Scalable_and_Sample-Efficient_Framework_for_Aligning_LLMs_to_Enhance_Reasoning_Capabili.pdf">InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities</a></h3><p><em>教育背景：</em> InfiX.ai</p><p>本文提出了InfiAlign框架，通过结合多维度自动化数据筛选、监督微调（SFT）和直接偏好优化（DPO），实现大语言模型在推理任务上的高效对齐。核心方法包括多维质量指标驱动的数据采样（涵盖多样性、难度、质量等），并采用模块化和可扩展设计，实现小数据量下的高性能推理能力。结论显示，InfiAlign在仅用12%数据量下达到与主流强基线相当性能，DPO进一步提升数学推理表现，实现了高数据利用率和易扩展性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="86-Can-Large-Language-Models-Generate-Effective-Datasets-for-Emotion-Recognition-in-Conversations"><a href="#86-Can-Large-Language-Models-Generate-Effective-Datasets-for-Emotion-Recognition-in-Conversations" class="headerlink" title="86. Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?"></a>86. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Can_Large_Language_Models_Generate_Effective_Datasets_for_Emotion_Recognition_in_Conversations_.pdf">Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?</a></h3><p><em>教育背景：</em> University of Hamburg</p><p>本文提出利用小型、通用的大语言模型（Vicuna 1.5-13B）结合结构化提示工程，自动合成多样化的对话情绪识别（ERC）数据集，包括自然分布和均衡分布两种类型，针对主流ERC基准（MELD、EmoryNLP、IEMOCAP）分别生成相应的数据集，并通过多种现有ERC分类模型验证其有效性。实验结果和统计检验显示，基于LLM生成的数据集能提升情感识别模型的鲁棒性和性能，并对标签分布失衡等问题具有积极影响，结论为该方法能够高效、可复现地生成高质量ERC数据集，促进下游模型性能提升。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="87-Group-Causal-Policy-Optimization-for-Post-Training-Large-Language-Models"><a href="#87-Group-Causal-Policy-Optimization-for-Post-Training-Large-Language-Models" class="headerlink" title="87. Group Causal Policy Optimization for Post-Training Large Language Models"></a>87. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Group_Causal_Policy_Optimization_for_Post-Training_Large_Language_Models.pdf">Group Causal Policy Optimization for Post-Training Large Language Models</a></h3><p><em>教育背景：</em> Institute of Software, Chinese Academy of Sciences</p><p>本论文提出Group Causal Policy Optimization（GCPO）方法，通过引入结构化因果模型（SCM），发现LLM候选响应间因最终集成输出而产生因果关联，进而将候选输出投影到因果子空间，并结合KL正则项，引导策略优化在结构上更一致。实验结果表明，GCPO在多项数学与代码推理基准上均优于GRPO等现有方法，验证了因果结构建模对LLM后训练效果的提升和鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="88-LLM-based-Multi-Agent-Copilot-for-Quantum-Sensor"><a href="#88-LLM-based-Multi-Agent-Copilot-for-Quantum-Sensor" class="headerlink" title="88. LLM-based Multi-Agent Copilot for Quantum Sensor"></a>88. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/LLM-based_Multi-Agent_Copilot_for_Quantum_Sensor.pdf">LLM-based Multi-Agent Copilot for Quantum Sensor</a></h3><p><em>教育背景：</em> National University of Defense Technology</p><p>本文提出QCopilot，一个基于大语言模型（LLM）的多智能体框架，集成外部知识访问、主动学习和不确定性量化，用于量子传感器的设计与诊断。QCopilot通过专用智能体实现实验参数自适应优化、自动建模分析和独立故障诊断，实验在冷原子原型系统中实现了无人工干预下的高效制冷和异常参数自主识别，大幅提升了实验效率和智能化水平。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="89-StructVRM-Aligning-Multimodal-Reasoning-with-Structured-and-Verifiable-Reward-Models"><a href="#89-StructVRM-Aligning-Multimodal-Reasoning-with-Structured-and-Verifiable-Reward-Models" class="headerlink" title="89. StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models"></a>89. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/StructVRM__Aligning_Multimodal_Reasoning_with_Structured_and_Verifiable_Reward_Models.pdf">StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models</a></h3><p><em>教育背景：</em> ByteDance Seed China</p><p>StructVRM提出了一种基于模型的细粒度奖励机制，通过训练可验证的奖励模型（verifier），在多模态推理任务中对多子问题进行结构化、可验证的评分，实现了部分正确性反馈，并结合PPO强化学习优化模型推理能力。实验证明，StructVRM在六个公开多模态基准和高难度STEM-Bench上取得SOTA表现，显著提升了复杂现实推理任务中的多模态模型能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="90-The-Term-‘Agent’-Has-Been-Diluted-Beyond-Utility-and-Requires-Redefinition"><a href="#90-The-Term-‘Agent’-Has-Been-Diluted-Beyond-Utility-and-Requires-Redefinition" class="headerlink" title="90. The Term ‘Agent’ Has Been Diluted Beyond Utility and Requires Redefinition"></a>90. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/The_Term_'Agent'_Has_Been_Diluted_Beyond_Utility_and_Requires_Redefinition.pdf">The Term ‘Agent’ Has Been Diluted Beyond Utility and Requires Redefinition</a></h3><p><em>教育背景：</em> Duke University</p><p>本文提出了针对“agent”一词在人工智能领域被广泛滥用和定义模糊的重新定义框架。通过历史和现有用法分析，作者提出了五维度（环境交互、目标导向、时间一致性、学习适应性和自主性）评价体系，并明确了成为‘agent’的最低要求。结论指出，该框架有助于提升研究方法的严谨性、沟通的清晰度和政策制定的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="91-LAG-Logic-Augmented-Generation-from-a-Cartesian-Perspective"><a href="#91-LAG-Logic-Augmented-Generation-from-a-Cartesian-Perspective" class="headerlink" title="91. LAG: Logic-Augmented Generation from a Cartesian Perspective"></a>91. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/LAG__Logic-Augmented_Generation_from_a_Cartesian_Perspective.pdf">LAG: Logic-Augmented Generation from a Cartesian Perspective</a></h3><p><em>教育背景：</em> The Hong Kong Polytechnic University</p><p>该论文提出了LAG（Logic-Augmented Generation），一种针对大型语言模型检索增强生成（RAG）系统的创新推理管道。通过自适应问题分解、逻辑链推理、逻辑终止机制，LAG能将复杂问题拆解为原子子问题，按逻辑依赖顺序逐步解决，并在各环节有效防止错误传递。实验表明LAG在多跳问答和复杂推理任务上显著提升准确率和合理性，优于主流RAG方法，实现更接近人类认知的推理过程。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="92-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs"><a href="#92-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs" class="headerlink" title="92. MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs"></a>92. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MELLA__Bridging_Linguistic_Capability_and_Cultural_Groundedness_for_Low-Resource_Language_MLLMs.pdf">MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs</a></h3><p><em>教育背景：</em> Shanghai Artificial Intelligence Laboratory</p><p>本文提出一种针对低资源语言多模态大语言模型（MLLM）的双目标与双源数据框架，强调语言能力和文化扎根性。通过构建MELLA大规模多语种多模态数据集，分别采集本地网页原生alt-text（文化知识）和MLLM生成描述（语言能力），并联合训练，显著提升模型在八种低资源语言上的描述准确度和文化认知能力。实验表明，经过MELLA微调后，多模态模型在各项指标上均优于现有方法，实现了更丰富的“厚描述”。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="93-GRAIL-Learning-to-Interact-with-Large-Knowledge-Graphs-for-Retrieval-Augmented-Reasoning"><a href="#93-GRAIL-Learning-to-Interact-with-Large-Knowledge-Graphs-for-Retrieval-Augmented-Reasoning" class="headerlink" title="93. GRAIL: Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning"></a>93. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/GRAIL_Learning_to_Interact_with_Large_Knowledge_Graphs_for_Retrieval_Augmented_Reasoning.pdf">GRAIL: Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning</a></h3><p><em>教育背景：</em> Institute for AI Industry Research (AIR), Tsinghua University</p><p>GRAIL提出了一个针对大型知识图谱的交互式检索增强推理框架，通过LLM引导的数据合成、两阶段（监督微调+强化学习）训练和动态交互检索机制，实现多步高效、结构感知的信息检索与推理。实验结果显示，GRAIL在多个知识图谱问答基准上取得了显著性能提升，平均准确率提升21.01%、F1提升22.43%，同时大幅减少冗余检索信息，实现检索深度与精度的高效平衡。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="94-MoMA-A-Mixture-of-Multimodal-Agents-Architecture-for-Enhancing-Clinical-Prediction-Modelling"><a href="#94-MoMA-A-Mixture-of-Multimodal-Agents-Architecture-for-Enhancing-Clinical-Prediction-Modelling" class="headerlink" title="94. MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling"></a>94. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MoMA__A_Mixture-of-Multimodal-Agents_Architecture_for_Enhancing_Clinical_Prediction_Modelling.pdf">MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling</a></h3><p><em>教育背景：</em> University of Wisconsin-Madison</p><p>该论文提出了MoMA架构，通过多智能体协作，利用多个大语言模型（LLM）将多模态电子健康记录（EHR）数据中的非文本模态（如医学影像、实验室结果）转化为结构化文本摘要，再由聚合智能体整合所有文本后交给预测智能体用于临床预测。评测结果表明，MoMA在多个多模态临床任务上超越了现有主流方法，表现出更高的预测准确性和灵活性，且可灵活适配新模态，降低对大规模配对数据的依赖。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="95-MyCulture-Exploring-Malaysia’s-Diverse-Culture-under-Low-Resource-Language-Constraints"><a href="#95-MyCulture-Exploring-Malaysia’s-Diverse-Culture-under-Low-Resource-Language-Constraints" class="headerlink" title="95. MyCulture: Exploring Malaysia’s Diverse Culture under Low-Resource Language Constraints"></a>95. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MyCulture__Exploring_Malaysia's_Diverse_Culture_under_Low-Resource_Language_Constraints.pdf">MyCulture: Exploring Malaysia’s Diverse Culture under Low-Resource Language Constraints</a></h3><p><em>教育背景：</em> Universiti Malaya</p><p>本文提出MyCulture基准，是首个面向马来西亚、以马来语呈现的多元文化LLM评测基准，覆盖艺术、服饰、习俗、娱乐、食物和宗教六大领域，并采用创新的无标准选项的开放式多选题格式（包括多答案、排序、匹配三种类型）以减少猜测和格式偏倚。实验结果显示，传统封闭式多选题高估了模型的文化理解能力，采用开放式题型后主流LLM的成绩平均下降至少17%，凸显更严格、文化嵌入型评测的重要性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="96-Large-Language-Models-Transform-Organic-Synthesis-From-Reaction-Prediction-to-Automation"><a href="#96-Large-Language-Models-Transform-Organic-Synthesis-From-Reaction-Prediction-to-Automation" class="headerlink" title="96. Large Language Models Transform Organic Synthesis: From Reaction Prediction to Automation"></a>96. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Large_Language_Models_Transform_Organic_Synthesis_From_Reaction_Prediction_to_Automation.pdf">Large Language Models Transform Organic Synthesis: From Reaction Prediction to Automation</a></h3><p><em>教育背景：</em> Government Boys Higher Secondary School, Bukera Sharif, Tando Allahyar, Affiliated with BISE Hyderabad, Sindh, Pakistan</p><p>本文系统综述了大型语言模型（LLMs）在有机合成中的应用进展，包括化学反应预测、逆合成规划、条件优化及与机器人平台的自动化集成。通过结合LLMs与图神经网络、强化学习和多模态数据，显著提升了反应预测准确率和实验自动化效率，同时讨论了数据偏差、模型解释性和安全风险等挑战。结论认为LLMs正在推动化学合成向高效、自动化和可持续方向变革，但仍需解决可靠性与可解释性等关键问题。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="97-NomicLaw-Emergent-Trust-and-Strategic-Argumentation-in-LLMs-During-Collaborative-Law-Making"><a href="#97-NomicLaw-Emergent-Trust-and-Strategic-Argumentation-in-LLMs-During-Collaborative-Law-Making" class="headerlink" title="97. NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making"></a>97. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/NomicLaw__Emergent_Trust_and_Strategic_Argumentation_in_LLMs_During_Collaborative_Law-Making.pdf">NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making</a></h3><p><em>教育背景：</em> University of Jyvaskyla</p><p>该论文提出NomicLaw框架，让多个开源大语言模型（LLMs）在法律治理场景下以“提案-论证-投票”循环协作立法，量化分析信任、互惠和联盟形成等社会动态，并通过主题分析揭示模型在多智能体协作中呈现的法律推理和说服能力。实验显示，模型异质性能减少自投、增加联盟切换并丰富法律论证类型，强调多样化模型组合可提升AI辅助立法的合理性和多元性，但当前LLM推理多为统计模式而非真正理解，需人类监管。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="98-A-Novel-Architecture-for-Symbolic-Reasoning-with-Decision-Trees-and-LLM-Agents"><a href="#98-A-Novel-Architecture-for-Symbolic-Reasoning-with-Decision-Trees-and-LLM-Agents" class="headerlink" title="98. A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents"></a>98. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Novel_Architecture_for_Symbolic_Reasoning_with_Decision_Trees_and_LLM_Agents.pdf">A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents</a></h3><p><em>教育背景：</em> Harvard University</p><p>本文提出一种结合决策树符号推理与大型语言模型（LLM）生成推理能力的多智能体系统架构。方法包括将决策树作为可调用的符号推理模块嵌入到由LLM驱动的协调推理流程中，通过中央编排器维护信念一致性并动态调用工具，实现结构化和非结构化知识的跨模态推理。实验表明该架构在ProofWriter、GSM8k和ARC等基准任务上推理一致性和准确率均超过现有方法，且系统具备高度可解释性和可扩展性，适用于医疗决策支持和科学发现等领域。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="99-B4DL-A-Benchmark-for-4D-LiDAR-LLM-in-Spatio-Temporal-Understanding"><a href="#99-B4DL-A-Benchmark-for-4D-LiDAR-LLM-in-Spatio-Temporal-Understanding" class="headerlink" title="99. B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding"></a>99. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/B4DL__A_Benchmark_for_4D_LiDAR_LLM_in_Spatio-Temporal_Understanding.pdf">B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding</a></h3><p><em>教育背景：</em> Korea Advanced Institute of Science and Technology</p><p>该论文提出B4DL基准与数据集，专为训练和评估多模态大语言模型在4D LiDAR时空理解任务中而设计。方法包括创新的数据生成管线（融合多视角图像与人工注释，自动生成LiDAR时空场景的问答对），以及提出B4DL模型，通过LiDAR编码器、对齐层和元信息模块实现4D LiDAR与语言的高效对齐，采用分阶段训练策略提升模型空间与时序推理能力。实验证明B4DL模型在空间及时序理解上显著优于现有3D或视频多模态LLM，在多项任务和跨数据集泛化方面表现突出。结论：B4DL为4D LiDAR多模态理解提供了首个高质量基准和方法体系，有效推动了真实世界时空动态理解领域的发展。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="100-Resource-Limited-Joint-Multimodal-Sentiment-Reasoning-and-Classification-via-Chain-of-Thought-Enhancement-and-Distillation"><a href="#100-Resource-Limited-Joint-Multimodal-Sentiment-Reasoning-and-Classification-via-Chain-of-Thought-Enhancement-and-Distillation" class="headerlink" title="100. Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation"></a>100. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Resource-Limited_Joint_Multimodal_Sentiment_Reasoning_and_Classification_via_Chain-of-Thought_Enhanc.pdf">Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation</a></h3><p><em>教育背景：</em> Northeastern University</p><p>本文提出MulCoT-RD框架，针对资源受限场景下的多模态情感推理与分类（JMSRC）任务，采用“教师-助理-学生”三阶段推理蒸馏范式，通过多任务学习及结构化CoT（Chain-of-Thought）增强和软硬标签联合优化，使轻量级多模态大模型能自主生成高质量情感推理链并实现高效情感分类。实验表明，MulCoT-RD在多个公开数据集上实现了较优的情感分类性能和推理解释性，且模型仅3B参数，具备良好的部署适应性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="101-VFlowOpt-A-Token-Pruning-Framework-for-LMMs-with-Visual-Information-Flow-Guided-Optimization"><a href="#101-VFlowOpt-A-Token-Pruning-Framework-for-LMMs-with-Visual-Information-Flow-Guided-Optimization" class="headerlink" title="101. VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization"></a>101. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/VFlowOpt__A_Token_Pruning_Framework_for_LMMs_with_Visual_Information_Flow-Guided_Optimization.pdf">VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization</a></h3><p><em>教育背景：</em> Shanghai AI Laboratory</p><p>本文提出VFlowOpt，一种针对大型多模态模型（LMMs）的可微视觉信息流引导的视觉token裁剪框架。方法通过结合注意力校准和信息熵，分阶段评估视觉token重要性，采用回收机制减少信息损失，并利用视觉信息流优化各阶段裁剪超参数。实验表明，VFlowOpt可在保留10%视觉token的情况下仍维持90%模型性能，显著降低显存占用和推理时延，适用于实际部署。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="102-FAITH-A-Framework-for-Assessing-Intrinsic-Tabular-Hallucinations-in-Finance"><a href="#102-FAITH-A-Framework-for-Assessing-Intrinsic-Tabular-Hallucinations-in-Finance" class="headerlink" title="102. FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in Finance"></a>102. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FAITH__A_Framework_for_Assessing_Intrinsic_Tabular_Hallucinations_in_finance.pdf">FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in Finance</a></h3><p><em>教育背景：</em> Asian Institute of Digital Finance, National University of Singapore</p><p>该论文提出了用于金融领域评估大型语言模型（LLM）内在幻觉的新框架，包括自动化数据集构建、基于真实年报的金融幻觉基准数据集，以及四类金融推理复杂度的分类方法。实验结果表明，主流LLM在复杂金融表格推理任务中依然存在较高幻觉率，准确性随推理复杂度下降，表明当前模型在高精度金融应用部署上仍有显著挑战。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="103-Incident-Response-Planning-Using-a-Lightweight-Large-Language-Model-with-Reduced-Hallucination"><a href="#103-Incident-Response-Planning-Using-a-Lightweight-Large-Language-Model-with-Reduced-Hallucination" class="headerlink" title="103. Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination"></a>103. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Incident_Response_Planning_Using_a_Lightweight_Large_Language_Model_with_Reduced_Hallucination.pdf">Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination</a></h3><p><em>教育背景：</em> University of Melbourne</p><p>本文提出了一种集成轻量级大语言模型（LLM）用于网络安全事件响应的方法，包括指令微调、信息检索和决策规划三步。该方法通过理论证明有效减少幻觉概率，并在多数据集上实验显示比前沿LLM恢复时间缩短最高22%，泛化能力强且资源消耗低。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="104-RLHF-Fine-Tuning-of-LLMs-for-Alignment-with-Implicit-User-Feedback-in-Conversational-Recommenders"><a href="#104-RLHF-Fine-Tuning-of-LLMs-for-Alignment-with-Implicit-User-Feedback-in-Conversational-Recommenders" class="headerlink" title="104. RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders"></a>104. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/RLHF_Fine-Tuning_of_LLMs_for_Alignment_with_Implicit_User_Feedback_in_Conversational_Recommenders.pdf">RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders</a></h3><p><em>教育背景：</em> Northeastern University</p><p>该论文提出了一种将RLHF（基于人类反馈的强化学习）用于大语言模型（LLM）在多轮对话推荐系统中的微调方法，通过隐式用户反馈（如停留时间、情感变化、语义一致性）构建奖励模型，并用PPO优化LLM生成的推荐结果。实验结果表明，该方法在REDIAL和OpenDialKG等数据集上显著提升了推荐准确率、对话流畅性和用户满意度，验证了隐式信号指导下的自适应推荐能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="105-Understanding-and-Mitigating-Errors-of-LLM-Generated-RTL-Code"><a href="#105-Understanding-and-Mitigating-Errors-of-LLM-Generated-RTL-Code" class="headerlink" title="105. Understanding and Mitigating Errors of LLM-Generated RTL Code"></a>105. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Understanding_and_Mitigating_Errors_of_LLM-Generated_RTL_Code.pdf">Understanding and Mitigating Errors of LLM-Generated RTL Code</a></h3><p><em>教育背景：</em> Institute of Computing Technology, Chinese Academy of Sciences</p><p>本论文系统分析LLM生成的RTL代码错误类型，发现主要问题为缺乏RTL编程知识、理解电路概念不足、描述歧义和多模态输入误解。作者提出RAG知识补充、规则描述修正、多模态转换及仿真调试等纠错机制，并集成至代码生成流程，在VerilogEval基准上准确率达91%，显著优于基线方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="106-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs"><a href="#106-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs" class="headerlink" title="106. MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs"></a>106. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MoBE__Mixture-of-Basis-Experts_for_Compressing_MoE-based_LLMs.pdf">MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs</a></h3><p><em>教育背景：</em> Inclusion AI</p><p>该论文提出Mixture-of-Basis-Experts (MoBE) 方法，通过将专家模块中的权重矩阵分解为专家特有的变换矩阵和共享的基矩阵线性组合，实现大规模MoE语言模型的高效参数压缩。实验表明，MoBE在主流MoE模型上能减少24%-30%的参数，仅带来1%-2%的准确率下降，显著优于现有压缩方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="107-Pruning-Large-Language-Models-by-Identifying-and-Preserving-Functional-Networks"><a href="#107-Pruning-Large-Language-Models-by-Identifying-and-Preserving-Functional-Networks" class="headerlink" title="107. Pruning Large Language Models by Identifying and Preserving Functional Networks"></a>107. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Pruning_Large_Language_Models_by_Identifying_and_Preserving_Functional_Networks.pdf">Pruning Large Language Models by Identifying and Preserving Functional Networks</a></h3><p><em>教育背景：</em> School of Automation, Northwestern Polytechnical University</p><p>本文提出了一种受神经科学启发的大语言模型（LLM）结构化剪枝新方法，通过独立成分分析（ICA）分解每层神经元信号，识别并保留LLM中的功能网络，实现高效剪枝。实验结果表明，该方法在不同模型和剪枝率下均优于现有主流剪枝方法，有效降低计算和内存消耗，同时保持模型性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="108-Cross-LoRA-A-Data-Free-LoRA-Transfer-Framework-across-Heterogeneous-LLMs"><a href="#108-Cross-LoRA-A-Data-Free-LoRA-Transfer-Framework-across-Heterogeneous-LLMs" class="headerlink" title="108. Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs"></a>108. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Cross-LoRA__A_Data-Free_LoRA_Transfer_Framework_across_Heterogeneous_LLMs.pdf">Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs</a></h3><p><em>教育背景：</em> Baidu Inc</p><p>本文提出Cross-LoRA，一种无需数据和训练即可将LoRA适配器在不同大语言模型间迁移的框架。方法包括通过截断奇异值分解（SVD）和Frobenius最优线性变换对源、目标模型进行子空间对齐（LoRA-Align），然后将源LoRA权重更新投影到目标模型参数空间（LoRA-Shift），实现架构无关的迁移。实验显示Cross-LoRA在多个NLP推理基准上实现了对基线模型的稳定提升，性能接近于直接训练的LoRA适配器，且在单卡20分钟内完成迁移，适合低资源和实际应用场景。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="109-SPEX-A-Vision-Language-Model-for-Land-Cover-Extraction-on-Spectral-Remote-Sensing-Images"><a href="#109-SPEX-A-Vision-Language-Model-for-Land-Cover-Extraction-on-Spectral-Remote-Sensing-Images" class="headerlink" title="109. SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images"></a>109. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SPEX__A_Vision-Language_Model_for_Land_Cover_Extraction_on_Spectral_Remote_Sensing_Images.pdf">SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images</a></h3><p><em>教育背景：</em> Xinjiang University</p><p>该论文提出了SPEX模型，通过融合多尺度视觉特征聚合、Token上下文压缩和多光谱视觉预训练，实现了基于指令的多模态大语言模型，用于遥感多光谱影像的地表覆盖物提取。构建了SPIE数据集，将地物光谱先验编码到可被LLM识别的文本属性，显著提升了对植被、水体、建筑等类别的像素级识别精度。实验结果表明，SPEX在五个公开多光谱数据集上均优于现有方法，且生成结果更具解释性与交互性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="110-EvoGraph-Hybrid-Directed-Graph-Evolution-toward-Software-3-0"><a href="#110-EvoGraph-Hybrid-Directed-Graph-Evolution-toward-Software-3-0" class="headerlink" title="110. EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0"></a>110. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EvoGraph__Hybrid_Directed_Graph_Evolution_toward_Software_3.0.pdf">EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0</a></h3><p><em>教育背景：</em> AutoHand AI</p><p>EvoGraph提出了一个利用专门小语言模型（SLM）驱动的混合定向图进化框架，用于自动演化企业软件系统的代码、文档、构建流程和工单等全部软件制品。其方法包括基于有类型定向图的统一表征、多种SLM引导的变异算子、带安全约束的多目标进化选择，并在多语言现代化（COBOL、.NET、Lisp等）任务中实现了82-96%语义等价和90%算力节省。结论显示，EvoGraph有效支持企业级遗留系统现代化，SLM优于大模型并经济实用，推进了自主演化的软件3.0路径。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="111-QA-Dragon-Query-Aware-Dynamic-RAG-System-for-Knowledge-Intensive-Visual-Question-Answering"><a href="#111-QA-Dragon-Query-Aware-Dynamic-RAG-System-for-Knowledge-Intensive-Visual-Question-Answering" class="headerlink" title="111. QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering"></a>111. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/QA-Dragon__Query-Aware_Dynamic_RAG_System_for_Knowledge-Intensive_Visual_Question_Answering.pdf">QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering</a></h3><p><em>教育背景：</em> The Hong Kong Polytechnic University</p><p>该论文提出了QA-Dragon系统，通过引入领域路由器、搜索路由器、工具路由器以及多阶段重排序机制，实现了动态多模态检索增强生成（RAG）以提升知识密集型视觉问答（VQA）的准确性。其方法结合了图像和文本的混合检索、领域感知推理和多轮多跳推理，有效提高了复杂VQA任务中的答案准确率和知识重叠度，在KDD Cup 2025 Meta CRAG-MM Challenge中全面超越现有强基线方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="112-Attention-Basin-Why-Contextual-Position-Matters-in-Large-Language-Models"><a href="#112-Attention-Basin-Why-Contextual-Position-Matters-in-Large-Language-Models" class="headerlink" title="112. Attention Basin: Why Contextual Position Matters in Large Language Models"></a>112. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Attention_Basin__Why_Contextual_Position_Matters_in_Large_Language_Models.pdf">Attention Basin: Why Contextual Position Matters in Large Language Models</a></h3><p><em>教育背景：</em> Sun Yat-sen University</p><p>本文系统揭示了大语言模型（LLM）在处理结构化输入时存在“attention basin”现象，即模型会对输入序列开头和结尾的信息分配更高关注，中间部分则被忽视。为此，作者提出了一种无需训练、模型无关的Attention-Driven Reranking（AttnRank）方法，通过少量校准集估算模型固有的注意力位置偏好，并据此重排关键内容，使其位于模型关注度高的位置。实验表明，AttnRank在多跳问答和few-shot学习等任务中，无需修改模型参数便能显著提升10种主流LLM的表现。结论：AttnRank高效缓解了位置偏差，提高了信息利用率，且与现有推理加速框架兼容。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="113-Beyond-Automation-Socratic-AI-Epistemic-Agency-and-the-Implications-of-the-Emergence-of-Orchestrated-Multi-Agent-Learning-Architectures"><a href="#113-Beyond-Automation-Socratic-AI-Epistemic-Agency-and-the-Implications-of-the-Emergence-of-Orchestrated-Multi-Agent-Learning-Architectures" class="headerlink" title="113. Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures"></a>113. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Beyond_Automation__Socratic_AI,_Epistemic_Agency,_and_the_Implications_of_the_Emergence_of_Orchestra.pdf">Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures</a></h3><p><em>教育背景：</em> University of Kassel</p><p>该论文提出并实证验证了一种基于Socratic方法的对话式大语言模型AI导师，通过结构化提问促进学生批判性、独立性和反思性思维，并与通用AI聊天机器人进行了对比实验。结果显示，Socratic AI显著提升了学生的元认知参与和高阶思维，推动了AI教育从单一工具向多智能体协作生态系统转变，并对高等教育的教师角色、课程设计、基础设施及评估体系提出了系统性建议。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="114-Align-Don-t-Divide-Revisiting-the-LoRA-Architecture-in-Multi-Task-Learning"><a href="#114-Align-Don-t-Divide-Revisiting-the-LoRA-Architecture-in-Multi-Task-Learning" class="headerlink" title="114. Align, Don&#39;t Divide: Revisiting the LoRA Architecture in Multi-Task Learning"></a>114. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Align,_Don't_Divide__Revisiting_the_LoRA_Architecture_in_Multi-Task_Learning.pdf">Align, Don&#39;t Divide: Revisiting the LoRA Architecture in Multi-Task Learning</a></h3><p><em>教育背景：</em> Jilin University</p><p>本文系统分析了参数高效微调（PEFT）方法LoRA在多任务学习（MTL）中的主流多头&#x2F;多适配器结构，发现高结构复杂度和任务特异性分离并未带来更优泛化性能，反而简单单头高秩LoRA表现更优。基于此，作者提出Align-LoRA，通过在训练目标中引入KL或MMD对齐损失，显式增强任务间共享表征，无需增加推理开销且获得了显著优于复杂结构的多任务泛化效果。结论：对齐表征、强化任务共享知识比结构分离更关键，Align-LoRA为多任务PEFT提供了更高效新范式。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="115-Aligning-LLMs-on-a-Budget-Inference-Time-Alignment-with-Heuristic-Reward-Models"><a href="#115-Aligning-LLMs-on-a-Budget-Inference-Time-Alignment-with-Heuristic-Reward-Models" class="headerlink" title="115. Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models"></a>115. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Aligning_LLMs_on_a_Budget__Inference-Time_Alignment_with_Heuristic_Reward_Models.pdf">Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models</a></h3><p><em>教育背景：</em> University of Massachusetts Amherst</p><p>该论文提出HIA（Heuristic-Guided Inference-time Alignment）方法，通过轻量级启发式奖励模型和两阶段筛选机制，在不需微调和仅需黑盒访问的前提下，实现大语言模型推理时的多目标、个性化对齐。实验证明，在HELPSTEER和COMPRED等真实数据集上，HIA在相同推理预算下优于BoN采样、束搜索和贪心搜索，尤其在推理预算低至1-2次查询时提升显著。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="116-Speech-LLMs-in-Low-Resource-Scenarios-Data-Volume-Requirements-and-the-Impact-of-Pretraining-on-High-Resource-Languages"><a href="#116-Speech-LLMs-in-Low-Resource-Scenarios-Data-Volume-Requirements-and-the-Impact-of-Pretraining-on-High-Resource-Languages" class="headerlink" title="116. Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages"></a>116. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Speech_LLMs_in_Low-Resource_Scenarios__Data_Volume_Requirements_and_the_Impact_of_Pretraining_on_Hig.pdf">Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages</a></h3><p><em>教育背景：</em> University of Trento</p><p>该论文基于SLAM-ASR框架，研究了在低资源语音识别场景中，训练数据量及高资源语言预训练对模型性能的影响。作者通过将Whisper-large-v3-turbo语音编码器、轻量线性投影器和多语种LLM（EuroLLM&#x2F;Salamandra）结合，实验证明：需100-200小时训练数据才能达到Whisper-only模型效果，且投影器在高资源语言上预训练并迁移到低资源语言，尤其是在仅有10-15小时微调数据时，能显著提升识别准确率，多语种投影器进一步提高泛化能力。结论指出，预训练和微调策略为低资源语种语音识别提供了优化方向，但对数据量和跨领域泛化仍存在挑战。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="117-Towards-Assessing-Medical-Ethics-from-Knowledge-to-Practice"><a href="#117-Towards-Assessing-Medical-Ethics-from-Knowledge-to-Practice" class="headerlink" title="117. Towards Assessing Medical Ethics from Knowledge to Practice"></a>117. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Assessing_Medical_Ethics_from_Knowledge_to_Practice.pdf">Towards Assessing Medical Ethics from Knowledge to Practice</a></h3><p><em>教育背景：</em> The Chinese University of Hong Kong, Shenzhen</p><p>该论文提出了PrinciplismQA基准，用于系统性评估大语言模型（LLM）在医疗伦理领域的知识掌握与实际应用能力，结合多项选择题和开放式案例分析，采用专家校验与LLM自动评分流程。实验结果显示所有模型均存在“知识-实践鸿沟”，尤其在实际情境动态应用伦理原则（如仁慈性）方面表现较弱，医疗领域微调虽提升实际伦理表现但略有知识遗忘，未来需强化模型伦理对齐。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="118-Exploring-Superior-Function-Calls-via-Reinforcement-Learning"><a href="#118-Exploring-Superior-Function-Calls-via-Reinforcement-Learning" class="headerlink" title="118. Exploring Superior Function Calls via Reinforcement Learning"></a>118. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Exploring_Superior_Function_Calls_via_Reinforcement_Learning.pdf">Exploring Superior Function Calls via Reinforcement Learning</a></h3><p><em>教育背景：</em> AWorld Team, Inclusion AI</p><p>本文提出了FunRL，一种针对大语言模型函数调用任务的强化学习框架。方法创新性地将Chain-of-Thought（CoT）熵纳入GRPO优势估计，通过两阶段数据准备（LLM和AST评估）与精确的二元奖励函数，促进模型在推理和参数验证上探索多样化思路。实验表明，FunRL在BFCLv2基准上实现了开源模型最优的86.02%总体准确率，显著优于标准GRPO，特别是在代码预训练模型上提升效果明显。结论：FunRL显著提升了LLM工具调用的复杂推理能力和实际应用可靠性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="119-EasySize-Elastic-Analog-Circuit-Sizing-via-LLM-Guided-Heuristic-Search"><a href="#119-EasySize-Elastic-Analog-Circuit-Sizing-via-LLM-Guided-Heuristic-Search" class="headerlink" title="119. EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search"></a>119. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EasySize__Elastic_Analog_Circuit_Sizing_via_LLM-Guided_Heuristic_Search.pdf">EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search</a></h3><p><em>教育背景：</em> Shanghai Jiao Tong University</p><p>该论文提出EasySize，一种基于微调Qwen3-8B模型的轻量化模拟电路尺寸调整框架，通过结合大语言模型生成动态损失函数和启发式搜索（DE与PSO），实现跨工艺节点和电路拓扑的通用高效优化。实验表明EasySize在无需额外训练的情况下性能优于AutoCkt，并大幅减少仿真资源消耗，显著提升模拟电路设计自动化的效率与适应性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="120-JPS-Jailbreak-Multimodal-Large-Language-Models-with-Collaborative-Visual-Perturbation-and-Textual-Steering"><a href="#120-JPS-Jailbreak-Multimodal-Large-Language-Models-with-Collaborative-Visual-Perturbation-and-Textual-Steering" class="headerlink" title="120. JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering"></a>120. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/JPS__Jailbreak_Multimodal_Large_Language_Models_with_Collaborative_Visual_Perturbation_and_Textual_S.pdf">JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering</a></h3><p><em>教育背景：</em> Tsinghua University</p><p>本文提出JPS方法，通过协同优化视觉扰动与多智能体系统生成的文本引导，实现对多模态大语言模型（MLLM）的高质量越狱攻击。JPS将安全绕过任务分配给目标引导的图像对抗扰动，将高效恶意意图实现交由多智能体优化的引导提示，两者迭代协同提升攻击效果。实验结果表明，JPS在多种模型和基准上，越狱成功率（ASR）和恶意意图实现率（MIFR）均达到最新水平，有效提升了实际攻击效用。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="121-MedMKEB-A-Comprehensive-Knowledge-Editing-Benchmark-for-Medical-Multimodal-Large-Language-Models"><a href="#121-MedMKEB-A-Comprehensive-Knowledge-Editing-Benchmark-for-Medical-Multimodal-Large-Language-Models" class="headerlink" title="121. MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models"></a>121. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MedMKEB__A_Comprehensive_Knowledge_Editing_Benchmark_for_Medical_Multimodal_Large_Language_Models.pdf">MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models</a></h3><p><em>教育背景：</em> Peking University</p><p>该论文提出MedMKEB，这是首个专为医学多模态大语言模型设计的知识编辑综合基准，涵盖视觉和文本信息的系统性编辑评估，包含可靠性、局部性、泛化性、可迁移性和鲁棒性五大指标。实验表明，现有知识编辑方法在医学多模态场景下表现有限，需要针对医学领域开发更精细化的编辑算法，MedMKEB将推动医学知识编辑技术的进步。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="122-Finding-Needles-in-Images-Can-Multimodal-LLMs-Locate-Fine-Details"><a href="#122-Finding-Needles-in-Images-Can-Multimodal-LLMs-Locate-Fine-Details" class="headerlink" title="122. Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?"></a>122. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Finding_Needles_in_Images__Can_Multimodal_LLMs_Locate_Fine_Details_.pdf">Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?</a></h3><p><em>教育背景：</em> Fujitsu Research India</p><p>本论文提出NiM-Benchmark，用于评估多模态大语言模型（MLLMs）在复杂文档中定位和推理细粒度信息的能力，并提出Spot-IT方法：结合语义相似性分块和自适应高斯注意力，引导模型关注关键区域。实验表明，Spot-IT在多个DocVQA数据集上均显著优于现有方法，有效提升了复杂布局中文档细节提取的准确率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="123-Evaluation-of-Large-Language-Model-Architectures-in-Abstract-Meaning-Representation-Parsing"><a href="#123-Evaluation-of-Large-Language-Model-Architectures-in-Abstract-Meaning-Representation-Parsing" class="headerlink" title="123. Evaluation of Large Language Model Architectures in Abstract Meaning Representation Parsing"></a>123. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Evaluation_of_LLMs_in_AMR_Parsing.pdf">Evaluation of Large Language Model Architectures in Abstract Meaning Representation Parsing</a></h3><p><em>教育背景：</em> University College London</p><p>本文系统评估了四种开源解码器式大语言模型（Phi-3.5、Gemma-2、LLaMA-3.2、DeepSeek-R1-LLaMA-Distilled）在AMR语义解析任务中的微调表现，采用LoRA高效参数微调和SMATCH图对齐指标，在LDC2020T02 Gold AMR3.0等数据集上进行对比分析。结果显示，经过简单微调后，LLM模型（尤其是LLaMA-3.2）在语义解析性能上接近专用SOTA解析器，Phi-3.5则在结构有效性上表现最优，证明LLM无需复杂架构即可高效完成AMR解析任务。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="124-Skin-SOAP-A-Weakly-Supervised-Framework-for-Generating-Structured-SOAP-Notes"><a href="#124-Skin-SOAP-A-Weakly-Supervised-Framework-for-Generating-Structured-SOAP-Notes" class="headerlink" title="124. Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes"></a>124. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Skin-SOAP__A_Weakly_Supervised_Framework_for_Generating_Structured_SOAP_Notes.pdf">Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes</a></h3><p><em>教育背景：</em> University of Maryland, Baltimore County</p><p>该论文提出了Skin-SOAP，一种弱监督多模态框架，通过结合皮损图像和稀疏临床文本，利用生成式语言模型生成临床描述、检索增强知识整合，并对Vision-LLaMA模型进行参数高效微调，实现结构化SOAP病历自动生成。实验结果显示，Skin-SOAP在临床相关性和结构一致性上与最先进的LLM模型（如GPT-4o等）性能相当，有助于缓解医生文档负担并提升皮肤科临床流程效率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="125-SPaRFT-Self-Paced-Reinforcement-Fine-Tuning-for-Large-Language-Models"><a href="#125-SPaRFT-Self-Paced-Reinforcement-Fine-Tuning-for-Large-Language-Models" class="headerlink" title="125. SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models"></a>125. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SPaRFT__Self-Paced_Reinforcement_Fine-Tuning_for_Large_Language_Models.pdf">SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models</a></h3><p><em>教育背景：</em> Deakin University</p><p>本论文提出SPaRFT，一种自适应的强化学习微调框架，针对大语言模型的高效训练。方法首先使用语义和难度结合的聚类进行数据精简，再以多臂赌博机动态分配训练样本，实现性能驱动的课程学习。实验显示，SPaRFT在数学推理任务中以极少样本达到或超越现有方法，显著提升小模型推理能力，同时资源消耗极低。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="126-Can-Large-Language-Models-Integrate-Spatial-Data-Empirical-Insights-into-Reasoning-Strengths-and-Computational-Weaknesses"><a href="#126-Can-Large-Language-Models-Integrate-Spatial-Data-Empirical-Insights-into-Reasoning-Strengths-and-Computational-Weaknesses" class="headerlink" title="126. Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses"></a>126. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Can_Large_Language_Models_Integrate_Spatial_Data__Empirical_Insights_into_Reasoning_Strengths_and_Co.pdf">Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses</a></h3><p><em>教育背景：</em> University of Washington</p><p>本论文系统评估了大语言模型（LLM）在城市空间数据整合中的能力，提出通过引入几何特征（如最小角度、最小距离、最大重叠面积）显著提升LLM在空间数据匹配、融合等任务的表现。实验发现，LLM难以仅凭自然语言指令完成空间推理，但若给定相关特征，则可推断出合理阈值，准确率最高可达99.5%，提出的“review-and-refine”两步自我修正方法进一步提升了结果的可靠性。结论认为，LLM能有效辅助空间数据集成，但尚不适合复杂计算几何任务。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="127-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data"><a href="#127-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data" class="headerlink" title="127. R-Zero: Self-Evolving Reasoning LLM from Zero Data"></a>127. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/R-Zero__Self-Evolving_Reasoning_LLM_from_Zero_Data.pdf">R-Zero: Self-Evolving Reasoning LLM from Zero Data</a></h3><p><em>教育背景：</em> Tencent AI Seattle Lab</p><p>本文提出了R-Zero框架，实现了无需任何外部数据、完全自主进化的推理型大语言模型。方法采用双模型协同进化机制（Challenger和Solver），通过Group Relative Policy Optimization（GRPO）实现模型自我生成、过滤与解答难题的循环，完全摆脱人工标注与现有任务依赖。实验证明，R-Zero能显著提升不同基础LLM在数学及通用推理领域的能力，并可作为有监督微调的有效增强方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="128-A-Multi-Stage-Large-Language-Model-Framework-for-Extracting-Suicide-Related-Social-Determinants-of-Health"><a href="#128-A-Multi-Stage-Large-Language-Model-Framework-for-Extracting-Suicide-Related-Social-Determinants-of-Health" class="headerlink" title="128. A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health"></a>128. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Multi-Stage_Large_Language_Model_Framework_for_Extracting_Suicide-Related_Social_Determinants_of_H.pdf">A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health</a></h3><p><em>教育背景：</em> Cockrell School of Engineering, The University of Texas at Austin</p><p>本论文提出一种多阶段大语言模型（LLM）框架，包括上下文检索、相关性验证和SDoH因子抽取三个步骤，实现对自杀相关社会健康决定因素（SDoH）从非结构化文本中的高效提取。方法在零样本设定下，对比BioBERT、GPT-3.5-turbo和DeepSeek-R1等基线，在提取少见和常见SDoH因子上取得更高的准确率和召回率，并通过提供中间解释提高了模型可解释性和人工标注效率。结论：本方法提升了从文本中提取自杀相关SDoH因子的准确性与透明度，有助于早期风险识别和干预策略制定。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="129-Situated-Epistemic-Infrastructures-A-Diagnostic-Framework-for-Post-Coherence-Knowledge"><a href="#129-Situated-Epistemic-Infrastructures-A-Diagnostic-Framework-for-Post-Coherence-Knowledge" class="headerlink" title="129. Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge"></a>129. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Situated_Epistemic_Infrastructures__A_Diagnostic_Framework_for_Post-Coherence_Knowledge.pdf">Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge</a></h3><p><em>教育背景：</em> University of Toronto</p><p>本文提出了“Situated Epistemic Infrastructures (SEI)”框架，用以诊断和分析在大语言模型（LLM）等AI技术推动下，知识生产、流通和权威认定过程中的基础结构变革。该方法强调通过基础设施、权力签名、符号压缩和故障动态四个维度，分析知识权威是如何在算法、机构、人类协作等混合系统中不断被重塑。结论指出，SEI能够帮助学者和机构识别和适应由AI驱动的知识体系不稳定性，促进更具反思性和适应性的知识组织模式。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="130-A-Metric-for-MLLM-Alignment-in-Large-scale-Recommendation"><a href="#130-A-Metric-for-MLLM-Alignment-in-Large-scale-Recommendation" class="headerlink" title="130. A Metric for MLLM Alignment in Large-scale Recommendation"></a>130. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Metric_for_MLLM_Alignment_in_Large-scale_Recommendation.pdf">A Metric for MLLM Alignment in Large-scale Recommendation</a></h3><p><em>教育背景：</em> Xiaohongshu Inc.</p><p>本文提出了Leakage Impact Score (LIS)，一种用于多模态大语言模型（MLLM）在大规模推荐系统中对齐评价的新指标。LIS通过预先量化偏好数据的上界，辅助MLLM对齐流程，显著提升了多模态推荐部署效率。实验在小红书真实场景下，包括内容推荐和广告推荐，A&#x2F;B测试显示用户时长、广告价值等指标均有显著提升，验证了方法的实际价值。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h2 id="ml">Machine Learning</h2><h3 id="131-Learning-Geometric-Aware-Quadrature-Rules-for-Functional-Minimization"><a href="#131-Learning-Geometric-Aware-Quadrature-Rules-for-Functional-Minimization" class="headerlink" title="131. Learning Geometric-Aware Quadrature Rules for Functional Minimization"></a>131. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Learning_Geometric-Aware_Quadrature_Rules_for_Functional_Minimization.pdf">Learning Geometric-Aware Quadrature Rules for Functional Minimization</a></h3><p><em>教育背景：</em> University of the Aegean</p><p>该论文提出了QuadrANN，一种基于图神经网络(GNN)的新型架构，用于从非均匀点云几何中直接学习最优积分权重，实现对变分原理下泛函最小化问题的高效数值积分。通过多层消息传递结合局部密度和全局域形状特征，QuadrANN显著降低了积分估计的方差，并在多种积分和PDE求解任务中优于传统QMC方法，提升了深度学习变分求解器的稳定性和精度。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="132-MolSnap-Snap-Fast-Molecular-Generation-with-Latent-Variational-Mean-Flow"><a href="#132-MolSnap-Snap-Fast-Molecular-Generation-with-Latent-Variational-Mean-Flow" class="headerlink" title="132. MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow"></a>132. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MolSnap__Snap-Fast_Molecular_Generation_with_Latent_Variational_Mean_Flow.pdf">MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow</a></h3><p><em>教育背景：</em> University of Kentucky</p><p>该论文提出了MolSnap框架，通过创新的因果感知Transformer（CAT）和变分均值流（VMF）方法，实现了基于文本描述的高质量、多样化分子生成。CAT显式建模分子图与文本间的因果依赖，VMF将潜在空间建模为高斯混合分布，支持高效一步推断。实验表明方法在新颖性、多样性和有效性上均优于SOTA模型，且推理速度提升显著。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="133-Multi-Modal-Multi-Behavior-Sequential-Recommendation-with-Conditional-Diffusion-Based-Feature-Denoising"><a href="#133-Multi-Modal-Multi-Behavior-Sequential-Recommendation-with-Conditional-Diffusion-Based-Feature-Denoising" class="headerlink" title="133. Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising"></a>133. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Multi-Modal_Multi-Behavior_Sequential_Recommendation_with_Conditional_Diffusion-Based_Feature_Denois.pdf">Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising</a></h3><p><em>教育背景：</em> Takway.AI</p><p>本文提出了一种多模态多行为序列推荐模型M3BSR，通过条件扩散模型分别对多模态特征和行为特征进行去噪，并引入多专家兴趣提取层以建模用户在不同模态和行为下的共性和特性兴趣。实验证明，M3BSR在公开数据集上显著优于现有先进方法，提升了偏好建模和推荐准确性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="134-EnergyPatchTST-Multi-scale-Time-Series-Transformers-with-Uncertainty-Estimation-for-Energy-Forecasting"><a href="#134-EnergyPatchTST-Multi-scale-Time-Series-Transformers-with-Uncertainty-Estimation-for-Energy-Forecasting" class="headerlink" title="134. EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting"></a>134. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EnergyPatchTST__Multi-scale_Time_Series_Transformers_with_Uncertainty_Estimation_for_Energy_Forecast.pdf">EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting</a></h3><p><em>教育背景：</em> Shanghai University</p><p>该论文提出EnergyPatchTST模型，用于能源领域的时间序列预测。方法包括多尺度特征提取、蒙特卡罗Dropout不确定性估计、未来变量集成以及预训练-微调迁移学习，有效提升能源预测的准确性和可靠性。实验结果显示，在多个能源数据集上，该方法较主流方法误差降低7-12%，并提供了可靠的不确定性区间预测，显著促进了能源领域的时间序列分析性能。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="135-Harmonic-fractal-transformation-for-modeling-complex-neuronal-effects-from-bursting-and-noise-shaping-to-waveform-sensitivity-and-noise-induced-subthreshold-spiking"><a href="#135-Harmonic-fractal-transformation-for-modeling-complex-neuronal-effects-from-bursting-and-noise-shaping-to-waveform-sensitivity-and-noise-induced-subthreshold-spiking" class="headerlink" title="135. Harmonic fractal transformation for modeling complex neuronal effects: from bursting and noise shaping to waveform sensitivity and noise-induced subthreshold spiking"></a>135. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Harmonic_fractal_transformation_for_modeling_complex_neuronal_effects__from_bursting_and_noise_shapi.pdf">Harmonic fractal transformation for modeling complex neuronal effects: from bursting and noise shaping to waveform sensitivity and noise-induced subthreshold spiking</a></h3><p><em>教育背景：</em> 90 Navigation street, Birmingham, UK</p><p>本文提出了谐波分形变换（HFT）作为一种全新的频率映射方法，能够以简单形式复刻神经元复杂效应，包括信号激发、噪声整形、波形敏感性和亚阈值噪声诱导脉冲。HFT通过分形重组输入频谱，激发新的谐波分量，实现对噪声的鲁棒性和信号放大，结论表明该方法能灵活建模多种脉冲形态与神经元特性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="136-Estimating-Musical-Surprisal-from-Audio-in-Autoregressive-Diffusion-Model-Noise-Spaces"><a href="#136-Estimating-Musical-Surprisal-from-Audio-in-Autoregressive-Diffusion-Model-Noise-Spaces" class="headerlink" title="136. Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces"></a>136. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Estimating_Musical_Surprisal_from_Audio_in_Autoregressive_Diffusion_Model_Noise_Spaces.pdf">Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces</a></h3><p><em>教育背景：</em> Johannes Kepler University Linz</p><p>该论文提出利用自回归扩散模型（EDM和RFF）在连续音频嵌入空间估计音乐惊奇性（surprisal），通过信息含量（IC）和负对数似然（NLL）衡量模型性能，并在单音高惊奇性和多轨音频分段检测两任务上与GIVT模型对比。实验结果显示，扩散模型在描述音乐数据和捕捉音高惊奇性方面优于GIVT模型，且在适当噪声水平下对音色具有更高不变性，提升分段检测精度与召回率。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="137-FlowState-Sampling-Rate-Invariant-Time-Series-Forecasting"><a href="#137-FlowState-Sampling-Rate-Invariant-Time-Series-Forecasting" class="headerlink" title="137. FlowState: Sampling Rate Invariant Time Series Forecasting"></a>137. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FlowState__Sampling_Rate_Invariant_Time_Series_Forecasting.pdf">FlowState: Sampling Rate Invariant Time Series Forecasting</a></h3><p><em>教育背景：</em> IBM Research Europe – Zurich, Switzerland</p><p>该论文提出了FlowState，一种结合S5状态空间模型（SSM）编码器和功能基解码器（FBD）的时间序列基础模型，实现了对不同采样率的自适应与连续时间建模。通过并行预测训练方案和因果归一化，FlowState在GIFT-ZS和Chronos-ZS基准上达到新的SOTA，优于体积更大的模型，具备出色的泛化能力和鲁棒性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="138-FDC-Net-Rethinking-the-association-between-EEG-artifact-removal-and-multi-dimensional-affective-computing"><a href="#138-FDC-Net-Rethinking-the-association-between-EEG-artifact-removal-and-multi-dimensional-affective-computing" class="headerlink" title="138. FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing"></a>138. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FDC-Net__Rethinking_the_association_between_EEG_artifact_removal_and_multi-dimensional_affective_com.pdf">FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing</a></h3><p><em>教育背景：</em> Beijing University of Technology</p><p>本文提出了一种端到端的EEG情感识别与去伪影协同框架FDC-Net，采用双向梯度反馈优化和频带可学习编码的Transformer（EEGSPTransformer），实现了去噪与情感识别的深度耦合与动态协作。实验结果表明，在强噪声干扰下，该方法在DEAP和DREAMER数据集上的去噪和情感识别性能均优于现有主流方法，验证了其鲁棒性和有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="139-Balancing-Accuracy-and-Novelty-with-Sub-Item-Popularity"><a href="#139-Balancing-Accuracy-and-Novelty-with-Sub-Item-Popularity" class="headerlink" title="139. Balancing Accuracy and Novelty with Sub-Item Popularity"></a>139. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Balancing_Accuracy_and_Novelty_with_Sub-Item_Popularity.pdf">Balancing Accuracy and Novelty with Sub-Item Popularity</a></h3><p><em>教育背景：</em> Politecnico di Bari</p><p>本文提出在Transformer-based推荐系统RecJPQ中引入个性化流行度信号，包括基于item-ID与sub-ID的个性化流行度分数（PPS和sPPS），通过对sub-ID流行度建模，实现更细粒度的用户重复行为捕捉。实验证明，sub-ID粒度的流行度建模在提升推荐新颖性的同时不损失准确性，并可灵活调控准确性与新颖性的权衡。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="140-Hybrid-quantum-tensor-networks-for-aeroelastic-applications"><a href="#140-Hybrid-quantum-tensor-networks-for-aeroelastic-applications" class="headerlink" title="140. Hybrid quantum tensor networks for aeroelastic applications"></a>140. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Hybrid_quantum_tensor_networks_for_aeroelastic_applications.pdf">Hybrid quantum tensor networks for aeroelastic applications</a></h3><p><em>教育背景：</em> Institute for AI Safety and Security, German Aerospace Center (DLR)</p><p>本文提出了一种面向气动弹性问题的混合量子张量网络算法，集成了张量网络（TN）降维、数据编码和张量网络启发的可训练变分量子电路（VQC），实现端到端训练。方法在气动弹性时序数据的二分类任务中取得了极高准确率（F1&gt;0.99），并在多&#x2F;单变量回归任务中表现出有前景的性能，但模型的超参数选择依然是主要挑战。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="141-Salt-Rock-Creep-Deformation-Forecasting-Using-Deep-Neural-Networks-and-Analytical-Models-for-Subsurface-Energy-Storage-Applications"><a href="#141-Salt-Rock-Creep-Deformation-Forecasting-Using-Deep-Neural-Networks-and-Analytical-Models-for-Subsurface-Energy-Storage-Applications" class="headerlink" title="141. Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications"></a>141. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Salt-Rock_Creep_Deformation_Forecasting_Using_Deep_Neural_Networks_and_Analytical_Models_for_Subsurf.pdf">Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications</a></h3><p><em>教育背景：</em> IIT (ISM) Dhanbad</p><p>该论文提出了融合深度神经网络（N-BEATS、TCN、RNN、Transformer）与分析模型（如Power Law、Burger等）的方法，对盐岩在不同压力下的蠕变变形进行时间序列预测。结果显示，深度神经网络（尤其N-BEATS和TCN）在预测精度上优于传统统计和分析模型，准确率提升15-20%，为地下盐穴能源储存的安全管理提供了更可靠的建模基础。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="142-High-Dimensional-Differentially-Private-Quantile-Regression-Distributed-Estimation-and-Statistical-Inference"><a href="#142-High-Dimensional-Differentially-Private-Quantile-Regression-Distributed-Estimation-and-Statistical-Inference" class="headerlink" title="142. High-Dimensional Differentially Private Quantile Regression: Distributed Estimation and Statistical Inference"></a>142. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/High-Dimensional_Differentially_Private_Quantile_Regression__Distributed_Estimation_and_Statistical_.pdf">High-Dimensional Differentially Private Quantile Regression: Distributed Estimation and Statistical Inference</a></h3><p><em>教育背景：</em> School of Statistics and Data Science, Shanghai University of Finance and Economics</p><p>该论文提出了一种适用于高维分布式数据的差分隐私分位数回归方法，核心包括利用Newton型变换将非光滑分位数回归问题转化为普通最小二乘问题，并结合Noisy Hard Thresholding算法实现迭代式分布式参数估计，保证稀疏性和差分隐私；在推断阶段，提出了差分隐私去偏估计和分布式私有bootstrap方法用于多重假设检验和置信区间构建。理论分析给出了估计误差界和推断一致性，实验证明算法在异质和重尾分布下具有稳健性和有效性，且揭示了隐私保护与统计精度之间的基本权衡。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="143-Graph-based-Event-Log-Repair"><a href="#143-Graph-based-Event-Log-Repair" class="headerlink" title="143. Graph-based Event Log Repair"></a>143. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Graph-based_Event_Log_Repair.pdf">Graph-based Event Log Repair</a></h3><p><em>教育背景：</em> University of Trento</p><p>该论文提出了SANAGRAPH，一种利用异构图神经网络（HGNN）编码事件日志痕迹并修复其中缺失数据的方法。方法将日志中的每个事件和属性映射为图节点，并通过SAGEConv卷积操作实现信息传播，使用多种损失函数同时优化分类和回归任务。实验对比了现有的自编码器方法，结果显示SANAGRAPH在重建活动和时间戳方面准确率更高，对所有属性的修复也表现良好，显著提升了流程挖掘数据质量。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="144-Align-for-Fusion-Harmonizing-Triple-Preferences-via-Dual-oriented-Diffusion-for-Cross-domain-Sequential-Recommendation"><a href="#144-Align-for-Fusion-Harmonizing-Triple-Preferences-via-Dual-oriented-Diffusion-for-Cross-domain-Sequential-Recommendation" class="headerlink" title="144. Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation"></a>144. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Align-for-Fusion__Harmonizing_Triple_Preferences_via_Dual-oriented_Diffusion_for_Cross-domain_Sequen.pdf">Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation</a></h3><p><em>教育背景：</em> National University of Defense Technology</p><p>本文提出了一种面向跨域序列推荐的新框架HorizonRec，结合混合条件分布检索（MDR）和双向扩散偏好建模（DPD），利用扩散模型在源域、目标域和混合域间进行细粒度的用户兴趣融合。实验表明该方法在多个真实数据集上显著提升了跨域推荐的准确性和效率，验证了MDR和DPD模块对于多域偏好对齐及鲁棒建模的有效性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="145-Q-DPTS-Quantum-Differentially-Private-Time-Series-Forecasting-via-Variational-Quantum-Circuits"><a href="#145-Q-DPTS-Quantum-Differentially-Private-Time-Series-Forecasting-via-Variational-Quantum-Circuits" class="headerlink" title="145. Q-DPTS: Quantum Differentially Private Time Series Forecasting via Variational Quantum Circuits"></a>145. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Q-DPTS__Quantum_Differentially_Private_Time_Series_Forecasting_via_Variational_Quantum_Circuits.pdf">Q-DPTS: Quantum Differentially Private Time Series Forecasting via Variational Quantum Circuits</a></h3><p><em>教育背景：</em> Independent Researcher</p><p>本文提出Q-DPTS框架，将变分量子电路（VQC）与微分隐私（Differential Privacy, DP）机制结合，用于时序数据预测。采用DP-SGD算法中的每样本梯度裁剪与高斯噪声注入，并用Rényi微分隐私会计，为量子-经典混合模型提供严格的隐私保障。实验在ETT数据集上，系统对比了LSTM、QASA、QRWKV、QLSTM等模型在不同隐私预算下的预测误差，结果表明，Q-DPTS量子模型在牺牲最小精度的前提下实现优良的隐私-效用权衡，尤其QASA和QRWKV在高噪声环境下仍优于经典模型。结论：量子差分隐私预测模型不仅可行且有效，能在敏感领域实现安全高效的时序建模。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="146-Fairness-in-Dysarthric-Speech-Synthesis-Understanding-Intrinsic-Bias-in-Dysarthric-Speech-Cloning-using-F5-TTS"><a href="#146-Fairness-in-Dysarthric-Speech-Synthesis-Understanding-Intrinsic-Bias-in-Dysarthric-Speech-Cloning-using-F5-TTS" class="headerlink" title="146. Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS"></a>146. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Fairness_in_Dysarthric_Speech_Synthesis__Understanding_Intrinsic_Bias_in_Dysarthric_Speech_Cloning_u.pdf">Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS</a></h3><p><em>教育背景：</em> International Institute of Information Technology-Hyderabad</p><p>本文提出了一个框架，利用F5-TTS零样本语音克隆技术和TORGO数据库，系统评估了合成dysarthric（构音障碍）语音在可懂度、说话人相似性和韵律保留方面的效果，并通过Parity Difference和Disparate Impact等公平性指标分析不同病情严重程度下的偏差。研究发现，F5-TTS对语音可懂度偏置较大，尤其对高严重度患者，导致数据增强时对相关辅助语音识别任务效果不佳，强调了在语音数据增强中需考虑公平性。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="147-Disentangling-Bias-by-Modeling-Intra-and-Inter-modal-Causal-Attention-for-Multimodal-Sentiment-Analysis"><a href="#147-Disentangling-Bias-by-Modeling-Intra-and-Inter-modal-Causal-Attention-for-Multimodal-Sentiment-Analysis" class="headerlink" title="147. Disentangling Bias by Modeling Intra- and Inter-modal Causal Attention for Multimodal Sentiment Analysis"></a>147. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Disentangling_Bias_by_Modeling_Intra-_and_Inter-modal_Causal_Attention_for_Multimodal_Sentiment_Anal.pdf">Disentangling Bias by Modeling Intra- and Inter-modal Causal Attention for Multimodal Sentiment Analysis</a></h3><p><em>教育背景：</em> School of Computer Science, South China Normal University</p><p>本文提出了一种多关系多模态因果干预模型（MMCI），利用因果理论中的背门调整机制，通过构建多关系图和图注意力网络显式建模文本、音频、视觉模态间的内外部依赖，分别估算和解耦因果特征与捷径特征，并动态结合以抵御数据分布偏移。实验表明，MMCI在多项基准和OOD测试数据上有效去偏、提升了多模态情感分析的泛化能力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="148-Compressed-Decentralized-Momentum-Stochastic-Gradient-Methods-for-Nonconvex-Optimization"><a href="#148-Compressed-Decentralized-Momentum-Stochastic-Gradient-Methods-for-Nonconvex-Optimization" class="headerlink" title="148. Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization"></a>148. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Compressed_Decentralized_Momentum_Stochastic_Gradient_Methods_for_Nonconvex_Optimization.pdf">Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization</a></h3><p><em>教育背景：</em> Rensselaer Polytechnic Institute</p><p>本文提出了两种用于多智能体非凸随机优化的压缩去中心化动量随机梯度算法（DAMSCo和DaSHCo），分别针对梯度有界和数据异质两种场景，结合了动量&#x2F;自适应技术与通信压缩以加速收敛并降低通信成本。理论上，两种方法均证明能达到最优收敛速率，且在一定误差容忍区间内实现线性加速和算法参数的拓扑无关性，实验上在训练深度神经网络和Transformer时优于现有方法。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr><h3 id="149-MENDR-Manifold-Explainable-Neural-Data-Representations"><a href="#149-MENDR-Manifold-Explainable-Neural-Data-Representations" class="headerlink" title="149. MENDR: Manifold Explainable Neural Data Representations"></a>149. <a href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MENDR__Manifold_Explainable_Neural_Data_Representations.pdf">MENDR: Manifold Explainable Neural Data Representations</a></h3><p><em>教育背景：</em> Georgia Institute of Technology</p><p>该论文提出MENDR，一种用于脑电（EEG）信号的基础模型，创新性地结合离散小波包变换、图神经网络（GNN）空间归一化、自监督对比学习和Riemannian流形Transformer结构，实现对EEG信号的可解释和高效表示。实验显示，MENDR在多个EEG下游任务上参数量更低但表现接近主流模型，同时提升了表示可解释性和重建能力，展现出高效、可解释、临床适用的潜力。</p><img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" alt="论文配图" width="300px" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>今日论文报纸</title>
    <link href="/2024/01/2024-01-01_article/"/>
    <url>/2024/01/2024-01-01_article/</url>
    
    <content type="html"><![CDATA[<center><h1>今日论文报纸</h1></center><p>📅 <strong>日期</strong>：2024-01-01<br>📄 <strong>发现论文数量</strong>：3  </p><h2 id="robot">Robotics</h2><h3 id="1-Uni-Fusion-Universal-Continuous-Mapping"><a href="#1-Uni-Fusion-Universal-Continuous-Mapping" class="headerlink" title="1. Uni-Fusion: Universal Continuous Mapping"></a>1. <a href="http://172.16.18.71:5000/pdfs/papers/TRO/2024/Uni_Fusion__Universal_Continuous_Mapping_10_1109_TRO_2024_3351548_1750327244.pdf">Uni-Fusion: Universal Continuous Mapping</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Julius-Maximilians-University of Würzburg</span></p><p>该论文提出了Uni-Fusion，一种无需训练的通用连续映射框架，能够对几何、表面属性（如颜色、红外）及高维特征（如CLIP嵌入）进行隐式编码。方法通过高效的高斯过程回归（GPR）和稀疏体素，将点云数据分割为网格体素并生成局部隐式特征图（LIM），逐步融合为全局三维重建，支持表面重建、属性转移和开放词汇场景理解。实验证明Uni-Fusion能在多种机器人感知任务中实现高效且高质量的连续映射，并在表面和颜色重建、语义理解等应用中达到或优于现有方法的性能。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2024-01-01/Uni-Fusion%3A%20Universal%20Continuous%20Mapping.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="2-Multivehicle-Perimeter-Defense-in-Conical-Environments"><a href="#2-Multivehicle-Perimeter-Defense-in-Conical-Environments" class="headerlink" title="2. Multivehicle Perimeter Defense in Conical Environments"></a>2. <a href="http://172.16.18.71:5000/pdfs/papers/TRO/2024/Multivehicle_Perimeter_Defense_in_Conical_Environm_10_1109_TRO_2024_3351556_1750327918.pdf">Multivehicle Perimeter Defense in Conical Environments</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Michigan State University</span></p><p>本文提出了多车协同防御锥形环境周界的在线算法，通过竞争分析框架，设计并分析了三种分散式和两种协作式算法，分别在不同参数条件下获得了明确的竞争比界（1、1.5、2等）。结论表明：分散式与协作式算法可根据车辆数量、入侵者速度等参数选择，均可在最坏情况下有效拦截入侵者，实现防御性能的理论保障。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2024-01-01/Multivehicle%20Perimeter%20Defense%20in%20Conical%20Environments.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr><h3 id="3-Fast-Contact-Implicit-Model-Predictive-Control"><a href="#3-Fast-Contact-Implicit-Model-Predictive-Control" class="headerlink" title="3. Fast Contact-Implicit Model Predictive Control"></a>3. <a href="http://172.16.18.71:5000/pdfs/papers/TRO/2024/Fast_Contact_Implicit_Model_Predictive_Control_10_1109_TRO_2024_3351554_1750382880.pdf">Fast Contact-Implicit Model Predictive Control</a></h3><p><span style="    font-size: 15px !important;     color: #555 !important;     background: #f9f9f9 !important;     padding: 6px 10px !important;     border-radius: 4px !important;     margin: 6px 0 8px !important;     display: inline-block !important;     font-style: italic !important;     /* 斜体 */    font-weight: bold !important;      /* 加粗 */">Stanford University</span></p><p>本论文提出了一种适用于与环境发生接触和断开接触的机器人系统的通用控制方法——接触隐式模型预测控制（CI-MPC）。该方法通过双层优化，将下层接触动力学建模为时变线性互补问题，并采用自定义结构化内点法求解实现高效可微分的接触动力学计算；上层则快速优化轨迹。论文在多种机器人系统（仿真与硬件）中验证了CI-MPC能够在存在模型失配和外部扰动的情况下，实时生成并跟踪非周期接触序列，表现出良好的鲁棒性和实时性。</p><img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2024-01-01/Fast%20Contact-Implicit%20Model%20Predictive%20Control.jpg" alt="论文配图" width="300px" style="display: block; margin: 0 auto;" /><hr>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
