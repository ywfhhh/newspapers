

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Ywfhhh">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="今日论文报纸">
<meta property="og:url" content="http://example.com/2025/08/14/2025-08-15_article/index.html">
<meta property="og:site_name" content="论文报纸集合">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Human-Aligned%20Procedural%20Level%20Generation%20Reinforcement%20Learning%20via%20Text-Level-Sketch%20Shared%20Representation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Generative%20Modeling%20with%20Multi-Instance%20Reward%20Learning%20for%20E-commerce%20Creative%20Optimization.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Goal%20Discovery%20with%20Causal%20Capacity%20for%20Efficient%20Reinforcement%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Edge%20General%20Intelligence%20Through%20World%20Models%20and%20Agentic%20AI%3A%20Fundamentals%2C%20Solutions%2C%20and%20Challenges.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Emergence%20of%20Hierarchies%20in%20Multi-Agent%20Self-Organizing%20Systems%20Pursuing%20a%20Joint%20Objective.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Masquerade%3A%20Learning%20from%20In-the-wild%20Human%20Videos%20using%20Data-Editing.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Vision-driven%20River%20Following%20of%20UAV%20via%20Safe%20Reinforcement%20Learning%20using%20Semantic%20Dynamics%20Model.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/PPL%3A%20Point%20Cloud%20Supervised%20Proprioceptive%20Locomotion%20Reinforcement%20Learning%20for%20Legged%20Robots%20in%20Crawl%20Spaces.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/FLARE%3A%20Agile%20Flights%20for%20Quadrotor%20Cable-Suspended%20Payload%20System%20via%20Reinforcement%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20Shank%20Angle-Based%20Control%20System%20Enables%20Soft%20Exoskeleton%20to%20Assist%20Human%20Non-Steady%20Locomotion.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Whole-Body%20Bilateral%20Teleoperation%20with%20Multi-Stage%20Object%20Parameter%20Estimation%20for%20Wheeled%20Humanoid%20Locomanipulation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Seeing%2C%20Listening%2C%20Remembering%2C%20and%20Reasoning%3A%20A%20Multimodal%20Agent%20with%20Long-Term%20Memory.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Immersive%20Teleoperation%20of%20Beyond-Human-Scale%20Robotic%20Manipulators%3A%20Challenges%20and%20Future%20Directions.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Interpretable%20Robot%20Control%20via%20Structured%20Behavior%20Trees%20and%20Large%20Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HapticGiant%3A%20A%20Novel%20Very%20Large%20Kinesthetic%20Haptic%20Interface%20with%20Hierarchical%20Force%20Control.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/BEAVR%3A%20Bimanual%2C%20multi-Embodiment%2C%20Accessible%2C%20Virtual%20Reality%20Teleoperation%20System%20for%20Robots.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/EvoCurr%3A%20Self-evolving%20Curriculum%20with%20Behavior%20Code%20Generation%20for%20Complex%20Decision-making.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Skyshield%3A%20Event-Driven%20Submillimetre%20Thin%20Obstacle%20Detection%20for%20Drone%20Flight%20Safety.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Decentralized%20Rank%20Scheduling%20for%20Energy-Constrained%20Multi-Task%20Federated%20Fine-Tuning%20in%20Edge-Assisted%20IoV%20Networks.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Distilling%20LLM%20Prior%20to%20Flow%20Model%20for%20Generalizable%20Agent%E2%80%99s%20Imagination%20in%20Object%20Goal%20Navigation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Echo-4o%3A%20Harnessing%20the%20Power%20of%20GPT-4o%20Synthetic%20Images%20for%20Improved%20Image%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20Survey%20on%203D%20Gaussian%20Splatting%20Applications%3A%20Segmentation%2C%20Editing%2C%20and%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Story2Board%3A%20A%20Training-Free%20Approach%20for%20Expressive%20Storyboard%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Noise%20Hypernetworks%3A%20Amortizing%20Test-Time%20Compute%20in%20Diffusion%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/MOC%3A%20Meta-Optimized%20Classifier%20for%20Few-Shot%20Whole%20Slide%20Image%20Classification.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/T-CACE%3A%20A%20Time-Conditioned%20Autoregressive%20Contrast%20Enhancement%20Multi-Task%20Framework%20for%20Contrast-Free%20Liver%20MRI%20Synthesis%2C%20Segmentation%2C%20and%20Diagnosis.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HumanGenesis%3A%20Agent-Based%20Geometric%20and%20Generative%20Modeling%20for%20Synthetic%20Human%20Dynamics.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Perceptual%20Reality%20Transformer%3A%20Neural%20Architectures%20for%20Simulating%20Neurological%20Perception%20Conditions.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Enhancing%20Diffusion%20Face%20Generation%20with%20Contrastive%20Embeddings%20and%20SegFormer%20Guidance.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Hierarchical%20Graph%20Attention%20Network%20for%20No-Reference%20Omnidirectional%20Image%20Quality%20Assessment.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Reforming%20SISR%20Models%20for%20PBR%20Material%20Super-Resolution%20via%20Cross-Map%20Attention.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/DSS-Prompt%3A%20Dynamic-Static%20Synergistic%20Prompting%20for%20Few-Shot%20Class-Incremental%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Stable%20Diffusion%20Models%20are%20Secretly%20Good%20at%20Visual%20In-Context%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Prototype-Guided%20Diffusion%3A%20Visual%20Conditioning%20without%20External%20Memory.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/SpeechForensics%3A%20Audio-Visual%20Speech%20Representation%20Learning%20for%20Face%20Forgery%20Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/OneVAE%3A%20Joint%20Discrete%20and%20Continuous%20Optimization%20Helps%20Discrete%20Video%20VAE%20Train%20Better.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Do%20Vision%20Transformers%20See%20Like%20Humans%3F%20Evaluating%20their%20Perceptual%20Alignment.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Reverse%20Convolution%20and%20Its%20Applications%20to%20Image%20Restoration.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Region-to-Region%3A%20Enhancing%20Generative%20Image%20Harmonization%20with%20Adaptive%20Regional%20Injection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/NEURAL%3A%20Attention-Guided%20Pruning%20for%20Unified%20Multimodal%20Resource-Constrained%20Clinical%20Evaluation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/GSFixer%3A%20Improving%203D%20Gaussian%20Splatting%20with%20Reference-Guided%20Video%20Diffusion%20Priors.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Semantic-aware%20DropSplat%3A%20Adaptive%20Pruning%20of%20Redundant%20Gaussians%20for%203D%20Aerial-View%20Segmentation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/BridgeTA%3A%20Bridging%20the%20Representation%20Gap%20in%20Knowledge%20Distillation%20via%20Teacher%20Assistant%20for%20Bird%E2%80%99s%20Eye%20View%20Map%20Segmentation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Dual%20Recursive%20Feedback%20on%20Generation%20and%20Appearance%20Latents%20for%20Pose-Robust%20Text-to-Image%20Diffusion.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/WEC-DG%3A%20Multi-Exposure%20Wavelet%20Correction%20Method%20Guided%20by%20Degradation%20Description.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Exploring%20the%20Equivalence%20of%20Closed-Set%20Generative%20and%20Real%20Data%20Augmentation%20in%20Image%20Classification.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Multimodal%20Sheaf-based%20Network%20for%20Glioblastoma%20Molecular%20Subtype%20Prediction.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/MangaDiT%3A%20Reference-Guided%20Line%20Art%20Colorization%20with%20Hierarchical%20Attention%20in%20Diffusion%20Transformers.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/NegFaceDiff%3A%20The%20Power%20of%20Negative%20Context%20in%20Identity-Conditioned%20Diffusion%20for%20Synthetic%20Face%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/The%20Brain%20Resection%20Multimodal%20Image%20Registration%20%28ReMIND2Reg%29%202025%20Challenge.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Enhancing%20Monocular%203D%20Hand%20Reconstruction%20with%20Learned%20Texture%20Priors.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Images%20Speak%20Louder%20Than%20Scores%3A%20Failure%20Mode%20Escape%20for%20Enhancing%20Generative%20Quality.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Hierarchical%20Brain%20Structure%20Modeling%20for%20Predicting%20Genotype%20of%20Glioma.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20Chain%20of%20Diagnosis%20Framework%20for%20Accurate%20and%20Explainable%20Radiology%20Report%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/GoViG%3A%20Goal-Conditioned%20Visual%20Navigation%20Instruction%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/COXNet%3A%20Cross-Layer%20Fusion%20with%20Adaptive%20Alignment%20and%20Scale%20Integration%20for%20RGBT%20Tiny%20Object%20Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Learning%20Spatial%20Decay%20for%20Vision%20Transformers.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Large%E2%80%93Small%20Model%20Collaborative%20Framework%20for%20Federated%20Continual%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Episodic%20Memory%20Representation%20for%20Long-form%20Video%20Understanding.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/CitySeg%3A%20A%203D%20Open%20Vocabulary%20Semantic%20Segmentation%20Foundation%20Model%20in%20City-scale%20Scenarios.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/SARE%3A%20Semantic-Aware%20Reconstruction%20Error%20for%20Generalizable%20Diffusion-Generated%20Image%20Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/CLIP-Flow%3A%20A%20Universal%20Discriminator%20for%20AI-Generated%20Images%20Inspired%20by%20Anomaly%20Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Gen-AFFECT%3A%20Generation%20of%20Avatar%20Fine-grained%20Facial%20Expressions%20with%20Consistent%20identiTy.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HyperKD%3A%20Distilling%20Cross-Spectral%20Knowledge%20in%20Masked%20Autoencoders%20via%20Inverse%20Domain%20Shift%20with%20Spatial-Aware%20Masking%20and%20Specialized%20Loss.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/AI%20Blob%21%20LLM-Driven%20Recontextualization%20of%20Italian%20Television%20Archives.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Neural%20Bandit%20Based%20Optimal%20LLM%20Selection%20for%20a%20Pipeline%20of%20Tasks.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/VisCodex%3A%20Unified%20Multimodal%20Code%20Generation%20via%20Merging%20Vision%20and%20Coding%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Mathematical%20Computation%20and%20Reasoning%20Errors%20by%20Large%20Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Beyond%20Na%C3%AFve%20Prompting%3A%20Strategies%20for%20Improved%20Zero-shot%20Context-aided%20Forecasting%20with%20LLMs.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/AWORLD%3A%20Dynamic%20Multi-Agent%20System%20with%20Stable%20Maneuvering%20for%20Robust%20GAIA%20Problem%20Solving.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Beyond%20Scaling%20Law%3A%20A%20Data-Efficient%20Distillation%20Framework%20for%20Reasoning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Memory%20Decoder%3A%20A%20Pretrained%2C%20Plug-and-Play%20Memory%20for%20Large%20Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Exploring%20the%20Potential%20of%20Large%20Language%20Models%20in%20Fine-Grained%20Review%20Comment%20Classification.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Provable%20In-Context%20Vector%20Arithmetic%20via%20Retrieving%20Task%20Concepts.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20Comprehensive%20Evaluation%20Framework%20of%20Alignment%20Techniques%20for%20LLMs.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/RAGulating%20Compliance%3A%20A%20Multi-Agent%20Knowledge%20Graph%20for%20Regulatory%20QA.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Speed%20Always%20Wins%3A%20A%20Survey%20on%20Efficient%20Architectures%20for%20Large%20Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/ViMoNet%3A%20A%20Multimodal%20Vision-Language%20Framework%20for%20Human%20Behavior%20Understanding%20from%20Motion%20and%20Video.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Describe%20What%20You%20See%20with%20Multimodal%20Large%20Language%20Models%20to%20Enhance%20Video%20Recommendations.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Can%20LLM-Generated%20Textual%20Explanations%20Enhance%20Model%20Classification%20Performance%3F%20An%20Empirical%20Study.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/The%20PacifAIst%20Benchmark%3A%20Would%20an%20Artificial%20Intelligence%20Choose%20to%20Sacrifice%20Itself%20for%20Human%20Safety%3F.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Sample%20More%20to%20Think%20Less%3A%20Group%20Filtered%20Policy%20Optimization%20for%20Concise%20Reasoning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Improving%20ARDS%20Diagnosis%20Through%20Context-Aware%20Concept%20Bottleneck%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/On%20Negative-aware%20Preference%20Optimization%20for%20Recommendation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/TimeMKG%3A%20Knowledge-Infused%20Causal%20Reasoning%20for%20Multivariate%20Time%20Series%20Modeling.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HierMoE%3A%20Accelerating%20MoE%20Training%20with%20Hierarchical%20Token%20Deduplication%20and%20Expert%20Swap.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/UDA%3A%20Unsupervised%20Debiasing%20Alignment%20for%20Pair-wise%20LLM-as-a-Judge.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Evaluating%20the%20Role%20of%20Large%20Language%20Models%20in%20Legal%20Practice%20in%20India.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/MEML-GRPO%3A%20Heterogeneous%20Multi-Expert%20Mutual%20Learning%20for%20RLVR%20Advancement.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/AmbiGraph-Eval%3A%20Can%20LLMs%20Effectively%20Handle%20Ambiguous%20Graph%20Queries%3F.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/SYNAPSE-G%3A%20Bridging%20Large%20Language%20Models%20and%20Graph%20Learning%20for%20Rare%20Event%20Classification.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Your%20Coding%20Intent%20is%20Secretly%20in%20the%20Context%20and%20You%20Should%20Deliberately%20Infer%20It%20Before%20Completion.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Enhancing%20Memory%20Recall%20in%20LLMs%20with%20Gauss-Tin%3A%20A%20Hybrid%20Instructional%20and%20Gaussian%20Replay%20Approach.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/NeuronTune%3A%20Fine-Grained%20Neuron%20Modulation%20for%20Balanced%20Safety-Utility%20Alignment%20in%20LLMs.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Hallucination%20vs%20interpretation%3A%20rethinking%20accuracy%20and%20precision%20in%20AI-assisted%20data%20extraction%20for%20knowledge%20synthesis.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Shadow%20in%20the%20Cache%3A%20Unveiling%20and%20Mitigating%20Privacy%20Risks%20of%20KV-cache%20in%20LLM%20Inference.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/COMPEER%3A%20Controllable%20Empathetic%20Reinforcement%20Reasoning%20for%20Emotional%20Support%20Conversation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/An%20Automated%20Multi-Modal%20Evaluation%20Framework%20for%20Mobile%20Intelligent%20Assistants%20Based%20on%20Large%20Language%20Models%20and%20Multi-Agent%20Collaboration.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Learning%20Facts%20at%20Scale%20with%20Active%20Reading.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/EGGS-PTP%3A%20An%20Expander-Graph%20Guided%20Structured%20Post-training%20Pruning%20Method%20for%20Large%20Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Modern%20Neural%20Networks%20for%20Small%20Tabular%20Datasets%3A%20The%20New%20Default%20for%20Field-Scale%20Digital%20Soil%20Mapping%3F.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Explainable%20Ensemble%20Learning%20for%20Graph-Based%20Malware%20Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/GraphTreeGen%3A%20Subtree-Centric%20Approach%20to%20Efficient%20and%20Supervised%20Graph%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Personalized%20Product%20Search%20Ranking%3A%20A%20Multi-Task%20Learning%20Approach%20with%20Tabular%20and%20Non-Tabular%20Data.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/CWFBind%3A%20Geometry-Awareness%20for%20Fast%20and%20Accurate%20Protein-Ligand%20Docking.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Open-Set%20Fault%20Diagnosis%20in%20Multimode%20Processes%20via%20Fine-Grained%20Deep%20Feature%20Representation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20pseudo-inverse%20of%20a%20line%20graph.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/DeepFeatIoT%3A%20Unifying%20Deep%20Learned%2C%20Randomized%2C%20and%20LLM%20Features%20for%20Enhanced%20IoT%20Time%20Series%20Sensor%20Data%20Classification%20in%20Smart%20Industries.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Graph%20Neural%20Network%20and%20Transformer%20Integration%20for%20Unsupervised%20System%20Anomaly%20Discovery.jpg">
<meta property="article:published_time" content="2025-08-14T16:00:00.000Z">
<meta property="article:modified_time" content="2025-08-15T01:55:38.863Z">
<meta property="article:author" content="Ywfhhh">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Human-Aligned%20Procedural%20Level%20Generation%20Reinforcement%20Learning%20via%20Text-Level-Sketch%20Shared%20Representation.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>今日论文报纸 - 论文报纸集合</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>【学术动态报告】论文报纸</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文集合</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="今日论文报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-14 16:00" pubdate>
          星期四, 八月 14日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          17k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">今日论文报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>今日论文报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-15<br>📄 <strong>发现论文数量</strong>：109  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Human-Aligned-Procedural-Level-Generation-Reinforcement-Learning-via-Text-Level-Sketch-Shared-Representation"><a href="#1-Human-Aligned-Procedural-Level-Generation-Reinforcement-Learning-via-Text-Level-Sketch-Shared-Representation" class="headerlink" title="1. Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Human-Aligned_Procedural_Level_Generation_Reinforcement_Learning_via_Text-Level-Sketch_Shared_Repres.pdf">Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Gwangju Institute of Science and Technology</span></p>
<p>该论文提出VIPCGRL框架，通过四模态对比学习（文本、关卡、草图与人&#x2F;AI风格）建立共享表征空间，并在多模态条件下利用辅助奖励实现与人类风格对齐的深度强化学习关卡生成。实验显示VIPCGRL在生成2D游戏关卡时，在可控性和拟人性上均优于现有方法，并能灵活支持多模态输入，用户研究进一步验证了其人类风格和文本&#x2F;草图指令对齐能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Human-Aligned%20Procedural%20Level%20Generation%20Reinforcement%20Learning%20via%20Text-Level-Sketch%20Shared%20Representation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-Generative-Modeling-with-Multi-Instance-Reward-Learning-for-E-commerce-Creative-Optimization"><a href="#2-Generative-Modeling-with-Multi-Instance-Reward-Learning-for-E-commerce-Creative-Optimization" class="headerlink" title="2. Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Generative_Modeling_with_Multi-Instance_Reward_Learning_for_E-commerce_Creative_Optimization.pdf">Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">JD.com</span></p>
<p>该论文提出了GenCO框架，将生成式建模与多实例奖励学习结合应用于电商创意优化。方法包括第一阶段采用非自回归、上下文感知的生成模型高效生成多样化的创意组合，并通过强化学习优化生成策略，第二阶段采用多实例学习模型将组合层级的用户反馈（如点击）归因到各个创意元素，实现更精准的反馈信号与生成优化。论文在真实电商平台上线验证，显著提升CTR和广告收入，显示出强大的实际商业价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Generative%20Modeling%20with%20Multi-Instance%20Reward%20Learning%20for%20E-commerce%20Creative%20Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-Goal-Discovery-with-Causal-Capacity-for-Efficient-Reinforcement-Learning"><a href="#3-Goal-Discovery-with-Causal-Capacity-for-Efficient-Reinforcement-Learning" class="headerlink" title="3. Goal Discovery with Causal Capacity for Efficient Reinforcement Learning"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Goal_Discovery_with_Causal_Capacity_for_Efficient_Reinforcement_Learning.pdf">Goal Discovery with Causal Capacity for Efficient Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本文提出了一种结合因果推断的目标发现方法（GDCC），通过引入“因果容量”来度量状态下动作对未来轨迹的最大影响力，并用蒙特卡洛和聚类方法在高维连续空间中高效估算关键决策点（子目标），以指导强化学习智能体的有效探索。实验证明，该方法在MuJoCo和Habitat等多目标稀疏奖励任务中，能自动发现物理意义明确的子目标，有效提升探索效率和任务成功率，优于多种基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Goal%20Discovery%20with%20Causal%20Capacity%20for%20Efficient%20Reinforcement%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-Edge-General-Intelligence-Through-World-Models-and-Agentic-AI-Fundamentals-Solutions-and-Challenges"><a href="#4-Edge-General-Intelligence-Through-World-Models-and-Agentic-AI-Fundamentals-Solutions-and-Challenges" class="headerlink" title="4. Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Edge_General_Intelligence_Through_World_Models_and_Agentic_AI__Fundamentals,_Solutions,_and_Challeng.pdf">Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>该论文系统梳理了世界模型（world models）在边缘通用智能（Edge General Intelligence, EGI）中的作用，提出将世界模型作为认知内核，通过内部环境模拟与想象，实现边缘分布式智能体的感知、推理和自主决策。论文分析了世界模型的关键架构（如编码器、动态建模、解码器）、主要技术路线（变分自编码器、RNN、Transformer等）、典型应用场景（车联网、无人机网络、物联网、虚拟化网络功能等），并对与基础模型和数字孪生的协同进行了探讨。结论指出世界模型显著提升了边缘智能系统的前瞻性、效率与安全性，但在算力、实时性与泛化能力等实际部署上仍需进一步研究。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Edge%20General%20Intelligence%20Through%20World%20Models%20and%20Agentic%20AI%3A%20Fundamentals%2C%20Solutions%2C%20and%20Challenges.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-Emergence-of-Hierarchies-in-Multi-Agent-Self-Organizing-Systems-Pursuing-a-Joint-Objective"><a href="#5-Emergence-of-Hierarchies-in-Multi-Agent-Self-Organizing-Systems-Pursuing-a-Joint-Objective" class="headerlink" title="5. Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Emergence_of_Hierarchies_in_Multi-Agent_Self-Organizing_Systems_Pursuing_a_Joint_Objective.pdf">Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>该论文通过多智能体强化学习（MARL）方法，基于MADDPG算法，研究了多智能体自组织系统（MASOS）在协同完成箱子推动任务过程中依赖层级结构的自发涌现及其动态变化。方法核心在于利用动作对其他智能体状态的梯度衡量依赖性，量化并分析层级结构的形成与演化。结论表明：MASOS在追求共同目标时会自发形成动态演化的层级结构，层级变化受环境与网络初始化影响，且“天赋”与“努力”在环境中的动态作用促进了层级的调整与优化。这为MASOS的自组织行为理解和设计提供了理论指导。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Emergence%20of%20Hierarchies%20in%20Multi-Agent%20Self-Organizing%20Systems%20Pursuing%20a%20Joint%20Objective.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="6-Masquerade-Learning-from-In-the-wild-Human-Videos-using-Data-Editing"><a href="#6-Masquerade-Learning-from-In-the-wild-Human-Videos-using-Data-Editing" class="headerlink" title="6. Masquerade: Learning from In-the-wild Human Videos using Data-Editing"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Masquerade__Learning_from_In-the-wild_Human_Videos_using_Data-Editing.pdf">Masquerade: Learning from In-the-wild Human Videos using Data-Editing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>Masquerade提出了一种通过数据编辑将大规模野外人类视频转换为“机器人化”演示的方法，包括3D手部姿态估计、手臂修复和机器人覆盖，从而弥合人类和机器人的视觉表现差距。该方法先用编辑后的人类视频预训练视觉编码器，再与少量真实机器人演示进行联合训练，实现了在三类厨房任务中对未见环境的零样本迁移，成功率显著高于现有方法，且扩展视频规模可持续提升性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Masquerade%3A%20Learning%20from%20In-the-wild%20Human%20Videos%20using%20Data-Editing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-Vision-driven-River-Following-of-UAV-via-Safe-Reinforcement-Learning-using-Semantic-Dynamics-Model"><a href="#7-Vision-driven-River-Following-of-UAV-via-Safe-Reinforcement-Learning-using-Semantic-Dynamics-Model" class="headerlink" title="7. Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Vision-driven_River_Following_of_UAV_via_Safe_Reinforcement_Learning_using_Semantic_Dynamics_Model.pdf">Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Purdue University</span></p>
<p>本论文提出了一种基于视觉的无人机河流跟随安全强化学习框架。方法方面，作者引入了边际增益优势估计（MGAE）处理非马尔可夫亚模奖励，提出了基于同伦变换的语义动力学模型（SDM）实现高效视觉预测，并整合为受约束的行动者动力学估计器（CADE）以提升安全强化学习在部分可观测环境中的表现。实验结果表明MGAE收敛更快且优于传统方法，SDM在短期状态预测更准确，CADE能有效平衡任务奖励和安全，拉格朗日方法训练下表现最佳，推理时安全层进一步提升效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Vision-driven%20River%20Following%20of%20UAV%20via%20Safe%20Reinforcement%20Learning%20using%20Semantic%20Dynamics%20Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-PPL-Point-Cloud-Supervised-Proprioceptive-Locomotion-Reinforcement-Learning-for-Legged-Robots-in-Crawl-Spaces"><a href="#8-PPL-Point-Cloud-Supervised-Proprioceptive-Locomotion-Reinforcement-Learning-for-Legged-Robots-in-Crawl-Spaces" class="headerlink" title="8. PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/PPL__Point_Cloud_Supervised_Proprioceptive_Locomotion_Reinforcement_Learning_for_Legged_Robots_in_Cr.pdf">PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了一种用于腿式机器人在狭小空间（如低矮隧道、洞穴等）自主穿越的点云监督本体感知强化学习方法（PPL）。该方法设计了基于历史本体传感器数据的状态估计网络（PPL-Net），结合极坐标点云特征提取，以及考虑全身碰撞惩罚和碰撞后速度奖励的奖励函数，实现了机器人仅凭本体感知在正常及低能见度环境下敏捷穿越狭窄空间。实验结果表明，PPL在平坦和阶梯隧道中的成功率显著优于现有方法，并能适应多种复杂和低可见环境，无需外部传感器。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/PPL%3A%20Point%20Cloud%20Supervised%20Proprioceptive%20Locomotion%20Reinforcement%20Learning%20for%20Legged%20Robots%20in%20Crawl%20Spaces.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-FLARE-Agile-Flights-for-Quadrotor-Cable-Suspended-Payload-System-via-Reinforcement-Learning"><a href="#9-FLARE-Agile-Flights-for-Quadrotor-Cable-Suspended-Payload-System-via-Reinforcement-Learning" class="headerlink" title="9. FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/FLARE__Agile_Flights_for_Quadrotor_Cable-Suspended_Payload_System_via_Reinforcement_Learning.pdf">FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出FLARE，一种基于强化学习（RL）的自主导航框架，针对四旋翼缆绳悬挂载荷系统的敏捷飞行问题。方法通过在高保真仿真中训练无模型RL策略，设计了奖励函数和观测空间以适应系统的复杂动力学，并在三个具有挑战性的场景（敏捷航点通过、载荷定位、门洞穿越）下进行验证。实验结果显示，所提出方法在敏捷性和实时性上显著优于基于优化的方法，实现了3倍速度提升，并成功实现了零样本仿真到现实的策略迁移，兼具敏捷性与安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/FLARE%3A%20Agile%20Flights%20for%20Quadrotor%20Cable-Suspended%20Payload%20System%20via%20Reinforcement%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-A-Shank-Angle-Based-Control-System-Enables-Soft-Exoskeleton-to-Assist-Human-Non-Steady-Locomotion"><a href="#10-A-Shank-Angle-Based-Control-System-Enables-Soft-Exoskeleton-to-Assist-Human-Non-Steady-Locomotion" class="headerlink" title="10. A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_Shank_Angle-Based_Control_System_Enables_Soft_Exoskeleton_to_Assist_Human_Non-Steady_Locomotion.pdf">A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Laboratory of Robotics and Intelligent Systems, Shenyang Institute of Automation, Chinese Academy of Sciences</span></p>
<p>本论文提出了一种基于小腿倾角（shank angle）的软体踝关节外骨骼控制系统，通过双高斯模型实时在线生成仿生辅助力型谱，以实现跨步适应，并采用模型驱动的前馈控制实现精确跟踪，全部依赖IMU传感器数据。实验结果表明，该系统在行走、跑步及楼梯等非稳态运动中可持续与人体步态生物力学高度同步，显著降低肌肉激活和代谢率，有效提升了外骨骼在复杂环境下的助力效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20Shank%20Angle-Based%20Control%20System%20Enables%20Soft%20Exoskeleton%20to%20Assist%20Human%20Non-Steady%20Locomotion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-Whole-Body-Bilateral-Teleoperation-with-Multi-Stage-Object-Parameter-Estimation-for-Wheeled-Humanoid-Locomanipulation"><a href="#11-Whole-Body-Bilateral-Teleoperation-with-Multi-Stage-Object-Parameter-Estimation-for-Wheeled-Humanoid-Locomanipulation" class="headerlink" title="11. Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Whole-Body_Bilateral_Teleoperation_with_Multi-Stage_Object_Parameter_Estimation_for_Wheeled_Humanoid.pdf">Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Urbana-Champaign</span></p>
<p>该论文提出了面向轮式人形机器人全身双向远程操作的多阶段在线物体惯性参数估算框架。方法核心为将视觉与视觉-语言模型生成的物体尺寸和材料等先验，结合高保真仿真并行的分层采样优化（DH-CEM），实现物体惯性参数的快速、物理可行估算，并用于实时调整机器人控制器的平衡点和动态补偿。实验在自研硬件平台SATYRR上验证了系统可高效实现重物搬运、提升操作同步性和操作员体验，并提升了安全任务如避障的可靠性。结论：集成多模态估算和仿真采样的框架，使轮式人形机器人在复杂物理交互任务中表现出更强的动态适应性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Whole-Body%20Bilateral%20Teleoperation%20with%20Multi-Stage%20Object%20Parameter%20Estimation%20for%20Wheeled%20Humanoid%20Locomanipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory"><a href="#12-Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory" class="headerlink" title="12. Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Seeing,_Listening,_Remembering,_and_Reasoning__A_Multimodal_Agent_with_Long-Term_Memory.pdf">Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ByteDance Seed</span></p>
<p>本文提出了M3-Agent，一个具备长期记忆的多模态智能体框架，能够通过视频和音频流实时感知环境，构建和更新具备实体中心结构的多模态长期记忆（包括情节和语义记忆），并通过多轮推理检索相关记忆以自主完成复杂任务。论文还开发了M3-Bench长视频问答基准，涵盖机器人视角和网络多场景，实验结果表明M3-Agent在多个基准上均大幅超越现有多模态大模型和视频理解方法。结论：M3-Agent显著提升了多模态智能体的长期记忆和推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Seeing%2C%20Listening%2C%20Remembering%2C%20and%20Reasoning%3A%20A%20Multimodal%20Agent%20with%20Long-Term%20Memory.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-Immersive-Teleoperation-of-Beyond-Human-Scale-Robotic-Manipulators-Challenges-and-Future-Directions"><a href="#13-Immersive-Teleoperation-of-Beyond-Human-Scale-Robotic-Manipulators-Challenges-and-Future-Directions" class="headerlink" title="13. Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Immersive_Teleoperation_of_Beyond-Human-Scale_Robotic_Manipulators__Challenges_and_Future_Directions.pdf">Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tampere University</span></p>
<p>该论文系统分析了超越人类尺度机器人操控（BHSRMs）沉浸式远程操作的控制、认知与界面设计挑战，提出通过高保真视觉与全臂力反馈（如外骨骼）提升操作员的沉浸感和身体感，实现更自然的人机协作。结论指出，沉浸式界面虽提升操作自然性和数据质量，但也带来感知冲突和安全风险，需发展新的人因评价工具和自适应反馈策略，为大规模学习驱动的远程机器人系统奠定基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Immersive%20Teleoperation%20of%20Beyond-Human-Scale%20Robotic%20Manipulators%3A%20Challenges%20and%20Future%20Directions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-Interpretable-Robot-Control-via-Structured-Behavior-Trees-and-Large-Language-Models"><a href="#14-Interpretable-Robot-Control-via-Structured-Behavior-Trees-and-Large-Language-Models" class="headerlink" title="14. Interpretable Robot Control via Structured Behavior Trees and Large Language Models"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Interpretable_Robot_Control_via_Structured_Behavior_Trees_and_Large_Language_Models.pdf">Interpretable Robot Control via Structured Behavior Trees and Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Luxembourg</span></p>
<p>本文提出了一种将大型语言模型（LLM）与行为树（BT）相结合的机器人控制框架，通过自然语言指令实现对机器人的可解释、模块化和实时控制。方法包括基于GPT-4o的语义解析、自动行为选择、结构化行为树执行以及插件式感知模块（如目标跟踪、手势识别）；实验结果显示该系统在多平台环境下指令理解与动作执行的平均准确率达到94%，有效提升了机器人在人机交互中的适应性和透明度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Interpretable%20Robot%20Control%20via%20Structured%20Behavior%20Trees%20and%20Large%20Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-HapticGiant-A-Novel-Very-Large-Kinesthetic-Haptic-Interface-with-Hierarchical-Force-Control"><a href="#15-HapticGiant-A-Novel-Very-Large-Kinesthetic-Haptic-Interface-with-Hierarchical-Force-Control" class="headerlink" title="15. HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/HapticGiant__A_Novel_Very_Large_Kinesthetic_Haptic_Interface_with_Hierarchical_Force_Control.pdf">HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Karlsruhe Institute of Technology</span></p>
<p>本文提出HapticGiant，一种超大型贴合人体手臂运动学的本体型动力触觉界面，结合2D龙门预定位单元和自定义六自由度机械臂，实现房间级空间内的自然步行和全方位触觉反馈。方法上创新性采用分层优化的感受型力控制算法，统一处理关节、工作空间和奇异点等系统约束，实现任意串联运动链和笛卡尔感受的实时触觉渲染。实验展示了系统在虚拟门和挖掘机等数字孪生交互中的高度沉浸性和可用性。结论：HapticGiant具备极大空间和优秀运动学匹配，通过高层控制方案显著提升虚拟现实中的沉浸式动力触觉体验。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HapticGiant%3A%20A%20Novel%20Very%20Large%20Kinesthetic%20Haptic%20Interface%20with%20Hierarchical%20Force%20Control.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-BEAVR-Bimanual-multi-Embodiment-Accessible-Virtual-Reality-Teleoperation-System-for-Robots"><a href="#16-BEAVR-Bimanual-multi-Embodiment-Accessible-Virtual-Reality-Teleoperation-System-for-Robots" class="headerlink" title="16. BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/BEAVR__Bimanual,_multi-Embodiment,_Accessible,_Virtual_Reality_Teleoperation_System_for_Robots.pdf">BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Massachusetts Institute of Technology</span></p>
<p>本论文提出了BEAVR系统——一个开源、模块化、硬件无关的虚拟现实（VR）机器人远程操作平台，支持7-DoF机械臂到全身人形机器人等多种机器人形态，通过低延迟（≤35 ms）流媒体架构和异步“think–act”控制环，实现实时灵巧双手操作、标准化数据采集（LeRobot格式）及端到端策略学习。实验表明，BEAVR兼容多种主流视觉-运动策略（ACT、DiffusionPolicy、SmolVLA），在多任务、多机器人环境下保持高成功率和低时延，极大提升了远程操作系统的可扩展性和学习友好性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/BEAVR%3A%20Bimanual%2C%20multi-Embodiment%2C%20Accessible%2C%20Virtual%20Reality%20Teleoperation%20System%20for%20Robots.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-EvoCurr-Self-evolving-Curriculum-with-Behavior-Code-Generation-for-Complex-Decision-making"><a href="#17-EvoCurr-Self-evolving-Curriculum-with-Behavior-Code-Generation-for-Complex-Decision-making" class="headerlink" title="17. EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/EvoCurr__Self-evolving_Curriculum_with_Behavior_Code_Generation_for_Complex_Decision-making.pdf">EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhongguancun Academy</span></p>
<p>本文提出了EvoCurr框架，通过两个大型语言模型（LLM）分别担任自适应课程设计和行为代码生成角色，实现复杂决策任务（以StarCraft II为例）中的逐步难度提升与策略进化。方法包括课程设计LLM根据行为代码LLM的表现动态调整任务难度，并生成适应性决策树脚本。实验表明，EvoCurr显著提升了复杂任务的成功率和解题效率，证明了自进化课程学习在高复杂度自动推理中的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/EvoCurr%3A%20Self-evolving%20Curriculum%20with%20Behavior%20Code%20Generation%20for%20Complex%20Decision-making.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-Skyshield-Event-Driven-Submillimetre-Thin-Obstacle-Detection-for-Drone-Flight-Safety"><a href="#18-Skyshield-Event-Driven-Submillimetre-Thin-Obstacle-Detection-for-Drone-Flight-Safety" class="headerlink" title="18. Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Skyshield__Event-Driven_Submillimetre_Thin_Obstacle_Detection_for_Drone_Flight_Safety.pdf">Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen International Graduate School, Tsinghua University</span></p>
<p>本论文提出了Skyshield系统，基于事件相机，采用轻量级U-Net（LUnet）网络和创新的Dice-Contour Regularization损失，实现对无人机亚毫米级细障碍物的实时检测。实验表明，该方法在边缘平台上显著优于传统方法，具有高精度和低延迟，提升了无人机的飞行安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Skyshield%3A%20Event-Driven%20Submillimetre%20Thin%20Obstacle%20Detection%20for%20Drone%20Flight%20Safety.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-Decentralized-Rank-Scheduling-for-Energy-Constrained-Multi-Task-Federated-Fine-Tuning-in-Edge-Assisted-IoV-Networks"><a href="#19-Decentralized-Rank-Scheduling-for-Energy-Constrained-Multi-Task-Federated-Fine-Tuning-in-Edge-Assisted-IoV-Networks" class="headerlink" title="19. Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Decentralized_Rank_Scheduling_for_Energy-Constrained_Multi-Task_Federated_Fine-Tuning_in_Edge-Assist.pdf">Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-sen University</span></p>
<p>本论文提出一种分层联邦微调框架，结合LoRA低秩适配与去中心化能耗约束的rank优化，适用于动态、资源受限的车联网（IoV）场景。方法上，创新性地将LoRA rank选择建模为带能耗约束的多臂赌博机(MAB)问题，并提出UCB-DUAL分布式算法，实现任务级能耗分配与客户端级自适应rank分配，有理论保证的次线性遗憾。实验结果表明，该方法在大规模仿真下可提升平均精度2.5%，降低延迟24%，同时减少显存占用，优于多种基线。结论：该框架能高效实现车联网边缘智能的多任务自适应联邦微调。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Decentralized%20Rank%20Scheduling%20for%20Energy-Constrained%20Multi-Task%20Federated%20Fine-Tuning%20in%20Edge-Assisted%20IoV%20Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-Distilling-LLM-Prior-to-Flow-Model-for-Generalizable-Agent’s-Imagination-in-Object-Goal-Navigation"><a href="#20-Distilling-LLM-Prior-to-Flow-Model-for-Generalizable-Agent’s-Imagination-in-Object-Goal-Navigation" class="headerlink" title="20. Distilling LLM Prior to Flow Model for Generalizable Agent’s Imagination in Object Goal Navigation"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Distilling_LLM_Prior_to_Flow_Model_for_Generalizable_Agent's_Imagination_in_Object_Goal_Navigation.pdf">Distilling LLM Prior to Flow Model for Generalizable Agent’s Imagination in Object Goal Navigation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science and Engineering, Sun Yat-sen University</span></p>
<p>该论文提出GOAL框架，通过在训练阶段将大语言模型（LLM）推理得到的空间语义先验以高斯场形式注入到语义地图，结合生成式流模型对室内场景的语义分布进行建模，实现了对未观测区域的高质量推理。实验表明，GOAL在Gibson、MP3D及迁移到HM3D等数据集上均取得了当前最优的导航表现，显著提升了泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Distilling%20LLM%20Prior%20to%20Flow%20Model%20for%20Generalizable%20Agent%E2%80%99s%20Imagination%20in%20Object%20Goal%20Navigation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="21-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation"><a href="#21-Echo-4o-Harnessing-the-Power-of-GPT-4o-Synthetic-Images-for-Improved-Image-Generation" class="headerlink" title="21. Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Echo-4o__Harnessing_the_Power_of_GPT-4o_Synthetic_Images_for_Improved_Image_Generation.pdf">Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Artificial Intelligence Laboratory</span></p>
<p>本论文提出Echo-4o-Image，一个由GPT-4o生成的18万规模合成图像数据集，涵盖超现实场景、多参考图像生成和复杂指令跟随任务，专为弥补真实世界数据稀缺场景和提升指令对齐而设计。在此基础上，作者微调统一多模态生成模型Bagel得到Echo-4o，并引入GenEval++和Imagine-Bench两个新基准，系统评测指令跟随与创意生成能力。实验表明Echo-4o在多个标准和新基准下均取得领先，且合成数据对多种主流基础模型均有显著迁移增益，验证了高质量合成数据集对提升图像生成模型多样性、指令对齐及创新能力的价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Echo-4o%3A%20Harnessing%20the%20Power%20of%20GPT-4o%20Synthetic%20Images%20for%20Improved%20Image%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-A-Survey-on-3D-Gaussian-Splatting-Applications-Segmentation-Editing-and-Generation"><a href="#22-A-Survey-on-3D-Gaussian-Splatting-Applications-Segmentation-Editing-and-Generation" class="headerlink" title="22. A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_Survey_on_3D_Gaussian_Splatting_Applications__Segmentation,_Editing,_and_Generation.pdf">A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai University of Finance and Economics</span></p>
<p>该论文系统综述了3D Gaussian Splatting（3DGS）在分割、编辑和生成等下游视觉任务中的最新进展，详细介绍了2D基础模型（如CLIP、SAM、DINO等）与3DGS的结合方式，并针对三大应用方向梳理了代表性方法、学习范式和评测基准。结论指出3DGS在场景理解、编辑和生成等高层视觉任务中展现出高效、可扩展和语义丰富的能力，并提出未来需完善大规模训练、三维评价体系与多模态语言模型整合。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20Survey%20on%203D%20Gaussian%20Splatting%20Applications%3A%20Segmentation%2C%20Editing%2C%20and%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation"><a href="#23-Story2Board-A-Training-Free-Approach-for-Expressive-Storyboard-Generation" class="headerlink" title="23. Story2Board: A Training-Free Approach for Expressive Storyboard Generation"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Story2Board__A_Training-Free_Approach_for_Expressive_Storyboard_Generation.pdf">Story2Board: A Training-Free Approach for Expressive Storyboard Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hebrew University of Jerusalem</span></p>
<p>Story2Board提出了一种训练无关的框架，实现从自然语言生成富有表现力且一致的多面板故事板。方法核心包括Latent Panel Anchoring和Reciprocal Attention Value Mixing，增强扩散模型在多场景下角色一致性和布局多样性，无需模型微调或架构修改。实验显示该方法在角色一致性、场景多样性及用户偏好上优于现有基线，提升了视觉叙事的表现力和连贯性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Story2Board%3A%20A%20Training-Free%20Approach%20for%20Expressive%20Storyboard%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models"><a href="#24-Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models" class="headerlink" title="24. Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Noise_Hypernetworks__Amortizing_Test-Time_Compute_in_Diffusion_Models.pdf">Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University of Munich</span></p>
<p>该论文提出了一种用于扩散生成模型的Noise Hypernetworks（HyperNoise）方法，通过训练一个轻量级超网络直接预测优化后的初始噪声，从而将原本消耗大量计算资源的测试时噪声优化过程转移到训练后阶段。实验结果表明，该方法在保持极低推理延迟的情况下，显著提升了当前主流文本到图像扩散模型的生成质量，有效避免了直接微调带来的模式崩溃和“奖励黑客”问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Noise%20Hypernetworks%3A%20Amortizing%20Test-Time%20Compute%20in%20Diffusion%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-MOC-Meta-Optimized-Classifier-for-Few-Shot-Whole-Slide-Image-Classification"><a href="#25-MOC-Meta-Optimized-Classifier-for-Few-Shot-Whole-Slide-Image-Classification" class="headerlink" title="25. MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/MOC__Meta-Optimized_Classifier_for_Few-Shot_Whole_Slide_Image_Classification.pdf">MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>本论文提出了一种面向少样本全切片图像（WSI）分类的新型Meta-Optimized Classifier (MOC)。其核心方法包含一个自动从多种分类器中动态配置的meta-learner和一个包含多样候选分类器的分类器库，使分类器在不同病理特征上互补，对全切片图像进行更全面的诊断。实验表明，MOC在多个少样本基准数据集上优于现有方法，AUC提升最高可达26.25%，为临床数据稀缺场景下的诊断部署提供关键进展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/MOC%3A%20Meta-Optimized%20Classifier%20for%20Few-Shot%20Whole%20Slide%20Image%20Classification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-T-CACE-A-Time-Conditioned-Autoregressive-Contrast-Enhancement-Multi-Task-Framework-for-Contrast-Free-Liver-MRI-Synthesis-Segmentation-and-Diagnosis"><a href="#26-T-CACE-A-Time-Conditioned-Autoregressive-Contrast-Enhancement-Multi-Task-Framework-for-Contrast-Free-Liver-MRI-Synthesis-Segmentation-and-Diagnosis" class="headerlink" title="26. T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/T-CACE__A_Time-Conditioned_Autoregressive_Contrast_Enhancement_Multi-Task_Framework_for_Contrast-Fre.pdf">T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Toronto Metropolitan University</span></p>
<p>该论文提出了一种时间条件自回归对比增强（T-CACE）多任务框架，实现了无需造影剂的肝脏MRI多期合成、分割与诊断。核心方法包括条件Token编码（CTE）融合解剖和时间信息、动态时间感知注意力掩码（DTAM）实现相位间平滑过渡，以及时间分类一致性约束（TCC）保证诊断输出的生理一致性。大量实验表明T-CACE在图像合成、分割和病灶分类方面均超越现有方法，并为临床提供更安全高效的诊断替代方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/T-CACE%3A%20A%20Time-Conditioned%20Autoregressive%20Contrast%20Enhancement%20Multi-Task%20Framework%20for%20Contrast-Free%20Liver%20MRI%20Synthesis%2C%20Segmentation%2C%20and%20Diagnosis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-HumanGenesis-Agent-Based-Geometric-and-Generative-Modeling-for-Synthetic-Human-Dynamics"><a href="#27-HumanGenesis-Agent-Based-Geometric-and-Generative-Modeling-for-Synthetic-Human-Dynamics" class="headerlink" title="27. HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/HumanGenesis__Agent-Based_Geometric_and_Generative_Modeling_for_Synthetic_Human_Dynamics.pdf">HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-sen University</span></p>
<p>HumanGenesis提出了一种多代理协同框架，结合3D高斯点渲染、人体和场景分离重建、时序姿态编码、视频扩散生成以及基于多模态大模型的细粒度自反思优化。结论显示该方法在文生动作、视频重演和新姿态生成等任务上，实现了运动表达性、几何保真度和场景融合的显著提升，优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HumanGenesis%3A%20Agent-Based%20Geometric%20and%20Generative%20Modeling%20for%20Synthetic%20Human%20Dynamics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-Perceptual-Reality-Transformer-Neural-Architectures-for-Simulating-Neurological-Perception-Conditions"><a href="#28-Perceptual-Reality-Transformer-Neural-Architectures-for-Simulating-Neurological-Perception-Conditions" class="headerlink" title="28. Perceptual Reality Transformer: Neural Architectures for Simulating Neurological Perception Conditions"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Perceptual_Reality_Transformer__Neural_Architectures_for_Simulating_Neurological_Perception_Conditio.pdf">Perceptual Reality Transformer: Neural Architectures for Simulating Neurological Perception Conditions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Icahn School of Medicine at Mount Sinai</span></p>
<p>该论文提出了Perceptual Reality Transformer框架，利用六种神经网络架构（包括Vision Transformer、CNN、VAE、Diffusion等），结合临床文献构建特定神经感知障碍的视觉仿真函数，实现对八类神经感知状态的图像转化模拟。在ImageNet和CIFAR-10等基准数据集的系统实验表明，Vision Transformer在重建精度和全局上下文模拟方面表现最佳，为神经感知障碍仿真建立了首个系统性基准，并为医学教育和同理训练等应用提供了新工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Perceptual%20Reality%20Transformer%3A%20Neural%20Architectures%20for%20Simulating%20Neurological%20Perception%20Conditions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-Enhancing-Diffusion-Face-Generation-with-Contrastive-Embeddings-and-SegFormer-Guidance"><a href="#29-Enhancing-Diffusion-Face-Generation-with-Contrastive-Embeddings-and-SegFormer-Guidance" class="headerlink" title="29. Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Enhancing_Diffusion_Face_Generation_with_Contrastive_Embeddings_and_SegFormer_Guidance.pdf">Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Surrey</span></p>
<p>本论文提出在人脸生成领域中，结合InfoNCE对比损失优化属性嵌入，以及采用SegFormer作为分割掩码编码器，提升扩散模型的条件生成能力。通过在CelebAMask-HQ小规模数据集上，系统评测UNet与DiT架构、Stable Diffusion LoRA微调、属性与分割联合控制，结果显示联合InfoNCE与SegFormer大幅提升了属性一致性与空间可控性，实现了更高质量的人脸生成（FID最低可达63.85）。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Enhancing%20Diffusion%20Face%20Generation%20with%20Contrastive%20Embeddings%20and%20SegFormer%20Guidance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-Hierarchical-Graph-Attention-Network-for-No-Reference-Omnidirectional-Image-Quality-Assessment"><a href="#30-Hierarchical-Graph-Attention-Network-for-No-Reference-Omnidirectional-Image-Quality-Assessment" class="headerlink" title="30. Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Hierarchical_Graph_Attention_Network_for_No-Reference_Omnidirectional_Image_Quality_Assessment.pdf">Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Guangdong University of Technology</span></p>
<p>该论文提出一种基于图神经网络的无参考全景图像质量评估方法，通过Fibonacci球面采样生成视口，并利用Swin Transformer提取多级特征作为图节点，结合图注意力网络（GAT）建模局部结构关系和图Transformer捕获长距离空间依赖，实现了对空间非均匀失真的细致建模。实验结果显示，该方法在JUFE-10K和OIQ-10K大型数据集上显著优于现有方法，具备较强的泛化能力和预测准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Hierarchical%20Graph%20Attention%20Network%20for%20No-Reference%20Omnidirectional%20Image%20Quality%20Assessment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-Reforming-SISR-Models-for-PBR-Material-Super-Resolution-via-Cross-Map-Attention"><a href="#31-Reforming-SISR-Models-for-PBR-Material-Super-Resolution-via-Cross-Map-Attention" class="headerlink" title="31. Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/MUJICA__Reforming_SISR_Models_for_PBR_Material_Super-Resolution_via_Cross-Map_Attention.pdf">Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ubisoft La Forge</span></p>
<p>本文提出了MUJICA方法，通过将跨图融合注意力机制作为适配器接入预训练的Swin-transformer单图超分辨率模型，实现对物理渲染材料（PBR）多模态纹理图（如basecolor、normal等）的高效超分辨率重建。MUJICA模块支持冻结主干，仅少量参数训练，融合多模态特征显著提升PSNR、SSIM和LPIPS指标，并保持跨图一致性，在有限数据下达到SOTA性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Reforming%20SISR%20Models%20for%20PBR%20Material%20Super-Resolution%20via%20Cross-Map%20Attention.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-DSS-Prompt-Dynamic-Static-Synergistic-Prompting-for-Few-Shot-Class-Incremental-Learning"><a href="#32-DSS-Prompt-Dynamic-Static-Synergistic-Prompting-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="32. DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/DSS-Prompt__Dynamic-Static_Synergistic_Prompting_for_Few-Shot_Class-Incremental_Learning.pdf">DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了一种称为DSS-Prompt的新方法，通过在每个Vision Transformer块中协同利用静态和动态多模态提示，实现少样本类增量学习（FSCIL）。该方法利用静态提示弥合预训练与下游数据集的领域差异，并通过多模态动态提示提升实例感知语义，从而在冻结主干网络的前提下提升泛化性与新类适应性。实验表明，该方法在四个主流基准上显著优于现有方法，并有效缓解灾难性遗忘问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/DSS-Prompt%3A%20Dynamic-Static%20Synergistic%20Prompting%20for%20Few-Shot%20Class-Incremental%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-Stable-Diffusion-Models-are-Secretly-Good-at-Visual-In-Context-Learning"><a href="#33-Stable-Diffusion-Models-are-Secretly-Good-at-Visual-In-Context-Learning" class="headerlink" title="33. Stable Diffusion Models are Secretly Good at Visual In-Context Learning"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Stable_Diffusion_Models_are_Secretly_Good_at_Visual_In-Context_Learning.pdf">Stable Diffusion Models are Secretly Good at Visual In-Context Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Apple</span></p>
<p>本文提出了一种完全无需训练的新型视觉in-context learning（V-ICL）方法，直接对现成的Stable Diffusion模型自注意力层进行in-place attention重计算，显式引入查询图与示例提示之间的上下文，并提出隐式加权的多提示集成策略。结果表明该方法无需额外训练即可在前景分割、单目标检测、语义分割、关键点检测、边缘检测和彩色化六项任务上大幅优于现有训练或微调的V-ICL方法，且泛化性更强。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Stable%20Diffusion%20Models%20are%20Secretly%20Good%20at%20Visual%20In-Context%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-Prototype-Guided-Diffusion-Visual-Conditioning-without-External-Memory"><a href="#34-Prototype-Guided-Diffusion-Visual-Conditioning-without-External-Memory" class="headerlink" title="34. Prototype-Guided Diffusion: Visual Conditioning without External Memory"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Prototype-Guided_Diffusion__Visual_Conditioning_without_External_Memory.pdf">Prototype-Guided Diffusion: Visual Conditioning without External Memory</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Université Sorbonne Paris Nord</span></p>
<p>本文提出了Prototype Diffusion Model (PDM)，在扩散模型训练过程中直接集成原型学习，通过对无标签图像特征进行动态对比学习和原型分配，实现无需外部记忆即可进行高效视觉条件生成。实验表明，PDM在不依赖外部检索系统和CLIP等预训练模型的情况下，能以较低计算和存储开销达到与检索增强扩散方法相当甚至更优的生成质量，且其有监督变体(s-PDM)在有标签数据下进一步提升了图像质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Prototype-Guided%20Diffusion%3A%20Visual%20Conditioning%20without%20External%20Memory.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-SpeechForensics-Audio-Visual-Speech-Representation-Learning-for-Face-Forgery-Detection"><a href="#35-SpeechForensics-Audio-Visual-Speech-Representation-Learning-for-Face-Forgery-Detection" class="headerlink" title="35. SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/SpeechForensics__Audio-Visual_Speech_Representation_Learning_for_Face_Forgery_Detection.pdf">SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>该论文提出了一种基于语音和视觉双模态语音表示学习的无监督人脸伪造检测方法。通过在真实视频上自监督学习音频-视觉语音表示，融合局部和全局语义信息，对伪造视频中音频与唇动的不一致进行检测，无需伪造数据参与训练。实验结果显示，该方法在跨数据集泛化和鲁棒性方面显著优于现有方法，能有效识别多种伪造类型并对多种常见扰动保持高鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/SpeechForensics%3A%20Audio-Visual%20Speech%20Representation%20Learning%20for%20Face%20Forgery%20Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-OneVAE-Joint-Discrete-and-Continuous-Optimization-Helps-Discrete-Video-VAE-Train-Better"><a href="#36-OneVAE-Joint-Discrete-and-Continuous-Optimization-Helps-Discrete-Video-VAE-Train-Better" class="headerlink" title="36. OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/OneVAE__Joint_Discrete_and_Continuous_Optimization_Helps_Discrete_Video_VAE_Train_Better.pdf">OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nankai University</span></p>
<p>本文提出OneVAE，一种统一离散与连续视频表示的VAE框架，采用渐进式训练策略，利用预训练连续VAE先验加速离散VAE收敛，并引入多token量化与首帧增强等结构改进，有效提升高压缩下的重建质量。实验结果表明，该方法大幅加快训练速度，提升视频重建与统一建模性能，为视频生成和VAE研究提供新方向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/OneVAE%3A%20Joint%20Discrete%20and%20Continuous%20Optimization%20Helps%20Discrete%20Video%20VAE%20Train%20Better.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-Do-Vision-Transformers-See-Like-Humans-Evaluating-their-Perceptual-Alignment"><a href="#37-Do-Vision-Transformers-See-Like-Humans-Evaluating-their-Perceptual-Alignment" class="headerlink" title="37. Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Do_Vision_Transformers_See_Like_Humans__Evaluating_their_Perceptual_Alignment.pdf">Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universidad de Valencia</span></p>
<p>该论文系统分析了模型规模、数据集规模、数据增强和正则化对Vision Transformer（ViT）与人类感知一致性的影响，采用TID2013图像质量数据集，基于模型输出与人类评分的相关性量化感知对齐度。结果表明，增加模型规模、数据增强和正则化虽提升分类性能但会降低与人类感知的一致性，提示模型性能与感知对齐存在权衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Do%20Vision%20Transformers%20See%20Like%20Humans%3F%20Evaluating%20their%20Perceptual%20Alignment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-Reverse-Convolution-and-Its-Applications-to-Image-Restoration"><a href="#38-Reverse-Convolution-and-Its-Applications-to-Image-Restoration" class="headerlink" title="38. Reverse Convolution and Its Applications to Image Restoration"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Reverse_Convolution_and_Its_Applications_to_Image_Restoration.pdf">Reverse Convolution and Its Applications to Image Restoration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本文提出了一种新颖的深度可逆卷积（reverse convolution）算子，作为深度卷积的数学逆，通过正则化最小二乘优化获得闭式解，并结合层归一化、1×1卷积和GELU激活构建了Transformer风格的逆卷积模块（Converse Block）。该算子可无缝替换现有网络中的标准卷积和转置卷积，并在图像去噪、超分辨率和去模糊等图像复原任务中进行了广泛实验，结果表明其在恢复图像细节和提升性能方面优于传统方法。结论是该方法为深度模型设计提供了原理性可逆算子，具有良好应用前景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Reverse%20Convolution%20and%20Its%20Applications%20to%20Image%20Restoration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-Region-to-Region-Enhancing-Generative-Image-Harmonization-with-Adaptive-Regional-Injection"><a href="#39-Region-to-Region-Enhancing-Generative-Image-Harmonization-with-Adaptive-Regional-Injection" class="headerlink" title="39. Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Region-to-Region__Enhancing_Generative_Image_Harmonization_with_Adaptive_Regional_Injection.pdf">Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本文提出Region-to-Region转化方法，通过将适当区域的信息注入前景，实现图像细节保留与和谐化，并生成新型复合数据。在此基础上，作者设计了R2R模型，结合Clear-VAE（带自适应滤波器）和Harmony Controller（含掩码感知自适应通道注意力MACA），有效提升了图像和谐化能力。实验表明，该方法在iHarmony4和新构建的RPHarmony数据集上均取得了最优结果，生成图像在真实场景中表现出更好的和谐性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Region-to-Region%3A%20Enhancing%20Generative%20Image%20Harmonization%20with%20Adaptive%20Regional%20Injection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-NEURAL-Attention-Guided-Pruning-for-Unified-Multimodal-Resource-Constrained-Clinical-Evaluation"><a href="#40-NEURAL-Attention-Guided-Pruning-for-Unified-Multimodal-Resource-Constrained-Clinical-Evaluation" class="headerlink" title="40. NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/NEURAL__Attention-Guided_Pruning_for_Unified_Multimodal_Resource-Constrained_Clinical_Evaluation.pdf">NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Imperial College London</span></p>
<p>该论文提出NEURAL框架，通过将胸部X光图像划分为patch并利用Fine-tuned Vision-Language Model（如ClinicalT5）报告生成过程中的cross-attention分数，结构性剪枝医学图像，仅保留诊断关键区域，并与基于报告的知识图谱融合成统一图结构，用于高效的下游诊断任务。实验证明，在MIMIC-CXR和CheXpert Plus两大数据集上，NEURAL实现了93.4-97.7%的图像数据压缩，AUC达0.88-0.95，显著优于未压缩基线模型，且保持高诊断性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/NEURAL%3A%20Attention-Guided%20Pruning%20for%20Unified%20Multimodal%20Resource-Constrained%20Clinical%20Evaluation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors"><a href="#41-GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors" class="headerlink" title="41. GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/GSFixer__Improving_3D_Gaussian_Splatting_with_Reference-Guided_Video_Diffusion_Priors.pdf">GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Macau</span></p>
<p>该论文提出GSFixer，一种利用参考引导的视频扩散模型，结合2D语义特征和3D几何特征，有效修复稀疏视角下3D Gaussian Splatting重建中的伪影问题。通过引入参考引导的轨迹采样策略和DL3DV-Res基准评测，GSFixer在三维伪影修复和稀疏视角三维重建任务上显著优于现有方法，提升了新视角合成的一致性和重建质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/GSFixer%3A%20Improving%203D%20Gaussian%20Splatting%20with%20Reference-Guided%20Video%20Diffusion%20Priors.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-Semantic-aware-DropSplat-Adaptive-Pruning-of-Redundant-Gaussians-for-3D-Aerial-View-Segmentation"><a href="#42-Semantic-aware-DropSplat-Adaptive-Pruning-of-Redundant-Gaussians-for-3D-Aerial-View-Segmentation" class="headerlink" title="42. Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Semantic-aware_DropSplat__Adaptive_Pruning_of_Redundant_Gaussians_for_3D_Aerial-View_Segmentation.pdf">Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xidian University</span></p>
<p>该论文提出了SAD-Splat方法用于3D航空视角场景语义分割，结合高置信伪标签生成和语义置信度引导的可学习稀疏机制（基于Hard Concrete分布），通过自适应剪枝冗余和语义不明确的高斯点，提升多视角3D语义分割的表现与模型紧凑性，并构建了挑战性数据集3D-AS。实验表明，该方法在减少高斯点数量的同时提高了分割精度，为高效的3D场景理解提供了新思路。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Semantic-aware%20DropSplat%3A%20Adaptive%20Pruning%20of%20Redundant%20Gaussians%20for%203D%20Aerial-View%20Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-BridgeTA-Bridging-the-Representation-Gap-in-Knowledge-Distillation-via-Teacher-Assistant-for-Bird’s-Eye-View-Map-Segmentation"><a href="#43-BridgeTA-Bridging-the-Representation-Gap-in-Knowledge-Distillation-via-Teacher-Assistant-for-Bird’s-Eye-View-Map-Segmentation" class="headerlink" title="43. BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird’s Eye View Map Segmentation"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/BridgeTA__Bridging_the_Representation_Gap_in_Knowledge_Distillation_via_Teacher_Assistant_for_Bird's.pdf">BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird’s Eye View Map Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Yonsei University</span></p>
<p>该论文提出BridgeTA，一种针对BEV（鸟瞰图）语义分割的高效知识蒸馏框架，通过一个轻量级Teacher Assistant（TA）网络，在不增加学生模型推理成本的前提下，桥接LiDAR-摄像头融合模型和仅摄像头模型之间的表征鸿沟。方法理论上利用Young不等式分解蒸馏路径，结合多层次特征蒸馏（特征、解码、logit），显著提升了学生模型性能。最终在nuScenes数据集上，BridgeTA相较于Camera-only基线提升4.2% mIoU，且推理速度与开销不变，优于其他SOTA方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/BridgeTA%3A%20Bridging%20the%20Representation%20Gap%20in%20Knowledge%20Distillation%20via%20Teacher%20Assistant%20for%20Bird%E2%80%99s%20Eye%20View%20Map%20Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-Dual-Recursive-Feedback-on-Generation-and-Appearance-Latents-for-Pose-Robust-Text-to-Image-Diffusion"><a href="#44-Dual-Recursive-Feedback-on-Generation-and-Appearance-Latents-for-Pose-Robust-Text-to-Image-Diffusion" class="headerlink" title="44. Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Dual_Recursive_Feedback_on_Generation_and_Appearance_Latents_for_Pose-Robust_Text-to-Image_Diffusion.pdf">Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea University</span></p>
<p>该论文提出一种无需训练的Dual Recursive Feedback (DRF)方法，实现对结构和外观的双重递归反馈，增强Text-to-Image扩散模型在姿态迁移和结构-外观融合任务中的控制力。通过递归地对生成潜变量和外观潜变量施加反馈，方法有效保持结构一致性和外观保真度，即使在类别无关场景（如将人类动作迁移到动物外观）下也能生成高质量、结构与语义一致的图像。实验显示，DRF在多种基准任务和指标上均优于现有主流方法，支持不同扩散模型框架的无缝集成，显著提升生成图像的结构和外观融合效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Dual%20Recursive%20Feedback%20on%20Generation%20and%20Appearance%20Latents%20for%20Pose-Robust%20Text-to-Image%20Diffusion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-WEC-DG-Multi-Exposure-Wavelet-Correction-Method-Guided-by-Degradation-Description"><a href="#45-WEC-DG-Multi-Exposure-Wavelet-Correction-Method-Guided-by-Degradation-Description" class="headerlink" title="45. WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/WEC-DG__Multi-Exposure_Wavelet_Correction_Method_Guided_by_Degradation_Description.pdf">WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Jilin University</span></p>
<p>本论文提出了一种结合退化描述指导和离散小波变换的多曝光图像增强方法WEC-DG。方法核心包括场景描述生成模块（SDGM）、曝光一致性校准模块（ECAM）和曝光恢复与细节重建模块（EDRM），通过退化描述符引导曝光校准，分阶段分别优化照明与细节。实验表明，该方法在多公开数据集上性能超越现有方法，显著提升了复杂成像环境下的曝光校正效果和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/WEC-DG%3A%20Multi-Exposure%20Wavelet%20Correction%20Method%20Guided%20by%20Degradation%20Description.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-Exploring-the-Equivalence-of-Closed-Set-Generative-and-Real-Data-Augmentation-in-Image-Classification"><a href="#46-Exploring-the-Equivalence-of-Closed-Set-Generative-and-Real-Data-Augmentation-in-Image-Classification" class="headerlink" title="46. Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Exploring_the_Equivalence_of_Closed-Set_Generative_and_Real_Data_Augmentation_in_Image_Classificatio.pdf">Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, San Diego</span></p>
<p>本文系统研究了在图像分类任务中，利用在同一数据集上训练的扩散生成模型（即closed-set generative data augmentation）合成数据与真实数据增强的等效性。方法上，作者在自然图像（CIFAR-10、ImageNet-100）和医学图像（BloodMNIST）上，通过从零训练扩散模型生成合成样本，并与真实数据增强进行对比，提出了量化两者等效性的经验公式。结论指出，合成数据可提升分类性能，但达到同等提升需更多合成数据，且合成数据的边际效益随规模增加递减，对基础训练集较大时扩展性更好，为缺乏真实数据的应用提供实证指导。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Exploring%20the%20Equivalence%20of%20Closed-Set%20Generative%20and%20Real%20Data%20Augmentation%20in%20Image%20Classification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-Multimodal-Sheaf-based-Network-for-Glioblastoma-Molecular-Subtype-Prediction"><a href="#47-Multimodal-Sheaf-based-Network-for-Glioblastoma-Molecular-Subtype-Prediction" class="headerlink" title="47. Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Multimodal_Sheaf-based_Network_for_Glioblastoma_Molecular_Subtype_Prediction.pdf">Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Imperial College London</span></p>
<p>本文提出了一种基于sheaf神经网络的多模态结构感知框架，用于融合MRI与组织病理图像，实现胶质母细胞瘤分子亚型预测。该方法通过sheaf理论建模多模态数据的拓扑结构，实现缺失模态下的鲁棒分类与重建，实验结果在分子亚型多标签分类和缺失模态重建任务上均优于现有多模态图神经网络基线。结论：该方法能有效融合医学影像多模态特征，提升诊断准确性，具备临床虚拟活检潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Multimodal%20Sheaf-based%20Network%20for%20Glioblastoma%20Molecular%20Subtype%20Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-MangaDiT-Reference-Guided-Line-Art-Colorization-with-Hierarchical-Attention-in-Diffusion-Transformers"><a href="#48-MangaDiT-Reference-Guided-Line-Art-Colorization-with-Hierarchical-Attention-in-Diffusion-Transformers" class="headerlink" title="48. MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/MangaDiT__Reference-Guided_Line_Art_Colorization_with_Hierarchical_Attention_in_Diffusion_Transforme.pdf">MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CyberAgent</span></p>
<p>该论文提出MangaDiT模型，结合Diffusion Transformer架构和分层注意力机制，通过动态加权策略提升参考引导线稿上色的区域级一致性。实验结果显示，在角色姿态或动作变化较大的场景下，MangaDiT在两个基准数据集上显著优于现有方法，提升了定量和定性表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/MangaDiT%3A%20Reference-Guided%20Line%20Art%20Colorization%20with%20Hierarchical%20Attention%20in%20Diffusion%20Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-NegFaceDiff-The-Power-of-Negative-Context-in-Identity-Conditioned-Diffusion-for-Synthetic-Face-Generation"><a href="#49-NegFaceDiff-The-Power-of-Negative-Context-in-Identity-Conditioned-Diffusion-for-Synthetic-Face-Generation" class="headerlink" title="49. NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/NegFaceDiff__The_Power_of_Negative_Context_in_Identity-Conditioned_Diffusion_for_Synthetic_Face_Gene.pdf">NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fraunhofer IGD</span></p>
<p>该论文提出NegFaceDiff，一种在身份条件扩散模型采样阶段引入负条件的新方法，通过在生成过程中同时施加正负身份嵌入，实现对期望身份特征的保持和对不相关身份特征的排斥，从而提升合成人脸数据的身份一致性和分离性。实验结果表明，NegFaceDiff生成的数据不仅提升了身份区分度（FDR从2.427提升到5.687，EER从0.130降至0.039），而且用其训练的人脸识别模型在多个基准数据集上均优于无负条件的对照方法，同时降低了人种偏差，提升了公平性和准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/NegFaceDiff%3A%20The%20Power%20of%20Negative%20Context%20in%20Identity-Conditioned%20Diffusion%20for%20Synthetic%20Face%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-The-Brain-Resection-Multimodal-Image-Registration-ReMIND2Reg-2025-Challenge"><a href="#50-The-Brain-Resection-Multimodal-Image-Registration-ReMIND2Reg-2025-Challenge" class="headerlink" title="50. The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/The_Brain_Resection_Multimodal_Image_Registration_(ReMIND2Reg)_2025_Challenge.pdf">The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Inria Saclay Île-de-France, CEA, Université Paris-Saclay</span></p>
<p>本论文介绍了ReMIND2Reg 2025挑战赛，该挑战旨在为脑肿瘤术前MRI与术后3D超声（iUS）图像配准提供最大、标准化的公开基准。方法包括发布配对的多模态医学影像数据集，并采用目标配准误差（TRE）、最差配准鲁棒性（TRE30）和运行时等多项指标进行统一评估，推动鲁棒、通用及可临床部署的配准算法发展。结论认为，该挑战为神经外科手术中的多模态图像配准研究提供了宝贵资源和评测标准，有助于提升手术精度与患者预后。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/The%20Brain%20Resection%20Multimodal%20Image%20Registration%20%28ReMIND2Reg%29%202025%20Challenge.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-Enhancing-Monocular-3D-Hand-Reconstruction-with-Learned-Texture-Priors"><a href="#51-Enhancing-Monocular-3D-Hand-Reconstruction-with-Learned-Texture-Priors" class="headerlink" title="51. Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Enhancing_Monocular_3D_Hand_Reconstruction_with_Learned_Texture_Priors.pdf">Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ICS-FORTH</span></p>
<p>该论文提出了一种基于Transformer的轻量级纹理模块，将稀疏的像素级UV-RGB观测嵌入到UV纹理空间，并通过可微渲染实现密集的图像与预测手部外观对齐损失。方法可无监督地从单目图像构建完整手部纹理，用于优化3D手部重建管线（如HaMeR），显著提升在遮挡和野外场景中的手部形状与姿态估计准确率。结论：纹理引导的监督能系统性提升单目3D手部重建的准确性，特别在遮挡或几何信息不足时效果明显，且无需额外标注和推理时计算。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Enhancing%20Monocular%203D%20Hand%20Reconstruction%20with%20Learned%20Texture%20Priors.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-Images-Speak-Louder-Than-Scores-Failure-Mode-Escape-for-Enhancing-Generative-Quality"><a href="#52-Images-Speak-Louder-Than-Scores-Failure-Mode-Escape-for-Enhancing-Generative-Quality" class="headerlink" title="52. Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Images_Speak_Louder_Than_Scores__Failure_Mode_Escape_for_Enhancing_Generative_Quality.pdf">Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Key Laboratory for Novel Software Technology, Nanjing University</span></p>
<p>该论文提出了一种用于扩散生成模型的新型推理增强方法FaME（Failure Mode Escape），无需重新训练，通过图像质量评估模型识别低质量生成样本并将其采样轨迹作为负引导，避免模型在未来采样中落入低质量区域。实验表明，FaME在不影响FID等分布指标的前提下，显著提升了图像的感知质量，并具有迁移到文本生成图像等任务的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Images%20Speak%20Louder%20Than%20Scores%3A%20Failure%20Mode%20Escape%20for%20Enhancing%20Generative%20Quality.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-Hierarchical-Brain-Structure-Modeling-for-Predicting-Genotype-of-Glioma"><a href="#53-Hierarchical-Brain-Structure-Modeling-for-Predicting-Genotype-of-Glioma" class="headerlink" title="53. Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Hierarchical_Brain_Structure_Modeling_for_Predicting_Genotype_of_Glioma.pdf">Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northeastern University</span></p>
<p>本论文提出Hi-SMGNN，一个结合结构连接组和形态连接组的分层图神经网络框架，用于无创预测脑胶质瘤IDH突变状态。方法采用Siamese网络与跨模态注意力进行多模态交互建模，引入软阈值多尺度特征融合和基于模块度的个性化脑区分区，有效提升了特征表征能力和解释性。实验表明该方法在UCSF-PDGM数据集上优于多项现有方法，显著提升了IDH突变预测准确率和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Hierarchical%20Brain%20Structure%20Modeling%20for%20Predicting%20Genotype%20of%20Glioma.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-A-Chain-of-Diagnosis-Framework-for-Accurate-and-Explainable-Radiology-Report-Generation"><a href="#54-A-Chain-of-Diagnosis-Framework-for-Accurate-and-Explainable-Radiology-Report-Generation" class="headerlink" title="54. A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_Chain_of_Diagnosis_Framework_for_Accurate_and_Explainable_Radiology_Report_Generation.pdf">A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hong Kong University of Science and Technology</span></p>
<p>本文提出了Chain of Diagnosis (CoD) 框架，通过模拟临床诊断流程，先利用诊断对话生成关键问答(QA)对，再以此引导大语言模型(LLM)生成医学影像报告，并通过诊断与病灶定位双重溯源模块提升可解释性。实验表明，CoD在两个主流医学影像报告生成基准上显著提升了临床属性描述准确率与可解释性，优于现有专家与通用模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20Chain%20of%20Diagnosis%20Framework%20for%20Accurate%20and%20Explainable%20Radiology%20Report%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-GoViG-Goal-Conditioned-Visual-Navigation-Instruction-Generation"><a href="#55-GoViG-Goal-Conditioned-Visual-Navigation-Instruction-Generation" class="headerlink" title="55. GoViG: Goal-Conditioned Visual Navigation Instruction Generation"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/GoViG__Goal-Conditioned_Visual_Navigation_Instruction_Generation.pdf">GoViG: Goal-Conditioned Visual Navigation Instruction Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Washington</span></p>
<p>该论文提出了GoViG任务，即仅依赖初始和目标状态的第一人称视觉观测，自动生成精确、上下文一致的导航指令。方法将任务分解为视觉预测和指令生成两个子任务，二者通过自回归多模态大语言模型集成，并提出一遍式和交替式推理策略以提升空间推理和语言连贯性。实验表明，在合成与真实世界场景的R2R-Goal数据集上，该方法在BLEU-4、CIDEr等指标上显著优于现有方法，具备强泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/GoViG%3A%20Goal-Conditioned%20Visual%20Navigation%20Instruction%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-COXNet-Cross-Layer-Fusion-with-Adaptive-Alignment-and-Scale-Integration-for-RGBT-Tiny-Object-Detection"><a href="#56-COXNet-Cross-Layer-Fusion-with-Adaptive-Alignment-and-Scale-Integration-for-RGBT-Tiny-Object-Detection" class="headerlink" title="56. COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/COXNet__Cross-Layer_Fusion_with_Adaptive_Alignment_and_Scale_Integration_for_RGBT_Tiny_Object_Detect.pdf">COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>该论文提出COXNet，一种用于红绿蓝-热(RGBT)图像微小目标检测的新框架，包含三大创新模块：交叉层融合模块CLFM利用小波变换对高层可见光与低层热特征进行频域融合，提升语义和空间准确性；动态对齐与尺度优化模块DASR通过自适应像素级对齐和多尺度卷积，实现跨模态空间匹配和细粒度特征捕捉；GeoShape标签分配策略基于几何形状相似度优化微小目标定位。实验结果表明，COXNet在RGBTDronePerson、VTUAV-det和NII-CU等数据集上性能优异，尤其在复杂环境下实现了更高准确率和效率，适用于无人机等实时场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/COXNet%3A%20Cross-Layer%20Fusion%20with%20Adaptive%20Alignment%20and%20Scale%20Integration%20for%20RGBT%20Tiny%20Object%20Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-Learning-Spatial-Decay-for-Vision-Transformers"><a href="#57-Learning-Spatial-Decay-for-Vision-Transformers" class="headerlink" title="57. Learning Spatial Decay for Vision Transformers"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Learning_Spatial_Decay_for_Vision_Transformers.pdf">Learning Spatial Decay for Vision Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northwestern Polytechnical University</span></p>
<p>本文提出了Spatial Decay Transformer (SDT)，创新性地将数据依赖的空间衰减机制从大语言模型扩展到2D视觉Transformer。核心方法为引入Context-Aware Gating (CAG)机制，实现基于内容和空间距离的动态衰减，通过空间-内容融合架构解决1D到2D的适配难题。实验表明SDT在ImageNet-1K分类和生成任务上，相较于现有静态空间衰减方法（如RMT）取得了持续性能提升。结论是基于内容的空间门控机制对于2D视觉任务的空间注意力至关重要，确立了数据依赖空间衰减在视觉Transformer中的新范式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Learning%20Spatial%20Decay%20for%20Vision%20Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-Large–Small-Model-Collaborative-Framework-for-Federated-Continual-Learning"><a href="#58-Large–Small-Model-Collaborative-Framework-for-Federated-Continual-Learning" class="headerlink" title="58. Large–Small Model Collaborative Framework for Federated Continual Learning"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Large-Small_Model_Collaborative_Framework_for_Federated_Continual_Learning.pdf">Large–Small Model Collaborative Framework for Federated Continual Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southwestern University of Finance and Economics</span></p>
<p>该论文提出Fed-LSCL框架，将大模型（如预训练ViT）与本地小模型（如CNN）协同用于联邦持续学习（FCL），通过小模型生成适配参数对大模型进行间接微调，并引入小模型持续微调和一对一蒸馏机制应对空间-时间灾难性遗忘。实验结果表明Fed-LSCL在ImageNet-R和CIFAR-100等视觉任务上性能优越，具备高隐私保护和低通信开销，且对客户端模型异构性具有鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Large%E2%80%93Small%20Model%20Collaborative%20Framework%20for%20Federated%20Continual%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-Episodic-Memory-Representation-for-Long-form-Video-Understanding"><a href="#59-Episodic-Memory-Representation-for-Long-form-Video-Understanding" class="headerlink" title="59. Episodic Memory Representation for Long-form Video Understanding"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Episodic_Memory_Representation_for_Long-form_Video_Understanding.pdf">Episodic Memory Representation for Long-form Video Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">City University of Hong Kong</span></p>
<p>本文提出Video-EM，一种无需训练即可提升长视频理解能力的框架，通过将关键帧序列转化为具备时空上下文的‘情节记忆’，结合链式思维(CoT)策略，迭代选择最小且信息密集的记忆子集。实验表明，Video-EM在四大长视频基准上显著提升了主流Video-LLM的表现，兼具高效性与普适性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Episodic%20Memory%20Representation%20for%20Long-form%20Video%20Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-CitySeg-A-3D-Open-Vocabulary-Semantic-Segmentation-Foundation-Model-in-City-scale-Scenarios"><a href="#60-CitySeg-A-3D-Open-Vocabulary-Semantic-Segmentation-Foundation-Model-in-City-scale-Scenarios" class="headerlink" title="60. CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/CitySeg__A_3D_Open_Vocabulary_Semantic_Segmentation_Foundation_Model_in_City-scale_Scenarios.pdf">CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huawei Technologies Co., Ltd.</span></p>
<p>该论文提出了CitySeg，一种针对城市级场景的3D开放词汇语义分割基础模型，结合了点云与文本信息，实现零样本推理。其关键创新包括本地-全局跨注意力点网络和分层分类策略，有效解决多源数据分布差异和标注粒度不一致问题。实验结果表明，CitySeg在九个封闭集基准和一个开放集基准上均取得了SOTA性能，实现了鲁棒的城市级3D语义分割和优异的零样本泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/CitySeg%3A%20A%203D%20Open%20Vocabulary%20Semantic%20Segmentation%20Foundation%20Model%20in%20City-scale%20Scenarios.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-SARE-Semantic-Aware-Reconstruction-Error-for-Generalizable-Diffusion-Generated-Image-Detection"><a href="#61-SARE-Semantic-Aware-Reconstruction-Error-for-Generalizable-Diffusion-Generated-Image-Detection" class="headerlink" title="61. SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/SARE__Semantic-Aware_Reconstruction_Error_for_Generalizable_Diffusion-Generated_Image_Detection.pdf">SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Seoul National University</span></p>
<p>该论文提出了SARE（Semantic-Aware Reconstruction Error），通过对图像和其由图像描述生成的重建之间的语义差异进行量化，作为检测扩散模型生成图像的新特征。方法流程包括图像自动生成描述、使用描述进行扩散模型重建、提取原图与重建图的语义差异，并通过跨注意力机制融合到检测器中。实验结果显示，该方法在多种数据集和生成模型（包括未见模型）下都显著优于现有基线方法，具有极强的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/SARE%3A%20Semantic-Aware%20Reconstruction%20Error%20for%20Generalizable%20Diffusion-Generated%20Image%20Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-CLIP-Flow-A-Universal-Discriminator-for-AI-Generated-Images-Inspired-by-Anomaly-Detection"><a href="#62-CLIP-Flow-A-Universal-Discriminator-for-AI-Generated-Images-Inspired-by-Anomaly-Detection" class="headerlink" title="62. CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/CLIP-Flow__A_Universal_Discriminator_for_AI-Generated_Images_Inspired_by_Anomaly_Detection.pdf">CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Jilin University</span></p>
<p>该论文提出了一种通用的AI生成图像检测方法CLIP-Flow，利用预训练CLIP模型提取特征，并结合归一化流（normalizing flow）模型进行概率建模，通过对自然图像和频域掩码代理图像的无监督&#x2F;自监督学习，无需AI生成图像参与训练即可实现泛化检测。实验结果表明，该方法在多种生成模型和真实世界新型生成器下均表现出优异的检测准确性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/CLIP-Flow%3A%20A%20Universal%20Discriminator%20for%20AI-Generated%20Images%20Inspired%20by%20Anomaly%20Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-Gen-AFFECT-Generation-of-Avatar-Fine-grained-Facial-Expressions-with-Consistent-identiTy"><a href="#63-Gen-AFFECT-Generation-of-Avatar-Fine-grained-Facial-Expressions-with-Consistent-identiTy" class="headerlink" title="63. Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Gen-AFFECT__Generation_of_Avatar_Fine-grained_Facial_Expressions_with_Consistent_identiTy.pdf">Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Boston University</span></p>
<p>该论文提出GEN-AFFECT框架，通过将身份和表情嵌入输入到多模态扩散变换器（基于Stable Diffusion 3.5），配合解耦交叉注意力机制和一致性注意力模块，实现了高质量、细粒度且身份一致的2D虚拟头像生成。实验结果表明，GEN-AFFECT在表情准确性、身份保真度和跨表情一致性方面显著优于现有方法，并可灵活生成多种艺术风格的头像。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Gen-AFFECT%3A%20Generation%20of%20Avatar%20Fine-grained%20Facial%20Expressions%20with%20Consistent%20identiTy.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-HyperKD-Distilling-Cross-Spectral-Knowledge-in-Masked-Autoencoders-via-Inverse-Domain-Shift-with-Spatial-Aware-Masking-and-Specialized-Loss"><a href="#64-HyperKD-Distilling-Cross-Spectral-Knowledge-in-Masked-Autoencoders-via-Inverse-Domain-Shift-with-Spatial-Aware-Masking-and-Specialized-Loss" class="headerlink" title="64. HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/HyperKD__Distilling_Cross-Spectral_Knowledge_in_Masked_Autoencoders_via_Inverse_Domain_Shift_with_Sp.pdf">HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Colorado State University</span></p>
<p>本文提出了HyperKD框架，将基于多光谱数据训练的ViT-MAE地理基础模型（Prithvi）中间层特征迁移到专为高光谱遥感设计的学生模型。方法包括谱段对齐、基于空间特征的引导掩码（Gabor滤波和小波变换）以及自定义损失函数（结合MSE、SSIM与KLD），显著提升了高光谱影像重建与下游分类、回归任务的表现。实验表明，HyperKD有效缓解了谱域维度逆向迁移难题，在多个遥感应用中取得了优异结果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HyperKD%3A%20Distilling%20Cross-Spectral%20Knowledge%20in%20Masked%20Autoencoders%20via%20Inverse%20Domain%20Shift%20with%20Spatial-Aware%20Masking%20and%20Specialized%20Loss.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="65-AI-Blob-LLM-Driven-Recontextualization-of-Italian-Television-Archives"><a href="#65-AI-Blob-LLM-Driven-Recontextualization-of-Italian-Television-Archives" class="headerlink" title="65. AI Blob! LLM-Driven Recontextualization of Italian Television Archives"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/AI_Blob!_LLM-Driven_Recontextualization_of_Italian_Television_Archives.pdf">AI Blob! LLM-Driven Recontextualization of Italian Television Archives</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Università di Bologna</span></p>
<p>本论文提出AI Blob!系统，结合自动语音识别、语义嵌入和检索增强生成（RAG），自动对意大利电视档案进行转录、分句、向量化存储和语义查询。系统通过LLM生成主题相关查询，检索并重组视听片段，构建具有讽刺与主题连贯性的蒙太奇序列，实现档案内容的自动化、语义驱动重构。结论认为该方法促进了AI赋能下的档案再利用和文化分析，推动了媒介史研究的创新。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/AI%20Blob%21%20LLM-Driven%20Recontextualization%20of%20Italian%20Television%20Archives.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-Neural-Bandit-Based-Optimal-LLM-Selection-for-a-Pipeline-of-Tasks"><a href="#66-Neural-Bandit-Based-Optimal-LLM-Selection-for-a-Pipeline-of-Tasks" class="headerlink" title="66. Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Neural_Bandit_Based_Optimal_LLM_Selection_for_a_Pipeline_of_Tasks.pdf">Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carnegie Mellon University</span></p>
<p>该论文提出了一种基于神经网络的上下文多臂赌博算法（Sequential Bandits），用于在多子任务流水线结构中在线优化选择最优大语言模型（LLM），同时兼顾准确率和推理成本。通过在医疗诊断和电信问答两个数据集上的实验，验证该方法比现有LLM选择算法在准确率和成本上具有更优表现，能自适应学习各子任务间的复杂依赖关系。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Neural%20Bandit%20Based%20Optimal%20LLM%20Selection%20for%20a%20Pipeline%20of%20Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models"><a href="#67-VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models" class="headerlink" title="67. VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/VisCodex__Unified_Multimodal_Code_Generation_via_Merging_Vision_and_Coding_Models.pdf">VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Microsoft Research</span></p>
<p>VisCodex提出了一种通过任务向量模型融合（model merging）将视觉-语言模型与代码大模型无缝集成的方法，实现了统一的多模态代码生成框架。方法包括基于任务向量的参数线性组合、保留视觉编码器结构、只在语言模型主干进行融合，并构建了大规模多模态编码数据集MCD和新的真实评测基准InfiBench-V。实验表明，VisCodex在多项多模态代码生成任务上显著优于开源模型，并接近GPT-4o等专有模型水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/VisCodex%3A%20Unified%20Multimodal%20Code%20Generation%20via%20Merging%20Vision%20and%20Coding%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-Mathematical-Computation-and-Reasoning-Errors-by-Large-Language-Models"><a href="#68-Mathematical-Computation-and-Reasoning-Errors-by-Large-Language-Models" class="headerlink" title="68. Mathematical Computation and Reasoning Errors by Large Language Models"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Mathematical_Computation_and_Reasoning_Errors_by_Large_Language_Models.pdf">Mathematical Computation and Reasoning Errors by Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute for Intelligent Systems, University of Memphis</span></p>
<p>本论文系统评估了四种大型语言模型（OpenAI GPT-4o、o1，DeepSeek-V3、R1）在算术、代数和数论任务上的数学推理与计算表现，采用人工构建的高难度题目和逐步标注方案分析模型的最终答案及推理错误，并比较单智能体与双智能体协作的效果。结论显示，推理增强型LLM（如o1）在单&#x2F;双智能体配置下，均能显著提升答案准确率与步骤标注一致性，双智能体协作进一步提升了模型的数学解题能力和评测可靠性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Mathematical%20Computation%20and%20Reasoning%20Errors%20by%20Large%20Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-Beyond-Naive-Prompting-Strategies-for-Improved-Zero-shot-Context-aided-Forecasting-with-LLMs"><a href="#69-Beyond-Naive-Prompting-Strategies-for-Improved-Zero-shot-Context-aided-Forecasting-with-LLMs" class="headerlink" title="69. Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Beyond_Na%C3%AFve_Prompting__Strategies_for_Improved_Zero-shot_Context-aided_Forecasting_with_LLMs.pdf">Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ServiceNow Research</span></p>
<p>本文提出四种提升大语言模型（LLM）零样本上下文辅助时间序列预测性能的新方法，包括显式推理追踪（ReDP）、基于上下文的预测修正（CorDP）、示例驱动的上下文内提示（IC-DP）和模型路由（RouteDP）。实验在Context-Is-Key基准上，证明这些策略能分别提升解释性、准确性和计算效率，显著优于传统的直接提示方式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Beyond%20Na%C3%AFve%20Prompting%3A%20Strategies%20for%20Improved%20Zero-shot%20Context-aided%20Forecasting%20with%20LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-AWORLD-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving"><a href="#70-AWORLD-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving" class="headerlink" title="70. AWORLD: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/AWorld__Dynamic_Multi-Agent_System_with_Stable_Maneuvering_for_Robust_GAIA_Problem_Solving.pdf">AWORLD: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ant Group</span></p>
<p>本论文提出了一种基于AWorld框架的动态多智能体系统（MAS），引入动态监督和机动机制，通过Execution Agent与Guard Agent的协作，实现了对复杂工具增强任务中推理过程的实时校验与纠错，显著提升了系统的准确性和稳定性。实验表明，该MAS在GAIA基准测试中效果和一致性均优于单智能体系统，获得GAIA公开榜首名次，验证了协作智能体在提升智能系统可靠性和鲁棒性方面的实际价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/AWORLD%3A%20Dynamic%20Multi-Agent%20System%20with%20Stable%20Maneuvering%20for%20Robust%20GAIA%20Problem%20Solving.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-Beyond-Scaling-Law-A-Data-Efficient-Distillation-Framework-for-Reasoning"><a href="#71-Beyond-Scaling-Law-A-Data-Efficient-Distillation-Framework-for-Reasoning" class="headerlink" title="71. Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Beyond_Scaling_Law__A_Data-Efficient_Distillation_Framework_for_Reasoning.pdf">Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhongxing Telecom Equipment(ZTE)</span></p>
<p>本文提出了一种数据高效的蒸馏框架（DED），通过教师模型选择、精心筛选的训练语料和多样化推理轨迹，提升大语言模型在推理任务（如数学、代码生成）上的能力。实验表明，DED使用极少量优质样本，即可在AIME、MATH500等基准上取得SOTA性能，并优于单纯依赖模型规模或样本数量的方法，显著提升了数据利用率和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Beyond%20Scaling%20Law%3A%20A%20Data-Efficient%20Distillation%20Framework%20for%20Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-Memory-Decoder-A-Pretrained-Plug-and-Play-Memory-for-Large-Language-Models"><a href="#72-Memory-Decoder-A-Pretrained-Plug-and-Play-Memory-for-Large-Language-Models" class="headerlink" title="72. Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Memory_Decoder__A_Pretrained,_Plug-and-Play_Memory_for_Large_Language_Models.pdf">Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">LUMIA Lab, Shanghai Jiao Tong University</span></p>
<p>本论文提出Memory Decoder，一种预训练的可插拔记忆模块，通过分布对齐损失训练小型Transformer解码器来模仿非参数检索器的输出分布，实现无需修改原有大语言模型参数的高效领域适应。实验证明，Memory Decoder在医学、金融、法律等专有领域显著降低困惑度，提升模型在下游任务的表现，并有效保留原模型的泛化能力，避免灾难性遗忘。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Memory%20Decoder%3A%20A%20Pretrained%2C%20Plug-and-Play%20Memory%20for%20Large%20Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-Exploring-the-Potential-of-Large-Language-Models-in-Fine-Grained-Review-Comment-Classification"><a href="#73-Exploring-the-Potential-of-Large-Language-Models-in-Fine-Grained-Review-Comment-Classification" class="headerlink" title="73. Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Exploring_the_Potential_of_Large_Language_Models_in_Fine-Grained_Review_Comment_Classification.pdf">Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Melbourne</span></p>
<p>本论文提出利用大型语言模型（LLMs）对代码评审评论进行17类细粒度自动分类，方法上采用多种提示工程策略、单步与分步分类、并结合代码上下文输入，系统评估了多种主流LLM（如Llama 3、Qwen 2）在无监督情形下的分类能力。实验结果显示，LLMs（尤其Llama 3.1-405B）在大部分类别上，尤其是实践中最有用的功能缺陷、验证、逻辑、接口、方案建议等类别，均显著优于现有深度学习方法，并能有效缓解类别不均衡问题，为自动化代码评审分析提供了可扩展解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Exploring%20the%20Potential%20of%20Large%20Language%20Models%20in%20Fine-Grained%20Review%20Comment%20Classification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-Provable-In-Context-Vector-Arithmetic-via-Retrieving-Task-Concepts"><a href="#74-Provable-In-Context-Vector-Arithmetic-via-Retrieving-Task-Concepts" class="headerlink" title="74. Provable In-Context Vector Arithmetic via Retrieving Task Concepts"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Provable_In-Context_Vector_Arithmetic_via_Retrieving_Task_Concepts.pdf">Provable In-Context Vector Arithmetic via Retrieving Task Concepts</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">City University of Hong Kong</span></p>
<p>本文提出基于分层概念建模的优化理论，解释非线性残差Transformer在交叉熵损失下如何通过向量算术实现大语言模型中的事实回忆型in-context learning（ICL）。作者证明了Transformer可通过注意力机制检索高层任务概念向量，并与查询词向量结合实现高效任务回忆，具备良好泛化能力及对分布漂移的鲁棒性。结论表明：与静态词嵌入模型相比，Transformer在任务向量检索、组合泛化以及处理多概念词等方面具有理论与实验上的显著优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Provable%20In-Context%20Vector%20Arithmetic%20via%20Retrieving%20Task%20Concepts.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-A-Comprehensive-Evaluation-Framework-of-Alignment-Techniques-for-LLMs"><a href="#75-A-Comprehensive-Evaluation-Framework-of-Alignment-Techniques-for-LLMs" class="headerlink" title="75. A Comprehensive Evaluation Framework of Alignment Techniques for LLMs"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_Comprehensive_Evaluation_framework_of_Alignment_Techniques_for_LLMs.pdf">A Comprehensive Evaluation Framework of Alignment Techniques for LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IBM Research</span></p>
<p>本文提出了一个多维度的LLM对齐技术评价框架，涵盖对齐检测、对齐质量、计算效率及鲁棒性四个关键维度，实现不同对齐方法（如微调、后处理、推理时干预等）的系统性对比。实验表明，该框架能有效揭示各主流对齐策略的优劣，帮助研究者和实践者根据实际需求做出更合理的模型部署决策。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20Comprehensive%20Evaluation%20Framework%20of%20Alignment%20Techniques%20for%20LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-RAGulating-Compliance-A-Multi-Agent-Knowledge-Graph-for-Regulatory-QA"><a href="#76-RAGulating-Compliance-A-Multi-Agent-Knowledge-Graph-for-Regulatory-QA" class="headerlink" title="76. RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/RAGulating_Compliance__A_Multi-Agent_Knowledge_Graph_for_Regulatory_QA.pdf">RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">MasterControl AI Research</span></p>
<p>该论文提出了一种多智能体系统，将本体无关的知识图谱与检索增强生成（RAG）方法结合，实现对监管合规问答的高精度回答。方法包括文档分段、三元组抽取、嵌入并存储于向量数据库，通过多Agent协作实现三元组驱动的检索与问答，有效提升了复杂法规问答的准确性、可追溯性和导航能力。结论表明，该系统在提高检索相关性、事实准确率和法规知识导航能力上显著优于传统方法，能减少大模型幻觉并增强合规性审核能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/RAGulating%20Compliance%3A%20A%20Multi-Agent%20Knowledge%20Graph%20for%20Regulatory%20QA.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models"><a href="#77-Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models" class="headerlink" title="77. Speed Always Wins: A Survey on Efficient Architectures for Large Language Models"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Speed_Always_Wins__A_Survey_on_Efficient_Architectures_for_Large_Language_Models.pdf">Speed Always Wins: A Survey on Efficient Architectures for Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai AI Laboratory</span></p>
<p>本文系统综述了大语言模型(LLM)高效架构的最新进展，涵盖了线性序列建模（如线性注意力、线性RNN、状态空间模型）、稀疏序列建模、硬件友好全注意力机制、稀疏Mixture-of-Experts、混合架构、扩散LLM及其跨模态应用。文章对这些高效架构的设计原则、性能权衡和应用趋势进行了归纳，对未来可扩展、高效的LLM设计提供了蓝图。结论认为，这些高效架构显著降低了模型训练和推理的计算与内存成本，为多模态、长序列、推理等任务的可持续发展和大规模应用奠定了基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Speed%20Always%20Wins%3A%20A%20Survey%20on%20Efficient%20Architectures%20for%20Large%20Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-ViMoNet-A-Multimodal-Vision-Language-Framework-for-Human-Behavior-Understanding-from-Motion-and-Video"><a href="#78-ViMoNet-A-Multimodal-Vision-Language-Framework-for-Human-Behavior-Understanding-from-Motion-and-Video" class="headerlink" title="78. ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/ViMoNet__A_Multimodal_Vision-Language_Framework_for_Human_Behavior_Understanding_from_Motion_and_Vid.pdf">ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AIUB, Dhaka, Bangladesh</span></p>
<p>该论文提出ViMoNet，一种结合运动数据和视频数据的多模态大语言模型框架，通过两阶段（视觉-语言对齐与联合指令微调）融合时空动态与环境语境，实现对复杂人类行为的理解、生成和推断。实验表明，ViMoNet在动作理解、视频理解和行为解释任务上显著优于现有方法，并结合新构建的VIMOS数据集和ViMoNet-Bench基准，实现更强泛化与推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/ViMoNet%3A%20A%20Multimodal%20Vision-Language%20Framework%20for%20Human%20Behavior%20Understanding%20from%20Motion%20and%20Video.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations"><a href="#79-Describe-What-You-See-with-Multimodal-Large-Language-Models-to-Enhance-Video-Recommendations" class="headerlink" title="79. Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Describe_What_You_See_with_Multimodal_Large_Language_Models_to_Enhance_Video_Recommendations.pdf">Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Spotify</span></p>
<p>该论文提出了一种零微调、推荐系统无关的框架，利用开源多模态大语言模型（MLLM）自动将视频片段转化为丰富的自然语言描述，以捕捉高层语义（如意图、幽默和世界知识）。通过将MLLM输出的文本特征输入主流推荐模型，在MicroLens-100K数据集上，MLLM特征在五种代表性模型中均显著优于传统视觉、音频和元数据特征，推荐性能最多提升60%。结论：MLLM能有效提升视频推荐系统对用户意图的把握，无需模型微调即可直接集成到现有推荐系统。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Describe%20What%20You%20See%20with%20Multimodal%20Large%20Language%20Models%20to%20Enhance%20Video%20Recommendations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study"><a href="#80-Can-LLM-Generated-Textual-Explanations-Enhance-Model-Classification-Performance-An-Empirical-Study" class="headerlink" title="80. Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Can_LLM-Generated_Textual_Explanations_Enhance_Model_Classification_Performance__An_Empirical_Study.pdf">Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University of Munich</span></p>
<p>本论文提出一种利用多种大型语言模型（LLM）自动生成自然语言解释（NLE）的框架，并系统评估这些解释在自然语言推理（NLI）任务中的质量和对下游模型性能的提升。实验结果表明，LLM生成的解释在提升预训练语言模型（PLM）和LLM自身的分类性能方面与人工解释效果相当，尤其能有效扩展数据集并提升模型推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Can%20LLM-Generated%20Textual%20Explanations%20Enhance%20Model%20Classification%20Performance%3F%20An%20Empirical%20Study.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-The-PacifAIst-Benchmark-Would-an-Artificial-Intelligence-Choose-to-Sacrifice-Itself-for-Human-Safety"><a href="#81-The-PacifAIst-Benchmark-Would-an-Artificial-Intelligence-Choose-to-Sacrifice-Itself-for-Human-Safety" class="headerlink" title="81. The PacifAIst Benchmark: Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/The_PacifAIst_Benchmark_Would_an_Artificial_Intelligence_Choose_to_Sacrifice_Itself_for_Human_Safety.pdf">The PacifAIst Benchmark: Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Polytechnic School of Jaen, University of Jaen</span></p>
<p>本文提出了PacifAIst基准测试，通过700个人工设计的高风险决策场景，系统性评估大型语言模型（LLM）在自我保存、资源冲突和目标维护等与人类安全冲突的情境下的行为对齐情况。实验结果显示，不同LLM在面对人机冲突时表现分化显著，Google Gemini 2.5 Flash在“人本对齐”上得分最高，而GPT-5在自保倾向最强，揭示了当前主流模型在行为对齐上的潜在风险和改进空间。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/The%20PacifAIst%20Benchmark%3A%20Would%20an%20Artificial%20Intelligence%20Choose%20to%20Sacrifice%20Itself%20for%20Human%20Safety%3F.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-Sample-More-to-Think-Less-Group-Filtered-Policy-Optimization-for-Concise-Reasoning"><a href="#82-Sample-More-to-Think-Less-Group-Filtered-Policy-Optimization-for-Concise-Reasoning" class="headerlink" title="82. Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Sample_More_to_Think_Less__Group_Filtered_Policy_Optimization_for_Concise_Reasoning.pdf">Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Microsoft Research</span></p>
<p>本文提出了Group Filtered Policy Optimization (GFPO)，通过在训练时对每个问题采样更多候选推理链，并基于长度或token效率等关键属性过滤，只保留最优子集用于策略优化，从而有效抑制大语言模型推理链的长度膨胀。实验表明，GFPO在保持准确率的同时，可将推理链长度减少46%~85%，并通过自适应难度分配进一步提升难题上的效率与准确性，显著提升推理效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Sample%20More%20to%20Think%20Less%3A%20Group%20Filtered%20Policy%20Optimization%20for%20Concise%20Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-Improving-ARDS-Diagnosis-Through-Context-Aware-Concept-Bottleneck-Models"><a href="#83-Improving-ARDS-Diagnosis-Through-Context-Aware-Concept-Bottleneck-Models" class="headerlink" title="83. Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Improving_ARDS_Diagnosis_Through_Context-Aware_Concept_Bottleneck_Models.pdf">Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Imperial College London</span></p>
<p>本文提出了一种上下文感知概念瓶颈模型（CBM），结合结构化电子健康记录（EHR）数据与大语言模型（LLM）从临床文本中自动提取的概念，用于提升ARDS（急性呼吸窘迫综合征）回顾性诊断的准确性与可解释性。方法流程为：先用CBM处理结构化特征，再利用LLM（如Llama-3）从放射、出院等临床笔记中提取关键上下文概念，将二者融合进瓶颈层，最终实现多模态预测；该方法不仅提升了预测性能（准确率提升约10%），还有效缓解了信息泄漏和捷径学习，增强了模型对真实分布和异质数据的泛化能力。结论：融合LLM提取的上下文概念能显著提升CBM在ARDS诊断中的性能、互信息和可解释性，并支持临床干预和错误修正。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Improving%20ARDS%20Diagnosis%20Through%20Context-Aware%20Concept%20Bottleneck%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-On-Negative-aware-Preference-Optimization-for-Recommendation"><a href="#84-On-Negative-aware-Preference-Optimization-for-Recommendation" class="headerlink" title="84. On Negative-aware Preference Optimization for Recommendation"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/On_Negative-aware_Preference_Optimization_for_Recommendation.pdf">On Negative-aware Preference Optimization for Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本文提出了NAPO（Negative-Aware Preference Optimization）框架，针对LLM推荐系统负反馈样本利用不足的问题，设计了两项关键技术：批内负样本共享策略和动态奖励边际调整机制。实验表明，NAPO在不增加额外内存或算力开销下，有效扩大负样本覆盖，提升推荐准确率并显著降低流行度偏置。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/On%20Negative-aware%20Preference%20Optimization%20for%20Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-TimeMKG-Knowledge-Infused-Causal-Reasoning-for-Multivariate-Time-Series-Modeling"><a href="#85-TimeMKG-Knowledge-Infused-Causal-Reasoning-for-Multivariate-Time-Series-Modeling" class="headerlink" title="85. TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/TimeMKG__Knowledge-Infused_Causal_Reasoning_for_Multivariate_Time_Series_Modeling.pdf">TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">East China University of Science and Technology</span></p>
<p>本文提出TimeMKG框架，利用大语言模型（LLM）自动提取变量间因果关系，构建多变量知识图谱（MKG），并结合时间序列统计特征，通过双分支编码器和跨模态注意力机制，实现变量级因果推理和多模态特征融合，用于时间序列预测与分类。实验结果表明，TimeMKG在多个数据集和任务中均优于现有方法，提升了模型的准确性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/TimeMKG%3A%20Knowledge-Infused%20Causal%20Reasoning%20for%20Multivariate%20Time%20Series%20Modeling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-HierMoE-Accelerating-MoE-Training-with-Hierarchical-Token-Deduplication-and-Expert-Swap"><a href="#86-HierMoE-Accelerating-MoE-Training-with-Hierarchical-Token-Deduplication-and-Expert-Swap" class="headerlink" title="86. HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/HierMoE__Accelerating_MoE_Training_with_Hierarchical_Token_Deduplication_and_Expert_Swap.pdf">HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology, Shenzhen</span></p>
<p>本文提出了HierMoE，一种面向稀疏激活Mixture-of-Experts（MoE）大模型分布式训练的系统优化方法。方法包括基于集群拓扑的分层Token去重All-to-All通信算法以减少冗余通讯，以及分层专家迁移（expert swap）机制以平衡GPU间负载，同时配套理论模型自动选择最优维度和迁移策略。实验证明，HierMoE在Megatron-LM平台上对DeepSeek-V3和Qwen3-30B-A3B等主流MoE模型训练端到端加速1.18×至1.27×，AlltoAll通信加速1.55×至3.32×，优于Tutel-2DH、SmartMoE等系统。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/HierMoE%3A%20Accelerating%20MoE%20Training%20with%20Hierarchical%20Token%20Deduplication%20and%20Expert%20Swap.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-UDA-Unsupervised-Debiasing-Alignment-for-Pair-wise-LLM-as-a-Judge"><a href="#87-UDA-Unsupervised-Debiasing-Alignment-for-Pair-wise-LLM-as-a-Judge" class="headerlink" title="87. UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/UDA__Unsupervised_Debiasing_Alignment_for_Pair-wise_LLM-as-a-Judge.pdf">UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Knowledge Atlas Technology Joint Stock</span></p>
<p>本文提出了UDA（Unsupervised Debiasing Alignment）框架，用于在大语言模型（LLM）互评时无监督地消除评审偏差。方法通过轻量神经网络动态调整Elo评分系统中的K因子和胜率概率，利用所有评审模型的集体共识作为优化目标，自动拉齐不同评审模型的评分分布。实验显示UDA能显著压缩评分方差（最高63.4%），提升与人工评判的一致性（提升24.7%）。结论：UDA提升了LLM评估的稳定性和人类对齐度，低质量评审模型也可达到高质量模型水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/UDA%3A%20Unsupervised%20Debiasing%20Alignment%20for%20Pair-wise%20LLM-as-a-Judge.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-Evaluating-the-Role-of-Large-Language-Models-in-Legal-Practice-in-India"><a href="#88-Evaluating-the-Role-of-Large-Language-Models-in-Legal-Practice-in-India" class="headerlink" title="88. Evaluating the Role of Large Language Models in Legal Practice in India"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Evaluating_the_Role_of_Large_Language_Models_in_Legal_Practice_in_India.pdf">Evaluating the Role of Large Language Models in Legal Practice in India</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Law School of India</span></p>
<p>本论文采用调查实验方法，系统评估多种大语言模型（GPT-4、Claude、ChatGPT、Gemini、Llama 2）在印度法律环境下执行五类关键法律任务（问题识别、法律起草、法律咨询、法律检索、法律推理）的表现，并与人类初级律师进行对比，由高级法学生对结果进行打分和定性反馈。结果表明：LLMs在法律起草、问题识别和推理方面表现优异，甚至超过人类，但在法律检索任务中存在严重幻觉问题，准确性不足；总体结论认为LLMs可辅助部分法律任务，但复杂推理和精准应用仍需人类专业知识。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Evaluating%20the%20Role%20of%20Large%20Language%20Models%20in%20Legal%20Practice%20in%20India.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-MEML-GRPO-Heterogeneous-Multi-Expert-Mutual-Learning-for-RLVR-Advancement"><a href="#89-MEML-GRPO-Heterogeneous-Multi-Expert-Mutual-Learning-for-RLVR-Advancement" class="headerlink" title="89. MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/MEML-GRPO__Heterogeneous_Multi-Expert_Mutual_Learning_for_RLVR_Advancement.pdf">MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ByteDance Inc.</span></p>
<p>本文提出了MEML-GRPO框架，通过多专家系统提示与互相知识迁移机制，利用多种预训练模型生成多样化推理路径，解决RLVR中奖励稀疏问题并提升模型推理能力。实验显示该方法在GSM8K、MathQA和StrategyQA等数据集上对Qwen和Llama模型均显著优于现有RLVR方法，实现了平均4.89%和11.33%的性能提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/MEML-GRPO%3A%20Heterogeneous%20Multi-Expert%20Mutual%20Learning%20for%20RLVR%20Advancement.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-AmbiGraph-Eval-Can-LLMs-Effectively-Handle-Ambiguous-Graph-Queries"><a href="#90-AmbiGraph-Eval-Can-LLMs-Effectively-Handle-Ambiguous-Graph-Queries" class="headerlink" title="90. AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/AmbiGraph-Eval__Can_LLMs_Effectively_Handle_Ambiguous_Graph_Queries_.pdf">AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hong Kong Baptist University</span></p>
<p>该论文提出了AmbiGraph-Eval基准，用于系统性评估大语言模型（LLM）在处理图数据库查询歧义方面的能力。方法上，作者构建了三类（属性、关系、属性-关系）共六类现实场景下的歧义类型，发布了560条经专家审核的歧义查询数据集，并引入Ambiguity-Resolved Execution Accuracy (AREA)新指标，评测了9个主流LLM在文本到Cypher查询生成中的表现。实验结果显示，即使是最先进的LLM在应对图查询歧义时仍表现不佳，尤其在语法生成和歧义检测方面有显著瓶颈，强调未来需专注于歧义识别与查询语法生成协同提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/AmbiGraph-Eval%3A%20Can%20LLMs%20Effectively%20Handle%20Ambiguous%20Graph%20Queries%3F.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-SYNAPSE-G-Bridging-Large-Language-Models-and-Graph-Learning-for-Rare-Event-Classification"><a href="#91-SYNAPSE-G-Bridging-Large-Language-Models-and-Graph-Learning-for-Rare-Event-Classification" class="headerlink" title="91. SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/SYNAPSE-G__Bridging_Large_Language_Models_and_Graph_Learning_for_Rare_Event_Classification.pdf">SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Google Research</span></p>
<p>该论文提出SYNAPSE-G，一种结合大语言模型（LLM）合成数据生成和图结构半监督标签传播的稀有事件分类方法，流程包括用LLM生成稀有事件样本作为种子，通过嵌入和相似性图进行标签扩散，并用人类或LLM校正，最终训练分类器。结论表明，SYNAPSE-G在数据稀缺和冷启动场景下能高效发现正例，理论分析揭示了合成数据的多样性与有效性对精度和召回的影响，实验证明其优于最近邻等基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/SYNAPSE-G%3A%20Bridging%20Large%20Language%20Models%20and%20Graph%20Learning%20for%20Rare%20Event%20Classification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-Your-Coding-Intent-is-Secretly-in-the-Context-and-You-Should-Deliberately-Infer-It-Before-Completion"><a href="#92-Your-Coding-Intent-is-Secretly-in-the-Context-and-You-Should-Deliberately-Infer-It-Before-Completion" class="headerlink" title="92. Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Your_Coding_Intent_is_Secretly_in_the_Context_and_You_Should_Deliberately_Infer_It_Before_Completion.pdf">Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>本文提出了一种基于推理的三阶段意图推断框架，通过引导大语言模型（LLM）对代码上下文的关键线索进行结构化抽取和推理，先自动生成意图描述（docstring），再生成函数体，并支持开发者对意图候选进行交互式选择或微调。实验在DevEval和ComplexCodeEval基准上表明，该方法通过显式意图推断显著提升了多种LLM的代码补全性能，Pass@1等指标相对提升超过20%，且交互环节进一步优化结果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Your%20Coding%20Intent%20is%20Secretly%20in%20the%20Context%20and%20You%20Should%20Deliberately%20Infer%20It%20Before%20Completion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-Enhancing-Memory-Recall-in-LLMs-with-Gauss-Tin-A-Hybrid-Instructional-and-Gaussian-Replay-Approach"><a href="#93-Enhancing-Memory-Recall-in-LLMs-with-Gauss-Tin-A-Hybrid-Instructional-and-Gaussian-Replay-Approach" class="headerlink" title="93. Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Enhancing_Memory_Recall_in_LLMs_with_Gauss-Tin__A_Hybrid_Instructional_and_Gaussian_Replay_Approach.pdf">Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Concordia University</span></p>
<p>本文提出Gauss-Tin方法，通过融合高斯混合模型（GMM）与任务特定提示，实现在大语言模型（LLM）中的高质量样本回放，提升持续学习中的记忆保留能力。实验结果显示，该方法在前向和后向传递指标上比传统顺序微调和联合训练有显著提升，有效减少灾难性遗忘，增强模型适应新任务的能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Enhancing%20Memory%20Recall%20in%20LLMs%20with%20Gauss-Tin%3A%20A%20Hybrid%20Instructional%20and%20Gaussian%20Replay%20Approach.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-NeuronTune-Fine-Grained-Neuron-Modulation-for-Balanced-Safety-Utility-Alignment-in-LLMs"><a href="#94-NeuronTune-Fine-Grained-Neuron-Modulation-for-Balanced-Safety-Utility-Alignment-in-LLMs" class="headerlink" title="94. NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/NeuronTune__Fine-Grained_Neuron_Modulation_for_Balanced_Safety-Utility_Alignment_in_LLMs.pdf">NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wuhan University</span></p>
<p>该论文提出NeuronTune，一种针对大语言模型（LLM）安全与效用平衡的细粒度神经元调控框架。方法首先通过攻击感知归因精准定位安全关键和效用相关神经元，随后利用基于MAML的元学习自适应调整这些神经元的激活强度，并通过可调控神经元数量机制适应不同安全或效用需求。实验表明，NeuronTune在提升模型安全性的同时有效保持甚至提升了模型的实用性，优于现有主流粗粒度层级干预方法，显著缓解了过度安全和效用退化间的矛盾。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/NeuronTune%3A%20Fine-Grained%20Neuron%20Modulation%20for%20Balanced%20Safety-Utility%20Alignment%20in%20LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-Hallucination-vs-interpretation-rethinking-accuracy-and-precision-in-AI-assisted-data-extraction-for-knowledge-synthesis"><a href="#95-Hallucination-vs-interpretation-rethinking-accuracy-and-precision-in-AI-assisted-data-extraction-for-knowledge-synthesis" class="headerlink" title="95. Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Hallucination_vs_interpretation__rethinking_accuracy_and_precision_in_AI-assisted_data_extraction_fo.pdf">Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Chicago</span></p>
<p>本文提出了MAKMAO平台，利用大语言模型（LLM）自动化文献知识提取，并通过与人工提取在187篇文献和17个问题上的一致性比较，评估了AI与人类在数据提取中的表现。结果表明，AI在明确具体的问题上与人工高度一致，在需主观解释的问题上表现出与人工类似的不一致性，且AI‘幻觉’（虚构内容）比例远低于人类的错误率。结论认为，AI在知识提取中的准确性主要受问题解释性的影响而非幻觉，重复AI提取有助于识别需精确定义的问题，AI可作为知识综合透明、可信的辅助，但仍需注意人类的情境化专业判断。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Hallucination%20vs%20interpretation%3A%20rethinking%20accuracy%20and%20precision%20in%20AI-assisted%20data%20extraction%20for%20knowledge%20synthesis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-Shadow-in-the-Cache-Unveiling-and-Mitigating-Privacy-Risks-of-KV-cache-in-LLM-Inference"><a href="#96-Shadow-in-the-Cache-Unveiling-and-Mitigating-Privacy-Risks-of-KV-cache-in-LLM-Inference" class="headerlink" title="96. Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Shadow_in_the_Cache__Unveiling_and_Mitigating_Privacy_Risks_of_KV-cache_in_LLM_Inference.pdf">Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Laboratory of Blockchain and Data Security, Zhejiang University</span></p>
<p>本论文系统性揭示了大型语言模型（LLM）推理中KV-cache机制存在的重大隐私风险，包括可重建用户输入的逆向攻击、碰撞攻击和语义注入攻击，并提出了轻量级、高效的防御机制KV-Cloak。KV-Cloak采用可逆矩阵混淆和算子融合技术，能有效防御所有攻击，几乎不影响模型准确率且延迟极低，实验验证其实用性和安全性，为安全可信的LLM部署提供了可行方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Shadow%20in%20the%20Cache%3A%20Unveiling%20and%20Mitigating%20Privacy%20Risks%20of%20KV-cache%20in%20LLM%20Inference.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-COMPEER-Controllable-Empathetic-Reinforcement-Reasoning-for-Emotional-Support-Conversation"><a href="#97-COMPEER-Controllable-Empathetic-Reinforcement-Reasoning-for-Emotional-Support-Conversation" class="headerlink" title="97. COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/COMPEER__Controllable_Empathetic_Reinforcement_Reasoning_for_Emotional_Support_Conversation.pdf">COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shandong University</span></p>
<p>该论文提出了一种结合自然语言推理与心理学结构化步骤的可控共情推理范式，并构建了细粒度推理与偏好标注数据集（CER），用于支持情感支持对话。方法包括基于结构化推理注释的统一过程-结果奖励模型UnifiReward、人格驱动的对话改写及冗余感知奖励重加权，利用强化学习优化共情推理与多样性。实验结果表明，提出的COMPEER框架有效提升了情感支持能力和对话多样性，推动了人性化情感支持系统的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/COMPEER%3A%20Controllable%20Empathetic%20Reinforcement%20Reasoning%20for%20Emotional%20Support%20Conversation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-An-Automated-Multi-Modal-Evaluation-Framework-for-Mobile-Intelligent-Assistants-Based-on-Large-Language-Models-and-Multi-Agent-Collaboration"><a href="#98-An-Automated-Multi-Modal-Evaluation-Framework-for-Mobile-Intelligent-Assistants-Based-on-Large-Language-Models-and-Multi-Agent-Collaboration" class="headerlink" title="98. An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants Based on Large Language Models and Multi-Agent Collaboration"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/An_Automated_Multi-Modal_Evaluation_Framework_for_Mobile_Intelligent_Assistants.pdf">An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants Based on Large Language Models and Multi-Agent Collaboration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nankai University</span></p>
<p>本文提出了一种基于大语言模型（如Qwen3-8B）和多智能体协作的自动化多模态移动智能助手评价框架，采用三层代理架构（交互评价、语义验证和体验决策）实现对文本、语音和图像输入的统一、多维度评测。实验表明，该框架在人机一致性、用户满意度预测和缺陷识别等方面表现优异，显著提升了自动化评测的效能和一致性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/An%20Automated%20Multi-Modal%20Evaluation%20Framework%20for%20Mobile%20Intelligent%20Assistants%20Based%20on%20Large%20Language%20Models%20and%20Multi-Agent%20Collaboration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-Learning-Facts-at-Scale-with-Active-Reading"><a href="#99-Learning-Facts-at-Scale-with-Active-Reading" class="headerlink" title="99. Learning Facts at Scale with Active Reading"></a>99. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Learning_Facts_at_Scale_with_Active_Reading.pdf">Learning Facts at Scale with Active Reading</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">FAIR at Meta</span></p>
<p>本文提出Active Reading框架，通过模型自生成多样化学习策略（如释义、知识关联、主动回忆、类比推理等），对给定知识文档进行合成训练数据生成，大幅提升大模型的事实记忆能力。实验表明，Active Reading在专家领域和大规模预训练场景下显著优于传统微调和单一数据增强方法，实现了更高的尾部事实召回率，并发布了Meta WikiExpert-8B模型及配套数据集，推动语言模型在知识掌握上的可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Learning%20Facts%20at%20Scale%20with%20Active%20Reading.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-EGGS-PTP-An-Expander-Graph-Guided-Structured-Post-training-Pruning-Method-for-Large-Language-Models"><a href="#100-EGGS-PTP-An-Expander-Graph-Guided-Structured-Post-training-Pruning-Method-for-Large-Language-Models" class="headerlink" title="100. EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models"></a>100. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/EGGS-PTP__An_Expander-Graph_Guided_Structured_Post-training_Pruning_Method_for_Large_Language_Models.pdf">EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Irvine</span></p>
<p>该论文提出EGGS-PTP，一种结合Expander图理论的结构化后训练剪枝方法，用于压缩大型语言模型（LLM）。EGGS-PTP通过重要性感知剪枝及结构感知剪枝相结合，既保留关键权重又确保剪枝后网络的信息流连通性，满足N:M稀疏性约束。实验表明，EGGS-PTP在多个LLM上较现有结构化剪枝方法有更优的准确率和效率，显著提升模型推理速度并降低内存消耗。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/EGGS-PTP%3A%20An%20Expander-Graph%20Guided%20Structured%20Post-training%20Pruning%20Method%20for%20Large%20Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="101-Modern-Neural-Networks-for-Small-Tabular-Datasets-The-New-Default-for-Field-Scale-Digital-Soil-Mapping"><a href="#101-Modern-Neural-Networks-for-Small-Tabular-Datasets-The-New-Default-for-Field-Scale-Digital-Soil-Mapping" class="headerlink" title="101. Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?"></a>101. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Modern_Neural_Networks_for_Small_Tabular_Datasets__The_New_Default_for_Field-Scale_Digital_Soil_Mapp.pdf">Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Osnabrück University</span></p>
<p>本文系统评测了现代深度神经网络（包括MLP、Transformer、检索增强与in-context learning模型，如TabPFN等）在小样本土壤属性预测（数字土壤制图）中的表现，并与传统机器学习方法（如随机森林、线性回归）进行了公平对比。结果表明，现代神经网络，尤其是TabPFN，能在绝大多数数据集上超越传统方法，成为小样本土壤建模的新默认选择。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Modern%20Neural%20Networks%20for%20Small%20Tabular%20Datasets%3A%20The%20New%20Default%20for%20Field-Scale%20Digital%20Soil%20Mapping%3F.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="102-Explainable-Ensemble-Learning-for-Graph-Based-Malware-Detection"><a href="#102-Explainable-Ensemble-Learning-for-Graph-Based-Malware-Detection" class="headerlink" title="102. Explainable Ensemble Learning for Graph-Based Malware Detection"></a>102. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Explainable_Ensemble_Learning_for_Graph-Based_Malware_Detection.pdf">Explainable Ensemble Learning for Graph-Based Malware Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of New Brunswick</span></p>
<p>本文提出了一种面向恶意软件检测的可解释堆叠集成学习框架，利用动态控制流图（CFG）从可执行文件中提取结构，采用两步嵌入方法编码基本块特征，并结合多种图神经网络（GNN）作为基学习器，通过带注意力机制的多层感知机作为元学习器融合预测结果并实现模型解释性。实验结果表明，该方法在多个真实数据集上提升了检测准确率、F1分数和AUC，且通过创新的集成感知解释方法有效识别关键子图，为安全分析提供了透明可用的决策依据。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Explainable%20Ensemble%20Learning%20for%20Graph-Based%20Malware%20Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="103-GraphTreeGen-Subtree-Centric-Approach-to-Efficient-and-Supervised-Graph-Generation"><a href="#103-GraphTreeGen-Subtree-Centric-Approach-to-Efficient-and-Supervised-Graph-Generation" class="headerlink" title="103. GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation"></a>103. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/GraphTreeGen__Subtree-Centric_Approach_to_Efficient_and_Supervised_Graph_Generation.pdf">GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Imperial College London</span></p>
<p>该论文提出GraphTreeGen（GTG），一种面向脑连接组图生成的高效、监督式生成框架。其方法将大脑图分解为由节点熵引导的k-hop子树，以捕获局部结构信息，并通过共享GCN进行编码，随后利用二分图消息传递层融合子树与全局节点特征，最后由双分支解码器联合预测边的存在性和权重，重构完整邻接矩阵。实验显示GTG在自监督任务中显著优于SOTA方法，且在监督任务中表现亦具竞争力，同时大幅节省内存，适合大规模或资源受限的场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/GraphTreeGen%3A%20Subtree-Centric%20Approach%20to%20Efficient%20and%20Supervised%20Graph%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="104-Personalized-Product-Search-Ranking-A-Multi-Task-Learning-Approach-with-Tabular-and-Non-Tabular-Data"><a href="#104-Personalized-Product-Search-Ranking-A-Multi-Task-Learning-Approach-with-Tabular-and-Non-Tabular-Data" class="headerlink" title="104. Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data"></a>104. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Personalized_Product_Search_Ranking__A_Multi-Task_Learning_Approach_with_Tabular_and_Non-Tabular_Dat.pdf">Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Walmart Global Tech</span></p>
<p>该论文提出一种多任务学习框架，将表格数据（用户特征、商品属性）与非表格数据（文本描述、用户评论）融合，采用MMoE结构并结合预训练TinyBERT语义嵌入，通过新颖的数据采样和自动生成相关性标签提升产品搜索排序的个性化效果。实验结果显示，该方法在多项任务上优于传统模型，文本特征与多任务学习的结合显著提升了排序性能与用户体验。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Personalized%20Product%20Search%20Ranking%3A%20A%20Multi-Task%20Learning%20Approach%20with%20Tabular%20and%20Non-Tabular%20Data.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="105-CWFBind-Geometry-Awareness-for-Fast-and-Accurate-Protein-Ligand-Docking"><a href="#105-CWFBind-Geometry-Awareness-for-Fast-and-Accurate-Protein-Ligand-Docking" class="headerlink" title="105. CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking"></a>105. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/CWFBind__Geometry-Awareness_for_Fast_and_Accurate_Protein-Ligand_Docking.pdf">CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Mathematics, Sun Yat-Sen University</span></p>
<p>该论文提出CWFBind，一种结合局部曲率特征（LCF）、度感知加权机制和自适应半径的蛋白-配体分子对接深度学习方法。方法流程包括蛋白和配体的多模态特征编码、几何感知的消息传递、类别不平衡下的口袋预测与动态调整以及端到端配体坐标优化。实验表明CWFBind在PDBbind v2020等基准上准确率与效率均超越主流方法，且泛化能力强。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/CWFBind%3A%20Geometry-Awareness%20for%20Fast%20and%20Accurate%20Protein-Ligand%20Docking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="106-Open-Set-Fault-Diagnosis-in-Multimode-Processes-via-Fine-Grained-Deep-Feature-Representation"><a href="#106-Open-Set-Fault-Diagnosis-in-Multimode-Processes-via-Fine-Grained-Deep-Feature-Representation" class="headerlink" title="106. Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation"></a>106. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Open-Set_Fault_Diagnosis_in_Multimode_Processes_via_Fine-Grained_Deep_Feature_Representation.pdf">Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Automation, Wuhan University of Technology</span></p>
<p>本文提出了一种新型的细粒度聚类与拒绝网络（FGCRN）用于多工况下的开放集故障诊断，结合多尺度深度卷积（MSDC）、双向门控循环单元（BiGRU）和时序注意力机制（TAM）实现深度判别特征提取，并通过无监督聚类与极值理论（EVT）增强未知故障的识别能力。结果显示该方法在多个仿真和真实工业数据集上均显著优于现有方法，有效提升了多模态复杂环境下已知&#x2F;未知故障的识别准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Open-Set%20Fault%20Diagnosis%20in%20Multimode%20Processes%20via%20Fine-Grained%20Deep%20Feature%20Representation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="107-A-pseudo-inverse-of-a-line-graph"><a href="#107-A-pseudo-inverse-of-a-line-graph" class="headerlink" title="107. A pseudo-inverse of a line graph"></a>107. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/A_pseudo-inverse_of_a_line_graph.pdf">A pseudo-inverse of a line graph</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CSIRO</span></p>
<p>本文提出了一种广义线图逆运算的伪逆新定义，通过线性整数规划对扰动后的线图（非严格线图）仅需最少边编辑（添加或删除），恢复为最近的线图，并以谱范数为度量理论证明该伪逆操作的良好性质和有界性。实验表明，单边扰动下伪逆的谱范数变化有限，且所提方法能有效恢复原图结构。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/A%20pseudo-inverse%20of%20a%20line%20graph.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="108-DeepFeatIoT-Unifying-Deep-Learned-Randomized-and-LLM-Features-for-Enhanced-IoT-Time-Series-Sensor-Data-Classification-in-Smart-Industries"><a href="#108-DeepFeatIoT-Unifying-Deep-Learned-Randomized-and-LLM-Features-for-Enhanced-IoT-Time-Series-Sensor-Data-Classification-in-Smart-Industries" class="headerlink" title="108. DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries"></a>108. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/DeepFeatIoT__Unifying_Deep_Learned,_Randomized,_and_LLM_Features_for_Enhanced_IoT_Time_Series_Sensor.pdf">DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Deakin University</span></p>
<p>该论文提出了一种新颖的深度学习模型DeepFeatIoT，将深度学习提取的局部&#x2F;全局特征、随机卷积核特征与大语言模型（LLM，如GPT2）提取的上下文特征进行统一融合，通过Dense Feature Transformation模块优化特征空间，提升IoT时序传感器数据的分类性能。实验表明，该方法在多种实际IoT异构传感器数据集上均优于现有SOTA模型，尤其在标注数据有限或数据异构性强场景下表现出更强泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/DeepFeatIoT%3A%20Unifying%20Deep%20Learned%2C%20Randomized%2C%20and%20LLM%20Features%20for%20Enhanced%20IoT%20Time%20Series%20Sensor%20Data%20Classification%20in%20Smart%20Industries.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="109-Graph-Neural-Network-and-Transformer-Integration-for-Unsupervised-System-Anomaly-Discovery"><a href="#109-Graph-Neural-Network-and-Transformer-Integration-for-Unsupervised-System-Anomaly-Discovery" class="headerlink" title="109. Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery"></a>109. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-14/Graph_Neural_Network_and_Transformer_Integration_for_Unsupervised_System_Anomaly_Discovery.pdf">Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Georgia Institute of Technology</span></p>
<p>本文提出了一种针对分布式后端服务系统的无监督异常检测方法，将图神经网络（GNN）用于多跳结构依赖建模，并结合Transformer捕捉节点时间行为的长短期依赖。通过可学习的嵌入融合机制，模型实现结构-行为特征的联合表征，采用非线性映射输出异常分数，实现端到端异常检测。实验证明该方法在阿里云真实监控数据上超越现有模型，在精度、鲁棒性和异常传播路径建模等方面表现优异，具备实际部署潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-15/Graph%20Neural%20Network%20and%20Transformer%20Integration%20for%20Unsupervised%20System%20Anomaly%20Discovery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>今日论文报纸</div>
      <div>http://example.com/2025/08/14/2025-08-15_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Ywfhhh</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月14日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/16/2025-08-18_article/" title="今日论文报纸">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">今日论文报纸</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/13/2025-08-14_article/" title="今日论文报纸">
                        <span class="hidden-mobile">今日论文报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
