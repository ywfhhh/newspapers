

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="具身ADT">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;26报纸">
<meta property="og:url" content="http://ywfhhh.github.io/2025/08/2025-08-26_article/index.html">
<meta property="og:site_name" content="最新前沿论文分享">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Guiding_Diffusion_Models_with_Reinforcement_Learning_for_Stable_Molecule_Generation.jpg">
<meta property="article:published_time" content="2025-08-26T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-25T18:45:23.681Z">
<meta property="article:author" content="具身ADT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Guiding_Diffusion_Models_with_Reinforcement_Learning_for_Stable_Molecule_Generation.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/26报纸 - 最新前沿论文分享</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ywfhhh.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<!-- hexo injector head_end start -->
<script src="/js/custom.js"></script>
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文库</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/26报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-26 00:00" pubdate>
          星期二, 八月 26日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          16k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/26报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/26报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-26<br>📄 <strong>发现论文数量</strong>：100  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Guiding-Diffusion-Models-with-Reinforcement-Learning-for-Stable-Molecule-Generation"><a href="#1-Guiding-Diffusion-Models-with-Reinforcement-Learning-for-Stable-Molecule-Generation" class="headerlink" title="1. Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Guiding_Diffusion_Models_with_Reinforcement_Learning_for_Stable_Molecule_Generation.pdf">Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>该论文提出了RLPF框架，将基于物理反馈的强化学习（PPO）与等变扩散模型结合，用于3D分子结构生成。通过将扩散去噪过程建模为马尔可夫决策过程，并利用力场评价作为奖励信号，RLPF显著提升了分子的物理稳定性和化学有效性，在QM9与GEOM-drug数据集上均优于现有方法。结论表明，基于物理反馈的RL微调能提升分子生成质量，且框架对不同扩散模型具备良好适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Guiding_Diffusion_Models_with_Reinforcement_Learning_for_Stable_Molecule_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-On-Zero-Shot-Reinforcement-Learning"><a href="#2-On-Zero-Shot-Reinforcement-Learning" class="headerlink" title="2. On Zero-Shot Reinforcement Learning"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/On_Zero-Shot_Reinforcement_Learning.pdf">On Zero-Shot Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cambridge</span></p>
<p>本文针对现实场景中零样本强化学习（Zero-Shot RL）面临的数据质量、可观测性和数据可用性三大约束，提出了保守性零样本RL算法、基于记忆的零样本RL方法和PEARL主动系统辨识算法。方法分别通过正则化、引入记忆模型和概率模型预测，系统性提升了低质量数据、部分可观测和无历史数据情况下的泛化性能。结论显示，这些方法可在现实任务中以有限甚至零先验数据实现高效泛化和任务迁移，推动RL实用化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/On_Zero-Shot_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-Double-Check-My-Desired-Return-Transformer-with-Target-Alignment-for-Offline-Reinforcement-Learning"><a href="#3-Double-Check-My-Desired-Return-Transformer-with-Target-Alignment-for-Offline-Reinforcement-Learning" class="headerlink" title="3. Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Double_Check_My_Desired_Return__Transformer_with_Target_Alignment_for_Offline_Reinforcement_Learning.pdf">Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本文提出Doctor方法，通过双重检查机制提升Transformer在离线强化学习中的目标回报对齐能力。其关键流程包括利用双向Transformer进行序列重建和动作-价值预测，并在推理时对候选动作进行采样和价值验证，实现对目标回报的精准控制。实验结果表明，Doctor在多项基准任务中实现了优于现有方法的目标回报对齐，无论是在数据集覆盖范围内还是外部均表现出色，且在医疗决策等实际场景下能够平衡治疗效果与风险，具有广泛应用前景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Double_Check_My_Desired_Return_Transformer_with_Target_Alignment_for_Offline_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-Constraints-Guided-Diffusion-Reasoner-for-Neuro-Symbolic-Learning"><a href="#4-Constraints-Guided-Diffusion-Reasoner-for-Neuro-Symbolic-Learning" class="headerlink" title="4. Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Constraints-Guided_Diffusion_Reasoner_for_Neuro-Symbolic_Learning.pdf">Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本文提出了DDReasoner，一种基于扩散模型的神经-符号学习方法，采用两阶段训练流程：首先通过掩码扩散模型进行有监督学习，培养基础推理能力；随后将扩散去噪过程形式化为马尔可夫决策过程，利用改进的PPO算法和基于约束一致性的奖励信号进行强化学习微调。实验结果显示，该方法在数独、迷宫、路径规划等符号推理任务上，能有效内化逻辑约束，显著提升逻辑一致性和准确率，并优于同类神经网络方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Constraints-Guided_Diffusion_Reasoner_for_Neuro-Symbolic_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-Reinforcement-Learning-based-Control-via-Y-wise-Affine-Neural-Networks-YANNs"><a href="#5-Reinforcement-Learning-based-Control-via-Y-wise-Affine-Neural-Networks-YANNs" class="headerlink" title="5. Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Reinforcement_Learning-based_Control_via_Y-wise_Affine_Neural_Networks_(YANNs).pdf">Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">West Virginia University</span></p>
<p>本文提出了一种基于Y-wise Affine Neural Networks (YANNs)的新型强化学习算法。该方法利用YANNs精确表示线性模型预测控制的分段仿射显式解，实现可解释和高效的RL初始策略，并通过在网络中注入可训练的非线性层提升对复杂系统的适应性。实验证明，在安全约束下，YANN-RL显著优于现代深度确定性策略梯度（DDPG）算法，尤其是在安全关键系统控制中表现出更高的稳定性和安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Reinforcement_Learning-based_Control_via_Y-wise_Affine_Neural_Networks_%28YANNs%29.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-OPERA-A-Reinforcement-Learning–Enhanced-Orchestrated-Planner-Executor-Architecture-for-Reasoning-Oriented-Multi-Hop-Retrieval"><a href="#6-OPERA-A-Reinforcement-Learning–Enhanced-Orchestrated-Planner-Executor-Architecture-for-Reasoning-Oriented-Multi-Hop-Retrieval" class="headerlink" title="6. OPERA: A Reinforcement Learning–Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/OPERA__A_Reinforcement_Learning--Enhanced_Orchestrated_Planner-Executor_Architecture_for_Reasoning-O.pdf">OPERA: A Reinforcement Learning–Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>本文提出OPERA框架，通过多智能体架构（包括规划、分析-回答和重写代理）分离复杂多跳检索任务中的战略规划和执行，并利用多智能体递进式群体相对策略优化（MAPGRPO）进行逐步训练。实验表明，该方法在多跳问答基准上显著提升了检索和推理能力，尤其在复杂任务上优于现有强化学习和检索增强生成系统，体系结构的模块化和专用奖励进一步促进了性能提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/OPERA_A_Reinforcement_Learning%E2%80%93Enhanced_Orchestrated_Planner-Executor_Architecture_for_Reasoning-Oriented_Multi-Hop_Retrieval.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-Representation-Learning-of-Auxiliary-Concepts-for-Improved-Student-Modeling-and-Exercise-Recommendation"><a href="#7-Representation-Learning-of-Auxiliary-Concepts-for-Improved-Student-Modeling-and-Exercise-Recommendation" class="headerlink" title="7. Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Representation_Learning_of_Auxiliary_Concepts_for_Improved_Student_Modeling_and_Exercise_Recommendat.pdf">Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Karlsruhe University of Applied Sciences</span></p>
<p>该论文提出了一种深度学习模型（SBRKT），通过学习稀疏二值辅助知识概念（auxiliary KCs），为每道题生成可解释的特征标签。模型将辅助KCs与传统的人类标注KCs结合，提升了学生知识追踪（如BKT、DKT）与个性化习题推荐算法的性能，并在多数据集实验中证明了辅助KCs在强化学习推荐和规划方法（如Expectimax）中的效果提升。结论显示：辅助KCs能显著提升学生模型预测和习题推荐的准确性，兼具可解释性和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Representation_Learning_of_Auxiliary_Concepts_for_Improved_Student_Modeling_and_Exercise_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-Pareto-Actor-Critic-for-Communication-and-Computation-Co-Optimization-in-Non-Cooperative-Federated-Learning-Services"><a href="#8-Pareto-Actor-Critic-for-Communication-and-Computation-Co-Optimization-in-Non-Cooperative-Federated-Learning-Services" class="headerlink" title="8. Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Pareto_Actor-Critic_for_Communication_and_Computation_Co-Optimization_in_Non-Cooperative_Federated_L.pdf">Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出PAC-MCoFL框架，利用Pareto Actor-Critic（PAC）结合expectile回归，在多服务提供商（SPs）非合作联邦学习场景下进行通信与计算资源的多目标联合优化。方法包括TCAD高维动作空间分解和参数化策略生成器（PAC-MCoFL-p），理论上保证收敛并显著优于现有多智能体强化学习基线，在奖励和Pareto前沿指标上提升5.8%和4.2%。结论表明，PAC-MCoFL能够有效实现多SP系统下的均衡和高效资源优化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Pareto_Actor-Critic_for_Communication_and_Computation_Co-Optimization_in_Non-Cooperative_Federated_Learning_Services.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-Breaking-Barriers-in-Software-Testing-The-Power-of-AI-Driven-Automation"><a href="#9-Breaking-Barriers-in-Software-Testing-The-Power-of-AI-Driven-Automation" class="headerlink" title="9. Breaking Barriers in Software Testing: The Power of AI-Driven Automation"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Breaking_Barriers_in_Software_Testing__The_Power_of_AI-Driven_Automation.pdf">Breaking Barriers in Software Testing: The Power of AI-Driven Automation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">MUFG Bank</span></p>
<p>本论文提出了一个结合自然语言处理、强化学习和预测模型的AI自动化测试框架，通过将需求文档自动转化为测试用例并持续优化，实现高效、低偏差的软件测试流程。结论显示该框架在真实金融服务应用中大幅缩短测试周期、提高缺陷检测率和合规性，为复杂软件环境下的质量保障提供了可行方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Breaking_Barriers_in_Software_Testing_The_Power_of_AI-Driven_Automation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="10-Hierarchical-Decision-Making-for-Autonomous-Navigation-Integrating-Deep-Reinforcement-Learning-and-Fuzzy-Logic-in-Four-Wheel-Independent-Steering-and-Driving-Systems"><a href="#10-Hierarchical-Decision-Making-for-Autonomous-Navigation-Integrating-Deep-Reinforcement-Learning-and-Fuzzy-Logic-in-Four-Wheel-Independent-Steering-and-Driving-Systems" class="headerlink" title="10. Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Hierarchical_Decision-Making_for_Autonomous_Navigation__Integrating_Deep_Reinforcement_Learning_and_.pdf">Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Central South University</span></p>
<p>该论文提出了一种用于四轮独立转向与驱动(4WISD)系统自主导航的分层决策框架，将高层深度强化学习（DRL）用于全局导航决策，底层模糊逻辑控制器负责物理可行的轮速与转向控制。实验结果表明，该方法在动态工业环境中优于传统方法，训练效率和稳定性更高，并有效减少机械应力和轮胎打滑。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Hierarchical_Decision-Making_for_Autonomous_Navigation_Integrating_Deep_Reinforcement_Learning_and_Fuzzy_Logic_in_Four-Wheel_Independent_Steering_and_Driving_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-On-Kinodynamic-Global-Planning-in-a-Simplicial-Complex-Environment-A-Mixed-Integer-Approach"><a href="#11-On-Kinodynamic-Global-Planning-in-a-Simplicial-Complex-Environment-A-Mixed-Integer-Approach" class="headerlink" title="11. On Kinodynamic Global Planning in a Simplicial Complex Environment: A Mixed Integer Approach"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/On_Kinodynamic_Global_Planning_in_a_Simplicial_Complex_Environment__A_Mixed_Integer_Approach.pdf">On Kinodynamic Global Planning in a Simplicial Complex Environment: A Mixed Integer Approach</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Innopolis University</span></p>
<p>本文提出了一种混合整数动力学(MIKD)规划器，将汽车类车辆在3D三角网格地形上的动力学路径规划建模为带有二次约束的混合整数优化问题，通过变量变换和McCormick松弛将其转化为混合整数线性规划，显著提升了约束满足性和计算效率。实验结果表明，该方法在满足动力学约束的同时，相较于MPPI等主流方法计算速度提升约10^4倍，且路径可行性更高，适用于复杂三维地形的全局路径规划。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/On_Kinodynamic_Global_Planning_in_a_Simplicial_Complex_Environment_A_Mixed_Integer_Approach.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-GPL-SLAM-A-Laser-SLAM-Framework-with-Gaussian-Process-Based-Extended-Landmarks"><a href="#12-GPL-SLAM-A-Laser-SLAM-Framework-with-Gaussian-Process-Based-Extended-Landmarks" class="headerlink" title="12. GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended Landmarks"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/GPL-SLAM__A_Laser_SLAM_Framework_with_Gaussian_Process_Based_Extended_Landmarks.pdf">GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended Landmarks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">TU Delft</span></p>
<p>本文提出了一种基于高斯过程（GP）对象轮廓建模的激光SLAM方法，采用递归贝叶斯框架联合估计机器人位姿和基于对象的地图。方法以星凸集为对象表示，结合递归IEKF实现在线SLAM，显著提升了定位与建图的精度和效率，同时为对象形状提供不确定性边界。实验表明，该方法在仿真和真实环境中均优于传统网格法，具备更高的鲁棒性和内存效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/GPL-SLAM_A_Laser_SLAM_Framework_with_Gaussian_Process_Based_Extended_Landmarks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-Integrated-Noise-and-Safety-Management-in-UAM-via-A-Unified-Reinforcement-Learning-Framework"><a href="#13-Integrated-Noise-and-Safety-Management-in-UAM-via-A-Unified-Reinforcement-Learning-Framework" class="headerlink" title="13. Integrated Noise and Safety Management in UAM via A Unified Reinforcement Learning Framework"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Integrated_Noise_and_Safety_Management_in_UAM_via_A_Unified_Reinforcement_Learning_Framework.pdf">Integrated Noise and Safety Management in UAM via A Unified Reinforcement Learning Framework</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Texas at Austin</span></p>
<p>该论文提出了一个多智能体强化学习框架，集成了城市空中交通(UAM)中的噪音影响与安全分离管理，通过基于PPO的深度神经网络(D2MAV-A架构)实现航迹高度动态调整，考虑噪音、分离和能耗三重目标。实验结果表明，RL方法能在高密度交通下平衡分离安全与噪音暴露，并揭示了安全、环保和能效间的关键权衡，为未来城市空中交通管理系统提供了可扩展的智能化解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Integrated_Noise_and_Safety_Management_in_UAM_via_A_Unified_Reinforcement_Learning_Framework.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-SAMFusion-Sensor-Adaptive-Multimodal-Fusion-for-3D-Object-Detection-in-Adverse-Weather"><a href="#14-SAMFusion-Sensor-Adaptive-Multimodal-Fusion-for-3D-Object-Detection-in-Adverse-Weather" class="headerlink" title="14. SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/SAMFusion__Sensor-Adaptive_Multimodal_Fusion_for_3D_Object_Detection_in_Adverse_Weather.pdf">SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Torc Robotics</span></p>
<p>本文提出SAMFusion，一种针对恶劣天气下自主驾驶场景的多模态3D目标检测方法。该方法将RGB、LiDAR、雷达和近红外门控相机特征通过深度引导的自适应融合、基于距离加权的transformer解码器在鸟瞰视角（BEV）中有效集成，实现多源传感器信息的动态加权与融合。实验表明，SAMFusion在夜间、雾天、雪天等复杂环境下，尤其是远距离和行人检测方面，显著优于当前多模态融合方法，提升最大达17.2 AP。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/SAMFusion_Sensor-Adaptive_Multimodal_Fusion_for_3D_Object_Detection_in_Adverse_Weather.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-Do-What-Teaching-Vision-Language-Action-Models-to-Reject-the-Impossible"><a href="#15-Do-What-Teaching-Vision-Language-Action-Models-to-Reject-the-Impossible" class="headerlink" title="15. Do What? Teaching Vision-Language-Action Models to Reject the Impossible"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Do_What__Teaching_Vision-Language-Action_Models_to_Reject_the_Impossible.pdf">Do What? Teaching Vision-Language-Action Models to Reject the Impossible</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Berkeley</span></p>
<p>该论文提出了Instruct-Verify-and-Act (IVA) 框架，基于LLARVA模型，结合大规模指令微调和结构化多模态数据，使机器人能够检测、澄清并纠正因场景缺失或不合理而无法实现的自然语言指令。实验表明IVA在识别和处理虚假前提指令方面显著优于基线，并在常规任务表现稳定，推动了语言感知机器人与人类更安全自然的交互。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Do_What_Teaching_Vision-Language-Action_Models_to_Reject_the_Impossible.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-Take-That-for-Me-Multimodal-Exophora-Resolution-with-Interactive-Questioning-for-Ambiguous-Out-of-View-Instructions"><a href="#16-Take-That-for-Me-Multimodal-Exophora-Resolution-with-Interactive-Questioning-for-Ambiguous-Out-of-View-Instructions" class="headerlink" title="16. Take That for Me: Multimodal Exophora Resolution with Interactive Questioning for Ambiguous Out-of-View Instructions"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Take_That_for_Me__Multimodal_Exophora_Resolution_with_Interactive_Questioning_for_Ambiguous_Out-of-V.pdf">Take That for Me: Multimodal Exophora Resolution with Interactive Questioning for Ambiguous Out-of-View Instructions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ritsumeikan University</span></p>
<p>该论文提出MIEL方法，通过结合声音源定位（SSL）、语义地图、视觉-语言模型（VLMs）及GPT-4o交互式提问，实现机器人在面对含指示词且目标物体或用户不在视野范围内的模糊指令时的多模态指代消解。实验结果表明，MIEL在用户可见时比无SSL和交互模块的方法提升约1.3倍成功率，用户不可见时提升约2倍，显著增强了机器人在实际环境下对模糊指令的理解和执行能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Take_That_for_Me_Multimodal_Exophora_Resolution_with_Interactive_Questioning_for_Ambiguous_Out-of-View_Instructions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-CoVeRaP-Cooperative-Vehicular-Perception-through-mmWave-FMCW-Radars"><a href="#17-CoVeRaP-Cooperative-Vehicular-Perception-through-mmWave-FMCW-Radars" class="headerlink" title="17. CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/CoVeRaP__Cooperative_Vehicular_Perception_through_mmWave_FMCW_Radars.pdf">CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">UC Davis</span></p>
<p>该论文提出CoVeRaP数据集，为多车协作毫米波FMCW雷达感知提供高质量时间对齐的雷达、摄像头和GPS数据，并设计了基于多分支自注意力网络的协作感知框架，支持中融合和晚融合两种特征整合策略，实现了多视角雷达点云的空间对齐和融合。实验结果表明中融合结合强度编码能在高IoU阈值下将mAP提升至单车基线的9倍，且始终优于晚融合，验证了多车雷达共享在提高检测鲁棒性上的显著作用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/CoVeRaP_Cooperative_Vehicular_Perception_through_mmWave_FMCW_Radars.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="18-MV-RAG-Retrieval-Augmented-Multiview-Diffusion"><a href="#18-MV-RAG-Retrieval-Augmented-Multiview-Diffusion" class="headerlink" title="18. MV-RAG: Retrieval Augmented Multiview Diffusion"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/MV-RAG__Retrieval_Augmented_Multiview_Diffusion.pdf">MV-RAG: Retrieval Augmented Multiview Diffusion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hebrew University of Jerusalem</span></p>
<p>该论文提出MV-RAG，一种检索增强的多视图扩散框架，用于文本到多视图（3D）生成。方法核心在于：首先根据文本提示从大规模2D图像库检索相关图像，利用CLIP-ViT编码和Resampler提取局部特征，再通过多视图扩散模型结合检索到的图像与文本条件生成一致且高保真的多视图图像。训练时采用混合策略，结合结构化3D多视图数据和大规模2D图集，分别采用视图重建和held-out视图预测目标，提升3D一致性与泛化能力。论文还提出自适应融合机制，根据输入分布自适应权衡模型先验与检索信号，并构建OOD-Eval基准集专门评估稀有&#x2F;分布外概念。实验表明，MV-RAG在稀有和OOD概念的3D一致性、写实性及文本对齐方面显著优于SOTA方法，同时在常规基准上表现同样优异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/MV-RAG_Retrieval_Augmented_Multiview_Diffusion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-Closer-to-Reality-Practical-Semi-Supervised-Federated-Learning-for-Foundation-Model-Adaptation"><a href="#19-Closer-to-Reality-Practical-Semi-Supervised-Federated-Learning-for-Foundation-Model-Adaptation" class="headerlink" title="19. Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Closer_to_Reality__Practical_Semi-Supervised_Federated_Learning_for_Foundation_Model_Adaptation.pdf">Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sony AI</span></p>
<p>本文提出了一种面向现实应用的半监督联邦学习（PSSFL）新设定，针对边缘设备上仅有低分辨率无标签数据、服务器拥有有限高分辨率有标签数据的场景，提出了FedMox框架。FedMox通过稀疏空间专家混合（Mixture-of-Experts）结构和Soft-Mixture参数融合策略，解决了冻结骨干网络、分辨率不匹配和顺序更新导致的泛化与稳定性难题。实验表明，FedMox在自动驾驶等真实数据集上显著优于现有方法，实现了高效、隐私保护的基础模型适配，推动了复杂任务下联邦学习的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Closer_to_Reality_Practical_Semi-Supervised_Federated_Learning_for_Foundation_Model_Adaptation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-Towards-Open-World-Detection-A-Survey"><a href="#20-Towards-Open-World-Detection-A-Survey" class="headerlink" title="20. Towards Open World Detection: A Survey"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Towards_Open_World_Detection__A_Survey.pdf">Towards Open World Detection: A Survey</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universitatea Politehnica Timisoara</span></p>
<p>本论文系统梳理并提出“开放世界检测（OWD）”作为统一视觉领域中类别无关、任务泛化检测模型的总称，涵盖了显著性检测、前景&#x2F;背景分离、分布外检测、零样本检测、开放世界目标检测、视觉大语言模型等子领域，强调它们的融合趋势和未来一体化发展。结论指出，随着大规模多模态基础模型的兴起，视觉感知任务正逐步融合，未来OWD将成为应对新颖性与动态变化场景的核心技术路径，对多领域有重大影响，尤其在需要鲁棒性和动态更新的应用中将日益重要。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Towards_Open_World_Detection_A_Survey.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-Disentangled-Multi-modal-Learning-of-Histology-and-Transcriptomics-for-Cancer-Characterization"><a href="#21-Disentangled-Multi-modal-Learning-of-Histology-and-Transcriptomics-for-Cancer-Characterization" class="headerlink" title="21. Disentangled Multi-modal Learning of Histology and Transcriptomics for Cancer Characterization"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Disentangled_Multi-modal_Learning_of_Histology_and_Transcriptomics_for_Cancer_Characterization.pdf">Disentangled Multi-modal Learning of Histology and Transcriptomics for Cancer Characterization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cambridge</span></p>
<p>该论文提出一个两阶段的多模态学习框架，利用解耦融合模块（DMSF）、置信度引导梯度协调（CGC），以及尺度间基因表达一致性（IGC）策略，将癌症组织病理图像（WSI）和转录组数据分解到肿瘤和微环境子空间并多尺度融合。通过子空间知识蒸馏（SKD）和信息聚合模块（ITA），实现仅用WSI的高效、可解释癌症诊断、分级与生存预测。实验显示该方法在多项任务和数据集均优于现有方法，具备广泛临床适用性和较强泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Disentangled_Multi-modal_Learning_of_Histology_and_Transcriptomics_for_Cancer_Characterization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-HOSt3R-Keypoint-free-Hand-Object-3D-Reconstruction-from-RGB-images"><a href="#22-HOSt3R-Keypoint-free-Hand-Object-3D-Reconstruction-from-RGB-images" class="headerlink" title="22. HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/HOSt3R__Keypoint-free_Hand-Object_3D_Reconstruction_from_RGB_images.pdf">HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">NAVER LABS Europe</span></p>
<p>本文提出了一种无需关键点检测的手-物体三维重建方法HOSt3R，核心为基于Vision Transformer的点云对齐和姿态平均，通过对单目视频每对图像估计稠密3D点图和相对姿态，再进行多视角位姿平均及神经隐式建模，实现无模板、无相机内参假设的高精度三维重建。实验表明，该方法在SHOWMe基准上表现优异，并能泛化至HO3D等新数据集，在复杂遮挡和纹理弱物体下也具备鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/HOSt3R_Keypoint-free_Hand-Object_3D_Reconstruction_from_RGB_images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-HAMSt3R-Human-Aware-Multi-view-Stereo-3D-Reconstruction"><a href="#23-HAMSt3R-Human-Aware-Multi-view-Stereo-3D-Reconstruction" class="headerlink" title="23. HAMSt3R: Human-Aware Multi-view Stereo 3D Reconstruction"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/HAMSt3R__Human-Aware_Multi-view_Stereo_3D_Reconstruction.pdf">HAMSt3R: Human-Aware Multi-view Stereo 3D Reconstruction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KAUST</span></p>
<p>本文提出HAMSt3R，一种基于MASt3R扩展的全新端到端前馈式多视角3D重建方法，能够从稀疏、非标定多视角图像中联合重建场景与人体。方法核心是采用DUNE蒸馏ViT编码器，并引入实例分割、DensePose和深度估计分支，实现3D点云中直接嵌入人体语义信息。实验表明，HAMSt3R在EgoHumans和EgoExo4D等具挑战性的人体场景下优于现有方法，同时在传统3D重建任务中也表现出色。结论是HAMSt3R高效地桥接了人体与场景3D理解，为实际应用提供了更强的适用性和扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/HAMSt3R_Human-Aware_Multi-view_Stereo_3D_Reconstruction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-A-Disease-Centric-Vision-Language-Foundation-Model-for-Precision-Oncology-in-Kidney-Cancer"><a href="#24-A-Disease-Centric-Vision-Language-Foundation-Model-for-Precision-Oncology-in-Kidney-Cancer" class="headerlink" title="24. A Disease-Centric Vision-Language Foundation Model for Precision Oncology in Kidney Cancer"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/A_Disease-Centric_Vision-Language_Foundation_Model_for_Precision_Oncology_in_Kidney_Cancer.pdf">A Disease-Centric Vision-Language Foundation Model for Precision Oncology in Kidney Cancer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>该论文提出了RenalCLIP，一个针对肾癌的视觉-语言基础模型，通过大规模多中心CT影像与放射报告两阶段预训练（知识增强+对比学习），实现了肾肿瘤解剖评分、诊断分型、预后预测、报告生成、跨模态检索等多任务一体化自动化分析。实验表明，RenalCLIP在多项临床任务（如恶性&#x2F;侵袭性诊断及生存预测）上均显著优于主流医学影像基础模型，具备极强泛化能力与数据效率，有望提高肾癌精准诊断与个体化管理水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/A_Disease-Centric_Vision-Language_Foundation_Model_for_Precision_Oncology_in_Kidney_Cancer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-Time-Aware-One-Step-Diffusion-Network-for-Real-World-Image-Super-Resolution"><a href="#25-Time-Aware-One-Step-Diffusion-Network-for-Real-World-Image-Super-Resolution" class="headerlink" title="25. Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Time-Aware_One_Step_Diffusion_Network_for_Real-World_Image_Super-Resolution.pdf">Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nankai University</span></p>
<p>本文提出了一种面向真实世界图像超分辨率的时序感知单步扩散网络（TADSR），引入时序感知VAE编码器（TAE）与时序感知变分分数蒸馏损失（TAVSD），使模型能够根据不同时间步充分利用Stable Diffusion的生成先验，实现对细节与真实感的可控权衡。实验结果表明，TADSR在多个合成和真实数据集上达到了同类方法中的最优性能，并显著提升了图像的真实性和语义表达能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Time-Aware_One_Step_Diffusion_Network_for_Real-World_Image_Super-Resolution.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-Seeing-Clearly-Forgetting-Deeply-Revisiting-Fine-Tuned-Video-Generators-for-Driving-Simulation"><a href="#26-Seeing-Clearly-Forgetting-Deeply-Revisiting-Fine-Tuned-Video-Generators-for-Driving-Simulation" class="headerlink" title="26. Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Seeing_Clearly,_Forgetting_Deeply__Revisiting_Fine-Tuned_Video_Generators_for_Driving_Simulation.pdf">Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Delft University of Technology</span></p>
<p>本文系统分析了针对自动驾驶仿真的视频生成模型微调过程中的效果，发现虽能显著提升视觉质量和表面真实感，但却可能导致对动态场景理解能力下降，尤其是对关键交通参与者的空间准确性。作者通过引入简单的持续学习策略（如跨领域样本回放），证明可在不损失视觉质量的前提下，缓解微调引发的动态信息遗忘问题，从而更好地兼顾视觉与语义一致性，提升仿真安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Seeing_Clearly_Forgetting_Deeply_Revisiting_Fine-Tuned_Video_Generators_for_Driving_Simulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-Modular-Embedding-Recomposition-for-Zero-Shot-Incremental-Learning"><a href="#27-Modular-Embedding-Recomposition-for-Zero-Shot-Incremental-Learning" class="headerlink" title="27. Modular Embedding Recomposition for Zero-Shot Incremental Learning"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Modular_Embedding_Recomposition_for_Incremental_Learning.pdf">Modular Embedding Recomposition for Zero-Shot Incremental Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Modena and Reggio Emilia</span></p>
<p>该论文提出了MoDER方法，通过模块化嵌入重组，增量训练多个针对单类的文本专家模块，并在推理时组合专家以生成更适合未见类别的原型，从而提升视觉语言模型的零样本分类能力。实验表明，MoDER在14个增量学习数据集上显著提升了零样本性能，并有效缓解了遗忘问题，优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Modular_Embedding_Recomposition_for_Zero-Shot_Incremental_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-Attention-Mechanism-in-Randomized-Time-Warping"><a href="#28-Attention-Mechanism-in-Randomized-Time-Warping" class="headerlink" title="28. Attention Mechanism in Randomized Time Warping"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Attention_Mechanism_in_Randomized_Time_Warping.pdf">Attention Mechanism in Randomized Time Warping</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Japan Research Institute, Limited</span></p>
<p>该论文提出将Randomized Time Warping (RTW)的核心机制解释为一种自注意力机制，并与Transformer中的多头自注意力进行了比较。通过在动作识别任务（Something-Something V2数据集）中实验证明，RTW在整个输入序列上进行注意力分配，且在小样本数据下性能优于Transformer，准确率提升约5%。结论表明RTW可作为自注意力模块的有效替代方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Attention_Mechanism_in_Randomized_Time_Warping.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-Exploiting-Information-Redundancy-in-Attention-Maps-for-Extreme-Quantization-of-Vision-Transformers"><a href="#29-Exploiting-Information-Redundancy-in-Attention-Maps-for-Extreme-Quantization-of-Vision-Transformers" class="headerlink" title="29. Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Exploiting_Information_Redundancy_in_Attention_Maps_for_Extreme_Quantization_of_Vision_Transformers.pdf">Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universite Paris-Saclay</span></p>
<p>该论文提出基于熵的注意力图分析方法（Entropy Attention Maps, EAM），通过量化每个注意力头的信息冗余，对低熵注意力头进行权重冻结和极低比特量化（如4位），从而减少模型推理的计算与内存开销。实验表明，在ImageNet-1K数据集和多种ViT架构下，EAM在10%-20%稀疏度时可提升或保持准确率，最高可达40%稀疏度而性能损失极小。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Exploiting_Information_Redundancy_in_Attention_Maps_for_Extreme_Quantization_of_Vision_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-Robust-Small-Methane-Plume-Segmentation-in-Satellite-Imagery"><a href="#30-Robust-Small-Methane-Plume-Segmentation-in-Satellite-Imagery" class="headerlink" title="30. Robust Small Methane Plume Segmentation in Satellite Imagery"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Robust_Small_Methane_Plume_Segmentation_in_Satellite_Imagery.pdf">Robust Small Methane Plume Segmentation in Satellite Imagery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Curtin University</span></p>
<p>本文提出基于U-Net结合ResNet34编码器的深度学习方法，通过集成Varon ratio和Sanchez regression两种光谱增强技术优化卫星影像输入特征，实现对小型甲烷羽流（最小至400㎡，即单像素级）的精准分割。实验结果表明，该方法在敏感性和精度上明显优于传统遥感技术，验证集F1分数达78.39%，为自动化甲烷监测系统提供了强有力的解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Robust_Small_Methane_Plume_Segmentation_in_Satellite_Imagery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-UniEM-3M-A-Universal-Electron-Micrograph-Dataset-for-Microstructural-Segmentation-and-Generation"><a href="#31-UniEM-3M-A-Universal-Electron-Micrograph-Dataset-for-Microstructural-Segmentation-and-Generation" class="headerlink" title="31. UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/UniEM-3M__A_Universal_Electron_Micrograph_Dataset_for_Microstructural_Segmentation_and_Generation.pdf">UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">DP Technology</span></p>
<p>本论文提出了UniEM-3M，这是首个大规模多模态电子显微图（EM）数据集，包含5091张高分辨率EM图像、约300万实例分割标注及结构化文本描述，并引入了专为高密度微结构分割设计的UniEM-Net基线模型。团队还基于结构化描述训练了文本到图像扩散模型，用以数据增强和隐私保护。多项实验显示，UniEM-Net在高密度分割任务上大幅优于现有方法，生成数据能有效提升模型性能。结论：UniEM-3M及其配套工具显著推动了材料自动分析与视觉-语言模型研究。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/UniEM-3M_A_Universal_Electron_Micrograph_Dataset_for_Microstructural_Segmentation_and_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-PromptFlare-Prompt-Generalized-Defense-via-Cross-Attention-Decoy-in-Diffusion-Based-Inpainting"><a href="#32-PromptFlare-Prompt-Generalized-Defense-via-Cross-Attention-Decoy-in-Diffusion-Based-Inpainting" class="headerlink" title="32. PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/PromptFlare__Prompt-Generalized_Defense_via_Cross-Attention_Decoy_in_Diffusion-Based_Inpainting.pdf">PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sungkyunkwan University</span></p>
<p>PromptFlare提出了一种针对扩散模型图像修复的通用防御方法，通过在跨注意力机制中注入对抗性噪声，专门干扰无语义信息的共享BOS token，使模型难以根据文本提示进行恶意修改。该方法在EditBench数据集上实现了最优防护效果，显著降低了计算资源消耗，验证了对抗性保护的高效性和稳健性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/PromptFlare_Prompt-Generalized_Defense_via_Cross-Attention_Decoy_in_Diffusion-Based_Inpainting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-OmniCache-A-Trajectory-Oriented-Global-Perspective-on-Training-Free-Cache-Reuse-for-Diffusion-Transformer-Models"><a href="#33-OmniCache-A-Trajectory-Oriented-Global-Perspective-on-Training-Free-Cache-Reuse-for-Diffusion-Transformer-Models" class="headerlink" title="33. OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/OmniCache__A_Trajectory-Oriented_Global_Perspective_on_Training-Free_Cache_Reuse_for_Diffusion_Trans.pdf">OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhipu AI</span></p>
<p>OmniCache提出了一种基于采样轨迹全局分析的训练无关缓存复用方法，通过采样轨迹曲率选择最佳复用节点，并引入噪声动态估计与高&#x2F;低通滤波纠正缓存噪声，提升Diffusion Transformer模型的推理速度。实验表明该方法在视频和图像生成任务中能实现2-2.5倍加速且几乎无性能损失，尤其在采样步数较少或模型冗余低的场景下优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/OmniCache_A_Trajectory-Oriented_Global_Perspective_on_Training-Free_Cache_Reuse_for_Diffusion_Transformer_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-T-MASK-Temporal-Masking-for-Probing-Foundation-Models-across-Camera-Views-in-Driver-Monitoring"><a href="#34-T-MASK-Temporal-Masking-for-Probing-Foundation-Models-across-Camera-Views-in-Driver-Monitoring" class="headerlink" title="34. T-MASK: Temporal Masking for Probing Foundation Models across Camera Views in Driver Monitoring"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/_textsc%7BT-Mask%7D__Temporal_Masking_for_Probing_Foundation_Models_across_Camera_Views_in_Driver_Monito.pdf">T-MASK: Temporal Masking for Probing Foundation Models across Camera Views in Driver Monitoring</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Stuttgart</span></p>
<p>本文提出了T-MASK，一种用于驾驶员监控的视觉基础模型跨摄像头视角的轻量级探测方法。T-MASK通过计算视频帧间的嵌入差异，动态屏蔽静态、视角依赖性强的令牌，使探测模块聚焦于运动相关特征，从而提升模型对未见视角和数据稀缺动作的鲁棒性。在Drive&amp;Act数据集上的实验表明，T-MASK在不增加模型参数的情况下，显著提升了跨视角识别准确率，优于常规探测和参数高效微调方法，成为驾驶员活动识别领域的新基线。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/T-MASK_Temporal_Masking_for_Probing_Foundation_Models_across_Camera_Views_in_Driver_Monitoring.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-RAGSR-Regional-Attention-Guided-Diffusion-for-Image-Super-Resolution"><a href="#35-RAGSR-Regional-Attention-Guided-Diffusion-for-Image-Super-Resolution" class="headerlink" title="35. RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/RAGSR__Regional_Attention_Guided_Diffusion_for_Image_Super-Resolution.pdf">RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science, Wuhan University</span></p>
<p>该论文提出了RAGSR方法，通过结合区域细粒度文本描述和区域引导注意力机制，提升了文本驱动扩散模型在单幅图像超分辨上的表现。具体流程为先用大视觉语言模型对降质低分辨图像生成全局与区域文本描述，再通过自研区域注意力机制对齐文本与图像区域，引导扩散模型重建高质量细节。实验表明，该方法在多个公开数据集上超越现有最佳方法，有效提升了结构精度和语义一致性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/RAGSR_Regional_Attention_Guided_Diffusion_for_Image_Super-Resolution.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-Vision-encoders-should-be-image-size-agnostic-and-task-driven"><a href="#36-Vision-encoders-should-be-image-size-agnostic-and-task-driven" class="headerlink" title="36. Vision encoders should be image size agnostic and task driven"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Vision_encoders_should_be_image_size_agnostic_and_task_driven.pdf">Vision encoders should be image size agnostic and task driven</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">INSAIT, Sofia University “St. Kliment Ohridski”, Bulgaria</span></p>
<p>本文提出下一代视觉编码器应具备图像尺寸无关性和任务驱动性，受到生物视觉系统效率启发。方法上，提出了一种结合多尺度（multi-zoom）自顶向下图像patch提取、基于Transformer的迭代内部状态更新，以及强化学习（GRPO）驱动的注意力策略的视觉编码新范式，并在ImageNet-1K上通过分类任务验证了其可行性。结论表明，该方法为高效、灵活的视觉编码器设计提供了新思路，尤其是在处理大尺寸图像和任务适应性方面展现出潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Vision_encoders_should_be_image_size_agnostic_and_task_driven.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-A-Multimodal-Multitask-Framework-with-Cross-modal-Relation-and-Hierarchical-Interactive-Attention-for-Semantic-Comprehension"><a href="#37-A-Multimodal-Multitask-Framework-with-Cross-modal-Relation-and-Hierarchical-Interactive-Attention-for-Semantic-Comprehension" class="headerlink" title="37. A Multimodal-Multitask Framework with Cross-modal Relation and Hierarchical Interactive Attention for Semantic Comprehension"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/A_Multimodal-Multitask_Framework_with_Cross-modal_Relation_and_Hierarchical_Interactive_Attention_fo.pdf">A Multimodal-Multitask Framework with Cross-modal Relation and Hierarchical Interactive Attention for Semantic Comprehension</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Indian Institute of Technology Indore</span></p>
<p>本文提出了一种新颖的多模态多任务框架MM-ORIENT，通过跨模态关系图（Cross-modal Relation Graphs）和分层交互注意力机制（Hierarchical Interactive Monomodal Attention, HIMA），实现了对图像和文本的联合理解，适用于情感、幽默、讽刺、冒犯和动机等多任务。该方法在特征层减少不同模态的直接交互，降低了噪声传播，并通过多层注意力增强判别性特征，最终在三大公开多模态数据集上取得了优于现有方法的效果，验证了方法的有效性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/A_Multimodal-Multitask_Framework_with_Cross-modal_Relation_and_Hierarchical_Interactive_Attention_for_Semantic_Comprehension.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-EdgeDoc-Hybrid-CNN-Transformer-Model-for-Accurate-Forgery-Detection-and-Localization-in-ID-Documents"><a href="#38-EdgeDoc-Hybrid-CNN-Transformer-Model-for-Accurate-Forgery-Detection-and-Localization-in-ID-Documents" class="headerlink" title="38. EdgeDoc: Hybrid CNN-Transformer Model for Accurate Forgery Detection and Localization in ID Documents"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/EdgeDoc__Hybrid_CNN-Transformer_Model_for_Accurate_Forgery_Detection_and_Localization_in_ID_Document.pdf">EdgeDoc: Hybrid CNN-Transformer Model for Accurate Forgery Detection and Localization in ID Documents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Idiap Research Institute</span></p>
<p>本文提出了EdgeDoc，一种结合轻量级卷积-Transformer混合架构和辅助NoisePrint特征的文档伪造检测与定位方法。方法流程包括利用TruFor管线提取NoisePrint，与原始图像融合后输入卷积-Transformer网络，实现同时的分类和伪造区域定位。实验结果表明，EdgeDoc在FantasyID数据集上表现优于现有基线方法，并在ICCV 2025 DeepID Challenge中取得第三名，验证了其实用性和竞争力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/EdgeDoc_Hybrid_CNN-Transformer_Model_for_Accurate_Forgery_Detection_and_Localization_in_ID_Documents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-An-Investigation-of-Visual-Foundation-Models-Robustness"><a href="#39-An-Investigation-of-Visual-Foundation-Models-Robustness" class="headerlink" title="39. An Investigation of Visual Foundation Models Robustness"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/An_Investigation_of_Visual_Foundation_Models_Robustness.pdf">An Investigation of Visual Foundation Models Robustness</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Queen’s University Belfast</span></p>
<p>本文系统性梳理和分析了视觉基础模型（VFMs）在计算机视觉任务中的鲁棒性问题，涵盖目标检测、图像分类、分割、姿态估计等场景。作者详细探讨了VFMs在应对分布漂移、噪声、空间畸变和对抗攻击时的主要需求，评估了包括对抗检测、输入变换、对抗训练、认证防御、蒸馏和对抗学习等主流防御与训练机制的有效性及其局限性。结论指出，当前防御机制多集中于有限类型攻击，建议在实际部署中需全面评估模型对分布漂移、噪声扰动和对抗扰动的鲁棒性，以提升其在关键应用领域的可靠性和适用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/An_Investigation_of_Visual_Foundation_Models_Robustness.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-Forecast-then-Calibrate-Feature-Caching-as-ODE-for-Efficient-Diffusion-Transformers"><a href="#40-Forecast-then-Calibrate-Feature-Caching-as-ODE-for-Efficient-Diffusion-Transformers" class="headerlink" title="40. Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Forecast_then_Calibrate__Feature_Caching_as_ODE_for_Efficient_Diffusion_Transformers.pdf">Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了FoCa（Forecast-then-Calibrate），将扩散Transformer中的特征缓存过程视为常微分方程（ODE）求解，采用BDF2预测器与Heun校正器结合的训练无关预测-校正框架，有效整合历史特征，实现高稳定性和大步长下的高精度特征预测。实验表明，FoCa在图像合成、视频生成和超分辨率等任务中，在无需额外训练的情况下实现了5.50×~6.45×的近无损加速，并优于现有缓存方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Forecast_then_Calibrate_Feature_Caching_as_ODE_for_Efficient_Diffusion_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-Deep-learning-enabled-virtual-multiplexed-immunostaining-of-label-free-tissue-for-vascular-invasion-assessment"><a href="#41-Deep-learning-enabled-virtual-multiplexed-immunostaining-of-label-free-tissue-for-vascular-invasion-assessment" class="headerlink" title="41. Deep learning-enabled virtual multiplexed immunostaining of label-free tissue for vascular invasion assessment"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Deep_learning-enabled_virtual_multiplexed_immunostaining_of_label-free_tissue_for_vascular_invasion_.pdf">Deep learning-enabled virtual multiplexed immunostaining of label-free tissue for vascular invasion assessment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Los Angeles</span></p>
<p>该论文提出了一种基于深度学习的虚拟多重免疫组化染色方法，利用条件生成对抗网络(cGAN)和数字染色矩阵，将无标记组织的自发荧光显微图像快速转化为H&amp;E、ERG和PanCK等多重染色的明场等效图像，实现甲状腺癌血管侵犯的精准定位与解读。实验结果显示，虚拟染色与真实化学染色高度一致，部分情况下虚拟染色在染色强度和特异性上优于传统方法。该方法能显著提升组织病理诊断效率和准确性，并减少传统染色中因组织丢失等带来的误判风险。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Deep_learning-enabled_virtual_multiplexed_immunostaining_of_label-free_tissue_for_vascular_invasion_assessment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-Through-the-Looking-Glass-A-Dual-Perspective-on-Weakly-Supervised-Few-Shot-Segmentation"><a href="#42-Through-the-Looking-Glass-A-Dual-Perspective-on-Weakly-Supervised-Few-Shot-Segmentation" class="headerlink" title="42. Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Through_the_Looking_Glass__A_Dual_Perspective_on_Weakly-Supervised_Few-Shot_Segmentation.pdf">Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Science and Technology</span></p>
<p>本文提出了一种同源异构网络（TLG）用于弱监督少样本语义分割，通过异构视觉聚合（HA）模块增强支持-查询对的互补性，同时设计异构转运（HT）模块减少语义噪声，并引入异构CLIP（HC）模块提升多模态泛化能力。实验表明，TLG仅用1&#x2F;24参数即可在Pascal-5i和COCO-20i等数据集上超越现有SOTA模型，并首次实现基于相同主干网络的弱监督模型优于全监督模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Through_the_Looking_Glass_A_Dual_Perspective_on_Weakly-Supervised_Few-Shot_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-Ensemble-learning-of-foundation-models-for-precision-oncology"><a href="#43-Ensemble-learning-of-foundation-models-for-precision-oncology" class="headerlink" title="43. Ensemble learning of foundation models for precision oncology"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Ensemble_learning_of_foundation_models_for_precision_oncology.pdf">Ensemble learning of foundation models for precision oncology</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University School of Medicine</span></p>
<p>该论文提出了ELF框架，通过集成五个主流病理基础模型（如GigaPath、CONCH、Virchow2等），采用对比学习和弱监督学习，在53,699张全切片病理图像上进行预训练，生成统一的切片级表示。ELF在疾病分类、分型、分子标志物检测及抗癌及免疫治疗响应预测等115个临床任务中，均显著优于现有基础模型和滑片级模型，表现出更高的准确性和泛化能力，适用于数据有限的临床场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Ensemble_learning_of_foundation_models_for_precision_oncology.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-Wavelet-Space-Super-Resolution-for-Real-Time-Rendering"><a href="#44-Wavelet-Space-Super-Resolution-for-Real-Time-Rendering" class="headerlink" title="44. Wavelet-Space Super-Resolution for Real-Time Rendering"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Wavelet-Space_Super-Resolution_for_Real-Time_Rendering.pdf">Wavelet-Space Super-Resolution for Real-Time Rendering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Thapathali Campus, IOE, Tribhuwan University</span></p>
<p>该论文提出了一种基于小波域特征分解的神经网络超分辨率方法，提高实时渲染管线的重建质量。通过在小波域中分离低、高频细节，并利用空间G-buffer和时序历史帧预测小波系数，结合逆小波合成重建，高效保留纹理细节和结构一致性。实验结果表明，与传统RGB空间回归和现有DFASR框架相比，采用SWT的小波表示显著提升了PSNR和LPIPS指标，能够在现代GPU上实现实时性能，适用于高保真图形渲染。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Wavelet-Space_Super-Resolution_for_Real-Time_Rendering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-Two-flow-Feedback-Multi-scale-Progressive-Generative-Adversarial-Network"><a href="#45-Two-flow-Feedback-Multi-scale-Progressive-Generative-Adversarial-Network" class="headerlink" title="45. Two-flow Feedback Multi-scale Progressive Generative Adversarial Network"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Two-flow_Feedback_Multi-scale_Progressive_Generative_Adversarial_Network.pdf">Two-flow Feedback Multi-scale Progressive Generative Adversarial Network</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Qilu Normal University</span></p>
<p>该论文提出了一种创新的两流反馈多尺度渐进生成对抗网络（MSPG-SEN），融合了动态嵌入注意力机制（DEMA）、全局连接的两流动态残差网络（GCTDRN）、自适应感知-行为反馈环（APFL）以及对抗特征增强模块，显著提升了图像生成质量和多样性，并优化了训练过程和效率。实验表明，该方法在多种公开数据集上实现了高质量、高多样性的图像生成，具备良好的泛化能力与计算效率，尤其在复杂场景和细节保留方面表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Two-flow_Feedback_Multi-scale_Progressive_Generative_Adversarial_Network.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-A-Unified-Voxel-Diffusion-Module-for-Point-Cloud-3D-Object-Detection"><a href="#46-A-Unified-Voxel-Diffusion-Module-for-Point-Cloud-3D-Object-Detection" class="headerlink" title="46. A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/A_Unified_Voxel_Diffusion_Module_for_Point_Cloud_3D_Object_Detection.pdf">A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了一种新颖的体素扩散模块（VDM），结合稀疏3D卷积、子流形稀疏卷积和残差连接，用于在点云三维目标检测中增强体素级空间扩散和特征聚合。通过在序列化前对体素特征进行扩散和细粒度空间信息聚合，VDM可无缝集成到主流Transformer或SSM检测模型中，在Waymo、nuScenes、Argoverse 2和ONCE等多个基准数据集上显著提升检测精度，达到了新的SOTA水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/A_Unified_Voxel_Diffusion_Module_for_Point_Cloud_3D_Object_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-Wavelet-Enhanced-PaDiM-for-Industrial-Anomaly-Detection"><a href="#47-Wavelet-Enhanced-PaDiM-for-Industrial-Anomaly-Detection" class="headerlink" title="47. Wavelet-Enhanced PaDiM for Industrial Anomaly Detection"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Wavelet-Enhanced_PaDiM_for_Industrial_Anomaly_Detection.pdf">Wavelet-Enhanced PaDiM for Industrial Anomaly Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Saint Louis University</span></p>
<p>该论文提出了Wavelet-Enhanced PaDiM (WE-PaDiM)方法，通过在多层CNN特征上应用二维离散小波变换(DWT)，选择特定频率子带系数并进行对齐与拼接，再采用PaDiM的高斯建模，实现了结构化、可解释的特征选择用于工业图像异常检测。实验表明，WE-PaDiM在MVTec AD工业异常数据集上获得了平均99.32%的Image-AUC和92.10%的Pixel-AUC，显示出在检测和定位工业异常方面的高效和强竞争力，并且可根据任务需求优化配置。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Wavelet-Enhanced_PaDiM_for_Industrial_Anomaly_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="48-GLARE-Agentic-Reasoning-for-Legal-Judgment-Prediction"><a href="#48-GLARE-Agentic-Reasoning-for-Legal-Judgment-Prediction" class="headerlink" title="48. GLARE: Agentic Reasoning for Legal Judgment Prediction"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/GLARE__Agentic_Reasoning_for_Legal_Judgment_Prediction.pdf">GLARE: Agentic Reasoning for Legal Judgment Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Gaoling School of Artificial Intelligence, Renmin University of China</span></p>
<p>本文提出了GLARE（Agentic Legal Reasoning Framework），通过三大模块（候选罪名扩展、判例推理演示、法律搜索增强推理）动态获取关键法律知识，辅助大语言模型进行法律判决预测。实验证明该方法在真实法律数据集上显著提升了判决准确率和推理链条的可解释性，特别是在处理低频或混淆罪名时效果突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/GLARE_Agentic_Reasoning_for_Legal_Judgment_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-LLM-Based-Agents-for-Competitive-Landscape-Mapping-in-Drug-Asset-Due-Diligence"><a href="#49-LLM-Based-Agents-for-Competitive-Landscape-Mapping-in-Drug-Asset-Due-Diligence" class="headerlink" title="49. LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/LLM-Based_Agents_for_Competitive_Landscape_Mapping_in_Drug_Asset_Due_Diligence.pdf">LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bioptic.io</span></p>
<p>本文提出了一种基于LLM的多智能体系统，用于药物资产尽职调查中的竞争格局映射。方法包括多阶段REACT风格的Web智能体进行竞争药物发现，并结合LLM-as-judge“Competitor-Validator”过滤假阳性，流程涵盖多模态信息解析、规范化、属性抽取与迭代多步推理。实验在真实生物医药VC尽调备忘录数据集上，召回率和精度均优于OpenAI Deep Research与Perplexity Labs等通用系统，显著提升分析师工作效率（提速约20倍），且在难样本上仍保持稳定表现，系统已落地企业生产环境。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/LLM-Based_Agents_for_Competitive_Landscape_Mapping_in_Drug_Asset_Due_Diligence.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-Boardwalk-Towards-a-Framework-for-Creating-Board-Games-with-LLMs"><a href="#50-Boardwalk-Towards-a-Framework-for-Creating-Board-Games-with-LLMs" class="headerlink" title="50. Boardwalk: Towards a Framework for Creating Board Games with LLMs"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Boardwalk__Towards_a_Framework_for_Creating_Board_Games_with_LLMs.pdf">Boardwalk: Towards a Framework for Creating Board Games with LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universidade Federal do Rio Grande do Sul</span></p>
<p>该论文提出了Boardwalk，一个用于简化抽象棋盘游戏实现的Python API，并系统评估了三种主流大语言模型（Claude, DeepSeek, ChatGPT）在根据自然语言规则生成棋盘游戏代码的能力。实验表明Claude模型表现最佳，55.6%的代码无错误，且大多数错误可快速修复，证明LLM辅助棋盘游戏开发具有可行性和效率提升价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Boardwalk_Towards_a_Framework_for_Creating_Board_Games_with_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-PediatricsMQA-a-Multi-modal-Pediatrics-Question-Answering-Benchmark"><a href="#51-PediatricsMQA-a-Multi-modal-Pediatrics-Question-Answering-Benchmark" class="headerlink" title="51. PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/PediatricsMQA__a_Multi-modal_Pediatrics_Question_Answering_Benchmark.pdf">PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mohammed 6 Polytechnic University</span></p>
<p>本论文提出PediatricsMQA，一个覆盖文本与视觉多模态的儿科医学问答基准，包含3417道文本多选题和2067道视觉多选题，涵盖131个儿科主题和67种影像模态，数据通过手动与自动结合流程从文献、题库等多源构建。实验结果显示该基准对于当前主流LLMs和VLMs极具挑战性，尤其在低龄组表现显著下降，凸显了儿科医学领域AI模型的不足，有助于推动针对儿童医学的AI公平性和专用能力提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/PediatricsMQA_a_Multi-modal_Pediatrics_Question_Answering_Benchmark.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-RL-Is-Neither-a-Panacea-Nor-a-Mirage-Understanding-Supervised-vs-Reinforcement-Learning-Fine-Tuning-for-LLMs"><a href="#52-RL-Is-Neither-a-Panacea-Nor-a-Mirage-Understanding-Supervised-vs-Reinforcement-Learning-Fine-Tuning-for-LLMs" class="headerlink" title="52. RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/RL_Is_Neither_a_Panacea_Nor_a_Mirage__Understanding_Supervised_vs._Reinforcement_Learning_Fine-Tunin.pdf">RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">PolyTechnique Montreal</span></p>
<p>本论文系统分析了大语言模型（LLM）在监督微调（SFT）与强化学习微调（RL-FT）两阶段中的表现与参数变化，采用谱分析方法揭示SFT导致模型表征方向漂移（而非奇异值变化）是泛化能力下降的主因，RL-FT主要通过恢复这些方向来挽回出分布泛化性能。结论表明，RL微调可有效恢复中度SFT遗失的泛化能力，但无法修复极度过拟合导致的损失，且模型核心容量基本保持不变。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/RL_Is_Neither_a_Panacea_Nor_a_Mirage_Understanding_Supervised_vs._Reinforcement_Learning_Fine-Tuning_for_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-Beyond-Interpretability-Exploring-the-Comprehensibility-of-Adaptive-Video-Streaming-through-Large-Language-Models"><a href="#53-Beyond-Interpretability-Exploring-the-Comprehensibility-of-Adaptive-Video-Streaming-through-Large-Language-Models" class="headerlink" title="53. Beyond Interpretability: Exploring the Comprehensibility of Adaptive Video Streaming through Large Language Models"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Beyond_Interpretability__Exploring_the_Comprehensibility_of_Adaptive_Video_Streaming_through_Large_L.pdf">Beyond Interpretability: Exploring the Comprehensibility of Adaptive Video Streaming through Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出ComTree框架，将决策树生成与大语言模型（LLM）结合，用于自适应视频流的码率自适应算法设计。方法上，ComTree首先用特征筛选和教师—学生学习框架生成Rashomon集合，再用多LLM集成进行决策树可理解性评估，最终选出既高性能又高可理解性的算法。实验证明ComTree在多种网络环境下实现了与SOTA持平甚至更优的性能，并大幅提升了算法的工程师可理解性，便于持续优化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Beyond_Interpretability_Exploring_the_Comprehensibility_of_Adaptive_Video_Streaming_through_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-CETVEL-A-Unified-Benchmark-for-Evaluating-Language-Understanding-Generation-and-Cultural-Capacity-of-LLMs-for-Turkish"><a href="#54-CETVEL-A-Unified-Benchmark-for-Evaluating-Language-Understanding-Generation-and-Cultural-Capacity-of-LLMs-for-Turkish" class="headerlink" title="54. CETVEL: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Cetvel__A_Unified_Benchmark_for_Evaluating_Language_Understanding,_Generation_and_Cultural_Capacity_.pdf">CETVEL: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KUIS AI Center</span></p>
<p>本文提出了CETVEL，一个专为评估土耳其语大语言模型（LLM）设计的统一基准，涵盖23项任务，兼顾任务多样性和土耳其文化、语言特性，包括文本分类、问答、语法纠错、翻译等。实验结果显示，土耳其语定制模型整体性能不及多语言或通用预训练模型（如Llama 3），语法纠错、翻译和抽取式问答在模型区分上最具信息性，未来需优化针对土耳其语的指令微调策略。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/CETVEL_A_Unified_Benchmark_for_Evaluating_Language_Understanding_Generation_and_Cultural_Capacity_of_LLMs_for_Turkish.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-LLM-GUARD-Large-Language-Model-Based-Detection-and-Repair-of-Bugs-and-Security-Vulnerabilities-in-C-and-Python"><a href="#55-LLM-GUARD-Large-Language-Model-Based-Detection-and-Repair-of-Bugs-and-Security-Vulnerabilities-in-C-and-Python" class="headerlink" title="55. LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/LLM-GUARD__Large_Language_Model-Based_Detection_and_Repair_of_Bugs_and_Security_Vulnerabilities_in_C.pdf">LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Texas A&amp;M University - Central Texas</span></p>
<p>本论文系统评估了ChatGPT-4、Claude 3、LLaMA 4三种主流大语言模型在C++和Python代码中自动检测和修复基础编程错误、经典安全漏洞和实际生产级Bug的能力，采用多阶段上下文感知提示和细致评分标准进行实验。结果发现，所有LLM在基础语法和语义错误检测上表现优异，适用于教育与初步代码审计，但在复杂安全漏洞和大规模生产代码分析能力有限，ChatGPT-4和Claude 3在上下文推理和安全分析方面优于LLaMA 4。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/LLM-GUARD_Large_Language_Model-Based_Detection_and_Repair_of_Bugs_and_Security_Vulnerabilities_in_C%2B%2B_and_Python.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-RoMedQA-The-First-Benchmark-for-Romanian-Medical-Question-Answering"><a href="#56-RoMedQA-The-First-Benchmark-for-Romanian-Medical-Question-Answering" class="headerlink" title="56. RoMedQA: The First Benchmark for Romanian Medical Question Answering"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/RoMedQA__The_First_Benchmark_for_Romanian_Medical_Question_Answering.pdf">RoMedQA: The First Benchmark for Romanian Medical Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Bucharest</span></p>
<p>该论文提出RoMedQA，首个罗马尼亚医学问答数据集，涵盖1,011名癌症患者的102,646个高质量QA对，并由医学专家耗时2,100小时人工标注。作者系统评估了四种主流大语言模型（包括罗马尼亚语专用、长文本支持和生物医学预训练模型），在零样本推理和LoRA高效微调两种模式下进行对比实验，发现微调后的模型在医学问答任务上显著优于零样本，且最佳模型F1仅达0.667，说明该任务具挑战性。结论表明，领域和语言专门化微调对提升医学问答准确性至关重要，RoMedQA填补了罗马尼亚医学NLP资源空缺。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/RoMedQA_The_First_Benchmark_for_Romanian_Medical_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-MizanQA-Benchmarking-Large-Language-Models-on-Moroccan-Legal-Question-Answering"><a href="#57-MizanQA-Benchmarking-Large-Language-Models-on-Moroccan-Legal-Question-Answering" class="headerlink" title="57. MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/MizanQA__Benchmarking_Large_Language_Models_on_Moroccan_Legal_Question_Answering.pdf">MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mohammed 6 Polytechnic University</span></p>
<p>本文提出了MizanQA，这是首个针对摩洛哥法律领域多项选择题问答的LLM评测基准，涵盖现代标准阿拉伯语、伊斯兰法、摩洛哥习惯法和法语法律影响。作者手工和自动混合构建了包含1700余题的高质量数据集，并设计了多答案、多类别、置信度校准等新评测指标，系统评估多语种与阿拉伯语LLM的法律推理能力。实验显示主流LLM在文化特定术语、复杂推理、多答案匹配等方面有较大短板，强调了低资源语境下发展本地化、领域特定法律AI评测与模型的必要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/MizanQA_Benchmarking_Large_Language_Models_on_Moroccan_Legal_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-Confusion-is-the-Final-Barrier-Rethinking-Jailbreak-Evaluation-and-Investigating-the-Real-Misuse-Threat-of-LLMs"><a href="#58-Confusion-is-the-Final-Barrier-Rethinking-Jailbreak-Evaluation-and-Investigating-the-Real-Misuse-Threat-of-LLMs" class="headerlink" title="58. Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Confusion_is_the_Final_Barrier__Rethinking_Jailbreak_Evaluation_and_Investigating_the_Real_Misuse_Th.pdf">Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Computing Technology, Chinese Academy of Sciences</span></p>
<p>本文提出VENOM框架，通过构建知识密集型问答和反事实任务，系统评估大模型在犯罪知识、任务规划及有害性判断等方面的真实能力，揭示现有LLM安全评估方法高估了模型对真实有害知识的掌握。结论表明，主流LLM的越狱成功率与实际有害知识掌握并不匹配，且判别模块过度依赖有害语言风格，无法准确识别真实威胁。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Confusion_is_the_Final_Barrier_Rethinking_Jailbreak_Evaluation_and_Investigating_the_Real_Misuse_Threat_of_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-LLMSymGuard-A-Symbolic-Safety-Guardrail-Framework-Leveraging-Interpretable-Jailbreak-Concepts"><a href="#59-LLMSymGuard-A-Symbolic-Safety-Guardrail-Framework-Leveraging-Interpretable-Jailbreak-Concepts" class="headerlink" title="59. LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/LLMSymGuard__A_Symbolic_Safety_Guardrail_Framework_Leveraging_Interpretable_Jailbreak_Concepts.pdf">LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Université Paris-Saclay</span></p>
<p>LLMSymGuard提出了一种利用稀疏自编码器（SAE）从大型语言模型（LLM）内部提取可解释的jailbreak相关概念的方法，通过对SAE激活特征进行分类和逻辑规则组合，构建具备透明度和鲁棒性的符号安全防护机制。实验证明，该方法可在不损失模型能力、无需额外微调的前提下，显著提升对有害内容的拦截率，并保持较低的误杀率，优于主流安全微调基线。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/LLMSymGuard_A_Symbolic_Safety_Guardrail_Framework_Leveraging_Interpretable_Jailbreak_Concepts.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-OwkinZero-Accelerating-Biological-Discovery-with-AI"><a href="#60-OwkinZero-Accelerating-Biological-Discovery-with-AI" class="headerlink" title="60. OwkinZero: Accelerating Biological Discovery with AI"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/OwkinZero__Accelerating_Biological_Discovery_with_AI.pdf">OwkinZero: Accelerating Biological Discovery with AI</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Owkin</span></p>
<p>本论文提出了OwkinZero框架，通过构建8个涵盖30万高质量生物学问答对的全新基准数据集，针对药物发现中的关键生物学推理任务，采用可验证奖励强化学习（RLVR）方法对开放大语言模型进行后训练。结果表明，经过RL后训练的8-32B参数量OwkinZero模型在生物学推理基准上的表现显著优于更大规模的商用LLM，并展现出跨任务泛化能力，但多任务训练时推理一致性有所降低，需进一步对齐。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/OwkinZero_Accelerating_Biological_Discovery_with_AI.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-Structuring-GUI-Elements-through-Vision-Language-Models-Towards-Action-Space-Generation"><a href="#61-Structuring-GUI-Elements-through-Vision-Language-Models-Towards-Action-Space-Generation" class="headerlink" title="61. Structuring GUI Elements through Vision Language Models: Towards Action Space Generation"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Structuring_GUI_Elements_through_Vision_Language_Models__Towards_Action_Space_Generation.pdf">Structuring GUI Elements through Vision Language Models: Towards Action Space Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了一种针对多模态大语言模型（MLLMs）用于图形用户界面（GUI）元素结构化的IoU增强最大似然（IAML）训练范式，通过IoU度量的坐标采样实现数据增强，并结合奖励机制缓解传统最大似然估计中的暴露偏差问题。实验结果表明，IAML方法在多元素动作空间生成和单元素定位任务上均超越了传统MLE和随机扰动方法，尤其在低资源场景下表现更优。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Structuring_GUI_Elements_through_Vision_Language_Models_Towards_Action_Space_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-On-the-Evolution-of-Federated-Post-Training-Large-Language-Models-A-Model-Accessibility-View"><a href="#62-On-the-Evolution-of-Federated-Post-Training-Large-Language-Models-A-Model-Accessibility-View" class="headerlink" title="62. On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/On_the_Evolution_of_Federated_Post-Training_Large_Language_Models__A_Model_Accessibility_View.pdf">On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen University</span></p>
<p>本论文系统梳理了联邦学习下大语言模型（LLMs）后训练的最新进展，提出了一个双轴分类法：一是模型可访问性（白盒、灰盒、黑盒），二是参数高效性，并详细分析了每种优化方案，包括LoRA、Prompt Tuning等关键技术。结论指出，随着模型访问受限，黑盒推理API范式和高效参数优化将成为未来主流，强调了推理黑盒场景下的隐私和安全性挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/On_the_Evolution_of_Federated_Post-Training_Large_Language_Models_A_Model_Accessibility_View.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-LLM-Assisted-Semantic-Alignment-and-Integration-in-Collaborative-Model-Based-Systems-Engineering-Using-SysML-v2"><a href="#63-LLM-Assisted-Semantic-Alignment-and-Integration-in-Collaborative-Model-Based-Systems-Engineering-Using-SysML-v2" class="headerlink" title="63. LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/LLM-Assisted_Semantic_Alignment_and_Integration_in_Collaborative_Model-Based_Systems_Engineering_Usi.pdf">LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technische Universität Ilmenau</span></p>
<p>本文提出了一种基于GPT类大语言模型（LLM）和SysML v2结构的分阶段、结构化、提示驱动方法，用于跨组织MBSE模型的语义对齐与集成。方法包括模型抽取、语义匹配、验证、结构化导出等流程，利用SysML v2的alias、import和元数据扩展实现模型结构独立性和可追溯软对齐，并在测量系统协作场景下展示了可行性。结论表明，该方法提升了对齐效率、语义透明性和用户交互性，但在深层语义一致性和自动化方面仍需进一步研究。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/LLM-Assisted_Semantic_Alignment_and_Integration_in_Collaborative_Model-Based_Systems_Engineering_Using_SysML_v2.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-Graph-RAG-as-Human-Choice-Model-Building-a-Data-Driven-Mobility-Agent-with-Preference-Chain"><a href="#64-Graph-RAG-as-Human-Choice-Model-Building-a-Data-Driven-Mobility-Agent-with-Preference-Chain" class="headerlink" title="64. Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Graph_RAG_as_Human_Choice_Model__Building_a_Data-Driven_Mobility_Agent_with_Preference_Chain.pdf">Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China University of Technology</span></p>
<p>本论文提出了一种Preference Chain方法，将图检索增强生成（Graph RAG）与大语言模型（LLM）结合，用于在数据稀缺环境下模拟交通系统中的人类行为。该方法通过构建BDI行为图、相似性搜索、概率建模和LLM偏好重塑四步，实现了基于小样本数据的个体行为模拟，并在Replica出行数据集上验证了其在出行方式模拟和动态交通预测方面优于传统LLM和机器学习方法，特别适合新兴城市等数据稀缺场景，结论表明该方法提升了模拟的准确性和泛化能力，但在推理速度和连续性建模方面仍有局限。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Graph_RAG_as_Human_Choice_Model_Building_a_Data-Driven_Mobility_Agent_with_Preference_Chain.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-Towards-Recommending-Usability-Improvements-with-Multimodal-Large-Language-Models"><a href="#65-Towards-Recommending-Usability-Improvements-with-Multimodal-Large-Language-Models" class="headerlink" title="65. Towards Recommending Usability Improvements with Multimodal Large Language Models"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Towards_Recommending_Usability_Improvements_with_Multimodal_Large_Language_Models.pdf">Towards Recommending Usability Improvements with Multimodal Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Graz University of Technology</span></p>
<p>该论文将可用性评估问题形式化为推荐任务，利用多模态大语言模型（LLM）结合应用描述、用户角色、评估标准及界面截图，自动识别并按严重性排序可用性问题，并生成针对性的改进建议。通过与专业可用性专家的评估对比，证明LLM在结构化、二元评分等场景下能较好辅助发现和优先排序关键可用性问题，但尚不能完全取代专家分析，可为资源有限的团队提供高效辅助。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Towards_Recommending_Usability_Improvements_with_Multimodal_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-Retrieval-Enhanced-Feedback-via-In-context-Neural-Error-book"><a href="#66-Retrieval-Enhanced-Feedback-via-In-context-Neural-Error-book" class="headerlink" title="66. Retrieval Enhanced Feedback via In-context Neural Error-book"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Retrieval_Enhanced_Feedback_via_In-context_Neural_Error-book.pdf">Retrieval Enhanced Feedback via In-context Neural Error-book</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">POSTECH</span></p>
<p>该论文提出了一种名为REFINE的教师-学生框架，通过构建结构化神经错误本（Error-book），利用三种系统化反馈（Feed-Target, Feed-Check, Feed-Path）对多模态大模型进行有针对性的纠错与推理增强。实验表明，REFINE在多模态推理准确性和推理效率上显著优于现有方法，同时具备良好的泛化能力和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Retrieval_Enhanced_Feedback_via_In-context_Neural_Error-book.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications"><a href="#67-AgentScope-1-0-A-Developer-Centric-Framework-for-Building-Agentic-Applications" class="headerlink" title="67. AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/AgentScope_1.0__A_Developer-Centric_Framework_for_Building_Agentic_Applications.pdf">AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Alibaba Group</span></p>
<p>该论文提出AgentScope 1.0，一个以开发者为中心、基于ReAct范式的LLM智能体开发框架。方法上，AgentScope抽象了消息、模型、内存和工具等基础组件，提供统一接口，支持异步与并行工具调用、多智能体协作、可视化开发与安全沙箱部署，并内建多种典型智能体（如深度研究、浏览器操作、Meta规划）。结论认为，AgentScope能高效、灵活地支撑工具增强型LLM智能体在真实环境下的开发与部署，提升扩展性和开发体验。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/AgentScope_1.0_A_Developer-Centric_Framework_for_Building_Agentic_Applications.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-MCPVerse-An-Expansive-Real-World-Benchmark-for-Agentic-Tool-Use"><a href="#68-MCPVerse-An-Expansive-Real-World-Benchmark-for-Agentic-Tool-Use" class="headerlink" title="68. MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/MCPVerse__An_Expansive,_Real-World_Benchmark_for_Agentic_Tool_Use.pdf">MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">SenseTime Research</span></p>
<p>本文提出了MCPVerse基准，用于大规模、真实场景下评估大语言模型（LLMs）的工具调用与推理能力。MCPVerse整合了550+真实可执行工具，设计了覆盖多复杂度和时效性的250个任务，采用混合结果导向评价体系，通过标准化接口和多种评测模式，全面测试了主流LLM在大规模工具集下的表现和局限。实验发现，拓展的工具集对具备agentic能力的模型（如Claude-4-Sonnet）有益，能提升其复杂问题解决能力。结论指出，MCPVerse弥补了现有基准在真实性和规模上的不足，是推动LLM工具使用能力发展的重要基准。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/MCPVerse_An_Expansive_Real-World_Benchmark_for_Agentic_Tool_Use.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-MedOmni-45°-A-Safety–Performance-Benchmark-for-Reasoning-Oriented-LLMs-in-Medicine"><a href="#69-MedOmni-45°-A-Safety–Performance-Benchmark-for-Reasoning-Oriented-LLMs-in-Medicine" class="headerlink" title="69. MedOmni-45°: A Safety–Performance Benchmark for Reasoning-Oriented LLMs in Medicine"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/MedOmni-45%C2%B0__A_Safety-Performance_Benchmark_for_Reasoning-Oriented_LLMs_in_Medicine.pdf">MedOmni-45°: A Safety–Performance Benchmark for Reasoning-Oriented LLMs in Medicine</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">East China Normal University</span></p>
<p>该论文提出了MedOmni-45°，一个面向医学领域推理型大语言模型（LLM）的安全-性能权衡评测基准。方法包括构建涵盖六大临床专科和三类推理任务的1804道多选题，并系统性地添加7类操纵性提示，组合出2.7万条输入，基于三重指标（准确率、推理链忠实度、反拍马屁性）对7个主流LLM进行系统性评测，并以45°可视化图揭示安全与性能间的权衡。结论显示所有模型均存在安全与性能之间的普遍性权衡，最优模型QwQ-32B在安全性和准确率上接近理想线，但无模型能完全兼顾两者，说明当前医学推理型LLM在操纵性提示下仍存在关键安全短板，MedOmni-45°为未来模型对齐与医疗AI安全研究提供了有力工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/MedOmni-45%C2%B0_A_Safety%E2%80%93Performance_Benchmark_for_Reasoning-Oriented_LLMs_in_Medicine.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-SPECVLM-Enhancing-Speculative-Decoding-of-Video-LLMs-via-Verifier-Guided-Token-Pruning"><a href="#70-SPECVLM-Enhancing-Speculative-Decoding-of-Video-LLMs-via-Verifier-Guided-Token-Pruning" class="headerlink" title="70. SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/SpecVLM__Enhancing_Speculative_Decoding_of_Video_LLMs_via_Verifier-Guided_Token_Pruning.pdf">SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>SPECVLM提出了一种针对视频大语言模型（Vid-LLMs）的无训练推理加速框架，通过引入基于Verifier（目标模型）注意力引导的两阶段视频Token剪枝，大幅减少冗余视频Token输入，有效提升推理速度。实验结果表明，SPECVLM能够在保持生成质量无损的前提下，实现最高2.68倍的视频LLM解码加速，并具备良好的通用性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/SPECVLM_Enhancing_Speculative_Decoding_of_Video_LLMs_via_Verifier-Guided_Token_Pruning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-Seeing-is-Believing-Emotion-Aware-Audio-Visual-Language-Modeling-for-Expressive-Speech-Generation"><a href="#71-Seeing-is-Believing-Emotion-Aware-Audio-Visual-Language-Modeling-for-Expressive-Speech-Generation" class="headerlink" title="71. Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for Expressive Speech Generation"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Seeing_is_Believing__Emotion-Aware_Audio-Visual_Language_Modeling_for_Expressive_Speech_Generation.pdf">Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for Expressive Speech Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Johns Hopkins University</span></p>
<p>本文提出了一种音频视觉语言模型（AVLM），通过在预训练的表达式语音模型中集成全脸视觉线索，实现了情感感知的多模态语音生成。方法上，系统性探索了多种视觉编码器与模态融合策略（如Q-Former Prefix），并在情感识别及表达式对话任务上微调，显著优于仅使用语音的基线（如情感识别F1提升5分以上）。结论为：AVLM有效利用视觉信息提升语音生成的情感表达与准确性，为多模态对话系统奠定基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Seeing_is_Believing_Emotion-Aware_Audio-Visual_Language_Modeling_for_Expressive_Speech_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-The-Fools-are-Certain-the-Wise-are-Doubtful-Exploring-LLM-Confidence-in-Code-Completion"><a href="#72-The-Fools-are-Certain-the-Wise-are-Doubtful-Exploring-LLM-Confidence-in-Code-Completion" class="headerlink" title="72. The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/The_Fools_are_Certain;_the_Wise_are_Doubtful__Exploring_LLM_Confidence_in_Code_Completion.pdf">The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Athens University of Economics and Business</span></p>
<p>本文提出了一种可复现的方法和开源工具包，用于利用多种大型语言模型（LLM）评估代码补全任务中的模型信心，通过计算不同编程语言、模型和数据集的代码困惑度（perplexity）。实验发现，强类型语言困惑度较低，脚本语言较高，且困惑度主要受模型影响而非数据集影响，结论强调困惑度可作为代码生成任务中LLM可信度和功能正确性的便捷度量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/The_Fools_are_Certain_the_Wise_are_Doubtful_Exploring_LLM_Confidence_in_Code_Completion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-AgentFly-Fine-tuning-LLM-Agents-without-Fine-tuning-LLMs"><a href="#73-AgentFly-Fine-tuning-LLM-Agents-without-Fine-tuning-LLMs" class="headerlink" title="73. AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/AgentFly__Fine-tuning_LLM_Agents_without_Fine-tuning_LLMs.pdf">AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AI Centre, UCL</span></p>
<p>本文提出了一种无需微调底层大模型的LLM Agent自适应学习新范式AgentFly，通过基于记忆的在线强化学习实现持续适应。AgentFly将任务建模为记忆增强型马尔可夫决策过程（M-MDP），利用神经案例选择策略从外部记忆中检索历史经验，结合规划–执行架构实现经验积累与决策改进。实验表明，AgentFly在GAIA、DeepResearcher等多个基准上超越现有方法，具备高效可扩展的持续学习能力，显著提升了泛化与实时适应性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/AgentFly_Fine-tuning_LLM_Agents_without_Fine-tuning_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-CommonKV-Compressing-KV-Cache-with-Cross-layer-Parameter-Sharing"><a href="#74-CommonKV-Compressing-KV-Cache-with-Cross-layer-Parameter-Sharing" class="headerlink" title="74. CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/CommonKV__Compressing_KV_Cache_with_Cross-layer_Parameter_Sharing.pdf">CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology</span></p>
<p>本文提出了一种针对大型语言模型KV缓存压缩的无训练方法CommonKV，通过对相邻层的参数进行SVD分解实现跨层权重共享，从而获得更一致的潜在KV缓存，并设计了基于余弦相似度的自适应压缩预算分配策略，有效避免性能损失。实验结果表明，CommonKV在多种主流模型和长文本任务上实现最高98%压缩率且性能损失极小，并与其他压缩和淘汰方法高度兼容。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/CommonKV_Compressing_KV_Cache_with_Cross-layer_Parameter_Sharing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-Bridging-the-Gap-in-Ophthalmic-AI-MM-Retinal-Reason-Dataset-and-OphthaReason-Model-toward-Dynamic-Multimodal-Reasoning"><a href="#75-Bridging-the-Gap-in-Ophthalmic-AI-MM-Retinal-Reason-Dataset-and-OphthaReason-Model-toward-Dynamic-Multimodal-Reasoning" class="headerlink" title="75. Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Bridging_the_Gap_in_Ophthalmic_AI__MM-Retinal-Reason_Dataset_and_OphthaReason_Model_toward_Dynamic_M.pdf">Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southeast University</span></p>
<p>本文提出了MM-Retinal-Reason，这是首个覆盖基础与复杂推理的眼科多模态数据集，整合了多种医学影像及临床信息，并具备全链路推理轨迹。基于该数据集，作者开发了OphthaReason模型，通过三阶段流程（多模态对齐、链式思维监督微调和强化学习）结合创新的不确定性动态思维机制（UADT），实现了对不同推理难度的自适应探索。实验表明，OphthaReason在基础与复杂推理任务上均大幅超越现有通用及医学多模态大模型，提升最高达24.92%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Bridging_the_Gap_in_Ophthalmic_AI_MM-Retinal-Reason_Dataset_and_OphthaReason_Model_toward_Dynamic_Multimodal_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-Extending-FKG-in-Towards-a-Food-Claim-Traceability-Network"><a href="#76-Extending-FKG-in-Towards-a-Food-Claim-Traceability-Network" class="headerlink" title="76. Extending FKG.in: Towards a Food Claim Traceability Network"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Extending_FKG.in__Towards_a_Food_Claim_Traceability_Network.pdf">Extending FKG.in: Towards a Food Claim Traceability Network</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ashoka University</span></p>
<p>该论文提出了Food Claim-Traceability Network（FCN），作为印度食品知识图谱FKG.in的扩展，建立了用于结构化、可追溯地表示和验证食品相关主张的本体及数据处理流程。方法包括本体设计、LLM辅助信息抽取、验证源关联和人工审核，流程在Reddit数据集上实现了半自动化主张提取与验证。结论显示，FCN可提升食品主张的透明度和可解释性，为食品虚假信息检测和公众健康干预奠定基础，并具备全球推广潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Extending_FKG.in_Towards_a_Food_Claim_Traceability_Network.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-CYCLE-INSTRUCT-Fully-Seed-Free-Instruction-Tuning-via-Dual-Self-Training-and-Cycle-Consistency"><a href="#77-CYCLE-INSTRUCT-Fully-Seed-Free-Instruction-Tuning-via-Dual-Self-Training-and-Cycle-Consistency" class="headerlink" title="77. CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual Self-Training and Cycle Consistency"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/CYCLE-INSTRUCT__Fully_Seed-Free_Instruction_Tuning_via_Dual_Self-Training_and_Cycle_Consistency.pdf">CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual Self-Training and Cycle Consistency</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出CYCLE-INSTRUCT框架，通过双自训练环路和循环一致性机制，实现了无需任何人工标注种子数据或外部教师模型的指令微调。方法仅利用原始未标注文本，自动分割生成问答，双模型互相监督重建原文，生成高质量指令-响应数据。实验结果显示，CYCLE-INSTRUCT在四类数据集上性能超越传统种子驱动的回译方法，并可达到强监督模型的水平，显著提升了LLM自动化对齐能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/CYCLE-INSTRUCT_Fully_Seed-Free_Instruction_Tuning_via_Dual_Self-Training_and_Cycle_Consistency.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-Cooperative-Design-Optimization-through-Natural-Language-Interaction"><a href="#78-Cooperative-Design-Optimization-through-Natural-Language-Interaction" class="headerlink" title="78. Cooperative Design Optimization through Natural Language Interaction"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Cooperative_Design_Optimization_through_Natural_Language_Interaction.pdf">Cooperative Design Optimization through Natural Language Interaction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">OMRON SINIC X Corporation</span></p>
<p>本文提出了一种结合大语言模型（LLM）与贝叶斯优化（BO）的协同设计优化框架，实现设计师可通过自然语言与优化系统互动，灵活干预设计参数的探索过程。用户实验表明，该方法在保持优化性能的同时，显著提升了用户的主观能动性并降低了认知负担，且与现有协同方法相比具备更高的易用性和灵活性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Cooperative_Design_Optimization_through_Natural_Language_Interaction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-Generative-Foundation-Model-for-Structured-and-Unstructured-Electronic-Health-Records"><a href="#79-Generative-Foundation-Model-for-Structured-and-Unstructured-Electronic-Health-Records" class="headerlink" title="79. Generative Foundation Model for Structured and Unstructured Electronic Health Records"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Generative_Foundation_Model_for_Structured_and_Unstructured_Electronic_Health_Records.pdf">Generative Foundation Model for Structured and Unstructured Electronic Health Records</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Pittsburgh</span></p>
<p>该论文提出Generative Deep Patient (GDP)，一种融合结构化EHR时间序列（通过CNN-Transformer编码器）与非结构化文本（通过BioClinicalBERT和LLaMA-based解码器，采用跨模态注意力）的多模态基础模型。GDP采用两阶段训练：首先进行生成式预训练（包括生成临床叙述、掩码特征预测和下一个时间步预测），随后多任务微调以完成疾病预测和再入院风险等任务。实验表明，GDP在心衰（AUROC&#x3D;0.923）、2型糖尿病（AUROC&#x3D;0.817）和30天再入院（AUROC&#x3D;0.627）等临床预测显著优于现有方法，在生成高质量临床文本方面也超过主流医疗LLM，提升了预测准确性并减轻医生文档负担。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Generative_Foundation_Model_for_Structured_and_Unstructured_Electronic_Health_Records.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-OpenWHO-A-Document-Level-Parallel-Corpus-for-Health-Translation-in-Low-Resource-Languages"><a href="#80-OpenWHO-A-Document-Level-Parallel-Corpus-for-Health-Translation-in-Low-Resource-Languages" class="headerlink" title="80. OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/OpenWHO__A_Document-Level_Parallel_Corpus_for_Health_Translation_in_Low-Resource_Languages.pdf">OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Melbourne</span></p>
<p>本论文提出OpenWHO语料库，这是一个基于世界卫生组织e-learning平台、涵盖20余种语言（含9种低资源语言）的健康领域文档级平行语料库。作者利用该数据集系统评估了现代大语言模型（如Gemini 2.5 Flash）与传统机器翻译模型（如NLLB-54B）在低资源健康翻译中的表现，结果显示LLMs在文档级上下文下显著优于传统模型（Gemini 2.5 Flash在低资源测试集上比NLLB-54B提升4.79 ChrF分），并且文档级翻译在健康等专业领域的优势尤为突出。结论是：文档级上下文和最强LLM能够提升低资源专业领域翻译质量，OpenWHO语料库为相关研究提供了新基准。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/OpenWHO_A_Document-Level_Parallel_Corpus_for_Health_Translation_in_Low-Resource_Languages.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-Spacetime-GR-A-Spacetime-Aware-Generative-Model-for-Large-Scale-Online-POI-Recommendation"><a href="#81-Spacetime-GR-A-Spacetime-Aware-Generative-Model-for-Large-Scale-Online-POI-Recommendation" class="headerlink" title="81. Spacetime-GR: A Spacetime-Aware Generative Model for Large Scale Online POI Recommendation"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Spacetime-GR__A_Spacetime-Aware_Generative_Model_for_Large_Scale_Online_POI_Recommendation.pdf">Spacetime-GR: A Spacetime-Aware Generative Model for Large Scale Online POI Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AMAP, Alibaba Group</span></p>
<p>本文提出Spacetime-GR，一种面向大规模在线兴趣点（POI）推荐的时空感知生成模型。方法结合地理分层POI索引、时空编码模块与多模态POI嵌入，基于Llama 2架构，通过预训练和多种后训练策略（嵌入特征、排序分数与候选POI生成）支持多种下游应用。实验显示其在工业及公开数据集上显著提升推荐准确性和排序质量，已成功部署于覆盖数亿用户和POI的在线系统。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Spacetime-GR_A_Spacetime-Aware_Generative_Model_for_Large_Scale_Online_POI_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-From-Indirect-Object-Identification-to-Syllogisms-Exploring-Binary-Mechanisms-in-Transformer-Circuits"><a href="#82-From-Indirect-Object-Identification-to-Syllogisms-Exploring-Binary-Mechanisms-in-Transformer-Circuits" class="headerlink" title="82. From Indirect Object Identification to Syllogisms: Exploring Binary Mechanisms in Transformer Circuits"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/From_Indirect_Object_Identification_to_Syllogisms__Exploring_Binary_Mechanisms_in_Transformer_Circui.pdf">From Indirect Object Identification to Syllogisms: Exploring Binary Mechanisms in Transformer Circuits</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Los Angeles</span></p>
<p>本文采用机械可解释性技术（如Path Patching和Logit Lens）系统分析GPT-2小模型在处理三类二元逻辑推理任务（简单、反向、复杂三段论）时的内部机制。研究发现，模型通过特定的注意力头和MLP模块实现真值的复制和否定，构建了高忠实度的子电路，揭示了逻辑推理和二元机制在语言模型中的具体实现方式。结论表明GPT-2在二元逻辑任务中表现出可解释的电路结构，并且这些机制在多种模型间具有一定可迁移性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/From_Indirect_Object_Identification_to_Syllogisms_Exploring_Binary_Mechanisms_in_Transformer_Circuits.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-CEQuest-Benchmarking-Large-Language-Models-for-Construction-Estimation"><a href="#83-CEQuest-Benchmarking-Large-Language-Models-for-Construction-Estimation" class="headerlink" title="83. CEQuest: Benchmarking Large Language Models for Construction Estimation"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/CEQuest__Benchmarking_Large_Language_Models_for_Construction_Estimation.pdf">CEQuest: Benchmarking Large Language Models for Construction Estimation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Florida International University</span></p>
<p>本论文提出了CEQuest，一个专为建筑领域设计的基准数据集，用于评估大型语言模型（LLMs）在施工图纸解读和估算任务上的表现。作者系统测试了五种主流LLM（Gemma 3, Phi4, LLaVA, Llama 3.3, GPT-4.1），比较了它们在准确率、执行时间和模型规模上的性能，发现当前LLM在建筑领域任务上仍有很大提升空间。结论指出：需融合领域知识以提升LLM在专业任务的准确性，CEQuest将开源以推动领域专用LLM的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/CEQuest_Benchmarking_Large_Language_Models_for_Construction_Estimation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-Integrating-Time-Series-into-LLMs-via-Multi-layer-Steerable-Embedding-Fusion-for-Enhanced-Forecasting"><a href="#84-Integrating-Time-Series-into-LLMs-via-Multi-layer-Steerable-Embedding-Fusion-for-Enhanced-Forecasting" class="headerlink" title="84. Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Integrating_Time_Series_into_LLMs_via_Multi-layer_Steerable_Embedding_Fusion_for_Enhanced_Forecastin.pdf">Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-Sen University</span></p>
<p>本文提出了多层可控嵌入融合（MSEF）框架，将时间序列基础模型提取的语义嵌入通过分层可学习的引导向量动态融合到大型语言模型（LLM）各层，实现时序信息的深度整合，并仅训练引导向量与输出层以保证高效。实验在七个公开基准数据集上表明，MSEF在少样本时序预测任务上较主流方法平均MSE提升31.8%，显著提升LLM对时序数据的理解和预测能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Integrating_Time_Series_into_LLMs_via_Multi-layer_Steerable_Embedding_Fusion_for_Enhanced_Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-MMAPG-A-Training-Free-Framework-for-Multimodal-Multi-hop-Question-Answering-via-Adaptive-Planning-Graphs"><a href="#85-MMAPG-A-Training-Free-Framework-for-Multimodal-Multi-hop-Question-Answering-via-Adaptive-Planning-Graphs" class="headerlink" title="85. MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/MMAPG__A_Training-Free_Framework_for_Multimodal_Multi-hop_Question_Answering_via_Adaptive_Planning_G.pdf">MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of New South Wales</span></p>
<p>本文提出MMAPG框架，通过自适应规划图整合规划、检索和推理模块，实现了无需训练的多模态多跳问答。该方法结合LLM动态规划推理路径，并采用模块化、模态自适应检索策略，能灵活处理文本和图像等多源信息。实验表明，MMAPG在MultimodalQA和WebQA数据集上，无需额外训练即可达到或优于主流有监督模型，具备较强的灵活性和扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/MMAPG_A_Training-Free_Framework_for_Multimodal_Multi-hop_Question_Answering_via_Adaptive_Planning_Graphs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="86-Enhanced-NIRMAL-Optimizer-With-Damped-Nesterov-Acceleration-A-Comparative-Analysis"><a href="#86-Enhanced-NIRMAL-Optimizer-With-Damped-Nesterov-Acceleration-A-Comparative-Analysis" class="headerlink" title="86. Enhanced NIRMAL Optimizer With Damped Nesterov Acceleration: A Comparative Analysis"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Enhanced_NIRMAL_Optimizer_With_Damped_Nesterov_Acceleration__A_Comparative_Analysis.pdf">Enhanced NIRMAL Optimizer With Damped Nesterov Acceleration: A Comparative Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ThinkAI - A Machine Learning Community</span></p>
<p>该论文提出了Enhanced NIRMAL优化器，通过集成(α, r)-damped Nesterov加速机制，提升了原NIRMAL优化器的收敛稳定性，并保留了其基于棋局的多策略融合（梯度下降、动量、随机扰动、自适应学习率和非线性变换）。实验在MNIST、FashionMNIST、CIFAR-10和CIFAR-100数据集上，使用定制CNN结构，与Adam、SGD with Momentum、Nesterov及原NIRMAL进行对比。结果显示Enhanced NIRMAL在复杂数据集（如CIFAR-100）上表现出更优的稳定性和泛化能力，特别是在测试损失和准确率方面优于原NIRMAL，并接近SGD with Momentum。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Enhanced_NIRMAL_Optimizer_With_Damped_Nesterov_Acceleration_A_Comparative_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-NeuroKoop-Neural-Koopman-Fusion-of-Structural–Functional-Connectomes-for-Identifying-Prenatal-Drug-Exposure-in-Adolescents"><a href="#87-NeuroKoop-Neural-Koopman-Fusion-of-Structural–Functional-Connectomes-for-Identifying-Prenatal-Drug-Exposure-in-Adolescents" class="headerlink" title="87. NeuroKoop: Neural Koopman Fusion of Structural–Functional Connectomes for Identifying Prenatal Drug Exposure in Adolescents"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/NeuroKoop__Neural_Koopman_Fusion_of_Structural-Functional_Connectomes_for_Identifying_Prenatal_Drug_.pdf">NeuroKoop: Neural Koopman Fusion of Structural–Functional Connectomes for Identifying Prenatal Drug Exposure in Adolescents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Georgia State University</span></p>
<p>本论文提出了NeuroKoop框架，将结构（SBM）和功能（FNC）脑网络通过神经Koopman算子和图神经网络（GNN）融合，结合工作记忆分数实现动态潜在空间的个性化表征，提升了产前药物暴露（PDE）识别的准确性。在ABCD青少年大规模数据集上，NeuroKoop显著优于现有多模态GNN基线方法，能够揭示PDE对大脑结构-功能耦合的影响，为神经发育机制研究和个体化风险评估提供了新工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/NeuroKoop_Neural_Koopman_Fusion_of_Structural%E2%80%93Functional_Connectomes_for_Identifying_Prenatal_Drug_Exposure_in_Adolescents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-Domain-aligned-generative-downscaling-enhances-projections-of-extreme-climate-events"><a href="#88-Domain-aligned-generative-downscaling-enhances-projections-of-extreme-climate-events" class="headerlink" title="88. Domain-aligned generative downscaling enhances projections of extreme climate events"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Domain-aligned_generative_downscaling_enhances_projections_of_extreme_climate_events.pdf">Domain-aligned generative downscaling enhances projections of extreme climate events</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本文提出了基于生成式机器学习的空间-时间下采样模型DACD，通过域适应和Flow Matching训练框架，将全球低分辨率气候数据转换为高分辨率本地气候信息，实现多变量和多时间尺度的极端气候事件精准模拟。实验表明该方法在历史和未来情景下均显著提升了极端高温、强降水、强风和热带气旋等事件的模拟精度，能够更好地捕捉极端事件的空间分布与动态变化，为应对气候变化和制定适应策略提供了科学支撑。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Domain-aligned_generative_downscaling_enhances_projections_of_extreme_climate_events.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-MuST2-Learn-Multi-view-Spatial-Temporal-Type-Learning-for-Heterogeneous-Municipal-Service-Time-Estimation"><a href="#89-MuST2-Learn-Multi-view-Spatial-Temporal-Type-Learning-for-Heterogeneous-Municipal-Service-Time-Estimation" class="headerlink" title="89. MuST2-Learn: Multi-view Spatial-Temporal-Type Learning for Heterogeneous Municipal Service Time Estimation"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/MuST2-Learn__Multi-view_Spatial-Temporal-Type_Learning_for_Heterogeneous_Municipal_Service_Time_Esti.pdf">MuST2-Learn: Multi-view Spatial-Temporal-Type Learning for Heterogeneous Municipal Service Time Estimation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Tennessee at Chattanooga</span></p>
<p>本文提出MuST2-Learn框架，通过联合空间、时间以及服务类型三维度的信息，采用Transformer、CNN、注意力机制和高斯过程回归，结合LLM自动分配请求工作量，实现对市政多类型服务请求处理时间的高精度预测。实验表明，该方法在两个真实数据集上平均绝对误差降低至少32.5%，显著优于现有方法，具备良好泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/MuST2-Learn_Multi-view_Spatial-Temporal-Type_Learning_for_Heterogeneous_Municipal_Service_Time_Estimation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-Post-Hoc-Regression-Refinement-via-Pairwise-Rankings"><a href="#90-Post-Hoc-Regression-Refinement-via-Pairwise-Rankings" class="headerlink" title="90. Post Hoc Regression Refinement via Pairwise Rankings"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Post_Hoc_Regression_Refinement_via_Pairwise_Rankings.pdf">Post Hoc Regression Refinement via Pairwise Rankings</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">MPI-INF</span></p>
<p>本文提出了一种名为RankRefine的模型无关后处理方法，通过引入专家或大语言模型（LLM）提供的成对排序信息，结合逆方差加权策略，对已有回归模型的输出进行校正，无需重新训练。在多个合成和真实数据集（尤其是分子属性预测）上的实验表明，RankRefine即使仅有少量成对比较和中等准确度的排序器，也能带来可观的均方绝对误差（MAE）降低，且优于已有的同类后处理方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Post_Hoc_Regression_Refinement_via_Pairwise_Rankings.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-Probabilistic-Pretraining-for-Neural-Regression"><a href="#91-Probabilistic-Pretraining-for-Neural-Regression" class="headerlink" title="91. Probabilistic Pretraining for Neural Regression"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Probabilistic_Pretraining_for_Neural_Regression.pdf">Probabilistic Pretraining for Neural Regression</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Amazon</span></p>
<p>本文提出了NIAQUE（Neural Interpretable Any-Quantile Estimation），一种通过排列不变架构实现的深度概率回归模型，专为概率回归的迁移学习设计。该方法通过在多样化的下游回归数据集上预训练，并在目标数据集上微调实现知识迁移，有效提升了单个回归任务的性能。实验表明，NIAQUE在101个公开回归数据集和Kaggle竞赛中均优于强基线（如XGBoost、CatBoost、TabDPT和TabPFN），同时支持不确定性量化和特征重要性解释。结论显示，NIAQUE具备强大的可扩展性和泛化能力，是概率回归任务中迁移学习的有力框架。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Probabilistic_Pretraining_for_Neural_Regression.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-Vevo2-Bridging-Controllable-Speech-and-Singing-Voice-Generation-via-Unified-Prosody-Learning"><a href="#92-Vevo2-Bridging-Controllable-Speech-and-Singing-Voice-Generation-via-Unified-Prosody-Learning" class="headerlink" title="92. Vevo2: Bridging Controllable Speech and Singing Voice Generation via Unified Prosody Learning"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Vevo2__Bridging_Controllable_Speech_and_Singing_Voice_Generation_via_Unified_Prosody_Learning.pdf">Vevo2: Bridging Controllable Speech and Singing Voice Generation via Unified Prosody Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong, Shenzhen</span></p>
<p>Vevo2提出了一种统一的可控语音与歌声生成框架，通过两个核心音频分词器——无音乐记谱要求的韵律分词器和低帧率内容-风格分词器，实现文本、韵律（旋律）、风格和音色的灵活解耦建模。其方法包含自回归内容-风格建模和流匹配声学建模，并通过联合表达性和韵律相似性目标的多目标后训练显著提升了模型的可控性和泛化能力。实验结果表明，Vevo2在语音和歌声合成、转换与编辑等多种任务上表现优异，同时验证了语音与歌声统一建模的互补优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Vevo2_Bridging_Controllable_Speech_and_Singing_Voice_Generation_via_Unified_Prosody_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-Modeling-User-Preferences-as-Distributions-for-Optimal-Transport-based-Cross-domain-Recommendation-under-Non-overlapping-Settings"><a href="#93-Modeling-User-Preferences-as-Distributions-for-Optimal-Transport-based-Cross-domain-Recommendation-under-Non-overlapping-Settings" class="headerlink" title="93. Modeling User Preferences as Distributions for Optimal Transport-based Cross-domain Recommendation under Non-overlapping Settings"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Modeling_User_Preferences_as_Distributions_for_Optimal_Transport-based_Cross-domain_Recommendation_u.pdf">Modeling User Preferences as Distributions for Optimal Transport-based Cross-domain Recommendation under Non-overlapping Settings</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Tokyo</span></p>
<p>该论文提出了DUP-OT框架，利用高斯混合模型（GMM）对用户偏好进行分布建模，并通过最优传输（Optimal Transport）在无重叠用户或物品的跨域推荐场景下实现知识迁移。方法包括三阶段：评论嵌入与降维，用户GMM权重学习，以及通过最优传输进行跨域偏好迁移。实验表明，DUP-OT能有效缓解域间分布差异，并在Amazon数据集上优于现有无重叠跨域推荐方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Modeling_User_Preferences_as_Distributions_for_Optimal_Transport-based_Cross-domain_Recommendation_under_Non-overlapping_Settings.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-Set-Transformer-Architectures-and-Synthetic-Data-Generation-for-Flow-Guided-Nanoscale-Localization"><a href="#94-Set-Transformer-Architectures-and-Synthetic-Data-Generation-for-Flow-Guided-Nanoscale-Localization" class="headerlink" title="94. Set Transformer Architectures and Synthetic Data Generation for Flow-Guided Nanoscale Localization"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Set_Transformer_Architectures_and_Synthetic_Data_Generation_for_Flow-Guided_Nanoscale_Localization.pdf">Set Transformer Architectures and Synthetic Data Generation for Flow-Guided Nanoscale Localization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Polytechnic University of Catalonia</span></p>
<p>本论文提出将Set Transformer架构应用于血流引导的纳米级定位（FGL），以提升模型对解剖结构变异的泛化能力，方法上采用自注意力机制处理无序、变长的纳米设备循环时间集合，摒弃手工特征和静态图结构。此外，引入CGAN、WGAN、WGAN-GP和CVAE等深度生成模型进行合成数据增强以缓解数据稀缺与类别不均衡。实验结果表明，Set Transformer在区域分类准确率上能媲美或优于GNN基线，且更具灵活性和泛化性，但仍存在区分对称区域的困难和稀疏采样下的过拟合风险。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Set_Transformer_Architectures_and_Synthetic_Data_Generation_for_Flow-Guided_Nanoscale_Localization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-Motor-Imagery-EEG-Signal-Classification-Using-Minimally-Random-Convolutional-Kernel-Transform-and-Hybrid-Deep-Learning"><a href="#95-Motor-Imagery-EEG-Signal-Classification-Using-Minimally-Random-Convolutional-Kernel-Transform-and-Hybrid-Deep-Learning" class="headerlink" title="95. Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Motor_Imagery_EEG_Signal_Classification_Using_Minimally_Random_Convolutional_Kernel_Transform_and_Hy.pdf">Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">City University of London</span></p>
<p>本文提出了一种结合MiniRocket特征提取与线性分类器，以及CNN-LSTM混合神经网络的新方法，用于高效分类脑电（EEG）运动想象信号。通过在PhysioNet公开数据集上的实验，MiniRocket方法以更低计算成本取得了98.63%的平均准确率，优于98.06%的CNN-LSTM混合模型，显著提升了EEG运动想象分类的效率和准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Motor_Imagery_EEG_Signal_Classification_Using_Minimally_Random_Convolutional_Kernel_Transform_and_Hybrid_Deep_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-EGRA-Toward-Enhanced-Behavior-Graphs-and-Representation-Alignment-for-Multimodal-Recommendation"><a href="#96-EGRA-Toward-Enhanced-Behavior-Graphs-and-Representation-Alignment-for-Multimodal-Recommendation" class="headerlink" title="96. EGRA: Toward Enhanced Behavior Graphs and Representation Alignment for Multimodal Recommendation"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/EGRA_Toward_Enhanced_Behavior_Graphs_and_Representation_Alignment_for_Multimodal_Recommendation.pdf">EGRA: Toward Enhanced Behavior Graphs and Representation Alignment for Multimodal Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>该论文提出了一种新型多模态推荐框架EGRA，通过引入基于预训练多模态推荐模型表示构建的item-item图增强行为图，并提出了双层动态对齐权重机制，在实体和训练周期上自适应地调整模态与行为表示的对齐强度。实验结果表明，EGRA在五个公开数据集上均显著优于最新方法，在提升长尾项目推荐和整体性能方面展现出卓越效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/EGRA_Toward_Enhanced_Behavior_Graphs_and_Representation_Alignment_for_Multimodal_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-On-the-Collapse-Errors-Induced-by-the-Deterministic-Sampler-for-Diffusion-Models"><a href="#97-On-the-Collapse-Errors-Induced-by-the-Deterministic-Sampler-for-Diffusion-Models" class="headerlink" title="97. On the Collapse Errors Induced by the Deterministic Sampler for Diffusion Models"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/On_the_Collapse_Errors_Induced_by_the_Deterministic_Sampler_for_Diffusion_Models.pdf">On the Collapse Errors Induced by the Deterministic Sampler for Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Data Science, The University of Hong Kong</span></p>
<p>本文揭示了扩散模型中使用确定性采样器（如ODE-based采样）时出现的“坍塌错误”现象，即生成样本在数据空间中过度集中。作者提出了新的度量指标TID，系统分析了坍塌错误的普遍性及其根本原因——低噪声与高噪声区间的得分学习互相影响（see-saw效应），导致高噪声区间拟合失误并在采样过程中累积，从而产生坍塌。作者通过采样、训练和模型结构等多维度实验，验证了引入随机性、分区训练和跳连结构等方法能有效缓解坍塌错误，强调需关注得分学习与采样器动态的耦合作用。结论为：确定性采样器下扩散模型存在普遍的坍塌错误，其本质为模型泛化与采样器机制的协同失效。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/On_the_Collapse_Errors_Induced_by_the_Deterministic_Sampler_for_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-Training-a-Foundation-Model-for-Materials-on-a-Budget"><a href="#98-Training-a-Foundation-Model-for-Materials-on-a-Budget" class="headerlink" title="98. Training a Foundation Model for Materials on a Budget"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Training_a_Foundation_Model_for_Materials_on_a_Budget.pdf">Training a Foundation Model for Materials on a Budget</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Massachusetts Institute of Technology</span></p>
<p>本文提出Nequix模型，一种基于简化NequIP架构的E(3)-等变图神经网络，通过采用现代训练技术（如等变RMS层归一化、动态批处理和Muon优化器）实现了高效的材料基础模型训练。结果显示，Nequix在Matbench-Discovery和MDR Phonon基准上以远低于主流方法的训练成本获得了前三的准确率，并大幅提升了推断速度，降低了资源门槛。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Training_a_Foundation_Model_for_Materials_on_a_Budget.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-Time-Series-Based-Network-Intrusion-Detection-using-MTF-Aided-Transformer"><a href="#99-Time-Series-Based-Network-Intrusion-Detection-using-MTF-Aided-Transformer" class="headerlink" title="99. Time Series Based Network Intrusion Detection using MTF-Aided Transformer"></a>99. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/Time_Series_Based_Network_Intrusion_Detection_using_MTF-Aided_Transformer.pdf">Time Series Based Network Intrusion Detection using MTF-Aided Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Singapore</span></p>
<p>本文提出了一种结合马尔可夫转移场（MTF）和Transformer的时间序列分类方法，针对软件定义网络（SDN）中的入侵检测问题。该方法通过MTF模块对网络流量时序数据进行结构化编码，再与Transformer自注意力机制结合，捕捉时空依赖关系，并融合空间结构信息实现精细化多类异常检测。实验结果表明，MTF-aided Transformer在数据稀疏环境下，精准率和F1分数较传统方法提升最高可达26.5%，并能在高达40%数据缺失时保持优异表现，同时推理速度提升50%以上，适合实际SDN部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/Time_Series_Based_Network_Intrusion_Detection_using_MTF-Aided_Transformer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-HePGA-A-Heterogeneous-Processing-in-Memory-based-GNN-Training-Accelerator"><a href="#100-HePGA-A-Heterogeneous-Processing-in-Memory-based-GNN-Training-Accelerator" class="headerlink" title="100. HePGA: A Heterogeneous Processing-in-Memory based GNN Training Accelerator"></a>100. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-25/arxiv/HePGA__A_Heterogeneous_Processing-in-Memory_based_GNN_Training_Accelerator.pdf">HePGA: A Heterogeneous Processing-in-Memory based GNN Training Accelerator</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Washington State University</span></p>
<p>本文提出了一种三维异构PIM架构HePGA，结合SRAM、ReRAM和FeFET等多种存储设备，通过多目标优化方法实现GNN层与处理单元和层级的高效映射，提升能效和计算效率。实验结果表明，HePGA在能效和面积效率上较同类架构最高提升3.8倍和6.8倍，并保持模型准确率，同时适用于Transformer等新兴模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-26/HePGA_A_Heterogeneous_Processing-in-Memory_based_GNN_Training_Accelerator.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/26报纸</div>
      <div>http://ywfhhh.github.io/2025/08/2025-08-26_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>具身ADT</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月26日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/2025-08-27_article/" title="2025/08/27报纸">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">2025/08/27报纸</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-25_article/" title="2025/08/25报纸">
                        <span class="hidden-mobile">2025/08/25报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
