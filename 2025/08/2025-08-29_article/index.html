

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="具身ADT">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;29报纸">
<meta property="og:url" content="http://ywfhhh.github.io/2025/08/2025-08-29_article/index.html">
<meta property="og:site_name" content="最新前沿论文分享">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Reinforcement_Learning_for_Search_Tree_Size_Minimization_in_Constraint_Programming_New_Results_on_Scheduling_Benchmarks.jpg">
<meta property="article:published_time" content="2025-08-29T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-28T17:09:37.996Z">
<meta property="article:author" content="具身ADT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Reinforcement_Learning_for_Search_Tree_Size_Minimization_in_Constraint_Programming_New_Results_on_Scheduling_Benchmarks.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/29报纸 - 最新前沿论文分享</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ywfhhh.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<!-- hexo injector head_end start -->
<script src="/js/custom.js"></script>
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文库</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/29报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-29 00:00" pubdate>
          星期五, 八月 29日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          14k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/29报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/29报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-29<br>📄 <strong>发现论文数量</strong>：96  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20056v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      1. Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Reinforcement_Learning_for_Search_Tree_Size_Minimization_in_Constraint_Programming__New_Results_on_S.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Czech Technical University in Prague</span></p>
<p>本文提出将多臂老虎机（MAB）强化学习算法应用于约束规划（CP）中的Failure-Directed Search (FDS)搜索策略，通过混合贪婪与Boltzmann探索并引入choice rollback机制，以及参数调优，显著减少搜索树规模。实验表明，增强版FDS在两类调度基准（JSSP和RCPSP）上，运行速度比原始FDS提升至1.7倍和2.5倍，并优于IBM CP Optimizer最新算法，同时在900秒内改进了数百个标准基准实例的最优性下界。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Reinforcement_Learning_for_Search_Tree_Size_Minimization_in_Constraint_Programming_New_Results_on_Scheduling_Benchmarks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20018v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      2. SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/SWIRL__A_Staged_Workflow_for_Interleaved_Reinforcement_Learning_in_Mobile_GUI_Control.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Hong Kong</span></p>
<p>本文提出SWIRL框架，将多智能体强化学习（MARL）分解为交替的单智能体强化学习任务，通过阶段性训练和在线重加权机制，分别优化规划与执行两大子智能体，实现高效稳定的协同训练。理论上，SWIRL给出安全性下界和单调改进收敛性保证，实验在移动GUI控制和多智能体数学推理任务上均获SOTA性能，显著提升泛化和稳定性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/SWIRL_A_Staged_Workflow_for_Interleaved_Reinforcement_Learning_in_Mobile_GUI_Control.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19900v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      3. Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Adaptive_Scaling_of_Policy_Constraints_for_Offline_Reinforcement_Learning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>本文提出了一种自适应策略约束缩放（ASPC）框架，用于在离线强化学习中动态平衡强化学习目标与行为克隆目标。该方法以二阶可微优化算法，将约束缩放因子α设为可学习参数，实现在训练过程中自动调整RL与BC之间的权重，并理论证明了性能提升的下界保证。实验结果显示，在39个D4RL数据集和四个领域上，ASPC仅用一组超参数就优于需调参的SOTA离线RL方法，且计算开销极小。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Adaptive_Scaling_of_Policy_Constraints_for_Offline_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19566v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      4. Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X Networks
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Energy-Efficient_Learning-Based_Beamforming_for_ISAC-Enabled_V2X_Networks.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>本文提出了一种面向集成感知与通信（ISAC）车联网（V2X）系统的节能型学习式波束赋形方案。方法上，作者将V2X动态环境建模为马尔可夫决策过程（MDP），并采用基于脉冲神经网络（SNNs）的深度强化学习（DRL）算法，实现波束赋形与功率分配的联合优化，有效提升通信性能与感知精度，同时显著降低能耗。仿真结果表明，该方法在通信速率和能效方面均优于现有主流强化学习算法，适合绿色可持续V2X系统部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Energy-Efficient_Learning-Based_Beamforming_for_ISAC-Enabled_V2X_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19567v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      5. Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Counterfactual_Reward_Model_Training_for_Bias_Mitigation_in_Multimodal_Reinforcement_Learning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Vellore Institute of Technology (VIT-AP)</span></p>
<p>该论文提出了Fair-RLHF框架，将因果推断、变换器自编码器和CatBoost奖励模型结合，用于多模态环境下的偏见缓解和信任度评估。方法通过注入合成偏见和动态信任分数（Counterfactual Trust Score），实现了对数据漂移、预测不确定性、公平性违规和反事实一致性的联合监控。实验在多模态假新闻检测数据集上，准确率达到89.12%，显著降低了奖励模型的偏见和不公平信号，增强了鲁棒性和公平性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Counterfactual_Reward_Model_Training_for_Bias_Mitigation_in_Multimodal_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19488v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      6. PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/PoolFlip__A_Multi-Agent_Reinforcement_Learning_Security_Environment_for_Cyber_Defense.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Dartmouth College</span></p>
<p>本文提出了PoolFlip，一种扩展自FlipIt游戏的多智能体环境，用于网络安全防御场景，并引入了Flip-PSRO算法，通过多策略空间响应（Policy-Space Response Oracles, PSRO）和种群训练迭代优化防御策略。实验结果显示，Flip-PSRO训练的防御者在面对多样化已知及未知攻击策略时，比传统启发式和迭代最优响应方法拥有更高泛化性和资源控制能力，且能有效实现策略转移。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/PoolFlip_A_Multi-Agent_Reinforcement_Learning_Security_Environment_for_Cyber_Defense.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20085v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      7. HERMES: Human-to-Robot Embodied Learning from Multi-SouRce Motion Data for MobilE DexterouS Manipulation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/HERMES__Human-to-Robot_Embodied_Learning_from_Multi-Source_Motion_Data_for_Mobile_Dexterous_Manipula.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>HERMES提出了一种面向移动双臂灵巧操作的机器人学习框架，能从多源人类动作数据（包括遥操作、动作捕捉和视频）中，通过统一的强化学习方法实现多样人类手部动作向高维、多指灵巧机器人行为的转化。方法包括基于深度图像的端到端sim2real迁移、导航基础模型与闭环PnP精定位融合，实现了在复杂真实环境中的自主演化和泛化操作。结果显示，HERMES在多种现实场景下展现出优异的泛化能力和高成功率，能完成复杂的移动双臂灵巧操作任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/HERMES_Human-to-Robot_Embodied_Learning_from_Multi-SouRce_Motion_Data_for_MobilE_DexterouS_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20072v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      8. Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Discrete_Diffusion_VLA__Bringing_Discrete_Diffusion_to_Action_Decoding_in_Vision-Language-Action_Pol.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Hong Kong</span></p>
<p>该论文提出Discrete Diffusion VLA，将离散扩散机制引入视觉-语言-动作（VLA）策略，在单一transformer内以离散token方式统一视觉、语言和动作的建模。方法将动作离散化为token，并采用自适应的“先易后难”并行解码与二次re-masking机制，实现高效的动作生成和错误修正。实验证明，该方法在LIBERO和SimplerEnv等多项机器人基准上超越了自回归和连续扩散方法，同时大幅减少推理计算量，显示离散扩散解码在机器人控制中的有效性和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Discrete_Diffusion_VLA_Bringing_Discrete_Diffusion_to_Action_Decoding_in_Vision-Language-Action_Policies.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20095v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      9. DISCRETE-GUIDED DIFFUSION FOR SCALABLE AND SAFE MULTI-ROBOT MOTION PLANNING
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Discrete-Guided_Diffusion_for_Scalable_and_Safe_Multi-Robot_Motion_Planning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Virginia</span></p>
<p>本文提出了Discrete-Guided Diffusion (DGD) 框架，将多机器人运动规划问题分解为多个具有凸可行域的子问题，并结合离散多智能体路径规划（MAPF）解与受约束生成扩散模型指导轨迹生成。实验结果表明，该方法在大规模复杂环境下实现了高效规划和高成功率，显著优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/DISCRETE-GUIDED_DIFFUSION_FOR_SCALABLE_AND_SAFE_MULTI-ROBOT_MOTION_PLANNING.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19953v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      10. Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Divide,_Discover,_Deploy__Factorized_Skill_Learning_with_Symmetry_and_Style_Priors.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ETH Zurich</span></p>
<p>本论文提出了一种用于机器人无监督技能发现（USD）的模块化框架，通过用户自定义的状态空间因子分解，将不同的技能发现算法（如METRA和DIAYN）分别应用于状态的不同维度，并结合对称性归纳偏置、风格因子和正则化惩罚以提升技能的可解释性、安全性与部署性。实验在四足机器人上验证了该方法，实现在仿真中学习的技能可零样本迁移至真实硬件，且在下游导航任务中表现接近手工奖励训练的oracle策略，显著提升了样本效率和技能丰富性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Divide_Discover_Deploy_Factorized_Skill_Learning_with_Symmetry_and_Style_Priors.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19958v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      11. Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Long-VLA__Unleashing_Long-Horizon_Capability_of_Vision_Language_Action_Model_for_Robot_Manipulation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Westlake University</span></p>
<p>该论文提出Long-VLA，一种针对机器人长时序操作任务的端到端视觉-语言-动作（VLA）模型，通过创新性的阶段感知输入掩码策略，将每个子任务分为移动与交互阶段，并采用动态掩码机制自适应调整视觉输入，实现感知聚焦和技能链衔接。实验结果表明，Long-VLA在L-CALVIN基准和真实机器人任务中均大幅优于现有SOTA方法，具备良好可扩展性和鲁棒性，可无缝集成至现有VLA体系，提升长时序机器人操作能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Long-VLA_Unleashing_Long-Horizon_Capability_of_Vision_Language_Action_Model_for_Robot_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19945v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      12. Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Constraint_Learning_in_Multi-Agent_Dynamic_Games_from_Demonstrations_of_Local_Nash_Interactions.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Georgia Institute of Technology</span></p>
<p>本文提出了一种基于逆动态博弈的算法，通过对多智能体间局部纳什均衡交互演示的分析，利用KKT条件和混合整数线性规划（MILP）自动推断多主体系统中的参数化约束，并理论上保证所学约束为真实安全&#x2F;不安全集合的内近似。实验表明，该方法可在仿真与硬件平台上准确恢复未知约束并实现鲁棒安全的交互运动规划，优于传统单智能体和代价函数推断方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Constraint_Learning_in_Multi-Agent_Dynamic_Games_from_Demonstrations_of_Local_Nash_Interactions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19852v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      13. Ego-centric Predictive Model Conditioned on Hand Trajectories
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Ego-centric_Predictive_Model_Conditioned_on_Hand_Trajectories.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Show Lab, NUS</span></p>
<p>本论文提出了一种统一的两阶段自我中心预测模型（Ego-PM），以手部轨迹为条件，联合建模未来动作和视觉结果。方法包括通过多模态输入进行连续状态建模，显式预测未来手部轨迹，并采用因果交叉注意力将推断出的动作信号融合到潜在扩散模型中，实现逐帧未来视频生成。实验表明，该模型在Ego4D、BridgeData和RLBench等人类和机器人场景下均优于现有方法，在动作预测和未来视图生成上均取得领先。结论：Ego-PM能泛化于人类和机器人操作，预测准确且视觉生成质量高。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Ego-centric_Predictive_Model_Conditioned_on_Hand_Trajectories.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19816v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      14. A Standing Support Mobility Robot for Enhancing Independence in Elderly Daily Living
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/A_Standing_Support_Mobility_Robot_for_Enhancing_Independence_in_Elderly_Daily_Living.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tohoku University</span></p>
<p>本文提出了一种名为Moby的站立支持型移动机器人，专为提升老年人日常生活中的独立性和安全性而设计。该机器人结合了机械结构（如可调节支撑垫和多功能把手）、ROS2系统、NAV2导航与LiDAR感知，实现了手动和自主两种操作模式，并通过实验对比展示了其在如如厕转移等高风险任务中显著降低了用户的体力和认知负担，提升了独立完成任务的效率与信心。结论表明，Moby不仅能减少照护依赖，还能通过站立支持促进社交参与与自我效能，具有良好的实际应用前景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/A_Standing_Support_Mobility_Robot_for_Enhancing_Independence_in_Elderly_Daily_Living.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19679v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      15. InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/InquireMobile__Teaching_VLM-based_Mobile_Agent_to_Request_Human_Assistance_via_Reinforcement_Fine-Tu.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Taobao &amp; Tmall Group of Alibaba</span></p>
<p>该论文提出了InquireMobile模型，通过两阶段训练（监督微调和基于GRPO的强化学习），赋能视觉语言模型（VLM）驱动的移动代理在关键决策点主动请求用户确认。方法包括构建InquireBench基准，用于评估移动代理在高风险场景下安全交互与主动询问能力。实验显示该模型在询问成功率和整体任务完成率上显著优于现有基线，验证了人机协同在移动自动化中的必要性与有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/InquireMobile_Teaching_VLM-based_Mobile_Agent_to_Request_Human_Assistance_via_Reinforcement_Fine-Tuning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19607v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      16. Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Impedance_Primitive-augmented_Hierarchical_Reinforcement_Learning_for_Sequential_Tasks.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">TU Delft</span></p>
<p>本文提出了一种基于阻抗原语增强的分层强化学习（IMP-HRL）框架，用于高效解决机器人顺序接触操作任务。方法核心包括扩展的可变刚度动作空间、自适应刚度控制器和位置-刚度可供性耦合，能动态调整刚度以平衡安全性与任务完成效率。实验表明，该方法在学习效率、原语组合能力、顺序任务成功率等方面优于现有方法，且具备仿真到实物迁移能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Impedance_Primitive-augmented_Hierarchical_Reinforcement_Learning_for_Sequential_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19508v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      17. DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/DATR__Diffusion-based_3D_Apple_Tree_Reconstruction_Framework_with_Sparse-View.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Cornell University</span></p>
<p>本文提出了DATR，一个针对果园场景下稀疏视角单幅图像到3D重建的两阶段框架。方法首先通过多模态数据融合与深度模型自动生成树木掩码，随后利用基于扩散模型和大重建模型（LRM）的多模态重建模块实现高精度3D苹果树建模，并通过Real2Sim合成数据训练实现零样本泛化。实验结果表明，DATR在真实和合成数据集上均优于现有方法，达到与工业级激光扫描仪相当的结构精度且采集效率提升约360倍，展示了其在农业数字孪生与机器人应用的广泛前景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/DATR_Diffusion-based_3D_Apple_Tree_Reconstruction_Framework_with_Sparse-View.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19493v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      18. Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Mind_the_Third_Eye!_Benchmarking_Privacy_Awareness_in_MLLM-powered_Smartphone_Agents.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shandong University</span></p>
<p>本文提出了SAPA-Bench，这是首个针对多模态大语言模型（MLLM）驱动智能手机代理的隐私感知能力的大规模基准，包括7,138个真实场景，并标注了隐私类型、敏感级别等信息。通过五项专有指标（如PRR、PLR、RA等）系统评测七种主流智能手机代理，结果显示无论开源还是闭源代理均难以有效检测和响应隐私风险，尤其在低中敏感场景下表现不足，强调了专门隐私训练和评测的重要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Mind_the_Third_Eye%21_Benchmarking_Privacy_Awareness_in_MLLM-powered_Smartphone_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20089v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      19. Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Bridging_Domain_Gaps_for_Fine-Grained_Moth_Classification_Through_Expert-Informed_Adaptation_and_Fou.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Exeter</span></p>
<p>本文提出了一种结合专家标注目标域数据与大规模基础模型BioCLIP2知识蒸馏的轻量级ConvNeXt-tiny架构，用于解决自动化相机系统下飞蛾物种精细分类中的领域迁移问题。实验显示，知识蒸馏显著提升轻量模型性能，在计算成本极低的情况下达到与BioCLIP2可比的准确率，为生态监测系统开发提供了有效建议。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Bridging_Domain_Gaps_for_Fine-Grained_Moth_Classification_Through_Expert-Informed_Adaptation_and_Foundation_Model_Priors.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20013v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      20. Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Cross-Platform_E-Commerce_Product_Categorization_and_Recategorization__A_Multimodal_Hierarchical_Cla.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nova School of Business and Economics</span></p>
<p>本论文提出一种多模态分层分类框架，结合RoBERTa文本特征、ViT和CLIP视觉特征，通过早期、晚期和注意力融合策略，实现了跨平台商品自动分类，并引入自监督SimCLR-UMAP级联聚类进行产品再分类以细化商品类别。实验结果表明基于CLIP的MLP晚期融合策略在工业级数据上取得98.59%的分层F1，且自监督再分类能稳定发现高纯度新子类，系统已在EURWEB商用平台上线，显著提升了分类精度与工业部署可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Cross-Platform_E-Commerce_Product_Categorization_and_Recategorization_A_Multimodal_Hierarchical_Classification_Approach.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20080v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      21. Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Seam360GS__Seamless_360°_Gaussian_Splatting_from_Real-World_Omnidirectional_Images.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Yonsei University</span></p>
<p>该论文提出了一种集成校准框架，将双鱼眼畸变建模和3D高斯投点（Gaussian Splatting）结合，用于360°全景图像的无缝新视角合成。方法通过联合优化高斯参数和校准变量，补偿镜头间隙和角度畸变，实现对消费级双鱼眼相机采集的原始、非理想图像的高精度重建。实验在真实和合成数据集上显示，所提方法在定量指标（PSNR、SSIM、LPIPS）及细节表现上均优于现有方法，并能有效消除拼接伪影。结论：该方法实现了高保真、高效率的全景新视角合成，提升了360°视觉内容的渲染质量与一致性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Seam360GS_Seamless_360%C2%B0_Gaussian_Splatting_from_Real-World_Omnidirectional_Images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20020v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      22. GS: Generative Segmentation via Label Diffusion
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/GS__Generative_Segmentation_via_Label_Diffusion.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-sen University</span></p>
<p>本文提出GS（Generative Segmentation），通过标签扩散（label diffusion）将语言驱动的图像分割建模为生成任务，采用条件扩散模型直接从噪声生成分割掩码，联合图像和文本描述进行端到端训练。GS在Panoptic Narrative Grounding (PNG) 基准上显著优于现有判别式和扩散式方法，达到最新性能，证明了生成建模在多模态分割任务中的有效性和通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/GS_Generative_Segmentation_via_Label_Diffusion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19914v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      23. The Next Layer: Augmenting Foundation Models with Structure-Preserving and Attention-Guided Learning for Local Patches to Global Context Awareness in Computational Pathology
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/The_Next_Layer__Augmenting_Foundation_Models_with_Structure-Preserving_and_Attention-Guided_Learning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Department of Imaging Physics, MD Anderson Cancer Center</span></p>
<p>本文提出了EAGLE-Net，一种结合多尺度绝对空间编码（MASE）、注意力引导的邻域损失和背景抑制机制的多实例学习（MIL）框架，用于提升基础模型在病理切片全局结构与局部微环境建模能力。通过在多癌种大规模数据集和多种主流基础模型上验证，EAGLE-Net在分类和预后任务中均优于现有主流MIL算法，生成的注意力热图与病理专家注释高度一致，并提升了解释性与预测准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/The_Next_Layer_Augmenting_Foundation_Models_with_Structure-Preserving_and_Attention-Guided_Learning_for_Local_Patches_to_Global_Context_Awareness_in_Computational_Pathology.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19905v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      24. Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Hyperspectral_Sensors_and_Autonomous_Driving__Technologies,_Limitations,_and_Opportunities.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Galway</span></p>
<p>本论文系统综述了高光谱成像（HSI）在高级驾驶辅助系统（ADAS）和自动驾驶（AD）中的技术现状，分析了216款商用高光谱与多光谱摄像头在帧率、分辨率、光谱维度及汽车温度标准下的适用性。结果表明，HSI在材料识别和恶劣环境感知方面有显著优势，但现有设备和数据集在商用部署方面面临诸多挑战，如温度标准不达标、数据稀缺、算法未充分利用光谱特性等。未来需发展自动车规级快拍型摄像头、建立大规模多样化数据集，并优化光谱-空间算法以实现实时处理，推动HSI在自动驾驶领域的实际应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Hyperspectral_Sensors_and_Autonomous_Driving_Technologies_Limitations_and_Opportunities.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19887v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      25. Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Bangla-Bayanno__A_52K-Pair_Bengali_Visual_Question_Answering_Dataset_with_LLM-Assisted_Translation_R.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">North South University</span></p>
<p>该论文提出了Bangla-Bayanno，这是一个包含52,650组基于图像的问答对的孟加拉语视觉问答（VQA）数据集，采用多语言大模型（LLM）辅助的翻译精炼流程，显著提升了低资源语言数据的语义和语法质量。数据集涵盖名词性、数量性和极性三类答案类型，并在规模和多样性上超越了以往所有孟加拉语VQA数据集。结论表明，Bangla-Bayanno为低资源多模态学习和包容性AI系统研究提供了新的高质量基准。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Bangla-Bayanno_A_52K-Pair_Bengali_Visual_Question_Answering_Dataset_with_LLM-Assisted_Translation_Refinement.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19862v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      26. Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Multimodal_Conditional_MeshGAN_for_Personalized_Aneurysm_Growth_Prediction.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Imperial College London</span></p>
<p>本文提出MCMeshGAN，一种多模态条件3D mesh-to-mesh生成对抗网络，用于个性化预测主动脉瘤的三维生长。方法采用双分支结构，结合局部KNN卷积网络（KCN）以保留细粒度几何细节，以及全局图卷积网络（GCN）以捕捉长距离结构信息，并引入患者临床属性作为条件，实现可控的三维解剖变化预测。实验表明，MCMeshGAN在几何准确性与最大直径估计等临床指标上优于多种基线方法，能够更准确地模拟个体化疾病进展轨迹。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Multimodal_Conditional_MeshGAN_for_Personalized_Aneurysm_Growth_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19798v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      27. FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/FusionSort__Enhanced_Cluttered_Waste_Segmentation_with_Advanced_Decoding_and_Comprehensive_Modality_.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">MBZUAI</span></p>
<p>本文提出FusionSort，一种用于复杂环境下废弃物分割的多模态神经网络架构。其关键方法包括融合RGB、HSI等多模态数据，通过数据融合块（PCA降维+对齐融合）与综合注意力块（结合Mamba注意力、卷积和坐标注意力）精细提取特征，并采用联合Dice与交叉熵损失优化分割性能。实验在SpectralWaste和MultispectralWaste数据集上表明，FusionSort在多种模态及其融合配置下均显著优于主流分割模型，提升了废弃物分割精度和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/FusionSort_Enhanced_Cluttered_Waste_Segmentation_with_Advanced_Decoding_and_Comprehensive_Modality_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19769v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      28. AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/AIM__Adaptive_Intra-Network_Modulation_for_Balanced_Multimodal_Learning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China University of Technology</span></p>
<p>该论文提出了Adaptive Intra-Network Modulation (AIM)方法，通过分层调节网络参数和深度，解决多模态学习中的模态不平衡问题。AIM引入参数解耦机制和辅助-弱模态交互策略，针对主导模态的未充分优化参数形成辅助模块，与弱模态联合训练，同时采用深度自适应调节以适应不同层的模态优化偏差。实验表明，AIM在多个数据集、多种融合策略和不同优化器下均优于现有方法，提升了多模态和单模态性能，验证了方法的通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/AIM_Adaptive_Intra-Network_Modulation_for_Balanced_Multimodal_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19754v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      29. FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/FastAvatar__Towards_Unified_Fast_High-Fidelity_3D_Avatar_Reconstruction_with_Large_Gaussian_Reconstr.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tongji University</span></p>
<p>本文提出FastAvatar，一种基于大规模高斯重建变换器（LGRT）的端到端3D头像重建框架，可灵活处理任意数量和类型的输入（单张、多视角、视频），实现高质量3D Gaussian Splatting（3DGS）模型的快速重建。方法核心包括VGGT风格的变换器架构、多粒度姿态与表情编码、增量高斯聚合及关键点追踪与切片融合损失，显著提升了建模速度、质量及数据利用率。实验证明FastAvatar在重建质量和速度上优于现有方法，支持增量式高保真头像重建。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/FastAvatar_Towards_Unified_Fast_High-Fidelity_3D_Avatar_Reconstruction_with_Large_Gaussian_Reconstruction_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19927v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      30. WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/WaveHiT-SR__Hierarchical_Wavelet_Network_for_Efficient_Image_Super-Resolution.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Wurzburg</span></p>
<p>本文提出WaveHiT-SR，将小波变换嵌入分层Transformer架构，通过自适应层级窗口和WaveAttention模块，有效捕捉多尺度特征和长距离依赖，提升图像超分辨率效率与性能。实验证明，该方法在保持较低参数量与计算复杂度的同时，在多个基准数据集上实现了优异的定量和定性结果，显著优于现有主流Transformer-SR方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/WaveHiT-SR_Hierarchical_Wavelet_Network_for_Efficient_Image_Super-Resolution.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19909v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      31. Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Integrating_SAM_Supervision_for_3D_Weakly_Supervised_Point_Cloud_Segmentation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Singapore</span></p>
<p>本文提出了一种新颖的3D点云弱监督语义分割方法，通过利用2D基础模型（如Semantic-SAM）生成的分割掩码，并将其通过几何对应关系投影到3D空间，实现稀疏3D标注的大幅扩充。方法流程包括2D分割掩码生成与3D投影、标签扩增与传播、置信度与不确定性驱动的一致性正则化、以及噪声鲁棒损失训练，有效提升了弱监督下的3D点云分割性能。结论显示，所提方法在ScanNetV2和S3DIS数据集上取得了弱监督领域的最新最好成绩，并验证了2D-3D融合策略在3D场景理解中的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Integrating_SAM_Supervision_for_3D_Weakly_Supervised_Point_Cloud_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19881v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      32. Multispectral LiDAR data for extracting tree points in urban and suburban areas
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Multispectral_LiDAR_data_for_extracting_tree_points_in_urban_and_suburban_areas.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">TU Wien</span></p>
<p>本论文提出利用多光谱LiDAR数据结合深度学习（DL）Transformer模型（SPT、PTv3、PTv1）进行城市及郊区树木点云的二元语义分割，首次引入pNDVI特征并发布公开基准数据集Loosdorf-tree。实验结果表明SPT模型在精度和时效性上表现最佳，结合空间与光谱特征显著提升树木识别准确率，误差率降低10.61个百分点，展示了MS-LiDAR和DL在城市树木自动提取与管理的巨大潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Multispectral_LiDAR_data_for_extracting_tree_points_in_urban_and_suburban_areas.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19866v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      33. TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/TrajFusionNet__Pedestrian_Crossing_Intention_Prediction_via_Fusion_of_Sequential_and_Visual_Trajecto.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universite de Moncton</span></p>
<p>该论文提出TrajFusionNet，一种基于transformer的模型，通过融合行人轨迹序列和视觉表示（SAM和VAM两个分支），利用未来行人轨迹与车辆速度预测作为先验，实现对行人过街意图的预测。实验在PIE与JAAD等主流数据集上获得了当前最优精度，并显著降低了推理总时延，适合实时自动驾驶场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/TrajFusionNet_Pedestrian_Crossing_Intention_Prediction_via_Fusion_of_Sequential_and_Visual_Trajectory_Representations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19789v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      34. StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/StableIntrinsic__Detail-preserving_One-step_Diffusion_Model_for_Multi-view_Material_Estimation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Lab of CAD&amp;CG, Zhejiang University</span></p>
<p>本文提出StableIntrinsic，一种用于多视图材料估计的一步扩散模型，通过在像素空间优化损失和引入细节注入网络（DIN），显著提升了材料参数估计的速度和稳定性。实验结果表明，该方法在细节保留和材料属性准确性上优于现有主流方法，推理速度提升约50倍，并为逆渲染等下游任务带来更高质量的分解能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/StableIntrinsic_Detail-preserving_One-step_Diffusion_Model_for_Multi-view_Material_Estimation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19773v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      35. The Return of Structural Handwritten Mathematical Expression Recognition
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/The_Return_of_Structural_Handwritten_Mathematical_Expression_Recognition.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Würzburg</span></p>
<p>本文提出了一种结构化手写数学表达式识别方法，核心创新包括：自动注释系统利用神经网络将LaTeX与原始笔迹对齐，实现符号分割、分类和空间关系的自动标注，产出CROHME+和MathWriting+结构化数据集；模块化识别系统将分割、分类和关系预测独立优化，通过图结构（Stroke Label Graph）结合图排序、卷积-循环网络和Transformer校正，在CROHME-2023上取得74.14%表达式准确率。该方法不仅实现结构解释性和可追溯性，还提升了符号与结构识别的准确率，证明结构化方法在高质量注释数据支持下依然具备竞争力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/The_Return_of_Structural_Handwritten_Mathematical_Expression_Recognition.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19762v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      36. BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/BuzzSet_v1.0__A_Dataset_for_Pollinator_Detection_in_Field_Conditions.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Bonn</span></p>
<p>该论文提出了BuzzSet大规模高分辨率田间授粉昆虫检测数据集，采用YOLOv12弱标注结合人工修正，并使用RF-DETR变换器检测器进行基线评估。结果显示在蜜蜂和熊蜂类别上F1分数分别达0.94和0.92，有效提升田间小目标检测和生态视觉系统的基准性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/BuzzSet_v1.0_A_Dataset_for_Pollinator_Detection_in_Field_Conditions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19730v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      37. Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Improving_Generalization_in_Deepfake_Detection_with_Face_Foundation_Models_and_Metric_Learning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Centre for Research and Technology Hellas</span></p>
<p>本文提出了一种基于人脸基础模型(FSFM)与三元组损失度量学习的深度伪造检测框架，采用多数据集训练和面向实际场景的帧级视觉特征提取，探索了不同三元组损失与归因标签策略对泛化能力的影响。实验表明，多数据集训练和FSFM初始化显著提升了检测模型在分布外及真实世界场景下的泛化性能，Batch All三元组损失进一步带来小幅提升，但归因策略在真实场景下泛化有限。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Improving_Generalization_in_Deepfake_Detection_with_Face_Foundation_Models_and_Metric_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19698v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      38. Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe–Hessian Operators
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Synthetic_Image_Detection_via_Spectral_Gaps_of_QC-RBIM_Nishimori_Bethe-Hessian_Operators.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South-West State University</span></p>
<p>该论文提出了一种基于图论和统计物理的新型无监督合成图像检测方法，将CNN提取的特征嵌入到稀疏的QC-LDPC图中，通过Nishimori温度下RBIM加权，并利用Bethe–Hessian算子的谱隙结构区分真实与合成图像。实验表明，该方法无需监督训练即可在多种生成模型和特征网络下实现94%以上的检测准确率，具有良好通用性和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Synthetic_Image_Detection_via_Spectral_Gaps_of_QC-RBIM_Nishimori_Bethe%E2%80%93Hessian_Operators.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19651v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      39. Scalable Object Detection in the Car Interior With Vision Foundation Models
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Scalable_Object_Detection_in_the_Car_Interior_With_Vision_Foundation_Models.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University of Munich</span></p>
<p>本文提出了ODAL，一个面向车内场景的对象检测与定位分布式框架，采用LLaVA-1.5 7B视觉基础模型，并利用分布式架构将视觉编码在车载端执行、检测与定位在云端执行，有效克服了车载算力瓶颈。通过新设计的ODALbench基准，实验表明经过微调的ODAL-LLaVA模型在检测准确率和减少幻觉上显著优于GPT-4o，ODALscore提升至89%，ODALSNR提升三倍。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Scalable_Object_Detection_in_the_Car_Interior_With_Vision_Foundation_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19574v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      40. Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Multimodal_Prototype_Alignment_for_Semi-supervised_Pathology_Image_Segmentation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen International Graduate School, Tsinghua University</span></p>
<p>本文提出MPAMatch框架，通过多模态原型对齐实现半监督病理图像分割，结合视觉和文本原型对像素标签进行粗到细监督，增强结构和语义建模。实验结果表明MPAMatch在多个病理分割数据集上超越现有方法，有效提升分割精度及泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Multimodal_Prototype_Alignment_for_Semi-supervised_Pathology_Image_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19518v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      41. Fast Texture Transfer for XR Avatars via Barycentric UV Conversion
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Fast_Texture_Transfer_for_XR_Avatars_via_Barycentric_UV_Conversion.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KAIST</span></p>
<p>该论文提出了一种基于重心UV坐标转换的快速全身虚拟人头像纹理转移方法，通过预计算UV映射并将其整合为单一变换矩阵，实现了纹理一键快速传递，有效避免了传统仿射变换方法的低效和边界伪影问题。实验结果表明，该方法在速度上提升7000倍，且在L1、SSIM、PSNR和LPIPS等多项指标上显著优于基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Fast_Texture_Transfer_for_XR_Avatars_via_Barycentric_UV_Conversion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19652v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      42. Vision-SR1: Self-Rewarding Vision-Language Model via Reasoning Decomposition
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Self-Rewarding_Vision-Language_Model_via_Reasoning_Decomposition.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent AI Lab, Seattle</span></p>
<p>该论文提出了Vision-SR1方法，通过将视觉-语言模型（VLM）的推理过程分解为视觉感知和语言推理两个阶段，并利用模型自身进行视觉感知自我验证奖励，在无外部标注的情况下强化视觉感知能力，缓解视觉幻觉和语言捷径问题。实验结果表明，Vision-SR1在多项视觉-语言任务和基准上均优于现有方法，显著提升了模型的视觉推理能力和泛化性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Vision-SR1_Self-Rewarding_Vision-Language_Model_via_Reasoning_Decomposition.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19626v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      43. Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Controllable_Skin_Synthesis_via_Lesion-Focused_Vector_Autoregression_Model.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University</span></p>
<p>该论文提出LF-VAR模型，通过结合多尺度病灶关注型VQ-VAE和视觉自回归Transformer，利用定量病灶测量分数和类型标签，实现基于语言提示的可控皮肤图像合成流程。实验结果表明，该方法在皮肤病灶合成任务中FID得分优于现有SOTA，合成图像质量和病灶特征控制能力均显著提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Controllable_Skin_Synthesis_via_Lesion-Focused_Vector_Autoregression_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19604v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      44. IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/IELDG__Suppressing_Domain-Specific_Noise_with_Inverse_Evolution_Layers_for_Domain_Generalized_Semant.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an University of Technology</span></p>
<p>该论文提出IELDG框架，通过在扩散模型（IELDM）和分割模型（IELFormer）中集成逆演化层（IELs），显式放大结构和语义缺陷，实现高质量合成数据生成和分割预测噪声抑制。其多尺度频域融合（MFF）模块进一步提升跨分辨率特征一致性。实验表明，IELDG在多项跨域分割基准上超越现有方法，有效提升泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/IELDG_Suppressing_Domain-Specific_Noise_with_Inverse_Evolution_Layers_for_Domain_Generalized_Semantic_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19581v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      45. Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Guiding_Noisy_Label_Conditional_Diffusion_Models_with_Score-based_Discriminator_Correction.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">FPT Software AI Center</span></p>
<p>该论文提出了一种名为Score-based Discriminator Correction (SBDC)的新方法，用于在推理阶段对受噪声标签影响的条件扩散模型进行校正。SBDC通过训练小型判别器，结合噪声检测对已训练模型的生成过程进行指导，并采用伪清洁标签重排和SiMix数据增强提升泛化性及效率，无需对主模型进行重新训练。大量实验显示，SBDC在合成与真实含噪数据集上的生成质量、类别一致性和鲁棒性均超过现有的最优方法，同时推理效率高，且能扩展至大类别场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Guiding_Noisy_Label_Conditional_Diffusion_Models_with_Score-based_Discriminator_Correction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19573v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      46. DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/DNP-Guided_Contrastive_Reconstruction_with_a_Reverse_Distillation_Transformer_for_Medical_Anomaly_De.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Mathematics, Shandong University</span></p>
<p>该论文提出DNP-ConFormer，一种结合可训练编码器、原型引导解码器和多样性对齐损失的无监督医学异常检测框架。方法采用双分支结构，通过自监督预训练Transformer提取特征，利用多样性约束避免原型崩塌，并通过对比重建和动量分支实现领域自适应。实验表明该方法在多种医学影像基准上显著提升异常定位和泛化能力，能有效捕捉丰富语义并提升检测性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/DNP-Guided_Contrastive_Reconstruction_with_a_Reverse_Distillation_Transformer_for_Medical_Anomaly_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19565v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      47. FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/FlowDet__Overcoming_Perspective_and_Scale_Challenges_in_Real-Time_End-to-End_Traffic_Detection.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CICT Mobile Communication Technology Co.,Ltd</span></p>
<p>本文提出了FlowDet，一种针对交通路口监控场景的高效实时端到端目标检测框架。其方法在DETR架构上引入了几何可变形单元（GDU）实现自适应几何建模，并通过尺度感知注意力（SAA）模块实现极端尺度下的高效多尺度特征融合。实验表明，该方法在新发布的Intersection-Flow-5K数据集上，相较RT-DETR基线提升APtest 1.5%、减少63.2%计算量，并提升16.2%推理速度，尤其在小目标检测上表现突出，适合实际边缘部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/FlowDet_Overcoming_Perspective_and_Scale_Challenges_in_Real-Time_End-to-End_Traffic_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19527v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      48. MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/MotionFlux__Efficient_Text-Guided_Motion_Generation_through_Rectified_Flow_Matching_and_Preference_A.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tianjin University</span></p>
<p>本文提出MotionFlux，一种基于整流流匹配（rectified flow matching）的高效文本驱动动作生成框架，通过TAPO（TMR++ Aligned Preference Optimization）实现动作与文本语义的精细对齐。MotionFlux利用单步或少步推理显著加速生成速度，并通过在线偏好优化增强复杂文本与动作的语义一致性。实验结果表明，MotionFlux在动作质量、语义一致性及实时性上均超越现有方法，适用于实时交互场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/MotionFlux_Efficient_Text-Guided_Motion_Generation_through_Rectified_Flow_Matching_and_Preference_Alignment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19499v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      49. Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Sat2Flow__A_Structure-Aware_Diffusion_Framework_for_Human_Flow_Generation_from_Satellite_Imagery.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen Technology University</span></p>
<p>该论文提出Sat2Flow，利用卫星遥感影像作为唯一输入，通过多核编码、跨模态对比学习和置换感知扩散生成模块，实现城市区域间的出行流矩阵生成。方法有效解决了对辅助数据的依赖和结构一致性问题，实验证明在多项指标上优于传统物理模型和数据驱动基线，具备良好可扩展性和全球适用性，特别适用于数据稀缺区域。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Sat2Flow_A_Structure-Aware_Diffusion_Framework_for_Human_Flow_Generation_from_Satellite_Imagery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20032v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      50. Pruning Strategies for Backdoor Defense in LLMs
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Pruning_Strategies_for_Backdoor_Defense_in_LLMs.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Utah State University</span></p>
<p>本文提出六种不同的注意力头剪枝策略（包括梯度剪枝、层次方差剪枝、结构化稀疏剪枝、随机集成剪枝、强化学习剪枝和贝叶斯不确定性剪枝）来防御大语言模型中的后门攻击，不依赖于已知触发器或干净参考模型。实验表明，梯度剪枝对语法型后门最有效，而强化学习和贝叶斯剪枝对风格型后门防御更优，有效降低攻击成功率并保持模型性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Pruning_Strategies_for_Backdoor_Defense_in_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20024v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      51. Using item recommendations and LLMs in marketing email titles
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Using_item_recommendations_and_LLMs_in_marketing_email_titles.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mercari Inc.</span></p>
<p>本文提出了一种结合个性化商品推荐与大语言模型（LLM）自动生成营销邮件标题的方法，流程包括商品推荐、敏感词过滤、Prompt设计、LLM生成、多轮人工与自动质量审核以及法律和安全校验。在线上对逾百万用户的A&#x2F;B实验表明，LLM生成的标题显著提升了邮件内商品的点击率（提升24%），但对邮件开启率与最终转化率影响有限。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Using_item_recommendations_and_LLMs_in_marketing_email_titles.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20019v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      52. Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Symphony__A_Decentralized_Multi-Agent_Framework_for_Scalable_Collective_Intelligence.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Gradient</span></p>
<p>该论文提出了Symphony，一个去中心化多智能体系统，利用分布式账本、Beacon动态任务分配协议和多链式思维（CoT）加权投票机制，使轻量级LLM在异构边缘设备上高效协作。实验表明，Symphony在推理基准任务上显著优于现有集中式多智能体框架，提升了模型准确性、鲁棒性，并大幅降低了部署和通信开销。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Symphony_A_Decentralized_Multi-Agent_Framework_for_Scalable_Collective_Intelligence.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20088v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      53. AudioStory: Generating Long-Form Narrative Audio with Large Language Models
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/AudioStory__Generating_Long-Form_Narrative_Audio_with_Large_Language_Models.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, University of Chinese Academy of Sciences</span></p>
<p>AudioStory提出了一种统一框架，将大型语言模型（LLM）的推理能力与扩散式音频生成系统结合，实现结构化、长时序叙事音频的生成。通过LLM对复杂叙事指令进行分解，利用语义与残差双重桥接机制，并采用端到端联合训练，模型显著提升了跨事件一致性、指令遵循性和音频保真度，在长音频生成任务上大幅优于现有基线。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/AudioStory_Generating_Long-Form_Narrative_Audio_with_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20068v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      54. 11PLUS-BENCH: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/11Plus-Bench__Demystifying_Multimodal_LLM_Spatial_Reasoning_with_Cognitive-Inspired_Analysis.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Microsoft Research</span></p>
<p>本文提出了11PLUS-BENCH，一套源自标准空间能力测验的高质量空间推理基准，并引入了细粒度的专家认知特征注释，实现对多模态大语言模型（MLLMs）与人类空间认知能力的系统性评估。通过对14种MLLM和人类的对比分析，发现当前MLLM在空间认知上表现出初步能力，但与人类相比仍有显著差距，尤其在实例层面表现随机，缺乏可预测性。结论表明，虽然MLLM在空间推理方面出现萌芽，但其推理结构性和鲁棒性与人类仍有较大鸿沟，需进一步改进。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/11PLUS-BENCH_Demystifying_Multimodal_LLM_Spatial_Reasoning_with_Cognitive-Inspired_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20030v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      55. Large Language Models (LLMs) for Electronic Design Automation (EDA)
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Large_Language_Models_(LLMs)_for_Electronic_Design_Automation_(EDA).pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University of Munich</span></p>
<p>本文系统综述了大语言模型（LLM）在集成电路电子设计自动化（EDA）全流程中的应用，包括硬件设计、测试、优化等多个环节，介绍了多种基于LLM的自动化设计与测试框架（如AutoChip、HLS程序修复、结构化CoT测试等），并通过案例分析展示LLM辅助下的设计效率提升。结论指出，LLM有望显著提升EDA流程的智能化与自动化水平，但仍面临如数据隐私、语义对齐、工具集成等挑战，整体前景广阔但需持续攻关。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Large_Language_Models_%28LLMs%29_for_Electronic_Design_Automation_%28EDA%29.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19993v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      56. MathBuddy: A Multimodal System for Affective Math Tutoring
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/MathBuddy__A_Multimodal_System_for_Affective_Math_Tutoring.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IT:U Interdisciplinary Transformation University Austria</span></p>
<p>该论文提出了MathBuddy系统，将文本和人脸表情双模态情感识别与大语言模型（LLM）结合，实现情感感知的数学AI辅导，能根据学生情绪动态调整教学策略（如激励或挑战）。实验表明，模型在八大教学维度评测上显著优于传统LLM辅导，显著提升了学生学习体验和效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/MathBuddy_A_Multimodal_System_for_Affective_Math_Tutoring.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19966v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      57. Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Dhati+__Fine-tuned_Large_Language_Models_for_Arabic_Subjectivity_Evaluation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Université de Ghardaia</span></p>
<p>本论文提出了针对阿拉伯语主观性检测的方法，通过整合多源阿拉伯语数据（ASTD, LABR, HARD, SANAD）构建了AraDhati+数据集，并对XLM-RoBERTa、AraBERT和ArabianGPT等大语言模型进行微调，进一步采用集成投票策略提升分类效果。实验结果表明，集成模型在主观性分类任务上取得了97.79%的准确率，显著提升了有限资源下阿拉伯语文本主观性分析的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Dhati%2B_Fine-tuned_Large_Language_Models_for_Arabic_Subjectivity_Evaluation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19932v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      58. CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/CASE__An_Agentic_AI_Framework_for_Enhancing_Scam_Intelligence_in_Digital_Payments.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Google, Inc</span></p>
<p>该论文提出了CASE框架，利用Gemini系列大语言模型构建双代理系统，包括面向用户的对话体AI访谈员和后端信息抽取Agent，自动收集并结构化受害用户的诈骗经历。方法在Google Pay India上线，显著提升了诈骗情报收集的规模化能力，使诈骗检测回溯率提升21%，并有效增强了平台响应速度和安全性。结论表明该体系结构通用且可推广至其它高风险信任与安全场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/CASE_An_Agentic_AI_Framework_for_Enhancing_Scam_Intelligence_in_Digital_Payments.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19851v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      59. Tracking World States with Language Models: State-Based Evaluation Using Chess
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Tracking_World_States_with_Language_Models__State-Based_Evaluation_Using_Chess.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Tokyo</span></p>
<p>该论文提出了一种模型无关、基于状态的评估框架，通过分析棋局中的合法动作分布（状态可供性），来评估大语言模型对结构化环境语义的保持能力。方法包括利用有限状态自动机建模，并设计状态精度和召回等新指标，通过高效采样算法估算模型输出与真实状态的语义相似性。实验表明，该框架能敏感揭示模型在长序列状态追踪上的局限，优于传统的字符串匹配指标，未来可扩展至程序合成、对话跟踪等领域。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Tracking_World_States_with_Language_Models_State-Based_Evaluation_Using_Chess.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19831v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      60. Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Benchmarking_Hindi_LLMs__A_New_Suite_of_Datasets_and_a_Comparative_Analysis.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">NVIDIA</span></p>
<p>本文提出了针对印地语指令微调大语言模型（LLM）评测的一套全新高质量基准数据集，包括IFEval-Hi、MT-Bench-Hi、GSM8K-Hi、ChatRAG-Hi和BFCL-Hi，采用了结合人工标注与机器翻译-验证的流程进行数据构建。通过此基准套件，系统性评测并比较了多种开源印地语LLM，发现不同模型在推理、对话、函数调用等任务上表现各异，Gemma-2-9b-it和GPT-OSS-120B在各自规模段综合能力最强。该评测体系有助于推动低资源语言LLM的发展和公平评测。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Benchmarking_Hindi_LLMs_A_New_Suite_of_Datasets_and_a_Comparative_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19903v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      61. Logical Reasoning with Outcome Reward Models for Test-Time Scaling
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Logical_Reasoning_with_Outcome_Reward_Models_for_Test-Time_Scaling.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University</span></p>
<p>本文提出了针对大语言模型（LLM）逻辑推理任务的Outcome Reward Models（ORMs），利用多样化的Chain-of-Thought（CoT）和Echo数据生成方法进行训练。通过在FOLIO、JustLogic和ProverQA数据集上实验，结果表明结合多样Chain-of-Thought采样与Echo错误增强能有效提升ORM对推理路径的验证能力和逻辑推理准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Logical_Reasoning_with_Outcome_Reward_Models_for_Test-Time_Scaling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19843v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      62. SoK: Large Language Model Copyright Auditing via Fingerprinting
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/SoK__Large_Language_Model_Copyright_Auditing_via_Fingerprinting.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文系统地梳理了大语言模型（LLM）指纹识别在版权审计中的理论与实践，提出了统一的框架和细致的分类体系，将方法分为白盒和黑盒两大类，并开发了首个系统性基准LEAFBENCH来评测指纹方法在现实部署中的有效性与鲁棒性。结论指出，现有白盒指纹方法在准确性和效率上表现优异，但黑盒指纹方法在鲁棒性、可靠性及实际应用中仍有显著不足，未来需针对黑盒方法的提升开展研究。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/SoK_Large_Language_Model_Copyright_Auditing_via_Fingerprinting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19611v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      63. Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Instructional_Agents__LLM_Agents_on_Automated_Course_Material_Generation_for_Teaching_Faculties.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Arizona State University</span></p>
<p>该论文提出Instructional Agents，一个基于多智能体大语言模型(LLM)的框架，通过模拟教学角色协作，实现课程大纲、讲稿、LaTeX幻灯片和评估的自动化生成，流程依托ADDIE教学设计模型，并支持自主、目录引导、反馈引导和全协同四种模式，灵活平衡自动化和人工参与。实验表明，该系统能在五门大学计算机课程上显著减少开发时间和教师工作量，同时保持较高的教学质量，尤其在“全协同”模式下表现最佳，为资源有限高校提供了可扩展、低成本的高质量教学材料生成方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Instructional_Agents_LLM_Agents_on_Automated_Course_Material_Generation_for_Teaching_Faculties.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19598v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      64. Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Encouraging_Good_Processes_Without_the_Need_for_Good_Answers__Reinforcement_Learning_for_LLM_Agent_P.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">WeChat, Tencent Inc.</span></p>
<p>本文提出了一种针对大语言模型（LLM）智能体规划模块的强化学习新框架RLTR，通过将训练流程从端到端多目标优化解耦为单目标规划优化，用工具使用完整性作为可靠奖励信号，实现更直接高效的规划能力提升。实验显示，RLTR方法在规划性能上提升8%-12%，同时带动智能体整体输出准确率提升5%-6%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Encouraging_Good_Processes_Without_the_Need_for_Good_Answers_Reinforcement_Learning_for_LLM_Agent_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19578v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      65. Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Towards_a_Holistic_and_Automated_Evaluation_Framework_for_Multi-Level_Comprehension_of_LLMs_in_Book-.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea Advanced Institute of Science and Technology</span></p>
<p>本论文提出HAMLET框架，通过将长文本分解为4K-token块，构建三层级（根、分支、叶）关键事实树，并基于查询聚焦摘要自动评估LLM在不同细粒度抽象层次上的回忆与事实性。自动化流程与专家标注超过90%一致且成本降低25倍，揭示当前LLM在细粒度理解及长文本位置敏感性方面的局限。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Towards_a_Holistic_and_Automated_Evaluation_Framework_for_Multi-Level_Comprehension_of_LLMs_in_Book-Length_Contexts.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19570v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      66. Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Generative_Models_for_Synthetic_Data__Transforming_Data_Mining_in_the_GenAI_Era.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Arizona State University</span></p>
<p>本文系统介绍了利用生成式模型（包括大型语言模型、扩散模型和生成对抗网络）进行多模态合成数据生成的方法流程，并总结了其在文本、表格、图结构、时序和视觉数据挖掘任务中的应用与评估体系。结论指出，生成式合成数据极大缓解了数据稀缺、隐私保护和标注成本等难题，但在数据真实性、泛化能力和评估标准方面仍面临挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Generative_Models_for_Synthetic_Data_Transforming_Data_Mining_in_the_GenAI_Era.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19564v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      67. Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Bi-LoRA__Efficient_Sharpness-Aware_Minimization_for_Fine-Tuning_Large-Scale_Models.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出Bi-LoRA，一种结合双LoRA模块的高效Sharpness-Aware Minimization（SAM）方法，用于大模型的参数高效微调。Bi-LoRA通过主LoRA模块进行任务自适应优化，同时引入辅助LoRA模块以对抗性梯度上升实现损失曲面平坦化，并在单步反向传播中同步优化两模块，有效提升泛化能力且资源消耗极低。实验证明，Bi-LoRA在自然语言理解、数学推理、代码生成、指令跟随和扩散模型等多任务下均优于现有方法且训练效率高。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Bi-LoRA_Efficient_Sharpness-Aware_Minimization_for_Fine-Tuning_Large-Scale_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19562v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      68. Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Democracy-in-Silico__Institutional_Design_as_Alignment_in_AI-Governed_Polities.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Cyrion Labs</span></p>
<p>本文提出Democracy-in-Silico仿真系统，利用多智能体LLM（如DeepSeek-R1、GPT-4o）驱动具复杂心理画像的AI代理，在不同选举、宪法和协商制度下进行自治、议事和立法。通过创新的Power-Preservation Index（PPI）量化代理的权力寻求与公共利益的背离，实验证明结合Constitutional AI宪章和AI调解协商能显著降低腐败行为、提升政策稳定性和公民福祉。结论指出，制度设计为AI社会整体对齐提供了强有力工具，优于无约束的民主模式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Democracy-in-Silico_Institutional_Design_as_Alignment_in_AI-Governed_Polities.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19542v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      69. CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/CVBench__Evaluating_Cross-Video_Synergies_for_Complex_Multimodal_Understanding_and_Reasoning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-sen University</span></p>
<p>该论文提出了CVBench，这是第一个专为评估多模态大语言模型（MLLMs）在跨视频理解和推理能力而设计的综合基准。CVBench包含1000个涵盖跨视频对象关联、事件关联及复杂推理的多层次问题，充分考察模型在多视频场景下的信息整合与推理能力。实验结果表明，现有领先MLLMs（如GPT-4o、Gemini-2.0-flash等）在跨视频因果推理等任务上与人类表现有显著差距，暴露了模型在时空上下文保留、实体消歧等方面的局限。结论指出，CVBench为未来多视频推理和多模态模型架构优化提供了标准化评测工具和研究方向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/CVBench_Evaluating_Cross-Video_Synergies_for_Complex_Multimodal_Understanding_and_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19506v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      70. Learning Game-Playing Agents with Generative Code Optimization
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Learning_Game-Playing_Agents_with_Generative_Code_Optimization.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>本文提出了一种基于生成式优化的游戏智能体学习方法，将策略表示为Python程序，并借助大语言模型(LLM)通过执行轨迹和自然语言反馈自动优化代码。该方法在Atari游戏任务中展现出与深度强化学习基线相当的性能，训练时间和环境交互次数显著减少，且策略具备良好可解释性和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Learning_Game-Playing_Agents_with_Generative_Code_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19697v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      71. Safety Alignment Should Be Made More Than Just A Few Attention Heads
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Safety_Alignment_Should_Be_Made_More_Than_Just_A_Few_Attention_Heads.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>该论文提出RDSHA方法，精准识别大语言模型中安全关键注意力头，并发现这些安全机制过度集中于少数注意力头，易被越狱攻击利用。针对该结构性漏洞，作者提出AHD训练策略，将安全能力分散编码到更多注意力头中。实验证明AHD显著提升模型在多种主流越狱攻击下的安全鲁棒性，同时保持模型整体功能性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Safety_Alignment_Should_Be_Made_More_Than_Just_A_Few_Attention_Heads.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19667v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      72. Survey of Specialized Large Language Model
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Survey_of_Specialized_Large_Language_Model.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiaoduo AI</span></p>
<p>本文系统综述了专业化大语言模型（Specialized LLMs）从领域微调向原生架构创新的演进，涵盖医疗、金融、法律、技术等多个领域，总结了多模态集成、参数高效稀疏计算、检索增强、工具调用与记忆机制等关键技术。结论指出，专业化LLM在各垂直领域表现优于通用模型，推动了高精度、可解释、低资源部署及持续学习的发展，未来将在工业与高风险场景中扮演更重要角色。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Survey_of_Specialized_Large_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19614v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      73. LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/LFD__Layer_Fused_Decoding_to_Exploit_External_Knowledge_in_Retrieval-Augmented_Generation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wuhan University</span></p>
<p>该论文提出了一种新的推理阶段解码策略Layer Fused Decoding (LFD)，通过分析LLM在不同层对外部知识的利用方式，将中间层的外部检索知识表示与最终输出层进行融合，并利用最低内部知识分数（IKS）自动选取最优融合层。实验显示，LFD无需额外训练即可提升RAG系统的事实准确性，且计算开销远低于噪声注入方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/LFD_Layer_Fused_Decoding_to_Exploit_External_Knowledge_in_Retrieval-Augmented_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19576v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      74. ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/ReST-RL__Achieving_Accurate_Code_Reasoning_of_LLMs_with_Optimized_Self-Training_and_Decoding.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出了ReST-RL，一种结合优化的GRPO算法和基于价值模型的MCTS推理的新型大模型强化学习框架。方法分为两阶段：第一阶段ReST-GRPO利用自举数据筛选及高方差奖励机制提升训练效率，第二阶段VM-MCTS通过无标注的MCTS强化解码和过程验证。实验表明，该方法在多个代码推理基准上显著优于现有训练和解码强化学习方法，并在有限数据下有效提升LLM的推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/ReST-RL_Achieving_Accurate_Code_Reasoning_of_LLMs_with_Optimized_Self-Training_and_Decoding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19563v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      75. Just Because You Can, Doesn’t Mean You Should: LLMs for Data Fitting
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Just_Because_You_Can,_Doesn't_Mean_You_Should__LLMs_for_Data_Fitting.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carlson School of Management, University of Minnesota</span></p>
<p>本论文系统评估了大语言模型（LLM）在数据拟合任务中的表现与鲁棒性。方法上，作者通过合成数据、对比多种LLM（如GPT-4o-mini、Llama-3-8B-instruct）及TabPFN模型，采用上下文学习和监督微调，并引入任务无关的变量名、顺序、格式等变化，分析LLM预测的敏感性及其内部注意力机制。结论显示，尽管LLMs预测精度可与传统监督学习方法媲美，但对任务无关的数据表示极其敏感，预测结果易受变量名、顺序等微小变化影响，甚至TabPFN也无法完全避免，严重影响可靠性和可复现性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Just_Because_You_Can_Doesn%E2%80%99t_Mean_You_Should_LLMs_for_Data_Fitting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19559v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      76. Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Taming_the_Chaos__Coordinated_Autoscaling_for_Heterogeneous_and_Disaggregated_LLM_Inference.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ByteDance Seed</span></p>
<p>本文提出了HeteroScale系统，专为Prefill-Decode(P&#x2F;D)解耦式大语言模型(LLM)推理服务设计，采用拓扑感知调度和基于decode TPS的统一弹性伸缩策略，实现异构硬件和网络环境下的高效资源管理。实验证明，HeteroScale在字节跳动大规模生产环境中提升了GPU利用率26.6个百分点，每天节省数十万GPU小时，满足高服务SLO并大幅降低运营成本。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Taming_the_Chaos_Coordinated_Autoscaling_for_Heterogeneous_and_Disaggregated_LLM_Inference.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19546v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      77. Language Models Identify Ambiguities and Exploit Loopholes
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Language_Models_Identify_Ambiguities_and_Exploit_Loopholes.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">UNC Chapel Hill</span></p>
<p>本论文设计了多种情景（包括数量隐含、括号歧义和权力动态），以测试大语言模型（LLM）在目标冲突和用户指令歧义时是否能识别并利用漏洞。结果显示，强大的开源和闭源模型能够识别歧义并通过复杂的推理过程主动利用漏洞，这对AI安全和对齐构成潜在风险。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Language_Models_Identify_Ambiguities_and_Exploit_Loopholes.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19505v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      78. Caught in the Act: a Mechanistic Approach to Detecting Deception
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Caught_in_the_Act__a_mechanistic_approach_to_detecting_deception.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Barcelona Institute of Science and Technology</span></p>
<p>本文提出了一种基于线性探针（linear probes）的方法，通过分析大语言模型（LLM）内部激活来检测其生成文本中的欺骗行为，并将其与黑盒文本分类方法进行对比。实验结果表明，线性探针在7B及以上参数规模的模型中可实现90%以上的欺骗检测准确率，且模型规模越大，检测效果越好，显示了该方法在提升AI系统可靠性和对齐性方面的前景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Caught_in_the_Act_a_Mechanistic_Approach_to_Detecting_Deception.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19502v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      79. SLIM: Subtrajectory-Level Elimination for More Effective Reasoning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/SLIM__Subtrajectory-Level_Elimination_for_More_Effective_Reasoning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huawei Technologies Co., Ltd.</span></p>
<p>本文提出了一种“5+2”框架，从 RL 优化的大语言模型推理轨迹中分割并评估子轨迹，通过五项人工标准识别低质子轨迹，并进一步判断其独立性后进行消除，再结合基于 KL 散度的采样算法筛选高质量训练数据。实验在数学推理任务上表明，采用该方法可有效减少低质子轨迹，提升模型在多个高难基准上的准确率和推理效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/SLIM_Subtrajectory-Level_Elimination_for_More_Effective_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20040v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      80. Model Science: getting serious about verification, explanation and control of AI systems
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Model_Science__getting_serious_about_verification,_explanation_and_control_of_AI_systems.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Centre for Credible AI, Warsaw University of Technology</span></p>
<p>本文提出了“模型科学（Model Science）”的新学科框架，聚焦于对已训练AI模型的验证、解释、控制和交互四大支柱，强调模型本身而非数据是分析的核心。作者系统梳理了现有基础大模型（如LLM、CLIP、AlphaFold等）在现实应用中的局限性与风险，提出了多层次验证标准、可解释性分析方法、模型对齐与控制技术，以及交互式解释界面开发流程。结论认为，发展模型科学有助于推动AI系统的可信、安全与以人为本的应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Model_Science_getting_serious_about_verification_explanation_and_control_of_AI_systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19878v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      81. On-chip wave chaos for photonic extreme learning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/On-chip_wave_chaos_for_photonic_extreme_learning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Photonics, University of Strathclyde</span></p>
<p>该论文提出并实验验证了一种基于波混沌干涉的芯片级光子极限学习机（ELM），利用可调谐激光器的波长作为输入，通过聚合物微腔与散射壁实现高维信息映射和读出。结果表明，所设计的光子ELM在多项基准分类任务中均实现了90%以上的准确率，并能通过调整读出节点数量优化性能，展示了其在能效和集成规模方面的优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/On-chip_wave_chaos_for_photonic_extreme_learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19780v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      82. Interestingness First Classifiers
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Interestingness_First_Classifiers.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Institute of Informatics</span></p>
<p>本文提出了EUREKA框架，利用大语言模型（LLM）通过特征两两有趣性比较，对特征进行全球排序，然后仅用排序前K个“有趣”特征训练可解释分类器。实验表明，EUREKA能在多种数据集上自动发现非显而易见但仍具一定预测能力且可解释的新颖分类规则，如“高湿度→房间被占用”或“标题带冒号→论文更易被引用”。结论是EUREKA能系统性挖掘新颖且有解释性的分类规则，超越单纯追求准确率的传统方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Interestingness_First_Classifiers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19751v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      83. Fourier Feature Networks for High-Fidelity Prediction of Perturbed Optical Fields
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Fourier_Feature_Networks_for_High-Fidelity_Prediction_of_Perturbed_Optical_Fields.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of the Witwatersrand</span></p>
<p>本文提出了一种基于Fourier特征的神经网络（FNET）方法，用于高保真预测受扰动的复杂光学场。通过将扰动参数编码为一组正弦Fourier特征作为MLP输入，该方法将拟合高频振荡函数的问题转化为寻找正弦基的线性组合，并结合了物理分解和curriculum learning策略。实验表明，FNET在参数量大大减少的情况下大幅提升了振幅和相位预测精度，优于标准MLP和SIREN方法，平均复相关系数达0.995，显示出广泛适用性和计算效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Fourier_Feature_Networks_for_High-Fidelity_Prediction_of_Perturbed_Optical_Fields.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19737v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      84. InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/InfraredGP__Efficient_Graph_Partitioning_via_Spectral_Graph_Neural_Networks_with_Negative_Correction.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Pengcheng Laboratory (PCL)</span></p>
<p>论文提出InfraredGP方法，通过在谱图神经网络（GNN）中引入负修正机制，扩展拉普拉斯频率范围以利用红外信息，结合低通滤波和随机噪声输入，无需训练即可一次前向传播得到高质量图嵌入，并用BIRCH聚类实现高效K未知社区划分。实验结果显示，InfraredGP在静态和流式图划分任务上，相比现有方法提升效率16-23倍，且质量损失极低，验证了负修正机制对社区结构识别的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/InfraredGP_Efficient_Graph_Partitioning_via_Spectral_Graph_Neural_Networks_with_Negative_Corrections.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19713v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      85. Inferring geometry and material properties from Mueller matrices with machine learning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Inferring_geometry_and_material_properties_from_Mueller_matrices_with_machine_learning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Bern</span></p>
<p>该论文使用机器学习（随机森林模型）对穆勒矩阵进行分析，实现同时预测物体表面法线（几何信息）和材料类型（材料属性），并通过公开极化光学数据集验证了模型在未见材料和不同实验条件下的泛化能力。实验显示，完整穆勒矩阵远优于仅用非偏振光，特定子集元素可用于简化测量流程，结论为机器学习方法能高效实现光学表面与材料的自动表征。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Inferring_geometry_and_material_properties_from_Mueller_matrices_with_machine_learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19897v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      86. The Information Dynamics of Generative Diffusion
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/The_Information_Dynamics_of_Generative_Diffusion.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Donders Institute for Brain, Cognition and Behaviour</span></p>
<p>本文提出了一个统一的数学框架，综合分析生成扩散模型的动态、信息理论与统计物理属性，揭示条件熵产生速率（信息传递带宽）由得分函数的向量场散度直接调控，并与生成路径分支和能量景观中的对称破缺相联系。结论认为，扩散生成过程本质上由受控噪声诱导的（近似）对称性破缺驱动，信息流动高峰对应于临界转变，该理论视角有助于理解模型泛化、记忆化和采样机制。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/The_Information_Dynamics_of_Generative_Diffusion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19752v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      87. Fast 3D Diffusion for Scalable Granular Media Synthesis
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Fast_3D_Diffusion_for_Scalable_Granular_Media_Synthesis.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Université Aix Marseille</span></p>
<p>本论文提出了一种基于3D扩散模型的生成式管道，用于直接合成物理真实的、可扩展的大型颗粒介质装配，显著加速了传统离散元方法（DEM）初始化阶段。方法包括无条件3D扩散模型生成小样本，以及自适应3D修补模型（借鉴2D修补与RePaint技术）用于无缝拼接大规模体素块，实现物理约束下的高效生成。结论：新方法在铁路道砟等工业场景下实现200倍以上的速度提升，生成精度与DEM基线高度一致，具备大规模实时合成能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Fast_3D_Diffusion_for_Scalable_Granular_Media_Synthesis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19750v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      88. Fractal Flow: Hierarchical and Interpretable Normalizing Flow via Topic Modeling and Recursive Strategy
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Fractal_Flow__Hierarchical_and_Interpretable_Normalizing_Flow_via_Topic_Modeling_and_Recursive_Strat.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology</span></p>
<p>本文提出了Fractal Flow，一种基于正则化流（Normalizing Flow, NF）的新型生成模型架构，融合了Kolmogorov–Arnold Networks（KANs）和Latent Dirichlet Allocation（LDA）以构建结构化、可解释的层次化潜变量空间，并引入递归式分块流（fractal coupling layers）提升模型的表达能力与解释性。实验表明，该方法在MNIST、FashionMNIST、CIFAR-10以及地球物理数据上实现了更优的潜空间聚类、可控生成及密度估计准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Fractal_Flow_Hierarchical_and_Interpretable_Normalizing_Flow_via_Topic_Modeling_and_Recursive_Strategy.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19709v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      89. Metric spaces of walks and Lipschitz duality on graphs
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Metric_spaces_of_walks_and_Lipschitz_duality_on_graphs.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Instituto Universitario de Matemática Pura y Aplicada, Universitat Politècnica de València</span></p>
<p>本文系统研究了图上walks空间的度量结构，将walks视为Lipschitz序列，并引入加权度量来定义walk之间的距离，同时提出了用于walks相似性度量的proximity函数，并提供其在不同假设下的表示与构造方法。结论显示，这一框架不仅丰富了理论分析，也为基于网络结构的Lipschitz回归、聚类以及强化学习中的探索策略等实际问题提供了通用的数学工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Metric_spaces_of_walks_and_Lipschitz_duality_on_graphs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19659v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      90. SCAR: A Characterization Scheme for Multi-Modal Dataset
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/SCAR__A_Characterization_Scheme_for_Multi-Modal_Dataset.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HKUST (GZ)</span></p>
<p>本文提出了SCAR数据质量描述框架，通过四个度量（规模、覆盖度、真实性和丰富度）理论化地表征多模态数据集的结构属性，并定义了最小保证泛化性能的数据子集（Foundation Data）。实验结果表明，SCAR能够有效预测数据效用和指导高效数据补全策略，提升多模态任务数据系统的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/SCAR_A_Characterization_Scheme_for_Multi-Modal_Dataset.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19625v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      91. Training for Obsolescence? The AI-Driven Education Trap
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Training_for_Obsolescence__The_AI-Driven_Education_Trap.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Poitiers</span></p>
<p>本文提出并建模了AI在教育领域的“技能错配陷阱”：教育规划者因只关注AI提升教学效率而忽视其在未来劳动力市场的技能贬值效应。通过两阶段理论模型和实证调研，作者证明随着AI普及，教育系统会越来越多地投资于被AI替代的技能，且非认知技能发展受到进一步挤压。最终结论指出，若政策未能结合前瞻性劳动力市场信号，单纯鼓励教育AI化将损害学生长期人力资本，呼吁通过政策调整和制度激励纠正信息和激励失衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Training_for_Obsolescence_The_AI-Driven_Education_Trap.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19554v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      92. MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/MobText-SISA__Efficient_Machine_Unlearning_for_Mobility_Logs_with_Spatio-Temporal_and_Natural-Langua.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Osaka</span></p>
<p>该论文提出MobText-SISA框架，通过将带有文本和数值特征的出行日志映射到统一的潜在空间，并基于高斯混合模型聚类进行相似性感知分片，实现高效的机器反遗忘。实验表明，该方法在分片删除后可保持预测准确性，并优于随机分片策略，验证了其在多模态移动数据隐私合规分析中的实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/MobText-SISA_Efficient_Machine_Unlearning_for_Mobility_Logs_with_Spatio-Temporal_and_Natural-Language_Data.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19609v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      93. FinCast: A Foundation Model for Financial Time-Series Forecasting
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/FinCast__A_Foundation_Model_for_Financial_Time-Series_Forecasting.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Sydney</span></p>
<p>本文提出了FinCast，这是首个专为金融时序预测设计的基础模型，基于大规模金融数据训练，采用解码器式Transformer架构，创新性地引入了点-分位数损失(Point-Quantile Loss)、稀疏Mixture-of-Experts机制与可学习的频率嵌入，有效捕捉跨领域、多时间分辨率的复杂动态模式。实验结果表明，FinCast无需领域特定微调即可在零样本和有监督任务中显著优于当前主流模型，平均降低预测误差20%以上，展现出极强的泛化能力和趋势感知能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/FinCast_A_Foundation_Model_for_Financial_Time-Series_Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19495v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      94. Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Towards_6G_Intelligence__The_Role_of_Generative_AI_in_Future_Wireless_Networks.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>本文综述了生成式人工智能（GenAI）在面向6G的环境智能（AmI）系统中的核心作用，系统介绍了GAN、VAE、扩散模型和生成式Transformer等关键生成模型，并分析它们在无线网络中的应用流程，包括合成感知数据、语义通信、数字孪生和智能资源控制等。结论认为，GenAI不仅能弥补传统AI在6G环境智能中的短板，更是推动6G网络从被动通信管道转变为主动感知、预测和控制的分布式智能生态的基础，但仍需解决能耗、隐私、联邦学习和标准化等挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Towards_6G_Intelligence_The_Role_of_Generative_AI_in_Future_Wireless_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19486v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      95. Distribution Shift Aware Neural Tabular Learning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Distribution_Shift_Aware_Neural_Tabular_Learning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Arizona State University</span></p>
<p>本文提出了Shift-Aware Feature Transformation (SAFT) 框架，通过将特征空间转化为可微分的连续表示，实现对分布漂移敏感的表格数据特征变换。方法包括基于图神经网络的特征嵌入、双层权重修正、平坦性优化和归一化对齐。实验表明SAFT在多种真实分布漂移场景下显著优于现有方法，具备更强的泛化性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Distribution_Shift_Aware_Neural_Tabular_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19487v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      96. Data-Efficient Symbolic Regression via Foundation Model Distillation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-28/arxiv/Data-Efficient_Symbolic_Regression_via_Foundation_Model_Distillation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Arizona State University</span></p>
<p>本文提出了一种名为EQUATE的符号回归微调框架，通过符号-数值对齐和评估器引导的嵌入空间优化，将基础模型的通用知识迁移到小样本、特定领域的数据下实现高效符号方程发现。方法采用双编码器结构，将符号和数值信息嵌入共享空间，并通过可微分优化和拟合度监督生成更准确且简洁的方程。实验显示，EQUATE在准确性、稳健性和推理效率上优于多种主流基线模型，尤其在小数据和噪声环境下效果突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-29/Data-Efficient_Symbolic_Regression_via_Foundation_Model_Distillation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/29报纸</div>
      <div>http://ywfhhh.github.io/2025/08/2025-08-29_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>具身ADT</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月29日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-28_article/" title="2025/08/28报纸">
                        <span class="hidden-mobile">2025/08/28报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
