

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="具身ADT">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;13报纸">
<meta property="og:url" content="http://ywfhhh.github.io/2025/08/2025-08-13_article/index.html">
<meta property="og:site_name" content="最新前沿论文分享">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/From_Imitation_to_Optimization_A_Comparative_Study_of_Offline_Learning_for_Autonomous_Driving.jpg">
<meta property="article:published_time" content="2025-08-13T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-19T08:49:23.868Z">
<meta property="article:author" content="具身ADT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/From_Imitation_to_Optimization_A_Comparative_Study_of_Offline_Learning_for_Autonomous_Driving.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/13报纸 - 最新前沿论文分享</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ywfhhh.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文库</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/13报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-13 00:00" pubdate>
          星期三, 八月 13日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          46k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/13报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/13报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-13<br>📄 <strong>发现论文数量</strong>：300  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-From-Imitation-to-Optimization-A-Comparative-Study-of-Offline-Learning-for-Autonomous-Driving"><a href="#1-From-Imitation-to-Optimization-A-Comparative-Study-of-Offline-Learning-for-Autonomous-Driving" class="headerlink" title="1. From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Imitation_to_Optimization__A_Comparative_Study_of_Offline_Learning_for_Autonomous_Driving.pdf">From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent Researcher</span></p>
<p>本文构建了一个完整的离线学习流水线，系统比较了基于模仿学习（Behavioral Cloning, BC）与保守Q学习（Conservative Q-Learning, CQL）在自动驾驶任务中的表现。通过在Waymo Open Motion Dataset大规模数据集上实验，作者发现即使使用结构化状态表示和Transformer模型，BC方法在长时序任务中因累计误差表现不佳，而CQL通过值函数的保守正则化显著提升了鲁棒性和成功率，碰撞率大幅下降，证明了离线强化学习对于自动驾驶策略学习的关键性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/From_Imitation_to_Optimization_A_Comparative_Study_of_Offline_Learning_for_Autonomous_Driving.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-Consensus-based-Decentralized-Multi-agent-Reinforcement-Learning-for-Random-Access-Network-Optimization"><a href="#2-Consensus-based-Decentralized-Multi-agent-Reinforcement-Learning-for-Random-Access-Network-Optimization" class="headerlink" title="2. Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Consensus-based_Decentralized_Multi-agent_Reinforcement_Learning_for_Random_Access_Network_Optimizat.pdf">Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Ohio State University</span></p>
<p>本文提出了一种基于共识机制的全分布式多智能体强化学习（MARL）方法，用于无线随机接入（RA）网络的MAC层优化。该方法在actor-critic结构下，仅通过本地奖励的局部信息交换实现全局收敛，显著减少通信开销，并在理论上证明了有限步收敛性。实验结果表明，该算法在无中心化训练的情况下，实现了与集中训练-分布式执行（CTDE）方法相当的网络吞吐量和公平性，同时大幅降低了系统开销。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Consensus-based_Decentralized_Multi-agent_Reinforcement_Learning_for_Random_Access_Network_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-Simulating-Biological-Intelligence-Active-Inference-with-Experiment-Informed-Generative-Model"><a href="#3-Simulating-Biological-Intelligence-Active-Inference-with-Experiment-Informed-Generative-Model" class="headerlink" title="3. Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Simulating_Biological_Intelligence__Active_Inference_with_Experiment-Informed_Generative_Model.pdf">Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Turner Institute for Brain and Mental Health, School of Psychological Sciences, Monash University</span></p>
<p>本论文提出了一种基于主动推断（Active Inference）的框架，利用实验启发的生成模型，模拟生物神经网络在Pong游戏环境下的决策行为。通过对不同决策机制（包括记忆驱动的反事实学习与基于规划的主动推断）的系统仿真，结果表明具备记忆机制的主动推断智能体在动态环境中的学习效率和适应性优于基于规划的方法，强调记忆在生物决策中的关键作用，并为可解释和生物启发AI的设计提供理论基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Simulating_Biological_Intelligence_Active_Inference_with_Experiment-Informed_Generative_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-AR-GRPO-Training-Autoregressive-Image-Generation-Models-via-Reinforcement-Learning"><a href="#4-AR-GRPO-Training-Autoregressive-Image-Generation-Models-via-Reinforcement-Learning" class="headerlink" title="4. AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AR-GRPO__Training_Autoregressive_Image_Generation_Models_via_Reinforcement_Learning.pdf">AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou Technology</span></p>
<p>本论文提出AR-GRPO方法，将在线强化学习（RL）训练集成到自回归（AR）图像生成模型中，通过引入Group Relative Policy Optimization（GRPO）算法，并设计多维奖励函数（包括文本-图像语义一致性、图像质量和真实感）来优化生成结果。实验结果表明，在类别条件和文本条件的图像生成任务中，RL优化显著提升了生成图像的质量和人类偏好，且可在不同模型规模和分辨率下有效泛化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AR-GRPO_Training_Autoregressive_Image_Generation_Models_via_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-Pushdown-Reward-Machines-for-Reinforcement-Learning"><a href="#5-Pushdown-Reward-Machines-for-Reinforcement-Learning" class="headerlink" title="5. Pushdown Reward Machines for Reinforcement Learning"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pushdown_Reward_Machines_for_Reinforcement_Learning.pdf">Pushdown Reward Machines for Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Utrecht Universiteit</span></p>
<p>该论文提出了Pushdown Reward Machines（pdRMs），这是基于确定性下推自动机的奖励机器扩展，可以编码由确定性上下文无关语言描述的时序任务。作者设计了两种pdRM策略（全栈访问和Top-k访问）、提出了最优性判别方法，并分析了表达能力与空间复杂度。实验表明，pdRMs在多个RL环境下样本效率高于计数奖励自动机（CRA），尤其是在任务可用上下文无关语言表达时，且Top-k策略能显著减小策略空间。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Pushdown_Reward_Machines_for_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-Offline-to-Online-Reinforcement-Learning-with-Classifier-Free-Diffusion-Generation"><a href="#6-Offline-to-Online-Reinforcement-Learning-with-Classifier-Free-Diffusion-Generation" class="headerlink" title="6. Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Offline-to-Online_Reinforcement_Learning_with_Classifier-Free_Diffusion_Generation.pdf">Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了一种新的数据增强方法：Classifier-Free Diffusion Generation (CFDG)，用于离线到在线强化学习（O2O RL）。方法利用无分类器引导的扩散模型对离线和在线数据进行高质量的联合生成，并通过重加权机制提升与在线策略一致的数据比例，提升训练稳定性和性能。实验证明，CFDG可无缝集成到主流O2O RL算法（如IQL、PEX、APL）中，在D4RL基准测试上平均提升15%性能，优于现有数据增强方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Offline-to-Online_Reinforcement_Learning_with_Classifier-Free_Diffusion_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-Sparsity-Driven-Plasticity-in-Multi-Task-Reinforcement-Learning"><a href="#7-Sparsity-Driven-Plasticity-in-Multi-Task-Reinforcement-Learning" class="headerlink" title="7. Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sparsity-Driven_Plasticity_in_Multi-Task_Reinforcement_Learning.pdf">Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Groningen</span></p>
<p>该论文系统性研究了稀疏化方法（特别是逐步幅值剪枝GMP和稀疏进化训练SET）对多任务强化学习（MTRL）中神经网络适应性的提升作用。通过在MTRL主流架构和标准基准上实验，作者发现稀疏化显著缓解了神经元休眠和表征塌缩等塑性退化现象，通常带来更优的多任务表现，尤其在MTPPO和MoE架构下稀疏网络优于密集网络，并与显式塑性干预或常规正则化方法相比具有竞争力。结论认为，动态稀疏化是一种强大且依赖上下文的工具，不仅提升MTRL系统的适应性与泛化能力，还带来计算效率和隐式正则化优势，适用于开发更具备弹性的多任务强化学习系统。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Sparsity-Driven_Plasticity_in_Multi-Task_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-Multi-level-Advantage-Credit-Assignment-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#8-Multi-level-Advantage-Credit-Assignment-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="8. Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-level_Advantage_Credit_Assignment_for_Cooperative_Multi-Agent_Reinforcement_Learning.pdf">Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mila - Quebec AI Institute</span></p>
<p>本文提出Multi-level Advantage Credit Assignment (MACA) 方法，通过多层次优势函数显式建模多智能体协作中的多级信用分配问题。MACA结合了个体、联合及相关代理子集的优势函数，并利用自注意力机制动态识别代理间相关性，显著提升了在复杂协作任务中的表现。实验结果表明，MACA在StarCraft等多智能体强化学习基准任务中优于现有方法，理论分析也证明了其收敛性和有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multi-level_Advantage_Credit_Assignment_for_Cooperative_Multi-Agent_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-Natural-Language-Driven-Viewpoint-Navigation-for-Volume-Exploration-via-Semantic-Block-Representation"><a href="#9-Natural-Language-Driven-Viewpoint-Navigation-for-Volume-Exploration-via-Semantic-Block-Representation" class="headerlink" title="9. Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Natural_Language-Driven_Viewpoint_Navigation_for_Volume_Exploration_via_Semantic_Block_Representatio.pdf">Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-sen University</span></p>
<p>本论文提出了一种结合自然语言交互和语义分块表示的体数据自动视角导航框架。方法上，采用CLIP模型对体数据渲染图像与文本描述进行对齐，通过分块编码捕捉局部结构，并用强化学习（PPO）根据语义奖励动态选择最符合用户意图的视角。结论显示，该方法有效提升了体数据探索的效率和可解释性，能准确反映用户查询，实现复杂场景下的快速语义导航。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Natural_Language-Driven_Viewpoint_Navigation_for_Volume_Exploration_via_Semantic_Block_Representation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-Stackelberg-Coupling-of-Online-Representation-Learning-and-Reinforcement-Learning"><a href="#10-Stackelberg-Coupling-of-Online-Representation-Learning-and-Reinforcement-Learning" class="headerlink" title="10. Stackelberg Coupling of Online Representation Learning and Reinforcement Learning"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Stackelberg_Coupling_of_Online_Representation_Learning_and_Reinforcement_Learning.pdf">Stackelberg Coupling of Online Representation Learning and Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fordham University</span></p>
<p>本文提出了SCORER框架，将表示学习（感知网络）与强化学习（控制网络）的交互建模为Stackelberg博弈，通过两阶段梯度下降算法实现感知网络对控制网络的预期优化。实验结果表明SCORER能够显著提升强化学习的采样效率和最终性能，无需复杂的辅助目标或架构，展示了博弈论结构化感知-控制协同的优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Stackelberg_Coupling_of_Online_Representation_Learning_and_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-Efficient-Reward-Identification-In-Max-Entropy-Reinforcement-Learning-with-Sparsity-and-Rank-Priors"><a href="#11-Efficient-Reward-Identification-In-Max-Entropy-Reinforcement-Learning-with-Sparsity-and-Rank-Priors" class="headerlink" title="11. Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Efficient_Reward_Identification_In_Max_Entropy_Reinforcement_Learning_with_Sparsity_and_Rank_Priors.pdf">Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Michigan</span></p>
<p>该论文提出了两种高效的时间变化奖励函数恢复方法：一种通过贪心区间划分算法利用奖励稀疏性先验，另一种通过低秩矩阵核范数松弛恢复特征基奖励结构，均在最大熵强化学习框架下实现。实验结果显示，所提方法在奖励恢复精度和迁移泛化能力上优于现有方法，且对策略估计噪声有较强鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Efficient_Reward_Identification_In_Max_Entropy_Reinforcement_Learning_with_Sparsity_and_Rank_Priors.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-Pentest-R1-Towards-Autonomous-Penetration-Testing-Reasoning-Optimized-via-Two-Stage-Reinforcement-Learning"><a href="#12-Pentest-R1-Towards-Autonomous-Penetration-Testing-Reasoning-Optimized-via-Two-Stage-Reinforcement-Learning" class="headerlink" title="12. Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pentest-R1__Towards_Autonomous_Penetration_Testing_Reasoning_Optimized_via_Two-Stage_Reinforcement_L.pdf">Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Laboratory of Cyberspace Security Defense, Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>Pentest-R1提出了一种基于大型语言模型的自动化渗透测试框架，通过两阶段强化学习流程提升模型推理与自我纠错能力：第一阶段利用500余条真实多步攻击流程离线强化学习建立攻击逻辑，第二阶段在交互式CTF环境中通过在线RL优化模型的自适应策略与错误恢复。实验表明，该方法在AutoPenBench、Cybench等基准上表现优异，超过绝大多数开源及部分专有SOTA模型，验证了两阶段强化学习的协同效应。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Pentest-R1_Towards_Autonomous_Penetration_Testing_Reasoning_Optimized_via_Two-Stage_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-Policy-Newton-methods-for-Distortion-Riskmetrics"><a href="#13-Policy-Newton-methods-for-Distortion-Riskmetrics" class="headerlink" title="13. Policy Newton methods for Distortion Riskmetrics"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Policy_Newton_methods_for_Distortion_Riskmetrics.pdf">Policy Newton methods for Distortion Riskmetrics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Indian Institute of Technology Madras</span></p>
<p>该论文提出了一种针对畸变风险度量（Distortion Riskmetric, DRM）的策略Newton方法（CRPN-DRM），在有限时域马尔可夫决策过程（MDP）中，通过最大化累计折扣奖励的DRM来求解风险敏感型最优策略。作者推导了DRM目标的策略Hessian定理，给出了基于样本轨迹的Hessian估计，并提出了带三次正则化的策略Newton算法，在理论上首次证明了其能以O(ϵ^-3.5)的样本复杂度收敛到风险敏感目标的ϵ-二阶驻点（SOSP），有效避免鞍点，实验验证了算法在经典强化学习环境中的优越性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Policy_Newton_methods_for_Distortion_Riskmetrics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-Pref-GUIDE-Continual-Policy-Learning-from-Real-Time-Human-Feedback-via-Preference-Based-Learning"><a href="#14-Pref-GUIDE-Continual-Policy-Learning-from-Real-Time-Human-Feedback-via-Preference-Based-Learning" class="headerlink" title="14. Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pref-GUIDE__Continual_Policy_Learning_from_Real-Time_Human_Feedback_via_Preference-Based_Learning.pdf">Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Duke University</span></p>
<p>该论文提出Pref-GUIDE，通过将实时人类标注的标量反馈转化为局部偏好数据，实现更稳健的奖励模型学习以持续训练RL策略。Pref-GUIDE Individual利用移动窗口采样和无偏好区间，将个体反馈变为偏好标签，Pref-GUIDE Voting通过聚合多评估者奖励模型获得群体共识偏好。实验显示，在三个视觉RL任务中，Pref-GUIDE显著优于标量反馈基线，并在复杂任务中超过专家设计的稠密奖励，实现了高质量人类反馈的高效利用和反馈质量差异的鲁棒性提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Pref-GUIDE_Continual_Policy_Learning_from_Real-Time_Human_Feedback_via_Preference-Based_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-Reinforcement-Learning-in-Vision-A-Survey"><a href="#15-Reinforcement-Learning-in-Vision-A-Survey" class="headerlink" title="15. Reinforcement Learning in Vision: A Survey"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Reinforcement_Learning_in_Vision__A_Survey.pdf">Reinforcement Learning in Vision: A Survey</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Singapore</span></p>
<p>该论文系统综述了视觉领域强化学习（Visual RL）的最新进展，涵盖多模态大语言模型、视觉生成、统一模型和视觉-语言-动作（VLA）模型四大方向，详细梳理了政策优化、奖励建模和评测体系，并提出了RLHF、GRPO、RLVR等主流技术路线。结论指出视觉RL正向统一架构、可扩展奖励监督和丰富基准演进，但仍面临数据效率、泛化性和奖励设计等挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Reinforcement_Learning_in_Vision_A_Survey.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-ReconDreamer-RL-Enhancing-Reinforcement-Learning-via-Diffusion-based-Scene-Reconstruction"><a href="#16-ReconDreamer-RL-Enhancing-Reinforcement-Learning-via-Diffusion-based-Scene-Reconstruction" class="headerlink" title="16. ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ReconDreamer-RL__Enhancing_Reinforcement_Learning_via_Diffusion-based_Scene_Reconstruction.pdf">ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">GigaAI</span></p>
<p>该论文提出ReconDreamer-RL框架，通过集成视频扩散先验与场景重建，提升端到端自动驾驶的强化学习训练。方法包括ReconSimulator（结合扩散模型实现高保真环境和运动学建模）、动态对手代理（DAA，自动生成极端交通场景）以及Cousin Trajectory Generator（CTG，提升轨迹多样性），显著缩小仿真与现实差距。实验表明，该框架在端到端自动驾驶任务上将碰撞率降低5倍，提升模型在复杂场景下的泛化与鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ReconDreamer-RL_Enhancing_Reinforcement_Learning_via_Diffusion-based_Scene_Reconstruction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-Pose-RFT-Enhancing-MLLMs-for-3D-Pose-Generation-via-Hybrid-Action-Reinforcement-Fine-Tuning"><a href="#17-Pose-RFT-Enhancing-MLLMs-for-3D-Pose-Generation-via-Hybrid-Action-Reinforcement-Fine-Tuning" class="headerlink" title="17. Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pose-RFT__Enhancing_MLLMs_for_3D_Pose_Generation_via_Hybrid_Action_Reinforcement_Fine-Tuning.pdf">Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CASIA</span></p>
<p>该论文提出Pose-RFT，一种专为3D人体姿态生成设计的多模态大语言模型(MLLMs)强化微调框架。方法将3D姿态生成建模为混合动作空间的强化学习问题，提出HyGRPO算法，对离散语言生成和连续姿态参数进行联合优化，并结合空间&#x2F;语义对齐等任务奖励，显著提升文本和图像到3D姿态的生成质量。实验结果表明，该方法在多个基准上超过现有姿态MLLMs，强化微调有效提升3D姿态生成能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Pose-RFT_Enhancing_MLLMs_for_3D_Pose_Generation_via_Hybrid_Action_Reinforcement_Fine-Tuning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-A-Tutorial-An-Intuitive-Explanation-of-Offline-Reinforcement-Learning-Theory"><a href="#18-A-Tutorial-An-Intuitive-Explanation-of-Offline-Reinforcement-Learning-Theory" class="headerlink" title="18. A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Tutorial__An_Intuitive_Explanation_of_Offline_Reinforcement_Learning_Theory.pdf">A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Alberta</span></p>
<p>本论文系统梳理了离线强化学习（Offline RL）的理论基础，围绕函数表示、数据覆盖等核心假设，分析了样本复杂度的下界、上界及难点，提出了Bootstrapping偏差累积、泛化难题与虚假数据干扰等关键挑战。结论指出，当前具备多步回报（如λ-return）、悲观估计等技术且满足单策略覆盖与Bellman完备性条件下，离线RL能实现多项式样本效率，但仍需针对弱假设开发新算法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Tutorial_An_Intuitive_Explanation_of_Offline_Reinforcement_Learning_Theory.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-Symmetry-Aware-Transformer-Training-for-Automated-Planning"><a href="#19-Symmetry-Aware-Transformer-Training-for-Automated-Planning" class="headerlink" title="19. Symmetry-Aware Transformer Training for Automated Planning"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Symmetry-Aware_Transformer_Training_for_Automated_Planning.pdf">Symmetry-Aware Transformer Training for Automated Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Linköping University</span></p>
<p>本文提出了一种对称感知的对比学习目标，结合架构改进，使Transformer模型能够显式处理自动规划任务中由于对象命名和状态目标排列引入的对称性。通过无位置编码的编码器、原子级嵌入和对比损失，模型在多个规划领域中展现出比现有PlanGPT方法更强的泛化和外推能力，尤其在未见过的大规模问题上表现更优，但在规模极大或结构复杂的任务上仍存在局限性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Symmetry-Aware_Transformer_Training_for_Automated_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-Robust-Reinforcement-Learning-over-Wireless-Networks-with-Homomorphic-State-Representations"><a href="#20-Robust-Reinforcement-Learning-over-Wireless-Networks-with-Homomorphic-State-Representations" class="headerlink" title="20. Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Robust_Reinforcement_Learning_over_Wireless_Networks_with_Homomorphic_State_Representations.pdf">Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Padova</span></p>
<p>本文提出了一种名为HR3L（Homomorphic Robust Remote Reinforcement Learning）的新框架，针对无线网络环境下远程强化学习（RRL）中因信道损失、延迟和带宽受限导致的观测不完全问题。HR3L采用基于马尔可夫决策过程同态（MDP Homomorphism）的状态表示，由发射端对环境状态进行特征压缩编码，接收端通过该紧凑表示进行策略学习，并采用异步模型更新以避免高通信和计算开销。实验表明，HR3L在多种网络受损情景下比现有方法更具鲁棒性和通信效率，显著减少带宽和延迟开销，同时保持甚至提升强化学习性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Robust_Reinforcement_Learning_over_Wireless_Networks_with_Homomorphic_State_Representations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-MORE-CLEAR-Multimodal-Offline-Reinforcement-learning-for-Clinical-notes-Leveraged-Enhanced-State-Representation"><a href="#21-MORE-CLEAR-Multimodal-Offline-Reinforcement-learning-for-Clinical-notes-Leveraged-Enhanced-State-Representation" class="headerlink" title="21. MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MORE-CLEAR__Multimodal_Offline_Reinforcement_learning_for_Clinical_notes_Leveraged_Enhanced_State_Re.pdf">MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Seoul National University Hospital</span></p>
<p>本文提出了MORE-CLEAR框架，将结构化数据（如检验结果和生命体征）与由大规模语言模型（LLM）处理的临床文本笔记进行多模态融合，通过上下文感知门控融合和双向跨模态注意力机制，增强患者状态表达，并用于离线强化学习优化脓毒症治疗策略。实验在MIMIC-III、MIMIC-IV和私有数据集上表明，该方法在存活率估计和策略性能上显著优于单一模态RL方法，提升了策略的泛化与稳健性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MORE-CLEAR_Multimodal_Offline_Reinforcement_learning_for_Clinical_notes_Leveraged_Enhanced_State_Representation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="22-DexFruit-Dexterous-Manipulation-and-Gaussian-Splatting-Inspection-of-Fruit"><a href="#22-DexFruit-Dexterous-Manipulation-and-Gaussian-Splatting-Inspection-of-Fruit" class="headerlink" title="22. DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DexFruit__Dexterous_Manipulation_and_Gaussian_Splatting_Inspection_of_Fruit.pdf">DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>本文提出了DexFruit框架，结合光学触觉传感和扩散式模仿学习策略，实现了机器人对草莓、西红柿和黑莓等易损水果的温和自主操作，显著降低损伤率。创新地引入FruitSplat方法，利用3D高斯Splatting技术对水果进行高精度三维重建和损伤定量分析，仅需摄像头即可实现可量化的外伤检测。实验结果显示DexFruit在抓取成功率和损伤最小化方面优于视觉或触觉单独策略，平均抓取成功率达92%，视觉损伤减少20%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DexFruit_Dexterous_Manipulation_and_Gaussian_Splatting_Inspection_of_Fruit.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-Communication-Efficient-Multi-Agent-3D-Detection-via-Hybrid-Collaboration"><a href="#23-Communication-Efficient-Multi-Agent-3D-Detection-via-Hybrid-Collaboration" class="headerlink" title="23. Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Communication-Efficient_Multi-Agent_3D_Detection_via_Hybrid_Collaboration.pdf">Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了HyComm，一个面向多智能体协作的通信高效LiDAR三维检测系统，核心方法为基于置信度和不确定性的混合协作消息打包（融合紧凑的感知输出和丰富的原始观测），并通过动态优先级选择关键数据实现自适应消息压缩。实验结果表明，HyComm在真实与仿真数据集上均显著优于以往方法，在极大降低通信量的同时提升检测性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Communication-Efficient_Multi-Agent_3D_Detection_via_Hybrid_Collaboration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-From-Data-to-Safe-Mobile-Robot-Navigation-An-Efficient-and-Modular-Robust-MPC-Design-Pipeline"><a href="#24-From-Data-to-Safe-Mobile-Robot-Navigation-An-Efficient-and-Modular-Robust-MPC-Design-Pipeline" class="headerlink" title="24. From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Data_to_Safe_Mobile_Robot_Navigation__An_Efficient_and_Modular_Robust_MPC_Design_Pipeline.pdf">From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Delft University of Technology</span></p>
<p>本文提出了一套高效且模块化的鲁棒模型预测控制（MPC）设计流程，结合闭环实验数据，通过迭代的移动地平线估计（MHE）方法自动量化非线性移动机器人模型的不确定性，并据此合成鲁棒的输出反馈MPC方案（ROHMPC）。实验证明，该流程能在存在结构性模型失配和测量噪声的情况下，保证机器人安全避障与约束满足，且递归可行，流程可复现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/From_Data_to_Safe_Mobile_Robot_Navigation_An_Efficient_and_Modular_Robust_MPC_Design_Pipeline.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-ForeSight-Multi-View-Streaming-Joint-Object-Detection-and-Trajectory-Forecasting"><a href="#25-ForeSight-Multi-View-Streaming-Joint-Object-Detection-and-Trajectory-Forecasting" class="headerlink" title="25. ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ForeSight__Multi-View_Streaming_Joint_Object_Detection_and_Trajectory_Forecasting.pdf">ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Toronto</span></p>
<p>该论文提出了ForeSight，一个面向自动驾驶的多视角联合3D目标检测与轨迹预测流式框架，采用统一Transformer架构和双向查询记忆机制，在检测与预测间无缝流转信息，消除追踪依赖。实验结果显示，ForeSight在nuScenes数据集上超越现有方法，提升检测与预测准确性和鲁棒性，特别适用于复杂动态驾驶场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ForeSight_Multi-View_Streaming_Joint_Object_Detection_and_Trajectory_Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-Model-Predictive-Control-for-Crowd-Navigation-via-Learning-Based-Trajectory-Prediction"><a href="#26-Model-Predictive-Control-for-Crowd-Navigation-via-Learning-Based-Trajectory-Prediction" class="headerlink" title="26. Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Model_Predictive_Control_for_Crowd_Navigation_via_Learning-Based_Trajectory_Prediction.pdf">Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bauhaus-Universitat Weimar</span></p>
<p>该论文提出将深度学习的Social-Implicit（SI）行人轨迹预测模型集成到模型预测控制（MPC）框架中，在真实机器人平台下实现对动态人群的安全导航。实验显示，SI-MPC系统在不同密度下显著提升了轨迹预测准确性、安全性和运动平滑性，并在闭环部署中展现了更为谨慎和适应性的导航行为，优于传统恒速模型。结论：SI-MPC方案有效增强了机器人在人群环境中的安全性和社会适应性，但高密度下需权衡效率与保守性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Model_Predictive_Control_for_Crowd_Navigation_via_Learning-Based_Trajectory_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-P3-Toward-Versatile-Embodied-Agents"><a href="#27-P3-Toward-Versatile-Embodied-Agents" class="headerlink" title="27. P3: Toward Versatile Embodied Agents"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/$_mathcal%7BP%7D^3$__Toward_Versatile_Embodied_Agents.pdf">P3: Toward Versatile Embodied Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southern University of Science and Technology</span></p>
<p>该论文提出了P3框架，通过统一的实时感知模块和动态多任务调度，实现主动环境感知、无反馈工具即插即用、以及基于优先级和依赖关系的多任务执行。实验表明，该框架显著提升了机器人在真实动态环境下的适应性和通用性，有效弥合了基准测试与实际部署的差距。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/P3_Toward_Versatile_Embodied_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-EGS-SLAM-RGB-D-Gaussian-Splatting-SLAM-with-Events"><a href="#28-EGS-SLAM-RGB-D-Gaussian-Splatting-SLAM-with-Events" class="headerlink" title="28. EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EGS-SLAM__RGB-D_Gaussian_Splatting_SLAM_with_Events.pdf">EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>该论文提出了一种融合事件相机数据、RGB图像与深度信息的3D高斯喷溅SLAM新框架（EGS-SLAM），通过显式建模曝光期间的相机连续轨迹，实现运动模糊场景下的事件与图像联合跟踪与高保真3D重建。引入可学习的相机响应函数及无事件损失，有效解决模态对齐与重建伪影问题，实验显示该方法在定位精度与重建质量上均优于现有GS-SLAM系统。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/EGS-SLAM_RGB-D_Gaussian_Splatting_SLAM_with_Events.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-Манипулятор-для-помощи-людям-с-ограниченными-возможностями"><a href="#29-Манипулятор-для-помощи-людям-с-ограниченными-возможностями" class="headerlink" title="29. Манипулятор для помощи людям с ограниченными возможностями"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Manipulator_for_people_with_limited_abilities.pdf">Манипулятор для помощи людям с ограниченными возможностями</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Московский государственный технический университет имени Н.Э. Баумана</span></p>
<p>该论文提出并实现了一个专为行动受限人士设计的四自由度机器人喂食臂RoboBK，包括机械结构设计、动力学与运动学分析、ROS平台下的控制系统开发，并集成了基于技术视觉的自动人脸跟踪算法（OpenCV Haar特征）。实验结果表明，系统能够实现自主喂食和精准定位，有效提升了目标群体的生活质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/%D0%9C%D0%B0%D0%BD%D0%B8%D0%BF%D1%83%D0%BB%D1%8F%D1%82%D0%BE%D1%80_%D0%B4%D0%BB%D1%8F_%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D0%B8_%D0%BB%D1%8E%D0%B4%D1%8F%D0%BC_%D1%81_%D0%BE%D0%B3%D1%80%D0%B0%D0%BD%D0%B8%D1%87%D0%B5%D0%BD%D0%BD%D1%8B%D0%BC%D0%B8_%D0%B2%D0%BE%D0%B7%D0%BC%D0%BE%D0%B6%D0%BD%D0%BE%D1%81%D1%82%D1%8F%D0%BC%D0%B8.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-Imaginative-World-Modeling-with-Scene-Graphs-for-Embodied-Agent-Navigation"><a href="#30-Imaginative-World-Modeling-with-Scene-Graphs-for-Embodied-Agent-Navigation" class="headerlink" title="30. Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Imaginative_World_Modeling_with_Scene_Graphs_for_Embodied_Agent_Navigation.pdf">Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Michigan, Ann Arbor</span></p>
<p>该论文提出SGImagineNav框架，通过符号化世界建模和场景图想象，结合视觉-语言模型（VLM）和大语言模型（LLM），主动推理和预测未观测环境区域，实现具备前瞻性的目标导航。实验结果显示，SGImagineNav在仿真和现实场景下均取得优异表现，提升导航成功率并支持跨房间、跨楼层的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Imaginative_World_Modeling_with_Scene_Graphs_for_Embodied_Agent_Navigation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-PANAMA-A-Network-Aware-MARL-Framework-for-Multi-Agent-Path-Finding-in-Digital-Twin-Ecosystems"><a href="#31-PANAMA-A-Network-Aware-MARL-Framework-for-Multi-Agent-Path-Finding-in-Digital-Twin-Ecosystems" class="headerlink" title="31. PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PANAMA__A_Network-Aware_MARL_Framework_for_Multi-Agent_Path_Finding_in_Digital_Twin_Ecosystems.pdf">PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huawei Canada Advanced Research Center</span></p>
<p>本文提出PANAMA算法，通过优先级不对称和网络感知的多智能体强化学习（MARL）方法，实现数字孪生生态中多智能体路径规划。采用集中训练-分布执行架构、异步actor-learner体系和动态优先级机制，提升训练效率和自适应任务执行能力。实验结果表明，在复杂环境下PANAMA在路径准确性、速度及可扩展性上优于现有方法，并显著改善网络通信质量，提升多智能体协作的鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/PANAMA_A_Network-Aware_MARL_Framework_for_Multi-Agent_Path_Finding_in_Digital_Twin_Ecosystems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-Energy-Efficient-Task-Offloading-in-UAV-Enabled-MEC-Using-a-Fully-Decentralized-Deep-Reinforcement-Learning-Approach"><a href="#32-Energy-Efficient-Task-Offloading-in-UAV-Enabled-MEC-Using-a-Fully-Decentralized-Deep-Reinforcement-Learning-Approach" class="headerlink" title="32. Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Energy_Efficient_Task_Offloading_in_UAV-Enabled_MEC_Using_a_Fully_Decentralized_Deep_Reinforcement_L.pdf">Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Iran University of Science and Technology</span></p>
<p>该论文提出了一种基于图注意力网络（GAT）与经验和参数共享PPO（EPS-PPO）的完全去中心化多智能体深度强化学习方法，用于优化无人机（UAV）在多接入边缘计算（MEC）中的轨迹和任务卸载决策。每架UAV只与邻近UAV局部通信并共享经验、参数，通过GAT整合邻域信息，提升对全局环境的认知，实现高效、鲁棒且可扩展的任务分配和能耗优化。实验结果表明该方法在能耗、任务完成率、收敛速度等方面优于主流半去中心化方法（如MADDPG），同时消除了中心节点带来的通信&#x2F;计算瓶颈与单点失效问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Energy_Efficient_Task_Offloading_in_UAV-Enabled_MEC_Using_a_Fully_Decentralized_Deep_Reinforcement_Learning_Approach.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-D3P-Dynamic-Denoising-Diffusion-Policy-via-Reinforcement-Learning"><a href="#33-D3P-Dynamic-Denoising-Diffusion-Policy-via-Reinforcement-Learning" class="headerlink" title="33. D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/D3P__Dynamic_Denoising_Diffusion_Policy_via_Reinforcement_Learning.pdf">D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出Dynamic Denoising Diffusion Policy（D3P），通过为不同关键性动作自适应分配扩散去噪步数，并引入轻量级自适应器，采用两层POMDP建模和强化学习联合优化扩散基政策与自适应器，实现推理效率和任务表现的平衡。实验表明D3P在八项机器人操作任务中较基线方法平均推理加速2.2倍且无性能损失，并在真实机器人上取得1.9倍加速。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/D3P_Dynamic_Denoising_Diffusion_Policy_via_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-Learning-a-Vision-Based-Footstep-Planner-for-Hierarchical-Walking-Control"><a href="#34-Learning-a-Vision-Based-Footstep-Planner-for-Hierarchical-Walking-Control" class="headerlink" title="34. Learning a Vision-Based Footstep Planner for Hierarchical Walking Control"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_a_Vision-Based_Footstep_Planner_for_Hierarchical_Walking_Control.pdf">Learning a Vision-Based Footstep Planner for Hierarchical Walking Control</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Pennsylvania</span></p>
<p>本文提出了一种视觉驱动的分层控制框架，用于双足机器人在复杂地形上的行走。方法包括使用深度摄像头生成地形高程图，通过强化学习（RL）训练的高层步态规划器结合基于ALIP（角动量线性倒立摆）模型的低维状态表示，输出步伐命令，并由低层操作空间控制器跟踪步伐轨迹。结果在仿真和真实硬件（Cassie机器人）上进行了验证，表现出在平地和不规则地形上的优异性能，但在阶梯等高度变化显著的地形中表现有限。结论指出：该分层结构和ALIP模型提升了训练效率和系统解释性，但在复杂环境及现实部署中存在局限，需要更具表达力的模型与更紧密的层间协同。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Learning_a_Vision-Based_Footstep_Planner_for_Hierarchical_Walking_Control.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-Triple-S-A-Collaborative-Multi-LLM-Framework-for-Solving-Long-Horizon-Implicative-Tasks-in-Robotics"><a href="#35-Triple-S-A-Collaborative-Multi-LLM-Framework-for-Solving-Long-Horizon-Implicative-Tasks-in-Robotics" class="headerlink" title="35. Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Triple-S__A_Collaborative_Multi-LLM_Framework_for_Solving_Long-Horizon_Implicative_Tasks_in_Robotics.pdf">Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northeastern University</span></p>
<p>本文提出Triple-S框架，通过多大语言模型（LLM）协作，采用简化、解决和总结三阶段流程，以提升机器人在长时序隐含任务中的策略代码生成准确率和鲁棒性。实验结果表明，Triple-S显著减少API参数、注释和顺序错误，在仿真和真实机器人环境中均实现了高成功率，并具备较强的通用性和稳定性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Triple-S_A_Collaborative_Multi-LLM_Framework_for_Solving_Long-Horizon_Implicative_Tasks_in_Robotics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-Multimodal-Spiking-Neural-Network-for-Space-Robotic-Manipulation"><a href="#36-Multimodal-Spiking-Neural-Network-for-Space-Robotic-Manipulation" class="headerlink" title="36. Multimodal Spiking Neural Network for Space Robotic Manipulation"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multimodal_Spiking_Neural_Network_for_Space_Robotic_Manipulation.pdf">Multimodal Spiking Neural Network for Space Robotic Manipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology</span></p>
<p>该论文提出了一种基于多模态输入和脉冲神经网络（SNN）的空间机器人臂自主控制框架，融合几何状态、触觉及语义信息，并采用双通道三阶段课程强化学习（CRL）训练策略。实验结果表明，在仿真空间站环境下，该方法在任务成功率与能耗效率方面显著优于传统人工神经网络（ANN）方法，适合空间机器人实际应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multimodal_Spiking_Neural_Network_for_Space_Robotic_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-MonoMPC-Monocular-Vision-Based-Navigation-with-Learned-Collision-Model-and-Risk-Aware-Model-Predictive-Control"><a href="#37-MonoMPC-Monocular-Vision-Based-Navigation-with-Learned-Collision-Model-and-Risk-Aware-Model-Predictive-Control" class="headerlink" title="37. MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MonoMPC__Monocular_Vision_Based_Navigation_with_Learned_Collision_Model_and_Risk-Aware_Model_Predict.pdf">MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Tartu</span></p>
<p>该论文提出了一种基于单目视觉的机器人导航方法，采用深度估计作为概率式碰撞模型的输入，通过联合训练碰撞模型和风险度量，实现风险感知的模型预测控制（MPC）。实验表明，该方法在复杂环境中显著减少碰撞率，超越NoMaD及ROS导航栈，能安全高效地引导机器人前进。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MonoMPC_Monocular_Vision_Based_Navigation_with_Learned_Collision_Model_and_Risk-Aware_Model_Predictive_Control.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-A-Hybrid-Force-Position-Strategy-for-Shape-Control-of-Deformable-Linear-Objects-With-Graph-Attention-Networks"><a href="#38-A-Hybrid-Force-Position-Strategy-for-Shape-Control-of-Deformable-Linear-Objects-With-Graph-Attention-Networks" class="headerlink" title="38. A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Hybrid_Force-Position_Strategy_for_Shape_Control_of_Deformable_Linear_Objects_With_Graph_Attention.pdf">A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了一种针对可变形线性物体（DLO）形状控制的混合力-位置策略，结合了基于力空间的状态轨迹规划与基于位置空间的模型预测控制（MPC），并通过显式动作编码、属性提取及图注意力网络（GAT）实现高精度动态建模。实验证明该方法能高效、稳定地分解大变形任务，避免局部最优，提升DLO操作的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Hybrid_Force-Position_Strategy_for_Shape_Control_of_Deformable_Linear_Objects_With_Graph_Attention_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-Integrating-Neurosymbolic-AI-in-Advanced-Air-Mobility-A-Comprehensive-Survey"><a href="#39-Integrating-Neurosymbolic-AI-in-Advanced-Air-Mobility-A-Comprehensive-Survey" class="headerlink" title="39. Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Integrating_Neurosymbolic_AI_in_Advanced_Air_Mobility__A_Comprehensive_Survey.pdf">Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland, Baltimore County</span></p>
<p>本论文系统综述了神经符号人工智能（Neurosymbolic AI）在先进空中交通（AAM）领域的应用，包括需求预测、飞机设计、实时空中交通管理等关键环节。方法层面，论文梳理了神经符号AI如何结合神经网络的自适应与符号推理能力，支持安全、合规、可解释的自动化决策，并分析了神经符号强化学习、智能仿真、预测性维护等具体技术流程。结论指出，尽管神经符号AI在动态优化和安全保障等方面展现出潜力，但仍面临规模化、鲁棒性及标准合规等挑战，需多学科协作推动其在AAM中可靠落地。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Integrating_Neurosymbolic_AI_in_Advanced_Air_Mobility_A_Comprehensive_Survey.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-ODYSSEY-Open-World-Quadrupeds-Exploration-and-Manipulation-for-Long-Horizon-Tasks"><a href="#40-ODYSSEY-Open-World-Quadrupeds-Exploration-and-Manipulation-for-Long-Horizon-Tasks" class="headerlink" title="40. ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ODYSSEY__Open-World_Quadrupeds_Exploration_and_Manipulation_for_Long-Horizon_Tasks.pdf">ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>ODYSSEY提出了一个统一的移动操作框架，结合层次化视觉-语言任务规划和强化学习驱动的全身控制策略，实现四足机器人在复杂地形下的长时序任务分解与精准执行。实验验证了系统在多样室内外场景的广泛泛化和强鲁棒性，成功实现了仿真到现实的迁移，提升了机器人在非结构化环境中的实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ODYSSEY_Open-World_Quadrupeds_Exploration_and_Manipulation_for_Long-Horizon_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-BeyondMimic-From-Motion-Tracking-to-Versatile-Humanoid-Control-via-Guided-Diffusion"><a href="#41-BeyondMimic-From-Motion-Tracking-to-Versatile-Humanoid-Control-via-Guided-Diffusion" class="headerlink" title="41. BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BeyondMimic__From_Motion_Tracking_to_Versatile_Humanoid_Control_via_Guided_Diffusion.pdf">BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>本文提出BeyondMimic，一个基于引导扩散（guided diffusion）的端到端人形机器人运动控制框架，包含高质量的运动跟踪管线和基于状态-动作的扩散策略。该方法实现了从大规模人体运动捕捉数据到真实机器人多样动态技能的迁移，并通过引导扩散实现零样本下多任务目标控制。实验结果表明，该系统在真实硬件上实现了包括导航、遥操作、避障等多种复杂任务，具备高度的通用性、鲁棒性和人类风格。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BeyondMimic_From_Motion_Tracking_to_Versatile_Humanoid_Control_via_Guided_Diffusion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-Vision-Based-Localization-and-LLM-based-Navigation-for-Indoor-Environments"><a href="#42-Vision-Based-Localization-and-LLM-based-Navigation-for-Indoor-Environments" class="headerlink" title="42. Vision-Based Localization and LLM-based Navigation for Indoor Environments"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Vision-Based_Localization_and_LLM-based_Navigation_for_Indoor_Environments.pdf">Vision-Based Localization and LLM-based Navigation for Indoor Environments</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Brown University</span></p>
<p>该论文提出了一种将视觉定位和大型语言模型（LLM）结合的室内导航方法。方法包括用经过两阶段微调的ResNet-50卷积神经网络实现手机摄像头视觉定位，并用ChatGPT结合预处理楼层平面图生成逐步导航指令，实验在实际办公楼走廊环境下进行，定位模块在重复特征和视野受限条件下准确率达96%，导航模块平均正确率为75%。结论表明，该方法无需专用硬件或信号基础设施，能在资源有限环境下实现可扩展的室内导航，但LLM在空间理解和实时性方面仍有待提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Vision-Based_Localization_and_LLM-based_Navigation_for_Indoor_Environments.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-COM-PACT-COMponent-Aware-Pruning-for-Accelerated-Control-Tasks-in-Latent-Space-Models"><a href="#43-COM-PACT-COMponent-Aware-Pruning-for-Accelerated-Control-Tasks-in-Latent-Space-Models" class="headerlink" title="43. COM-PACT: COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/COMponent-Aware_Pruning_for_Accelerated_Control_Tasks_in_Latent_Space_Models.pdf">COM-PACT: COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">RPTU University Kaiserslautern-Landau</span></p>
<p>本文提出了一种面向控制任务的组件感知结构化剪枝方法，通过优化每个模块的剪枝比例，实现神经网络控制器在嵌入式硬件上的高效部署，并以Lyapunov稳定性作为主要约束，确保控制系统压缩后的稳定性。实验结果表明，该方法可在保持系统稳定和性能的前提下，大幅压缩模型规模，并明确界定了安全压缩上限。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/COM-PACT_COMponent-Aware_Pruning_for_Accelerated_Control_Tasks_in_Latent_Space_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-AimBot-A-Simple-Auxiliary-Visual-Cue-to-Enhance-Spatial-Awareness-of-Visuomotor-Policies"><a href="#44-AimBot-A-Simple-Auxiliary-Visual-Cue-to-Enhance-Spatial-Awareness-of-Visuomotor-Policies" class="headerlink" title="44. AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AimBot__A_Simple_Auxiliary_Visual_Cue_to_Enhance_Spatial_Awareness_of_Visuomotor_Policies.pdf">AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Michigan</span></p>
<p>本文提出AimBot，一种轻量级视觉增强技术，通过在多视角RGB图像上叠加射线和瞄准镜准星，将末端执行器（EE）的空间状态显式编码为辅助视觉线索。AimBot利用深度图、摄像头外参及EE位姿生成空间关系提示，无需更改模型架构，计算开销极低，并可直接用于现有视-语言-动作（VLA）模型。实验结果表明，无论在仿真还是真实机器人操作中，AimBot均显著提升多种视觉运动策略的任务成功率，尤其在复杂和长时序任务中空间对齐能力增强明显。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AimBot_A_Simple_Auxiliary_Visual_Cue_to_Enhance_Spatial_Awareness_of_Visuomotor_Policies.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-GRID2GUIDE-A-ENABLED-SMALL-LANGUAGE-MODEL-FOR-INDOOR-NAVIGATION"><a href="#45-GRID2GUIDE-A-ENABLED-SMALL-LANGUAGE-MODEL-FOR-INDOOR-NAVIGATION" class="headerlink" title="45. GRID2GUIDE: A* ENABLED SMALL LANGUAGE MODEL FOR INDOOR NAVIGATION"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Grid2Guide__A__Enabled_Small_Language_Model_for_Indoor_Navigation.pdf">GRID2GUIDE: A* ENABLED SMALL LANGUAGE MODEL FOR INDOOR NAVIGATION</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Alabama</span></p>
<p>本文提出Grid2Guide混合导航框架，将A*搜索算法与小型语言模型（SLM）结合，实现从二值化占用栅格地图中高效计算最优路径，并通过SLM将压缩后的导航步骤转化为自然语言指令。实验表明，该方法无需专用硬件基础设施，在多种室内环境下能实时生成准确易懂的导航指导，显著提升速度与准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/GRID2GUIDE_A_ENABLED_SMALL_LANGUAGE_MODEL_FOR_INDOOR_NAVIGATION.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-PCHands-PCA-based-Hand-Pose-Synergy-Representation-on-Manipulators-with-N-DoF"><a href="#46-PCHands-PCA-based-Hand-Pose-Synergy-Representation-on-Manipulators-with-N-DoF" class="headerlink" title="46. PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PCHands__PCA-based_Hand_Pose_Synergy_Representation_on_Manipulators_with_N-DoF.pdf">PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Istituto Italiano di Tecnologia</span></p>
<p>本文提出PCHands，一种结合锚点描述格式（ADF）、条件变分自编码器（CVAE）与主成分分析（PCA）的方法，实现不同结构和自由度操作器（包括人手与多种机器人手）间通用的手部姿态协同表示。通过统一锚点定义、潜在空间对齐及端执行器帧的迭代优化，PCHands可高效表征和迁移多种操作器的手部动作，并显著提升强化学习操作任务的学习效率和一致性，支持跨操作器演示迁移及仿真到现实的直接部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/PCHands_PCA-based_Hand_Pose_Synergy_Representation_on_Manipulators_with_N-DoF.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space"><a href="#47-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space" class="headerlink" title="47. MolmoAct: Action Reasoning Models that can Reason in Space"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MolmoAct__Action_Reasoning_Models_that_can_Reason_in_Space.pdf">MolmoAct: Action Reasoning Models that can Reason in Space</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Allen Institute for AI</span></p>
<p>MolmoAct提出了一类全新的视觉-语言-动作模型（Action Reasoning Models, ARMs），通过深度感知、视觉轨迹推理和动作预测三阶段结构化流程，将感知、规划与控制有机结合。该方法首次引入可编辑空间轨迹和可解释行为，显著提升机器人的通用性、可解释性和任务适应能力。实验结果显示MolmoAct在仿真与现实环境中均超越多项主流基线，并在LIBERO等基准任务上取得最高成功率，同时在开源数据和模型方面推动了机器人基础模型的开放发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MolmoAct_Action_Reasoning_Models_that_can_Reason_in_Space.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-DETACH-Cross-domain-Learning-for-Long-Horizon-Tasks-via-Mixture-of-Disentangled-Experts"><a href="#48-DETACH-Cross-domain-Learning-for-Long-Horizon-Tasks-via-Mixture-of-Disentangled-Experts" class="headerlink" title="48. DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DETACH__Cross-domain_Learning_for_Long-Horizon_Tasks_via_Mixture_of_Disentangled_Experts.pdf">DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing University of Technology</span></p>
<p>该论文提出DETACH框架，通过类脑双流解耦架构，将环境感知与自身体态处理拆分为环境编码器和自编码器两路，分别独立优化并采用多策略融合机制，实现技能和环境的跨域泛化。实验表明DETACH在多步长任务中技能复用性和环境适应性均显著优于现有方法，成功率和效率提升明显。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DETACH_Cross-domain_Learning_for_Long-Horizon_Tasks_via_Mixture_of_Disentangled_Experts.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-Touch-Speaks-Sound-Feels-A-Multimodal-Approach-to-Affective-and-Social-Touch-from-Robots-to-Humans"><a href="#49-Touch-Speaks-Sound-Feels-A-Multimodal-Approach-to-Affective-and-Social-Touch-from-Robots-to-Humans" class="headerlink" title="49. Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Touch_Speaks,_Sound_Feels__A_Multimodal_Approach_to_Affective_and_Social_Touch_from_Robots_to_Humans.pdf">Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ghent University</span></p>
<p>本论文提出了一种多模态人机交互系统，结合5×5振动马达阵列与音频回放，实现机器人通过同步的触觉与听觉刺激向人类传递情感和社交手势。32名中国参与者对多模态、单一触觉和单一听觉条件下的10种情感和6种手势进行了识别实验。结果表明，多模态整合显著提升了情感与手势解码准确率，且触觉和听觉各自对不同情感表达具备优势，但单独的手势难以有效传达情感。该研究强调多感官整合对提升人机情感交互的重要性，并为未来社交型机器人设计提供了依据。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Touch_Speaks%2C_Sound_Feels_A_Multimodal_Approach_to_Affective_and_Social_Touch_from_Robots_to_Humans.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-AgentWorld-An-Interactive-Simulation-Platform-for-Scene-Construction-and-Mobile-Robotic-Manipulation"><a href="#50-AgentWorld-An-Interactive-Simulation-Platform-for-Scene-Construction-and-Mobile-Robotic-Manipulation" class="headerlink" title="50. AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AgentWorld__An_Interactive_Simulation_Platform_for_Scene_Construction_and_Mobile_Robotic_Manipulatio.pdf">AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent Robotics X</span></p>
<p>该论文提出了AgentWorld，一个集程序化场景生成、高保真物理仿真与多模态远程操作于一体的仿真平台，支持多种家庭环境下多机器人形态（轮式、类人等）的操作数据采集和多阶段任务标注。通过基于行为克隆、动作块变换器、扩散策略和视觉-语言-动作模型的模仿学习算法，实验验证了平台和数据集在机器人技能训练和sim-to-real迁移方面的有效性，尤其在复杂场景和多阶段任务上展现出较好泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AgentWorld_An_Interactive_Simulation_Platform_for_Scene_Construction_and_Mobile_Robotic_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-NeeCo-Image-Synthesis-of-Novel-Instrument-States-Based-on-Dynamic-and-Deformable-3D-Gaussian-Reconstruction"><a href="#51-NeeCo-Image-Synthesis-of-Novel-Instrument-States-Based-on-Dynamic-and-Deformable-3D-Gaussian-Reconstruction" class="headerlink" title="51. NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/NeeCo__Image_Synthesis_of_Novel_Instrument_States_Based_on_Dynamic_and_Deformable_3D_Gaussian_Recons.pdf">NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Leeds</span></p>
<p>本文提出NeeCo，一种基于动态3D高斯重建的外科手术器械图像合成方法，通过动态高斯模型和MLP学习器械运动，实现从有限数据生成新姿态和变形下的真实感图像，并自动生成分割与检测标注。实验结果显示，合成数据用于训练医疗神经网络在目标检测和分割任务上性能提升超过15%，显著优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/NeeCo_Image_Synthesis_of_Novel_Instrument_States_Based_on_Dynamic_and_Deformable_3D_Gaussian_Reconstruction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-Autonomous-Navigation-of-Cloud-Controlled-Quadcopters-in-Confined-Spaces-Using-Multi-Modal-Perception-and-LLM-Driven-High-Semantic-Reasoning"><a href="#52-Autonomous-Navigation-of-Cloud-Controlled-Quadcopters-in-Confined-Spaces-Using-Multi-Modal-Perception-and-LLM-Driven-High-Semantic-Reasoning" class="headerlink" title="52. Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Autonomous_Navigation_of_Cloud-Controlled_Quadcopters_in_Confined_Spaces_Using_Multi-Modal_Perceptio.pdf">Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Rajshahi University of Engineering and Technology</span></p>
<p>该论文提出一种云端智能感知系统，实现了无人机在GPS-denied室内环境下的自主导航。方法上，系统集成YOLOv11目标检测、Depth Anything V2单目深度估计、自研PCB（含IMU和6路ToF传感器）、视觉语言模型（VLM）和经LoRA微调的轻量大语言模型（LLM），通过多线程并行和云端推理，实现多模态信息融合、3D空间感知与高语义决策，并采用安全保护壳机制实现碰撞规避。实验结果显示，目标检测mAP50达到0.6，深度估计MAE为7.2cm，42次试验仅16次安全壳触发，端到端延迟低于1秒。该系统有效提升了无人机在狭小空间中的智能自主性和安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Autonomous_Navigation_of_Cloud-Controlled_Quadcopters_in_Confined_Spaces_Using_Multi-Modal_Perception_and_LLM-Driven_High_Semantic_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-Learning-Satellite-Attitude-Dynamics-with-Physics-Informed-Normalising-Flow"><a href="#53-Learning-Satellite-Attitude-Dynamics-with-Physics-Informed-Normalising-Flow" class="headerlink" title="53. Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_Satellite_Attitude_Dynamics_with_Physics-Informed_Normalising_Flow.pdf">Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Politecnico di Torino</span></p>
<p>本文提出了一种基于Real NVP归一化流神经网络和自注意机制的物理信息神经网络（PINN），用于学习卫星姿态动力学，并通过引入物理约束损失提升模型的泛化性和鲁棒性。实验结果表明，融合物理知识的模型在预测精度、模型稳定性和抗噪声能力方面均显著优于纯数据驱动方法，集成到MPC框架后控制表现提升最大可达42.86%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Learning_Satellite_Attitude_Dynamics_with_Physics-Informed_Normalising_Flow.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-SwarmVLM-VLM-Guided-Impedance-Control-for-Autonomous-Navigation-of-Heterogeneous-Robots-in-Dynamic-Warehousing"><a href="#54-SwarmVLM-VLM-Guided-Impedance-Control-for-Autonomous-Navigation-of-Heterogeneous-Robots-in-Dynamic-Warehousing" class="headerlink" title="54. SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SwarmVLM__VLM-Guided_Impedance_Control_for_Autonomous_Navigation_of_Heterogeneous_Robots_in_Dynamic_.pdf">SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Skolkovo Institute of Science and Technology</span></p>
<p>该论文提出了SwarmVLM系统，将无人机和地面机器人通过虚拟阻抗控制实现协作导航，并结合视觉语言模型（VLM）与检索增强生成（RAG）框架，根据环境变化自适应调整阻抗参数。实验验证显示该系统在动态仓储环境下导航成功率为92%，在良好光照条件下障碍物检测与参数选择准确率达80%，地面机器人能有效避开短障碍物并与无人机保持协作。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SwarmVLM_VLM-Guided_Impedance_Control_for_Autonomous_Navigation_of_Heterogeneous_Robots_in_Dynamic_Warehousing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-Risk-Map-As-Middleware-Towards-Interpretable-Cooperative-End-to-end-Autonomous-Driving-for-Risk-Aware-Planning"><a href="#55-Risk-Map-As-Middleware-Towards-Interpretable-Cooperative-End-to-end-Autonomous-Driving-for-Risk-Aware-Planning" class="headerlink" title="55. Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Risk_Map_As_Middleware__Towards_Interpretable_Cooperative_End-to-end_Autonomous_Driving_for_Risk-Awa.pdf">Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tongji University</span></p>
<p>本文提出Risk Map as Middleware (RiskMM)，一个面向协同自动驾驶的端到端可解释风险感知规划框架。方法上，RiskMM通过Transformer实现多智能体时空表征，利用注意力机制建模交互风险，并将其作为解释性中间件，结合可微分的学习型MPC模块实现风险感知轨迹规划，兼容物理约束和不同车辆类型。实验在V2XPnP-Seq真实数据集上表明，RiskMM在风险感知规划和可解释性上优于现有方法，表现出强健性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Risk_Map_As_Middleware_Towards_Interpretable_Cooperative_End-to-end_Autonomous_Driving_for_Risk-Aware_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-AIS-LLM-A-Unified-Framework-for-Maritime-Trajectory-Prediction-Anomaly-Detection-and-Collision-Risk-Assessment-with-Explainable-Forecasting"><a href="#56-AIS-LLM-A-Unified-Framework-for-Maritime-Trajectory-Prediction-Anomaly-Detection-and-Collision-Risk-Assessment-with-Explainable-Forecasting" class="headerlink" title="56. AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AIS-LLM__A_Unified_Framework_for_Maritime_Trajectory_Prediction,_Anomaly_Detection,_and_Collision_Ri.pdf">AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hanbat National University</span></p>
<p>该论文提出AIS-LLM框架，通过多尺度时间序列编码器、提示编码器与跨模态对齐模块，将AIS船舶导航数据与大型语言模型（LLM）结合，实现航迹预测、异常检测与碰撞风险评估三大任务，并生成可解释的自然语言分析报告。实验表明，AIS-LLM在精度、鲁棒性及解释能力上均优于现有方法，显著提升了海事交通管理的智能化与决策支持能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AIS-LLM_A_Unified_Framework_for_Maritime_Trajectory_Prediction%2C_Anomaly_Detection%2C_and_Collision_Risk_Assessment_with_Explainable_Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-End-to-End-Humanoid-Robot-Safe-and-Comfortable-Locomotion-Policy"><a href="#57-End-to-End-Humanoid-Robot-Safe-and-Comfortable-Locomotion-Policy" class="headerlink" title="57. End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/End-to-End_Humanoid_Robot_Safe_and_Comfortable_Locomotion_Policy.pdf">End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>该论文提出了一种端到端的类人机器人运动策略，利用原始时空LiDAR点云输入，通过Gated Recurrent Unit和多层感知机结构，直接输出电机控制命令。方法将安全性通过控制屏障函数(CBF)转化为约束马尔可夫决策过程(CMDP)中的代价项，并采用P3O算法在训练阶段严格约束安全与舒适性目标。实验证明该方法在复杂三维障碍环境中实现了高安全性、舒适性和高成功率的导航，并成功在仿真到实际机器人平台进行了迁移。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/End-to-End_Humanoid_Robot_Safe_and_Comfortable_Locomotion_Policy.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-LAURON-VI-A-Six-Legged-Robot-for-Dynamic-Walking"><a href="#58-LAURON-VI-A-Six-Legged-Robot-for-Dynamic-Walking" class="headerlink" title="58. LAURON VI: A Six-Legged Robot for Dynamic Walking"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LAURON_VI__A_Six-Legged_Robot_for_Dynamic_Walking.pdf">LAURON VI: A Six-Legged Robot for Dynamic Walking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">FZI Research Center for Information Technology</span></p>
<p>本文提出了LAURON VI，一种具备18个串联弹性关节的六足动态步态机器人平台，支持卡氏运动学、模型预测控制和强化学习三种步态控制方法，并进行了实验室和类火星环境测试。结果表明，LAURON VI在不同控制策略下展现出优异的灵活性、负载能力和适应复杂地形的鲁棒性，验证了六足机器人在动态行走和实际任务中的应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LAURON_VI_A_Six-Legged_Robot_for_Dynamic_Walking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-EMPATHIA-Multi-Faceted-Human-AI-Collaboration-for-Refugee-Integration"><a href="#59-EMPATHIA-Multi-Faceted-Human-AI-Collaboration-for-Refugee-Integration" class="headerlink" title="59. EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EMPATHIA__Multi-Faceted_Human-AI_Collaboration_for_Refugee_Integration.pdf">EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Texas A&amp;M University</span></p>
<p>该论文提出了EMPATHIA，一个以多智能体为核心的人机协作框架，专为难民安置与长期融合设计。方法包括三阶段模块（SEED, RISE, THRIVE）和情感、文化、伦理三视角的selector–validator架构，利用LLaMA-3模型实现高可解释性与透明度的人道主义推荐。实验在UN Kakuma数据集上（6,359名难民）验证了框架的高一致性（87.4%验证收敛）、公平性和可扩展性。结论表明该方法能在人道大规模场景下以多视角保护人类尊严，实现AI与人的协同决策。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/EMPATHIA_Multi-Faceted_Human-AI_Collaboration_for_Refugee_Integration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-GraphCoT-VLA-A-3D-Spatial-Aware-Reasoning-Vision-Language-Action-Model-for-Robotic-Manipulation-with-Ambiguous-Instructions"><a href="#60-GraphCoT-VLA-A-3D-Spatial-Aware-Reasoning-Vision-Language-Action-Model-for-Robotic-Manipulation-with-Ambiguous-Instructions" class="headerlink" title="60. GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/GraphCoT-VLA__A_3D_Spatial-Aware_Reasoning_Vision-Language-Action_Model_for_Robotic_Manipulation_wit.pdf">GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Noah’s Ark Lab, Huawei</span></p>
<p>该论文提出GraphCoT-VLA，一种结合3D Pose-Object图和多层次Chain-of-Thought（CoT）结构的端到端视觉-语言-动作模型，提升机器人在面对模糊指令和未知环境时的理解、推理与操作能力。实验表明，该模型在真实机器人复杂任务中显著提升了任务成功率和响应速度，具备良好的泛化性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/GraphCoT-VLA_A_3D_Spatial-Aware_Reasoning_Vision-Language-Action_Model_for_Robotic_Manipulation_with_Ambiguous_Instructions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-Breaking-Down-and-Building-Up-Mixture-of-Skill-Based-Vision-and-Language-Navigation-Agents"><a href="#61-Breaking-Down-and-Building-Up-Mixture-of-Skill-Based-Vision-and-Language-Navigation-Agents" class="headerlink" title="61. Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Breaking_Down_and_Building_Up__Mixture_of_Skill-Based_Vision-and-Language_Navigation_Agents.pdf">Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Michigan State University</span></p>
<p>该论文提出SkillNav，一种模块化的视觉-语言导航（VLN）框架，将导航任务分解为可解释的原子技能（如方向调整、垂直移动、区域识别等），并通过大规模VLM路由器在每一步动态选择最合适的技能代理。SkillNav在R2R和GSA-R2R等基准上取得了新的SOTA表现，显著提升了对新环境和指令的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Breaking_Down_and_Building_Up_Mixture_of_Skill-Based_Vision-and-Language_Navigation_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="62-Sensory-Robustness-through-Top-Down-Feedback-and-Neural-Stochasticity-in-Recurrent-Vision-Models"><a href="#62-Sensory-Robustness-through-Top-Down-Feedback-and-Neural-Stochasticity-in-Recurrent-Vision-Models" class="headerlink" title="62. Sensory Robustness through Top-Down Feedback and Neural Stochasticity in Recurrent Vision Models"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sensory_robustness_through_top-down_feedback_and_neural_stochasticity_in_recurrent_vision_models.pdf">Sensory Robustness through Top-Down Feedback and Neural Stochasticity in Recurrent Vision Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Tübingen</span></p>
<p>本论文提出在卷积循环神经网络（ConvRNN）中结合顶层反馈（top-down feedback）和类神经随机性（通过dropout模拟）的方法，系统比较了有无反馈及有无dropout对图像分类、速度-准确性权衡和对噪声与对抗攻击的鲁棒性影响。研究发现，仅有顶层反馈无法提升鲁棒性，唯有顶层反馈与dropout协同，模型在速度-准确性权衡和对分布外噪声、对抗扰动的鲁棒性上显著优于其他结构；其机制包括通过反馈和dropout约束表征分布于低维流形上、稳定表征动力学、提升分布外环境下信息编码效率。结论是顶层反馈与神经噪声的协同可极大提升感知鲁棒性，兼顾泛化与表征稳定性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Sensory_Robustness_through_Top-Down_Feedback_and_Neural_Stochasticity_in_Recurrent_Vision_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-TeSO-Representing-and-Compressing-3D-Point-Cloud-Scenes-with-Textured-Surfel-Octree"><a href="#63-TeSO-Representing-and-Compressing-3D-Point-Cloud-Scenes-with-Textured-Surfel-Octree" class="headerlink" title="63. TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TeSO__Representing_and_Compressing_3D_Point_Cloud_Scenes_with_Textured_Surfel_Octree.pdf">TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">New York University</span></p>
<p>本文提出了Textured Surfel Octree (TeSO)，一种新型3D点云场景高效表示与压缩方法。通过将点云转化为八叉树结构的表面元素，并为每个表面元素分配高频纹理贴图，TeSO在保持渲染质量的同时显著减少表示所需的几何基元数量。配合基于学习的熵模型和标准视频&#x2F;点云编码的压缩方案，TeSO在不同分辨率下比主流基线方法（G-PCC、B2P等）实现了更优的比特率-渲染质量权衡。实验结果表明，该方法支持高效解码和实时渲染，适合3D视觉内容流媒体应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TeSO_Representing_and_Compressing_3D_Point_Cloud_Scenes_with_Textured_Surfel_Octree.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-BrainATCL-Adaptive-Temporal-Brain-Connectivity-Learning-for-Functional-Link-Prediction-and-Age-Estimation"><a href="#64-BrainATCL-Adaptive-Temporal-Brain-Connectivity-Learning-for-Functional-Link-Prediction-and-Age-Estimation" class="headerlink" title="64. BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BrainATCL__Adaptive_Temporal_Brain_Connectivity_Learning_for_Functional_Link_Prediction_and_Age_Esti.pdf">BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">New Jersey Institute of Technology</span></p>
<p>本文提出BrainATCL，一种无监督的动态自适应脑连接性学习框架，利用基于新颖性指数的自适应时序窗口和GINE-Mamba2骨干网络，对fMRI动态脑网络进行时空嵌入建模，并融合左&#x2F;右脑半球与子网络等生物学先验信息。实验证明在功能连接预测与年龄估计任务上，BrainATCL在跨会话和同会话设定中均优于现有静态与动态图神经网络模型，泛化能力强。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BrainATCL_Adaptive_Temporal_Brain_Connectivity_Learning_for_Functional_Link_Prediction_and_Age_Estimation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-Large-Language-Model-Evaluated-Stand-alone-Attention-Assisted-Graph-Neural-Network-with-Spatial-and-Structural-Information-Interaction-for-Precise-Endoscopic-Image-Segmentation"><a href="#65-Large-Language-Model-Evaluated-Stand-alone-Attention-Assisted-Graph-Neural-Network-with-Spatial-and-Structural-Information-Interaction-for-Precise-Endoscopic-Image-Segmentation" class="headerlink" title="65. Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Large_Language_Model_Evaluated_Stand-alone_Attention-Assisted_Graph_Neural_Network_with_Spatial_and_.pdf">Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong</span></p>
<p>本文提出FOCUS-Med模型，融合空间和结构图神经网络（Dual-GCN）、位置融合的自注意力模块（LFSA）以及加权快速归一化融合机制（WFNF），实现多尺度特征的高效集成与增强。通过引入LLM（如GPT-4o）进行专家对齐的定性评价，模型在多个公开医学图像分割数据集上获得了多项指标上的最新性能，显示出临床应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Large_Language_Model_Evaluated_Stand-alone_Attention-Assisted_Graph_Neural_Network_with_Spatial_and_Structural_Information_Interaction_for_Precise_Endoscopic_Image_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-TerraMAE-Learning-Spatial-Spectral-Representations-from-Hyperspectral-Earth-Observation-Data-via-Adaptive-Masked-Autoencoders"><a href="#66-TerraMAE-Learning-Spatial-Spectral-Representations-from-Hyperspectral-Earth-Observation-Data-via-Adaptive-Masked-Autoencoders" class="headerlink" title="66. TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TerraMAE__Learning_Spatial-Spectral_Representations_from_Hyperspectral_Earth_Observation_Data_via_Ad.pdf">TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Colorado State University</span></p>
<p>本文提出了TerraMAE，一种针对高光谱卫星图像的自监督预训练框架，采用基于光谱相似性的自适应通道分组和融合结构相似性与光谱信息散度的多目标重构损失，提升空间-光谱特征嵌入。实验结果表明，TerraMAE在影像重建精度和多种下游地理任务（作物分类、土地覆盖、土壤质地预测）中均优于标准MAE和ResNet-50，证明其空间-光谱表示迁移能力强。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TerraMAE_Learning_Spatial-Spectral_Representations_from_Hyperspectral_Earth_Observation_Data_via_Adaptive_Masked_Autoencoders.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-OctreeNCA-Single-Pass-184-MP-Segmentation-on-Consumer-Hardware"><a href="#67-OctreeNCA-Single-Pass-184-MP-Segmentation-on-Consumer-Hardware" class="headerlink" title="67. OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/OctreeNCA__Single-Pass_184_MP_Segmentation_on_Consumer_Hardware.pdf">OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University of Darmstadt</span></p>
<p>本文提出了OctreeNCA架构，通过将神经元元胞自动机（NCA）与八叉树结构结合，实现了高分辨率医学图像与视频的高效单步分割。关键技术包括多级NCA推理、隐藏通道上采样、定制CUDA实现，大幅降低了训练与推理时的显存需求。实验表明，OctreeNCA在病理、放射与手术视频分割任务中，参数量低、推理速度快，单步可处理184MP病理切片或1分钟手术视频，显著优于UNet和Transformer等主流方法。结论：OctreeNCA可在低成本硬件上实现高质量大规模医学图像分割，缩小不同地区医疗AI应用的硬件鸿沟。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/OctreeNCA_Single-Pass_184_MP_Segmentation_on_Consumer_Hardware.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-TADoc-Robust-Time-Aware-Document-Image-Dewarping"><a href="#68-TADoc-Robust-Time-Aware-Document-Image-Dewarping" class="headerlink" title="68. TADoc: Robust Time-Aware Document Image Dewarping"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TADoc__Robust_Time-Aware_Document_Image_Dewarping.pdf">TADoc: Robust Time-Aware Document Image Dewarping</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>本文提出了一种时间感知的文档图像去扭曲方法TADoc，将去扭曲建模为多步动态过程，通过时间步嵌入与轻量级网络结构，逐步恢复文档图像的几何变形。实验显示，TADoc在多个公开基准数据集上表现出强鲁棒性和优越性能，同时引入Document Layout Similarity (DLS)新指标，补充传统OCR评估，能全面衡量去扭曲效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TADoc_Robust_Time-Aware_Document_Image_Dewarping.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-WeatherDiffusion-Weather-Guided-Diffusion-Model-for-Forward-and-Inverse-Rendering"><a href="#69-WeatherDiffusion-Weather-Guided-Diffusion-Model-for-Forward-and-Inverse-Rendering" class="headerlink" title="69. WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/WeatherDiffusion__Weather-Guided_Diffusion_Model_for_Forward_and_Inverse_Rendering.pdf">WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>该论文提出WeatherDiffusion，一种针对自动驾驶场景下复杂天气和光照条件的前向与逆向渲染扩散框架。方法以Stable Diffusion 3.5为基础，结合自主构建的WeatherSynthetic和WeatherReal数据集，通过引入Intrinsic map-aware attention（MAA）模块，实现了对场景的几何、材质和光照等属性的高质量分解和可控编辑。实验表明，该方法在多项基准上超越现有方法，提升了恶劣天气下目标检测与分割等下游任务的鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/WeatherDiffusion_Weather-Guided_Diffusion_Model_for_Forward_and_Inverse_Rendering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-Beyond-Frequency-Seeing-Subtle-Cues-Through-the-Lens-of-Spatial-Decomposition-for-Fine-Grained-Visual-Classification"><a href="#70-Beyond-Frequency-Seeing-Subtle-Cues-Through-the-Lens-of-Spatial-Decomposition-for-Fine-Grained-Visual-Classification" class="headerlink" title="70. Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Beyond_Frequency__Seeing_Subtle_Cues_Through_the_Lens_of_Spatial_Decomposition_for_Fine-Grained_Visu.pdf">Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Anhui University</span></p>
<p>本文提出了Subtle-Cue Oriented Perception Engine (SCOPE)，通过空间域自适应滤波模块（Subtle Detail Extractor和Salient Semantic Refiner），联合增强细粒度图像的局部细节和全局语义，实现更灵活的多尺度特征融合。实验表明，该方法在四个主流细粒度视觉分类数据集上取得了新的SOTA效果，显著提升了细粒度分类的准确率和细节保持能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Beyond_Frequency_Seeing_Subtle_Cues_Through_the_Lens_of_Spatial_Decomposition_for_Fine-Grained_Visual_Classification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-CannyEdit-Selective-Canny-Control-and-Dual-Prompt-Guidance-for-Training-free-Image-Editing"><a href="#71-CannyEdit-Selective-Canny-Control-and-Dual-Prompt-Guidance-for-Training-free-Image-Editing" class="headerlink" title="71. CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-free Image Editing"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CannyEdit__Selective_Canny_Control_and_Dual-Prompt_Guidance_for_Training-Free_Image_Editing.pdf">CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-free Image Editing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>该论文提出了CannyEdit，一种无需训练的区域图像编辑方法，通过Selective Canny Control（在用户指定区域屏蔽Canny ControlNet的结构引导、保留未编辑区域细节）和Dual-Prompt Guidance（结合局部和全局文本提示）实现目标区域的精确编辑和未编辑区域的高保真保持。实验表明，CannyEdit在文本遵循性与上下文保真度间取得更优平衡，编辑无缝性显著提升，用户难以分辨AI编辑痕迹，优于现有同类方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CannyEdit_Selective_Canny_Control_and_Dual-Prompt_Guidance_for_Training-free_Image_Editing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-MMReID-Bench-Unleashing-the-Power-of-MLLMs-for-Effective-and-Versatile-Person-Re-identification"><a href="#72-MMReID-Bench-Unleashing-the-Power-of-MLLMs-for-Effective-and-Versatile-Person-Re-identification" class="headerlink" title="72. MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MMReID-Bench__Unleashing_the_Power_of_MLLMs_for_Effective_and_Versatile_Person_Re-identification.pdf">MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">East China Normal University</span></p>
<p>本文提出了MMReID-Bench，多任务多模态的benchmark，专为多模态大语言模型（MLLMs）在行人重识别（Person ReID）任务设计，涵盖10种主流ReID任务和20710个多模态查询及图库图像，支持RGB、热成像、红外、素描、文本描述等多模态输入。作者系统评测了15种主流MLLMs，并设计了统一的对话模板和任务先验融合方法，揭示了MLLMs在大多数ReID任务中表现优异，但在热成像和红外等跨模态任务上仍有较大挑战。结论指出，MMReID-Bench推动了多模态基础模型在实际复杂场景中的应用与发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MMReID-Bench_Unleashing_the_Power_of_MLLMs_for_Effective_and_Versatile_Person_Re-identification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-A-Simple-yet-Powerful-Instance-Aware-Prompting-Framework-for-Training-free-Camouflaged-Object-Segmentation"><a href="#73-A-Simple-yet-Powerful-Instance-Aware-Prompting-Framework-for-Training-free-Camouflaged-Object-Segmentation" class="headerlink" title="73. A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Simple_yet_Powerful_Instance-Aware_Prompting_Framework_for_Training-free_Camouflaged_Object_Segmen.pdf">A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai University</span></p>
<p>该论文提出了一个训练自由的Instance-Aware Prompting Framework（IAPF），通过多模态大语言模型生成图像特定前景&#x2F;背景标签，利用Grounding DINO生成精确的实例级框提示，结合单前景多背景点采样策略（SFMBP），并通过自一致性投票机制选取最优分割结果，实现了对伪装物体的细粒度实例分割，无需任何训练或额外标注。实验结果显示IAPF在多个COS基准上显著超越现有训练自由方法，尤其在多实例场景中表现优异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Simple_yet_Powerful_Instance-Aware_Prompting_Framework_for_Training-free_Camouflaged_Object_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-MV-CoRe-Multimodal-Visual-Conceptual-Reasoning-for-Complex-Visual-Question-Answering"><a href="#74-MV-CoRe-Multimodal-Visual-Conceptual-Reasoning-for-Complex-Visual-Question-Answering" class="headerlink" title="74. MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MV-CoRe__Multimodal_Visual-Conceptual_Reasoning_for_Complex_Visual_Question_Answering.pdf">MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shaanxi University of Technology</span></p>
<p>该论文提出MV-CoRe模型，通过深度融合预训练视觉大模型（VLM）、语言大模型（LLM）的全局嵌入与细粒度语义感知视觉特征（包括目标检测和场景图表征），利用创新的多模态融合Transformer，实现复杂视觉问答任务中的深层跨模态推理。实验表明，MV-CoRe在GQA、A-OKVQA和OKVQA等复杂VQA基准上均优于主流LVLM基线，显著提升准确率和推理深度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MV-CoRe_Multimodal_Visual-Conceptual_Reasoning_for_Complex_Visual_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-HiMat-DiT-based-Ultra-High-Resolution-SVBRDF-Generation"><a href="#75-HiMat-DiT-based-Ultra-High-Resolution-SVBRDF-Generation" class="headerlink" title="75. HiMat: DiT-based Ultra-High Resolution SVBRDF Generation"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HiMat__DiT-based_Ultra-High_Resolution_SVBRDF_Generation.pdf">HiMat: DiT-based Ultra-High Resolution SVBRDF Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nankai University</span></p>
<p>该论文提出HiMat，一种基于Diffusion Transformer（DiT）的高效4K分辨率SVBRDF（空间变化双向反射分布函数）生成框架，可根据文本提示直接生成超高分辨率物理材质。方法上，HiMat采用高效线性注意力的DiT骨干，并引入CrossStitch模块实现多通道间高效一致性，还结合SWT（Stationary Wavelet Transform）频域监督增强细节。实验证明HiMat在细节保真度、结构一致性和推理效率上均优于现有方法，并支持消费级硬件快速生成。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/HiMat_DiT-based_Ultra-High_Resolution_SVBRDF_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-Spatio-Temporal-Conditional-Diffusion-Models-for-Forecasting-Future-Multiple-Sclerosis-Lesion-Masks-Conditioned-on-Treatments"><a href="#76-Spatio-Temporal-Conditional-Diffusion-Models-for-Forecasting-Future-Multiple-Sclerosis-Lesion-Masks-Conditioned-on-Treatments" class="headerlink" title="76. Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Spatio-Temporal_Conditional_Diffusion_Models_for_Forecasting_Future_Multiple_Sclerosis_Lesion_Masks_.pdf">Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">McGill University</span></p>
<p>本文提出了一种基于ControlNet的时空条件扩散模型，能够结合多模态MRI和治疗信息，预测多发性硬化症患者未来新发和扩大的T2病灶掩膜。实验结果表明，该模型在六种不同疗效的治疗下，能准确预测病灶位置、数量及分类，并可生成不同治疗条件下的反事实医学图像，具有临床应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Spatio-Temporal_Conditional_Diffusion_Models_for_Forecasting_Future_Multiple_Sclerosis_Lesion_Masks_Conditioned_on_Treatments.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-Can-Multitask-Learning-Enhance-Model-Explainability"><a href="#77-Can-Multitask-Learning-Enhance-Model-Explainability" class="headerlink" title="77. Can Multitask Learning Enhance Model Explainability?"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Can_Multitask_Learning_Enhance_Model_Explainability_.pdf">Can Multitask Learning Enhance Model Explainability?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kaiserslautern-Landau University</span></p>
<p>该论文提出了一种将多模态遥感数据中的部分模态转为多任务学习中的辅助任务（而非输入），通过分析主任务与辅助任务之间的预测误差相关性，实现对模型行为的内在解释。实验证明在 CropYield、Benge 和 TreeSAT 三个遥感数据集上，该方法在不损失主任务性能的同时，减少了推理阶段对额外数据的需求，并能够通过任务间误差分析提升模型可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Can_Multitask_Learning_Enhance_Model_Explainability_.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-Intrinsic-Explainability-of-Multimodal-Learning-for-Crop-Yield-Prediction"><a href="#78-Intrinsic-Explainability-of-Multimodal-Learning-for-Crop-Yield-Prediction" class="headerlink" title="78. Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Intrinsic_Explainability_of_Multimodal_Learning_for_Crop_Yield_Prediction.pdf">Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">RPTU Kaiserslautern-Landau</span></p>
<p>本论文提出了一种以Transformer为核心的多模态学习框架，集成了卫星多光谱时序、气象时间序列、地形高程和土壤属性四种模态，实现农作物亚田块级产量预测，并系统对比了Attention Rollout (AR)、Generic Attention (GA)和Shapley Value Sampling (SVS)三种特征归因方法。实验表明，Transformer在精度和推理速度上优于卷积和循环网络，AR方法在特征归因的稳健性和一致性上表现最佳，结论指出Transformer的自注意力机制能提升多模态模型的内在可解释性，为农业遥感等数据密集领域的可信模型部署提供理论和实证基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Intrinsic_Explainability_of_Multimodal_Learning_for_Crop_Yield_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-eMotions-A-Large-Scale-Dataset-and-Audio-Visual-Fusion-Network-for-Emotion-Analysis-in-Short-form-Videos"><a href="#79-eMotions-A-Large-Scale-Dataset-and-Audio-Visual-Fusion-Network-for-Emotion-Analysis-in-Short-form-Videos" class="headerlink" title="79. eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/eMotions__A_Large-Scale_Dataset_and_Audio-Visual_Fusion_Network_for_Emotion_Analysis_in_Short-form_V.pdf">eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong University</span></p>
<p>该论文提出了eMotions数据集，包含27996条短视频及六类情感注释，通过多阶段人工标注和人员分配策略提升数据质量。作者设计了AV-CANet音视频融合网络，采用Video Swin-Transformer和多级局部-全局融合模块，结合EP-CE损失函数，显著提升短视频情感识别效果，并在多个公开数据集上取得优异表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/eMotions_A_Large-Scale_Dataset_and_Audio-Visual_Fusion_Network_for_Emotion_Analysis_in_Short-form_Videos.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-VSI-Visual–Subtitle-Integration-for-Keyframe-Selection-to-Enhance-Long-Video-Understanding"><a href="#80-VSI-Visual–Subtitle-Integration-for-Keyframe-Selection-to-Enhance-Long-Video-Understanding" class="headerlink" title="80. VSI: Visual–Subtitle Integration for Keyframe Selection to Enhance Long Video Understanding"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VSI__Visual_Subtitle_Integration_for_Keyframe_Selection_to_enhance_Long_Video_Understanding.pdf">VSI: Visual–Subtitle Integration for Keyframe Selection to Enhance Long Video Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AI Thrust, HKUST(GZ)</span></p>
<p>本文提出了一种名为VISUAL–SUBTITLE INTEGRATION（VSI）的多模态关键帧检索方法，结合了基于视觉对象检测（YOLO-World）与字幕内容语义匹配（基于对比句向量），通过双流融合机制引导视频采样分布，提升长视频理解中的关键帧定位与下游问答性能。实验表明，VSI方法在LONGVIDEOBENCH等数据集上显著提升关键帧定位准确率和问答任务表现，具有训练无关、轻量高效、易集成等优点。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/VSI_Visual%E2%80%93Subtitle_Integration_for_Keyframe_Selection_to_Enhance_Long_Video_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-Low-Rank-Expert-Merging-for-Multi-Source-Domain-Adaptation-in-Person-Re-Identification"><a href="#81-Low-Rank-Expert-Merging-for-Multi-Source-Domain-Adaptation-in-Person-Re-Identification" class="headerlink" title="81. Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Low-Rank_Expert_Merging_for_Multi-Source_Domain_Adaptation_in_Person_Re-Identification.pdf">Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ETS Montreal</span></p>
<p>该论文提出了一种面向行人再识别的多源无监督域自适应方法SAGE-reID，核心技术为利用低秩适配器(LoRA)对各源模型在目标域进行高效微调，并通过轻量化门控网络动态融合多个LoRA专家，实现高效跨域知识转移。实验结果表明，该方法在Market-1501、DukeMTMC-reID和MSMT17等多个基准数据集上，在准确率和效率上均优于现有主流多源域自适应方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Low-Rank_Expert_Merging_for_Multi-Source_Domain_Adaptation_in_Person_Re-Identification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-Dual-Resolution-Residual-Architecture-with-Artifact-Suppression-for-Melanocytic-Lesion-Segmentation"><a href="#82-Dual-Resolution-Residual-Architecture-with-Artifact-Suppression-for-Melanocytic-Lesion-Segmentation" class="headerlink" title="82. Dual-Resolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DualResolution_Residual_Architecture_with_Artifact_Suppression_for_Melanocytic_Lesion_Segmentation.pdf">Dual-Resolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">SNAIT Research</span></p>
<p>本文提出了一种用于皮肤镜图像黑色素瘤分割的双分辨率残差网络架构，结合全分辨率边界保留流和多尺度上下文聚合流，通过边界感知残差连接和通道注意力模块实现精确分割。实验表明，该方法在公开数据集上明显提升了分割边界的准确性和鲁棒性，优于标准FCN基线，具备临床应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Dual-Resolution_Residual_Architecture_with_Artifact_Suppression_for_Melanocytic_Lesion_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-Mode-Aware-Non-Linear-Tucker-Autoencoder-for-Tensor-based-Unsupervised-Learning"><a href="#83-Mode-Aware-Non-Linear-Tucker-Autoencoder-for-Tensor-based-Unsupervised-Learning" class="headerlink" title="83. Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Mode-Aware_Non-Linear_Tucker_Autoencoder_for_Tensor-based_Unsupervised_Learning.pdf">Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了一种新的模式感知非线性Tucker自动编码器（MA-NTAE），通过递归Pick-Unfold-Encode-Fold操作，将经典Tucker分解与现代自动编码器相结合，实现高阶张量的灵活模式编码。实验结果显示，MA-NTAE在张量压缩和聚类任务中，相比传统向量化自动编码器（DAE）和线性Tucker网络（TFNN），在多视角图像和视频数据上的重建精度更高，训练效率更优，能更好地提取和保留样本结构。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Mode-Aware_Non-Linear_Tucker_Autoencoder_for_Tensor-based_Unsupervised_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-FoundBioNet-A-Foundation-Based-Model-for-IDH-Genotyping-of-Glioma-from-Multi-Parametric-MRI"><a href="#84-FoundBioNet-A-Foundation-Based-Model-for-IDH-Genotyping-of-Glioma-from-Multi-Parametric-MRI" class="headerlink" title="84. FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FoundBioNet__A_Foundation-Based_Model_for_IDH_Genotyping_of_Glioma_from_Multi-Parametric_MRI.pdf">FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tehran University of Medical Sciences</span></p>
<p>该论文提出了FoundBioNet模型，基于SWIN-UNETR架构，结合肿瘤感知特征编码（TAFE）和跨模态差异（CMD）模块，从多参数MRI中无创预测胶质瘤IDH突变状态。模型在多中心大样本数据集上表现出优越的泛化能力和准确率，显著超过传统卷积和Transformer基线方法，提升了诊断准确性和解释性，有望集成于临床流程。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/FoundBioNet_A_Foundation-Based_Model_for_IDH_Genotyping_of_Glioma_from_Multi-Parametric_MRI.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-FormCoach-Lift-Smarter-Not-Harder"><a href="#85-FormCoach-Lift-Smarter-Not-Harder" class="headerlink" title="85. FormCoach: Lift Smarter, Not Harder"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FormCoach__Lift_Smarter,_Not_Harder.pdf">FormCoach: Lift Smarter, Not Harder</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Pennsylvania</span></p>
<p>本文提出FormCoach系统，利用视觉-语言模型（VLM）通过摄像头实时对用户健身动作与专家演示进行对比分析，生成个性化纠正反馈。作者构建并公开了1700对专业标注的视频数据集，系统性评测多种VLM模型在运动姿态纠正任务上的表现。结果显示，当前VLM能提供高可操作性但在精准识别和描述细微姿态错误方面与人类教练仍有差距，未来结合3D关节估计和可穿戴传感器有望提升准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/FormCoach_Lift_Smarter%2C_Not_Harder.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-AURA-A-Fine-Grained-Benchmark-and-Decomposed-Metric-for-Audio-Visual-Reasoning"><a href="#86-AURA-A-Fine-Grained-Benchmark-and-Decomposed-Metric-for-Audio-Visual-Reasoning" class="headerlink" title="86. AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AURA__A_Fine-Grained_Benchmark_and_Decomposed_Metric_for_Audio-Visual_Reasoning.pdf">AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland, College Park</span></p>
<p>本文提出AURA基准，专为评估音频-视觉大语言模型（AV-LLMs）和全模态语言模型（OLMs）在六类细粒度认知任务上的跨模态推理能力，包括因果推理、音色&#x2F;音高、节奏&#x2F;同步、不可回答性、隐性干扰和表演者技能分析。其创新点在于AuraScore评估指标，将模型推理分解为事实一致性与核心推理两部分，从而揭示现有模型在正确率与推理质量间的显著差距。结论显示，主流AV-LLM在答案准确率高但推理一致性较低，强调了更细致的多模态推理评价的必要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AURA_A_Fine-Grained_Benchmark_and_Decomposed_Metric_for_Audio-Visual_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-Freeze-and-Reveal-Exposing-Modality-Bias-in-Vision-Language-Models"><a href="#87-Freeze-and-Reveal-Exposing-Modality-Bias-in-Vision-Language-Models" class="headerlink" title="87. Freeze and Reveal: Exposing Modality Bias in Vision-Language Models"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Freeze_and_Reveal__Exposing_Modality_Bias_in_Vision-Language_Models.pdf">Freeze and Reveal: Exposing Modality Bias in Vision-Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IIIT Hyderabad</span></p>
<p>本文提出了一种针对视觉-语言模型中性别偏见的模态定向去偏框架，利用Counterfactual Data Augmentation（CDA）、Task Vector和新提出的Degree of Stereotypicality（DoS）及其数据增强方法（DAUDoS）对视觉和文本编码器分别进行去偏，并在VisoGender基准和自己整理的性别标注数据集上评估。实验表明，CLIP模型的视觉编码器偏见更重，PaliGemma2的文本编码器偏见更重，所提方法能有效降低偏见且保持模型性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Freeze_and_Reveal_Exposing_Modality_Bias_in_Vision-Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-CLUE-Leveraging-Low-Rank-Adaptation-to-Capture-Latent-Uncovered-Evidence-for-Image-Forgery-Localization"><a href="#88-CLUE-Leveraging-Low-Rank-Adaptation-to-Capture-Latent-Uncovered-Evidence-for-Image-Forgery-Localization" class="headerlink" title="88. CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CLUE__Leveraging_Low-Rank_Adaptation_to_Capture_Latent_Uncovered_Evidence_for_Image_Forgery_Localiza.pdf">CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Guangdong Provincial Key Laboratory of Intelligent Information Processing, Shenzhen University</span></p>
<p>本论文提出CLUE框架，通过低秩适应（LoRA）高效微调Stable Diffusion 3（SD3）和Segment Anything Model（SAM），将生成模型的内部生成过程用于图像伪造定位。方法采用SD3的Rectified Flow机制对潜变量引入多级噪声，放大伪造区域的微小不一致性，并融合SAM提取的空间语义特征，最终实现像素级伪造区域检测。实验显示，CLUE在多项公开基准上显著优于现有方法，具备更强泛化性和对社交网络压缩等后处理攻击的鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CLUE_Leveraging_Low-Rank_Adaptation_to_Capture_Latent_Uncovered_Evidence_for_Image_Forgery_Localization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-VA-Blueprint-Uncovering-Building-Blocks-for-Visual-Analytics-System-Design"><a href="#89-VA-Blueprint-Uncovering-Building-Blocks-for-Visual-Analytics-System-Design" class="headerlink" title="89. VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VA-Blueprint__Uncovering_Building_Blocks_for_Visual_Analytics_System_Design.pdf">VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Chicago</span></p>
<p>该论文提出了一种结合人工与大语言模型（LLM）的方法，系统性地从101篇城市视觉分析（VA）系统论文中抽取、归纳并结构化其核心构建模块，形成多层次知识库（VA-Blueprint）。方法包括人工手动分析建立初步代码本体，随后用LLM批量抽取其余论文组件，最终生成可查询、层次化、可视化的系统组件蓝图。结论显示，VA-Blueprint不仅揭示了VA系统复杂性随时间增长的趋势，还为系统设计、复用和分析提供结构化基础，LLM在抽取环节表现良好但需专家校正。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/VA-Blueprint_Uncovering_Building_Blocks_for_Visual_Analytics_System_Design.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-ForensicsSAM-Toward-Robust-and-Unified-Image-Forgery-Detection-and-Localization-Resisting-to-Adversarial-Attack"><a href="#90-ForensicsSAM-Toward-Robust-and-Unified-Image-Forgery-Detection-and-Localization-Resisting-to-Adversarial-Attack" class="headerlink" title="90. ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ForensicsSAM__Toward_Robust_and_Unified_Image_Forgery_Detection_and_Localization_Resisting_to_Advers.pdf">ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen University</span></p>
<p>该论文提出了ForensicsSAM，一种针对图像伪造检测与定位（IFDL）任务的统一框架，通过在SAM主干中注入共享伪造专家、对抗检测器和自适应对抗专家，形成三阶段训练流程，实现了对真实、伪造及对抗样本的图像级检测和像素级定位。实验显示，ForensicsSAM在多种数据集和对抗攻击下均具备优异的鲁棒性和SOTA伪造检测性能，有效提升了安全性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ForensicsSAM_Toward_Robust_and_Unified_Image_Forgery_Detection_and_Localization_Resisting_to_Adversarial_Attack.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-Tight-Bounds-for-Schrodinger-Potential-Estimation-in-Unpaired-Image-to-Image-Translation-Problems"><a href="#91-Tight-Bounds-for-Schrodinger-Potential-Estimation-in-Unpaired-Image-to-Image-Translation-Problems" class="headerlink" title="91. Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Tight_Bounds_for_Schr%C3%B6dinger_Potential_Estimation_in_Unpaired_Image-to-Image_Translation_Problems.pdf">Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HSE University</span></p>
<p>本文针对无配对样本的图像到图像翻译问题，提出了一种基于Schrödinger桥和Ornstein-Uhlenbeck过程的生成建模方法，通过最小化耦合的KL散度，在高概率下给出了经验风险最小化器的泛化误差收敛界，并实现了近乎最优的收敛速率。实验证明该方法在生成建模和图像风格迁移任务中稳定优于传统方法，能够更好地保持源分布的关键属性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Tight_Bounds_for_Schr%C3%B6dinger_Potential_Estimation_in_Unpaired_Image-to-Image_Translation_Problems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-Invert4TVG-A-Temporal-Video-Grounding-Framework-with-Inversion-Tasks-for-Enhanced-Action-Understanding"><a href="#92-Invert4TVG-A-Temporal-Video-Grounding-Framework-with-Inversion-Tasks-for-Enhanced-Action-Understanding" class="headerlink" title="92. Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Invert4TVG__A_Temporal_Video_Grounding_Framework_with_Inversion_Tasks_for_Enhanced_Action_Understand.pdf">Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiamen University</span></p>
<p>本文提出Invert4TVG框架，通过引入三种反向任务（动词补全、动作识别、视频描述）将原有的Temporal Video Grounding任务反转，增强模型对视频动作语义理解。方法整合强化学习，动态平衡主任务与反向任务训练，显著提升时序视频定位与动作理解精度，实验在Charades-STA等数据集上超越现有SOTA方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Invert4TVG_A_Temporal_Video_Grounding_Framework_with_Inversion_Tasks_for_Enhanced_Action_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-KLASSify-to-Verify-Audio-Visual-Deepfake-Detection-Using-SSL-based-Audio-and-Handcrafted-Visual-Features"><a href="#93-KLASSify-to-Verify-Audio-Visual-Deepfake-Detection-Using-SSL-based-Audio-and-Handcrafted-Visual-Features" class="headerlink" title="93. KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/KLASSify_to_Verify__Audio-Visual_Deepfake_Detection_Using_SSL-based_Audio_and_Handcrafted_Visual_Fea.pdf">KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KLASS Engineering and Solutions</span></p>
<p>该论文提出了KLASSify系统，通过结合自监督学习（SSL）音频特征与手工视觉特征实现高效的音视频深度伪造检测与定位。方法上，视频采用基于脸部区域的手工特征与轻量级时序卷积网络（TCN），音频采用Wav2Vec-AASIST与图注意力网络（GAT），最终融合音视频分数并校准，定位任务则在音频端引入边界感知注意力机制。实验结果表明，该系统在AV-Deepfake1M++数据集上深度伪造分类AUC达到92.78%，音频单模态定位IoU为0.3536，方法兼具可解释性和泛化性，适合实际部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/KLASSify_to_Verify_Audio-Visual_Deepfake_Detection_Using_SSL-based_Audio_and_Handcrafted_Visual_Features.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-RORPCap-Retrieval-based-Objects-and-Relations-Prompt-for-Image-Captioning"><a href="#94-RORPCap-Retrieval-based-Objects-and-Relations-Prompt-for-Image-Captioning" class="headerlink" title="94. RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/RORPCap__Retrieval-based_Objects_and_Relations_Prompt_for_Image_Captioning.pdf">RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Yunnan University</span></p>
<p>该论文提出了一种名为RORPCap的新型图像描述方法，利用对象与关系提取模型（OREM）从检索到的相关文本中抽取关键的对象和关系词，并将其填充到预定义的提示模板中，经GPT-2编码获得提示嵌入。再通过Mamba映射网络将CLIP提取的图像特征快速桥接到视觉-文本嵌入，最后将两者拼接后的特征输入GPT-2生成描述。实验显示，在MS-COCO和nocaps数据集上，RORPCap无需检测器和GCN即可达到与最先进模型相当的准确率，并显著缩短训练时间，具备优良的泛化与零样本能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/RORPCap_Retrieval-based_Objects_and_Relations_Prompt_for_Image_Captioning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-BEVANet-Bilateral-Efficient-Visual-Attention-Network-for-Real-Time-Semantic-Segmentation"><a href="#95-BEVANet-Bilateral-Efficient-Visual-Attention-Network-for-Real-Time-Semantic-Segmentation" class="headerlink" title="95. BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BEVANet__Bilateral_Efficient_Visual_Attention_Network_for_Real-Time_Semantic_Segmentation.pdf">BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Taiwan University</span></p>
<p>本论文提出了一种高效的语义分割网络BEVANet，核心方法包括稀疏分解大可分离核注意力（SDLSKA）、综合核选择（CKS）、深层大核金字塔池化（DLKPPM）、双分支架构和边界引导自适应融合（BGAF）模块，实现多尺度特征融合、动态感受野调整和边界精细化。实验表明，BEVANet在Cityscapes和CamVid数据集上以低计算量实现了SOTA的分割精度和实时推理速度，验证了其在速度与精度间的优越平衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BEVANet_Bilateral_Efficient_Visual_Attention_Network_for_Real-Time_Semantic_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-Understanding-Dynamic-Scenes-in-Ego-Centric-4D-Point-Clouds"><a href="#96-Understanding-Dynamic-Scenes-in-Ego-Centric-4D-Point-Clouds" class="headerlink" title="96. Understanding Dynamic Scenes in Ego Centric 4D Point Clouds"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Understanding_Dynamic_Scenes_in_Ego_Centric_4D_Point_Clouds.pdf">Understanding Dynamic Scenes in Ego Centric 4D Point Clouds</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了EgoDynamic4D，一个面向动态4D场景理解的首创问答基准，涵盖927K对QA任务，支持物体运动、人与物体交互等12类时空推理任务。作者设计了端到端时空推理框架，采用实例感知编码、时间与相机嵌入及适应性降采样，将大规模4D场景压缩为LLM可处理的序列，在EgoDynamic4D数据集上验证了其多模态时序建模的有效性。实验结果显示，该方法在动态场景理解任务上显著优于现有基线，提升了4D场景推理的精度和可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Understanding_Dynamic_Scenes_in_Ego_Centric_4D_Point_Clouds.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-HaDM-ST-Histology-Assisted-Differential-Modeling-for-Spatial-Transcriptomics-Generation"><a href="#97-HaDM-ST-Histology-Assisted-Differential-Modeling-for-Spatial-Transcriptomics-Generation" class="headerlink" title="97. HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HaDM-ST__Histology-Assisted_Differential_Modeling_for_Spatial_Transcriptomics_Generation.pdf">HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cambridge</span></p>
<p>该论文提出HaDM-ST框架，结合H&amp;E染色组织切片和低分辨率空间转录组（ST）数据，通过扩散模型生成高分辨率ST图。方法包括H&amp;E驱动的语义提取模块（HSD）、跨模态空间对齐模块（CMSA）、以及基于图神经网络的基因差异对抗学习模块（GDAL），有效提取表达相关形态特征并实现空间精细对齐和基因通道建模。实验在多数据集多基因上，HaDM-ST在结构相似性和RMSE等指标均优于现有方法，生成的高分辨率ST图具有更高空间保真度和基因表达准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/HaDM-ST_Histology-Assisted_Differential_Modeling_for_Spatial_Transcriptomics_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-Unsupervised-Real-World-Super-Resolution-via-Rectified-Flow-Degradation-Modelling"><a href="#98-Unsupervised-Real-World-Super-Resolution-via-Rectified-Flow-Degradation-Modelling" class="headerlink" title="98. Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Unsupervised_Real-World_Super-Resolution_via_Rectified_Flow_Degradation_Modelling.pdf">Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology Beijing</span></p>
<p>该论文提出了一种基于整流流（Rectified Flow）和傅里叶先验引导模块（FGDM）的无监督真实世界超分辨率方法。方法通过整流流退化建模（RFDM）和傅里叶相位结构引导，生成与真实降质分布一致的训练对，提高超分辨率网络在真实场景下的泛化能力。实验结果显示，该方法在多个真实世界数据集上显著优于现有合成训练对生成方法，提升了超分辨率模型的重建质量和细节还原能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Unsupervised_Real-World_Super-Resolution_via_Rectified_Flow_Degradation_Modelling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-SODiff-Semantic-Oriented-Diffusion-Model-for-JPEG-Compression-Artifacts-Removal"><a href="#99-SODiff-Semantic-Oriented-Diffusion-Model-for-JPEG-Compression-Artifacts-Removal" class="headerlink" title="99. SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal"></a>99. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SODiff__Semantic-Oriented_Diffusion_Model_for_JPEG_Compression_Artifacts_Removal.pdf">SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong Univercity</span></p>
<p>本文提出SODiff，一种面向语义的单步扩散模型，专注于JPEG压缩伪影去除。核心方法包括语义对齐的图像提示提取器（SAIPE）与质量因子感知步长预测器，通过提取图像嵌入并与文本语义空间对齐，以及自适应选择扩散步长，提升高压缩条件下的图像细节恢复能力。实验表明，SODiff在多项定量和视觉指标上均优于现有主流方法，尤其在极端压缩下能有效还原丢失结构与细节，具有较强的鲁棒性和实际应用价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SODiff_Semantic-Oriented_Diffusion_Model_for_JPEG_Compression_Artifacts_Removal.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-CoAR-Concept-Injection-into-Autoregressive-Models-for-Personalized-Text-to-Image-Generation"><a href="#100-CoAR-Concept-Injection-into-Autoregressive-Models-for-Personalized-Text-to-Image-Generation" class="headerlink" title="100. CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation"></a>100. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CoAR__Concept_Injection_into_Autoregressive_Models_for_Personalized_Text-to-Image_Generation.pdf">CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出了CoAR框架，通过在多模态自回归模型中注入主体和风格概念，实现个性化文本到图像生成。方法采用层级多模态上下文学习、双重先验保持和上下文自正则化，无需微调主干参数，仅优化少量可学习token；支持训练自由的主体-风格组合生成。实验结果表明，CoAR在主体忠实度、指令对齐和风格保存方面超过现有方法，同时参数效率显著提升，训练参数不到0.1M，性能与最新扩散法持平甚至更优。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CoAR_Concept_Injection_into_Autoregressive_Models_for_Personalized_Text-to-Image_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="101-Representation-Understanding-via-Activation-Maximization"><a href="#101-Representation-Understanding-via-Activation-Maximization" class="headerlink" title="101. Representation Understanding via Activation Maximization"></a>101. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Representation_Understanding_via_Activation_Maximization.pdf">Representation Understanding via Activation Maximization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Manchester</span></p>
<p>该论文提出了一种统一的特征可视化框架，利用Activation Maximization（激活最大化）方法对卷积神经网络（CNNs）和视觉变换器（ViTs）进行中间层和输出层的表征分析，通过在频率域优化，生成更自然且可解释的神经元激活图像，并揭示不同架构间的结构差异。此外，作者还将激活最大化拓展用于生成对抗样本，展示了其在模型脆弱性分析中的有效性。结论认为该方法提升了模型可解释性和泛化能力，并为理解神经网络的表示结构和对抗鲁棒性提供了新视角。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Representation_Understanding_via_Activation_Maximization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="102-Small-Large-Collaboration-Training-efficient-Concept-Personalization-for-Large-VLM-using-a-Meta-Personalized-Small-VLM"><a href="#102-Small-Large-Collaboration-Training-efficient-Concept-Personalization-for-Large-VLM-using-a-Meta-Personalized-Small-VLM" class="headerlink" title="102. Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM"></a>102. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Small-Large_Collaboration__Training-efficient_Concept_Personalization_for_Large_VLM_using_a_Meta_Per.pdf">Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出了Small-Large Collaboration (SLC) 框架，通过让小型视觉-语言模型（VLM）负责用户特定信息检测，大型VLM执行推理和反思，有效实现了大模型个性化且训练高效。实验表明，SLC能在不需针对每个用户单独微调大模型的前提下，兼顾个性化、准确性和训练效率，大幅降低训练成本并减少幻觉现象。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Small-Large_Collaboration_Training-efficient_Concept_Personalization_for_Large_VLM_using_a_Meta_Personalized_Small_VLM.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="103-SUIT-Spatial-Spectral-Union-Intersection-Interaction-Network-for-Hyperspectral-Object-Tracking"><a href="#103-SUIT-Spatial-Spectral-Union-Intersection-Interaction-Network-for-Hyperspectral-Object-Tracking" class="headerlink" title="103. SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking"></a>103. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SUIT__Spatial-Spectral_Union-Intersection_Interaction_Network_for_Hyperspectral_Object_Tracking.pdf">SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Science and Technology</span></p>
<p>本文提出了一种新颖的空间-光谱联合交互网络（SUIT），用于高光谱视频目标跟踪。方法上，SUIT利用Transformer实现模板与搜索区域在各光谱带间的空间交互，并基于集合论的容斥原理融合各带间的共享与特有空间信息，同时引入光谱损失保证模板与预测区域材料分布一致性。实验表明，该方法在多个高光谱跟踪基准上达到最优或竞争性表现，显著提升了小目标、遮挡、复杂背景等场景下的鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SUIT_Spatial-Spectral_Union-Intersection_Interaction_Network_for_Hyperspectral_Object_Tracking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="104-Consistent-and-Controllable-Image-Animation-with-Motion-Linear-Diffusion-Transformers"><a href="#104-Consistent-and-Controllable-Image-Animation-with-Motion-Linear-Diffusion-Transformers" class="headerlink" title="104. Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers"></a>104. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Consistent_and_Controllable_Image_Animation_with_Motion_Linear_Diffusion_Transformers.pdf">Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University</span></p>
<p>该论文提出MiraMo模型，通过线性注意力Transformer架构实现高效的图像动画生成。方法包括线性注意力机制、运动残差学习和基于DCT的噪声优化，可提升动画的一致性与运动平滑性，并支持动态度可控。实验结果表明，MiraMo在外观一致性、运动平滑性和推理速度方面优于现有方法，同时易于扩展到视频编辑和运动迁移等任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Consistent_and_Controllable_Image_Animation_with_Motion_Linear_Diffusion_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="105-Bridging-Semantic-Logic-Gaps-A-Cognition-Inspired-Multimodal-Boundary-Preserving-Network-for-Image-Manipulation-Localization"><a href="#105-Bridging-Semantic-Logic-Gaps-A-Cognition-Inspired-Multimodal-Boundary-Preserving-Network-for-Image-Manipulation-Localization" class="headerlink" title="105. Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization"></a>105. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Bridging_Semantic_Logic_Gaps__A_Cognition-Inspired_Multimodal_Boundary-Preserving_Network_for_Image_.pdf">Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xinjiang University</span></p>
<p>本文提出了一种认知启发的多模态边界保留网络（CMB-Net），用于图像篡改区域的精准定位。该方法结合大语言模型（LLMs）生成的文本提示与视觉特征，并引入图像-文本中心歧义模块（ITCAM）抑制幻觉文本的影响、图像-文本交互模块（ITIM）实现细粒度特征融合，以及基于可逆神经网络的边界恢复解码器（RED）无损保留篡改边界信息。实验结果表明，CMB-Net在多个公开数据集上显著优于现有主流方法，尤其在复杂场景下具有更高的鲁棒性和准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Bridging_Semantic_Logic_Gaps_A_Cognition-Inspired_Multimodal_Boundary-Preserving_Network_for_Image_Manipulation_Localization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="106-Intention-Aware-Diffusion-Model-for-Pedestrian-Trajectory-Prediction"><a href="#106-Intention-Aware-Diffusion-Model-for-Pedestrian-Trajectory-Prediction" class="headerlink" title="106. Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction"></a>106. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Intention-Aware_Diffusion_Model_for_Pedestrian_Trajectory_Prediction.pdf">Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southern University of Science and Technology</span></p>
<p>本文提出了一种融合短期与长期意图的扩散模型用于行人轨迹预测。方法上，采用残差极坐标表示建模短期意图，并通过可学习的Token化端点预测器捕获长期多模态目标，同时引入软掩码引导和残差噪声预测模块优化扩散过程。实验结果显示该方法在ETH、UCY和SDD等公开数据集上超越或媲美现有主流方法，显著提升了轨迹预测精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Intention-Aware_Diffusion_Model_for_Pedestrian_Trajectory_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="107-Large-scale-Multi-sequence-Pretraining-for-Generalizable-MRI-Analysis-in-Versatile-Clinical-Applications"><a href="#107-Large-scale-Multi-sequence-Pretraining-for-Generalizable-MRI-Analysis-in-Versatile-Clinical-Applications" class="headerlink" title="107. Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications"></a>107. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Large-scale_Multi-sequence_Pretraining_for_Generalizable_MRI_Analysis_in_Versatile_Clinical_Applicat.pdf">Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>该论文提出了PRISM，一种基于Swin Transformer骨干的大规模多序列MRI预训练基础模型。其创新性在于通过解耦解剖结构不变特征与序列特异性变化，并结合像素级掩码重建、图像到图像翻译、元数据预测、对比学习等多任务自监督策略，提升模型在多器官多协议MRI分析中的泛化能力和鲁棒性。实验覆盖44个下游任务，PRISM在39项任务中统计学显著优于当前主流方法，尤其在分割、分类、回归、配准和医学报告生成等多应用场景下表现突出。结论：PRISM能跨MRI协议稳定泛化，提升临床适用性和AI翻译潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Large-scale_Multi-sequence_Pretraining_for_Generalizable_MRI_Analysis_in_Versatile_Clinical_Applications.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="108-SketchAnimator-Animate-Sketch-via-Motion-Customization-of-Text-to-Video-Diffusion-Models"><a href="#108-SketchAnimator-Animate-Sketch-via-Motion-Customization-of-Text-to-Video-Diffusion-Models" class="headerlink" title="108. SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models"></a>108. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SketchAnimator__Animate_Sketch_via_Motion_Customization_of_Text-to-Video_Diffusion_Models.pdf">SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing University of Posts and Telecommunications</span></p>
<p>该论文提出SketchAnimator方法，通过三阶段流程（外观学习、运动学习和视频先验蒸馏），结合LoRA微调和可微Bezier曲线渲染，实现输入草图根据参考视频自定义动作的动画生成。实验表明，该方法显著提升草图外观保真与运动对齐能力，优于现有运动迁移与视频生成方法，拓展了创意动画生成的可能性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SketchAnimator_Animate_Sketch_via_Motion_Customization_of_Text-to-Video_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="109-Perceptual-Evaluation-of-GANs-and-Diffusion-Models-for-Generating-X-rays"><a href="#109-Perceptual-Evaluation-of-GANs-and-Diffusion-Models-for-Generating-X-rays" class="headerlink" title="109. Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays"></a>109. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Perceptual_Evaluation_of_GANs_and_Diffusion_Models_for_Generating_X-rays.pdf">Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Pontificia Universidad Católica de Chile</span></p>
<p>该论文针对医疗影像领域，比较了GANs与Diffusion Models在生成胸部X光片（包含四种异常情况）上的表现，通过专家（放射科医生）参与的双重评估任务，分析了模型生成图像的真实感与条件准确性。研究发现Diffusion Models整体生成更真实，但GANs在某些特定条件下（如无心脏轮廓扩大的情况）表现更佳，揭示了模型在医疗应用中尚未完全解决真实感与医学相关性的问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Perceptual_Evaluation_of_GANs_and_Diffusion_Models_for_Generating_X-rays.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="110-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning"><a href="#110-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning" class="headerlink" title="110. Cut2Next: Generating Next Shot via In-Context Tuning"></a>110. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Cut2Next__Generating_Next_Shot_via_In-Context_Tuning.pdf">Cut2Next: Generating Next Shot via In-Context Tuning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong</span></p>
<p>本论文提出了Next Shot Generation (NSG)任务，旨在生成符合专业剪辑模式且保持电影级连贯性的高质量后续镜头。方法上，Cut2Next框架基于Diffusion Transformer，采用分层多提示策略（Hierarchical Multi-Prompting），并通过上下文感知条件注入（CACI）和分层注意力掩码（HAM）实现视觉与文本信号的高效融合。实验结果表明Cut2Next在视觉一致性、文本忠实度及用户偏好上优于现有方法，能有效生成叙事性强、剪辑合理的后续镜头。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Cut2Next_Generating_Next_Shot_via_In-Context_Tuning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="111-THAT-Token-wise-High-frequency-Augmentation-Transformer-for-Hyperspectral-Pansharpening"><a href="#111-THAT-Token-wise-High-frequency-Augmentation-Transformer-for-Hyperspectral-Pansharpening" class="headerlink" title="111. THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening"></a>111. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/THAT__Token-wise_High-frequency_Augmentation_Transformer_for_Hyperspectral_Pansharpening.pdf">THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">JPMorgan Chase</span></p>
<p>该论文提出了Token-wise High-frequency Augmentation Transformer (THAT)用于高光谱图像融合（pansharpening），通过引入Pivotal Token Selective Attention (PTSA)模块动态筛选有效特征token并抑制冗余，以及Multi-level Variance-aware Feed-forward Network (MVFN)层以增强高频细节建模。实验结果显示，THAT在多个公开数据集和不同放大倍数下均达到最先进的重建质量和效率，显著提升了高光谱图像的空间分辨率和光谱保真度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/THAT_Token-wise_High-frequency_Augmentation_Transformer_for_Hyperspectral_Pansharpening.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="112-Mitigating-Biases-in-Surgical-Operating-Rooms-with-Geometry"><a href="#112-Mitigating-Biases-in-Surgical-Operating-Rooms-with-Geometry" class="headerlink" title="112. Mitigating Biases in Surgical Operating Rooms with Geometry"></a>112. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Mitigating_Biases_in_Surgical_Operating_Rooms_with_Geometry.pdf">Mitigating Biases in Surgical Operating Rooms with Geometry</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">TU Munich</span></p>
<p>本论文通过对两个人工智能手术室数据集的GradCAM可视化分析发现，传统RGB图像的CNN模型在人员再识别任务中容易陷入依赖鞋子、眼镜等偶然视觉特征的偏差，难以泛化到真实手术环境。作者提出将手术人员建模为3D点云序列，通过捕捉几何属性（如身形、步态和动作模式）有效减少视觉偏差，实验证明在真实手术室数据集上点云方法比RGB方法准确率高12%。结论是几何特征对抗视觉偏差、提升人员再识别准确率具有明显优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Mitigating_Biases_in_Surgical_Operating_Rooms_with_Geometry.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="113-StableAvatar-Infinite-Length-Audio-Driven-Avatar-Video-Generation"><a href="#113-StableAvatar-Infinite-Length-Audio-Driven-Avatar-Video-Generation" class="headerlink" title="113. StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation"></a>113. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/StableAvatar__Infinite-Length_Audio-Driven_Avatar_Video_Generation.pdf">StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>该论文提出StableAvatar，一种端到端视频扩散Transformer，可根据参考图像和音频生成无限长度、高质量且身份一致的虚拟人视频。其方法创新性地引入了时间步感知音频适配器以减少长视频中的潜在分布误差积累，并在推理阶段采用音频本地引导机制和动态加权滑动窗口策略提升音频同步及视频平滑度。实验表明，StableAvatar在身份保持、音频同步和视频质量方面显著优于现有方法，即使生成数千帧后依然保持高一致性和自然度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/StableAvatar_Infinite-Length_Audio-Driven_Avatar_Video_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="114-VGGSounder-Audio-Visual-Evaluations-for-Foundation-Models"><a href="#114-VGGSounder-Audio-Visual-Evaluations-for-Foundation-Models" class="headerlink" title="114. VGGSounder: Audio-Visual Evaluations for Foundation Models"></a>114. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VGGSounder__Audio-Visual_Evaluations_for_Foundation_Models.pdf">VGGSounder: Audio-Visual Evaluations for Foundation Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University of Munich</span></p>
<p>本文提出VGGSounder数据集，通过对现有VGGSound音视频分类测试集进行多标签和多模态（可听、可见、可听且可见）人工重标注，补充了原数据集中标签不全、类别重叠、模态错配等问题，实现对音视频基础模型更精细的能力评估。实验以多种主流音视频基础模型为对象，利用细致的模态评估和引入的模态混淆新指标，揭示模型多模态融合中的不足，并指出当前基础模型更依赖视觉信息，强调了VGGSounder在多模态模型评测中的重要价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/VGGSounder_Audio-Visual_Evaluations_for_Foundation_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="115-SAGOnline-Segment-Any-Gaussians-Online"><a href="#115-SAGOnline-Segment-Any-Gaussians-Online" class="headerlink" title="115. SAGOnline: Segment Any Gaussians Online"></a>115. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SAGOnline__Segment_Any_Gaussians_Online.pdf">SAGOnline: Segment Any Gaussians Online</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Waterloo</span></p>
<p>本文提出SAGOnline，一种面向3D高斯场的轻量级、零样本、实时3D分割框架。方法创新性地将3D分割任务解耦为基于视频基础模型（如SAM 2）的2D视图一致分割和GPU加速的3D掩码生成，并为每个高斯原语赋予唯一标识，实现了跨视角的多实例分割与追踪。实验在NVOS和Spin-NeRF数据集上实现了最新SOTA性能（92.7%&#x2F;95.2%mIoU），推理速度比现有方法快15–1500倍。结论是SAGOnline实现了高效、准确、实时的3D多目标分割与跟踪，适用于AR&#x2F;VR和机器人等场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SAGOnline_Segment_Any_Gaussians_Online.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="116-KARMA-Efficient-Structural-Defect-Segmentation-via-Kolmogorov-Arnold-Representation-Learning"><a href="#116-KARMA-Efficient-Structural-Defect-Segmentation-via-Kolmogorov-Arnold-Representation-Learning" class="headerlink" title="116. KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning"></a>116. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/KARMA__Efficient_Structural_Defect_Segmentation_via_Kolmogorov-Arnold_Representation_Learning.pdf">KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of New Orleans</span></p>
<p>该论文提出了KARMA架构，将Kolmogorov-Arnold表示学习（KAN）与优化的特征金字塔网络（FPN）结合，采用低秩Tiny KAN模块、分离卷积和静动态原型机制以提升结构性缺陷分割的效率与准确性。实验表明，KARMA在CSDD和S2DS结构缺陷数据集上以远低于主流模型的参数量（0.959M，减少97%）实现了同等或更优mIoU性能，适用于实时部署，验证了其高效性和实用价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/KARMA_Efficient_Structural_Defect_Segmentation_via_Kolmogorov-Arnold_Representation_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="117-RedDino-A-Foundation-Model-for-Red-Blood-Cell-Analysis"><a href="#117-RedDino-A-Foundation-Model-for-Red-Blood-Cell-Analysis" class="headerlink" title="117. RedDino: A Foundation Model for Red Blood Cell Analysis"></a>117. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/RedDino__A_foundation_model_for_red_blood_cell_analysis.pdf">RedDino: A Foundation Model for Red Blood Cell Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cagliari</span></p>
<p>本文提出RedDino，一种专为红细胞图像分析设计的自监督基础模型，基于DINOv2框架并结合大规模红细胞图像数据集，通过定制数据增强和去除Koleo正则项等技术优化模型结构。实验表明，RedDino在红细胞分类和特征泛化能力方面显著优于现有SOTA方法，具备强大的跨数据源适应性和临床应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/RedDino_A_Foundation_Model_for_Red_Blood_Cell_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="118-MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision"><a href="#118-MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision" class="headerlink" title="118. MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision"></a>118. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MedReasoner__Reinforcement_Learning_Drives_Reasoning_Grounding_from_Clinical_Thought_to_Pixel-Level_.pdf">MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing University of Posts and Telecommunications</span></p>
<p>该论文提出了统一医学推理定位（UMRG）任务，并发布了U-MRG-14K数据集，将隐式临床语言推理与像素级图像分割结合。作者设计了MedReasoner框架，采用模块化结构，将多模态大语言模型（MLLM）为推理模块、冻结分割专家为分割模块，通过强化学习（RL）优化推理与空间对齐，实现了隐式临床查询到精确空间定位的映射。结果显示，MedReasoner在U-MRG-14K数据集上实现了当前最优的分割与定位精度，并具有较强的泛化能力，证明了RL在医学图像可解释定位中的优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MedReasoner_Reinforcement_Learning_Drives_Reasoning_Grounding_from_Clinical_Thought_to_Pixel-Level_Precision.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="119-Integrating-Task-Specific-and-Universal-Adapters-for-Pre-Trained-Model-based-Class-Incremental-Learning"><a href="#119-Integrating-Task-Specific-and-Universal-Adapters-for-Pre-Trained-Model-based-Class-Incremental-Learning" class="headerlink" title="119. Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning"></a>119. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Integrating_Task-Specific_and_Universal_Adapters_for_Pre-Trained_Model-based_Class-Incremental_Learn.pdf">Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>该论文提出了一种集成任务特定适配器与通用适配器（TUNA）的增量学习方法，利用预训练视觉变换器（ViT）冻结主干，仅训练轻量级适配器以适应新任务。通过正交损失训练任务特定适配器，并引入基于熵的选择机制动态选择最优适配器，进一步将多任务适配器融合为通用适配器以捕获跨任务的共享特征，推理时结合两者输出。实验在CIFAR100、ImageNet-R&#x2F;A和ObjectNet等多个基准上取得了SOTA性能，验证了该方法对抗灾难性遗忘和提升泛化能力的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Integrating_Task-Specific_and_Universal_Adapters_for_Pre-Trained_Model-based_Class-Incremental_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="120-FantasyStyle-Controllable-Stylized-Distillation-for-3D-Gaussian-Splatting"><a href="#120-FantasyStyle-Controllable-Stylized-Distillation-for-3D-Gaussian-Splatting" class="headerlink" title="120. FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting"></a>120. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FantasyStyle__Controllable_Stylized_Distillation_for_3D_Gaussian_Splatting.pdf">FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computing and Artificial Intelligence, Shanghai University of Finance and Economics</span></p>
<p>该论文提出FantasyStyle框架，专注于3D高斯撒点(3DGS)的风格迁移，通过完全依赖扩散模型蒸馏实现3D风格转化。方法包括多视角频率一致性（MVFC），利用3D频率域滤波提升视角一致性，以及可控风格化蒸馏（CSD），结合负向引导防止内容泄露并优化3D场景。实验显示，FantasyStyle在风格化质量和内容保留方面均优于现有方法，推动2D风格迁移技术向3D领域的灵活扩展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/FantasyStyle_Controllable_Stylized_Distillation_for_3D_Gaussian_Splatting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="121-Hyperspectral-Imaging"><a href="#121-Hyperspectral-Imaging" class="headerlink" title="121. Hyperspectral Imaging"></a>121. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hyperspectral_Imaging.pdf">Hyperspectral Imaging</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southeast University</span></p>
<p>本文全面综述了高光谱成像（HSI）的物理原理、传感器架构、数据采集与校准流程，并系统梳理了图像恢复、增强、降维、分类、光谱解混等关键分析方法，重点介绍了深度学习等AI驱动的新技术及其在地球观测、精密农业、生物医学、工业质检、文化遗产与安全防御等领域的代表性应用。结论指出，HSI正由实验转向大规模现实应用，未来将朝向可扩展、实时、嵌入式系统发展，AI基础模型和跨模态融合等新范式将极大拓展其科学与社会影响力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Hyperspectral_Imaging.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="122-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation"><a href="#122-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation" class="headerlink" title="122. Matrix-3D: Omnidirectional Explorable 3D World Generation"></a>122. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Matrix-3D__Omnidirectional_Explorable_3D_World_Generation.pdf">Matrix-3D: Omnidirectional Explorable 3D World Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Skywork AI</span></p>
<p>Matrix-3D提出了一个基于全景视频扩散模型和三维重建的新方法，可从单张图片或文本生成可自由探索的全方位3D世界。其核心流程包括利用场景网格渲染做轨迹指导的全景视频生成，再通过优化或大模型推理实现高质量与高效的3D重建。实验结果显示，该方法在全景视频生成和3D世界重建任务上均达到了当前最优性能，支持更广泛的空间探索与应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Matrix-3D_Omnidirectional_Explorable_3D_World_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="123-BadPromptFL-A-Novel-Backdoor-Threat-to-Prompt-based-Federated-Learning-in-Multimodal-Models"><a href="#123-BadPromptFL-A-Novel-Backdoor-Threat-to-Prompt-based-Federated-Learning-in-Multimodal-Models" class="headerlink" title="123. BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models"></a>123. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BadPromptFL__A_Novel_Backdoor_Threat_to_Prompt-based_Federated_Learning_in_Multimodal_Models.pdf">BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Dalian University of Technology</span></p>
<p>本文提出BadPromptFL，一种针对多模态对比模型中基于prompt的联邦学习的新型后门攻击方法。该方法通过恶意客户端联合优化本地后门触发器和prompt嵌入，在联邦聚合过程中注入隐蔽的后门prompt，使后门行为在推理时普遍激活且不影响模型参数。实验结果表明，BadPromptFL能在多种数据集和聚合协议下以极高的攻击成功率 (&gt;90%) 实现后门注入，同时对正常任务性能影响极小，揭示了prompt聚合在真实部署中的安全隐患。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BadPromptFL_A_Novel_Backdoor_Threat_to_Prompt-based_Federated_Learning_in_Multimodal_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="124-CD-TVD-Contrastive-Diffusion-for-3D-Super-Resolution-with-Scarce-High-Resolution-Time-Varying-Data"><a href="#124-CD-TVD-Contrastive-Diffusion-for-3D-Super-Resolution-with-Scarce-High-Resolution-Time-Varying-Data" class="headerlink" title="124. CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data"></a>124. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CD-TVD__Contrastive_Diffusion_for_3D_Super-Resolution_with_Scarce_High-Resolution_Time-Varying_Data.pdf">CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tianjin University</span></p>
<p>该论文提出了CD-TVD框架，结合对比学习和扩散模型，实现在高分辨率时序数据极度稀缺情况下的3D超分辨率重建。通过历史模拟数据预训练对比编码器和扩散超分辨率模块，模型能够捕获退化特征和细粒度结构，并利用局部注意机制提升重建效率；在新场景下，仅需极少高分辨率样本即可微调模型，实现对所有低分辨率时刻的准确恢复。实验结果表明，CD-TVD在科学仿真等任务下显著优于现有方法，兼顾精细结构恢复和资源效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CD-TVD_Contrastive_Diffusion_for_3D_Super-Resolution_with_Scarce_High-Resolution_Time-Varying_Data.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="125-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control"><a href="#125-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control" class="headerlink" title="125. Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control"></a>125. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Follow-Your-Shape__Shape-Aware_Image_Editing_via_Trajectory-Guided_Region_Control.pdf">Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HKUST</span></p>
<p>该论文提出了Follow-Your-Shape，一个训练和掩码无关的形状感知图像编辑框架。方法核心为Trajectory Divergence Map (TDM)，通过对比源与目标文本提示下的去噪轨迹差异，动态定位可编辑区域，并结合分阶段的Key-Value注入与ControlNet结构指导，实现大幅度形状变换同时严格保持非目标内容。提出的ReShapeBench用于系统评测形状编辑能力。实验结果表明该方法在大规模形状替换任务中，编辑精度和视觉保真度均优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Follow-Your-Shape_Shape-Aware_Image_Editing_via_Trajectory-Guided_Region_Control.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="126-MDD-Net-Multimodal-Depression-Detection-through-Mutual-Transformer"><a href="#126-MDD-Net-Multimodal-Depression-Detection-through-Mutual-Transformer" class="headerlink" title="126. MDD-Net: Multimodal Depression Detection through Mutual Transformer"></a>126. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MDD-Net__Multimodal_Depression_Detection_through_Mutual_Transformer.pdf">MDD-Net: Multimodal Depression Detection through Mutual Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Waterloo</span></p>
<p>本文提出了MDD-Net，一种融合音频与视觉数据的多模态抑郁症检测网络，核心采用互信息Transformer模块实现跨模态特征高效提取与融合。通过在社交媒体D-Vlog数据集上的实验，MDD-Net在F1-Score上相比现有方法有最高17.37%的提升，验证了其在多模态抑郁检测任务中的优越性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MDD-Net_Multimodal_Depression_Detection_through_Mutual_Transformer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="127-Sample-aware-RandAugment-Search-free-Automatic-Data-Augmentation-for-Effective-Image-Recognition"><a href="#127-Sample-aware-RandAugment-Search-free-Automatic-Data-Augmentation-for-Effective-Image-Recognition" class="headerlink" title="127. Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition"></a>127. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sample-aware_RandAugment__Search-free_Automatic_Data_Augmentation_for_Effective_Image_Recognition.pdf">Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Chinese Academy of Sciences</span></p>
<p>本文提出了Sample-aware RandAugment (SRA)，一种无搜索、样本感知的自动数据增强（AutoDA）方法，核心创新包括：引入启发式评分模块Magnitude Instructor Score (MIS)以动态评估样本难度，并采用非对称增强策略（探索和精炼阶段）对训练批次进行分割与差异化增强，从而提升模型泛化能力。实验表明，SRA在ImageNet（ResNet-50），CIFAR、Food101等多项视觉分类任务上实现了性能超越当前无搜索方法甚至可媲美部分有搜索方法，且无需超参数调优，提升了下游检测任务精度，具备良好兼容性与广泛适用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Sample-aware_RandAugment_Search-free_Automatic_Data_Augmentation_for_Effective_Image_Recognition.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="128-Prompt-Guided-Relational-Reasoning-for-Social-Behavior-Understanding-with-Vision-Foundation-Models"><a href="#128-Prompt-Guided-Relational-Reasoning-for-Social-Behavior-Understanding-with-Vision-Foundation-Models" class="headerlink" title="128. Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models"></a>128. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Prompt-Guided_Relational_Reasoning_for_Social_Behavior_Understanding_with_Vision_Foundation_Models.pdf">Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Stuttgart</span></p>
<p>该论文提出了ProGraD框架，通过在冻结的视觉基础模型（如DINOv2）上引入可学习的群体提示（group prompts）和轻量化的两层GroupContext Transformer，实现对视频中社会群体行为的高效检测与推理。作者在Cafe和Social-CAD两个数据集上验证了方法的有效性，ProGraD仅用约1000万可训练参数便超过现有方法，尤其在多组场景下表现突出，且具备良好的可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Prompt-Guided_Relational_Reasoning_for_Social_Behavior_Understanding_with_Vision_Foundation_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="129-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation"><a href="#129-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation" class="headerlink" title="129. Omni-Effects: Unified and Spatially Controllable Visual Effects Generation"></a>129. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Omni-Effects__Unified_and_Spatially-Controllable_Visual_Effects_Generation.pdf">Omni-Effects: Unified and Spatially Controllable Visual Effects Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AMAP, Alibaba Group</span></p>
<p>本文提出了Omni-Effects，一种统一且可空间控制的视觉特效生成框架。方法核心包括LoRA-MoE模块（以专家混合方式减少多特效混合训练时的干扰）和SAP-IIF机制（融合空间掩码与文本提示，实现像素级多特效独立控制），并配套构建Omni-VFX数据集及评测体系。实验表明该方法在单&#x2F;多特效与空间控制方面显著优于现有方案，实现了高保真、多样化且可控的特效视频合成。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Omni-Effects_Unified_and_Spatially_Controllable_Visual_Effects_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="130-Score-Augmentation-for-Diffusion-Models"><a href="#130-Score-Augmentation-for-Diffusion-Models" class="headerlink" title="130. Score Augmentation for Diffusion Models"></a>130. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Score_Augmentation_for_Diffusion_Models.pdf">Score Augmentation for Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou Technology</span></p>
<p>该论文针对扩散模型在数据有限情况下的过拟合问题，提出了Score Augmentation（ScoreAug）数据增强框架。ScoreAug在噪声数据上施加变换，并要求去噪器预测变换后的目标，实现等变学习目标，从而提升模型泛化能力。实验表明，ScoreAug在CIFAR-10、FFHQ、AFHQv2和ImageNet等基准数据集上显著提升了生成性能，有效缓解了过拟合，并可与传统数据增强方法协同提升效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Score_Augmentation_for_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="131-Diffusing-the-Blind-Spot-Uterine-MRI-Synthesis-with-Diffusion-Models"><a href="#131-Diffusing-the-Blind-Spot-Uterine-MRI-Synthesis-with-Diffusion-Models" class="headerlink" title="131. Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models"></a>131. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Diffusing_the_Blind_Spot__Uterine_MRI_Synthesis_with_Diffusion_Models.pdf">Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Friedrich–Alexander University Erlangen–Nürnberg</span></p>
<p>本文提出了一种结合无条件和条件去噪扩散概率模型（DDPM）及潜在扩散模型（LDM）的2D和3D扩散模型框架，实现了高保真、解剖结构合理的子宫MRI图像合成，同时引入文本和类别条件提升生成控制力，并通过隐私过滤保证数据安全。实验结果显示，合成数据在弱监督和全监督下，分类任务表现优于真实数据，推动了妇科领域的AI发展和隐私保护医学影像数据增广。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Diffusing_the_Blind_Spot_Uterine_MRI_Synthesis_with_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="132-Architectural-Co-Design-for-Zero-Shot-Anomaly-Detection-Decoupling-Representation-and-Dynamically-Fusing-Features-in-CLIP"><a href="#132-Architectural-Co-Design-for-Zero-Shot-Anomaly-Detection-Decoupling-Representation-and-Dynamically-Fusing-Features-in-CLIP" class="headerlink" title="132. Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP"></a>132. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Architectural_Co-Design_for_Zero-Shot_Anomaly_Detection__Decoupling_Representation_and_Dynamically_F.pdf">Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Big Data Institute, Central South University</span></p>
<p>本文提出了ACD-CLIP架构，通过参数高效的Conv-LoRA适配器为视觉特征注入局部归纳偏置，并采用动态融合网关（DFG）实现视觉引导的跨模态灵活特征融合，提升了基础视觉-语言模型在零样本异常检测任务中的表现。实验结果显示该方法在工业和医学多个基准上显著优于现有方法，表现出更强的鲁棒性和精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Architectural_Co-Design_for_Zero-Shot_Anomaly_Detection_Decoupling_Representation_and_Dynamically_Fusing_Features_in_CLIP.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="133-MIND-A-Noise-Adaptive-Denoising-Framework-for-Medical-Images-Integrating-Multi-Scale-Transformer"><a href="#133-MIND-A-Noise-Adaptive-Denoising-Framework-for-Medical-Images-Integrating-Multi-Scale-Transformer" class="headerlink" title="133. MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer"></a>133. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MIND__A_Noise-Adaptive_Denoising_Framework_for_Medical_Images_Integrating_Multi-Scale_Transformer.pdf">MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>本文提出了一种集成多尺度Transformer和卷积结构的医学图像自适应去噪模型MIND，结合噪声水平估计器（NLE）和噪声自适应注意力模块（NAAB），实现了通道-空间注意调节和跨模态特征融合。大规模实验表明，MIND在PSNR、SSIM、LPIPS等指标及下游诊断任务中均显著优于主流方法，提升了结构恢复和诊断敏感性，具备强泛化与实际应用价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MIND_A_Noise-Adaptive_Denoising_Framework_for_Medical_Images_Integrating_Multi-Scale_Transformer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="134-DiTVR-Zero-Shot-Diffusion-Transformer-for-Video-Restoration"><a href="#134-DiTVR-Zero-Shot-Diffusion-Transformer-for-Video-Restoration" class="headerlink" title="134. DiTVR: Zero-Shot Diffusion Transformer for Video Restoration"></a>134. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DiTVR__Zero-Shot_Diffusion_Transformer_for_Video_Restoration.pdf">DiTVR: Zero-Shot Diffusion Transformer for Video Restoration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Wurzburg</span></p>
<p>该论文提出DiTVR，一种零样本视频复原框架，结合了扩散Transformer（DiT）、基于光流的时空邻居缓存、轨迹感知注意力和波形引导的一致采样器。方法通过在关键时刻引入光流信息，动态选择相关token并沿运动轨迹对齐特征，有效提升了视频分辨率、去噪和去模糊任务的时空一致性和细节保留。实验结果表明，DiTVR在多个视频恢复基准上取得了新的零样本最优性能，无需任务特定训练即可实现高质量、时序连贯的视频重建，优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DiTVR_Zero-Shot_Diffusion_Transformer_for_Video_Restoration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="135-MambaTrans-Multimodal-Fusion-Image-Translation-via-Large-Language-Model-Priors-for-Downstream-Visual-Tasks"><a href="#135-MambaTrans-Multimodal-Fusion-Image-Translation-via-Large-Language-Model-Priors-for-Downstream-Visual-Tasks" class="headerlink" title="135. MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks"></a>135. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MambaTrans__Multimodal_Fusion_Image_Translation_via_Large_Language_Model_Priors_for_Downstream_Visua.pdf">MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Foshan University</span></p>
<p>本论文提出MambaTrans，一种融合红外与可见光图像的多模态图像翻译框架，利用多模态大语言模型生成的文本描述和语义分割掩码作为先验，通过多模态状态空间块（MM-SSB）实现文本、掩码与图像的深度语义交互和分布迁移。该方法在无需更改下游检测与分割模型参数的情况下，优化任务感知损失函数，有效提升了多模态融合图像在目标检测和语义分割任务上的性能，实验结果表明其在图像质量和下游任务表现间取得了优异平衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MambaTrans_Multimodal_Fusion_Image_Translation_via_Large_Language_Model_Priors_for_Downstream_Visual_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="136-RSVLM-QA-A-Benchmark-Dataset-for-Remote-Sensing-Vision-Language-Model-based-Question-Answering"><a href="#136-RSVLM-QA-A-Benchmark-Dataset-for-Remote-Sensing-Vision-Language-Model-based-Question-Answering" class="headerlink" title="136. RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering"></a>136. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/RSVLM-QA__A_Benchmark_Dataset_for_Remote_Sensing_Vision_Language_Model-based_Question_Answering.pdf">RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Technology Sydney</span></p>
<p>本论文提出了RSVLM-QA数据集，采用GPT-4.1大语言模型通过精细化提示工程和双轨注释流程，自动生成遥感影像的丰富描述、空间关系、语义标签及多类型VQA问答对，并联合四个主流遥感数据集，保证注释多样性及数量准确性。结论显示RSVLM-QA能有效挑战和评估当前主流视觉语言模型在遥感领域的理解与推理能力，是推动遥感视觉语言模型研究的重要基准。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/RSVLM-QA_A_Benchmark_Dataset_for_Remote_Sensing_Vision_Language_Model-based_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="137-Generative-Video-Matting"><a href="#137-Generative-Video-Matting" class="headerlink" title="137. Generative Video Matting"></a>137. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Generative_Video_Matting.pdf">Generative Video Matting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Adelaide</span></p>
<p>该论文提出GVM（Generative Video Matting），一种基于视频扩散模型的视频抠图方法，通过大规模合成与伪标注数据预训练，结合高质量细粒度合成发丝视频数据，针对人物和动物实现稳健的时序一致精细抠图。方法创新点在于利用预训练视频扩散模型的丰富空间与时序先验，并采用三阶段训练策略和混合空间—像素监督，实验在多个基准上显示出对复杂场景的优越泛化和细节恢复能力。结论：GVM在真实复杂场景下表现出色，优于现有主流方法，具备广泛应用前景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Generative_Video_Matting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="138-Being-M0-5-A-Real-Time-Controllable-Vision-Language-Motion-Model"><a href="#138-Being-M0-5-A-Real-Time-Controllable-Vision-Language-Motion-Model" class="headerlink" title="138. Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model"></a>138. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Being-M0.5__A_Real-Time_Controllable_Vision-Language-Motion_Model.pdf">Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CASIA</span></p>
<p>本文提出了Being-M0.5，一种基于7B参数LLM的实时可控视觉-语言-动作模型（VLMM），利用新构建的HuMo100M大规模多模态动作数据集，支持多模态输入输出，并引入了创新的部位感知残差量化（PRQ）技术，实现身体各部位的精细化独立控制。实验显示，该模型在多项动作生成基准任务上取得了最先进性能，并具备高效实时推理能力，极大提升了动作生成的可控性与实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Being-M0.5_A_Real-Time_Controllable_Vision-Language-Motion_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="139-Segmenting-and-Understanding-Region-aware-Semantic-Attention-for-Fine-grained-Image-Quality-Assessment-with-Large-Language-Models"><a href="#139-Segmenting-and-Understanding-Region-aware-Semantic-Attention-for-Fine-grained-Image-Quality-Assessment-with-Large-Language-Models" class="headerlink" title="139. Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models"></a>139. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Segmenting_and_Understanding__Region-aware_Semantic_Attention_for_Fine-grained_Image_Quality_Assessm.pdf">Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology</span></p>
<p>本文提出了一种新的无参考图像质量评估模型RSFIQA，通过结合Segment Anything Model（SAM）进行语义分割和多模态大语言模型（MLLM）实现分区域的多维失真分析，提升了图像质量评估的精细度。核心创新为区域语义注意力机制（RSA），能聚合局部区域特征并提升对不同语义区域的细粒度质量感知。实验结果表明该方法在多个主流数据集上均取得了优异性能，具有很强的泛化性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Segmenting_and_Understanding_Region-aware_Semantic_Attention_for_Fine-grained_Image_Quality_Assessment_with_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="140-Dream4D-Lifting-Camera-Controlled-I2V-towards-Spatiotemporally-Consistent-4D-Generation"><a href="#140-Dream4D-Lifting-Camera-Controlled-I2V-towards-Spatiotemporally-Consistent-4D-Generation" class="headerlink" title="140. Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation"></a>140. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Dream4D__Lifting_Camera-Controlled_I2V_towards_Spatiotemporally_Consistent_4D_Generation.pdf">Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong</span></p>
<p>该论文提出Dream4D框架，通过结合可控图像到视频生成（I2V）与神经4D重建，首次实现从单幅图像和文本提示出发，经由视觉语言模型（VLM）预测最优相机轨迹，利用姿态条件扩散模型生成空间一致的多视角视频序列，最终还原具有时空一致性的4D场景。实验表明，Dream4D在动态场景下具备优异的结构保持和时序一致性，定量与定性指标均优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Dream4D_Lifting_Camera-Controlled_I2V_towards_Spatiotemporally_Consistent_4D_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="141-Sea-Undistort-A-Dataset-for-Through-Water-Image-Restoration-in-High-Resolution-Airborne-Bathymetric-Mapping"><a href="#141-Sea-Undistort-A-Dataset-for-Through-Water-Image-Restoration-in-High-Resolution-Airborne-Bathymetric-Mapping" class="headerlink" title="141. Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping"></a>141. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sea-Undistort__A_Dataset_for_Through-Water_Image_Restoration_in_High_Resolution_Airborne_Bathymetric.pdf">Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technische Universität Berlin</span></p>
<p>本文提出了Sea-Undistort数据集，使用Blender合成了1200对高分辨率水下光学畸变和无畸变的图像，涵盖了真实的水面反射、波动和散射等复杂效果，并配套详细元数据。基于该数据集，作者对比了两种主流图像复原方法并提出了融合早期光斑掩码的轻量扩散模型，实验表明该方法在合成和真实航拍数据上都显著提升了水下地形建模的细节还原、深度范围和精度，同时抑制了光斑和散射等伪影。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Sea-Undistort_A_Dataset_for_Through-Water_Image_Restoration_in_High_Resolution_Airborne_Bathymetric_Mapping.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="142-Correspondence-as-Video-Test-Time-Adaption-on-SAM2-for-Reference-Segmentation-in-the-Wild"><a href="#142-Correspondence-as-Video-Test-Time-Adaption-on-SAM2-for-Reference-Segmentation-in-the-Wild" class="headerlink" title="142. Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild"></a>142. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Correspondence_as_Video__Test-Time_Adaption_on_SAM2_for_Reference_Segmentation_in_the_Wild.pdf">Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>该论文提出CAV-SAM方法，将参考-目标图像对的对应关系表示为伪视频序列，通过扩散模型生成平滑的语义变换序列（DBST模块），并利用测试时几何对齐（TTGA模块）对SAM2进行轻量级微调，提升在下游参考分割任务中的适应能力。实验结果显示，CAV-SAM在跨领域少样本分割基准上较SOTA方法mIoU提升约5%，具备强泛化和高效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Correspondence_as_Video_Test-Time_Adaption_on_SAM2_for_Reference_Segmentation_in_the_Wild.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="143-Enhancing-Small-Scale-Dataset-Expansion-with-Triplet-Connection-based-Sample-Re-Weighting"><a href="#143-Enhancing-Small-Scale-Dataset-Expansion-with-Triplet-Connection-based-Sample-Re-Weighting" class="headerlink" title="143. Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting"></a>143. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Enhancing_Small-Scale_Dataset_Expansion_with_Triplet-Connection-based_Sample_Re-Weighting.pdf">Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hunan University</span></p>
<p>该论文提出TriReWeight方法，通过理论分析三类生成图像监督方式（强连接、成对连接、无连接），发现现有生成数据扩增普遍采用的强连接监督易引入噪声标签，降低泛化能力。TriReWeight结合成对连接（triplet loss）与无连接（consistency regularization）监督，动态对生成样本赋权，显著提升了小样本数据集的分类准确率，且可无缝集成到各类生成数据扩增框架。实验验证该方法平均提升自然图像数据集7.9%、医学图像数据集3.4%的准确率，并具有良好泛化性和训练收敛性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Enhancing_Small-Scale_Dataset_Expansion_with_Triplet-Connection-based_Sample_Re-Weighting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="144-DoorDet-Semi-Automated-Multi-Class-Door-Detection-Dataset-via-Object-Detection-and-Large-Language-Models"><a href="#144-DoorDet-Semi-Automated-Multi-Class-Door-Detection-Dataset-via-Object-Detection-and-Large-Language-Models" class="headerlink" title="144. DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models"></a>144. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DoorDet__Semi-Automated_Multi-Class_Door_Detection_Dataset_via_Object_Detection_and_Large_Language_M.pdf">DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Melbourne</span></p>
<p>本文提出了一种结合深度目标检测（Co-DETR）与大语言模型（GPT-4.1）的半自动化流程，用于构建多类别门检测数据集DoorDet，流程包括单类门检测、LLM辅助门类型判别与人工校正。实验表明，该方法大幅减少人工标注成本，提升了数据集质量和多类别门检测的基准性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DoorDet_Semi-Automated_Multi-Class_Door_Detection_Dataset_via_Object_Detection_and_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="145-Make-Your-MoVe-Make-Your-3D-Contents-by-Adapting-Multi-View-Diffusion-Models-to-External-Editing"><a href="#145-Make-Your-MoVe-Make-Your-3D-Contents-by-Adapting-Multi-View-Diffusion-Models-to-External-Editing" class="headerlink" title="145. Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing"></a>145. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Make_Your_MoVe__Make_Your_3D_Contents_by_Adapting_Multi-View_Diffusion_Models_to_External_Editing.pdf">Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了一种无须微调的通用方案，可将外部2D编辑（如风格迁移、重光照）自适应整合到多视角扩散模型生成的3D内容中。核心方法包括并行双通道推理、几何保持模块（用原始法线引导编辑生成）和注入切换器（动态控制法线监督程度），实现了外观变化下的几何一致性。实验表明，该方法在多种模型和编辑工具下显著提升了3D资产的多视角一致性与网格质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Make_Your_MoVe_Make_Your_3D_Contents_by_Adapting_Multi-View_Diffusion_Models_to_External_Editing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="146-DiffVC-OSD-One-Step-Diffusion-based-Perceptual-Neural-Video-Compression-Framework"><a href="#146-DiffVC-OSD-One-Step-Diffusion-based-Perceptual-Neural-Video-Compression-Framework" class="headerlink" title="146. DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework"></a>146. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DiffVC-OSD__One-Step_Diffusion-based_Perceptual_Neural_Video_Compression_Framework.pdf">DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wuhan University</span></p>
<p>该论文提出了DiffVC-OSD，一种基于单步扩散模型的感知神经视频压缩框架。方法核心在于直接将重构的无噪声潜在表征输入扩散模型，并结合时序上下文，通过单步扩散实现高感知质量的视频重建，同时设计了时序上下文适配器（TCA）和端到端微调策略以提升整体压缩效率。实验表明该方法在多项感知指标上达到SOTA，解码速度提升约20倍，码率降低86.92%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DiffVC-OSD_One-Step_Diffusion-based_Perceptual_Neural_Video_Compression_Framework.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="147-X2Edit-Revisiting-Arbitrary-Instruction-Image-Editing-through-Self-Constructed-Data-and-Task-Aware-Representation-Learning"><a href="#147-X2Edit-Revisiting-Arbitrary-Instruction-Image-Editing-through-Self-Constructed-Data-and-Task-Aware-Representation-Learning" class="headerlink" title="147. X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning"></a>147. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/X2Edit__Revisiting_Arbitrary-Instruction_Image_Editing_through_Self-Constructed_Data_and_Task-Aware_.pdf">X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">OPPO AI Center</span></p>
<p>该论文提出了X2Edit Dataset，一个涵盖14种编辑任务的高质量自动化图像编辑数据集，并提出了轻量化的可插拔编辑模型X2Edit，采用任务感知MoE-LoRA与对比学习结构实现高效多任务图像编辑。实验表明，X2Edit数据集在任务多样性与质量上优于现有开源数据集，X2Edit模型在多项基准性能上与SOTA方法持平，具备良好的可扩展性与生态兼容性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/X2Edit_Revisiting_Arbitrary-Instruction_Image_Editing_through_Self-Constructed_Data_and_Task-Aware_Representation_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="148-Separation-and-Collaboration-Two-Level-Routing-Grouped-Mixture-of-Experts-for-Multi-Domain-Continual-Learning"><a href="#148-Separation-and-Collaboration-Two-Level-Routing-Grouped-Mixture-of-Experts-for-Multi-Domain-Continual-Learning" class="headerlink" title="148. Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning"></a>148. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Separation_and_Collaboration__Two-Level_Routing_Grouped_Mixture-of-Experts_for_Multi-Domain_Continua.pdf">Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Defense Technology</span></p>
<p>本文提出了Two-Level Routing Grouped Mixture-of-Experts (TRGE)方法，通过在冻结的CLIP模型基础上为每个新任务增设专家组，并采用两级路由机制（组内与组间路由）实现任务间协作与知识保留。结合多模态大语言模型进行任务识别，并动态融合适配器与预训练模型输出，有效缓解灾难性遗忘和前向遗忘。实验显示，该方法在多领域持续学习任务中以更少参数获得更优反遗忘和零样本泛化性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Separation_and_Collaboration_Two-Level_Routing_Grouped_Mixture-of-Experts_for_Multi-Domain_Continual_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="149-Training-Free-ANN-to-SNN-Conversion-for-High-Performance-Spiking-Transformer"><a href="#149-Training-Free-ANN-to-SNN-Conversion-for-High-Performance-Spiking-Transformer" class="headerlink" title="149. Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer"></a>149. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Training-Free_ANN-to-SNN_Conversion_for_High-Performance_Spiking_Transformer.pdf">Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>本文提出了一种针对Transformer架构的高性能、免训练ANN-to-SNN转换框架，核心方法为多基指数衰减（MBE）神经元，通过指数衰减策略和多基编码高效逼近各类非线性操作，无需对原始ANN权重进行微调。实验表明，该方法在CV、NLU、NLG等任务和主流Transformer架构（ViT、RoBERTa、GPT-2）上实现了近无损的转换精度与显著的低延迟，极大推动了Spiking Transformer在实际中的高效部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Training-Free_ANN-to-SNN_Conversion_for_High-Performance_Spiking_Transformer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="150-TAR-TVG-Enhancing-VLMs-with-Timestamp-Anchor-Constrained-Reasoning-for-Temporal-Video-Grounding"><a href="#150-TAR-TVG-Enhancing-VLMs-with-Timestamp-Anchor-Constrained-Reasoning-for-Temporal-Video-Grounding" class="headerlink" title="150. TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding"></a>150. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TAR-TVG__Enhancing_VLMs_with_Timestamp_Anchor-Constrained_Reasoning_for_Temporal_Video_Grounding.pdf">TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China University of Technology</span></p>
<p>该论文提出了TAR-TVG框架，通过在视频语言模型的推理过程中引入时间戳锚点，实现对推理链的显式监督和逐步精细化的时间预测。采用GRPO-SFT-GRPO三阶段训练策略，自动收集高质量推理样本并提升模型生成可验证、可解释的推理链。实验结果显示，该方法在Charades-STA等多项基准上达到最新SOTA，显著优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TAR-TVG_Enhancing_VLMs_with_Timestamp_Anchor-Constrained_Reasoning_for_Temporal_Video_Grounding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="151-LaRender-Training-Free-Occlusion-Control-in-Image-Generation-via-Latent-Rendering"><a href="#151-LaRender-Training-Free-Occlusion-Control-in-Image-Generation-via-Latent-Rendering" class="headerlink" title="151. LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering"></a>151. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LaRender__Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering.pdf">LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent</span></p>
<p>LaRender提出了一种训练自由的图像生成方法，通过在预训练扩散模型的潜空间中引入物理原理的体渲染机制，实现对图像中多对象遮挡关系的精确控制。该方法无需模型重训练或微调，仅通过替换扩散模型的cross-attention层为潜渲染层，结合遮挡图和对象透明度，实现物理一致的遮挡及丰富视觉特效。实验显示LaRender在遮挡准确性上显著优于主流文本和布局控制生成方法，且可灵活调整对象透明度，带来半透明、密度变化等多种视觉效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="152-Surgical-Knowledge-Rewrite-in-Compact-LLMs-An-‘Unlearn-then-Learn’-Strategy-with-IA-3-for-Localized-Factual-Modulation-and-Catastrophic-Forgetting-Mitigation"><a href="#152-Surgical-Knowledge-Rewrite-in-Compact-LLMs-An-‘Unlearn-then-Learn’-Strategy-with-IA-3-for-Localized-Factual-Modulation-and-Catastrophic-Forgetting-Mitigation" class="headerlink" title="152. Surgical Knowledge Rewrite in Compact LLMs: An ‘Unlearn-then-Learn’ Strategy with (IA^3) for Localized Factual Modulation and Catastrophic Forgetting Mitigation"></a>152. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Surgical_Knowledge_Rewrite_in_Compact_LLMs__An_'Unlearn-then-Learn'_Strategy_with_($IA^3$)_for_Local.pdf">Surgical Knowledge Rewrite in Compact LLMs: An ‘Unlearn-then-Learn’ Strategy with (IA^3) for Localized Factual Modulation and Catastrophic Forgetting Mitigation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanley Ngugi</span></p>
<p>本文提出了一种基于PEFT技术IA^3的“unlearn-then-learn”知识编辑策略，首先通过机制可解释性分析定位模型内部编码冲突事实的具体电路，再分两阶段分别抑制原有事实后引入新事实，实现了对紧凑型LLM（Phi-3-mini）内冲突知识的精准重写。实验表明该方法能以极高准确率（98.5%）编辑新事实，同时显著缓解灾难性遗忘（F_control准确率达72%），提出“软遗忘”机制，提升了模型的可控性与安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Surgical_Knowledge_Rewrite_in_Compact_LLMs_An_%27Unlearn-then-Learn%27_Strategy_with_%28IA%5E3%29_for_Localized_Factual_Modulation_and_Catastrophic_Forgetting_Mitigation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="153-Towards-Safer-AI-Moderation-Evaluating-LLM-Moderators-Through-a-Unified-Benchmark-Dataset-and-Advocating-a-Human-First-Approach"><a href="#153-Towards-Safer-AI-Moderation-Evaluating-LLM-Moderators-Through-a-Unified-Benchmark-Dataset-and-Advocating-a-Human-First-Approach" class="headerlink" title="153. Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach"></a>153. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Towards_Safer_AI_Moderation__Evaluating_LLM_Moderators_Through_a_Unified_Benchmark_Dataset_and_Advoc.pdf">Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fordham University</span></p>
<p>本文提出了一个涵盖49类情感、攻击性及偏见内容的统一人工标注基准数据集，用于系统性评估主流LLM内容审核模型（如OpenAI Moderator、Llama Guard），并开发了基于Phi-4的LoRA微调模型SafePhi。结果显示SafePhi在宏观F1分数上（0.89）显著优于其他模型，但所有LLM审核器在处理隐性和具文化背景的有害内容时均表现不足，作者呼吁引入多元化人工参与和持续人类反馈，提升模型鲁棒性与公平性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Towards_Safer_AI_Moderation_Evaluating_LLM_Moderators_Through_a_Unified_Benchmark_Dataset_and_Advocating_a_Human-First_Approach.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="154-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability"><a href="#154-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability" class="headerlink" title="154. ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability"></a>154. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ReasonRank__Empowering_Passage_Ranking_with_Strong_Reasoning_Ability.pdf">ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Renmin University of China</span></p>
<p>本文提出ReasonRank，一种增强推理能力的段落重排序方法，首先通过自动化数据合成框架生成高质量、跨领域的推理密集型训练数据，并采用两阶段训练：冷启动监督微调（SFT）学习推理模式，随后利用多视角排序奖励的强化学习进一步提升排序能力。实验表明，ReasonRank在BRIGHT和R2MED等推理密集型信息检索基准上大幅优于现有方法，且推理效率更高。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ReasonRank_Empowering_Passage_Ranking_with_Strong_Reasoning_Ability.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="155-K-Dense-Analyst-Towards-Fully-Automated-Scientific-Analysis"><a href="#155-K-Dense-Analyst-Towards-Fully-Automated-Scientific-Analysis" class="headerlink" title="155. K-Dense Analyst: Towards Fully Automated Scientific Analysis"></a>155. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/K-Dense_Analyst__Towards_Fully_Automated_Scientific_Analysis.pdf">K-Dense Analyst: Towards Fully Automated Scientific Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Biostate AI, Palo Alto, CA</span></p>
<p>该论文提出了K-Dense Analyst，一个采用分层多智能体与双环反馈架构的自主科学分析系统，集成规划、执行和多重验证流程，实现了复杂生物信息学数据的全自动化分析。通过在BixBench基准测试中达到29.2%准确率，显著超越当前最强语言模型（如GPT-5），结果显示系统架构创新能大幅提升科学分析能力，而不仅依赖于基础大模型规模。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/K-Dense_Analyst_Towards_Fully_Automated_Scientific_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="156-From-Nodes-to-Narratives-Explaining-Graph-Neural-Networks-with-LLMs-and-Graph-Context"><a href="#156-From-Nodes-to-Narratives-Explaining-Graph-Neural-Networks-with-LLMs-and-Graph-Context" class="headerlink" title="156. From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context"></a>156. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Nodes_to_Narratives__Explaining_Graph_Neural_Networks_with_LLMs_and_Graph_Context.pdf">From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Chicago</span></p>
<p>该论文提出了LOGIC，一种结合GNN与LLM的新型解释框架，通过投影GNN节点嵌入到LLM嵌入空间，并构造软提示与文本混合的混合提示，使LLM能够基于GNN内部表示生成自然语言解释和支持解释的子图。实验表明，LOGIC在保持高忠实度和紧凑解释规模的同时，提升了可解释性和人类理解度，优于现有GNN解释器。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/From_Nodes_to_Narratives_Explaining_Graph_Neural_Networks_with_LLMs_and_Graph_Context.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="157-Investigating-Intersectional-Bias-in-Large-Language-Models-using-Confidence-Disparities-in-Coreference-Resolution"><a href="#157-Investigating-Intersectional-Bias-in-Large-Language-Models-using-Confidence-Disparities-in-Coreference-Resolution" class="headerlink" title="157. Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution"></a>157. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Investigating_Intersectional_Bias_in_Large_Language_Models_using_Confidence_Disparities_in_Coreferen.pdf">Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Apple</span></p>
<p>本论文提出了WinoIdentity基准数据集，通过对WinoBias进行多属性人口统计标注扩展，并开发了新的群体公平性指标“共指置信度差异”，用于评估大语言模型在共指消解任务中的交叉群体偏见和不确定性。实验证明，现有LLM在涉及多重身份群体时表现不佳，置信度差异最高可达40%，在反刻板印象情境下对双重弱势群体表现出更大不确定性，且模型更依赖记忆而非推理，带来有效性和公平性双重风险。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Investigating_Intersectional_Bias_in_Large_Language_Models_using_Confidence_Disparities_in_Coreference_Resolution.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="158-Hide-or-Highlight-Understanding-the-Impact-of-Factuality-Expression-on-User-Trust"><a href="#158-Hide-or-Highlight-Understanding-the-Impact-of-Factuality-Expression-on-User-Trust" class="headerlink" title="158. Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust"></a>158. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hide_or_Highlight__Understanding_the_Impact_of_Factuality_Expression_on_User_Trust.pdf">Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IBM Research</span></p>
<p>本论文提出了四种不同的LLM输出低事实性内容的表达策略，包括高亮、移除、模糊化和基线，并在人类实验中对这些策略进行了对比。结果表明，通过隐藏或模糊低事实性内容（opaque和ambiguity策略）能够提升用户对AI的信任，同时保持回答质量不变，优于直接高亮或全部展示低事实性内容。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Hide_or_Highlight_Understanding_the_Impact_of_Factuality_Expression_on_User_Trust.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="159-SEADialogues-A-Multilingual-Culturally-Grounded-Multi-turn-Dialogue-Dataset-on-Southeast-Asian-Languages"><a href="#159-SEADialogues-A-Multilingual-Culturally-Grounded-Multi-turn-Dialogue-Dataset-on-Southeast-Asian-Languages" class="headerlink" title="159. SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages"></a>159. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SEADialogues__A_Multilingual_Culturally_Grounded_Multi-turn_Dialogue_Dataset_on_Southeast_Asian_Lang.pdf">SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">MBZUAI</span></p>
<p>论文提出SEADIALOGUES，一个涵盖八种东南亚语言、具备文化和身份特征的多轮对话数据集。方法采用模板生成、文化实体词汇化、LLM生成对话和人工&#x2F;自动评估流程，提升数据集的文化适应性和个性化。结论显示，封闭权重的大语言模型在流畅性、自然性、文化相关性等指标上优于开源模型，强调高质量文化数据集对提升开放权重LLMs的重要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SEADialogues_A_Multilingual_Culturally_Grounded_Multi-turn_Dialogue_Dataset_on_Southeast_Asian_Languages.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="160-Membership-and-Memorization-in-LLM-Knowledge-Distillation"><a href="#160-Membership-and-Memorization-in-LLM-Knowledge-Distillation" class="headerlink" title="160. Membership and Memorization in LLM Knowledge Distillation"></a>160. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Membership_and_Memorization_in_LLM_Knowledge_Distillation.pdf">Membership and Memorization in LLM Knowledge Distillation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文系统性研究了大语言模型知识蒸馏（LLM KD）过程中学生模型对教师模型隐私风险的继承性，综合评估了六种主流LLM KD技术在七个NLP任务中对成员推断攻击（MIA）和数据记忆攻击的脆弱性。研究表明，所有现有的LLM KD方法均无法完全阻止教师私有数据的隐私泄露，但不同蒸馏技术和训练流程对隐私风险的影响存在显著差异，且隐私泄漏在模型不同transformer block之间差异较大。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Membership_and_Memorization_in_LLM_Knowledge_Distillation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="161-Trustworthy-Medical-Imaging-with-Large-Language-Models-A-Study-of-Hallucinations-Across-Modalities"><a href="#161-Trustworthy-Medical-Imaging-with-Large-Language-Models-A-Study-of-Hallucinations-Across-Modalities" class="headerlink" title="161. Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities"></a>161. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Trustworthy_Medical_Imaging_with_Large_Language_Models__A_Study_of_Hallucinations_Across_Modalities.pdf">Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Akron</span></p>
<p>本论文系统性分析了大语言模型（LLMs）在医学影像任务中的幻觉现象，涵盖图像到文本（影像解读报告生成）和文本到图像（医学影像合成）两大方向。通过专家标准对不同模型在胸片、CT、MRI等多模态医学影像下的输出进行定量和定性评估，发现现有LLM在医学影像理解和生成过程中普遍存在事实不符和解剖失真等幻觉，严重影响临床可靠性。结论指出需通过更稳健的提示工程、医学知识约束和严格验证机制来提升LLM在临床应用中的安全性和可信度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Trustworthy_Medical_Imaging_with_Large_Language_Models_A_Study_of_Hallucinations_Across_Modalities.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="162-MultiMedEdit-A-Scenario-Aware-Benchmark-for-Evaluating-Knowledge-Editing-in-Medical-VQA"><a href="#162-MultiMedEdit-A-Scenario-Aware-Benchmark-for-Evaluating-Knowledge-Editing-in-Medical-VQA" class="headerlink" title="162. MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA"></a>162. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MultiMedEdit__A_Scenario-Aware_Benchmark_for_Evaluating_Knowledge_Editing_in_Medical_VQA.pdf">MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Aeronautics and Astronautics</span></p>
<p>该论文提出了MultiMedEdit，这是首个专为医学多模态场景下知识编辑评估设计的基准，涵盖理解与推理两类任务，包含三维度评估指标（reliability, generality, locality），并系统对比Prompt、LoRA、GRACE、WISE等代表性编辑范式在一般和医学多模态大模型上的表现。结论显示当前知识编辑方法在复杂医学推理和长期编辑中存在泛化差、顺序敏感和遗忘等明显局限，MultiMedEdit为后续更安全、可靠的医学知识编辑研究奠定了基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MultiMedEdit_A_Scenario-Aware_Benchmark_for_Evaluating_Knowledge_Editing_in_Medical_VQA.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="163-MASteer-Multi-Agent-Adaptive-Steer-Strategy-for-End-to-End-LLM-Trustworthiness-Repair"><a href="#163-MASteer-Multi-Agent-Adaptive-Steer-Strategy-for-End-to-End-LLM-Trustworthiness-Repair" class="headerlink" title="163. MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair"></a>163. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MASteer__Multi-Agent_Adaptive_Steer_Strategy_for_End-to-End_LLM_Trustworthiness_Repair.pdf">MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出MASteer框架，通过多智能体协作自动生成对比样本（AutoTester）和自适应表示工程策略（AutoRepairer），实现大语言模型（LLM）端到端可信度修复。MASteer利用anchor vector实现推理时的动态策略选择，显著提升LLM在真实性、公平性和安全性等主流与定制任务上的可信度，且保持模型原有能力不受损失。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MASteer_Multi-Agent_Adaptive_Steer_Strategy_for_End-to-End_LLM_Trustworthiness_Repair.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="164-BoRA-Towards-More-Expressive-Low-Rank-Adaptation-with-Block-Diversity"><a href="#164-BoRA-Towards-More-Expressive-Low-Rank-Adaptation-with-Block-Diversity" class="headerlink" title="164. BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity"></a>164. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BoRA__Towards_More_Expressive_Low-Rank_Adaptation_with_Block_Diversity.pdf">BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huazhong University of Science and Technology</span></p>
<p>本文提出了Block-Diversified Low-Rank Adaptation (BoRA)，通过将LoRA中的低秩矩阵分块并为每一块乘积引入独特的对角矩阵，有效提升了LoRA权重的秩，从而增强了大语言模型的参数高效微调能力。实验表明，BoRA在多个模型和数据集上均显著优于LoRA及其变体，在参数量相近的情况下提升了2-4%的准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BoRA_Towards_More_Expressive_Low-Rank_Adaptation_with_Block_Diversity.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="165-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance"><a href="#165-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance" class="headerlink" title="165. AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance"></a>165. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AMFT__Aligning_LLM_Reasoners_by_Meta-Learning_the_Optimal_Imitation-Exploration_Balance.pdf">AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出了Adaptive Meta Fine-Tuning (AMFT)，一种通过单阶段训练动态融合Supervised Fine-Tuning (SFT)与Reinforcement Learning (RL)的LLM对齐方法。核心创新是利用meta-gradient自适应权重控制器，根据验证集的长远任务表现自动调整SFT与RL的权重，实现从模仿到探索的最优平衡。实验表明AMFT在数学、视觉推理和视觉-语言导航等多模态基准上均达到了新的SOTA，在样本和计算效率上也优于现有方法。结论是AMFT能有效避免灾难性遗忘和策略崩溃，提升泛化与稳健性，是更具理论基础的自动化LLM对齐范式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AMFT_Aligning_LLM_Reasoners_by_Meta-Learning_the_Optimal_Imitation-Exploration_Balance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="166-CLAP-Coreference-Linked-Augmentation-for-Passage-Retrieval"><a href="#166-CLAP-Coreference-Linked-Augmentation-for-Passage-Retrieval" class="headerlink" title="166. CLAP: Coreference-Linked Augmentation for Passage Retrieval"></a>166. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CLAP__Coreference-Linked_Augmentation_for_Passage_Retrieval.pdf">CLAP: Coreference-Linked Augmentation for Passage Retrieval</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Adelaide</span></p>
<p>本文提出了CLAP框架，通过语义分块、指代消解和局部伪查询生成，将长文本分解为结构化且语义清晰的子块，并用LLM生成与检索器语义空间对齐的局部伪查询。最终通过融合全局（查询-文档）与局部（查询-伪查询）相关性得分，显著提升了稀疏和稠密检索性能，尤其在领域外场景下超越主流二阶段重排序模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CLAP_Coreference-Linked_Augmentation_for_Passage_Retrieval.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="167-CROP-Integrating-Topological-and-Spatial-Structures-via-Cross-View-Prefixes-for-Molecular-LLMs"><a href="#167-CROP-Integrating-Topological-and-Spatial-Structures-via-Cross-View-Prefixes-for-Molecular-LLMs" class="headerlink" title="167. CROP: Integrating Topological and Spatial Structures via Cross-View Prefixes for Molecular LLMs"></a>167. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CROP__Integrating_Topological_and_Spatial_Structures_via_Cross-View_Prefixes_for_Molecular_LLMs.pdf">CROP: Integrating Topological and Spatial Structures via Cross-View Prefixes for Molecular LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>该论文提出了CROP框架，通过SMILES引导的跨视图前缀联合采样分子图和分子图像特征，有效整合分子的拓扑结构和空间结构，增强多模态大语言模型对分子的理解力。实验结果表明，CROP在分子描述生成、IUPAC命名预测和分子属性预测等任务上显著优于现有方法，实现了更高的效率和准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CROP_Integrating_Topological_and_Spatial_Structures_via_Cross-View_Prefixes_for_Molecular_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="168-DocRefine-An-Intelligent-Framework-for-Scientific-Document-Understanding-and-Content-Optimization-based-on-Multimodal-Large-Model-Agents"><a href="#168-DocRefine-An-Intelligent-Framework-for-Scientific-Document-Understanding-and-Content-Optimization-based-on-Multimodal-Large-Model-Agents" class="headerlink" title="168. DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents"></a>168. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DocRefine__An_Intelligent_Framework_for_Scientific_Document_Understanding_and_Content_Optimization_b.pdf">DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shangqiu University</span></p>
<p>DocRefine提出了一种基于多模态大模型（如GPT-4o）的多智能体系统，通过布局结构分析、内容理解、指令分解、内容优化、摘要生成及一致性验证六个协作代理，实现科学PDF文档的智能理解与内容优化。实验结果显示其在DocEditBench数据集上语义一致性（SCS）、版式保真度（LFI）和指令遵循率（IAR）均超过现有方法，显著提升多模态复杂文档的编辑质量与控制力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DocRefine_An_Intelligent_Framework_for_Scientific_Document_Understanding_and_Content_Optimization_based_on_Multimodal_Large_Model_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="169-DATASETRESEARCH-Benchmarking-Agent-Systems-for-Demand-Driven-Dataset-Discovery"><a href="#169-DATASETRESEARCH-Benchmarking-Agent-Systems-for-Demand-Driven-Dataset-Discovery" class="headerlink" title="169. DATASETRESEARCH: Benchmarking Agent Systems for Demand-Driven Dataset Discovery"></a>169. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DatasetResearch__Benchmarking_Agent_Systems_for_Demand-Driven_Dataset_Discovery.pdf">DATASETRESEARCH: Benchmarking Agent Systems for Demand-Driven Dataset Discovery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了DATASETRESEARCH，首个系统性评测智能体在需求驱动数据集发现和合成方面能力的基准，覆盖208个真实NLP任务需求，并分类为知识型和推理型。通过多维度评测框架，发现搜索型智能体在知识任务表现突出，合成型智能体在推理任务中表现优异，但所有方法在“角落案例”上均失败，揭示现有技术在泛化和指令匹配上的重大不足。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DATASETRESEARCH_Benchmarking_Agent_Systems_for_Demand-Driven_Dataset_Discovery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="170-Large-Language-Models-Do-Not-Simulate-Human-Psychology"><a href="#170-Large-Language-Models-Do-Not-Simulate-Human-Psychology" class="headerlink" title="170. Large Language Models Do Not Simulate Human Psychology"></a>170. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Large_Language_Models_Do_Not_Simulate_Human_Psychology.pdf">Large Language Models Do Not Simulate Human Psychology</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bielefeld University</span></p>
<p>本论文通过理论分析和实证实验，质疑大型语言模型（如GPT-4、Llama-3.1、CENTAUR）能够模拟人类心理学的观点。作者通过对道德判断任务的对比实验证明，LLMs对语义微变的反应与真实人类存在显著差异，且不同模型表现不一致，结论认为LLMs无法可靠模拟人类心理学，建议心理学研究中须谨慎对待LLMs，并需针对具体任务重新验证。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Large_Language_Models_Do_Not_Simulate_Human_Psychology.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="171-When-Prompt-Engineering-Meets-Software-Engineering-CNL-P-as-Natural-and-Robust-“APIs”-for-Human-AI-Interaction"><a href="#171-When-Prompt-Engineering-Meets-Software-Engineering-CNL-P-as-Natural-and-Robust-“APIs”-for-Human-AI-Interaction" class="headerlink" title="171. When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust “APIs” for Human-AI Interaction"></a>171. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/When_Prompt_Engineering_Meets_Software_Engineering__CNL-P_as_Natural_and_Robust__APIs''_for_Human-AI.pdf">When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust “APIs” for Human-AI Interaction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CSIRO’s Data61</span></p>
<p>本论文提出了CNL-P（Controlled Natural Language for Prompt），结合了最佳的Prompt Engineering（PE）实践与软件工程（SE）核心原则，通过引入精确的语法结构与严格的语义规范，降低自然语言提示的歧义，提升用户意图表达的结构化和准确性。论文还开发了NL到CNL-P的自动转换工具和静态分析Linting工具，首次将静态分析技术引入自然语言提示，实验证明CNL-P能提升LLM输出一致性和质量，其语法和语义对LLM友好，且Linting工具能有效发现和定位提示中的语法及语义错误。结论：CNL-P为基于自然语言的新型软件工程范式和人机协作提供了基础，并推动了SE4AI基础设施的建设。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/When_Prompt_Engineering_Meets_Software_Engineering_CNL-P_as_Natural_and_Robust_%E2%80%9CAPIs%E2%80%9D_for_Human-AI_Interaction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="172-Model-Agnostic-Sentiment-Distribution-Stability-Analysis-for-Robust-LLM-Generated-Texts-Detection"><a href="#172-Model-Agnostic-Sentiment-Distribution-Stability-Analysis-for-Robust-LLM-Generated-Texts-Detection" class="headerlink" title="172. Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection"></a>172. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Model-Agnostic_Sentiment_Distribution_Stability_Analysis_for_Robust_LLM-Generated_Texts_Detection.pdf">Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science, Shanghai Jiao Tong University</span></p>
<p>本文提出SentiDetect，一种无需模型参数访问、无监督的LLM生成文本检测框架。方法基于情感分布稳定性假设，通过低情感重写和情感一致性、保持性两项指标，对原始及重写文本的情感分布进行特征分析来区分人类与LLM生成内容，实验证明其在多数据集和多模型下均显著优于现有方法，并对抗扰动和改写有更强鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Model-Agnostic_Sentiment_Distribution_Stability_Analysis_for_Robust_LLM-Generated_Texts_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="173-BASIC-Boosting-Visual-Alignment-with-Intrinsic-Refined-Embeddings-in-Multimodal-Large-Language-Models"><a href="#173-BASIC-Boosting-Visual-Alignment-with-Intrinsic-Refined-Embeddings-in-Multimodal-Large-Language-Models" class="headerlink" title="173. BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models"></a>173. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BASIC__Boosting_Visual_Alignment_with_Intrinsic_Refined_Embeddings_in_Multimodal_Large_Language_Mode.pdf">BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本文提出了BASIC方法，通过利用多模态大语言模型（MLLM）浅层内部细化的视觉嵌入作为监督信号，从方向对齐和语义分布两个角度直接优化初始视觉嵌入。实验结果表明，BASIC无需额外监督模型和人工标注，即可显著提升多种主流MLLM在多项视觉-语言基准测试中的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BASIC_Boosting_Visual_Alignment_with_Intrinsic_Refined_Embeddings_in_Multimodal_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="174-MeteorPred-A-Meteorological-Multimodal-Large-Model-and-Dataset-for-Severe-Weather-Event-Prediction"><a href="#174-MeteorPred-A-Meteorological-Multimodal-Large-Model-and-Dataset-for-Severe-Weather-Event-Prediction" class="headerlink" title="174. MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction"></a>174. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MeteorPred__A_Meteorological_Multimodal_Large_Model_and_Dataset_for_Severe_Weather_Event_Prediction.pdf">MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">MAIS, Institute of Automation, Chinese Academy of Sciences</span></p>
<p>该论文提出了MP-Bench，一个包含42万对原始多维气象数据与文本警报的大规模时序多模态数据集，并基于此开发了气象多模态大模型MMLM。MMLM创新性地引入了三种可插拔融合模块（DTGF、TGS、TGCA），实现对4D气象数据的时序、空间和垂直特征的动态提取与融合。实验结果显示，MMLM在多项极端天气事件预测任务上显著优于现有闭源和开源多模态大模型，实现了对极端天气的自动化理解与预警，推动了AI气象预报的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MeteorPred_A_Meteorological_Multimodal_Large_Model_and_Dataset_for_Severe_Weather_Event_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="175-Highlight-All-the-Phrases-Enhancing-LLM-Transparency-through-Visual-Factuality-Indicators"><a href="#175-Highlight-All-the-Phrases-Enhancing-LLM-Transparency-through-Visual-Factuality-Indicators" class="headerlink" title="175. Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators"></a>175. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Highlight_All_the_Phrases__Enhancing_LLM_Transparency_through_Visual_Factuality_Indicators.pdf">Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IBM Research</span></p>
<p>该论文系统设计并对比了六种可视化大语言模型（LLM）输出内容事实性分数的界面设计方案，通过两轮共208名用户参与的场景实验，评估了不同设计对用户信任、验证准确性难易度和偏好的影响。结果显示，将模型回复中所有短语按事实性分数进行颜色高亮（phrase-level highlight-all）最受用户青睐，显著提升了信任和验证准确性体验，且能帮助用户更有效校准对LLM的信任。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Highlight_All_the_Phrases_Enhancing_LLM_Transparency_through_Visual_Factuality_Indicators.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="176-BiXSE-Improving-Dense-Retrieval-via-Probabilistic-Graded-Relevance-Distillation"><a href="#176-BiXSE-Improving-Dense-Retrieval-via-Probabilistic-Graded-Relevance-Distillation" class="headerlink" title="176. BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation"></a>176. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BiXSE__Improving_Dense_Retrieval_via_Probabilistic_Graded_Relevance_Distillation.pdf">BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universite de Montréal</span></p>
<p>该论文提出了BiXSE方法，用于利用LLM生成的细粒度分级相关性标签，通过点对点二元交叉熵损失优化密集检索模型，显著提升了检索与句子嵌入任务的表现。实验结果显示，BiXSE优于传统的InfoNCE和Pairwise Ranking Loss，不仅缩小了与大型零样本LLM排序器的性能差距，还具备更强的噪声鲁棒性和数据利用效率，尤其适用于大规模LLM标注数据的高效蒸馏。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BiXSE_Improving_Dense_Retrieval_via_Probabilistic_Graded_Relevance_Distillation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="177-Zero-Direction-Probing-A-Linear-Algebraic-Framework-for-Deep-Analysis-of-Large-Language-Model-Drift"><a href="#177-Zero-Direction-Probing-A-Linear-Algebraic-Framework-for-Deep-Analysis-of-Large-Language-Model-Drift" class="headerlink" title="177. Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift"></a>177. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Zero-Direction_Probing__A_Linear-Algebraic_Framework_for_Deep_Analysis_of_Large-Language-Model_Drift.pdf">Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AI Analytics</span></p>
<p>该论文提出了零方向探测（Zero-Direction Probing, ZDP）理论框架，通过分析变换器层激活的右&#x2F;左零空间及其Fisher信息几何，在无需任务标签和输出的情况下刻画大语言模型的漂移。核心方法包括定义零泄漏功能量、提出方差-泄漏定理、Fisher零守恒、低秩更新的泄漏界、以及基于随机矩阵理论的漂移阈值，配合在线零空间追踪器与零对齐LoRA优化器，并提供了理论收敛和漂移报警保证。结论认为，通过“监听沉默”即监控零空间，可以无监督、理论上严格地检测和界定大语言模型表征的变化，为后续应用和漂移控制提供了可验证的理论基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Zero-Direction_Probing_A_Linear-Algebraic_Framework_for_Deep_Analysis_of_Large-Language-Model_Drift.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="178-SafePLUG-Empowering-Multimodal-LLMs-with-Pixel-Level-Insight-and-Temporal-Grounding-for-Traffic-Accident-Understanding"><a href="#178-SafePLUG-Empowering-Multimodal-LLMs-with-Pixel-Level-Insight-and-Temporal-Grounding-for-Traffic-Accident-Understanding" class="headerlink" title="178. SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding"></a>178. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SafePLUG__Empowering_Multimodal_LLMs_with_Pixel-Level_Insight_and_Temporal_Grounding_for_Traffic_Acc.pdf">SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Wisconsin–Madison</span></p>
<p>该论文提出SafePLUG框架，通过视觉和数字提示，将像素级理解与时间定位能力集成到多模态大语言模型中，实现复杂交通事故场景的细粒度分析。方法包括视觉区域提示编码、SAM像素分割、数字提示进行时序定位，并构建了首个支持区域QA和像素级标注的交通事故多模态数据集。实验表明SafePLUG在区域问答、像素分割、事故描述和时序定位等多项任务上优于现有主流模型，显著提升了交通事故理解的精度和实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SafePLUG_Empowering_Multimodal_LLMs_with_Pixel-Level_Insight_and_Temporal_Grounding_for_Traffic_Accident_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="179-Many-Turn-Jailbreaking"><a href="#179-Many-Turn-Jailbreaking" class="headerlink" title="179. Many-Turn Jailbreaking"></a>179. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Many-Turn_Jailbreaking.pdf">Many-Turn Jailbreaking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Santa Barbara</span></p>
<p>该论文首次系统提出了多轮次（many-turn）Jailbreaking攻击大语言模型（LLMs）的新威胁，突破以往只关注单轮攻击的局限。作者构建了MTJ-Bench多轮Jailbreaking基准数据集，并在15个主流开源及闭源模型上进行了系统实验，发现一旦模型被首轮攻破，后续相关或无关的恶意提问均极易获得有害回复，且这一漏洞普遍存在于各类LLM中。结论认为，多轮Jailbreaking显著增大模型安全风险，亟需社区关注与防护，MTJ-Bench可作为该方向研究新标准测试集。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Many-Turn_Jailbreaking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="180-MDK12-Bench-A-Comprehensive-Evaluation-of-Multimodal-Large-Language-Models-on-Multidisciplinary-Exams"><a href="#180-MDK12-Bench-A-Comprehensive-Evaluation-of-Multimodal-Large-Language-Models-on-Multidisciplinary-Exams" class="headerlink" title="180. MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams"></a>180. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MDK12-Bench__A_Comprehensive_Evaluation_of_Multimodal_Large_Language_Models_on_Multidisciplinary_Exa.pdf">MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai AI Laboratory</span></p>
<p>该论文提出MDK12-Bench，一个覆盖六大学科、包含14.1万题的大规模多模态大语言模型评测基准，具备多层知识点结构和多维度评估能力。方法包括动态评测框架（文本&#x2F;视觉扰动）和知识点增强生成（KP-RAG），揭示现有MLLM在跨难度、跨年份与复杂泛化任务上的局限性。结论显示，当前主流MLLM在上下文变化和复杂推理任务中普遍表现差，知识补充对高难题提升有限，为模型稳健性和教育应用提供指导。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MDK12-Bench_A_Comprehensive_Evaluation_of_Multimodal_Large_Language_Models_on_Multidisciplinary_Exams.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="181-Remote-Sensing-Image-Intelligent-Interpretation-with-the-Language-Centered-Perspective-Principles-Methods-and-Challenges"><a href="#181-Remote-Sensing-Image-Intelligent-Interpretation-with-the-Language-Centered-Perspective-Principles-Methods-and-Challenges" class="headerlink" title="181. Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges"></a>181. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Remote_Sensing_Image_Intelligent_Interpretation_with_the_Language-Centered_Perspective__Principles,_.pdf">Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Central South University</span></p>
<p>本论文提出了一种基于语言中心视角的遥感图像智能解译框架，借鉴人类认知中的全球工作空间理论，将大语言模型（LLM）作为认知中枢，统一融合感知、知识、任务和行动空间，实现多模态语义表示、知识关联和推理决策闭环。论文系统综述了该范式的关键技术挑战与现有进展，指出未来需关注多模态自适应对齐、动态知识约束下的任务理解、可信推理及自主交互。结论认为，语言中心框架能显著提升遥感解译系统的泛化性、交互性和推理能力，推动遥感领域迈向认知智能新阶段。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Remote_Sensing_Image_Intelligent_Interpretation_with_the_Language-Centered_Perspective_Principles%2C_Methods_and_Challenges.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="182-Technical-Report-Full-Stack-Fine-Tuning-for-the-Q-Programming-Language"><a href="#182-Technical-Report-Full-Stack-Fine-Tuning-for-the-Q-Programming-Language" class="headerlink" title="182. Technical Report: Full-Stack Fine-Tuning for the Q Programming Language"></a>182. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Technical_Report__Full-Stack_Fine-Tuning_for_the_Q_Programming_Language.pdf">Technical Report: Full-Stack Fine-Tuning for the Q Programming Language</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Morgan Stanley</span></p>
<p>本文提出了一套完整的开源方法，针对金融领域常用但网络样本稀缺的Q编程语言构建Leetcode风格的评测集，并基于Qwen-2.5系列大语言模型，通过预训练、监督微调和强化学习三阶段，系统提升模型在Q代码生成上的表现。最终，32B参数模型在Q基准测试中pass@1达59%，较Claude Opus-4提升29.5%，所有模型均超越GPT-4.1，验证了方法的有效性，并发布了数据、代码和训练流程，为小语种领域模型适配提供可复用蓝图，结论是大模型通过系统性领域适配能实现对专有编程语言的显著性能提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Technical_Report_Full-Stack_Fine-Tuning_for_the_Q_Programming_Language.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="183-LSDTs-LLM-Augmented-Semantic-Digital-Twins-for-Adaptive-Knowledge-Intensive-Infrastructure-Planning"><a href="#183-LSDTs-LLM-Augmented-Semantic-Digital-Twins-for-Adaptive-Knowledge-Intensive-Infrastructure-Planning" class="headerlink" title="183. LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning"></a>183. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LSDTs__LLM-Augmented_Semantic_Digital_Twins_for_Adaptive_Knowledge-Intensive_Infrastructure_Planning.pdf">LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland, College Park</span></p>
<p>该论文提出LSDTs框架，将大型语言模型（LLM）与语义数字孪生（DT）集成，实现从非结构化规划文件自动提取和组织知识为本体，然后驱动基础设施数字孪生进行合规、可解释的规划模拟和优化。结果表明，LSDTs可自动整合多源约束，提升基础设施规划的适应性和合规性，尤其在风电场布局与极端气象应对场景中表现出色。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LSDTs_LLM-Augmented_Semantic_Digital_Twins_for_Adaptive_Knowledge-Intensive_Infrastructure_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="184-PROPS-Progressively-Private-Self-alignment-of-Large-Language-Models"><a href="#184-PROPS-Progressively-Private-Self-alignment-of-Large-Language-Models" class="headerlink" title="184. PROPS: Progressively Private Self-alignment of Large Language Models"></a>184. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PROPS__Progressively_Private_Self-alignment_of_Large_Language_Models.pdf">PROPS: Progressively Private Self-alignment of Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Arizona</span></p>
<p>本文提出了PROPS（Progressively Private Self-alignment），一种多阶段隐私保护大语言模型（LLM）对齐框架，通过将训练数据分批并结合随机响应（RR）机制与中间模型自标注及最大似然估计（MLE），实现对人类偏好标签的差分隐私保护。实验结果表明，在相同隐私预算下，PROPS在对齐性能上显著优于传统的DP-SGD和RR方法，尤其在高隐私要求下模型效用提升显著。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/PROPS_Progressively_Private_Self-alignment_of_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="185-Story-Ribbons-Reimagining-Storyline-Visualizations-with-Large-Language-Models"><a href="#185-Story-Ribbons-Reimagining-Storyline-Visualizations-with-Large-Language-Models" class="headerlink" title="185. Story Ribbons: Reimagining Storyline Visualizations with Large Language Models"></a>185. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Story_Ribbons__Reimagining_Storyline_Visualizations_with_Large_Language_Models.pdf">Story Ribbons: Reimagining Storyline Visualizations with Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harvard University</span></p>
<p>该论文提出了一个基于大型语言模型（LLM）的自动数据解析流水线，能从小说和剧本中提取角色、地点、主题等叙事要素，并通过STORY RIBBONS交互系统进行可视化。实验和用户研究显示，该方法能提升文学分析的效率和深度，尽管当前LLM在把握文学细节和背景上仍有限，但整体上为文学研究带来了新的洞察和互动体验。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Story_Ribbons_Reimagining_Storyline_Visualizations_with_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="186-Fed-MobiLLM-Efficient-Federated-LLM-Fine-Tuning-over-Heterogeneous-Mobile-Devices-via-Server-Assisted-Side-Tuning"><a href="#186-Fed-MobiLLM-Efficient-Federated-LLM-Fine-Tuning-over-Heterogeneous-Mobile-Devices-via-Server-Assisted-Side-Tuning" class="headerlink" title="186. Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning"></a>186. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fed_MobiLLM__Efficient_Federated_LLM_Fine-Tuning_over_Heterogeneous_Mobile_Devices_via_Server_Assist.pdf">Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Houston</span></p>
<p>Fed MobiLLM提出了一种面向异构移动设备的高效联邦大语言模型(LLM)微调框架，采用异步服务端辅助的侧网络微调方式，将所有可训练参数集中在服务器侧，设备端仅需冻结主干模型并上传中间激活。该方法通过分层激活采样和跨结构对齐机制，支持不同容量主干模型协同训练共享侧网络，实现对设备异构性的鲁棒支持。实验显示，Fed MobiLLM显著降低了设备端内存、计算和通信开销，提升了收敛速度，并在各种数据异构条件下保持竞争性精度，验证其实用性和高效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Fed_MobiLLM_Efficient_Federated_LLM_Fine-Tuning_over_Heterogeneous_Mobile_Devices_via_Server_Assisted_Side-Tuning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="187-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding"><a href="#187-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding" class="headerlink" title="187. VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding"></a>187. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VisR-Bench__An_Empirical_Study_on_Visual_Retrieval-Augmented_Generation_for_Multilingual_Long_Docume.pdf">VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University at Buffalo</span></p>
<p>该论文提出了VisR-Bench，这是首个针对多语言、长文档多模态检索与问答的基准，支持16种语言，涵盖文本、表格、图像等多种结构信息。作者构建了包含35,000多个高质量QA对和1,286份文档的数据集，并系统评估了文本检索、多模态编码器和多模态大语言模型（MLLMs）的检索能力。实验显示，MLLMs在多模态检索上优于传统方法，但对结构化表格和低资源语言仍有明显短板，强调了多语种多模态检索的挑战。结论认为VisR-Bench推动了多模态RAG与文档理解系统的发展，并为未来改进多语言MLLM检索能力提供了新基准和分析工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/VisR-Bench_An_Empirical_Study_on_Visual_Retrieval-Augmented_Generation_for_Multilingual_Long_Document_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="188-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy"><a href="#188-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy" class="headerlink" title="188. Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy"></a>188. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Democratizing_Diplomacy__A_Harness_for_Evaluating_Any_Large_Language_Model_on_Full-Press_Diplomacy.pdf">Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Good Start Labs</span></p>
<p>该论文提出首个可使任意本地大型语言模型（LLM）无需微调即可参与高复杂度的全回合Diplomacy游戏的评测工具。方法包括优化文本化游戏状态表示、批量对比13种主流模型表现、引入关键状态分析（CSA）深度实验，并开发承诺追踪等自动化评估协议。实验发现，模型规模与战略表现正相关，且小型模型也具备完成游戏的能力。结论指出，战略推理能力可自然从广泛应用的LLM中涌现，框架降低了战略AI研究门槛，并揭示了AI在多智能体环境中的欺骗与适应性行为。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Democratizing_Diplomacy_A_Harness_for_Evaluating_Any_Large_Language_Model_on_Full-Press_Diplomacy.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="189-Event-Aware-Sentiment-Factors-from-LLM-Augmented-Financial-Tweets-A-Transparent-Framework-for-Interpretable-Quant-Trading"><a href="#189-Event-Aware-Sentiment-Factors-from-LLM-Augmented-Financial-Tweets-A-Transparent-Framework-for-Interpretable-Quant-Trading" class="headerlink" title="189. Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading"></a>189. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Event-Aware_Sentiment_Factors_from_LLM-Augmented_Financial_Tweets__A_Transparent_Framework_for_Inter.pdf">Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cambridge</span></p>
<p>本文提出了一种透明的量化交易框架，通过使用大型语言模型（LLM）对金融推文进行多标签事件分类和情感强度打分，将非结构化社交媒体文本转化为可解释、结构化的事件驱动因子，并系统性地评估其对未来1-7天股票收益的预测能力。实验显示，部分LLM标注的事件标签（如谣言&#x2F;投机、散户热议等）显著产生负alpha，表明这些语义丰富的情感因子具有独立的预测价值，并且研究代码与方法已开源，有助于推动行为金融和可解释算法交易研究。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Event-Aware_Sentiment_Factors_from_LLM-Augmented_Financial_Tweets_A_Transparent_Framework_for_Interpretable_Quant_Trading.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="190-Generative-AI-for-Strategic-Plan-Development"><a href="#190-Generative-AI-for-Strategic-Plan-Development" class="headerlink" title="190. Generative AI for Strategic Plan Development"></a>190. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Generative_AI_for_Strategic_Plan_Development.pdf">Generative AI for Strategic Plan Development</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Johns Hopkins University</span></p>
<p>本文提出了一个模块化认知模型，利用生成式人工智能（GAI）和主流自然语言处理技术（如BERTopic和NMF）自动化开发大型政府组织战略规划的流程。通过对美国能源部相关GAO报告的主题建模，评估BERTopic和NMF在提取战略规划愿景要素主题的有效性，结果显示BERTopic能覆盖100%愿景要素且中高相关性占比超过一半，优于NMF，证明GAI辅助战略规划具可行性和实用价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Generative_AI_for_Strategic_Plan_Development.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="191-LET-US-Long-Event-Text-Understanding-of-Scenes"><a href="#191-LET-US-Long-Event-Text-Understanding-of-Scenes" class="headerlink" title="191. LET-US: Long Event-Text Understanding of Scenes"></a>191. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LET-US__Long_Event-Text_Understanding_of_Scenes.pdf">LET-US: Long Event-Text Understanding of Scenes</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>该论文提出LET-US，一种针对长事件流与文本多模态理解的框架。方法包括跨模态引导压缩与层次聚类，显著减少输入事件数据体量并保留关键信息，通过两阶段优化（RGB预训练+事件流微调）实现事件-文本对齐，辅以大规模事件-文本数据集EIQA-1M和EVQA-Bench进行训练与评测。结果表明LET-US在事件流分类、推理、时序定位和描述等任务上大幅优于现有多模态大模型，实现对极长事件流的语义理解。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LET-US_Long_Event-Text_Understanding_of_Scenes.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="192-ALOPE-Adaptive-Layer-Optimization-for-Translation-Quality-Estimation-using-Large-Language-Models"><a href="#192-ALOPE-Adaptive-Layer-Optimization-for-Translation-Quality-Estimation-using-Large-Language-Models" class="headerlink" title="192. ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models"></a>192. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ALOPE__Adaptive_Layer_Optimization_for_Translation_Quality_Estimation_using_Large_Language_Models.pdf">ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Surrey</span></p>
<p>该论文提出ALOPE框架，通过在大语言模型（LLM）中集成回归头和低秩适配器（LoRA），实现对不同Transformer层进行自适应优化，以提升机器翻译质量估计（QE）任务的回归性能。ALOPE利用层内嵌回归头、动态加权及多头回归策略，有效提升了低资源语言对的翻译质量评估效果，并在多个公开数据集上超过了现有指令微调和编码器基线方法。结论指出，Transformer中间层（如TL-7）能捕捉更佳跨语种表示，ALOPE在多语言和单语回归任务中均表现优异，且具备高内存效率与可扩展性，适合资源受限环境。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ALOPE_Adaptive_Layer_Optimization_for_Translation_Quality_Estimation_using_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="193-Grounding-Natural-Language-for-Multi-agent-Decision-Making-with-Multi-agentic-LLMs"><a href="#193-Grounding-Natural-Language-for-Multi-agent-Decision-Making-with-Multi-agentic-LLMs" class="headerlink" title="193. Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs"></a>193. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Grounding_Natural_Language_for_Multi-agent_Decision-Making_with_Multi-agentic_LLMs.pdf">Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">UC Davis</span></p>
<p>本文提出了一个系统化的多智能体大语言模型（multi-agentic LLM）设计框架，将LLMs与多智能体决策算法结合，涵盖高级提示工程、记忆架构、多模态处理及通过微调实现对环境动态的对齐。实验表明，经过微调和机制设计的多智能体LLM能在经典博弈任务中实现更高效、可解释的协作与决策，支持复杂环境下的鲁棒和自适应行为。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Grounding_Natural_Language_for_Multi-agent_Decision-Making_with_Multi-agentic_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="194-Grounding-Multilingual-Multimodal-LLMs-With-Cultural-Knowledge"><a href="#194-Grounding-Multilingual-Multimodal-LLMs-With-Cultural-Knowledge" class="headerlink" title="194. Grounding Multilingual Multimodal LLMs With Cultural Knowledge"></a>194. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Grounding_Multilingual_Multimodal_LLMs_With_Cultural_Knowledge.pdf">Grounding Multilingual Multimodal LLMs With Cultural Knowledge</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carnegie Mellon University</span></p>
<p>本文提出了一种面向数据的方法，通过从Wikidata等大型知识图谱中提取实体及图片，结合LLM自动生成多语言多模态视觉问答（VQA）对，构建了22M条涵盖39种语言和42个国家的文化多样数据集CulturalGround，并据此微调开放式多模态大模型CulturalPangea。实验表明，CulturalPangea在多个文化相关多语言多模态基准上实现了SOTA（平均提升+5%），同时不损失主流视觉-语言任务性能，证明了数据驱动的文化知识注入可有效提升模型的跨文化包容性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Grounding_Multilingual_Multimodal_LLMs_With_Cultural_Knowledge.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="195-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems"><a href="#195-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems" class="headerlink" title="195. A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems"></a>195. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Comprehensive_Survey_of_Self-Evolving_AI_Agents__A_New_Paradigm_Bridging_Foundation_Models_and_Lif.pdf">A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Glasgow</span></p>
<p>本论文系统综述了自进化AI智能体的发展，提出了一个统一的反馈优化框架，涵盖系统输入、智能体系统、环境和优化器四大核心组件，系统梳理了单智能体、多智能体及领域特定（如生物医学、编程、金融等）自进化技术。结论指出，自进化AI智能体通过持续反馈与自我优化，推动了基础模型向具有终身学习和适应能力的自主智能体系统的转变，是未来构建安全、高效、可持续AI生态系统的关键方向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Comprehensive_Survey_of_Self-Evolving_AI_Agents_A_New_Paradigm_Bridging_Foundation_Models_and_Lifelong_Agentic_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="196-Urbanite-A-Dataflow-Based-Framework-for-Human-AI-Interactive-Alignment-in-Urban-Visual-Analytics"><a href="#196-Urbanite-A-Dataflow-Based-Framework-for-Human-AI-Interactive-Alignment-in-Urban-Visual-Analytics" class="headerlink" title="196. Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics"></a>196. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Urbanite__A_Dataflow-Based_Framework_for_Human-AI_Interactive_Alignment_in_Urban_Visual_Analytics.pdf">Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Chicago</span></p>
<p>Urbanite提出了一套以数据流为基础的人机协同城市可视分析框架，核心创新是将大语言模型（LLM）与模块化数据流建模结合，通过自然语言描述、高层任务分解、可追溯过程、解释性和多粒度交互，将用户意图转化为可执行分析流程。结论显示，Urbanite有效降低了非技术专家构建复杂城市可视分析系统的门槛，提高了透明度、可追溯性和人机意图对齐，专家评测和定量实验表明其能高效生成、对齐和迭代城市分析任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Urbanite_A_Dataflow-Based_Framework_for_Human-AI_Interactive_Alignment_in_Urban_Visual_Analytics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="197-Rethinking-Domain-Specific-LLM-Benchmark-Construction-A-Comprehensiveness-Compactness-Approach"><a href="#197-Rethinking-Domain-Specific-LLM-Benchmark-Construction-A-Comprehensiveness-Compactness-Approach" class="headerlink" title="197. Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach"></a>197. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Rethinking_Domain-Specific_LLM_Benchmark_Construction__A_Comprehensiveness-Compactness_Approach.pdf">Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong Polytechnic University</span></p>
<p>本文提出了COMP-COMP框架，通过综合性-紧凑性原则，指导领域专属LLM基准的自动化构建：利用高斯核密度估计和语义空间嵌入，动态评估和优化语料与问答集的覆盖度和代表性，并通过迭代扩展补齐语义空白，实现评估效率和资源利用率的显著提升。实证于学术领域构建的大规模闭集基准XUBench显示，该方法仅用1.7%的问题数和46.4%的语料量即可实现与传统方法相当的评测效果，显著提升领域LLM评测的科学性和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Rethinking_Domain-Specific_LLM_Benchmark_Construction_A_Comprehensiveness-Compactness_Approach.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="198-ObfusQAte-A-Proposed-Framework-to-Evaluate-LLM-Robustness-on-Obfuscated-Factual-Question-Answering"><a href="#198-ObfusQAte-A-Proposed-Framework-to-Evaluate-LLM-Robustness-on-Obfuscated-Factual-Question-Answering" class="headerlink" title="198. ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering"></a>198. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ObfusQAte__A_Proposed_Framework_to_Evaluate_LLM_Robustness_on_Obfuscated_Factual_Question_Answering.pdf">ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Indian Institute of Technology Patna</span></p>
<p>本文提出ObfusQAte技术和ObfusQA基准，通过多层次语言混淆（命名实体间接、干扰项引入、上下文过载）系统性评估大型语言模型（LLMs）在被混淆事实类问答任务中的鲁棒性。结论显示，主流LLM在处理此类复杂变体时准确率显著下降，凸显其推理和抗干扰能力的不足。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ObfusQAte_A_Proposed_Framework_to_Evaluate_LLM_Robustness_on_Obfuscated_Factual_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="199-HealthBranches-Synthesizing-Clinically-Grounded-Question-Answering-Datasets-via-Decision-Pathways"><a href="#199-HealthBranches-Synthesizing-Clinically-Grounded-Question-Answering-Datasets-via-Decision-Pathways" class="headerlink" title="199. HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways"></a>199. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HealthBranches__Synthesizing_Clinically-Grounded_Question_Answering_Datasets_via_Decision_Pathways.pdf">HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Calabria</span></p>
<p>该论文提出HealthBranches数据集，通过半自动化流程从医学决策路径中抽取结构化知识，生成真实病患案例及关联问答，涵盖17个医疗领域共4063个案例，每条数据均附有临床推理链。实验表明，结构化推理路径显著提升LLM在医学问答的多步推理能力，数据集也验证了LLM-as-a-judge与语义相似度指标与人工评价高度一致，为可信赖医学LLM评测和教育应用提供了坚实基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/HealthBranches_Synthesizing_Clinically-Grounded_Question_Answering_Datasets_via_Decision_Pathways.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="200-Fine-Tuning-Large-Language-Models-Using-EEG-Microstate-Features-for-Mental-Workload-Assessment"><a href="#200-Fine-Tuning-Large-Language-Models-Using-EEG-Microstate-Features-for-Mental-Workload-Assessment" class="headerlink" title="200. Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment"></a>200. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fine-Tuning_Large_Language_Models_Using_EEG_Microstate_Features_for_Mental_Workload_Assessment.pdf">Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technological University Dublin</span></p>
<p>该论文提出利用脑电（EEG）微状态特征，通过数据收集、微状态分割与回填、特征提取与提示工程，以及LLM模型选择与微调四个流程，优化大语言模型对认知负荷（休息&#x2F;负荷状态）的判别能力。实验结果显示微调后的LLM模型准确率显著提升，可有效实现认知负荷状态检测，推动认知神经科学与认知AI领域的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Fine-Tuning_Large_Language_Models_Using_EEG_Microstate_Features_for_Mental_Workload_Assessment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="201-MAQUA-Adaptive-Question-Asking-for-Multidimensional-Mental-Health-Screening-using-Item-Response-Theory"><a href="#201-MAQUA-Adaptive-Question-Asking-for-Multidimensional-Mental-Health-Screening-using-Item-Response-Theory" class="headerlink" title="201. MAQUA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory"></a>201. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MAQuA__Adaptive_Question-Asking_for_Multidimensional_Mental_Health_Screening_using_Item_Response_The.pdf">MAQUA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stony Brook University</span></p>
<p>本文提出了MAQUA，一种结合多目标语言建模、因子分析和多维项目反应理论（MIRT）的自适应提问框架，实现多维心理健康筛查。该方法通过优化每轮提问的信息增益，显著提升多维诊断准确性，并在不降低准确率的前提下将问题数量减少50-87%，有效降低用户负担，适用于大模型驱动的交互式临床筛查。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MAQUA_Adaptive_Question-Asking_for_Multidimensional_Mental_Health_Screening_using_Item_Response_Theory.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="202-LLM-based-Agents-for-Automated-Confounder-Discovery-and-Subgroup-Analysis-in-Causal-Inference"><a href="#202-LLM-based-Agents-for-Automated-Confounder-Discovery-and-Subgroup-Analysis-in-Causal-Inference" class="headerlink" title="202. LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference"></a>202. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LLM-based_Agents_for_Automated_Confounder_Discovery_and_Subgroup_Analysis_in_Causal_Inference.pdf">LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Sun Yat-Sen University</span></p>
<p>该论文提出了一种基于大语言模型（LLM）的智能体框架，通过迭代地自动发现混杂变量并进行亚组分析，结合因果树和专家知识检索（RAG），提升因果推断的精准性与可解释性。实验证明该方法在医疗数据上可有效缩小治疗效应估计的置信区间，减轻人工专家负担，并发现难以检测的混杂偏差。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LLM-based_Agents_for_Automated_Confounder_Discovery_and_Subgroup_Analysis_in_Causal_Inference.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="203-Can-Smaller-Large-Language-Models-Evaluate-Research-Quality"><a href="#203-Can-Smaller-Large-Language-Models-Evaluate-Research-Quality" class="headerlink" title="203. Can Smaller Large Language Models Evaluate Research Quality?"></a>203. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Can_Smaller_Large_Language_Models_Evaluate_Research_Quality_.pdf">Can Smaller Large Language Models Evaluate Research Quality?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Wolverhampton</span></p>
<p>本文系统评估了可下载的小型大语言模型（如Google Gemma-3-27b-it）在学术论文质量评估任务中的表现，方法为利用Gemma-3-27b-it对10万余篇英国REF 2021各学科论文的标题和摘要进行多次自动评分，并与专家评价的代理分数进行Spearman相关性分析。结果表明，小型离线LLM的评分与专家分数在几乎所有领域均显著正相关（平均相关强度为ChatGPT 4o的83.8%），且报告结构更标准化但多样性略低，重复评分对提升相关性作用有限。结论：离线可控的小型LLM可有效用于研究质量评估，虽略逊于最大模型，但适合安全、节约成本场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Can_Smaller_Large_Language_Models_Evaluate_Research_Quality_.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="204-AutoAssert-1-A-LoRA-Fine-Tuned-LLM-Model-for-Efficient-Automated-Assertion-Generation"><a href="#204-AutoAssert-1-A-LoRA-Fine-Tuned-LLM-Model-for-Efficient-Automated-Assertion-Generation" class="headerlink" title="204. AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation"></a>204. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AutoAssert_1__A_LoRA_Fine-Tuned_LLM_Model_for_Efficient_Automated_Assertion_Generation.pdf">AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Computing Technology, Chinese Academy of Sciences</span></p>
<p>本文提出了一种基于LoRA高效微调策略和Unsloth平台的轻量级大语言模型（LLM），用于自动生成硬件描述语言（HDL）的断言，显著降低了训练成本并提升了模型适应性和准确性。实验结果表明，该方法在生成语法和语义正确的断言方面优于传统方法，适合资源受限环境下的硬件验证与测试自动化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AutoAssert_1_A_LoRA_Fine-Tuned_LLM_Model_for_Efficient_Automated_Assertion_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="205-Hallucination-as-a-Computational-Boundary-A-Hierarchy-of-Inevitability-and-the-Oracle-Escape"><a href="#205-Hallucination-as-a-Computational-Boundary-A-Hierarchy-of-Inevitability-and-the-Oracle-Escape" class="headerlink" title="205. Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape"></a>205. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hallucination_as_a_Computational_Boundary__A_Hierarchy_of_Inevitability_and_the_Oracle_Escape.pdf">Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Changzhou University</span></p>
<p>本文将大语言模型（LLM）形式化为概率图灵机，提出“计算必然性层级”，并严格证明幻觉现象源于对角化、不可计算性和信息理论的边界。提出两条逃逸路径：一是将检索增强生成（RAG）建模为Oracle机器，理论上证明其可绝对规避幻觉；二是提出基于神经博弈论的新型持续学习机制作为内部Oracle，通过实验验证RAG-CL混合策略在准确性、效率和鲁棒性上的优势。结论认为，幻觉是LLM的根本限制，需通过“计算类对齐”（CCA）原则使任务复杂度与模型计算能力严格匹配，以保障AI安全可靠部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Hallucination_as_a_Computational_Boundary_A_Hierarchy_of_Inevitability_and_the_Oracle_Escape.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="206-Efficient-Edge-LLMs-Deployment-via-Hessian-Aware-Quantization-and-CPU–GPU-Collaborative"><a href="#206-Efficient-Edge-LLMs-Deployment-via-Hessian-Aware-Quantization-and-CPU–GPU-Collaborative" class="headerlink" title="206. Efficient Edge LLMs Deployment via Hessian-Aware Quantization and CPU–GPU Collaborative"></a>206. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Efficient_Edge_LLMs_Deployment_via_HessianAware_Quantization_and_CPU_GPU_Collaborative.pdf">Efficient Edge LLMs Deployment via Hessian-Aware Quantization and CPU–GPU Collaborative</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hong Kong Polytechnic University</span></p>
<p>本文提出了一种基于Hessian感知量化（HAQ）和CPU-GPU协同推理的高效MoE大语言模型边缘部署方案。方法包括自适应激活平滑与Hessian矩阵权重量化，实现激活与权重的联合8比特量化，有效抑制异常值导致的精度损失，并结合专家激活路径统计设计专家级协同卸载与缓存机制，在GPU和CPU间高效分配专家模块，大幅降低内存占用和推理延迟。实验证明该方法在多个主流大模型和数据集上低比特量化下接近全精度性能，显著减少60%显存占用并提升推理延迟稳定性，为边缘环境下MoE大模型高效部署提供了实用技术路径与工程参考。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Efficient_Edge_LLMs_Deployment_via_Hessian-Aware_Quantization_and_CPU%E2%80%93GPU_Collaborative.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="207-DocR1-Evidence-Page-Guided-GRPO-for-Multi-Page-Document-Understanding"><a href="#207-DocR1-Evidence-Page-Guided-GRPO-for-Multi-Page-Document-Understanding" class="headerlink" title="207. DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding"></a>207. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DocR1__Evidence_Page-Guided_GRPO_for_Multi-Page_Document_Understanding.pdf">DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>该论文提出DocR1，一种面向多页文档理解的多模态大语言模型（MLLM），采用新颖的强化学习（RL）框架EviGRPO（Evidence Page-Guided GRPO）。EviGRPO结合证据页奖励，推动模型先检索相关页面再进行推理，通过两阶段数据标注和课程学习训练，利用有限监督数据高效提升推理能力。实验结果显示，DocR1在多个公开多页文档基准上达到最优性能，同时保持单页任务的强表现，显著提升多页推理的准确率和证据定位能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DocR1_Evidence_Page-Guided_GRPO_for_Multi-Page_Document_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="208-MCITlib-Multimodal-Continual-Instruction-Tuning-Library-and-Benchmark"><a href="#208-MCITlib-Multimodal-Continual-Instruction-Tuning-Library-and-Benchmark" class="headerlink" title="208. MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark"></a>208. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MCITlib__Multimodal_Continual_Instruction_Tuning_Library_and_Benchmark.pdf">MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Advanced Interdisciplinary Sciences, UCAS</span></p>
<p>本文提出了MCITlib，这是一个用于多模态大语言模型（MLLMs）持续指令微调的开源代码库与基准，集成了8种代表性MCIT算法，并在精选的两个无信息泄漏基准上系统评测了各种方法的优劣。结论表明MCITlib为多模态持续学习领域的研究提供了公平、统一和可扩展的实验平台，推动了该领域的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MCITlib_Multimodal_Continual_Instruction_Tuning_Library_and_Benchmark.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="209-EndoAgent-A-Memory-Guided-Reflective-Agent-for-Intelligent-Endoscopic-Vision-to-Decision-Reasoning"><a href="#209-EndoAgent-A-Memory-Guided-Reflective-Agent-for-Intelligent-Endoscopic-Vision-to-Decision-Reasoning" class="headerlink" title="209. EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning"></a>209. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EndoAgent__A_Memory-Guided_Reflective_Agent_for_Intelligent_Endoscopic_Vision-to-Decision_Reasoning.pdf">EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southeast University</span></p>
<p>该论文提出了EndoAgent框架，通过双记忆机制和多轮反思，协调专家工具完成内镜视觉到决策推理任务。方法融合短期记忆记录行动轨迹和长期记忆积累经验，支持分类、检测、分割、编辑、VQA和报告生成等六大任务，并开发了EndoAgentBench基准用于系统评估。实验表明EndoAgent在细粒度视觉理解和开放式语言生成上均优于现有通用及医学多模态大模型，展现了极强的灵活性和推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/EndoAgent_A_Memory-Guided_Reflective_Agent_for_Intelligent_Endoscopic_Vision-to-Decision_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="210-“Pull-or-Not-to-Pull-”-Investigating-Moral-Biases-in-Leading-Large-Language-Models-Across-Ethical-Dilemmas"><a href="#210-“Pull-or-Not-to-Pull-”-Investigating-Moral-Biases-in-Leading-Large-Language-Models-Across-Ethical-Dilemmas" class="headerlink" title="210. “Pull or Not to Pull?”: Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas"></a>210. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/_Pull_or_Not_to_Pull_''__Investigating_Moral_Biases_in_Leading_Large_Language_Models_Across_Ethical_.pdf">“Pull or Not to Pull?”: Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of New South Wales</span></p>
<p>该论文系统评估了14个主流大语言模型（LLMs）在27种不同电车难题下的道德推理表现，涵盖了推理增强与通用模型，以及十种伦理框架（如功利主义、义务论、利他主义等），通过分因子提示协议收集3780组二元决策与自然语言解释，分析决策果断性、解释-答案一致性、人类道德对齐度及对无关伦理线索的敏感性。结果显示，推理增强模型更果断且解释结构更好，但未必更符合人类共识，尤其在亲情、法律或自利等框架下，模型易产生有争议的结论，公平、利他和美德伦理提示下模型表现出决策果断、低解释冲突和更接近人类判断的“甜区”。该研究建议将道德推理纳入LLM对齐主轴，并推动标准化伦理基准的建立。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/%E2%80%9CPull_or_Not_to_Pull_%E2%80%9D_Investigating_Moral_Biases_in_Leading_Large_Language_Models_Across_Ethical_Dilemmas.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="211-Selection-and-Exploitation-of-High-Quality-Knowledge-from-Large-Language-Models-for-Recommendation"><a href="#211-Selection-and-Exploitation-of-High-Quality-Knowledge-from-Large-Language-Models-for-Recommendation" class="headerlink" title="211. Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation"></a>211. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Selection_and_Exploitation_of_High-Quality_Knowledge_from_Large_Language_Models_for_Recommendation.pdf">Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou Technology</span></p>
<p>本文提出了KSER（Knowledge Selection &amp; Exploitation Recommendation）框架，通过设计知识过滤模块（ESFNet）对LLM生成的知识进行自适应选择与过滤，并通过注意力机制实现语义嵌入与推荐特征空间的对齐。实验表明，该方法能有效提升主流推荐模型的性能，并且提出的extractor-only训练策略可在几乎不改动主干模型的情况下提升模型效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Selection_and_Exploitation_of_High-Quality_Knowledge_from_Large_Language_Models_for_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="212-What-One-Cannot-Two-Can-Two-Layer-Transformers-Provably-Represent-Induction-Heads-on-Any-Order-Markov-Chains"><a href="#212-What-One-Cannot-Two-Can-Two-Layer-Transformers-Provably-Represent-Induction-Heads-on-Any-Order-Markov-Chains" class="headerlink" title="212. What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains"></a>212. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/What_One_Cannot,_Two_Can__Two-Layer_Transformers_Provably_Represent_Induction_Heads_on_Any-Order_Mar.pdf">What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Massachusetts Institute of Technology</span></p>
<p>该论文提出并理论证明了仅用两层、每层一个注意力头的Transformer即可表达任意阶Markov过程的条件k-gram模型（归纳头机制），改进了此前至少三层结构的结果。作者还分析了在一阶Markov链数据上，通过梯度下降训练该结构的学习动态，证明浅层Transformer可以高效实现强大的in-context learning能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/What_One_Cannot%2C_Two_Can_Two-Layer_Transformers_Provably_Represent_Induction_Heads_on_Any-Order_Markov_Chains.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="213-Adapting-LLMs-to-Time-Series-Forecasting-via-Temporal-Heterogeneity-Modeling-and-Semantic-Alignment"><a href="#213-Adapting-LLMs-to-Time-Series-Forecasting-via-Temporal-Heterogeneity-Modeling-and-Semantic-Alignment" class="headerlink" title="213. Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment"></a>213. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Adapting_LLMs_to_Time_Series_Forecasting_via_Temporal_Heterogeneity_Modeling_and_Semantic_Alignment.pdf">Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tianjin University</span></p>
<p>本文提出了TALON框架，利用异质时序编码器对多变量时间序列进行分段建模，并通过语义对齐模块将时间序列特征映射为LLM兼容的表示，实现无提示高效推理。大量实验表明，TALON在七个真实数据集上对比SOTA方法平均MSE提升达11%，验证了异构建模和语义对齐在提升LLM时序预测性能方面的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Adapting_LLMs_to_Time_Series_Forecasting_via_Temporal_Heterogeneity_Modeling_and_Semantic_Alignment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="214-Multi-Dimensional-Summarization-Agents-with-Context-Aware-Reasoning-over-Enterprise-Tables"><a href="#214-Multi-Dimensional-Summarization-Agents-with-Context-Aware-Reasoning-over-Enterprise-Tables" class="headerlink" title="214. Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables"></a>214. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-Dimensional_Summarization_Agents_with_Context-Aware_Reasoning_over_Enterprise_Tables.pdf">Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Amazon</span></p>
<p>该论文提出一种基于多智能体的框架，用于企业多维表格数据的自动化语义摘要，核心流程包括SliceAgent切分数据、VarianceAgent计算指标变化、ContextAgent融合上下文、SummaryAgent调用LLM生成商业叙述，采用LangGraph实现模块化管控。实验证明该方法在数据忠实度（83%）、重要变动覆盖率和业务相关性（4.4&#x2F;5）方面均优于传统模板与平铺式LLM，尤其能捕捉细微业务权衡，提升企业报告智能化水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multi-Dimensional_Summarization_Agents_with_Context-Aware_Reasoning_over_Enterprise_Tables.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="215-Dynamic-Benchmark-Construction-for-Evaluating-Large-Language-Models-on-Real-World-Codes"><a href="#215-Dynamic-Benchmark-Construction-for-Evaluating-Large-Language-Models-on-Real-World-Codes" class="headerlink" title="215. Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes"></a>215. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Dynamic_Benchmark_Construction_for_Evaluating_Large_Language_Models_on_Real-World_Codes.pdf">Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本文提出了CODE2BENCH，一个自动化端到端框架，可从真实GitHub代码库动态构建抗污染、严谨的代码生成基准。关键方法包括：自动动态数据收集、Scope Graph依赖分析实现任务分类，以及基于属性的自动化高覆盖率测试套件生成。通过CODE2BENCH-2505基准，对16种主流LLM进行评测发现：模型在需要复杂逻辑和跨语言迁移的自包含任务（SC）上表现不佳，在常见库交互任务（WSC，Python）上表现更好。该方法有效揭示了LLM在真实软件开发任务中的弱点，为代码生成模型的现实评估提供了新基准。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Dynamic_Benchmark_Construction_for_Evaluating_Large_Language_Models_on_Real-World_Codes.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="216-Improved-Personalized-Headline-Generation-via-Denoising-Fake-Interests-from-Implicit-Feedback"><a href="#216-Improved-Personalized-Headline-Generation-via-Denoising-Fake-Interests-from-Implicit-Feedback" class="headerlink" title="216. Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback"></a>216. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Improved_Personalized_Headline_Generation_via_Denoising_Fake_Interests_from_Implicit_Feedback.pdf">Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Henan Institute of Advanced Technology, Zhengzhou University</span></p>
<p>本文提出PHG-DIF框架，通过对用户点击历史中的噪声（如短停留时间和异常点击高峰）进行双重过滤，有效去除与个性化无关的假兴趣，并结合多层次时间融合动态建模用户多维兴趣，实现精准用户建模。实验结果显示，该方法在DT-PENS新数据集上显著提升了个性化新闻标题生成的质量，优于多种主流方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Improved_Personalized_Headline_Generation_via_Denoising_Fake_Interests_from_Implicit_Feedback.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="217-A-Stable-and-Principled-Loss-Function-for-Direct-Language-Model-Alignment"><a href="#217-A-Stable-and-Principled-Loss-Function-for-Direct-Language-Model-Alignment" class="headerlink" title="217. A Stable and Principled Loss Function for Direct Language Model Alignment"></a>217. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Stable_and_Principled_Loss_Function_for_Direct_Language_Model_Alignment.pdf">A Stable and Principled Loss Function for Direct Language Model Alignment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>该论文提出了一种新型的Stable Preference Optimization (SPO)损失函数，用于直接对齐大语言模型（LLMs）与人类偏好，改进了现有DPO方法的理论不一致性。通过理论分析和梯度对比，SPO避免了DPO的梯度爆炸和奖励劫持问题，实验结果表明在Qwen2.5-7B和Llama-3-8B模型上，SPO在偏好对齐任务中显著优于DPO，并与更大规模模型性能竞争。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Stable_and_Principled_Loss_Function_for_Direct_Language_Model_Alignment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="218-DySK-Attn-A-Framework-for-Efficient-Real-Time-Knowledge-Updating-in-Large-Language-Models-via-Dynamic-Sparse-Knowledge-Attention"><a href="#218-DySK-Attn-A-Framework-for-Efficient-Real-Time-Knowledge-Updating-in-Large-Language-Models-via-Dynamic-Sparse-Knowledge-Attention" class="headerlink" title="218. DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention"></a>218. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DySK-Attn__A_Framework_for_Efficient,_Real-Time_Knowledge_Updating_in_Large_Language_Models_via_Dyna.pdf">DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">San Francisco State University</span></p>
<p>该论文提出DySK-Attn框架，将大型语言模型与可实时更新的知识图谱结合，通过动态稀疏知识注意力机制实现高效的知识检索和融合。实验结果表明，该方法在时间敏感问答任务中显著提升了新知识的准确性和推理效率，相较于RAG和模型编辑等主流方法更优，且支持毫秒级知识更新，适用于实际动态环境。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/DySK-Attn_A_Framework_for_Efficient%2C_Real-Time_Knowledge_Updating_in_Large_Language_Models_via_Dynamic_Sparse_Knowledge_Attention.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="219-Schema-Lineage-Extraction-at-Scale-Multilingual-Pipelines-Composite-Evaluation-and-Language-Model-Benchmarks"><a href="#219-Schema-Lineage-Extraction-at-Scale-Multilingual-Pipelines-Composite-Evaluation-and-Language-Model-Benchmarks" class="headerlink" title="219. Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks"></a>219. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Schema_Lineage_Extraction_at_Scale__Multilingual_Pipelines,_Composite_Evaluation,_and_Language-Model.pdf">Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Microsoft</span></p>
<p>本文提出了一种面向多语言企业数据管道脚本的自动化细粒度模式血缘提取框架，关键方法包括标准化四要素（源模式、源表、变换逻辑、聚合操作）抽取流程和新型SLiCE复合指标以评估结构与语义正确性，并构建了包含1700个真实注释样本的基准数据集。实验表明，随着模型规模和推理丰富度提升，血缘提取效果显著增强，32B开源模型配合单次链式推理已可媲美GPT-4系列，为企业内部经济高效部署提供可行路径。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Schema_Lineage_Extraction_at_Scale_Multilingual_Pipelines%2C_Composite_Evaluation%2C_and_Language-Model_Benchmarks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="220-LL3M-Large-Language-3D-Modelers"><a href="#220-LL3M-Large-Language-3D-Modelers" class="headerlink" title="220. LL3M: Large Language 3D Modelers"></a>220. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LL3M__Large_Language_3D_Modelers.pdf">LL3M: Large Language 3D Modelers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Chicago</span></p>
<p>LL3M提出了一种利用多智能体大语言模型系统，通过编写可解释的Python代码在Blender中生成和编辑3D资产的方法。该系统结合检索增强生成（RAG）与多阶段自动与用户引导的迭代流程，实现了高质量、可编辑和模块化的3D资产生成。实验结果显示LL3M能生成结构清晰、细节丰富且易于人类理解和修改的3D模型，显著优于单智能体方法，并支持高效的用户交互与持续迭代优化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LL3M_Large_Language_3D_Modelers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="221-Multi-head-Transformers-Provably-Learn-Symbolic-Multi-step-Reasoning-via-Gradient-Descent"><a href="#221-Multi-head-Transformers-Provably-Learn-Symbolic-Multi-step-Reasoning-via-Gradient-Descent" class="headerlink" title="221. Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent"></a>221. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-head_Transformers_Provably_Learn_Symbolic_Multi-step_Reasoning_via_Gradient_Descent.pdf">Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carnegie Mellon University</span></p>
<p>该论文提出理论分析，证明一层多头Transformer可通过梯度下降学习符号化多步推理任务（如树结构路径查找），实现链式思维（Chain-of-Thought）推理。作者构建模型并详细分析训练动态，证明多头注意力能自主分工与协调，高效解决多阶段推理任务，并具备对未见结构的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multi-head_Transformers_Provably_Learn_Symbolic_Multi-step_Reasoning_via_Gradient_Descent.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="222-Learning-User-Preferences-for-Image-Generation-Models"><a href="#222-Learning-User-Preferences-for-Image-Generation-Models" class="headerlink" title="222. Learning User Preferences for Image Generation Models"></a>222. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_User_Preferences_for_Image_Generation_Model.pdf">Learning User Preferences for Image Generation Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Renmin University of China</span></p>
<p>本文提出了一种基于多模态大语言模型（MLLM）的用户个性化偏好学习方法，通过引入对比偏好损失和可学习的偏好token，实现了从用户历史交互中建模细粒度的“喜欢&#x2F;不喜欢”关系。实验结果表明，该方法在用户偏好预测准确率以及对个性化内容生成的指导能力上优于现有方法，能有效挖掘用户共性与个性，提升模型泛化与定制能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Learning_User_Preferences_for_Image_Generation_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="223-Human-Alignment-and-Calibration-of-Inference-Time-Uncertainty-in-Large-Language-Models"><a href="#223-Human-Alignment-and-Calibration-of-Inference-Time-Uncertainty-in-Large-Language-Models" class="headerlink" title="223. Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models"></a>223. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Human-Alignment_and_Calibration_of_Inference-Time_Uncertainty_in_Large_Language_Models.pdf">Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Vanderbilt University</span></p>
<p>本文提出并系统评估了多种推理时大语言模型（LLM）不确定性量化方法，重点分析其与人类群体不确定性和模型校准性的对齐程度。核心方法包括对模型输出概率分布的熵、top-k和top-p采样等指标进行统计，并以人类问卷数据为对照，发现choice entropy、top-k entropy等度量与人类不确定性高度相关，同时在MMLU数据集上显示出统计显著的校准能力。结论指出，部分熵类推理时不确定性指标不仅与人类不确定性行为高度一致，还具备模型校准性，为提升人机交互体验与信任度提供基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Human-Alignment_and_Calibration_of_Inference-Time_Uncertainty_in_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="224-Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning"><a href="#224-Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning" class="headerlink" title="224. Capabilities of GPT-5 on Multimodal Medical Reasoning"></a>224. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Capabilities_of_GPT-5_on_Multimodal_Medical_Reasoning.pdf">Capabilities of GPT-5 on Multimodal Medical Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Emory University</span></p>
<p>该论文系统评估了GPT-5在医学多模态推理中的能力，采用统一的zero-shot链式思维(CoT)提示，涵盖文本和视觉问答任务，并与GPT-4o、人类专家等进行对比。结果显示GPT-5在MedQA、MedXpertQA、MMLU医学子集、USMLE和VQA-RAD等标准数据集上均达到了最先进的推理和理解表现，显著超越GPT-4o和人类专家，尤其在多模态推理上提升明显，表明其可作为临床决策支持核心组件。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Capabilities_of_GPT-5_on_Multimodal_Medical_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="225-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning"><a href="#225-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning" class="headerlink" title="225. Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning"></a>225. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Part_I__Tricks_or_Traps__A_Deep_Dive_into_RL_for_LLM_Reasoning.pdf">Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Alibaba Group</span></p>
<p>本论文系统性评测并剖析了当前主流的RL（强化学习）技术在大语言模型（LLM）推理任务中的作用，通过统一开源框架，对Normalization、Clipping、Filtering与Loss Aggregation等技术进行细粒度实验，揭示其在多种数据难度、模型架构下的适用性和机制。结论表明，简单结合group-level mean与batch-level std的优势归一化及token-level loss aggregation两项技术即可在无critic的PPO基础上显著提升RL4LLM性能，优于现有复杂算法，并给出明确技术选择指南。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Part_I_Tricks_or_Traps_A_Deep_Dive_into_RL_for_LLM_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="226-Spatial-ORMLLM-Improve-Spatial-Relation-Understanding-in-the-Operating-Room-with-Multimodal-Large-Language-Model"><a href="#226-Spatial-ORMLLM-Improve-Spatial-Relation-Understanding-in-the-Operating-Room-with-Multimodal-Large-Language-Model" class="headerlink" title="226. Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model"></a>226. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Spatial-ORMLLM__Improve_Spatial_Relation_Understanding_in_the_Operating_Room_with_Multimodal_Large_L.pdf">Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hunan University</span></p>
<p>该论文提出Spatial-ORMLLM，一种专为手术室3D空间推理设计的多模态大语言模型，在仅使用RGB图像的条件下，通过3D空间模块生成伪深度图、全景分割和点云，并在空间增强特征融合模块中将这些模态投影为统一Token输入LLM，实现了无外部3D传感器的空间关系推理。实验表明，Spatial-ORMLLM在多个手术室空间理解基准和下游任务上均取得了领先性能，证实了即使在输入受限下也能实现高精度空间推理。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Spatial-ORMLLM_Improve_Spatial_Relation_Understanding_in_the_Operating_Room_with_Multimodal_Large_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="227-PYVERITAS-On-Verifying-Python-via-LLM-Based-Transpilation-and-Bounded-Model-Checking-for-C"><a href="#227-PYVERITAS-On-Verifying-Python-via-LLM-Based-Transpilation-and-Bounded-Model-Checking-for-C" class="headerlink" title="227. PYVERITAS: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C"></a>227. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PyVeritas__On_Verifying_Python_via_LLM-Based_Transpilation_and_Bounded_Model_Checking_for_C.pdf">PYVERITAS: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Oxford</span></p>
<p>该论文提出PYVERITAS框架，利用大语言模型（LLM）将Python代码高效转译为C代码，并结合CBMC进行有界模型检测和MaxSAT故障定位，实现对Python程序的形式化验证和可解释的错误定位。实验表明，LLM（如QWEN2.5-CODER）能在80%以上的案例中生成语义忠实的C代码，验证和定位Python代码中的错误，为Python程序员提供实际可用的验证与诊断工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/PYVERITAS_On_Verifying_Python_via_LLM-Based_Transpilation_and_Bounded_Model_Checking_for_C.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="228-Can-LLMs-Detect-Their-Confabulations-Estimating-Reliability-in-Uncertainty-Aware-Language-Models"><a href="#228-Can-LLMs-Detect-Their-Confabulations-Estimating-Reliability-in-Uncertainty-Aware-Language-Models" class="headerlink" title="228. Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models"></a>228. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Can_LLMs_Detect_Their_Confabulations__Estimating_Reliability_in_Uncertainty-Aware_Language_Models.pdf">Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KTH Royal Institute of Technology</span></p>
<p>该论文提出了一种基于不确定性引导的探针方法，通过分析大语言模型（LLM）在不同上下文（无、正确、误导）下的行为，结合 token 级不确定性（aleatoric 和 epistemic）从内部隐状态聚合特征，提升回答可靠性检测。实验证明，准确的上下文可提升模型正确性和置信度，而误导性上下文则导致模型自信地产生错误答案，且不确定性信号与实际正确性存在偏差；所提探针方法优于直接不确定性度量，能更有效检测多种开源LLM的不可靠输出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Can_LLMs_Detect_Their_Confabulations_Estimating_Reliability_in_Uncertainty-Aware_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="229-Optimal-Transport-Regularization-for-Speech-Text-Alignment-in-Spoken-Language-Models"><a href="#229-Optimal-Transport-Regularization-for-Speech-Text-Alignment-in-Spoken-Language-Models" class="headerlink" title="229. Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models"></a>229. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Optimal_Transport_Regularization_for_Speech_Text_Alignment_in_Spoken_Language_Models.pdf">Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mashang Consumer Finance Co., Ltd.</span></p>
<p>本论文提出了一种名为Optimal Transport Regularization (OTReg)的新方法，用于通过最优传输理论将语音和文本嵌入对齐，从而缩小语音与文本间的模态差距，提升Spoken Language Models（SLMs）的泛化能力。方法流程包括两阶段训练：第一阶段标准有监督微调，第二阶段引入OTReg正则化损失和基于相似度的语音嵌入压缩，无需额外标签或参数，可无缝集成至现有SLM训练流程。实验表明，OTReg显著改善了多语种ASR任务中SLM的跨数据集泛化能力和语音文本对齐效果，提高了模型鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Optimal_Transport_Regularization_for_Speech_Text_Alignment_in_Spoken_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="230-TBAC-UniImage-Unified-Understanding-and-Generation-by-Ladder-Side-Diffusion-Tuning"><a href="#230-TBAC-UniImage-Unified-Understanding-and-Generation-by-Ladder-Side-Diffusion-Tuning" class="headerlink" title="230. TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning"></a>230. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TBAC-UniImage__Unified_Understanding_and_Generation_by_Ladder-Side_Diffusion_Tuning.pdf">TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Basic Algorithm Center, PCG, Tencent</span></p>
<p>本文提出TBAC-UniImage，一种通过Ladder-Side Diffusion Tuning深度融合多模态大语言模型（MLLM）与扩散生成模型（DiT）的统一理解与生成架构。该方法通过在MLLM的不同中间层插入可学习查询，并将其多层隐藏状态分层输入DiT，实现更细粒度的跨模态条件生成，显著提升了理解与生成的统一性。实验结果表明，TBAC-UniImage在GenEval、DPG-Bench和TIIF-Bench等基准上取得了与主流方法媲美甚至更优的表现，尤其在复杂指令和高级跟随任务中展现出强大能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TBAC-UniImage_Unified_Understanding_and_Generation_by_Ladder-Side_Diffusion_Tuning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="231-Dual-Information-Speech-Language-Models-for-Emotional-Conversations"><a href="#231-Dual-Information-Speech-Language-Models-for-Emotional-Conversations" class="headerlink" title="231. Dual Information Speech Language Models for Emotional Conversations"></a>231. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Dual_Information_Speech_Language_Models_for_Emotional_Conversations.pdf">Dual Information Speech Language Models for Emotional Conversations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mashang Consumer Finance Co., Ltd.</span></p>
<p>本文提出了一种高效的情感对话语音语言模型（SLM）扩展方法，通过设计异构双适配器结构分别提取说话者语音中的副语言信息和语言信息，并采用弱监督训练策略实现信息解耦和上下文理解。实验结果显示，该方法仅需训练两个适配器，参数及数据高效，在情感对话任务上取得了与主流模型相当甚至更优的表现，有效融合了副语言与语言信息，提升了模型的情感识别与对话能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Dual_Information_Speech_Language_Models_for_Emotional_Conversations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="232-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches"><a href="#232-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches" class="headerlink" title="232. HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches"></a>232. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HierSearch__A_Hierarchical_Enterprise_Deep_Search_Framework_Integrating_Local_and_Web_Searches.pdf">HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Gaoling School of Artificial Intelligence, Renmin University of China</span></p>
<p>本文提出了HierSearch框架，采用分层强化学习（HRL）训练本地深度搜索代理和Web深度搜索代理，并由高层规划代理协调低层代理进行多知识源深度搜索。通过引入知识精炼器过滤冗余及幻觉信息，HierSearch在多个通用、财经、医疗领域基准上超越了平坦RL和多源RAG等主流方法，显著提升了搜索与推理的准确性与效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/HierSearch_A_Hierarchical_Enterprise_Deep_Search_Framework_Integrating_Local_and_Web_Searches.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="233-Robust-Anomaly-Detection-in-O-RAN-Leveraging-LLMs-against-Data-Manipulation-Attacks"><a href="#233-Robust-Anomaly-Detection-in-O-RAN-Leveraging-LLMs-against-Data-Manipulation-Attacks" class="headerlink" title="233. Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks"></a>233. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Robust_Anomaly_Detection_in_O-RAN__Leveraging_LLMs_against_Data_Manipulation_Attacks.pdf">Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University</span></p>
<p>本论文提出在5G O-RAN架构中利用大语言模型（LLMs）作为xApp进行异常检测，以应对通过恶意xApp进行的Unicode数据操控（如hypoglyphs）攻击。方法上，作者将传统ML模型（如AutoEncoder）与Llama-3.1-8B-Instruct对比，发现传统模型遇到数据操控时会崩溃，而LLM可稳健处理所有输入并维持低于0.07秒的检测延迟，展示出对数据操控的强抗干扰能力。结论是LLM能显著提升O-RAN安全性，未来通过提示工程有望进一步优化检测准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Robust_Anomaly_Detection_in_O-RAN_Leveraging_LLMs_against_Data_Manipulation_Attacks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="234-From-Natural-Language-to-Solver-Ready-Power-System-Optimization-An-LLM-Assisted-Validation-in-the-Loop-Framework"><a href="#234-From-Natural-Language-to-Solver-Ready-Power-System-Optimization-An-LLM-Assisted-Validation-in-the-Loop-Framework" class="headerlink" title="234. From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework"></a>234. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Natural_Language_to_Solver-Ready_Power_System_Optimization__An_LLM-Assisted,_Validation-in-the-.pdf">From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Southern California</span></p>
<p>本文提出了一种基于大型语言模型（LLM）辅助的智能代理系统，将电力系统优化场景的自然语言描述自动转化为可直接求解的数学优化模型，并结合系统化验证与迭代修复机制保证模型可行性。该方法通过领域感知的提示词、结构化模式和验证-修复环节，提升了模型的准确性和可靠性，并通过GNN辅助分支策略加速MILP求解。实验结果表明，该系统在单元承诺问题上实现了100%可行率和明显的计算加速，优于直接用LLM生成解的基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/From_Natural_Language_to_Solver-Ready_Power_System_Optimization_An_LLM-Assisted%2C_Validation-in-the-Loop_Framework.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="235-MuaLLM-A-Multimodal-Large-Language-Model-Agent-for-Circuit-Design-Assistance-with-Hybrid-Contextual-Retrieval-Augmented-Generation"><a href="#235-MuaLLM-A-Multimodal-Large-Language-Model-Agent-for-Circuit-Design-Assistance-with-Hybrid-Contextual-Retrieval-Augmented-Generation" class="headerlink" title="235. MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation"></a>235. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MuaLLM__A_Multimodal_Large_Language_Model_Agent_for_Circuit_Design_Assistance_with_Hybrid_Contextual.pdf">MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Utah</span></p>
<p>本文提出了MuaLLM，一个开源多模态大型语言模型（LLM）智能体，专为电路设计辅助而构建。MuaLLM结合了基于Reason+Act (ReAct) 的推理式多步检索与混合Sparse&#x2F;Dense检索的RAG框架，并集成自适应向量数据库和定制工具（如自动文献检索、数据库动态更新、自动电路网表生成），实现对文本和图像的高效检索和推理。实验表明，在电路设计文献问答与多步推理数据集上，MuaLLM可达到90.1%召回率和86.8%准确率，并且在处理大规模语料时较传统LLM更快、更低成本。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MuaLLM_A_Multimodal_Large_Language_Model_Agent_for_Circuit_Design_Assistance_with_Hybrid_Contextual_Retrieval-Augmented_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="236-BlindGuard-Safeguarding-LLM-based-Multi-Agent-Systems-under-Unknown-Attacks"><a href="#236-BlindGuard-Safeguarding-LLM-based-Multi-Agent-Systems-under-Unknown-Attacks" class="headerlink" title="236. BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks"></a>236. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BlindGuard__Safeguarding_LLM-based_Multi-Agent_Systems_under_Unknown_Attacks.pdf">BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, Jilin University</span></p>
<p>本文提出BlindGuard，一种用于LLM驱动多智能体系统（MAS）的无监督防御方法，无需攻击样本标签或先验知识。方法通过层次化代理编码器整合个体、邻域和全局信息，并设计语义级特征扰动与对比学习的攻击检测器，仅用正常交互数据即可检测多种未知攻击（如提示注入、内存投毒、工具利用），再联合边裁剪隔离可疑代理。实验证明，BlindGuard在多种拓扑和攻击类型下均能有效检测并缓解攻击，防御性能优于其它无监督方法，接近有监督上限，且具备良好泛化和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/BlindGuard_Safeguarding_LLM-based_Multi-Agent_Systems_under_Unknown_Attacks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="237-TeamMedAgents-Enhancing-Medical-Decision-Making-of-LLMs-Through-Structured-Teamwork"><a href="#237-TeamMedAgents-Enhancing-Medical-Decision-Making-of-LLMs-Through-Structured-Teamwork" class="headerlink" title="237. TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork"></a>237. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TeamMedAgents__Enhancing_Medical_Decision-Making_of_LLMs_Through_Structured_Teamwork.pdf">TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois, Chicago</span></p>
<p>本文提出TeamMedAgents框架，将人类协作心理学的六大团队工作机制（领导力、互相绩效监控、团队导向、共享心智模型、闭环沟通和互信）以模块化方式集成到大语言模型（LLM）多智能体医学决策系统中。通过在八个医学基准上的系统实验与消融研究，发现多智能体团队机制能显著提升医学决策准确率，且不同任务需针对性选择团队机制，全面激活所有机制反而可能带来协作负担。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TeamMedAgents_Enhancing_Medical_Decision-Making_of_LLMs_Through_Structured_Teamwork.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="238-Assessing-LLM-Text-Detection-in-Educational-Contexts-Does-Human-Contribution-Affect-Detection"><a href="#238-Assessing-LLM-Text-Detection-in-Educational-Contexts-Does-Human-Contribution-Affect-Detection" class="headerlink" title="238. Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?"></a>238. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Assessing_LLM_Text_Detection_in_Educational_Contexts__Does_Human_Contribution_Affect_Detection_.pdf">Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bielefeld University</span></p>
<p>本文提出了GEDE（Generative Essay Detection in Education）数据集，涵盖900余篇学生写作与超12500篇不同贡献级别的LLM生成文本，并系统评测了多种主流检测方法（如DetectGPT、Fast-DetectGPT、Ghostbuster、RoBERTa及GPTZero）在教育场景下的表现，重点关注学生对文本的不同参与程度。实验发现：当前检测器在区分完全人写与完全LLM生成文本时表现较好，但面对人类与LLM混合修改（如小幅润色或重写）时，误报率和泛化能力大幅下降，尤其在教育场景下易误伤学生，仍不适合实际部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Assessing_LLM_Text_Detection_in_Educational_Contexts_Does_Human_Contribution_Affect_Detection_.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="239-Investigating-the-Design-Space-of-Visual-Grounding-in-Multimodal-Large-Language-Model"><a href="#239-Investigating-the-Design-Space-of-Visual-Grounding-in-Multimodal-Large-Language-Model" class="headerlink" title="239. Investigating the Design Space of Visual Grounding in Multimodal Large Language Model"></a>239. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Investigating_the_Design_Space_of_Visual_Grounding_in_Multimodal_Large_Language_Model.pdf">Investigating the Design Space of Visual Grounding in Multimodal Large Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Chicago</span></p>
<p>本文系统性分析了多模态大语言模型（MLLM）在视觉指代（Visual Grounding, VG）任务中的多种设计选择，涵盖预测格式、归一化方式、监督方式、边框格式与数据组织等关键环节，并以LLaVA-1.5为基线进行全面消融实验。结论表明，采用归一化的整数格式、左上-右下坐标、one-hot监督、去重对话、以及专用VG数据，能显著提升MLLM的VG性能（RefCOCO&#x2F;+&#x2F;g三项提升+5.6%&#x2F;+6.9%&#x2F;+7.0%），并为未来VG能力开发提供了清晰设计准则。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Investigating_the_Design_Space_of_Visual_Grounding_in_Multimodal_Large_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="240-AdaptFlow-Adaptive-Workflow-Optimization-via-Meta-Learning"><a href="#240-AdaptFlow-Adaptive-Workflow-Optimization-via-Meta-Learning" class="headerlink" title="240. AdaptFlow: Adaptive Workflow Optimization via Meta-Learning"></a>240. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AdaptFlow__Adaptive_Workflow_Optimization_via_Meta-Learning.pdf">AdaptFlow: Adaptive Workflow Optimization via Meta-Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>该论文提出了AdaptFlow，一种基于MAML的自然语言元学习框架，通过双层优化（内循环基于LLM反馈细化工作流，外循环聚合并反思改进）实现工作流在不同子任务间的快速适应。实验覆盖问答、代码生成和数学推理等领域，结果显示AdaptFlow在通用性和性能上均优于手工和自动化基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/AdaptFlow_Adaptive_Workflow_Optimization_via_Meta-Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="241-FEAT-A-Multi-Agent-Forensic-AI-System-with-Domain-Adapted-Large-Language-Model-for-Automated-Cause-of-Death-Analysis"><a href="#241-FEAT-A-Multi-Agent-Forensic-AI-System-with-Domain-Adapted-Large-Language-Model-for-Automated-Cause-of-Death-Analysis" class="headerlink" title="241. FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis"></a>241. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FEAT__A_Multi-Agent_Forensic_AI_System_with_Domain-Adapted_Large_Language_Model_for_Automated_Cause-.pdf">FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong University</span></p>
<p>本文提出FEAT系统，一种专为法医学死因分析设计的多智能体AI框架，集成了任务分解的Planner、证据分析的Local Solvers、反思与记忆模块、结论综合的Global Solver，并采用领域微调大语言模型（Forensic-LLM）、工具增强推理与分层检索生成（H-RAG）、人类反馈等关键技术。实验表明FEAT在中国六地大规模法医案件中，长篇分析和短结论准确率均显著超越SOTA，达到专家级水平，并具备优良泛化性和司法适用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/FEAT_A_Multi-Agent_Forensic_AI_System_with_Domain-Adapted_Large_Language_Model_for_Automated_Cause-of-Death_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="242-Not-Yet-AlphaFold-for-the-Mind-Evaluating-Centaur-as-a-Synthetic-Participant"><a href="#242-Not-Yet-AlphaFold-for-the-Mind-Evaluating-Centaur-as-a-Synthetic-Participant" class="headerlink" title="242. Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant"></a>242. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Not_Yet_AlphaFold_for_the_Mind__Evaluating_Centaur_as_a_Synthetic_Participant.pdf">Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Osnabrück University</span></p>
<p>本文系统评估了经过人类行为数据微调的LLM Centaur在三类认知任务（反转学习、horizon bandit任务和威斯康星卡片分类测试）中的预测与生成能力，重点比较其与基础LLM及领域特定模型的表现。结果表明，虽然Centaur在预测试验中表现优异，但在生成新行为时与人类数据存在显著偏差，尚不能作为可靠的“虚拟参与者”或认知模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Not_Yet_AlphaFold_for_the_Mind_Evaluating_Centaur_as_a_Synthetic_Participant.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="243-CATP-Contextually-Adaptive-Token-Pruning-for-Efficient-and-Enhanced-Multimodal-In-Context-Learning"><a href="#243-CATP-Contextually-Adaptive-Token-Pruning-for-Efficient-and-Enhanced-Multimodal-In-Context-Learning" class="headerlink" title="243. CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning"></a>243. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CATP__Contextually_Adaptive_Token_Pruning_for_Efficient_and_Enhanced_Multimodal_In-Context_Learning.pdf">CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Brown University</span></p>
<p>本文提出CATP，一种针对多模态大模型（LVLMs）在多模态in-context learning（ICL）场景下的无训练图像token剪枝方法。CATP通过两阶段流程：第一阶段在投影器与解码器之间结合语义对齐和特征多样性，第二阶段在浅层解码器中结合层间注意力差异与语义相关性，逐步筛选最关键的图像token。实验表明，CATP在四种LVLMs和八个基准上，平均保留22.2%的图像token后，推理速度提升10.78%，性能较原始模型提升0.6%，优于所有现有方法，显著增强多模态ICL的效率和实际应用价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/CATP_Contextually_Adaptive_Token_Pruning_for_Efficient_and_Enhanced_Multimodal_In-Context_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="244-Pareto-Multi-Objective-Alignment-for-Language-Models"><a href="#244-Pareto-Multi-Objective-Alignment-for-Language-Models" class="headerlink" title="244. Pareto Multi-Objective Alignment for Language Models"></a>244. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pareto_Multi-Objective_Alignment_for_Language_Models.pdf">Pareto Multi-Objective Alignment for Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ruhr University Bochum</span></p>
<p>本文提出了PAreto Multi-Objective Alignment (PAMA)算法，用于高效地实现大语言模型（LLMs）在多目标（如信息量与简洁性、乐于助人与创造力等）之间的高效对齐。PAMA将多目标强化学习人类反馈（RLHF）问题转化为可解析的凸优化问题，极大降低了计算复杂度（从O(n²d)降至O(n)），并理论证明其收敛到Pareto平衡点。实验结果表明，PAMA在不同规模的LLM上对多目标对齐表现出优越的性能和稳定性，显著优于常用基线方法，成为大模型多目标对齐的高效可扩展方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Pareto_Multi-Objective_Alignment_for_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="245-Interpreting-Fedspeak-with-Confidence-A-LLM-Based-Uncertainty-Aware-Framework-Guided-by-Monetary-Policy-Transmission-Paths"><a href="#245-Interpreting-Fedspeak-with-Confidence-A-LLM-Based-Uncertainty-Aware-Framework-Guided-by-Monetary-Policy-Transmission-Paths" class="headerlink" title="245. Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths"></a>245. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Interpreting_Fedspeak_with_Confidence__A_LLM-Based_Uncertainty-Aware_Framework_Guided_by_Monetary_Po.pdf">Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>本文提出了一种基于大语言模型（LLM）、结合货币政策传导机制的领域知识推理和动态不确定性解码的框架，对美联储Fedspeak文本进行货币政策立场分析。方法通过金融实体关系抽取与政策传导路径增强输入，并引入感知不确定性（PU）度量模型置信度，实现对预测可靠性的自动评估。实验显示，该框架在FOMC数据集上实现了当前最优表现，并证实高PU与模型错误率显著相关，提升了模型的透明性和实际应用可靠性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Interpreting_Fedspeak_with_Confidence_A_LLM-Based_Uncertainty-Aware_Framework_Guided_by_Monetary_Policy_Transmission_Paths.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="246-The-Escalator-Problem-Identifying-Implicit-Motion-Blindness-in-AI-for-Accessibility"><a href="#246-The-Escalator-Problem-Identifying-Implicit-Motion-Blindness-in-AI-for-Accessibility" class="headerlink" title="246. The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility"></a>246. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/The_Escalator_Problem__Identifying_Implicit_Motion_Blindness_in_AI_for_Accessibility.pdf">The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>该论文提出了“自动扶梯问题”，揭示了多模态大语言模型（MLLMs）在感知连续、低信号运动（如自动扶梯运行方向）方面的重大缺陷，称之为“隐性运动盲”。作者分析了现有模型采用稀疏帧采样造成运动信息丢失的技术根源，并指出该问题会削弱盲人及视障人士对助理AI的信任。结论是呼吁从单纯语义识别转向注重物理感知，建立以用户安全和信任为核心的新评测基准，推动AI可访问性领域的范式转变。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/The_Escalator_Problem_Identifying_Implicit_Motion_Blindness_in_AI_for_Accessibility.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="247-WECHAT-YATT-A-Simple-Scalable-and-Balanced-RLHF-Trainer"><a href="#247-WECHAT-YATT-A-Simple-Scalable-and-Balanced-RLHF-Trainer" class="headerlink" title="247. WECHAT-YATT: A Simple, Scalable and Balanced RLHF Trainer"></a>247. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/WeChat-YATT__A_Simple,_Scalable_and_Balanced_RLHF_Trainer.pdf">WECHAT-YATT: A Simple, Scalable and Balanced RLHF Trainer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent</span></p>
<p>该论文提出了WeChat-YATT框架，用于大规模人类反馈强化学习（RLHF）训练，核心方法包括并行控制器编程模型和动态资源分配策略，实现多模型高效协同和硬件资源自适应分配，显著提升大模型及多模态RLHF训练的吞吐量与GPU利用率。实验结果表明WeChat-YATT在多种场景下优于主流RLHF框架，并已成功部署于微信产品线，验证了其实际应用的可扩展性与鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/WECHAT-YATT_A_Simple%2C_Scalable_and_Balanced_RLHF_Trainer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="248-X-evolve-Solution-space-evolution-powered-by-large-language-models"><a href="#248-X-evolve-Solution-space-evolution-powered-by-large-language-models" class="headerlink" title="248. X-evolve: Solution space evolution powered by large language models"></a>248. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/_(X_)-evolve__Solution_space_evolution_powered_by_large_language_models.pdf">X-evolve: Solution space evolution powered by large language models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>X-evolve提出了一种结合大语言模型（LLMs）和进化算法（EAs）的方法，不再进化单个解，而是进化参数化定义的解空间（solution spaces），LLM生成带可调参数的程序，构成解空间，随后用得分反馈指导高效空间搜索。该方法在cap set问题、Shannon容量问题和NP难的在线装箱问题上，均取得了与或超越前沿工作的结果，且模型调用成本降低至原有方法的百分之一甚至千分之一。结论：X-evolve极大提升了解空间探索效率和突破复杂优化问题的能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/X-evolve_Solution_space_evolution_powered_by_large_language_models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="249-MIMIC-Multimodal-Inversion-for-Model-Interpretation-and-Conceptualization"><a href="#249-MIMIC-Multimodal-Inversion-for-Model-Interpretation-and-Conceptualization" class="headerlink" title="249. MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization"></a>249. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MIMIC__Multimodal_Inversion_for_Model_Interpretation_and_Conceptualization.pdf">MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Twente</span></p>
<p>该论文提出了一种多模态反演框架MIMIC，通过联合VLM反演与特征对齐目标，并引入空间、自然图像及语义正则化，实现了对视觉-语言模型（VLM）内部表征的可视化。实验证明MIMIC可高效合成与语义概念对齐的高保真图像，提升了VLM的可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MIMIC_Multimodal_Inversion_for_Model_Interpretation_and_Conceptualization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="250-EvoCoT-Overcoming-the-Exploration-Bottleneck-in-Reinforcement-Learning"><a href="#250-EvoCoT-Overcoming-the-Exploration-Bottleneck-in-Reinforcement-Learning" class="headerlink" title="250. EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning"></a>250. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EvoCoT__Overcoming_the_Exploration_Bottleneck_in_Reinforcement_Learning.pdf">EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>该论文提出了EvoCoT，一种自进化课程学习框架，通过两阶段Chain-of-Thought（CoT）推理优化提升大语言模型（LLM）在稀疏奖励下的探索与推理能力。方法包括自生成并验证CoT轨迹，逐步缩短推理路径以控制探索空间，实现LLM对难题的稳定自我学习，显著提升模型在多种数学基准上的表现。结论：EvoCoT能突破探索瓶颈，提升LLM在复杂推理任务中的表现，且无需外部监督或更强教师模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/EvoCoT_Overcoming_the_Exploration_Bottleneck_in_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="251-UniSVG-A-Unified-Dataset-for-Vector-Graphic-Understanding-and-Generation-with-Multimodal-Large-Language-Models"><a href="#251-UniSVG-A-Unified-Dataset-for-Vector-Graphic-Understanding-and-Generation-with-Multimodal-Large-Language-Models" class="headerlink" title="251. UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models"></a>251. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/UniSVG__A_Unified_Dataset_for_Vector_Graphic_Understanding_and_Generation_with_Multimodal_Large_Lang.pdf">UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了UniSVG，这是首个面向多模态大语言模型（MLLMs）的大规模、开放且多任务的SVG（矢量图）理解与生成数据集，涵盖Image2SVG、Text2SVG及SVG理解三大任务，并构建了配套基准测试体系。实验结果表明，在UniSVG上微调后，开源MLLM在SVG生成和理解任务上显著优于包括GPT-4V等在内的多种SOTA闭源模型，推动了MLLM在矢量图领域的研究进展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/UniSVG_A_Unified_Dataset_for_Vector_Graphic_Understanding_and_Generation_with_Multimodal_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="252-LoSemB-Logic-Guided-Semantic-Bridging-for-Inductive-Tool-Retrieval"><a href="#252-LoSemB-Logic-Guided-Semantic-Bridging-for-Inductive-Tool-Retrieval" class="headerlink" title="252. LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval"></a>252. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LoSemB__Logic-Guided_Semantic_Bridging_for_Inductive_Tool_Retrieval.pdf">LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong Polytechnic University</span></p>
<p>本文提出了一种逻辑引导的语义桥接框架LoSemB，用于大语言模型（LLMs）在动态工具库中高效进行归纳式工具检索。方法上，LoSemB包括逻辑特征提取与嵌入对齐模块，结合图卷积网络对指令-工具逻辑关系建模，并通过关系增强检索机制，融合逻辑约束与图增强相似度，显著提升面对未见工具时的检索准确性。实验表明，该方法在归纳和传导场景下均优于现有方法，能有效缓解工具分布漂移与相似性检索脆弱性问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/LoSemB_Logic-Guided_Semantic_Bridging_for_Inductive_Tool_Retrieval.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="253-Semantic-Caching-for-Low-Cost-LLM-Serving-From-Offline-Learning-to-Online-Adaptation"><a href="#253-Semantic-Caching-for-Low-Cost-LLM-Serving-From-Offline-Learning-to-Online-Adaptation" class="headerlink" title="253. Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation"></a>253. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Semantic_Caching_for_Low-Cost_LLM_Serving__From_Offline_Learning_to_Online_Adaptation.pdf">Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carnegie Mellon University</span></p>
<p>本文提出了一种面向低成本大语言模型（LLM）推理的语义缓存框架，涵盖了参数已知的oracle、离线学习和在线自适应三种场景，核心包括引入带有语义距离的损失函数、逆向贪心算法、置信上&#x2F;下界学习（CUCB-SC与CLCB-SC-LS），并在理论上给出近最优性能保证。实验结果显示该方法在子最优性间隙、平均遗憾和缓存切换次数等指标上均优于现有基线，显著提升了LLM服务效率与可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Semantic_Caching_for_Low-Cost_LLM_Serving_From_Offline_Learning_to_Online_Adaptation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="254-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks"><a href="#254-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks" class="headerlink" title="254. GLiClass: Generalist Lightweight Model for Sequence Classification Tasks"></a>254. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/GLiClass__Generalist_Lightweight_Model_for_Sequence_Classification_Tasks.pdf">GLiClass: Generalist Lightweight Model for Sequence Classification Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Knowledgator Engineering</span></p>
<p>本文提出了GLiClass，一种基于标签条件编码器的Transformer家族模型，采用uni-encoder架构联合处理文本和标签，实现了高效的多标签序列分类。模型在DeBERTa等主流骨干上，通过改进的训练流程（包括LoRA适配、PPO强化学习、逻辑与NLI后训练等），在标准分类基准上取得了优于cross-encoder的准确率，推理速度在多标签情况下显著优于传统方法。结论显示，GLiClass在准确率与效率之间实现了优良平衡，适合高吞吐量、零样本和少样本场景，并为大标签集分类任务提供了可扩展的解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/GLiClass_Generalist_Lightweight_Model_for_Sequence_Classification_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="255-Multi-Turn-Jailbreaks-Are-Simpler-Than-They-Seem"><a href="#255-Multi-Turn-Jailbreaks-Are-Simpler-Than-They-Seem" class="headerlink" title="255. Multi-Turn Jailbreaks Are Simpler Than They Seem"></a>255. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-Turn_Jailbreaks_Are_Simpler_Than_They_Seem.pdf">Multi-Turn Jailbreaks Are Simpler Than They Seem</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Imperial College London</span></p>
<p>本文通过对GPT-4、Claude、Gemini等主流大模型的实证分析，基于StrongREJECT基准，提出并自动化了多轮（multi-turn）jailbreak攻击流程，并与单轮攻击等价重采样机制做对比。实验表明，多轮jailbreak的高成功率主要来源于可多次尝试而非对话复杂性，其效果基本等同于单轮攻击的多次采样。结论指出：当前安全评测低估了模型脆弱性，防御重点应转向对有害请求本身的鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multi-Turn_Jailbreaks_Are_Simpler_Than_They_Seem.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="256-Beyond-Single-A-Data-Selection-Principle-for-LLM-Alignment-via-Fine-Grained-Preference-Signals"><a href="#256-Beyond-Single-A-Data-Selection-Principle-for-LLM-Alignment-via-Fine-Grained-Preference-Signals" class="headerlink" title="256. Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals"></a>256. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Beyond_Single__A_Data_Selection_Principle_for_LLM_Alignment_via_Fine-Grained_Preference_Signals.pdf">Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本文提出了一种针对大型语言模型（LLM）对齐的新数据选择原则，核心为基于精细化偏好信号而非单一整体偏好。作者首先推导出多偏好直接优化（DMPO）目标，提出衡量子偏好冲突的Preference Divergence（PD）指标，并据此筛选高一致性样本用于DPO训练，配合PD估计和长度偏置修正。实验证明，该方法在多个冲突水平的数据集上，相比传统整体偏好和oracle方法获得超10%的相对提升，同时提升训练效率且无需繁琐的整体偏好标注。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Beyond_Single_A_Data_Selection_Principle_for_LLM_Alignment_via_Fine-Grained_Preference_Signals.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="257-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization"><a href="#257-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization" class="headerlink" title="257. Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization"></a>257. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Klear-Reasoner__Advancing_Reasoning_Capability_via_Gradient-Preserving_Clipping_Policy_Optimization.pdf">Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou Technology</span></p>
<p>Klear-Reasoner integrates long Chain-of-Thought supervised fine-tuning (CoT SFT) with a novel Gradient-Preserving Clipping Policy Optimization (GPPO) method in reinforcement learning, specifically designed for mathematical and programming reasoning tasks. The model uses high-quality, carefully curated data, and the GPPO technique preserves gradient signals from all tokens to enhance exploration and learning efficiency, overcoming limitations of traditional clipping. Evaluations show Klear-Reasoner consistently matches or surpasses state-of-the-art models on benchmarks such as AIME2024, AIME2025, and LiveCodeBench, demonstrating that targeted data, principled SFT, and advanced RL optimization together yield substantial improvements in long-form reasoning performance.</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Klear-Reasoner_Advancing_Reasoning_Capability_via_Gradient-Preserving_Clipping_Policy_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="258-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment"><a href="#258-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment" class="headerlink" title="258. Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment"></a>258. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_to_Align,_Aligning_to_Learn__A_Unified_Approach_for_Self-Optimized_Alignment.pdf">Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Intelligence Healthcare Department, AntGroup</span></p>
<p>本文提出了GRAO（Group Relative Alignment Optimization）统一对齐框架，将监督微调（SFT）的高效收敛与强化学习（RL）的主动探索能力结合。其方法包括多样本生成、组内相对优势加权的直接对齐损失和基于偏好动态的参数更新，并在理论上证明收敛性，同时在复杂人类对齐任务中显著超越SFT、DPO、PPO和GRPO等主流方法，提升对齐效率和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Learning_to_Align%2C_Aligning_to_Learn_A_Unified_Approach_for_Self-Optimized_Alignment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="259-Chimera-Harnessing-Multi-Agent-LLMs-for-Automatic-Insider-Threat-Simulation"><a href="#259-Chimera-Harnessing-Multi-Agent-LLMs-for-Automatic-Insider-Threat-Simulation" class="headerlink" title="259. Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation"></a>259. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Chimera__Harnessing_Multi-Agent_LLMs_for_Automatic_Insider_Threat_Simulation.pdf">Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Singapore Management University</span></p>
<p>该论文提出了Chimera——首个基于大语言模型（LLM）的多智能体框架，实现自动化企业内部威胁（insider threat）仿真，包括企业成员的正常与恶意行为模拟，并收集多模态日志，生成高真实性的数据集ChimeraLog。通过人类专家和定量评估，验证了ChimeraLog的真实性和多样性，且实验发现该数据集对现有威胁检测模型更具挑战性，有助于推动更强健的威胁检测方法发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Chimera_Harnessing_Multi-Agent_LLMs_for_Automatic_Insider_Threat_Simulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="260-Semantic-Enhanced-Time-Series-Forecasting-via-Large-Language-Models"><a href="#260-Semantic-Enhanced-Time-Series-Forecasting-via-Large-Language-Models" class="headerlink" title="260. Semantic-Enhanced Time-Series Forecasting via Large Language Models"></a>260. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Semantic-Enhanced_Time-Series_Forecasting_via_Large_Language_Models.pdf">Semantic-Enhanced Time-Series Forecasting via Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology Beijing</span></p>
<p>本文提出了SE-LLM框架，通过设计时序-语义交叉相关（TSCC）模块和时序适配器（Time-Adapter），将时间序列的周期性和异常特征嵌入LLM的语义空间，显著提升LLM对时序数据的理解和预测能力。实验证明，SE-LLM在多项标准数据集上优于现有SOTA方法，并高效提升了模型的泛化能力和计算效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Semantic-Enhanced_Time-Series_Forecasting_via_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="261-1-2-3-Check-Enhancing-Contextual-Privacy-in-LLM-via-Multi-Agent-Reasoning"><a href="#261-1-2-3-Check-Enhancing-Contextual-Privacy-in-LLM-via-Multi-Agent-Reasoning" class="headerlink" title="261. 1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning"></a>261. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/1-2-3_Check__Enhancing_Contextual_Privacy_in_LLM_via_Multi-Agent_Reasoning.pdf">1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carnegie Mellon University</span></p>
<p>该论文提出了一种多智能体框架，将隐私推理任务分解为提取、分类、生成和验证等子任务，通过Extractor、Checker和Executor三阶段协同，控制信息流并迭代校正，实现更可靠的情境隐私保护。实验显示，在ConfAIde和PrivacyLens基准测试上，多智能体系统能显著减少隐私泄露（最高达18%-19%），同时保持公共内容完整性，优于单智能体基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/1-2-3_Check_Enhancing_Contextual_Privacy_in_LLM_via_Multi-Agent_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="262-Designing-a-Feedback-Driven-Decision-Support-System-for-Dynamic-Student-Intervention"><a href="#262-Designing-a-Feedback-Driven-Decision-Support-System-for-Dynamic-Student-Intervention" class="headerlink" title="262. Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention"></a>262. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Designing_a_Feedback-Driven_Decision_Support_System_for_Dynamic_Student_Intervention.pdf">Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">WeAreGenius Research Institute</span></p>
<p>本论文提出了一种基于LightGBM的反馈驱动决策支持系统（DSS），结合Flask实现的实时Web界面和SHAP可解释性，核心创新为闭环架构：预测-干预-反馈-增量式重训练。系统能持续吸收学生干预后的真实成绩反馈，自动更新模型，提升预测精度。实验结果显示，模型重训练后RMSE下降10.7%，R2提升，所有反馈学生预测成绩持续上升，证明模型可动态自我改进，适应教育环境变化，推动教育分析从静态预测向主动、响应式、以人为本的智能决策转变。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Designing_a_Feedback-Driven_Decision_Support_System_for_Dynamic_Student_Intervention.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="263-QuProFS-An-Evolutionary-Training-free-Approach-to-Efficient-Quantum-Feature-Map-Search"><a href="#263-QuProFS-An-Evolutionary-Training-free-Approach-to-Efficient-Quantum-Feature-Map-Search" class="headerlink" title="263. QuProFS: An Evolutionary Training-free Approach to Efficient Quantum Feature Map Search"></a>263. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/QuProFS__An_Evolutionary_Training-free_Approach_to_Efficient_Quantum_Feature_Map_Search.pdf">QuProFS: An Evolutionary Training-free Approach to Efficient Quantum Feature Map Search</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Tokyo</span></p>
<p>本文提出了QuProFS，一种无需训练的进化量子架构搜索（QAS）框架，通过集成硬件约束、结构化与非结构化电路设计，并利用多种训练自由代理指标（如可训练性、硬件鲁棒性、表达能力和数据对齐）进行高效筛选和进化优化。该方法在多种真实、人工和量子生成数据集上，通过量子支持向量机任务，在模拟器与真实量子硬件上均实现了比现有QAS方法更高的准确率和2倍的搜索加速。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/QuProFS_An_Evolutionary_Training-free_Approach_to_Efficient_Quantum_Feature_Map_Search.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="264-SEF-MK-Speaker-Embedding-Free-Voice-Anonymization-through-Multi-k-means-Quantization"><a href="#264-SEF-MK-Speaker-Embedding-Free-Voice-Anonymization-through-Multi-k-means-Quantization" class="headerlink" title="264. SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means Quantization"></a>264. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SEF-MK__Speaker-Embedding-Free_Voice_Anonymization_through_Multi-k-means_Quantization.pdf">SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means Quantization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Duke Kunshan University</span></p>
<p>本文提出了一种无需说话人嵌入的语音匿名化方法SEF-MK，利用多k-means量化对自监督学习(SSL)提取的特征进行匿名化，具体流程为WavLM编码、随机选择多k-means模型量化、Conformer+HiFi-GAN解码。实验结果显示，多k-means量化比单一模型更好地保留了语音内容和情感，同时在攻击者使用多模型时会增强攻击有效性，揭示了多k-means策略在用户与攻击者视角下的隐私保护与实用性的权衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/SEF-MK_Speaker-Embedding-Free_Voice_Anonymization_through_Multi-k-means_Quantization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="265-A-Stage-Aware-Mixture-of-Experts-Framework-for-Neurodegenerative-Disease-Progression-Modelling"><a href="#265-A-Stage-Aware-Mixture-of-Experts-Framework-for-Neurodegenerative-Disease-Progression-Modelling" class="headerlink" title="265. A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling"></a>265. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Stage-Aware_Mixture_of_Experts_Framework_for_Neurodegenerative_Disease_Progression_Modelling.pdf">A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University College London</span></p>
<p>本文提出了一种阶段感知的专家混合（IGND-MoE）框架，通过结合路径生理模型与异质图神经扩散模型（GAE）和本地神经反应模块，并利用时序注意力机制动态调节各专家在疾病不同阶段的贡献，实现从稀疏、非规则采样的个体快照中重建队列级疾病进展轨迹。实验表明，该模型在人脑tau蛋白病理传播的长期预测上优于纯路径生理或纯神经网络模型，并可解释不同阶段的主导疾病机制，发现早期以图结构扩散为主，后期则有更多未知物理过程主导。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Stage-Aware_Mixture_of_Experts_Framework_for_Neurodegenerative_Disease_Progression_Modelling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="266-Multi-Level-Service-Performance-Forecasting-via-Spatiotemporal-Graph-Neural-Networks"><a href="#266-Multi-Level-Service-Performance-Forecasting-via-Spatiotemporal-Graph-Neural-Networks" class="headerlink" title="266. Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks"></a>266. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-Level_Service_Performance_Forecasting_via_Spatiotemporal_Graph_Neural_Networks.pdf">Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent Author</span></p>
<p>该论文提出一种基于时空图神经网络的性能预测算法，通过将分布式后端服务系统在不同时间切片抽象为图结构，结合服务节点的运行特征和调用关系，利用多层图卷积网络捕捉结构依赖信息，并通过门控循环单元建模性能指标的动态演化，引入时间编码机制提升对非平稳序列的表达能力。实验结果表明，该模型在不同时间窗口和高并发场景下均优于主流方法，表现出强鲁棒性和泛化能力，为分布式服务系统的性能预测和运维管理提供了有效技术支撑。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multi-Level_Service_Performance_Forecasting_via_Spatiotemporal_Graph_Neural_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="267-Improving-Real-Time-Concept-Drift-Detection-using-a-Hybrid-Transformer-Autoencoder-Framework"><a href="#267-Improving-Real-Time-Concept-Drift-Detection-using-a-Hybrid-Transformer-Autoencoder-Framework" class="headerlink" title="267. Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework"></a>267. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Improving_Real-Time_Concept_Drift_Detection_using_a_Hybrid_Transformer-Autoencoder_Framework.pdf">Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Vellore Institute of Technology (VIT-AP)</span></p>
<p>本文提出了一种结合Transformer与Autoencoder的混合框架，实现对复杂时序数据概念漂移的实时检测。该方法利用统计指标（PSI、JSD）、重构误差、预测不确定性、规则违规和分类器误差，融合为信任评分，实现更灵敏且可解释的漂移监测。实验结果表明，混合Transformer-Autoencoder比传统方法在检测准确性和响应速度上均有提升，可应用于实际流式场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Improving_Real-Time_Concept_Drift_Detection_using_a_Hybrid_Transformer-Autoencoder_Framework.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="268-Whisfusion-Parallel-ASR-Decoding-via-a-Diffusion-Transformer"><a href="#268-Whisfusion-Parallel-ASR-Decoding-via-a-Diffusion-Transformer" class="headerlink" title="268. Whisfusion: Parallel ASR Decoding via a Diffusion Transformer"></a>268. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Whisfusion__Parallel_ASR_Decoding_via_a_Diffusion_Transformer.pdf">Whisfusion: Parallel ASR Decoding via a Diffusion Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Seoul National University</span></p>
<p>本文提出了Whisfusion框架，将预训练Whisper语音编码器与文本扩散解码器通过轻量级交叉注意力适配器融合，实现非自回归（NAR）自动语音识别（ASR）并支持全并行解码。通过两阶段课程训练和并行扩散解码（PDD）策略，Whisfusion在LibriSpeech数据集上相较于Whisper-tiny取得更低的词错误率（8.3% vs 9.7%）且在长语音下推理速度提升2.6倍，验证了扩散解码器在高吞吐、低延迟ASR中的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Whisfusion_Parallel_ASR_Decoding_via_a_Diffusion_Transformer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="269-Balancing-Privacy-and-Efficiency-Music-Information-Retrieval-via-Additive-Homomorphic-Encryption"><a href="#269-Balancing-Privacy-and-Efficiency-Music-Information-Retrieval-via-Additive-Homomorphic-Encryption" class="headerlink" title="269. Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption"></a>269. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Balancing_Privacy_and_Efficiency__Music_Information_Retrieval_via_Additive_Homomorphic_Encryption.pdf">Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent Researcher</span></p>
<p>本文提出利用加法同态加密（AHE）实现音乐向量嵌入的隐私保护相似性检索，分析了音乐信息检索特有的威胁模型，并设计了结构感知的分块与加权层次内积方法，提高了加密域下的检索效率和可解释性。实验证明，AHE方案在加密音乐数据检索上较全同态加密（FHE）显著提升了速度与内存效率，兼顾安全与实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Balancing_Privacy_and_Efficiency_Music_Information_Retrieval_via_Additive_Homomorphic_Encryption.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="270-TurboBias-Universal-ASR-Context-Biasing-powered-by-GPU-accelerated-Phrase-Boosting-Tree"><a href="#270-TurboBias-Universal-ASR-Context-Biasing-powered-by-GPU-accelerated-Phrase-Boosting-Tree" class="headerlink" title="270. TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree"></a>270. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TurboBias__Universal_ASR_Context-Biasing_powered_by_GPU-accelerated_Phrase-Boosting_Tree.pdf">TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">NVIDIA</span></p>
<p>本文提出了一种通用的自动语音识别（ASR）上下文偏置框架，基于GPU加速的短语提升树，支持CTC、RNN-T和Attention Encoder-Decoder等主流ASR模型。该方法通过修改树中权重分布，实现了在greedy与beam search模式下高效且高准确率的短语提升，提升关键短语识别F-score 8-10%（greedy）及17-23%（beam），整体识别准确率WER也有所提升，速度开销仅2-5%，对上下文短语数量具有良好鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TurboBias_Universal_ASR_Context-Biasing_powered_by_GPU-accelerated_Phrase-Boosting_Tree.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="271-Neural-Channel-Knowledge-Map-Assisted-Scheduling-Optimization-of-Active-IRSs-in-Multi-User-Systems"><a href="#271-Neural-Channel-Knowledge-Map-Assisted-Scheduling-Optimization-of-Active-IRSs-in-Multi-User-Systems" class="headerlink" title="271. Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems"></a>271. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Neural_Channel_Knowledge_Map_Assisted_Scheduling_Optimization_of_Active_IRSs_in_Multi-User_Systems.pdf">Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiamen University</span></p>
<p>本文提出了基于神经网络的Channel Knowledge Map (CKM)方法，通过两级Transformer网络（LPS-Net和SE-Net）利用历史信道与吞吐量数据及用户位置预测多用户主动IRS系统中的链路功率统计和遍历频谱效率，并辅以低复杂度的Stable Matching-Iterative Balancing (SM-IB)调度算法实现近最优max-min吞吐量分配。实验结果表明，该方法在预测准确率和计算效率方面优于传统方法，SM-IB算法可在大规模系统中大幅降低复杂度，同时保证吞吐量接近上界。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Neural_Channel_Knowledge_Map_Assisted_Scheduling_Optimization_of_Active_IRSs_in_Multi-User_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="272-TLCCSP-A-Scalable-Framework-for-Enhancing-Time-Series-Forecasting-with-Time-Lagged-Cross-Correlations"><a href="#272-TLCCSP-A-Scalable-Framework-for-Enhancing-Time-Series-Forecasting-with-Time-Lagged-Cross-Correlations" class="headerlink" title="272. TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations"></a>272. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TLCCSP__A_Scalable_Framework_for_Enhancing_Time_Series_Forecasting_with_Time-Lagged_Cross-Correlatio.pdf">TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Normal University</span></p>
<p>该论文提出了TLCCSP框架，利用序列移动动态时间规整（SSDTW）算法自动识别并整合时间滞后交叉相关（TLCC）序列，以提升时间序列预测准确度，并通过对比学习编码器（CLE）高效近似SSDTW距离以降低计算开销。实验结果显示，TLCCSP在气象、金融和房地产等数据集上均显著降低预测误差，并通过对比学习方法将相关性计算时间缩短约99%，提升了模型的实时性和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/TLCCSP_A_Scalable_Framework_for_Enhancing_Time_Series_Forecasting_with_Time-Lagged_Cross-Correlations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="273-UniMove-A-Unified-Model-for-Multi-city-Human-Mobility-Prediction"><a href="#273-UniMove-A-Unified-Model-for-Multi-city-Human-Mobility-Prediction" class="headerlink" title="273. UniMove: A Unified Model for Multi-city Human Mobility Prediction"></a>273. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/UniMove__A_Unified_Model_for_Multi-city_Human_Mobility_Prediction.pdf">UniMove: A Unified Model for Multi-city Human Mobility Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出了UniMove，一种用于多城市人类移动预测的统一模型，采用轨迹-位置双塔结构：位置塔通过Deep &amp; Cross Net编码城市内位置特征，轨迹塔用MoE Transformer处理不同城市的多样移动模式。实验显示，UniMove能在多城市数据联合训练下提升预测准确率（最高提升35.7%），尤其对数据稀疏城市效果显著，具有良好的泛化和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/UniMove_A_Unified_Model_for_Multi-city_Human_Mobility_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="274-Neural-Beam-Field-for-Spatial-Beam-RSRP-Prediction"><a href="#274-Neural-Beam-Field-for-Spatial-Beam-RSRP-Prediction" class="headerlink" title="274. Neural Beam Field for Spatial Beam RSRP Prediction"></a>274. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Neural_Beam_Field_for_Spatial_Beam_RSRP_Prediction.pdf">Neural Beam Field for Spatial Beam RSRP Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiamen University</span></p>
<p>本文提出了一种混合神经-物理框架NBF用于高效且可解释的空间波束RSRP预测，核心方法包括引入多路径条件功率剖面（MCPP）作为物理传播与天线&#x2F;波束配置的中介，并采用Transformer结构深度神经网络学习MCPP，再结合解析公式推理波束RSRP统计量。实验结果显示，NBF在准确性、训练效率和泛化能力上显著优于传统表格型信道知识图和纯黑箱DNN，同时具备更高存储效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Neural_Beam_Field_for_Spatial_Beam_RSRP_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="275-Machine-Learning-Algorithms-for-Improving-Exact-Classical-Solvers-in-Mixed-Integer-Continuous-Optimization"><a href="#275-Machine-Learning-Algorithms-for-Improving-Exact-Classical-Solvers-in-Mixed-Integer-Continuous-Optimization" class="headerlink" title="275. Machine Learning Algorithms for Improving Exact Classical Solvers in Mixed Integer Continuous Optimization"></a>275. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Machine_Learning_Algorithms_for_Improving_Exact_Classical_Solvers_in_Mixed_Integer_Continuous_Optimi.pdf">Machine Learning Algorithms for Improving Exact Classical Solvers in Mixed Integer Continuous Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universität Wien</span></p>
<p>本文系统综述了机器学习（ML）和强化学习（RL）方法如何集成到整数、连续和混合整数非线性优化（INLP、CNLP、MINLP）问题的精确经典求解器中，重点聚焦于Branch-and-Bound等框架的分支、剪枝、节点选择、切割平面、参数控制等关键环节的ML&#x2F;RL增强技术流程。结论认为，ML和RL的嵌入可显著加速求解速度，提升大规模问题的收敛效率，同时不损失全局最优性和理论收敛保证，但仍需进一步解决泛化、可解释性与混合集成等挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Machine_Learning_Algorithms_for_Improving_Exact_Classical_Solvers_in_Mixed_Integer_Continuous_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="276-A-Score-based-Diffusion-Model-Approach-for-Adaptive-Learning-of-Stochastic-Partial-Differential-Equation-Solutions"><a href="#276-A-Score-based-Diffusion-Model-Approach-for-Adaptive-Learning-of-Stochastic-Partial-Differential-Equation-Solutions" class="headerlink" title="276. A Score-based Diffusion Model Approach for Adaptive Learning of Stochastic Partial Differential Equation Solutions"></a>276. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Score-based_Diffusion_Model_Approach_for_Adaptive_Learning_of_Stochastic_Partial_Differential_Equa.pdf">A Score-based Diffusion Model Approach for Adaptive Learning of Stochastic Partial Differential Equation Solutions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Oak Ridge National Laboratory</span></p>
<p>本文提出了一种基于score-based diffusion model的新型自适应学习框架，用于递归贝叶斯推断下随机偏微分方程（SPDEs）解的更新。该方法将SPDE物理知识以得分函数的形式编码进扩散模型，并结合观测数据通过反向SDE进行似然修正，实现观测稀疏和噪声环境下的高效自适应学习，提升解的精度和鲁棒性。大量数值实验表明，所提方法在Burgers方程、Navier–Stokes方程及Allen–Cahn方程等典型SPDEs下，能有效融合观测信息，显著优于现有主流滤波器（如LETKF）并保持物理一致性和能量守恒等关键性质。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/A_Score-based_Diffusion_Model_Approach_for_Adaptive_Learning_of_Stochastic_Partial_Differential_Equation_Solutions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="277-Hardness-Aware-Dynamic-Curriculum-Learning-for-Robust-Multimodal-Emotion-Recognition-with-Missing-Modalities"><a href="#277-Hardness-Aware-Dynamic-Curriculum-Learning-for-Robust-Multimodal-Emotion-Recognition-with-Missing-Modalities" class="headerlink" title="277. Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities"></a>277. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hardness-Aware_Dynamic_Curriculum_Learning_for_Robust_Multimodal_Emotion_Recognition_with_Missing_Mo.pdf">Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Inner Mongolia University</span></p>
<p>本文提出了一种硬度感知动态课程学习框架（HARDY-MER），用于提升在模态缺失情况下的多模态情感识别鲁棒性。该方法首先通过多视角硬度评估机制量化样本重建难度（结合重建误差与跨模态互信息），然后利用检索增强的动态课程学习策略，根据样本难度自适应检索支持样本，重点提升对难样本的学习。实验表明，该方法在IEMOCAP与CMU-MOSEI等基准数据集的各类模态缺失情形下均优于现有方法，实现了新的SOTA性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Hardness-Aware_Dynamic_Curriculum_Learning_for_Robust_Multimodal_Emotion_Recognition_with_Missing_Modalities.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="278-Noise-Aware-Generative-Microscopic-Traffic-Simulation"><a href="#278-Noise-Aware-Generative-Microscopic-Traffic-Simulation" class="headerlink" title="278. Noise-Aware Generative Microscopic Traffic Simulation"></a>278. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Noise-Aware_Generative_Microscopic_Traffic_Simulation.pdf">Noise-Aware Generative Microscopic Traffic Simulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Massachusetts Institute of Technology</span></p>
<p>本文提出了I-24 MOTION Scenario Dataset (I24-MSD)，该数据集为微观交通仿真提供了包含现实传感器噪声的标准化车辆轨迹数据。作者将生成式模型（如SMART）与噪声感知损失函数（标签平滑、focal loss、对称交叉熵）结合，提升了模型在数据不完美环境下的泛化能力和仿真真实度，实验证明噪声建模可显著优于传统基线。结论：噪声感知优化方法能有效提升微观交通仿真模型的准确性和实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Noise-Aware_Generative_Microscopic_Traffic_Simulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="279-Structured-Superposition-of-Autoencoders-for-UEP-Codes-at-Intermediate-Blocklengths"><a href="#279-Structured-Superposition-of-Autoencoders-for-UEP-Codes-at-Intermediate-Blocklengths" class="headerlink" title="279. Structured Superposition of Autoencoders for UEP Codes at Intermediate Blocklengths"></a>279. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Structured_Superposition_of_Autoencoders_for_UEP_Codes_at_Intermediate_Blocklengths.pdf">Structured Superposition of Autoencoders for UEP Codes at Intermediate Blocklengths</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Novi Sad</span></p>
<p>本文提出了一种结构化自编码器（AE）架构，通过将编码和解码过程分解为多个子块，实现了中等区块长度下高效可扩展的比特级不等错误保护（UEP）码设计。与传统的叠加编码和SIC解码方案相比，该方法在可调节不同可靠性等级的同时，显著扩展了可实现的误码概率区域，提升了性能并适用于资源受限环境。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Structured_Superposition_of_Autoencoders_for_UEP_Codes_at_Intermediate_Blocklengths.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="280-Lightning-Prediction-under-Uncertainty-DeepLight-with-Hazy-Loss"><a href="#280-Lightning-Prediction-under-Uncertainty-DeepLight-with-Hazy-Loss" class="headerlink" title="280. Lightning Prediction under Uncertainty: DeepLight with Hazy Loss"></a>280. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Lightning_Prediction_under_Uncertainty__DeepLight_with_Hazy_Loss.pdf">Lightning Prediction under Uncertainty: DeepLight with Hazy Loss</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bangladesh University of Engineering and Technology</span></p>
<p>该论文提出了一种用于雷电预测的深度学习模型DeepLight，采用多分支ConvLSTM结构以动态提取不同空间范围的相关性，融合真实雷电观测、雷达反射率和云特性等多源气象数据。创新的Hazy Loss损失函数通过邻域惩罚机制增强模型对雷电事件时空不确定性的适应能力。实验证明DeepLight在ETS等衡量指标上对比主流方法提升显著：1小时预测提升30%，3小时提升18-22%，6小时提升8-13%；Hazy Loss进一步提升准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Lightning_Prediction_under_Uncertainty_DeepLight_with_Hazy_Loss.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="281-Leveraging-GNN-to-Enhance-MEF-Method-in-Predicting-ENSO"><a href="#281-Leveraging-GNN-to-Enhance-MEF-Method-in-Predicting-ENSO" class="headerlink" title="281. Leveraging GNN to Enhance MEF Method in Predicting ENSO"></a>281. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Leveraging_GNN_to_Enhance_MEF_Method_in_Predicting_ENSO.pdf">Leveraging GNN to Enhance MEF Method in Predicting ENSO</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shiraz University of Technology</span></p>
<p>本文提出了一种结合3D卷积神经网络（3DCNN）和图神经网络（GNN）的多模态ENSO预测方法（MEF），通过构建输出相似性图筛选和聚合高质量的集成成员，显著提升了气候预测的准确性和稳定性。实验结果表明，该方法在长期（超过12个月）预测中比传统平均或加权集成方法更鲁棒，尤其适用于后2000年气候变异加剧的情景，且方法可泛化至其他气候模型集成后处理。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Leveraging_GNN_to_Enhance_MEF_Method_in_Predicting_ENSO.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="282-ProteoKnight-Convolution-based-phage-virion-protein-classification-and-uncertainty-analysis"><a href="#282-ProteoKnight-Convolution-based-phage-virion-protein-classification-and-uncertainty-analysis" class="headerlink" title="282. ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis"></a>282. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ProteoKnight__Convolution-based_phage_virion_protein_classification_and_uncertainty_analysis.pdf">ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">BRAC University</span></p>
<p>ProteoKnight提出了一种基于图像的Knight编码方法，将蛋白质序列通过极坐标和颜色映射编码为图像，利用预训练卷积神经网络（如GoogLeNet等）进行噬菌体衣壳蛋白的二分类与多分类，并通过Monte Carlo Dropout量化模型的不确定性。实验结果显示，该方法在二分类中达到90.8%的准确率，优于传统空间特征丢失的方法，并揭示了序列类别和长度对不确定性的影响。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/ProteoKnight_Convolution-based_phage_virion_protein_classification_and_uncertainty_analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="283-Finite-Time-Convergence-Analysis-of-ODE-based-Generative-Models-for-Stochastic-Interpolants"><a href="#283-Finite-Time-Convergence-Analysis-of-ODE-based-Generative-Models-for-Stochastic-Interpolants" class="headerlink" title="283. Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants"></a>283. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Finite-Time_Convergence_Analysis_of_ODE-based_Generative_Models_for_Stochastic_Interpolants.pdf">Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出对基于随机插值的生成模型中常用的ODE数值解法（前向Euler法和二阶Heun法）进行有限时间收敛性理论分析，首次建立了总变差距离的误差上界，并详细分析了维度、分布属性、步长及数值方法阶数对生成误差的影响。结论表明两种方法的收敛速率均可被理论界定，且二阶方法具备更优复杂度；实验结果证实了理论分析的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Finite-Time_Convergence_Analysis_of_ODE-based_Generative_Models_for_Stochastic_Interpolants.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="284-Fairness-of-Automatic-Speech-Recognition-Looking-Through-a-Philosophical-Lens"><a href="#284-Fairness-of-Automatic-Speech-Recognition-Looking-Through-a-Philosophical-Lens" class="headerlink" title="284. Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens"></a>284. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fairness_of_Automatic_Speech_Recognition__Looking_Through_a_Philosophical_Lens.pdf">Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Cornell University</span></p>
<p>本文通过哲学视角系统分析自动语音识别（ASR）系统的公平性，提出“尊重”概念并区分中性分类与有害歧视，强调ASR对非标准方言系统性误识别不仅是技术问题，更加剧历史性不公。结论认为，仅靠技术手段难以解决ASR偏见，需承认多样语言的合法性并以伦理和政治责任推动系统设计，确保不同群体发声权利和身份被尊重。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Fairness_of_Automatic_Speech_Recognition_Looking_Through_a_Philosophical_Lens.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="285-An-effective-potential-for-generative-modelling-with-active-matter"><a href="#285-An-effective-potential-for-generative-modelling-with-active-matter" class="headerlink" title="285. An effective potential for generative modelling with active matter"></a>285. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/An_effective_potential_for_generative_modelling_with_active_matter.pdf">An effective potential for generative modelling with active matter</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Queen Mary University of London</span></p>
<p>本文提出了一种基于主动粒子过程（如有色噪声的AOUP模型）的生成扩散模型，通过对位置坐标施加有效的时变势能实现时间反转生成过程。作者推导了Fox和UCN近似下的有效势能，并通过数值实验验证了方法在人工数据分布上的有效性，结果显示所提模型能准确逼近目标分布，具有较高的生成性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/An_effective_potential_for_generative_modelling_with_active_matter.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="286-OFAL-An-Oracle-Free-Active-Learning-Framework"><a href="#286-OFAL-An-Oracle-Free-Active-Learning-Framework" class="headerlink" title="286. OFAL: An Oracle-Free Active Learning Framework"></a>286. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/OFAL__An_Oracle-Free_Active_Learning_Framework.pdf">OFAL: An Oracle-Free Active Learning Framework</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Amirkabir University of Technology</span></p>
<p>该论文提出了OFAL，一种无需人工标注（oracle）的主动学习框架，通过神经网络的不确定性量化，结合Monte Carlo Dropout和变分自编码器（VAE），将模型高置信度的未标记样本转化为不确定样本以提升模型性能。实验表明，OFAL在无需额外标注代价的情况下，将模型在MNIST数据集的准确率提升了2.7%，并可与传统主动学习采样方法集成进一步提升性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/OFAL_An_Oracle-Free_Active_Learning_Framework.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="287-Learned-Regularization-for-Microwave-Tomography"><a href="#287-Learned-Regularization-for-Microwave-Tomography" class="headerlink" title="287. Learned Regularization for Microwave Tomography"></a>287. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learned_Regularization_for_Microwave_Tomography.pdf">Learned Regularization for Microwave Tomography</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本文提出了一种基于物理建模与扩散模型（Diffusion Model）结合的混合重建框架，用于微波层析成像（MWT）。方法创新性地将单步扩散正则化（SSD-Reg）作为Plug-and-Play模块嵌入可变分重建流程，通过Fréchet可微前向模型和无监督学习到的扩散先验，实现了无需成对数据的高效重建。实验结果显示，该方法在合成和真实数据上均显著优于传统和现有深度学习方法，提高了重建质量、稳定性和抗噪声鲁棒性，且收敛速度更快。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Learned_Regularization_for_Microwave_Tomography.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="288-C-MAG-Cascade-Multimodal-Attributed-Graphs-for-Supply-Chain-Link-Prediction"><a href="#288-C-MAG-Cascade-Multimodal-Attributed-Graphs-for-Supply-Chain-Link-Prediction" class="headerlink" title="288. C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction"></a>288. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/C-MAG__Cascade_Multimodal_Attributed_Graphs_for_Supply_Chain_Link_Prediction.pdf">C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AAITC, Lenovo</span></p>
<p>本论文提出了C-MAG（Cascade Multimodal Attributed Graph），通过两阶段级联架构融合文本、图像等多模态属性，先在制造商属性图中对文本和视觉特征对齐聚合，再通过多尺度消息传递提升制造商-产品链接预测准确率。实验表明，C-MAG在供应链知识图谱的链接预测任务上显著优于现有方法，且对噪声视觉数据具有较强鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/C-MAG_Cascade_Multimodal_Attributed_Graphs_for_Supply_Chain_Link_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="289-From-Source-to-Target-Leveraging-Transfer-Learning-for-Predictive-Process-Monitoring-in-Organizations"><a href="#289-From-Source-to-Target-Leveraging-Transfer-Learning-for-Predictive-Process-Monitoring-in-Organizations" class="headerlink" title="289. From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations"></a>289. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Source_to_Target__Leveraging_Transfer_Learning_for_Predictive_Process_Monitoring_in_Organizatio.pdf">From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Friedrich-Alexander-Universität Erlangen-Nürnberg</span></p>
<p>本文提出了一种基于迁移学习的预测性流程监控（PPM）技术，实现了在缺乏足够事件日志数据的目标组织中，无需微调即可从相似业务流程转移预测模型。该方法通过活动嵌入（使用预训练语言模型）、相对时间戳映射和两层LSTM结构，支持跨组织&#x2F;部门的模型迁移。实验证明，该技术在同组织内和跨组织场景下均显著优于传统PPM方法，能够高效提升目标业务流程的预测性能，尤其适合数据稀缺环境。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/From_Source_to_Target_Leveraging_Transfer_Learning_for_Predictive_Process_Monitoring_in_Organizations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="290-On-Understanding-of-the-Dynamics-of-Model-Capacity-in-Continual-Learning"><a href="#290-On-Understanding-of-the-Dynamics-of-Model-Capacity-in-Continual-Learning" class="headerlink" title="290. On Understanding of the Dynamics of Model Capacity in Continual Learning"></a>290. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/On_Understanding_of_the_Dynamics_of_Model_Capacity_in_Continual_Learning.pdf">On Understanding of the Dynamics of Model Capacity in Continual Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Capital One</span></p>
<p>该论文提出了持续学习中的动态模型容量（CLEMC）理论框架，通过递推方程刻画神经网络在多任务学习中的稳定性与可塑性平衡点随任务变化的非平稳性。理论证明即使每个新任务与之前任务仅有微小差异，模型容量最终也会发散，导致模型遗忘严重且表现变差，并通过在FNN、CNN、GNN和大规模语言模型上的实验证实这一结论。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/On_Understanding_of_the_Dynamics_of_Model_Capacity_in_Continual_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="291-Learning-to-Select-MCP-Algorithms-From-Traditional-ML-to-Dual-Channel-GAT-MLP"><a href="#291-Learning-to-Select-MCP-Algorithms-From-Traditional-ML-to-Dual-Channel-GAT-MLP" class="headerlink" title="291. Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP"></a>291. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_to_Select_MCP_Algorithms__From_Traditional_ML_to_Dual-Channel_GAT-MLP.pdf">Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">College of Mathematics and Computer, Shantou University</span></p>
<p>本文针对最大团问题（MCP）中缺乏面向实例的算法选择方法这一空白，提出了融合传统机器学习与图神经网络的两阶段框架。方法包括对图实例提取全局统计与局部结构特征，先用SVM、Random Forest等模型评估，再设计GAT-MLP双通道模型，分别建模局部结构与全局特征并融合分类。实验表明，GAT-MLP在算法选择任务上准确率达89.13%，显著优于传统方法，并能有效整合局部与全局信息。结论是双通道结构和图神经网络为组合算法选择任务提供了有效且具推广性的解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Learning_to_Select_MCP_Algorithms_From_Traditional_ML_to_Dual-Channel_GAT-MLP.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="292-MemoryKT-An-Integrative-Memory-and-Forgetting-Method-for-Knowledge-Tracing"><a href="#292-MemoryKT-An-Integrative-Memory-and-Forgetting-Method-for-Knowledge-Tracing" class="headerlink" title="292. MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing"></a>292. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MemoryKT__An_Integrative_Memory-and-Forgetting_Method_for_Knowledge_Tracing.pdf">MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China Normal University</span></p>
<p>该论文提出了MemoryKT，一种将记忆编码、存储和检索三阶段机制与个性化遗忘建模相结合的知识追踪方法。通过引入变分自编码器（VAE）和个性化遗忘模块，MemoryKT能够模拟学生记忆动态并精细捕捉个体遗忘特征。实验结果显示该方法在四个真实数据集上显著优于现有主流方法，实现了更高的预测精度和更强的个性化解释能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/MemoryKT_An_Integrative_Memory-and-Forgetting_Method_for_Knowledge_Tracing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="293-Fast-and-Generalizable-parameter-embedded-Neural-Operators-for-Lithium-Ion-Battery-Simulation"><a href="#293-Fast-and-Generalizable-parameter-embedded-Neural-Operators-for-Lithium-Ion-Battery-Simulation" class="headerlink" title="293. Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation"></a>293. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fast_and_Generalizable_parameter-embedded_Neural_Operators_for_Lithium-Ion_Battery_Simulation.pdf">Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">RWTH Aachen University</span></p>
<p>本文提出并系统比较了三种用于锂离子电池单颗粒模型（SPM）高效仿真的算子学习神经网络，包括Deep Operator Network (DeepONet)、Fourier Neural Operator (FNO)以及新颖的参数嵌入式FNO（PE-FNO），后者能显式处理材料半径和扩散系数等参数，实现对不同充放电电流、SOC、材料参数的泛化仿真。实验表明，PE-FNO在浓度和电压预测上能保持亚百分比误差，同时推理速度比传统SPM求解器快约200倍，并可用于快速贝叶斯参数估计任务。结论：PE-FNO为实时电池管理和大规模推断提供了高效且高保真的神经算子路径。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Fast_and_Generalizable_parameter-embedded_Neural_Operators_for_Lithium-Ion_Battery_Simulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="294-FNBT-Full-Negation-Belief-Transformation-for-Open-World-Information-Fusion-Based-on-Dempster-Shafer-Theory-of-Evidence"><a href="#294-FNBT-Full-Negation-Belief-Transformation-for-Open-World-Information-Fusion-Based-on-Dempster-Shafer-Theory-of-Evidence" class="headerlink" title="294. FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence"></a>294. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FNBT__Full_Negation_Belief_Transformation_for_Open-World_Information_Fusion_Based_on_Dempster-Shafer.pdf">FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China Normal University</span></p>
<p>该论文提出了一种基于Dempster-Shafer理论的全否定信念变换（FNBT）方法，专门用于解决开放世界异构帧下的信息融合问题。方法包括提出基于本质冲突的开放世界判别准则、帧扩展、以及全否定机制，将原始质量分配映射到扩展帧后再用经典组合规则融合。理论上证明了FNBT的质量函数不变性、遗传性和本质冲突消除等性质。实验表明，FNBT在多个真实分类任务中显著优于现有方法，具有高效性和适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/FNBT_Full_Negation_Belief_Transformation_for_Open-World_Information_Fusion_Based_on_Dempster-Shafer_Theory_of_Evidence.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="295-Multi-modal-Adaptive-Mixture-of-Experts-for-Cold-start-Recommendation"><a href="#295-Multi-modal-Adaptive-Mixture-of-Experts-for-Cold-start-Recommendation" class="headerlink" title="295. Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation"></a>295. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-modal_Adaptive_Mixture_of_Experts_for_Cold-start_Recommendation.pdf">Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">VNU University of Engineering and Technology</span></p>
<p>本文提出了MAMEX框架，通过结合模态特定的Mixture of Experts（MoE）层与可学习门控机制，实现了多模态信息的自适应融合，动态捕捉不同模态特征并平衡其贡献。实验结果在多个Amazon数据集上优于现有方法，显著提升了冷启动推荐的效果，验证了模型结构和正则化策略的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Multi-modal_Adaptive_Mixture_of_Experts_for_Cold-start_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="296-Deep-Learning-Based-Analysis-of-Power-Consumption-in-Gasoline-Electric-and-Hybrid-Vehicles"><a href="#296-Deep-Learning-Based-Analysis-of-Power-Consumption-in-Gasoline-Electric-and-Hybrid-Vehicles" class="headerlink" title="296. Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles"></a>296. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Deep_Learning-Based_Analysis_of_Power_Consumption_in_Gasoline,_Electric,_and_Hybrid_Vehicles.pdf">Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Research Council Canada</span></p>
<p>本文提出了一种基于深度学习的车辆能耗预测方法，结合TCN、LSTM、Transformer和传统随机森林模型，利用动力系统动态特征实现对汽油车、纯电车及混合动力车瞬时与累计能耗的精准预测，并进行不确定性分析。实验结果显示，所提出的数据驱动方法在ICE车辆上表现最优，在EV和HEV上也优于现有方法且模型不确定性随动力系统复杂度增加而增强。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Deep_Learning-Based_Analysis_of_Power_Consumption_in_Gasoline%2C_Electric%2C_and_Hybrid_Vehicles.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="297-Topological-Feature-Compression-for-Molecular-Graph-Neural-Networks"><a href="#297-Topological-Feature-Compression-for-Molecular-Graph-Neural-Networks" class="headerlink" title="297. Topological Feature Compression for Molecular Graph Neural Networks"></a>297. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Topological_Feature_Compression_for_Molecular_Graph_Neural_Networks.pdf">Topological Feature Compression for Molecular Graph Neural Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Imperial College London</span></p>
<p>本文提出了一种新颖的图神经网络（GNN）架构PACTNet，结合了高阶细胞复形压缩拓扑特征与标准分子特征，通过高效的ECC算法提取和融合3D结构信息，增强分子表示能力。实验在七个主流分子性质预测数据集上，PACTNet在五个任务中显著优于GCN、GAT、GIN等主流基线，且在所有任务中表现出色，验证了其准确性、鲁棒性和计算高效性。结论指出，本方法在提升分子机器学习模型性能的同时兼顾了解释性和可扩展性，可为未来分子建模提供基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Topological_Feature_Compression_for_Molecular_Graph_Neural_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="298-Frequency-Domain-Analysis-of-Time-Dependent-Multiomic-Data-in-Progressive-Neurodegenerative-Diseases-A-Proposed-Quantum-Classical-Hybrid-Approach-with-Quaternionic-Extensions"><a href="#298-Frequency-Domain-Analysis-of-Time-Dependent-Multiomic-Data-in-Progressive-Neurodegenerative-Diseases-A-Proposed-Quantum-Classical-Hybrid-Approach-with-Quaternionic-Extensions" class="headerlink" title="298. Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions"></a>298. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Frequency-Domain_Analysis_of_Time-Dependent_Multiomic_Data_in_Progressive_Neurodegenerative_Diseases.pdf">Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Massachusetts General Hospital</span></p>
<p>本论文提出一种理论框架，将神经退行性疾病的多组学时序数据通过傅里叶和拉普拉斯变换转入频域，结合哈密顿量建模与量子-经典混合计算（如VQE算法）以检测复杂动态模式，并扩展至四元数表示以捕捉多态神经动力学。结论认为，该方法有望提升高风险患者的轨迹预测与生物标志物识别能力，为未来量子增强医学奠定基础，但需在真实数据集上进行实证验证。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Frequency-Domain_Analysis_of_Time-Dependent_Multiomic_Data_in_Progressive_Neurodegenerative_Diseases_A_Proposed_Quantum-Classical_Hybrid_Approach_with_Quaternionic_Extensions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="299-Generative-Inversion-for-Property-Targeted-Materials-Design-Application-to-Shape-Memory-Alloys"><a href="#299-Generative-Inversion-for-Property-Targeted-Materials-Design-Application-to-Shape-Memory-Alloys" class="headerlink" title="299. Generative Inversion for Property-Targeted Materials Design: Application to Shape Memory Alloys"></a>299. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Generative_Inversion_for_Property-Targeted_Materials_Design__Application_to_Shape_Memory_Alloys.pdf">Generative Inversion for Property-Targeted Materials Design: Application to Shape Memory Alloys</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong University</span></p>
<p>该论文提出了一种基于生成对抗网络（GAN）反演的逆向材料设计框架，通过将预训练GAN与性质预测模型结合，在潜在空间中进行梯度优化，直接生成满足用户指定性能目标的形状记忆合金（SMA）成分及工艺参数。实验结果表明，所设计的Ni49.8Ti26.4Hf18.6Zr5.2合金在转变温度、机械功输出、转变焓和热滞后等方面均优于现有NiTi合金，验证了框架的有效性和通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Generative_Inversion_for_Property-Targeted_Materials_Design_Application_to_Shape_Memory_Alloys.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="300-Disentangling-Multiplex-Spatial-Temporal-Transition-Graph-Representation-Learning-for-Socially-Enhanced-POI-Recommendation"><a href="#300-Disentangling-Multiplex-Spatial-Temporal-Transition-Graph-Representation-Learning-for-Socially-Enhanced-POI-Recommendation" class="headerlink" title="300. Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation"></a>300. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Disentangling_Multiplex_Spatial-Temporal_Transition_Graph_Representation_Learning_for_Socially_Enhan.pdf">Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China Normal University</span></p>
<p>该论文提出了DiMuST模型，通过引入社会异构图和解耦变分多路图自编码器（DAE），联合建模用户社会关系与空间-时间转移模式，实现了空间-时间特征的私有与共享分布解耦，并采用专家乘积（PoE）机制融合共享特征、对私有特征进行对比约束去噪。结果表明，DiMuST在多个真实数据集上较现有方法有显著精度提升，关键模块有效提升了空间-时间关系建模和推荐鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-13/Disentangling_Multiplex_Spatial-Temporal_Transition_Graph_Representation_Learning_for_Socially_Enhanced_POI_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/13报纸</div>
      <div>http://ywfhhh.github.io/2025/08/2025-08-13_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>具身ADT</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月13日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/2025-08-14_article/" title="2025/08/14报纸">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">2025/08/14报纸</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-12_article/" title="2025/08/12报纸">
                        <span class="hidden-mobile">2025/08/12报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
