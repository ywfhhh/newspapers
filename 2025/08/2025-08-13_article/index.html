

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Ywfhhh">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="今日论文报纸">
<meta property="og:url" content="http://example.com/2025/08/2025-08-13_article/index.html">
<meta property="og:site_name" content="论文报纸集合">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="og:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
<meta property="article:published_time" content="2025-08-12T16:00:00.000Z">
<meta property="article:modified_time" content="2025-08-13T10:39:57.843Z">
<meta property="article:author" content="Ywfhhh">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>今日论文报纸 - 论文报纸集合</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>【学术动态报告】论文报纸</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文集合</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="今日论文报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-12 16:00" pubdate>
          星期二, 八月 12日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          45k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">今日论文报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>今日论文报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-13<br>📄 <strong>发现论文数量</strong>：300  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Consensus-based-Decentralized-Multi-agent-Reinforcement-Learning-for-Random-Access-Network-Optimization"><a href="#1-Consensus-based-Decentralized-Multi-agent-Reinforcement-Learning-for-Random-Access-Network-Optimization" class="headerlink" title="1. Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Consensus-based_Decentralized_Multi-agent_Reinforcement_Learning_for_Random_Access_Network_Optimizat.pdf">Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Ohio State University</span></p>
<p>该论文提出了一种基于共识的全分布式多智能体强化学习（MARL）方法，用于随机接入（RA）网络的MAC层优化。方法通过仅在邻居间交换局部奖励，结合actor-critic架构，在无需中心化训练的情况下实现全局收敛，显著降低了通信开销。理论分析证明了算法的有限时间收敛性，实验结果表明该方法在提升网络吞吐量和公平性方面与中心化方法相当，但更具实用性和扩展性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-AR-GRPO-Training-Autoregressive-Image-Generation-Models-via-Reinforcement-Learning"><a href="#2-AR-GRPO-Training-Autoregressive-Image-Generation-Models-via-Reinforcement-Learning" class="headerlink" title="2. AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AR-GRPO__Training_Autoregressive_Image_Generation_Models_via_Reinforcement_Learning.pdf">AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou Technology</span></p>
<p>该论文提出AR-GRPO方法，将在线强化学习（RL）训练集成到自回归（AR）图像生成模型中，并首次将Group Relative Policy Optimization（GRPO）算法应用于图像生成，通过多维度奖励函数（如语义一致性、视觉质量和真实感）细致优化模型输出。实验结果表明，RL增强的AR模型在类别条件和文本条件生成任务中，显著提升了图像质量和人类偏好指标，尤其在人类评价、分辨率和模型规模扩展方面展现强大泛化能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-Pushdown-Reward-Machines-for-Reinforcement-Learning"><a href="#3-Pushdown-Reward-Machines-for-Reinforcement-Learning" class="headerlink" title="3. Pushdown Reward Machines for Reinforcement Learning"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pushdown_Reward_Machines_for_Reinforcement_Learning.pdf">Pushdown Reward Machines for Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Utrecht Universiteit</span></p>
<p>该论文提出了Pushdown Reward Machines（pdRMs），基于确定性下推自动机，能够表达和奖励可由确定性上下文无关语言表示的非马尔可夫任务，显著提升了任务表达能力。作者设计了两种策略访问模式（全栈访问与top-k访问），并证明在部分场景下top-k策略与全栈策略在最优性上等价，理论和实验分析表明pdRMs在部分任务上较Counting Reward Automata具备更高空间效率，且在多离散和连续任务中提升了训练效率和泛化能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-Offline-to-Online-Reinforcement-Learning-with-Classifier-Free-Diffusion-Generation"><a href="#4-Offline-to-Online-Reinforcement-Learning-with-Classifier-Free-Diffusion-Generation" class="headerlink" title="4. Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Offline-to-Online_Reinforcement_Learning_with_Classifier-Free_Diffusion_Generation.pdf">Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了一种新的数据增强方法Classifier-Free Diffusion Generation (CFDG)，利用无分类器引导的扩散模型同时对离线数据和在线数据进行增强，并通过数据重加权使生成数据更贴合在线策略分布，可与主流O2O RL算法（如IQL、PEX、APL）集成。实验证明，CFDG在D4RL Locomotion和AntMaze基准上可平均提升15%的性能，生成的数据更高质量、与在线分布更一致。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-Sparsity-Driven-Plasticity-in-Multi-Task-Reinforcement-Learning"><a href="#5-Sparsity-Driven-Plasticity-in-Multi-Task-Reinforcement-Learning" class="headerlink" title="5. Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sparsity-Driven_Plasticity_in_Multi-Task_Reinforcement_Learning.pdf">Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Groningen</span></p>
<p>本文系统性研究了在多任务强化学习（MTRL）中，稀疏化方法（特别是Gradual Magnitude Pruning与Sparse Evolutionary Training）对提升神经网络可塑性及任务表现的作用。通过在多种MTRL架构与标准基准环境上的实证分析，结果表明稀疏化不仅缓解了神经元休眠和表示崩溃等可塑性退化现象，还常常提升了多任务性能，优于致力于可塑性提升的专用方法和常用正则化手段，尤其在常见架构如MTPPO和MoE中表现突出。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-Multi-level-Advantage-Credit-Assignment-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#6-Multi-level-Advantage-Credit-Assignment-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="6. Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-level_Advantage_Credit_Assignment_for_Cooperative_Multi-Agent_Reinforcement_Learning.pdf">Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mila - Quebec AI Institute</span></p>
<p>本文提出了一种多层次优势信号分配方法（MACA），通过显式的反事实推理，分别对个体、联合及强相关子集的动作贡献进行推断，并结合Transformer自注意力机制动态建模多智能体间的相关性，实现多层次的信用分配。实验结果表明，MACA在StarCraft多智能体强化学习基准任务中显著提升了协作效率和收敛性能，优于现有主流方法，理论分析也支持其有效性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-Natural-Language-Driven-Viewpoint-Navigation-for-Volume-Exploration-via-Semantic-Block-Representation"><a href="#7-Natural-Language-Driven-Viewpoint-Navigation-for-Volume-Exploration-via-Semantic-Block-Representation" class="headerlink" title="7. Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Natural_Language-Driven_Viewpoint_Navigation_for_Volume_Exploration_via_Semantic_Block_Representatio.pdf">Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-sen University</span></p>
<p>本文提出了一种基于自然语言交互的体数据探索框架，通过将体数据分割为语义块，并利用CLIP模型对图像和文本进行对齐，结合强化学习（PPO算法）自动优化视点选择。实验结果表明，该方法能准确捕捉用户意图，提升体数据导航效率和可解释性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-Stackelberg-Coupling-of-Online-Representation-Learning-and-Reinforcement-Learning"><a href="#8-Stackelberg-Coupling-of-Online-Representation-Learning-and-Reinforcement-Learning" class="headerlink" title="8. Stackelberg Coupling of Online Representation Learning and Reinforcement Learning"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Stackelberg_Coupling_of_Online_Representation_Learning_and_Reinforcement_Learning.pdf">Stackelberg Coupling of Online Representation Learning and Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fordham University</span></p>
<p>本文提出了SCORER框架，通过将感知网络（表征学习）和控制网络（强化学习）之间的协作建模为Stackelberg博弈，实现了两者的层级动态耦合。方法采用两时间尺度梯度下降近似博弈均衡，并在多种DQN变体及标准任务上验证，结果显示SCORER在样本效率和最终表现上均优于传统端到端方法。结论：基于博弈论的感知-控制动态设计可以在不增加复杂辅助目标或架构的前提下显著提升深度强化学习性能。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-Efficient-Reward-Identification-In-Max-Entropy-Reinforcement-Learning-with-Sparsity-and-Rank-Priors"><a href="#9-Efficient-Reward-Identification-In-Max-Entropy-Reinforcement-Learning-with-Sparsity-and-Rank-Priors" class="headerlink" title="9. Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Efficient_Reward_Identification_In_Max_Entropy_Reinforcement_Learning_with_Sparsity_and_Rank_Priors.pdf">Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Michigan</span></p>
<p>本文提出了在最大熵强化学习中高效识别时变奖励函数的优化方法，包括基于奖励切换稀疏性和特征低秩假设的算法。通过贪心区间划分和核范数优化，有效恢复奖励结构，在多种网格世界实验中显示出鲁棒性和较优的迁移能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-Pentest-R1-Towards-Autonomous-Penetration-Testing-Reasoning-Optimized-via-Two-Stage-Reinforcement-Learning"><a href="#10-Pentest-R1-Towards-Autonomous-Penetration-Testing-Reasoning-Optimized-via-Two-Stage-Reinforcement-Learning" class="headerlink" title="10. Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pentest-R1__Towards_Autonomous_Penetration_Testing_Reasoning_Optimized_via_Two-Stage_Reinforcement_L.pdf">Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Laboratory of Cyberspace Security Defense, Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>本文提出了Pentest-R1框架，通过两阶段强化学习提升大语言模型在自动渗透测试中的推理与自适应能力。方法首先在包含500多条真实多步专家渗透测试演练的数据集上进行离线强化学习，奠定基础攻击逻辑，再在交互式CTF环境中通过在线强化学习优化策略与错误自纠能力。实验表明，Pentest-R1在Cybench和AutoPenBench基准上超越了大部分同类开源及专有模型，验证了两阶段结合对提升自动渗透测试表现的关键作用。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-Policy-Newton-methods-for-Distortion-Riskmetrics"><a href="#11-Policy-Newton-methods-for-Distortion-Riskmetrics" class="headerlink" title="11. Policy Newton methods for Distortion Riskmetrics"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Policy_Newton_methods_for_Distortion_Riskmetrics.pdf">Policy Newton methods for Distortion Riskmetrics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Indian Institute of Technology Madras</span></p>
<p>本文提出了一种在有限时域Markov决策过程（MDP）中最大化畸变风险度量（Distortion Riskmetric, DRM）的策略Newton法。方法上，作者推导了DRM目标的策略Hessian定理，通过似然比法构建了基于样本轨迹的DRM梯度和Hessian估计器，并提出了带三次正则化的策略Newton算法（CRPN-DRM），理论上证明该算法以O(ϵ^-3.5)的样本复杂度收敛到ϵ-二阶稳定点，能有效规避鞍点。实验在Cliff Walk、Cart Pole及Humanoid等环境下表明，该算法在风险敏感目标下优于风险中性基线策略，获得更高期望回报。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-Pref-GUIDE-Continual-Policy-Learning-from-Real-Time-Human-Feedback-via-Preference-Based-Learning"><a href="#12-Pref-GUIDE-Continual-Policy-Learning-from-Real-Time-Human-Feedback-via-Preference-Based-Learning" class="headerlink" title="12. Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pref-GUIDE__Continual_Policy_Learning_from_Real-Time_Human_Feedback_via_Preference-Based_Learning.pdf">Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Duke University</span></p>
<p>本文提出Pref-GUIDE框架，通过将实时人类标注的噪声标量反馈转化为局部对比偏好数据，实现更稳定和高效的奖励模型训练，提升基于人类反馈的持续策略学习。Pref-GUIDE由两个关键模块组成：Pref-GUIDE Individual在局部时间窗口内将标量反馈转为偏好对，Pref-GUIDE Voting则在多评估者间聚合奖励模型以达成群体共识。实验表明，该方法在三个复杂视觉RL任务中优于基于标量回归的主流方法，并在部分任务上超过专家设计的稠密奖励，验证了其在高噪声、多人反馈下的稳健性和可扩展性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-Reinforcement-Learning-in-Vision-A-Survey"><a href="#13-Reinforcement-Learning-in-Vision-A-Survey" class="headerlink" title="13. Reinforcement Learning in Vision: A Survey"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Reinforcement_Learning_in_Vision__A_Survey.pdf">Reinforcement Learning in Vision: A Survey</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Singapore</span></p>
<p>本论文系统综述了视觉领域中强化学习（Visual RL）的最新进展，涵盖多模态大语言模型、视觉生成、统一模型和视觉-语言-动作（VLA）模型四大方向，梳理了从RLHF、PPO到GRPO等主流策略，并提出了以奖励粒度和监督方式为基础的视觉RL方法学分类框架。结论指出，视觉RL正推动感知、推理与生成任务的统一发展，但在样本效率、泛化能力、奖励设计等方面仍面临重大挑战。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning"><a href="#14-Part-I-Tricks-or-Traps-A-Deep-Dive-into-RL-for-LLM-Reasoning" class="headerlink" title="14. Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Part_I__Tricks_or_Traps__A_Deep_Dive_into_RL_for_LLM_Reasoning.pdf">Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Alibaba Group</span></p>
<p>本文系统性评测了当前大语言模型（LLM）推理中的主流强化学习（RL）技术，包括归一化、裁剪、过滤及损失聚合等关键技巧，并在统一开源平台下通过不同模型规模与数据难度的精细实验，揭示各技术的内在机制及适用场景。结论指出，多数RL技术对实验设置高度敏感，并提出只用分组均值+批次标准差归一化和token级损失聚合两项简单技术（Lite PPO）即可显著提升无评价器策略的学习能力，优于众多繁杂算法，为RL4LLM实践提供了清晰实用的选择指南。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-ReconDreamer-RL-Enhancing-Reinforcement-Learning-via-Diffusion-based-Scene-Reconstruction"><a href="#15-ReconDreamer-RL-Enhancing-Reinforcement-Learning-via-Diffusion-based-Scene-Reconstruction" class="headerlink" title="15. ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ReconDreamer-RL__Enhancing_Reinforcement_Learning_via_Diffusion-based_Scene_Reconstruction.pdf">ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">GigaAI</span></p>
<p>该论文提出ReconDreamer-RL框架，将视频扩散先验与三维高斯泼溅（3DGS）场景重建和动力学建模相结合，构建更真实的模拟环境以提升端到端自动驾驶中的强化学习。方法包括ReconSimulator实现高保真感知与物理建模，Dynamic Adversary Agent（DAA）自动生成复杂角落案例，Cousin Trajectory Generator（CTG）扩展训练数据多样性。实验表明，该方法比模仿学习方法将碰撞率降低5倍，显著提升了模型在极端场景下的安全性和泛化能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-Pose-RFT-Enhancing-MLLMs-for-3D-Pose-Generation-via-Hybrid-Action-Reinforcement-Fine-Tuning"><a href="#16-Pose-RFT-Enhancing-MLLMs-for-3D-Pose-Generation-via-Hybrid-Action-Reinforcement-Fine-Tuning" class="headerlink" title="16. Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pose-RFT__Enhancing_MLLMs_for_3D_Pose_Generation_via_Hybrid_Action_Reinforcement_Fine-Tuning.pdf">Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CASIA</span></p>
<p>该论文提出了Pose-RFT，一种专为3D人体姿态生成任务设计的强化微调框架，针对多模态大语言模型（MLLMs）。方法将3D姿态生成建模为离散（文本）与连续（姿态参数）混合动作空间下的强化学习问题，并引入HyGRPO算法，实现对文本和3D姿态的联合优化。Pose-RFT结合空间和语义对齐等任务特定奖励函数，有效提升了模型在图像到姿态、文本到姿态等任务上的表现。实验结果显示，该方法在多个基准上显著优于现有的姿态生成MLLMs，证实了混合动作强化微调在3D姿态生成中的有效性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-A-Tutorial-An-Intuitive-Explanation-of-Offline-Reinforcement-Learning-Theory"><a href="#17-A-Tutorial-An-Intuitive-Explanation-of-Offline-Reinforcement-Learning-Theory" class="headerlink" title="17. A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Tutorial__An_Intuitive_Explanation_of_Offline_Reinforcement_Learning_Theory.pdf">A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Alberta</span></p>
<p>本论文系统梳理了离线强化学习（Offline RL）的理论基础，重点分析了函数表示和数据覆盖的假设条件，并通过反例揭示了离线RL的固有难度，强调样本复杂度与覆盖条件的关系。结论指出，只有在满足特定的覆盖与表示假设、采用如悲观估计和λ-return等技术，才能保证多项式样本高效学习，当前理论为实际算法设计提供了清晰的边界和启示。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-Symmetry-Aware-Transformer-Training-for-Automated-Planning"><a href="#18-Symmetry-Aware-Transformer-Training-for-Automated-Planning" class="headerlink" title="18. Symmetry-Aware Transformer Training for Automated Planning"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Symmetry-Aware_Transformer_Training_for_Automated_Planning.pdf">Symmetry-Aware Transformer Training for Automated Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Linköping University</span></p>
<p>本文提出了一种面向自动规划的对称性感知Transformer训练方法，通过引入基于对称性的对比学习目标，使模型能够有效忽略输入中的对象命名和原子顺序等冗余对称性，并采用无位置编码的编码器-解码器架构处理规划任务。实验表明，该方法在多个规划领域相比PlanGPT显著提升了模型的泛化与外推能力，特别是在未见过的大规模问题上有较好表现，但在极大规模或复杂域仍存在一定局限。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-Robust-Reinforcement-Learning-over-Wireless-Networks-with-Homomorphic-State-Representations"><a href="#19-Robust-Reinforcement-Learning-over-Wireless-Networks-with-Homomorphic-State-Representations" class="headerlink" title="19. Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Robust_Reinforcement_Learning_over_Wireless_Networks_with_Homomorphic_State_Representations.pdf">Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Padova</span></p>
<p>该论文提出了一种名为Homomorphic Robust Remote Reinforcement Learning (HR3L)的新架构，用于在非理想无线网络中高效训练远程RL智能体。HR3L通过马尔可夫同态理论对环境状态进行压缩表征，采用推送式通信和异步模型更新，显著降低通信和计算负担，同时提升对信道丢包、延迟和带宽受限等场景的鲁棒性。实验证明HR3L在DeepMind Control Suite多个控制任务和多种信道条件下，样本效率、带宽占用和鲁棒性均优于现有方法，尤其在高维观测和有限资源下表现突出。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-MORE-CLEAR-Multimodal-Offline-Reinforcement-learning-for-Clinical-notes-Leveraged-Enhanced-State-Representation"><a href="#20-MORE-CLEAR-Multimodal-Offline-Reinforcement-learning-for-Clinical-notes-Leveraged-Enhanced-State-Representation" class="headerlink" title="20. MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MORE-CLEAR__Multimodal_Offline_Reinforcement_learning_for_Clinical_notes_Leveraged_Enhanced_State_Re.pdf">MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Seoul National University Hospital</span></p>
<p>该论文提出MORE-CLEAR框架，通过结合结构化数据（如实验室结果、生命体征）与非结构化临床文本（通过大语言模型编码与摘要），利用上下文门控融合与双向跨模态注意力机制，提升ICU脓毒症治疗中的患者状态表征，从而支持更鲁棒的离线强化学习策略优化。实验在MIMIC-III、MIMIC-IV和真实医院数据集上表明，多模态方法在存活率估计和策略表现上均显著优于单模态基线，并且该框架易于迁移至其它医学决策场景。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="21-DexFruit-Dexterous-Manipulation-and-Gaussian-Splatting-Inspection-of-Fruit"><a href="#21-DexFruit-Dexterous-Manipulation-and-Gaussian-Splatting-Inspection-of-Fruit" class="headerlink" title="21. DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DexFruit__Dexterous_Manipulation_and_Gaussian_Splatting_Inspection_of_Fruit.pdf">DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>该论文提出DexFruit框架，将光学触觉传感与扩散式模仿学习策略结合，实现对草莓、番茄和黑莓等脆弱水果的温和自主操作，有效减少损伤。其创新的FruitSplat方法基于3D Gaussian Splatting，能高精度定量分析水果表面损伤，实现高分辨率三维可视化。实验结果显示DexFruit在多种水果上抓取成功率达92%，外部损伤降低20%，显著优于视觉或触觉单一方法。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-Communication-Efficient-Multi-Agent-3D-Detection-via-Hybrid-Collaboration"><a href="#22-Communication-Efficient-Multi-Agent-3D-Detection-via-Hybrid-Collaboration" class="headerlink" title="22. Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Communication-Efficient_Multi-Agent_3D_Detection_via_Hybrid_Collaboration.pdf">Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了HyComm，一种基于混合协作的高效通信多智能体LiDAR 3D目标检测系统。方法通过不确定性引导下自适应融合紧凑的感知输出与丰富的原始观测数据，并对每类消息优先选择最重要的数据，实现了在不同通信带宽下的最优感知信息交换。实验结果显示HyComm在DAIR-V2X与OPV2V数据集上，无论模型同构或异构，都显著优于现有方法，实现了更低通信量下更高检测性能。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-From-Data-to-Safe-Mobile-Robot-Navigation-An-Efficient-and-Modular-Robust-MPC-Design-Pipeline"><a href="#23-From-Data-to-Safe-Mobile-Robot-Navigation-An-Efficient-and-Modular-Robust-MPC-Design-Pipeline" class="headerlink" title="23. From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Data_to_Safe_Mobile_Robot_Navigation__An_Efficient_and_Modular_Robust_MPC_Design_Pipeline.pdf">From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Delft University of Technology</span></p>
<p>本文提出了一套高效且模块化的鲁棒模型预测控制（MPC）设计流程，针对移动机器人导航中的安全性问题，系统性地从实验数据出发，通过迭代式移动地平线估计算法定量不确定性，并结合输出反馈鲁棒MPC方案实现碰撞规避和约束可行性。实验证明，该流程在存在扰动和测量噪声的情况下，能够在无人机仿真环境中保证鲁棒约束满足和递归可行性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-ForeSight-Multi-View-Streaming-Joint-Object-Detection-and-Trajectory-Forecasting"><a href="#24-ForeSight-Multi-View-Streaming-Joint-Object-Detection-and-Trajectory-Forecasting" class="headerlink" title="24. ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ForeSight__Multi-View_Streaming_Joint_Object_Detection_and_Trajectory_Forecasting.pdf">ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Toronto</span></p>
<p>本文提出了ForeSight，一种面向自动驾驶场景的多视角流式联合检测与轨迹预测框架。方法创新性地通过统一的Transformer架构和双向查询记忆，打破传统检测和预测串行处理的壁垒，使检测与预测任务在时序上实现信息共享和闭环反馈，支持无跟踪关联的端到端推理。实验证明，ForeSight在nuScenes数据集上实现了检测和预测的双SOTA性能，mAP与EPA指标均优于现有方法，显著提升了复杂场景下感知的准确性和鲁棒性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-Model-Predictive-Control-for-Crowd-Navigation-via-Learning-Based-Trajectory-Prediction"><a href="#25-Model-Predictive-Control-for-Crowd-Navigation-via-Learning-Based-Trajectory-Prediction" class="headerlink" title="25. Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Model_Predictive_Control_for_Crowd_Navigation_via_Learning-Based_Trajectory_Prediction.pdf">Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bauhaus-Universitat Weimar</span></p>
<p>该论文提出将深度学习驱动的Social-Implicit(SI)行人轨迹预测模型集成进模型预测控制(MPC)框架，并在实体机器人上进行了验证。实验显示，在不同人群密度下，SI-MPC系统显著提升了预测准确性和机器人导航的安全性与平滑性，尤其在低密度场景下预测误差减少高达76%。结论表明，学习型预测器在真实动态环境中能提升安全和社交适应性，但实际部署需关注系统级评估与效率-保守性权衡。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-P3-Toward-Versatile-Embodied-Agents"><a href="#26-P3-Toward-Versatile-Embodied-Agents" class="headerlink" title="26. P3: Toward Versatile Embodied Agents"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/$_mathcal%7BP%7D%5E3$__Toward_Versatile_Embodied_Agents.pdf">P3: Toward Versatile Embodied Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southern University of Science and Technology</span></p>
<p>本文提出了P3框架，通过引入统一的感知模块主动感知环境变化、消除对工具反馈的依赖，实现任意工具即插即用，并结合动态多任务调度器，实现任务的智能规划、优先级排序与执行。大量真实环境实验表明，P3极大提升了机器人对复杂动态环境的适应性和通用性，缩小了基准与实际部署的差距。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-Neural-Channel-Knowledge-Map-Assisted-Scheduling-Optimization-of-Active-IRSs-in-Multi-User-Systems"><a href="#27-Neural-Channel-Knowledge-Map-Assisted-Scheduling-Optimization-of-Active-IRSs-in-Multi-User-Systems" class="headerlink" title="27. Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Neural_Channel_Knowledge_Map_Assisted_Scheduling_Optimization_of_Active_IRSs_in_Multi-User_Systems.pdf">Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiamen University</span></p>
<p>本论文提出一种基于神经信道知识图（CKM）的多用户主动智能反射表面（AIRS）调度优化框架，通过设计Transformer深度神经网络（LPS-Net和SE-Net）实现历史信道与吞吐量数据的高效预测，并以此辅助时频资源与AIRS分配。实验结果显示，该方法显著提升了预测精度和计算效率，SM-IB算法在复杂性较低的情况下达到了近似最优的最大最小吞吐量。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-EGS-SLAM-RGB-D-Gaussian-Splatting-SLAM-with-Events"><a href="#28-EGS-SLAM-RGB-D-Gaussian-Splatting-SLAM-with-Events" class="headerlink" title="28. EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EGS-SLAM__RGB-D_Gaussian_Splatting_SLAM_with_Events.pdf">EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>本文提出了EGS-SLAM，一种融合事件相机数据、RGB图像和深度信息的3D Gaussian Splatting SLAM系统，显式建模曝光期间的连续相机轨迹，并引入可学习的相机响应函数（CRF）与no-event loss，实现事件-图像-深度的联合跟踪和高保真三维重建。实验表明，该方法在有严重运动模糊时，相较于现有GS-SLAM方案在定位和重建质量上均有显著提升。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-Манипулятор-для-помощи-людям-с-ограниченными-возможностями"><a href="#29-Манипулятор-для-помощи-людям-с-ограниченными-возможностями" class="headerlink" title="29. Манипулятор для помощи людям с ограниченными возможностями"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Manipulator_for_people_with_limited_abilities.pdf">Манипулятор для помощи людям с ограниченными возможностями</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Московский государственный технический университет имени Н.Э. Баумана</span></p>
<p>本论文提出了一种面向行动不便人士的机器人辅助喂食系统RoboBK，包括机械结构设计、动力与能耗分析、四自由度运动学建模、基于视觉的目标检测、ROS平台下的控制与运动规划、以及软硬件一体化实现。通过集成3D建模、动力学仿真、闭环控制和基于摄像头的人脸（鼻部）识别，最终完成了低成本原型开发与多模式实际测试，验证了系统的有效性和实用性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-From-Imitation-to-Optimization-A-Comparative-Study-of-Offline-Learning-for-Autonomous-Driving"><a href="#30-From-Imitation-to-Optimization-A-Comparative-Study-of-Offline-Learning-for-Autonomous-Driving" class="headerlink" title="30. From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Imitation_to_Optimization__A_Comparative_Study_of_Offline_Learning_for_Autonomous_Driving.pdf">From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent Researcher</span></p>
<p>本文提出了一个完整的自动驾驶离线学习流程，系统比较了行为克隆（BC）和保守Q学习（CQL）在Waymo Open Motion Dataset上的表现。通过构建结构化状态表示和Transformer架构，作者发现BC方法在长时序任务中易堆积误差，而采用CQL的离线强化学习能够显著提升策略的鲁棒性和成功率。结论表明，离线强化学习方法对于从静态专家数据中学习高鲁棒性的自动驾驶策略至关重要。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-Imaginative-World-Modeling-with-Scene-Graphs-for-Embodied-Agent-Navigation"><a href="#31-Imaginative-World-Modeling-with-Scene-Graphs-for-Embodied-Agent-Navigation" class="headerlink" title="31. Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Imaginative_World_Modeling_with_Scene_Graphs_for_Embodied_Agent_Navigation.pdf">Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Michigan, Ann Arbor</span></p>
<p>本文提出SGImagineNav，一个基于符号世界建模的主动导航框架，通过构建分层场景图并利用大语言模型对未知区域进行语义推理，实现环境的全局结构化表示。系统在仿真和真实场景下均显著提升了目标定位效率，跨楼层和跨房间导航成功率分别达65.4%和66.8%，展示了其泛化性和实用价值。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-PANAMA-A-Network-Aware-MARL-Framework-for-Multi-Agent-Path-Finding-in-Digital-Twin-Ecosystems"><a href="#32-PANAMA-A-Network-Aware-MARL-Framework-for-Multi-Agent-Path-Finding-in-Digital-Twin-Ecosystems" class="headerlink" title="32. PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PANAMA__A_Network-Aware_MARL_Framework_for_Multi-Agent_Path_Finding_in_Digital_Twin_Ecosystems.pdf">PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huawei Canada Advanced Research Center</span></p>
<p>本文提出了PANAMA算法，一种基于优先级非对称性的网络感知多智能体强化学习（MARL）框架，用于数字孪生生态系统中的多智能体路径规划。该方法结合Centralized Training with Decentralized Execution（CTDE）、异步actor-learner结构和优先经验回放，并引入动态优先级和网络信号质量感知，极大提升了多智能体在复杂环境下的协作与路径效率。实验结果显示，PANAMA在准确率、速度和可扩展性方面优于现有基线，尤其在拥挤和通信受限场景下表现突出，证明了网络感知对于多智能体系统高效协作和鲁棒性的关键作用。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-Energy-Efficient-Task-Offloading-in-UAV-Enabled-MEC-Using-a-Fully-Decentralized-Deep-Reinforcement-Learning-Approach"><a href="#33-Energy-Efficient-Task-Offloading-in-UAV-Enabled-MEC-Using-a-Fully-Decentralized-Deep-Reinforcement-Learning-Approach" class="headerlink" title="33. Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Energy_Efficient_Task_Offloading_in_UAV-Enabled_MEC_Using_a_Fully_Decentralized_Deep_Reinforcement_L.pdf">Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Iran University of Science and Technology</span></p>
<p>该论文提出了一种用于无人机（UAV）支持的多接入边缘计算（MEC）系统的全分布式深度强化学习方法，通过结合图注意力网络（GAT）与经验和参数共享的PPO（EPS-PPO），实现无人机任务卸载和轨迹优化，各无人机仅与邻居通信，无需中心节点。实验表明，该方法在能耗、任务处理数、收敛速度和鲁棒性等方面均优于半中心化的MADDPG方法，适用于动态MEC环境。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-D3P-Dynamic-Denoising-Diffusion-Policy-via-Reinforcement-Learning"><a href="#34-D3P-Dynamic-Denoising-Diffusion-Policy-via-Reinforcement-Learning" class="headerlink" title="34. D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/D3P__Dynamic_Denoising_Diffusion_Policy_via_Reinforcement_Learning.pdf">D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出了一种名为Dynamic Denoising Diffusion Policy (D3P)的方法，在机器人视觉操作任务中，根据动作关键性动态分配扩散去噪步骤。D3P包含基础扩散策略和轻量级适配器，并通过强化学习联合优化，三阶段训练策略保证收敛稳定。实验结果显示，D3P在八项仿真任务和实际机器人部署中，推理速度较传统方法平均加速2.2倍且任务成功率无显著下降，验证了自适应推理的效率和实用性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-Learning-a-Vision-Based-Footstep-Planner-for-Hierarchical-Walking-Control"><a href="#35-Learning-a-Vision-Based-Footstep-Planner-for-Hierarchical-Walking-Control" class="headerlink" title="35. Learning a Vision-Based Footstep Planner for Hierarchical Walking Control"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_a_Vision-Based_Footstep_Planner_for_Hierarchical_Walking_Control.pdf">Learning a Vision-Based Footstep Planner for Hierarchical Walking Control</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Pennsylvania</span></p>
<p>本文提出了一种基于视觉的分层控制框架，用于双足机器人步态规划。方法通过单目深度摄像头和局部高程图，结合强化学习训练的高层步态规划器，以及低层操作空间控制器，实现了在复杂地形上的实时步态决策和轨迹跟踪，并采用ALIP模型简化状态空间。结论显示该框架在仿真和硬件实验中均表现出优异的地形适应性与鲁棒性，但在复杂地形和实际部署时仍受限于模型表达能力和分层结构的迁移难题。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-Triple-S-A-Collaborative-Multi-LLM-Framework-for-Solving-Long-Horizon-Implicative-Tasks-in-Robotics"><a href="#36-Triple-S-A-Collaborative-Multi-LLM-Framework-for-Solving-Long-Horizon-Implicative-Tasks-in-Robotics" class="headerlink" title="36. Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Triple-S__A_Collaborative_Multi-LLM_Framework_for_Solving_Long-Horizon_Implicative_Tasks_in_Robotics.pdf">Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northeastern University</span></p>
<p>本文提出Triple-S框架，通过多大语言模型（LLM）协作，包括任务简化、示例检索、闭环决策和API封装，有效提升机器人在长时序复杂任务中的代码生成成功率和鲁棒性。实验在LDIP数据集和真实机器人中验证了Triple-S方法的有效性，任务成功率显著高于现有方法，展示了在指令和环境隐含条件下的强泛化能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-Multimodal-Spiking-Neural-Network-for-Space-Robotic-Manipulation"><a href="#37-Multimodal-Spiking-Neural-Network-for-Space-Robotic-Manipulation" class="headerlink" title="37. Multimodal Spiking Neural Network for Space Robotic Manipulation"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multimodal_Spiking_Neural_Network_for_Space_Robotic_Manipulation.pdf">Multimodal Spiking Neural Network for Space Robotic Manipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology</span></p>
<p>本文提出了一种基于多模态脉冲神经网络（SNN）的空间机器人臂控制框架，结合几何状态、触觉、语义信息提升环境感知，并以双通道三阶段课程强化学习（CRL）分阶段引导策略训练，实现能效高、自主操作的空间材料搬运与操控。实验表明，该方法在任务成功率与能耗方面均优于传统人工神经网络（ANN）基线，适用于资源受限的真实航天场景。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-MonoMPC-Monocular-Vision-Based-Navigation-with-Learned-Collision-Model-and-Risk-Aware-Model-Predictive-Control"><a href="#38-MonoMPC-Monocular-Vision-Based-Navigation-with-Learned-Collision-Model-and-Risk-Aware-Model-Predictive-Control" class="headerlink" title="38. MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MonoMPC__Monocular_Vision_Based_Navigation_with_Learned_Collision_Model_and_Risk-Aware_Model_Predict.pdf">MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Tartu</span></p>
<p>该论文提出了一种基于单目视觉的机器人导航方法，通过将噪声较大的估计深度作为输入，联合学习概率化碰撞模型和风险度量，并将其集成到风险感知MPC规划器中。方法利用PointNet++提取点云特征，联合控制序列，通过神经网络输出轨迹最小障碍物间隙分布，联合下游风险监督优化模型不确定性，实现安全高效导航。实验表明，该方法在复杂环境下成功率显著高于ROS导航和NoMaD，并能有效规避碰撞。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-A-Hybrid-Force-Position-Strategy-for-Shape-Control-of-Deformable-Linear-Objects-With-Graph-Attention-Networks"><a href="#39-A-Hybrid-Force-Position-Strategy-for-Shape-Control-of-Deformable-Linear-Objects-With-Graph-Attention-Networks" class="headerlink" title="39. A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Hybrid_Force-Position_Strategy_for_Shape_Control_of_Deformable_Linear_Objects_With_Graph_Attention.pdf">A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了一种面向可变形线状物体（DLO）形状控制的混合力-位控制策略，融合了基于图注意力网络（GAT）的动力学建模及力-位双模态状态表示。该方法通过力空间中的轨迹规划与位置空间中的模型预测控制（MPC）相结合，有效分解大变形任务，避免陷入局部最优。实验证明，该策略在仿真和现实场景下均能显著提升DLO操控的效率与稳定性，确保形状过渡平滑且鲁棒。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-Integrating-Neurosymbolic-AI-in-Advanced-Air-Mobility-A-Comprehensive-Survey"><a href="#40-Integrating-Neurosymbolic-AI-in-Advanced-Air-Mobility-A-Comprehensive-Survey" class="headerlink" title="40. Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Integrating_Neurosymbolic_AI_in_Advanced_Air_Mobility__A_Comprehensive_Survey.pdf">Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland, Baltimore County</span></p>
<p>该论文系统综述了Neurosymbolic AI（结合神经网络学习和符号推理）在先进空中移动（AAM）中的应用，包括需求预测、飞机设计、自动化管控、实时交通管理等关键领域，重点分析了Neurosymbolic Reinforcement Learning等方法在动态优化中的潜力和面临的可扩展性、稳健性及合规性挑战。论文结论认为，Neurosymbolic AI能显著提升AAM系统的透明性、安全性和适应性，但需持续推动跨学科合作与标准化，以解决数据融合、网络安全和认证等核心问题，助力下一代航空移动解决方案的落地。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-ODYSSEY-Open-World-Quadrupeds-Exploration-and-Manipulation-for-Long-Horizon-Tasks"><a href="#41-ODYSSEY-Open-World-Quadrupeds-Exploration-and-Manipulation-for-Long-Horizon-Tasks" class="headerlink" title="41. ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ODYSSEY__Open-World_Quadrupeds_Exploration_and_Manipulation_for_Long-Horizon_Tasks.pdf">ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>ODYSSEY提出了一个统一的移动操控框架，将层次化的视觉-语言任务规划与地形自适应全身控制结合，专为具备机械臂的敏捷四足机器人设计。系统通过视觉-语言模型驱动的分层规划和强化学习训练的全身控制策略，实现了在多样复杂地形下的长期任务分解与精确动作执行，并通过丰富室内外场景的仿真与实地测试验证了其通用性和鲁棒性。结论表明该方法能显著提升移动机器人在开放环境中的探索与操作能力，为通用型机器人助手的实际部署奠定基础。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-BeyondMimic-From-Motion-Tracking-to-Versatile-Humanoid-Control-via-Guided-Diffusion"><a href="#42-BeyondMimic-From-Motion-Tracking-to-Versatile-Humanoid-Control-via-Guided-Diffusion" class="headerlink" title="42. BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BeyondMimic__From_Motion_Tracking_to_Versatile_Humanoid_Control_via_Guided_Diffusion.pdf">BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Berkeley</span></p>
<p>本文提出了BeyondMimic框架，实现了从人类动作捕捉数据到多样化仿人机器人控制的端到端流程。方法上，首先通过统一的MDP和超参数训练高质量、高动态的运动跟踪策略，并利用离线知识蒸馏和引导扩散模型，将多种运动技能合成为单一策略，实现零样本下任务自适应控制，支持导航、遥控和避障等多任务。实验表明，该方法在真实硬件上展现出强健性、泛化性和人类动作风格，首次实现了极具挑战性的连续动态动作的仿人机器人真实执行。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-Spatial-ORMLLM-Improve-Spatial-Relation-Understanding-in-the-Operating-Room-with-Multimodal-Large-Language-Model"><a href="#43-Spatial-ORMLLM-Improve-Spatial-Relation-Understanding-in-the-Operating-Room-with-Multimodal-Large-Language-Model" class="headerlink" title="43. Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Spatial-ORMLLM__Improve_Spatial_Relation_Understanding_in_the_Operating_Room_with_Multimodal_Large_L.pdf">Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hunan University</span></p>
<p>Spatial-ORMLLM提出了一种用于手术室场景下3D空间关系理解的多模态大语言模型，仅依靠RGB图像，通过3D Spatial Block生成深度图、全景分割和点云等伪模态，利用空间增强特征融合块将多模态特征统一投影至LLM的token空间，实现细粒度空间推理和场景图生成。实验显示，该方法在空间推理和场景图任务上超越现有2D和3D视觉-语言模型，验证了在模态受限条件下可实现高精度空间理解和结构化场景建模。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-Vision-Based-Localization-and-LLM-based-Navigation-for-Indoor-Environments"><a href="#44-Vision-Based-Localization-and-LLM-based-Navigation-for-Indoor-Environments" class="headerlink" title="44. Vision-Based Localization and LLM-based Navigation for Indoor Environments"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Vision-Based_Localization_and_LLM-based_Navigation_for_Indoor_Environments.pdf">Vision-Based Localization and LLM-based Navigation for Indoor Environments</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Brown University</span></p>
<p>本论文提出了一种融合视觉定位与大语言模型（LLM）导航的室内导航系统：通过两阶段微调的ResNet-50卷积神经网络对智能手机摄像头图像进行定位，随后结合预处理楼层地图和ChatGPT大语言模型，生成逐步导航指令。实验结果表明，在复杂走廊环境下，视觉定位模块具备96%准确率，LLM导航模块平均指令准确率为75%，表明该方法有望实现资源有限环境下的可扩展、无基础设施室内导航，但导航模块仍受限于空间推理能力和响应速度。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-COM-PACT-COMponent-Aware-Pruning-for-Accelerated-Control-Tasks-in-Latent-Space-Models"><a href="#45-COM-PACT-COMponent-Aware-Pruning-for-Accelerated-Control-Tasks-in-Latent-Space-Models" class="headerlink" title="45. COM-PACT: COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/COMponent-Aware_Pruning_for_Accelerated_Control_Tasks_in_Latent_Space_Models.pdf">COM-PACT: COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">RPTU University Kaiserslautern-Landau</span></p>
<p>本文提出了一种基于组件感知的结构化剪枝方法，通过优化各组件的剪枝比例，实现神经网络控制器（NNC）在嵌入式硬件上的高效部署，并以Lyapunov稳定性为约束，确保压缩后模型仍能满足控制系统的稳定性需求。实验表明，该方法在TD-MPC控制任务中能显著减少模型规模，维持关键稳定性界限，并揭示不同组件对系统稳定性的敏感性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-AimBot-A-Simple-Auxiliary-Visual-Cue-to-Enhance-Spatial-Awareness-of-Visuomotor-Policies"><a href="#46-AimBot-A-Simple-Auxiliary-Visual-Cue-to-Enhance-Spatial-Awareness-of-Visuomotor-Policies" class="headerlink" title="46. AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AimBot__A_Simple_Auxiliary_Visual_Cue_to_Enhance_Spatial_Awareness_of_Visuomotor_Policies.pdf">AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Michigan</span></p>
<p>该论文提出了AimBot，一种轻量级视觉增强方法，通过在多视角RGB图像上叠加射线和准星，显式编码机械臂末端执行器的空间状态，增强视觉运动策略在机器人操作中的空间感知。AimBot无需更改模型架构，几乎不增加计算开销，实验表明其在模拟与真实环境中均能显著提升多种视觉语言动作模型的任务成功率，尤其在复杂长时序操作任务中效果突出。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-Grid2Guide-A-Enabled-Small-Language-Model-for-Indoor-Navigation"><a href="#47-Grid2Guide-A-Enabled-Small-Language-Model-for-Indoor-Navigation" class="headerlink" title="47. Grid2Guide: A* Enabled Small Language Model for Indoor Navigation"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Grid2Guide__A__Enabled_Small_Language_Model_for_Indoor_Navigation.pdf">Grid2Guide: A* Enabled Small Language Model for Indoor Navigation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Alabama</span></p>
<p>该论文提出Grid2Guide，一个结合A<em>搜索算法与小型语言模型（SLM）的室内导航方法。流程包括将楼层平面图转化为二值占用网格，通过A</em>算法规划最优路径，并用SLM将压缩后的路径指令转化为自然语言导航指令。实验表明，该方法在多种实际场景下实现了高精度、低延迟的实时导航，优于纯LLM方案，适合手持设备部署。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-PCHands-PCA-based-Hand-Pose-Synergy-Representation-on-Manipulators-with-N-DoF"><a href="#48-PCHands-PCA-based-Hand-Pose-Synergy-Representation-on-Manipulators-with-N-DoF" class="headerlink" title="48. PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PCHands__PCA-based_Hand_Pose_Synergy_Representation_on_Manipulators_with_N-DoF.pdf">PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Istituto Italiano di Tecnologia (IIT)</span></p>
<p>本文提出了PCHands框架，通过Anchor Description Format (ADF)、条件变分自编码器(CVAE)和主成分分析(PCA)结合，统一提取不同机械手（包括人手、类人手、二指夹爪等）姿态协同的可变长度表征。PCHands不仅提升了基于RL的灵巧操作任务的学习效率和一致性，还能在不同机械手之间实现高效的操作策略和演示迁移，并在真实机器人上实现了零样本迁移，验证了其实用性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space"><a href="#49-MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space" class="headerlink" title="49. MolmoAct: Action Reasoning Models that can Reason in Space"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MolmoAct__Action_Reasoning_Models_that_can_Reason_in_Space.pdf">MolmoAct: Action Reasoning Models that can Reason in Space</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Allen Institute for AI</span></p>
<p>MolmoAct提出了一种结构化三阶段推理流程的开放式行动推理模型（ARM），集成了感知、规划和控制：首先将图像和语言指令编码为深度感知Token，然后生成可编辑的空间轨迹表示，最后预测精确的机器人底层动作，提升了模型的可解释性和可控性。实验表明，MolmoAct在SimperEnv、LIBERO等仿真及真实环境下超越主流基线，具备优异的零样本泛化、快速适应和空间推理能力，并通过发布全套模型权重与数据集推动社区发展。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-DETACH-Cross-domain-Learning-for-Long-Horizon-Tasks-via-Mixture-of-Disentangled-Experts"><a href="#50-DETACH-Cross-domain-Learning-for-Long-Horizon-Tasks-via-Mixture-of-Disentangled-Experts" class="headerlink" title="50. DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DETACH__Cross-domain_Learning_for_Long-Horizon_Tasks_via_Mixture_of_Disentangled_Experts.pdf">DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing University of Technology</span></p>
<p>该论文提出DETACH框架，通过受生物启发的双流模块（环境编码器和自我编码器）实现环境感知与自我状态的功能解耦，结合多策略自适应特征融合机制，显著提升跨域泛化和技能复用能力。实验在多个人-场景交互长时序任务中验证，DETACH在任务成功率和执行效率上均超过现有方法，特别是在跨环境和多技能组合场景下表现优异。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-Touch-Speaks-Sound-Feels-A-Multimodal-Approach-to-Affective-and-Social-Touch-from-Robots-to-Humans"><a href="#51-Touch-Speaks-Sound-Feels-A-Multimodal-Approach-to-Affective-and-Social-Touch-from-Robots-to-Humans" class="headerlink" title="51. Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Touch_Speaks,_Sound_Feels__A_Multimodal_Approach_to_Affective_and_Social_Touch_from_Robots_to_Humans.pdf">Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ghent University</span></p>
<p>本文提出了一种机器人对人类的多模态情感与社交触觉交互系统，结合25点阵震动马达和音频同步反馈，实现触觉与听觉信息的融合；通过实验让32名中国参与者解码由机器人传递的10种情感和6种社交手势，分别在单一触觉、单一听觉及两者结合下进行识别。结果显示，触觉与听觉结合方式显著提升了情感识别的准确性，单一通道各有优势但难以区分部分低唤醒或相近情感，手势识别总体优于情感识别。研究强调多感官融合对于提升人机情感交互的必要性，并为社交型机器人设计提供了依据。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-AgentWorld-An-Interactive-Simulation-Platform-for-Scene-Construction-and-Mobile-Robotic-Manipulation"><a href="#52-AgentWorld-An-Interactive-Simulation-Platform-for-Scene-Construction-and-Mobile-Robotic-Manipulation" class="headerlink" title="52. AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AgentWorld__An_Interactive_Simulation_Platform_for_Scene_Construction_and_Mobile_Robotic_Manipulatio.pdf">AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent Robotics X</span></p>
<p>该论文提出AgentWorld，一个集成程序化场景构建和移动机器人操作的数据收集仿真平台，包含高保真3D资产库、物理属性仿真、移动和双臂&#x2F;灵巧手远程控制系统。通过搭建多样家庭环境与收集多阶段操作轨迹，并在模仿学习算法（如行为克隆、ACT、扩散策略、视觉语言动作模型）上基准测试，实现了有效的仿真到现实迁移。结论表明AgentWorld能大幅提升复杂家居环境下机器人泛化与技能学习能力，为实际部署提供了坚实基础。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-NeeCo-Image-Synthesis-of-Novel-Instrument-States-Based-on-Dynamic-and-Deformable-3D-Gaussian-Reconstruction"><a href="#53-NeeCo-Image-Synthesis-of-Novel-Instrument-States-Based-on-Dynamic-and-Deformable-3D-Gaussian-Reconstruction" class="headerlink" title="53. NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/NeeCo__Image_Synthesis_of_Novel_Instrument_States_Based_on_Dynamic_and_Deformable_3D_Gaussian_Recons.pdf">NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Leeds</span></p>
<p>本文提出NeeCo框架，利用动态可变形3D高斯重建技术，通过学习手术器械运动学和高斯渲染，自动合成不同姿态和变形下的手术器械高质量图像及其标注，并引入动态训练调整策略以应对真实场景下的相机姿态误差。实验显示，NeeCo生成的数据集在提升手术器械检测与分割神经网络性能方面优于SOTA方法，且自动标注精度高，有效解决了医学图像数据稀缺与标注难题。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-Autonomous-Navigation-of-Cloud-Controlled-Quadcopters-in-Confined-Spaces-Using-Multi-Modal-Perception-and-LLM-Driven-High-Semantic-Reasoning"><a href="#54-Autonomous-Navigation-of-Cloud-Controlled-Quadcopters-in-Confined-Spaces-Using-Multi-Modal-Perception-and-LLM-Driven-High-Semantic-Reasoning" class="headerlink" title="54. Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Autonomous_Navigation_of_Cloud-Controlled_Quadcopters_in_Confined_Spaces_Using_Multi-Modal_Perceptio.pdf">Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Rajshahi University of Engineering and Technology</span></p>
<p>本文提出了一种用于GPS-denied室内环境自主无人机导航的多模态感知与云端大模型决策系统。方法包括YOLOv11目标检测、Depth Anything V2单目深度估计、定制PCB集成ToF和IMU传感器、多线程架构、VLM场景理解，以及云端微调LLM进行高语义导航决策。实验表明，该系统在42次试验中仅发生16次防护罩突破，端到端延迟低于1秒，导航命令准确率达68%，实现了鲁棒的室内自主飞行和安全避障。结论：集成多模态感知与大模型语义推理显著提升了室内无人机的智能自主导航能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-Being-M0-5-A-Real-Time-Controllable-Vision-Language-Motion-Model"><a href="#55-Being-M0-5-A-Real-Time-Controllable-Vision-Language-Motion-Model" class="headerlink" title="55. Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Being-M0.5__A_Real-Time_Controllable_Vision-Language-Motion_Model.pdf">Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CASIA</span></p>
<p>该论文提出了Being-M0.5，一个具备实时、可控性的人体运动生成的视觉-语言-动作模型（VLMM），采用大规模多模态数据集HuMo100M和创新的部位感知残差量化（PRQ）技术，实现对身体各部位的细粒度控制。实验表明，该模型在多种动作生成基准上均达到了最新最优性能，并具备实际部署的高效率和广泛适应性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-Learning-Satellite-Attitude-Dynamics-with-Physics-Informed-Normalising-Flow"><a href="#56-Learning-Satellite-Attitude-Dynamics-with-Physics-Informed-Normalising-Flow" class="headerlink" title="56. Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_Satellite_Attitude_Dynamics_with_Physics-Informed_Normalising_Flow.pdf">Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Politecnico di Torino</span></p>
<p>本论文提出了一种结合物理约束和数据驱动的Real NVP正则化流神经网络（结合自注意力机制），用于学习卫星姿态动力学，并通过引入物理信息损失函数（基于Lagrangian dual方法动态调节权重）提升模型泛化和鲁棒性。实验证明，该方法在状态预测和嵌入MPC控制框架中的表现均优于纯数据驱动模型，平均相对误差降低27.08%~90.22%，闭环控制鲁棒性和稳定性提升显著。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-SwarmVLM-VLM-Guided-Impedance-Control-for-Autonomous-Navigation-of-Heterogeneous-Robots-in-Dynamic-Warehousing"><a href="#57-SwarmVLM-VLM-Guided-Impedance-Control-for-Autonomous-Navigation-of-Heterogeneous-Robots-in-Dynamic-Warehousing" class="headerlink" title="57. SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SwarmVLM__VLM-Guided_Impedance_Control_for_Autonomous_Navigation_of_Heterogeneous_Robots_in_Dynamic_.pdf">SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Skolkovo Institute of Science and Technology</span></p>
<p>该论文提出了SwarmVLM系统，将人工势场（APF）路径规划、虚拟阻抗控制、视觉语言模型（VLM）与检索增强生成（RAG）框架相结合，实现无人机与地面移动机器人在动态仓储环境下的协作导航。实验结果显示，该系统能根据障碍物分布自适应调整阻抗参数，真实场景下导航成功率达92%，VLM-RAG模块在良好光照下物体检测及参数选择准确率为80%。结论表明：该方法显著提升了异构机器人在密集动态环境中的安全与智能导航能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-Risk-Map-As-Middleware-Towards-Interpretable-Cooperative-End-to-end-Autonomous-Driving-for-Risk-Aware-Planning"><a href="#58-Risk-Map-As-Middleware-Towards-Interpretable-Cooperative-End-to-end-Autonomous-Driving-for-Risk-Aware-Planning" class="headerlink" title="58. Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Risk_Map_As_Middleware__Towards_Interpretable_Cooperative_End-to-end_Autonomous_Driving_for_Risk-Awa.pdf">Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tongji University</span></p>
<p>本文提出了Risk Map as Middleware（RiskMM），一种结合多智能体时空协作感知、Transformer架构与显式风险建模的端到端自动驾驶框架。RiskMM利用多车及基础设施的感知数据，生成可解释的时空风险图，并通过学习型MPC模块，在物理约束下进行风险感知的轨迹规划。实验表明，RiskMM在V2XPnP-Seq真实数据集上大幅提升了规划安全性和解释性，优于现有方法，并具备良好鲁棒性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-AIS-LLM-A-Unified-Framework-for-Maritime-Trajectory-Prediction-Anomaly-Detection-and-Collision-Risk-Assessment-with-Explainable-Forecasting"><a href="#59-AIS-LLM-A-Unified-Framework-for-Maritime-Trajectory-Prediction-Anomaly-Detection-and-Collision-Risk-Assessment-with-Explainable-Forecasting" class="headerlink" title="59. AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AIS-LLM__A_Unified_Framework_for_Maritime_Trajectory_Prediction,_Anomaly_Detection,_and_Collision_Ri.pdf">AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hanbat National University</span></p>
<p>该论文提出AIS-LLM框架，将多尺度AIS时序数据与大语言模型（LLM）结合，采用多任务学习结构同时实现船舶轨迹预测、异常检测和碰撞风险评估，并支持数值预测和自然语言解释。实验显示AIS-LLM在所有任务上均优于现有方法，显著提升了海事交通分析的准确性和可解释性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-End-to-End-Humanoid-Robot-Safe-and-Comfortable-Locomotion-Policy"><a href="#60-End-to-End-Humanoid-Robot-Safe-and-Comfortable-Locomotion-Policy" class="headerlink" title="60. End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/End-to-End_Humanoid_Robot_Safe_and_Comfortable_Locomotion_Policy.pdf">End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>本文提出了一种端到端的仿人机器人行走策略，利用原始时空LiDAR点云，经由GRU-MLP结构直接映射至机器人电机命令，实现复杂3D动态场景下的安全舒适导航。方法将控制障碍函数（CBF）原理融入约束马尔可夫决策过程（CMDP）并采用P3O算法训练，同时引入基于人机交互的舒适性奖励。实验结果表明，该策略在仿真和真实机器人上均可实现高效、安全、平滑且符合社会规范的避障行走，显著优于基线方法。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-Training-Free-ANN-to-SNN-Conversion-for-High-Performance-Spiking-Transformer"><a href="#61-Training-Free-ANN-to-SNN-Conversion-for-High-Performance-Spiking-Transformer" class="headerlink" title="61. Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Training-Free_ANN-to-SNN_Conversion_for_High-Performance_Spiking_Transformer.pdf">Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>该论文提出了面向Transformer架构的无训练人工神经网络（ANN）到脉冲神经网络（SNN）转换框架，核心方法是多基指数衰减（MBE）神经元，通过指数衰减和多基编码精准拟合Transformer中的非线性操作（如GELU、Softmax、LayerNorm、浮点乘法），无需修改预训练ANN权重。实验覆盖CV、NLU、NLG三大任务和ViT、RoBERTa、GPT-2等主流模型，结果显示该方法在保持近乎无损精度的同时显著降低推理延迟，为高效可扩展部署脉冲Transformer提供新思路。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-LAURON-VI-A-Six-Legged-Robot-for-Dynamic-Walking"><a href="#62-LAURON-VI-A-Six-Legged-Robot-for-Dynamic-Walking" class="headerlink" title="62. LAURON VI: A Six-Legged Robot for Dynamic Walking"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LAURON_VI__A_Six-Legged_Robot_for_Dynamic_Walking.pdf">LAURON VI: A Six-Legged Robot for Dynamic Walking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">FZI Research Center for Information Technology</span></p>
<p>本文提出了六足机器人平台LAURON VI，采用18个串联弹性关节驱动器，并设计实现了三种动态行走控制器：基于运动学的方法、模型预测控制（MPC）、和强化学习策略。实验表明，三种控制策略均能在不同环境下实现灵活且高效的运动，显著提升了六足机器人在复杂地形和实际任务中的速度和适应性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-GraphCoT-VLA-A-3D-Spatial-Aware-Reasoning-Vision-Language-Action-Model-for-Robotic-Manipulation-with-Ambiguous-Instructions"><a href="#63-GraphCoT-VLA-A-3D-Spatial-Aware-Reasoning-Vision-Language-Action-Model-for-Robotic-Manipulation-with-Ambiguous-Instructions" class="headerlink" title="63. GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/GraphCoT-VLA__A_3D_Spatial-Aware_Reasoning_Vision-Language-Action_Model_for_Robotic_Manipulation_wit.pdf">GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Noah’s Ark Lab, Huawei</span></p>
<p>本文提出了一种名为GraphCoT-VLA的端到端机器人操作模型，通过引入结构化Chain-of-Thought推理模块和实时可更新的3D Pose-Object图，实现了对多视角视觉、语言指令及机器人状态的联合理解和动态推理。实验结果表明，该模型在面对模糊指令和开放环境时，在任务成功率、响应速度和泛化能力方面均显著优于现有方法。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="64-Sensory-Robustness-through-Top-Down-Feedback-and-Neural-Stochasticity-in-Recurrent-Vision-Models"><a href="#64-Sensory-Robustness-through-Top-Down-Feedback-and-Neural-Stochasticity-in-Recurrent-Vision-Models" class="headerlink" title="64. Sensory Robustness through Top-Down Feedback and Neural Stochasticity in Recurrent Vision Models"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sensory_robustness_through_top-down_feedback_and_neural_stochasticity_in_recurrent_vision_models.pdf">Sensory Robustness through Top-Down Feedback and Neural Stochasticity in Recurrent Vision Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Department of Neural Dynamics and Magnetoencephalography, Hertie Institute for Clinical Brain Research, University of Tübingen</span></p>
<p>本文提出了一种结合卷积递归神经网络（ConvRNN）中的顶层-底层反馈通路和神经元随机性（通过dropout实现）的视觉模型，并在图像分类任务中系统比较了有无反馈和有无随机性的不同架构。研究发现：只有同时具备顶层反馈和神经元随机性时，模型才能在速度-准确率权衡、噪声扰动和对抗攻击等分布外场景下表现出显著的鲁棒性和高效性。进一步分析显示，反馈信号能显著塑造网络的表示空间，将特征约束到低维流形上，提升泛化能力和稳定性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-TeSO-Representing-and-Compressing-3D-Point-Cloud-Scenes-with-Textured-Surfel-Octree"><a href="#65-TeSO-Representing-and-Compressing-3D-Point-Cloud-Scenes-with-Textured-Surfel-Octree" class="headerlink" title="65. TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TeSO__Representing_and_Compressing_3D_Point_Cloud_Scenes_with_Textured_Surfel_Octree.pdf">TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">New York University</span></p>
<p>该论文提出了Textured Surfel Octree (TeSO)，一种结合点云和网格优势的新型3D场景表示方法。其流程包括利用点云构建八叉树结构的表面元素（surfel），并为每个surfel分配纹理贴图，同时设计了基于神经网络的熵模型和标准编解码器实现高效压缩。实验结果表明，TeSO在低比特率下可实现比传统点云和3D高斯基线更高质量的渲染，支持实时渲染且更适合3D内容流媒体和XR应用。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-BrainATCL-Adaptive-Temporal-Brain-Connectivity-Learning-for-Functional-Link-Prediction-and-Age-Estimation"><a href="#66-BrainATCL-Adaptive-Temporal-Brain-Connectivity-Learning-for-Functional-Link-Prediction-and-Age-Estimation" class="headerlink" title="66. BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BrainATCL__Adaptive_Temporal_Brain_Connectivity_Learning_for_Functional_Link_Prediction_and_Age_Esti.pdf">BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">New Jersey Institute of Technology</span></p>
<p>该论文提出了一种无监督、自适应的时序脑连接学习方法BrainATCL，通过基于新颖性指数动态调整每个时间点的历史窗口长度，并结合结构&#x2F;功能相关的边属性，利用GINE-Mamba2骨干网络编码fMRI动态功能连接时序图，实现了功能连接预测和年龄估计。实验表明，BrainATCL在Human Connectome Project数据上对功能连接预测和年龄估计任务表现优异，泛化能力强，超越了现有静态和动态图学习基线。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-Large-Language-Model-Evaluated-Stand-alone-Attention-Assisted-Graph-Neural-Network-with-Spatial-and-Structural-Information-Interaction-for-Precise-Endoscopic-Image-Segmentation"><a href="#67-Large-Language-Model-Evaluated-Stand-alone-Attention-Assisted-Graph-Neural-Network-with-Spatial-and-Structural-Information-Interaction-for-Precise-Endoscopic-Image-Segmentation" class="headerlink" title="67. Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Large_Language_Model_Evaluated_Stand-alone_Attention-Assisted_Graph_Neural_Network_with_Spatial_and_.pdf">Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong</span></p>
<p>本文提出FOCUS-Med，一种融合空间和结构图、带有注意力机制的图神经网络方法，用于内镜图像中精确息肉分割。方法包括Dual-GCN模块用于空间和拓扑特征提取、位置融合自注意力机制进行全局上下文整合、以及加权快速归一化融合实现多尺度特征聚合，并首次引入大型语言模型（LLM）辅助分割结果的专家级定性评估。实验结果显示，FOCUS-Med在多个公开医学图像数据集上分割性能超越现有先进模型，具有较高精度和临床应用潜力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-TerraMAE-Learning-Spatial-Spectral-Representations-from-Hyperspectral-Earth-Observation-Data-via-Adaptive-Masked-Autoencoders"><a href="#68-TerraMAE-Learning-Spatial-Spectral-Representations-from-Hyperspectral-Earth-Observation-Data-via-Adaptive-Masked-Autoencoders" class="headerlink" title="68. TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TerraMAE__Learning_Spatial-Spectral_Representations_from_Hyperspectral_Earth_Observation_Data_via_Ad.pdf">TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Colorado State University</span></p>
<p>本文提出TerraMAE，一种专为高光谱遥感影像设计的自监督Masked Autoencoder预训练框架，通过基于统计反射特性的自适应通道分组和结合空间结构与光谱一致性的复合损失函数，实现空间-光谱特征的高效建模。实验结果表明，TerraMAE在高保真影像重建与下游土壤质地预测、作物类型识别和土地覆盖分类等地理空间任务中均显著优于传统MAE和ResNet-50等基线方法，具有很强的泛化和迁移能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-OctreeNCA-Single-Pass-184-MP-Segmentation-on-Consumer-Hardware"><a href="#69-OctreeNCA-Single-Pass-184-MP-Segmentation-on-Consumer-Hardware" class="headerlink" title="69. OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/OctreeNCA__Single-Pass_184_MP_Segmentation_on_Consumer_Hardware.pdf">OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University of Darmstadt</span></p>
<p>本文提出了OctreeNCA，一种基于八叉树结构的神经元元胞自动机（NCA）分割架构，通过分层方式快速传播全局上下文并进行高分辨率医学图像和视频的端到端分割。该方法结合自定义CUDA推理内核，显著降低了推理所需的显存（VRAM），实现了在消费级硬件上对184MP病理切片和一分钟外科手术视频的单步高效分割。实验表明，OctreeNCA在分割精度上可媲美现有SOTA模型，同时大幅降低参数量和推理显存需求，适合低成本设备部署，有助于缩小全球医疗硬件差距。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-TADoc-Robust-Time-Aware-Document-Image-Dewarping"><a href="#70-TADoc-Robust-Time-Aware-Document-Image-Dewarping" class="headerlink" title="70. TADoc: Robust Time-Aware Document Image Dewarping"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TADoc__Robust_Time-Aware_Document_Image_Dewarping.pdf">TADoc: Robust Time-Aware Document Image Dewarping</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>本文提出了TADoc方法，将文档图像去畸变任务首次建模为一个多步骤的动态过程，通过时间嵌入和轻量级网络逐步预测不同程度的畸变恢复。提出了新的评测指标DLS用于评估去畸变效果在下游任务中的表现。实验结果显示，TADoc在多个公开基准上具有强鲁棒性和领先性能。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-WeatherDiffusion-Weather-Guided-Diffusion-Model-for-Forward-and-Inverse-Rendering"><a href="#71-WeatherDiffusion-Weather-Guided-Diffusion-Model-for-Forward-and-Inverse-Rendering" class="headerlink" title="71. WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/WeatherDiffusion__Weather-Guided_Diffusion_Model_for_Forward_and_Inverse_Rendering.pdf">WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>该论文提出WeatherDiffusion，一种基于大规模扩散模型的自动驾驶场景前向与逆向渲染框架，通过天气控制器和文本提示实现多天气与照明条件下的高质量图像生成与分解。引入Intrinsic Map-Aware Attention（MAA）模块，结合新构建的合成与真实数据集，显著提升逆向渲染精度和下游目标检测与分割在恶劣天气下的鲁棒性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-Beyond-Frequency-Seeing-Subtle-Cues-Through-the-Lens-of-Spatial-Decomposition-for-Fine-Grained-Visual-Classification"><a href="#72-Beyond-Frequency-Seeing-Subtle-Cues-Through-the-Lens-of-Spatial-Decomposition-for-Fine-Grained-Visual-Classification" class="headerlink" title="72. Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Beyond_Frequency__Seeing_Subtle_Cues_Through_the_Lens_of_Spatial_Decomposition_for_Fine-Grained_Visu.pdf">Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Anhui University</span></p>
<p>该论文提出了一种全新的细粒度视觉分类方法SCOPE（Subtle-Cue Oriented Perception Engine），通过空间自适应滤波替代传统频域变换，其中包含细致特征提取模块（SDE）和显著语义优化模块（SSR），逐层增强局部细节与全局结构。实验结果表明，SCOPE在四个主流细粒度分类数据集上取得了新的SOTA表现，显著提升了对微小判别线索的识别能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-CannyEdit-Selective-Canny-Control-and-Dual-Prompt-Guidance-for-Training-free-Image-Editing"><a href="#73-CannyEdit-Selective-Canny-Control-and-Dual-Prompt-Guidance-for-Training-free-Image-Editing" class="headerlink" title="73. CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-free Image Editing"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CannyEdit__Selective_Canny_Control_and_Dual-Prompt_Guidance_for_Training-Free_Image_Editing.pdf">CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-free Image Editing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>本文提出了CannyEdit，一种基于训练自由的区域图像编辑方法，结合Selective Canny Control和Dual-Prompt Guidance技术。Selective Canny Control通过对用户指定的编辑区域屏蔽Canny ControlNet结构引导，同时严格保留未编辑区域的原始细节，实现精确的文本驱动编辑；Dual-Prompt Guidance则融合局部和全局文本提示，确保编辑区域与整体场景的语义一致性。实验结果显示，CannyEdit在文本匹配度、上下文保真度及编辑自然度上优于现有方法，用户研究表明其编辑结果更难被辨识为AI生成。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-MMReID-Bench-Unleashing-the-Power-of-MLLMs-for-Effective-and-Versatile-Person-Re-identification"><a href="#74-MMReID-Bench-Unleashing-the-Power-of-MLLMs-for-Effective-and-Versatile-Person-Re-identification" class="headerlink" title="74. MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MMReID-Bench__Unleashing_the_Power_of_MLLMs_for_Effective_and_Versatile_Person_Re-identification.pdf">MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">East China Normal University</span></p>
<p>本论文提出了MMReID-Bench，这是首个专为多任务和多模态场景设计的人体重识别基准，覆盖10类典型任务（如RGB、红外、草图等），并系统性评估了15种主流多模态大语言模型（MLLMs）在该基准上的表现。结果表明MLLMs在大多数人体重识别任务中取得了优异成绩，但在热成像和红外等跨模态任务上存在明显短板，亟需提升跨模态理解能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-A-Simple-yet-Powerful-Instance-Aware-Prompting-Framework-for-Training-free-Camouflaged-Object-Segmentation"><a href="#75-A-Simple-yet-Powerful-Instance-Aware-Prompting-Framework-for-Training-free-Camouflaged-Object-Segmentation" class="headerlink" title="75. A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Simple_yet_Powerful_Instance-Aware_Prompting_Framework_for_Training-free_Camouflaged_Object_Segmen.pdf">A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai University</span></p>
<p>该论文提出了首个训练自由的实例感知提示框架（IAPF），用于伪装物体分割。方法包括三步：利用多模态大语言模型生成前景&#x2F;背景标签，借助Grounding DINO产生多实例级别的边界框和区域点提示，并结合自一致性实例掩码投票机制提升多实例分割精度。实验表明，IAPF在多个标准伪装分割数据集上显著优于现有方法。结论：该方法无需训练即可实现高精度多实例伪装分割，提升了训练自由分割领域的表现。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-MV-CoRe-Multimodal-Visual-Conceptual-Reasoning-for-Complex-Visual-Question-Answering"><a href="#76-MV-CoRe-Multimodal-Visual-Conceptual-Reasoning-for-Complex-Visual-Question-Answering" class="headerlink" title="76. MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MV-CoRe__Multimodal_Visual-Conceptual_Reasoning_for_Complex_Visual_Question_Answering.pdf">MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shaanxi University of Technology</span></p>
<p>MV-CoRe提出了一种多模态视觉-概念推理模型，通过融合预训练视觉大模型和语言大模型的全局特征，以及基于目标检测和场景图的细粒度语义视觉特征，利用创新的多模态融合Transformer进行深度跨模态融合和推理。在GQA、A-OKVQA和OKVQA等复杂VQA基准上，MV-CoRe显著优于主流LVLM模型，提升了视觉理解和复杂推理能力，尤其在人类评估中展现更高事实正确性和推理深度。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-HiMat-DiT-based-Ultra-High-Resolution-SVBRDF-Generation"><a href="#77-HiMat-DiT-based-Ultra-High-Resolution-SVBRDF-Generation" class="headerlink" title="77. HiMat: DiT-based Ultra-High Resolution SVBRDF Generation"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HiMat__DiT-based_Ultra-High_Resolution_SVBRDF_Generation.pdf">HiMat: DiT-based Ultra-High Resolution SVBRDF Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nankai University</span></p>
<p>该论文提出HiMat，一种基于Diffusion Transformer（DiT）的高效扩散模型，用于原生4K超高分辨率SVBRDF（空间变化双向反射分布函数）材料生成。方法结合线性注意力DiT骨干与轻量级CrossStitch卷积模块，实现多通道SVBRDF一致性和高频细节保留，并采用小波域监督提升微结构细节生成。结论显示，该方法在视觉质量、结构一致性和计算效率方面均优于现有方法，实现了高质量4K材料生成，并可扩展至内在分解等任务。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-Spatio-Temporal-Conditional-Diffusion-Models-for-Forecasting-Future-Multiple-Sclerosis-Lesion-Masks-Conditioned-on-Treatments"><a href="#78-Spatio-Temporal-Conditional-Diffusion-Models-for-Forecasting-Future-Multiple-Sclerosis-Lesion-Masks-Conditioned-on-Treatments" class="headerlink" title="78. Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Spatio-Temporal_Conditional_Diffusion_Models_for_Forecasting_Future_Multiple_Sclerosis_Lesion_Masks_.pdf">Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">McGill University</span></p>
<p>该论文提出了首个结合ControlNet的时空条件扩散模型，用于预测多发性硬化症（MS）患者在不同治疗方案下未来MRI上的新发和增大T2病灶掩膜。方法通过将3D MRI数据重构为伪2D片，融合多模态影像和治疗信息，实现高分辨率、体素级预测。实验显示模型在六种治疗组下效果优于基于统计的基线，并能支持临床相关任务如病灶计数、位置估计及反事实生成。结论：模型能准确预测未来病灶并辅助个体化医疗决策，展现了生成式AI在医学影像个性化预测中的应用潜力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-Can-Multitask-Learning-Enhance-Model-Explainability"><a href="#79-Can-Multitask-Learning-Enhance-Model-Explainability" class="headerlink" title="79. Can Multitask Learning Enhance Model Explainability?"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Can_Multitask_Learning_Enhance_Model_Explainability_.pdf">Can Multitask Learning Enhance Model Explainability?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kaiserslautern-Landau University</span></p>
<p>本文提出一种通过多任务学习增强模型可解释性的框架，针对遥感多模态数据，将部分输入模态转为辅助预测任务，主干采用多模态编码器融合卫星、气象等信息，分别在分割、分类和回归任务上进行实验。结果显示该方法在主任务性能基本持平甚至优于多模态基线，同时无需在部署阶段增加输入数据，并通过分析主任务与辅助任务的误差相关性，提升了模型行为的内在解释性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-Intrinsic-Explainability-of-Multimodal-Learning-for-Crop-Yield-Prediction"><a href="#80-Intrinsic-Explainability-of-Multimodal-Learning-for-Crop-Yield-Prediction" class="headerlink" title="80. Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Intrinsic_Explainability_of_Multimodal_Learning_for_Crop_Yield_Prediction.pdf">Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">RPTU Kaiserslautern-Landau</span></p>
<p>本文提出基于Transformer的多模态学习框架，用于作物产量预测，融合多时序卫星、气象、土壤和地形等四类数据，并系统性比较了Attention Rollout（AR）、Generic Attention（GA）和Shapley Value Sampling（SVS）三种特征归因方法。实验结果表明，Transformer在子田块和田块层面均优于卷积和循环网络，AR方法归因结果更稳定且更具一致性，揭示关键作物生长阶段与模型决策高度相关。模型在多区域多作物数据上验证了解释性方法的有效性，同时提出的WMA方法对多模态重要性分析显示与Shapley估计存在较大差异，需进一步研究。结论认为，Transformer内在可解释性和AR方法能提升多模态网络的透明度，推动农业遥感等数据密集领域的可解释AI发展。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-eMotions-A-Large-Scale-Dataset-and-Audio-Visual-Fusion-Network-for-Emotion-Analysis-in-Short-form-Videos"><a href="#81-eMotions-A-Large-Scale-Dataset-and-Audio-Visual-Fusion-Network-for-Emotion-Analysis-in-Short-form-Videos" class="headerlink" title="81. eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/eMotions__A_Large-Scale_Dataset_and_Audio-Visual_Fusion_Network_for_Emotion_Analysis_in_Short-form_V.pdf">eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong University</span></p>
<p>该论文提出了eMotions——首个专为短视频情感分析的大规模数据集，包含27,996个视频并配有六种情绪标签，同时发布了AV-CANet模型（基于Video Swin-Transformer和ResNet34）实现音视频特征融合，通过设计Local-Global Fusion模块和EP-CE损失函数提升情感识别性能。实验表明，所提方法在多个基准和公开数据集上均优于现有方法，并为未来短视频情感分析研究提供了基础。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-VSI-Visual–Subtitle-Integration-for-Keyframe-Selection-to-Enhance-Long-Video-Understanding"><a href="#82-VSI-Visual–Subtitle-Integration-for-Keyframe-Selection-to-Enhance-Long-Video-Understanding" class="headerlink" title="82. VSI: Visual–Subtitle Integration for Keyframe Selection to Enhance Long Video Understanding"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VSI__Visual_Subtitle_Integration_for_Keyframe_Selection_to_enhance_Long_Video_Understanding.pdf">VSI: Visual–Subtitle Integration for Keyframe Selection to Enhance Long Video Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HKUST(GZ)</span></p>
<p>该论文提出了VISUAL-SUBTITLE INTEGRATION (VSI)方法，通过视觉对象检测和字幕语义匹配的双流融合机制，实现长视频关键帧检索。该方法无需额外训练，能高效整合视觉与文本信息，提升关键帧定位和长视频问答准确率。实验结果显示，VSI在LONGVIDEOBENCH数据集上关键帧检索和视频问答任务均取得了领先性能，具备良好通用性和可扩展性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-Low-Rank-Expert-Merging-for-Multi-Source-Domain-Adaptation-in-Person-Re-Identification"><a href="#83-Low-Rank-Expert-Merging-for-Multi-Source-Domain-Adaptation-in-Person-Re-Identification" class="headerlink" title="83. Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Low-Rank_Expert_Merging_for_Multi-Source_Domain_Adaptation_in_Person_Re-Identification.pdf">Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ETS Montreal</span></p>
<p>该论文提出了SAGE-reID，一种无需源数据的多源域自适应方法，专为行人再识别任务设计。方法包括两阶段流程：首先对每个源域模型仅微调低秩适配器（LoRA），然后通过轻量级门控网络动态融合多个LoRA专家以实现跨域知识迁移。实验在Market-1501、DukeMTMC-reID和MSMT17等基准数据集上显示，SAGE-reID在准确性和计算效率上均优于现有方法。结论：SAGE-reID可高效实现多源知识融合，提升跨域行人识别表现。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-Dual-Resolution-Residual-Architecture-with-Artifact-Suppression-for-Melanocytic-Lesion-Segmentation"><a href="#84-Dual-Resolution-Residual-Architecture-with-Artifact-Suppression-for-Melanocytic-Lesion-Segmentation" class="headerlink" title="84. Dual-Resolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DualResolution_Residual_Architecture_with_Artifact_Suppression_for_Melanocytic_Lesion_Segmentation.pdf">Dual-Resolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">SNAIT Research</span></p>
<p>该论文提出一种针对皮肤镜图像中黑色素瘤病变分割的新型ResNet风格双分辨率架构，包括高分辨率边界流和多尺度上下文流，结合边界感知残差连接、通道注意力和轻量伪影抑制模块，并采用Dice–Tversky分割损失、边界损失和对比正则化的多任务训练目标。实验结果显示，该方法在公开数据集上显著提升了分割精度和边界贴合度，优于常规编码器-解码器基线，有助于自动化皮肤癌评估系统。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-Hardness-Aware-Dynamic-Curriculum-Learning-for-Robust-Multimodal-Emotion-Recognition-with-Missing-Modalities"><a href="#85-Hardness-Aware-Dynamic-Curriculum-Learning-for-Robust-Multimodal-Emotion-Recognition-with-Missing-Modalities" class="headerlink" title="85. Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hardness-Aware_Dynamic_Curriculum_Learning_for_Robust_Multimodal_Emotion_Recognition_with_Missing_Mo.pdf">Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Inner Mongolia University</span></p>
<p>该论文提出了HARDY-MER框架，通过多视角难度评估（结合模态重建误差与跨模态互信息）量化样本难度，并利用检索增强动态课程学习策略为高难度样本检索语义相关支持样本，动态调整训练重点，从而提升模型在缺失模态下的鲁棒性和泛化能力。实验表明，该方法在IEMOCAP和CMU-MOSEI等基准数据集的多种缺失模态设定下均优于现有方法，验证了其有效性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-SafePLUG-Empowering-Multimodal-LLMs-with-Pixel-Level-Insight-and-Temporal-Grounding-for-Traffic-Accident-Understanding"><a href="#86-SafePLUG-Empowering-Multimodal-LLMs-with-Pixel-Level-Insight-and-Temporal-Grounding-for-Traffic-Accident-Understanding" class="headerlink" title="86. SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SafePLUG__Empowering_Multimodal_LLMs_with_Pixel-Level_Insight_and_Temporal_Grounding_for_Traffic_Acc.pdf">SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Wisconsin–Madison</span></p>
<p>SafePLUG提出了一种多模态大语言模型框架，结合视觉提示编码器与SAM基础的像素级分割解码器，实现交通事故的区域感知、像素级理解和基于数字标记的时序定位。通过构建多模态问答数据集，SafePLUG在区域问答、像素分割、事故描述与时序定位等任务上均超越现有模型，显著提升了复杂交通场景下的细粒度理解能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-FoundBioNet-A-Foundation-Based-Model-for-IDH-Genotyping-of-Glioma-from-Multi-Parametric-MRI"><a href="#87-FoundBioNet-A-Foundation-Based-Model-for-IDH-Genotyping-of-Glioma-from-Multi-Parametric-MRI" class="headerlink" title="87. FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FoundBioNet__A_Foundation-Based_Model_for_IDH_Genotyping_of_Glioma_from_Multi-Parametric_MRI.pdf">FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tehran University of Medical Sciences</span></p>
<p>本文提出了一种基础模型FoundBioNet，基于SWIN-UNETR架构，结合肿瘤感知特征编码（TAFE）和跨模态差异（CMD）模块，从多参数MRI中无创预测胶质瘤IDH突变状态。模型在六个多中心公开数据集上训练，表现出较强泛化能力，AUC最高达90.58%，显著优于传统卷积和Transformer基线方法，结果具备较高准确性和可解释性，有望用于临床个体化管理。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-FormCoach-Lift-Smarter-Not-Harder"><a href="#88-FormCoach-Lift-Smarter-Not-Harder" class="headerlink" title="88. FormCoach: Lift Smarter, Not Harder"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FormCoach__Lift_Smarter,_Not_Harder.pdf">FormCoach: Lift Smarter, Not Harder</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Pennsylvania</span></p>
<p>FormCoach提出了一种基于视觉-语言模型（VLMs）的实时AI健身教练系统，利用摄像头对用户动作与专家参考视频进行对比分析，生成个性化、简明的动作纠正建议。作者构建并公开了包含1,700组专家标注视频对的数据集，系统性评测了多种主流VLM模型在人体运动差异分析与反馈生成上的性能。实验发现，VLMs在行动指导性上表现良好但在准确识别细微动作失误上仍有较大提升空间，现有模型易出现遗漏或幻觉反馈。结论认为，该工作推动了AI健身辅导迈向互动、精准和多模态融合的新方向。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding"><a href="#89-VisR-Bench-An-Empirical-Study-on-Visual-Retrieval-Augmented-Generation-for-Multilingual-Long-Document-Understanding" class="headerlink" title="89. VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VisR-Bench__An_Empirical_Study_on_Visual_Retrieval-Augmented_Generation_for_Multilingual_Long_Docume.pdf">VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University at Buffalo</span></p>
<p>本文提出了VisR-Bench，这是首个面向多语言长文档的视觉检索增强生成（RAG）基准，涵盖16种语言、1,286份多模态文档和35,000+高质量问答对，支持文本、表格和图像多证据类型的细粒度评测。系统性实验比较了文本检索、多模态编码器和多模态大模型（MLLMs），结果显示MLLMs在检索任务上优于传统方法，但在结构化表格内容和低资源语言上表现仍有限，揭示了多语言多模态检索的关键挑战。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-Noise-Aware-Generative-Microscopic-Traffic-Simulation"><a href="#90-Noise-Aware-Generative-Microscopic-Traffic-Simulation" class="headerlink" title="90. Noise-Aware Generative Microscopic Traffic Simulation"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Noise-Aware_Generative_Microscopic_Traffic_Simulation.pdf">Noise-Aware Generative Microscopic Traffic Simulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Massachusetts Institute of Technology</span></p>
<p>该论文提出了I-24 MOTION Scenario Dataset (I24-MSD)，保留基础设施摄像头采集的交通轨迹中的真实噪声，并将其作为学习问题的一部分。作者基于该数据集，使用GPT风格的Transformer模型（SMART），并引入了标签平滑、focal loss和对称交叉熵等噪声感知损失函数以提升模型在噪声环境下的生成效果。实验结果显示，噪声感知优化显著提升了微观交通仿真的真实度和鲁棒性。结论认为，显式建模数据噪声可提升仿真质量，并促进智能交通系统研究。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-CLUE-Leveraging-Low-Rank-Adaptation-to-Capture-Latent-Uncovered-Evidence-for-Image-Forgery-Localization"><a href="#91-CLUE-Leveraging-Low-Rank-Adaptation-to-Capture-Latent-Uncovered-Evidence-for-Image-Forgery-Localization" class="headerlink" title="91. CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CLUE__Leveraging_Low-Rank_Adaptation_to_Capture_Latent_Uncovered_Evidence_for_Image_Forgery_Localiza.pdf">CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen University</span></p>
<p>该论文提出了CLUE框架，通过低秩适应（LoRA）对Stable Diffusion 3（SD3）和Segment Anything Model（SAM）进行参数高效的联合微调，利用SD3的Rectified Flow噪声机制在潜空间放大伪造区域的微弱特征，并融合SAM图像编码器的空间语义线索，实现精确的伪造区域定位。实验结果显示，CLUE在多种公开数据集上的伪造定位表现和对后处理攻击的鲁棒性均显著优于现有方法，具备优异的泛化能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-VA-Blueprint-Uncovering-Building-Blocks-for-Visual-Analytics-System-Design"><a href="#92-VA-Blueprint-Uncovering-Building-Blocks-for-Visual-Analytics-System-Design" class="headerlink" title="92. VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VA-Blueprint__Uncovering_Building_Blocks_for_Visual_Analytics_System_Design.pdf">VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Chicago</span></p>
<p>该论文提出了一套系统方法（VA-Blueprint），通过人工和大语言模型（LLM）辅助，从101篇城市视觉分析（VA）系统论文中自动抽取、归类并结构化系统组件、操作和依赖，形成多层次、可查询的知识库。结论表明，VA-Blueprint能够揭示VA系统通用设计模式和演化趋势，提升系统复用性与开发效率，同时专家评估验证了方法的准确性和实用性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-ForensicsSAM-Toward-Robust-and-Unified-Image-Forgery-Detection-and-Localization-Resisting-to-Adversarial-Attack"><a href="#93-ForensicsSAM-Toward-Robust-and-Unified-Image-Forgery-Detection-and-Localization-Resisting-to-Adversarial-Attack" class="headerlink" title="93. ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ForensicsSAM__Toward_Robust_and_Unified_Image_Forgery_Detection_and_Localization_Resisting_to_Advers.pdf">ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen University</span></p>
<p>该论文提出ForensicsSAM，一种面向图像篡改检测与定位的统一框架，具有内置的对抗鲁棒性。方法通过在SAM主干的每个transformer block中注入共享的forgery experts提升篡改特征提取能力，并设计轻量级adversary detector识别对抗样本，再利用自适应adversary experts对特征漂移进行修正，三阶段训练实现对干净与对抗图像的独立优化。实验表明，ForensicsSAM在多种数据集与对抗攻击下均达到了图像级检测和像素级定位的最新性能和鲁棒性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-Tight-Bounds-for-Schrodinger-Potential-Estimation-in-Unpaired-Image-to-Image-Translation-Problems"><a href="#94-Tight-Bounds-for-Schrodinger-Potential-Estimation-in-Unpaired-Image-to-Image-Translation-Problems" class="headerlink" title="94. Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Tight_Bounds_for_Schr%C3%B6dinger_Potential_Estimation_in_Unpaired_Image-to-Image_Translation_Problems.pdf">Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HSE University</span></p>
<p>该论文提出了一种基于Ornstein-Uhlenbeck过程的Schrödinger桥理论，用于无配对的图像到图像转换问题，通过优化经验风险最小化器估计Schrödinger潜力，并首次给出了该方法在有限样本下的非渐近高概率泛化误差界。实验证明该理论在生成建模和风格迁移任务中具有更快的收敛速度和更好的性能。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-Invert4TVG-A-Temporal-Video-Grounding-Framework-with-Inversion-Tasks-for-Enhanced-Action-Understanding"><a href="#95-Invert4TVG-A-Temporal-Video-Grounding-Framework-with-Inversion-Tasks-for-Enhanced-Action-Understanding" class="headerlink" title="95. Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Invert4TVG__A_Temporal_Video_Grounding_Framework_with_Inversion_Tasks_for_Enhanced_Action_Understand.pdf">Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiamen University</span></p>
<p>该论文提出Invert4TVG框架，通过引入三种反转任务（动词补全、动作识别、视频描述），利用TVG原始标注，在不依赖额外数据的前提下提升模型对视频动作语义的理解。方法将TVG与Invert-TVG多任务整合进强化学习训练流程，通过动态采样和专门奖励设计，兼顾定位精度与动作理解，实验在多个数据集上显著优于现有方法，表明增强动作语义对提升时序视频定位上限具有关键作用。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-KLASSify-to-Verify-Audio-Visual-Deepfake-Detection-Using-SSL-based-Audio-and-Handcrafted-Visual-Features"><a href="#96-KLASSify-to-Verify-Audio-Visual-Deepfake-Detection-Using-SSL-based-Audio-and-Handcrafted-Visual-Features" class="headerlink" title="96. KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/KLASSify_to_Verify__Audio-Visual_Deepfake_Detection_Using_SSL-based_Audio_and_Handcrafted_Visual_Fea.pdf">KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KLASS Engineering and Solutions</span></p>
<p>本文提出KLASSify系统，通过手工视觉特征与基于自监督学习的音频特征，结合图注意力网络（GAT）与时序卷积网络（TCN），实现音视频多模态深度伪造检测与定位。方法在AV-Deepfake1M++数据集上，分类任务AUC达92.78%，音频定位IoU达0.3536，展现了高效、鲁棒且可解释的性能。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-RORPCap-Retrieval-based-Objects-and-Relations-Prompt-for-Image-Captioning"><a href="#97-RORPCap-Retrieval-based-Objects-and-Relations-Prompt-for-Image-Captioning" class="headerlink" title="97. RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/RORPCap__Retrieval-based_Objects_and_Relations_Prompt_for_Image_Captioning.pdf">RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Yunnan University</span></p>
<p>该论文提出了一种新颖的图像描述方法RORPCap，通过设计的对象与关系抽取模块（OREM）从检索到的相似描述句中提取关键对象和关系词，填充到模板形成prompt，并结合CLIP视觉特征经过Mamba映射网络转化为视觉-文本特征，再与prompt拼接后输入GPT-2生成描述。实验表明，RORPCap在MS-COCO等数据集上实现了与主流检测器和GCN方法可比的描述质量，但训练时间显著缩短，仅需2.6小时，具备良好零样本泛化能力，能有效降低训练成本。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-BEVANet-Bilateral-Efficient-Visual-Attention-Network-for-Real-Time-Semantic-Segmentation"><a href="#98-BEVANet-Bilateral-Efficient-Visual-Attention-Network-for-Real-Time-Semantic-Segmentation" class="headerlink" title="98. BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BEVANet__Bilateral_Efficient_Visual_Attention_Network_for_Real-Time_Semantic_Segmentation.pdf">BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Taiwan University</span></p>
<p>BEVANet提出了一种双分支高效视觉注意力网络，结合Sparse Decomposed Large Separable Kernel Attentions (SDLSKA)、Comprehensive Kernel Selection (CKS)和Deep Large Kernel Pyramid Pooling Module (DLKPPM)，实现了大感受野语义理解与精细轮廓分割。其创新的注意力机制和边界引导自适应融合模块实现了在Cityscapes和CamVid数据集上的实时语义分割，达到了81.0% mIoU和33 FPS的优异性能，显著提升了分割精度与效率。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-Understanding-Dynamic-Scenes-in-Ego-Centric-4D-Point-Clouds"><a href="#99-Understanding-Dynamic-Scenes-in-Ego-Centric-4D-Point-Clouds" class="headerlink" title="99. Understanding Dynamic Scenes in Ego Centric 4D Point Clouds"></a>99. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Understanding_Dynamic_Scenes_in_Ego_Centric_4D_Point_Clouds.pdf">Understanding Dynamic Scenes in Ego Centric 4D Point Clouds</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出EgoDynamic4D基准，涵盖927K问答对和12类任务，集成RGB-D视频、相机姿态、全局实例掩码和4D包围盒，实现高密度动态场景注释。作者提出端到端时空推理框架，通过实例感知特征编码、时间与相机嵌入和自适应下采样，将大规模4D场景压缩为LLM可处理的序列；实验表明该方法在多模态动态场景理解上显著优于现有基线，推进了自我中心动态视觉理解的发展。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-HaDM-ST-Histology-Assisted-Differential-Modeling-for-Spatial-Transcriptomics-Generation"><a href="#100-HaDM-ST-Histology-Assisted-Differential-Modeling-for-Spatial-Transcriptomics-Generation" class="headerlink" title="100. HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation"></a>100. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HaDM-ST__Histology-Assisted_Differential_Modeling_for_Spatial_Transcriptomics_Generation.pdf">HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cambridge</span></p>
<p>本文提出了HaDM-ST，一种基于扩散模型的高分辨率空间转录组图像生成框架，通过条件化H&amp;E组织切片和低分辨率ST数据，实现高分辨率ST图像生成。方法创新包括H&amp;E驱动的语义蒸馏网络、跨模态空间对齐模块以及基于图神经网络的基因通道判别学习。实验结果表明，HaDM-ST在空间保真度和基因层面表现上均优于现有方法，为精准医疗和组织分子机制研究奠定了基础。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="101-Unsupervised-Real-World-Super-Resolution-via-Rectified-Flow-Degradation-Modelling"><a href="#101-Unsupervised-Real-World-Super-Resolution-via-Rectified-Flow-Degradation-Modelling" class="headerlink" title="101. Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling"></a>101. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Unsupervised_Real-World_Super-Resolution_via_Rectified_Flow_Degradation_Modelling.pdf">Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology Beijing</span></p>
<p>本文提出了无监督的真实世界超分辨率方法，核心在于引入Rectified Flow Degradation Module (RFDM)和Fourier Prior Guided Degradation Module (FGDM)。RFDM通过建模退化轨迹实现对复杂真实退化的建模，FGDM利用傅里叶相位结构信息提升退化建模精度，二者结合生成高仿真低分辨率图像与高分辨率图像对，用于训练任意超分模型，实验证明对真实场景的SR性能提升显著。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="102-SODiff-Semantic-Oriented-Diffusion-Model-for-JPEG-Compression-Artifacts-Removal"><a href="#102-SODiff-Semantic-Oriented-Diffusion-Model-for-JPEG-Compression-Artifacts-Removal" class="headerlink" title="102. SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal"></a>102. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SODiff__Semantic-Oriented_Diffusion_Model_for_JPEG_Compression_Artifacts_Removal.pdf">SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong Univercity</span></p>
<p>该论文提出了SODiff，一种基于语义对齐的图像提示提取器（SAIPE）和质量因子感知时间预测器的单步扩散模型，用于JPEG压缩伪影去除。方法通过提取低质量图像的丰富语义特征并与文本编码器对齐，实现高效的语义指导扩散去伪影，同时根据压缩质量自适应选择降噪时刻。实验结果显示SODiff在多个数据集和指标上优于现有主流方法，尤其在极端压缩条件下表现出色。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="103-CoAR-Concept-Injection-into-Autoregressive-Models-for-Personalized-Text-to-Image-Generation"><a href="#103-CoAR-Concept-Injection-into-Autoregressive-Models-for-Personalized-Text-to-Image-Generation" class="headerlink" title="103. CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation"></a>103. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CoAR__Concept_Injection_into_Autoregressive_Models_for_Personalized_Text-to-Image_Generation.pdf">CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了CoAR框架，通过层级多模态上下文学习（Layerwise Multimodal Context Learning, LMCL）、双重先验保持损失（Dual Prior Preservation, DPP）和上下文感知自正则化（Context-Aware Self-Regularization, CASR），在完全冻结预训练参数的情况下，将个性化主体或风格概念高效注入多模态自回归（AR）文本到图像生成模型中。实验结果表明，CoAR在主体和风格定制任务上实现了更高的主体保真度、指令对齐和风格还原，且所需可训练参数少于0.1M，显著优于现有方法。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="104-EndoAgent-A-Memory-Guided-Reflective-Agent-for-Intelligent-Endoscopic-Vision-to-Decision-Reasoning"><a href="#104-EndoAgent-A-Memory-Guided-Reflective-Agent-for-Intelligent-Endoscopic-Vision-to-Decision-Reasoning" class="headerlink" title="104. EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning"></a>104. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EndoAgent__A_Memory-Guided_Reflective_Agent_for_Intelligent_Endoscopic_Vision-to-Decision_Reasoning.pdf">EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southeast University</span></p>
<p>EndoAgent提出了一个多模块、双记忆机制的智能代理框架，结合短期行动追踪和长期经验反思，支持多轮推理与工具协作，实现内镜图像分析的视觉到决策流程。通过EndoAgentBench大规模内镜基准测试，实验证明EndoAgent在细粒度视觉理解和开放式语言生成任务上均超越现有医学和通用多模态大模型，提升临床决策支持准确性和灵活性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="105-Representation-Understanding-via-Activation-Maximization"><a href="#105-Representation-Understanding-via-Activation-Maximization" class="headerlink" title="105. Representation Understanding via Activation Maximization"></a>105. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Representation_Understanding_via_Activation_Maximization.pdf">Representation Understanding via Activation Maximization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Manchester</span></p>
<p>本文提出了一种统一的特征可视化框架，基于激活最大化（Activation Maximization, AM），适用于卷积神经网络（CNNs）和视觉Transformer（ViT），能够对输出层及中间层进行可解释性分析，并通过在频域优化提升可视化的自然性和语义一致性。实验表明，该方法不仅有效揭示了不同模型架构的表征差异，还可生成结构化的对抗样本，从而兼顾模型可解释性与鲁棒性分析。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="106-Small-Large-Collaboration-Training-efficient-Concept-Personalization-for-Large-VLM-using-a-Meta-Personalized-Small-VLM"><a href="#106-Small-Large-Collaboration-Training-efficient-Concept-Personalization-for-Large-VLM-using-a-Meta-Personalized-Small-VLM" class="headerlink" title="106. Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM"></a>106. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Small-Large_Collaboration__Training-efficient_Concept_Personalization_for_Large_VLM_using_a_Meta_Per.pdf">Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出了一种高效的视觉语言模型（VLM）个性化新范式SLC（Small-Large Collaboration），利用小型VLM进行用户概念检测和结构化线索生成，大型VLM负责反思校验与最终答案生成。该方法通过元训练小模型并设计测试时反思机制，大幅降低训练成本并减少幻觉，实现对开源和闭源大模型的高效个性化，实验显示在多个基准上性能优异且易于实际部署。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="107-SUIT-Spatial-Spectral-Union-Intersection-Interaction-Network-for-Hyperspectral-Object-Tracking"><a href="#107-SUIT-Spatial-Spectral-Union-Intersection-Interaction-Network-for-Hyperspectral-Object-Tracking" class="headerlink" title="107. SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking"></a>107. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SUIT__Spatial-Spectral_Union-Intersection_Interaction_Network_for_Hyperspectral_Object_Tracking.pdf">SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Science and Technology</span></p>
<p>该论文提出了SUIT（Spatial-Spectral Union-Intersection Interaction Network），一种专为高光谱视频目标跟踪设计的空间-光谱联合交互网络。方法上，SUIT利用Transformer实现模板和搜索区域在各波段间的空间长距离关系建模，并首次引入集合论包容-排斥原理将空间交互的联合与交集区分为波段共享与波段特有信息进行融合，最后提出光谱损失以保证模板与预测区域的物质分布一致性，提升对变形、遮挡等复杂场景的鲁棒性。大规模实验证明，SUIT在多个高光谱跟踪数据集上取得了最新最优性能，显著优于现有方法。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="108-Consistent-and-Controllable-Image-Animation-with-Motion-Linear-Diffusion-Transformers"><a href="#108-Consistent-and-Controllable-Image-Animation-with-Motion-Linear-Diffusion-Transformers" class="headerlink" title="108. Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers"></a>108. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Consistent_and_Controllable_Image_Animation_with_Motion_Linear_Diffusion_Transformers.pdf">Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University</span></p>
<p>该论文提出了MiraMo框架，通过将线性注意力机制融入Transformer架构，实现了高效、外观一致且运动平滑的图像动画生成。MiraMo采用自建的文本到视频（T2V）线性Transformer作为基础，结合运动残差学习、基于DCT的噪声初始化与动态度控制，有效提升了动画的一致性和可控性，并显著减少了计算资源消耗。实验结果表明，MiraMo在外观一致性、运动平滑性及推理速度等方面均超越现有方法，并可扩展至运动迁移和视频编辑等任务。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="109-Bridging-Semantic-Logic-Gaps-A-Cognition-Inspired-Multimodal-Boundary-Preserving-Network-for-Image-Manipulation-Localization"><a href="#109-Bridging-Semantic-Logic-Gaps-A-Cognition-Inspired-Multimodal-Boundary-Preserving-Network-for-Image-Manipulation-Localization" class="headerlink" title="109. Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization"></a>109. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Bridging_Semantic_Logic_Gaps__A_Cognition-Inspired_Multimodal_Boundary-Preserving_Network_for_Image_.pdf">Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xinjiang University</span></p>
<p>该论文提出了一种认知启发的多模态边界保持网络（CMB-Net），用于图像篡改区域定位。方法上，模型结合了大语言模型（LLMs）生成的文本提示与视觉特征，通过图像-文本中心歧义模块（ITCAM）权重化文本特征，利用图像-文本交互模块（ITIM）实现细粒度融合，并引入可逆神经网络的边界解码器（RED）以无损保留篡改区域边界。实验结果表明，CMB-Net在多个公开数据集上显著优于现有SOTA方法，且文本信息能有效提升复杂场景下的篡改定位精度。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="110-Intention-Aware-Diffusion-Model-for-Pedestrian-Trajectory-Prediction"><a href="#110-Intention-Aware-Diffusion-Model-for-Pedestrian-Trajectory-Prediction" class="headerlink" title="110. Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction"></a>110. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Intention-Aware_Diffusion_Model_for_Pedestrian_Trajectory_Prediction.pdf">Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southern University of Science and Technology</span></p>
<p>该论文提出一种结合短期和长期意图的扩散模型用于行人轨迹预测：短期意图通过残差极坐标表示捕捉细粒度运动趋势，长期意图采用可学习的token端点预测器以概率方式生成多模态目标。模型还引入软掩码指导和残差噪声预测，通过在ETH、UCY和SDD数据集上实验证明，方法在ADE和FDE指标上优于现有方法，实现了更精确的轨迹预测。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="111-Large-scale-Multi-sequence-Pretraining-for-Generalizable-MRI-Analysis-in-Versatile-Clinical-Applications"><a href="#111-Large-scale-Multi-sequence-Pretraining-for-Generalizable-MRI-Analysis-in-Versatile-Clinical-Applications" class="headerlink" title="111. Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications"></a>111. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Large-scale_Multi-sequence_Pretraining_for_Generalizable_MRI_Analysis_in_Versatile_Clinical_Applicat.pdf">Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>本文提出PRISM，一种基于Swin Transformer的大规模多序列MRI自监督预训练基础模型，通过解耦解剖学不变特征与序列特异性变化，结合掩码图像重建、图像翻译、元数据预测和对比学习等多任务框架，获得对多种临床下游任务（分割、分类、回归、配准、报告生成）的强泛化能力。实验结果表明，PRISM在44项下游任务中有39项取得最优结果，有效提升了多序列MRI分析的准确性、鲁棒性和临床适用性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="112-SketchAnimator-Animate-Sketch-via-Motion-Customization-of-Text-to-Video-Diffusion-Models"><a href="#112-SketchAnimator-Animate-Sketch-via-Motion-Customization-of-Text-to-Video-Diffusion-Models" class="headerlink" title="112. SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models"></a>112. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SketchAnimator__Animate_Sketch_via_Motion_Customization_of_Text-to-Video_Diffusion_Models.pdf">SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing University of Posts and Telecommunications</span></p>
<p>该论文提出了SketchAnimator方法，实现了通过三阶段流程（外观学习、运动学习和视频先验蒸馏），利用LoRA微调和可微分贝塞尔曲线渲染，将静态手绘草图和参考视频中的运动信号结合，生成高质量且运动一致的草图动画。实验结果表明，该方法在外观保持和运动传递准确性上均优于现有方法，支持高度自定义和创意动画生成。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="113-Perceptual-Evaluation-of-GANs-and-Diffusion-Models-for-Generating-X-rays"><a href="#113-Perceptual-Evaluation-of-GANs-and-Diffusion-Models-for-Generating-X-rays" class="headerlink" title="113. Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays"></a>113. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Perceptual_Evaluation_of_GANs_and_Diffusion_Models_for_Generating_X-rays.pdf">Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Pontificia Universidad Católica de Chile</span></p>
<p>本论文提出了一种基于人类专家评估的框架，系统比较了GAN和Diffusion Models在合成胸部X光片的真实性和异常表现力，主要针对四种异常（肺不张、肺部不透明、胸腔积液、心影增大）。研究发现，Diffusion Models整体生成的图像更逼真，但在某些条件下GAN表现更优，且两者在条件准确性方面各有优势，表明现有生成模型尚未完全解决医学图像真实性问题。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="114-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning"><a href="#114-Cut2Next-Generating-Next-Shot-via-In-Context-Tuning" class="headerlink" title="114. Cut2Next: Generating Next Shot via In-Context Tuning"></a>114. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Cut2Next__Generating_Next_Shot_via_In-Context_Tuning.pdf">Cut2Next: Generating Next Shot via In-Context Tuning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong</span></p>
<p>该论文提出了Next Shot Generation（NSG）任务，旨在生成符合专业剪辑模式和严格电影连续性的高质量下一镜头。方法上，Cut2Next框架基于Diffusion Transformer，采用分层多提示策略和无参数新架构（CACI与HAM），结合大规模和精细化数据集（RawCuts与CuratedCuts），实现对多样化编辑模式的有效建模。结论显示，该方法在视觉一致性和文本对齐上显著优于现有基线，并在用户研究中获得极高偏好，尤其在剪辑模式遵循和电影连贯性方面表现突出。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="115-Learning-User-Preferences-for-Image-Generation-Models"><a href="#115-Learning-User-Preferences-for-Image-Generation-Models" class="headerlink" title="115. Learning User Preferences for Image Generation Models"></a>115. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_User_Preferences_for_Image_Generation_Model.pdf">Learning User Preferences for Image Generation Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Renmin University of China</span></p>
<p>该论文提出了一种基于多模态大语言模型（MLLM）的用户偏好学习框架，通过引入对比偏好损失和可学习偏好token，利用用户历史交互数据建模个性化视觉偏好。方法有效区分用户“喜欢”与“不喜欢”的内容，并通过偏好token捕捉用户间共享兴趣，实现分组结构和泛化能力。实验结果表明，该方法在偏好预测准确率和生成内容个性化指导方面优于现有方法，可准确识别用户审美倾向并提升生成效果。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="116-THAT-Token-wise-High-frequency-Augmentation-Transformer-for-Hyperspectral-Pansharpening"><a href="#116-THAT-Token-wise-High-frequency-Augmentation-Transformer-for-Hyperspectral-Pansharpening" class="headerlink" title="116. THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening"></a>116. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/THAT__Token-wise_High-frequency_Augmentation_Transformer_for_Hyperspectral_Pansharpening.pdf">THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">JPMorgan Chase</span></p>
<p>本文提出了一种新颖的Token-wise High-frequency Augmentation Transformer (THAT)框架，用于高光谱图像融合（pansharpening）。方法创新性地引入了Pivotal Token Selective Attention (PTSA)模块以动态筛选和聚焦于信息量大的token，并通过Multi-level Variance-aware Feed-forward Network (MVFN)增强高频细节学习。实验结果表明，THAT在多个公开数据集和不同放大倍数下均取得了优异的重建质量和效率，显著提升了高光谱影像的空间和光谱保真度。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="117-Mitigating-Biases-in-Surgical-Operating-Rooms-with-Geometry"><a href="#117-Mitigating-Biases-in-Surgical-Operating-Rooms-with-Geometry" class="headerlink" title="117. Mitigating Biases in Surgical Operating Rooms with Geometry"></a>117. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Mitigating_Biases_in_Surgical_Operating_Rooms_with_Geometry.pdf">Mitigating Biases in Surgical Operating Rooms with Geometry</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">TU Munich</span></p>
<p>该论文通过对手术室人员再识别任务中的深度神经网络进行梯度型显著性分析，发现模型容易依赖于服装等非本质视觉特征而产生偏见。作者提出利用3D点云序列编码人员身份，将几何信息（如形态和动作模式）与外观混淆因素分离，实验表明几何表示在真实临床环境下能有效提升识别准确性并减少偏见。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="118-StableAvatar-Infinite-Length-Audio-Driven-Avatar-Video-Generation"><a href="#118-StableAvatar-Infinite-Length-Audio-Driven-Avatar-Video-Generation" class="headerlink" title="118. StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation"></a>118. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/StableAvatar__Infinite-Length_Audio-Driven_Avatar_Video_Generation.pdf">StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>该论文提出StableAvatar，一种端到端视频扩散Transformer框架，实现了基于音频驱动的无限时长高质量虚拟人视频生成。方法核心在于引入Timestep-aware Audio Adapter进行步长感知的音频调制，防止潜在分布误差累积，并在推理阶段通过Audio Native Guidance提升音频同步和表情自然度，采用动态加权滑动窗口提升长视频平滑性。实验表明，StableAvatar显著优于现有方法，在身份一致性、音频同步和视频质量方面均达到新SOTA，支持长视频无明显质量衰减。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="119-VGGSounder-Audio-Visual-Evaluations-for-Foundation-Models"><a href="#119-VGGSounder-Audio-Visual-Evaluations-for-Foundation-Models" class="headerlink" title="119. VGGSounder: Audio-Visual Evaluations for Foundation Models"></a>119. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/VGGSounder__Audio-Visual_Evaluations_for_Foundation_Models.pdf">VGGSounder: Audio-Visual Evaluations for Foundation Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University of Munich</span></p>
<p>本论文提出VGGSounder，一个全面重注释的多标签音视频分类测试集，专为评估音视频基础模型设计。方法包括对VGGSound数据集进行多标签和模态（可听&#x2F;可见&#x2F;可听且可见）人工标注，添加背景音乐、画外音和静态图片等元标签，并引入“模态混淆”新指标以量化多模态输入对模型性能的负面影响。实验评测表明，现有的音视频基础模型在该数据集上的表现普遍较差，尤其在多模态输入下易出现性能下降，且模型容易偏向视觉或听觉信息。结论：VGGSounder能够更精确地揭示多模态模型的能力短板，为音视频基础模型的公平评测和未来改进提供了重要工具。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="120-SAGOnline-Segment-Any-Gaussians-Online"><a href="#120-SAGOnline-Segment-Any-Gaussians-Online" class="headerlink" title="120. SAGOnline: Segment Any Gaussians Online"></a>120. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SAGOnline__Segment_Any_Gaussians_Online.pdf">SAGOnline: Segment Any Gaussians Online</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Waterloo</span></p>
<p>SAGOnline提出了一种轻量级、零训练的实时3D高斯场分割框架，通过视频基础模型（如SAM 2）实现视图一致的2D分割，并利用GPU加速算法将2D掩码转为3D实例标签，实现高效的多对象追踪与分割。实验结果显示，在NVOS和Spin-NeRF基准上达到最新SOTA精度，并以27ms&#x2F;帧取得极快推理速度，显著优于现有方法。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="121-KARMA-Efficient-Structural-Defect-Segmentation-via-Kolmogorov-Arnold-Representation-Learning"><a href="#121-KARMA-Efficient-Structural-Defect-Segmentation-via-Kolmogorov-Arnold-Representation-Learning" class="headerlink" title="121. KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning"></a>121. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/KARMA__Efficient_Structural_Defect_Segmentation_via_Kolmogorov-Arnold_Representation_Learning.pdf">KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of New Orleans</span></p>
<p>本文提出KARMA，一种基于Kolmogorov-Arnold表示学习的高效结构性缺陷分割框架，核心创新包括参数高效的Tiny Kolmogorov-Arnold Network (TiKAN)模块、融合可分离卷积的多尺度特征金字塔结构，以及静-动态原型机制。KARMA在结构缺陷数据集上以不到百万参数实现与先进模型相当或更优的分割性能，且显著减少97%的模型参数和计算量，适合实时部署于基础设施自动巡检。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="122-REDDINO-A-FOUNDATION-MODEL-FOR-RED-BLOOD-CELL-ANALYSIS"><a href="#122-REDDINO-A-FOUNDATION-MODEL-FOR-RED-BLOOD-CELL-ANALYSIS" class="headerlink" title="122. REDDINO: A FOUNDATION MODEL FOR RED BLOOD CELL ANALYSIS"></a>122. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/RedDino__A_foundation_model_for_red_blood_cell_analysis.pdf">REDDINO: A FOUNDATION MODEL FOR RED BLOOD CELL ANALYSIS</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cagliari</span></p>
<p>本文提出RedDino，一种基于自监督学习且专为红细胞图像分析设计的基础模型，通过改进DINOv2框架并在大规模多源红细胞数据集上训练，采用特定增强和正则化策略。实验结果表明，RedDino在红细胞形态分类任务上较现有方法有显著提升，具有强泛化能力和适应不同数据源的鲁棒性，为自动化血液病诊断提供了可靠工具。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="123-MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision"><a href="#123-MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision" class="headerlink" title="123. MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision"></a>123. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MedReasoner__Reinforcement_Learning_Drives_Reasoning_Grounding_from_Clinical_Thought_to_Pixel-Level_.pdf">MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing University of Posts and Telecommunications</span></p>
<p>本文提出UMRG任务与U-MRG-14K数据集，要求模型将隐含临床问题转化为医学影像的像素级定位。方法上，MedReasoner采用模块化设计，将临床推理与分割解耦，通过强化学习优化推理模块，利用奖励函数对输出格式和空间精度进行约束，并用冻结分割专家实现空间提示到掩码的转换。实验结果显示，MedReasoner在U-MRG-14K上性能显著优于同类模型，能准确处理隐含医学问题，实现高精度定位。结论：RL方法有效提升了医学影像语义与像素级定位的解释性和泛化能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="124-Integrating-Task-Specific-and-Universal-Adapters-for-Pre-Trained-Model-based-Class-Incremental-Learning"><a href="#124-Integrating-Task-Specific-and-Universal-Adapters-for-Pre-Trained-Model-based-Class-Incremental-Learning" class="headerlink" title="124. Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning"></a>124. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Integrating_Task-Specific_and_Universal_Adapters_for_Pre-Trained_Model-based_Class-Incremental_Learn.pdf">Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, Nanjing University</span></p>
<p>本文提出TUNA方法，将任务专属adapter与通用adapter结合，用于预训练模型的类增量学习。方法包括利用正交损失训练任务专属adapter，通过熵机制选择最合适的adapter，并融合各adapter权重构建通用adapter，推理时集成专属与通用adapter预测。实验结果在CIFAR100、ImageNet-R&#x2F;A、ObjectNet等数据集上优于现有方法，有效减缓灾难性遗忘并提升分类准确率。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="125-FantasyStyle-Controllable-Stylized-Distillation-for-3D-Gaussian-Splatting"><a href="#125-FantasyStyle-Controllable-Stylized-Distillation-for-3D-Gaussian-Splatting" class="headerlink" title="125. FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting"></a>125. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FantasyStyle__Controllable_Stylized_Distillation_for_3D_Gaussian_Splatting.pdf">FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computing and Artificial Intelligence, Shanghai University of Finance and Economics</span></p>
<p>该论文提出了FantasyStyle，一种基于3D高斯泼溅的可控风格迁移框架，核心方法包括多视角频率一致性（MVFC）和可控风格化蒸馏（CSD），分别通过3D频域滤波提升风格跨视角一致性，并引入负向引导抑制内容泄漏，仅用扩散模型蒸馏实现2D到3D的灵活风格迁移。实验表明该方法在多场景、多风格下风格迁移质量和内容保持性均优于现有方法。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="126-Hyperspectral-Imaging"><a href="#126-Hyperspectral-Imaging" class="headerlink" title="126. Hyperspectral Imaging"></a>126. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hyperspectral_Imaging.pdf">Hyperspectral Imaging</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southeast University</span></p>
<p>本文系统综述了高光谱成像（HSI）的物理原理、传感器架构及数据采集、校准和处理流程，重点介绍了降维、分类、光谱解混等传统与深度学习分析方法，并总结了其在环境观测、农业、医学、工业检测等领域的代表性应用。结论指出，随着传感器小型化、自监督学习和基础模型的发展，高光谱成像正逐步成为跨学科通用平台，未来有望实现可扩展、实时和嵌入式应用，推动科学与社会的深刻变革。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="127-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation"><a href="#127-Matrix-3D-Omnidirectional-Explorable-3D-World-Generation" class="headerlink" title="127. Matrix-3D: Omnidirectional Explorable 3D World Generation"></a>127. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Matrix-3D__Omnidirectional_Explorable_3D_World_Generation.pdf">Matrix-3D: Omnidirectional Explorable 3D World Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Skywork AI</span></p>
<p>该论文提出Matrix-3D框架，实现从单张图片或文本提示生成可全方位自由探索的3D世界。方法包括基于全景表示的轨迹引导全景视频扩散模型与全景3D重建两种流程，并自建了Matrix-Pano大规模合成全景视频数据集。结论表明，Matrix-3D在全景视频生成和3D世界重建上均达到了业界最优的性能，实现了高质量、广覆盖、可控的3D场景生成。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="128-BadPromptFL-A-Novel-Backdoor-Threat-to-Prompt-based-Federated-Learning-in-Multimodal-Models"><a href="#128-BadPromptFL-A-Novel-Backdoor-Threat-to-Prompt-based-Federated-Learning-in-Multimodal-Models" class="headerlink" title="128. BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models"></a>128. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BadPromptFL__A_Novel_Backdoor_Threat_to_Prompt-based_Federated_Learning_in_Multimodal_Models.pdf">BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Dalian University of Technology</span></p>
<p>本文提出BadPromptFL，一种针对多模态模型中基于提示（Prompt）的联邦学习的新型后门攻击方法。该方法通过恶意客户端联合优化本地后门触发器和提示嵌入，并将中毒提示注入全局聚合流程，实现无需修改模型参数即可在推理时激活后门。实验表明，BadPromptFL在多种数据集和聚合协议下均能以极高的攻击成功率（&gt;90%），且对正常任务性能影响极小，暴露了基于提示的多模态联邦学习的安全隐患。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="129-CD-TVD-Contrastive-Diffusion-for-3D-Super-Resolution-with-Scarce-High-Resolution-Time-Varying-Data"><a href="#129-CD-TVD-Contrastive-Diffusion-for-3D-Super-Resolution-with-Scarce-High-Resolution-Time-Varying-Data" class="headerlink" title="129. CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data"></a>129. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CD-TVD__Contrastive_Diffusion_for_3D_Super-Resolution_with_Scarce_High-Resolution_Time-Varying_Data.pdf">CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tianjin University</span></p>
<p>该论文提出了CD-TVD框架，通过对历史科学仿真数据的对比学习，结合带局部注意力机制的扩散超分辨率模型，实现了在高分辨率时序数据稀缺的情况下的3D超分辨率重建。方法分为预训练和微调两个阶段，预训练阶段联合对比编码模块和扩散超分辨率模块共同学习退化模式和细节特征，微调阶段仅冻结编码模块，用极少量高分辨率样本即可适应新场景，实现对所有低分辨率时刻的准确重构。实验结果在多组科学仿真数据上均优于主流方法，显著降低了对高分辨率数据的依赖，提升了科学可视化任务中的实际应用价值。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="130-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control"><a href="#130-Follow-Your-Shape-Shape-Aware-Image-Editing-via-Trajectory-Guided-Region-Control" class="headerlink" title="130. Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control"></a>130. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Follow-Your-Shape__Shape-Aware_Image_Editing_via_Trajectory-Guided_Region_Control.pdf">Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HKUST</span></p>
<p>该论文提出了Follow-Your-Shape，一个无需训练和掩码的形状感知图像编辑框架，通过轨迹发散图（TDM）动态定位可编辑区域，并结合分阶段Key-Value特征注入，实现对目标对象形状的大幅变化同时精准保留背景内容。实验表明，该方法在新设计的ReShapeBench基准上形状编辑能力和视觉一致性均优于现有方法，实现了高质量的图像编辑。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="131-MDD-Net-Multimodal-Depression-Detection-through-Mutual-Transformer"><a href="#131-MDD-Net-Multimodal-Depression-Detection-through-Mutual-Transformer" class="headerlink" title="131. MDD-Net: Multimodal Depression Detection through Mutual Transformer"></a>131. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MDD-Net__Multimodal_Depression_Detection_through_Mutual_Transformer.pdf">MDD-Net: Multimodal Depression Detection through Mutual Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Waterloo</span></p>
<p>本文提出了一种基于互变换器（Mutual Transformer, MT）的多模态抑郁检测网络MDD-Net，融合了音频和视频两种社交媒体数据。方法包括声学和视觉特征提取模块、互变换器实现跨模态相关性建模和特征融合，最终通过检测层分类。实验在D-Vlog数据集上表明，MDD-Net在F1-Score上比现有方法提升1.82%~17.37%，有效提升抑郁检测性能。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="132-Sample-aware-RandAugment-Search-free-Automatic-Data-Augmentation-for-Effective-Image-Recognition"><a href="#132-Sample-aware-RandAugment-Search-free-Automatic-Data-Augmentation-for-Effective-Image-Recognition" class="headerlink" title="132. Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition"></a>132. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sample-aware_RandAugment__Search-free_Automatic_Data_Augmentation_for_Effective_Image_Recognition.pdf">Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Chinese Academy of Sciences</span></p>
<p>本文提出了一种无需搜索的自动数据增强方法Sample-aware RandAugment (SRA)，通过设计启发式评分模块（Magnitude Instructor Score, MIS）对训练样本难度进行动态评估，并采用不对称增强策略，分别针对样本探索和精细化训练，自动调整增强策略。实验结果表明，SRA在ImageNet、CIFAR等多项视觉任务上缩小了传统搜索型和无搜索型自动增强方法的性能差距，提升了模型泛化能力，且易于集成至现有视觉训练流程。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="133-Prompt-Guided-Relational-Reasoning-for-Social-Behavior-Understanding-with-Vision-Foundation-Models"><a href="#133-Prompt-Guided-Relational-Reasoning-for-Social-Behavior-Understanding-with-Vision-Foundation-Models" class="headerlink" title="133. Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models"></a>133. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Prompt-Guided_Relational_Reasoning_for_Social_Behavior_Understanding_with_Vision_Foundation_Models.pdf">Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Stuttgart</span></p>
<p>本文提出了一种名为ProGraD的Prompt驱动型群体活动检测方法，通过在冻结的视觉基础模型（如DINOv2）上注入可学习的群体提示，并设计了两层轻量级GroupContext Transformer，实现了对社会行为中群体成员关系与集体活动的高效建模。实验证明，ProGraD在Cafe和Social-CAD两个群体活动检测基准上均取得最新最优性能，仅需约1,000万可训练参数，尤其在多组复杂场景下表现突出，同时具备较强可解释性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="134-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation"><a href="#134-Omni-Effects-Unified-and-Spatially-Controllable-Visual-Effects-Generation" class="headerlink" title="134. Omni-Effects: Unified and Spatially Controllable Visual Effects Generation"></a>134. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Omni-Effects__Unified_and_Spatially-Controllable_Visual_Effects_Generation.pdf">Omni-Effects: Unified and Spatially Controllable Visual Effects Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AMAP, Alibaba Group</span></p>
<p>Omni-Effects提出了一个统一的视觉特效（VFX）生成框架，核心创新包括基于LoRA的专家混合（LoRA-MoE）模块，用于减少多特效混训时的相互干扰，以及结合空间感知提示（SAP）与独立信息流（IIF），实现对多特效在指定区域的精确控制。实验和新建Omni-VFX数据集上的评估表明，该方法显著提升了多特效在空间和类别上的可控性与生成质量，优于现有主流方法。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="135-Score-Augmentation-for-Diffusion-Models"><a href="#135-Score-Augmentation-for-Diffusion-Models" class="headerlink" title="135. Score Augmentation for Diffusion Models"></a>135. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Score_Augmentation_for_Diffusion_Models.pdf">Score Augmentation for Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou Technology</span></p>
<p>该论文提出了Score Augmentation（ScoreAug），一种针对扩散模型的创新数据增强框架。方法核心在于对噪声数据进行线性和非线性变换，并训练扩散模型的去噪器预测变换后的目标，从而实现分数空间的等变学习，同时理论分析了不同变换空间下分数的对应关系。实验证明，ScoreAug在CIFAR-10、FFHQ、AFHQv2和ImageNet等多个图像生成基准上显著缓解了扩散模型的过拟合，提升了生成质量且与常规增强方法兼容。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="136-Diffusing-the-Blind-Spot-Uterine-MRI-Synthesis-with-Diffusion-Models"><a href="#136-Diffusing-the-Blind-Spot-Uterine-MRI-Synthesis-with-Diffusion-Models" class="headerlink" title="136. Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models"></a>136. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Diffusing_the_Blind_Spot__Uterine_MRI_Synthesis_with_Diffusion_Models.pdf">Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Friedrich–Alexander University Erlangen–Nürnberg</span></p>
<p>本论文提出了一种基于扩散模型（包括DDPM和LDM）的2D和3D子宫MRI图像合成框架，结合无条件和条件（类别标签及文本描述）生成高保真、解剖结构准确的医学影像。该方法通过多种先进感知与分布度量评估生成质量，显著提升了下游诊断模型的准确率，特别在弱监督数据稀缺场景下，合成数据在分类任务上超越真实数据。论文最终将模型和数据公开，助力妇科AI研究的数据可复现与隐私保护。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="137-Architectural-Co-Design-for-Zero-Shot-Anomaly-Detection-Decoupling-Representation-and-Dynamically-Fusing-Features-in-CLIP"><a href="#137-Architectural-Co-Design-for-Zero-Shot-Anomaly-Detection-Decoupling-Representation-and-Dynamically-Fusing-Features-in-CLIP" class="headerlink" title="137. Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP"></a>137. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Architectural_Co-Design_for_Zero-Shot_Anomaly_Detection__Decoupling_Representation_and_Dynamically_F.pdf">Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Big Data Institute, Central South University</span></p>
<p>该论文提出了ACD-CLIP框架，结合参数高效的Conv-LoRA适配器与动态融合网关（DFG），分别注入局部归纳偏置并实现基于视觉内容的灵活双向文本特征融合，从而提升大规模视觉语言模型在零样本异常检测任务中的表现。实验表明该方法在工业和医学数据集上超越了主流方法，显著提升了分割与分类的准确性和稳定性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="138-MIND-A-Noise-Adaptive-Denoising-Framework-for-Medical-Images-Integrating-Multi-Scale-Transformer"><a href="#138-MIND-A-Noise-Adaptive-Denoising-Framework-for-Medical-Images-Integrating-Multi-Scale-Transformer" class="headerlink" title="138. MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer"></a>138. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MIND__A_Noise-Adaptive_Denoising_Framework_for_Medical_Images_Integrating_Multi-Scale_Transformer.pdf">MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>本文提出了一种集成多尺度卷积与Transformer结构的医学图像自适应去噪模型MIND，引入噪声水平估计器（NLE）和噪声自适应注意力模块（NAAB），实现基于噪声感知的通道-空间注意力调节和跨模态特征融合。该方法在多模态医学图像公开数据集上系统验证，显著提升了PSNR、SSIM、LPIPS等指标，并增强了结构还原和诊断敏感性，具有强泛化能力和实际应用价值。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="139-DiTVR-Zero-Shot-Diffusion-Transformer-for-Video-Restoration"><a href="#139-DiTVR-Zero-Shot-Diffusion-Transformer-for-Video-Restoration" class="headerlink" title="139. DiTVR: Zero-Shot Diffusion Transformer for Video Restoration"></a>139. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DiTVR__Zero-Shot_Diffusion_Transformer_for_Video_Restoration.pdf">DiTVR: Zero-Shot Diffusion Transformer for Video Restoration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Wurzburg</span></p>
<p>该论文提出了DiTVR，一种基于Diffusion Transformer（DiT）的零样本视频修复框架，结合了轨迹感知注意力机制、光流引导的扩散采样器和时空邻居缓存。方法通过对光流轨迹的建模和关键层的时空注意力，有效提升了视频恢复的细节保留和时序一致性。实验表明，DiTVR在多项视频修复基准上超越现有方法，尤其在时序一致性与高保真恢复方面表现优异，无需针对特定任务训练。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="140-MambaTrans-Multimodal-Fusion-Image-Translation-via-Large-Language-Model-Priors-for-Downstream-Visual-Tasks"><a href="#140-MambaTrans-Multimodal-Fusion-Image-Translation-via-Large-Language-Model-Priors-for-Downstream-Visual-Tasks" class="headerlink" title="140. MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks"></a>140. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MambaTrans__Multimodal_Fusion_Image_Translation_via_Large_Language_Model_Priors_for_Downstream_Visua.pdf">MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Foshan University</span></p>
<p>本论文提出了一种新颖的多模态融合图像模态翻译框架MambaTrans，通过引入多模态大语言模型生成的文本描述与分割掩码，联合3D-Selective Scan Module和mask-image-text cross-attention，有效实现了融合图像到可见光分布的转换。实验结果表明，MambaTrans在无需对下游模型参数调整的情况下，能够显著提升多模态融合图像在目标检测和语义分割等下游视觉任务中的表现，且保持甚至提升了视觉质量。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="141-RSVLM-QA-A-Benchmark-Dataset-for-Remote-Sensing-Vision-Language-Model-based-Question-Answering"><a href="#141-RSVLM-QA-A-Benchmark-Dataset-for-Remote-Sensing-Vision-Language-Model-based-Question-Answering" class="headerlink" title="141. RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering"></a>141. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/RSVLM-QA__A_Benchmark_Dataset_for_Remote_Sensing_Vision_Language_Model-based_Question_Answering.pdf">RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Technology Sydney</span></p>
<p>本文提出RSVLM-QA数据集，通过集成四个主流遥感分割和检测数据集（WHU、LoveDA、INRIA、iSAID），采用GPT-4.1驱动的双轨注释生成流程，自动生成丰富的图像描述、空间关系、语义标签及多样化VQA对。该数据集包括13820张图片和162373条VQA对，覆盖六大类问题，支持复杂推理与精确计数，并通过六个主流VLM模型的基准实验，验证了其对遥感视觉语言模型理解和推理能力评测的有效性。结论表明，RSVLM-QA为遥感VQA和多模态理解研究提供了高质量资源，能有效推动领域发展。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="142-Generative-Video-Matting"><a href="#142-Generative-Video-Matting" class="headerlink" title="142. Generative Video Matting"></a>142. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Generative_Video_Matting.pdf">Generative Video Matting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Adelaide</span></p>
<p>本文提出了一种基于生成扩散模型的视频抠像方法（GVM），通过大规模合成与伪标注分割数据进行多阶段训练，并结合细粒度合成抠像数据。核心流程包括利用Stable Video Diffusion模型预训练、流匹配监督加速推理、多空间混合损失提升细节保留与时序一致性。实验结果表明，该方法在多个基准测试集上大幅优于现有方法，具备极强的泛化能力与细节恢复能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="143-Segmenting-and-Understanding-Region-aware-Semantic-Attention-for-Fine-grained-Image-Quality-Assessment-with-Large-Language-Models"><a href="#143-Segmenting-and-Understanding-Region-aware-Semantic-Attention-for-Fine-grained-Image-Quality-Assessment-with-Large-Language-Models" class="headerlink" title="143. Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models"></a>143. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Segmenting_and_Understanding__Region-aware_Semantic_Attention_for_Fine-grained_Image_Quality_Assessm.pdf">Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology</span></p>
<p>该论文提出了RSFIQA模型，用于无参考图像质量评估。方法通过SAM模型对图像进行语义分割，结合多模态大语言模型(MLLM)对每个分割区域进行多维失真分析，并通过区域语义注意力机制（RSA）融合区域特征，实现对不同语义区域的细粒度质量感知。实验结果表明RSFIQA在多项IQA数据集上均优于现有方法，具有强泛化性和鲁棒性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="144-Dream4D-Lifting-Camera-Controlled-I2V-towards-Spatiotemporally-Consistent-4D-Generation"><a href="#144-Dream4D-Lifting-Camera-Controlled-I2V-towards-Spatiotemporally-Consistent-4D-Generation" class="headerlink" title="144. Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation"></a>144. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Dream4D__Lifting_Camera-Controlled_I2V_towards_Spatiotemporally_Consistent_4D_Generation.pdf">Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong</span></p>
<p>该论文提出Dream4D框架，通过将可控图像到视频生成与神经4D重建相结合，实现了从单张图像及文本提示生成时空一致的4D场景。其方法包括通过视觉语言模型预测最佳摄像机轨迹，采用姿态条件扩散模型生成多视角视频序列，最终以4D生成器模块映射为持久4D表示。实验结果显示，Dream4D在动态场景的几何精度和时空一致性方面显著超越现有方法，有效减少了时序闪烁和形状漂移，提升了动态场景理解能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="145-Sea-Undistort-A-Dataset-for-Through-Water-Image-Restoration-in-High-Resolution-Airborne-Bathymetric-Mapping"><a href="#145-Sea-Undistort-A-Dataset-for-Through-Water-Image-Restoration-in-High-Resolution-Airborne-Bathymetric-Mapping" class="headerlink" title="145. Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping"></a>145. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Sea-Undistort__A_Dataset_for_Through-Water_Image_Restoration_in_High_Resolution_Airborne_Bathymetric.pdf">Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technische Universitat Berlin</span></p>
<p>本文提出了Sea-Undistort，一个包含1200对高分辨率水下合成图像（有&#x2F;无光学畸变）的数据集，用于训练和评估深度学习图像恢复模型。通过对ResShift、NDR-Restore及其改进版ResShift+EF（融合太阳反光掩码）进行实验，结果显示基于该数据集训练的模型在真实航拍水下影像上提升了海底可视性与测深产品质量，尤其在深水区域细节恢复和误差抑制方面效果显著。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="146-Correspondence-as-Video-Test-Time-Adaption-on-SAM2-for-Reference-Segmentation-in-the-Wild"><a href="#146-Correspondence-as-Video-Test-Time-Adaption-on-SAM2-for-Reference-Segmentation-in-the-Wild" class="headerlink" title="146. Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild"></a>146. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Correspondence_as_Video__Test-Time_Adaption_on_SAM2_for_Reference_Segmentation_in_the_Wild.pdf">Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本文提出CAV-SAM方法，通过将参考-目标图像对转化为伪视频序列，实现对SAM2模型在下游任务的轻量级适应。方法包括DBST模块利用扩散模型生成语义平滑过渡序列，以及TTGA模块通过测试时几何对齐对SAM2进行微调和额外提示，实现几何变形的自适应。实验表明，CAV-SAM在CD-FSS基准上分割性能超过现有方法约5%，在复杂真实场景如Chest X-Ray数据上表现突出，验证了方法的高效性和鲁棒性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="147-Enhancing-Small-Scale-Dataset-Expansion-with-Triplet-Connection-based-Sample-Re-Weighting"><a href="#147-Enhancing-Small-Scale-Dataset-Expansion-with-Triplet-Connection-based-Sample-Re-Weighting" class="headerlink" title="147. Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting"></a>147. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Enhancing_Small-Scale_Dataset_Expansion_with_Triplet-Connection-based_Sample_Re-Weighting.pdf">Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hunan University</span></p>
<p>本文提出TriReWeight，一种基于三元组连接的样本重加权方法，用于提升生成式数据扩增在小规模图像数据集上的效果。该方法通过理论分析三类生成图像监督方式，结合三元组损失和一致性正则化，有效降低噪声图像影响，并证明可与任意生成式数据扩增方法结合且不会降低性能。实验结果表明TriReWeight在六个自然图像和三个医学图像数据集上平均提升分类准确率，验证了方法的有效性和理论正确性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="148-DoorDet-Semi-Automated-Multi-Class-Door-Detection-Dataset-via-Object-Detection-and-Large-Language-Models"><a href="#148-DoorDet-Semi-Automated-Multi-Class-Door-Detection-Dataset-via-Object-Detection-and-Large-Language-Models" class="headerlink" title="148. DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models"></a>148. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DoorDet__Semi-Automated_Multi-Class_Door_Detection_Dataset_via_Object_Detection_and_Large_Language_M.pdf">DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Melbourne</span></p>
<p>本文提出了一种结合先进目标检测器（如Co-DETR）和大语言模型（如GPT-4.1），并引入人类校正环节的半自动化流程，用于构建包含门多类别及功能性细分的高质量门检测数据集DoorDet。实验表明，该流程显著降低人工标注成本，提升标注效率，并且所构建的数据集可有效提升复杂场景下多类门检测模型的性能和泛化能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="149-Make-Your-MoVe-Make-Your-3D-Contents-by-Adapting-Multi-View-Diffusion-Models-to-External-Editing"><a href="#149-Make-Your-MoVe-Make-Your-3D-Contents-by-Adapting-Multi-View-Diffusion-Models-to-External-Editing" class="headerlink" title="149. Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing"></a>149. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Make_Your_MoVe__Make_Your_3D_Contents_by_Adapting_Multi-View_Diffusion_Models_to_External_Editing.pdf">Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了一种无须微调的、即插即用的方案，能够将多视角扩散模型与外部2D编辑工具结合，用以生成高一致性和高质量的3D内容。其核心方法包括几何保持模块和注入开关，分别用于在单次推理中保留原始几何结构并灵活控制法线监督程度。实验表明，该方法在多种编辑工具和多视角扩散模型的组合下显著提升了多视角一致性和生成3D网格的质量。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="150-DiffVC-OSD-One-Step-Diffusion-based-Perceptual-Neural-Video-Compression-Framework"><a href="#150-DiffVC-OSD-One-Step-Diffusion-based-Perceptual-Neural-Video-Compression-Framework" class="headerlink" title="150. DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework"></a>150. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DiffVC-OSD__One-Step_Diffusion-based_Perceptual_Neural_Video_Compression_Framework.pdf">DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wuhan University</span></p>
<p>该论文提出了一种名为DiffVC-OSD的单步扩散感知神经视频压缩框架，通过将重建后的无噪声潜表示直接输入单步扩散模型，并结合时序上下文进行条件引导，显著提升视频重建的感知质量。方法还包括时序上下文适配器和端到端微调策略，实验结果表明该方法在主流数据集上实现了86.92%的码率下降和约20倍的解码速度提升，达到最新感知压缩性能。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="151-X2Edit-Revisiting-Arbitrary-Instruction-Image-Editing-through-Self-Constructed-Data-and-Task-Aware-Representation-Learning"><a href="#151-X2Edit-Revisiting-Arbitrary-Instruction-Image-Editing-through-Self-Constructed-Data-and-Task-Aware-Representation-Learning" class="headerlink" title="151. X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning"></a>151. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/X2Edit__Revisiting_Arbitrary-Instruction_Image_Editing_through_Self-Constructed_Data_and_Task-Aware_.pdf">X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">OPPO AI Center</span></p>
<p>本文提出了X2Edit Dataset，一个涵盖14类编辑任务、规模达370万的高质量图像编辑数据集，并设计了基于FLUX.1的轻量级、多专家（MoE-LoRA）和任务感知对比学习的X2Edit模型。实验表明，该方法在多个开放基准上与SOTA模型性能相当甚至优于部分主流方法，具备良好的可扩展性与插拔能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="152-Separation-and-Collaboration-Two-Level-Routing-Grouped-Mixture-of-Experts-for-Multi-Domain-Continual-Learning"><a href="#152-Separation-and-Collaboration-Two-Level-Routing-Grouped-Mixture-of-Experts-for-Multi-Domain-Continual-Learning" class="headerlink" title="152. Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning"></a>152. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Separation_and_Collaboration__Two-Level_Routing_Grouped_Mixture-of-Experts_for_Multi-Domain_Continua.pdf">Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Defense Technology</span></p>
<p>论文提出了Two-Level Routing Grouped Mixture-of-Experts (TRGE)方法，针对多域连续学习中的灾难性遗忘和前向遗忘问题。方法通过在预训练的CLIP视觉语言模型基础上，为每个任务扩展专家组并冻结历史专家组，结合组内静态专家数和组间动态路由策略，提升任务协作与抗遗忘能力；利用多模态大语言模型实现语义任务识别，并对模型输出进行动态融合以增强零样本泛化。实验结果显示，TRGE在多域任务增量和类别增量场景下均优于现有方法，且参数量更少。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="153-TAR-TVG-Enhancing-VLMs-with-Timestamp-Anchor-Constrained-Reasoning-for-Temporal-Video-Grounding"><a href="#153-TAR-TVG-Enhancing-VLMs-with-Timestamp-Anchor-Constrained-Reasoning-for-Temporal-Video-Grounding" class="headerlink" title="153. TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding"></a>153. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TAR-TVG__Enhancing_VLMs_with_Timestamp_Anchor-Constrained_Reasoning_for_Temporal_Video_Grounding.pdf">TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China University of Technology</span></p>
<p>该论文提出TAR-TVG框架，在视频理解任务中利用时间戳锚点约束模型的推理过程，实现逐步细化的时序定位。方法采用三阶段训练策略（GRPO强化学习、监督微调、再强化学习），通过插入时间戳锚点提升推理可解释性和定位精度。实验表明，TAR-TVG在Charades-STA等基准数据集取得了当前最优性能，显著提升了视频时序定位能力和推理链条的可验证性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="154-LaRender-Training-Free-Occlusion-Control-in-Image-Generation-via-Latent-Rendering"><a href="#154-LaRender-Training-Free-Occlusion-Control-in-Image-Generation-via-Latent-Rendering" class="headerlink" title="154. LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering"></a>154. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LaRender__Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering.pdf">LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent</span></p>
<p>本文提出LaRender，一种无需训练的图像生成算法，能精准控制图像中物体间的遮挡关系。方法通过在预训练扩散模型的潜空间应用体积渲染原理，结合遮挡图和对象透射率，实现物体遮挡和透明度的物理一致性控制。实验表明，LaRender在遮挡准确性上显著优于现有方法，并支持丰富的视觉效果如对象透明度和光效调节。结论：LaRender实现了高效、物理一致的遮挡和概念强度控制，无需额外训练。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="155-Breaking-Down-and-Building-Up-Mixture-of-Skill-Based-Vision-and-Language-Navigation-Agents"><a href="#155-Breaking-Down-and-Building-Up-Mixture-of-Skill-Based-Vision-and-Language-Navigation-Agents" class="headerlink" title="155. Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents"></a>155. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Breaking_Down_and_Building_Up__Mixture_of_Skill-Based_Vision-and-Language_Navigation_Agents.pdf">Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Michigan State University</span></p>
<p>该论文提出SkillNav，一种将视觉-语言导航任务拆解为可解释的原子技能（如方向调整、垂直移动、地标检测、区域识别等）的模块化框架。通过LLM引导的指令重排序和VLM驱动的路由器动态选择最合适的技能代理，实现对复杂环境和新指令的强泛化能力。SkillNav在R2R和GSA-R2R等主流基准上取得了SOTA性能，实验表明其提升了导航效率和多样场景下的泛化能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="156-Surgical-Knowledge-Rewrite-in-Compact-LLMs-An-‘Unlearn-then-Learn’-Strategy-with-IA3-for-Localized-Factual-Modulation-and-Catastrophic-Forgetting-Mitigation"><a href="#156-Surgical-Knowledge-Rewrite-in-Compact-LLMs-An-‘Unlearn-then-Learn’-Strategy-with-IA3-for-Localized-Factual-Modulation-and-Catastrophic-Forgetting-Mitigation" class="headerlink" title="156. Surgical Knowledge Rewrite in Compact LLMs: An ‘Unlearn-then-Learn’ Strategy with (IA3) for Localized Factual Modulation and Catastrophic Forgetting Mitigation"></a>156. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Surgical_Knowledge_Rewrite_in_Compact_LLMs__An_'Unlearn-then-Learn'_Strategy_with_($IA%5E3$)_for_Local.pdf">Surgical Knowledge Rewrite in Compact LLMs: An ‘Unlearn-then-Learn’ Strategy with (IA3) for Localized Factual Modulation and Catastrophic Forgetting Mitigation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>本文提出了一种针对紧凑型大语言模型（LLM）的“unlearn-then-learn”知识编辑策略，利用IA3参数高效微调技术和电路定位分析，分阶段先抑制冲突旧知识再注入新事实，有效实现局部化事实重写并显著缓解灾难性遗忘。实验结果显示，该方法可精确地将新事实融入模型（98.5%准确率），同时大幅抑制原有冲突事实（96%遗忘率），且保留了72%无关知识，为安全、可控的动态知识管理带来突破。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="157-Towards-Safer-AI-Moderation-Evaluating-LLM-Moderators-Through-a-Unified-Benchmark-Dataset-and-Advocating-a-Human-First-Approach"><a href="#157-Towards-Safer-AI-Moderation-Evaluating-LLM-Moderators-Through-a-Unified-Benchmark-Dataset-and-Advocating-a-Human-First-Approach" class="headerlink" title="157. Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach"></a>157. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Towards_Safer_AI_Moderation__Evaluating_LLM_Moderators_Through_a_Unified_Benchmark_Dataset_and_Advoc.pdf">Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fordham University</span></p>
<p>本论文提出了一个统一的人类标注数据集，涵盖49类情感、攻击性言论及偏见内容，并据此评估了主流LLM内容审核器的能力。同时，作者提出并微调了SafePhi模型（基于Phi-4、采用QLoRA），其在宏F1分数上优于OpenAI Moderator和Llama Guard。结论指出，现有LLM审核器在隐含性仇恨、性别与种族偏见检测上表现有限，强调需引入多样化数据及人类参与，以提升模型可靠性和可解释性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="158-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability"><a href="#158-ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability" class="headerlink" title="158. ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability"></a>158. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ReasonRank__Empowering_Passage_Ranking_with_Strong_Reasoning_Ability.pdf">ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Renmin University of China</span></p>
<p>该论文提出ReasonRank，一种基于大语言模型的推理强化型文本重排方法。方法包括自动合成多领域推理密集型训练数据，通过DeepSeek-R1生成高质量标签，并采用自一致性数据筛选；再通过两阶段训练框架，先用监督微调(SFT)学习推理模式，再用多视角奖励的强化学习(RL)提升排序能力。结论显示ReasonRank在BRIGHT和R2MED推理密集型信息检索基准上取得SOTA性能，效率优于现有基线，并具备较强通用性和推理能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="159-K-Dense-Analyst-Towards-Fully-Automated-Scientific-Analysis"><a href="#159-K-Dense-Analyst-Towards-Fully-Automated-Scientific-Analysis" class="headerlink" title="159. K-Dense Analyst: Towards Fully Automated Scientific Analysis"></a>159. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/K-Dense_Analyst__Towards_Fully_Automated_Scientific_Analysis.pdf">K-Dense Analyst: Towards Fully Automated Scientific Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Biostate AI</span></p>
<p>K-Dense Analyst是一种基于层级多智能体系统的科学分析平台，通过双环架构实现自主生物信息学分析，包括规划、验证和执行。该方法在BixBench开放式生物分析基准上取得了29.2%的准确率，显著超过当前最强的语言模型（如GPT-5），证明架构创新比模型规模更重要，推动了科学分析的自动化进程。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="160-From-Nodes-to-Narratives-Explaining-Graph-Neural-Networks-with-LLMs-and-Graph-Context"><a href="#160-From-Nodes-to-Narratives-Explaining-Graph-Neural-Networks-with-LLMs-and-Graph-Context" class="headerlink" title="160. From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context"></a>160. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Nodes_to_Narratives__Explaining_Graph_Neural_Networks_with_LLMs_and_Graph_Context.pdf">From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Chicago</span></p>
<p>本文提出了LOGIC，一种利用大语言模型（LLM）对图神经网络（GNN）进行解释的轻量级后验框架。方法核心为将GNN节点嵌入投影到LLM的嵌入空间，构建融合软提示和结构文本的混合提示，驱动LLM生成与模型内部表示相关的自然语言解释及解释子图。LOGIC无需额外训练，可直接部署在预训练GNN和LLM上，实验证明其在保持高保真度的同时生成更紧凑且具备可解释性的子图解释，并在人类评测中提升了可理解性和说服力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="161-Investigating-Intersectional-Bias-in-Large-Language-Models-using-Confidence-Disparities-in-Coreference-Resolution"><a href="#161-Investigating-Intersectional-Bias-in-Large-Language-Models-using-Confidence-Disparities-in-Coreference-Resolution" class="headerlink" title="161. Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution"></a>161. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Investigating_Intersectional_Bias_in_Large_Language_Models_using_Confidence_Disparities_in_Coreferen.pdf">Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Apple</span></p>
<p>该论文提出WinoIdentity数据集，通过扩展WinoBias为10类属性、25个群体标记，设计245,700个探测句，并用“Coreference Confidence Disparity”不公平性指标，系统评估五种主流LLMs在多重交叉身份下的推理与公平性。结果发现LLMs在交叉身份的指代消解任务中表现较差，存在高达40%的置信度差异，尤其对双重劣势群体偏见更严重，且模型更多依赖记忆而非真实推理，凸显现实应用中的社会伤害风险。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="162-Hide-or-Highlight-Understanding-the-Impact-of-Factuality-Expression-on-User-Trust"><a href="#162-Hide-or-Highlight-Understanding-the-Impact-of-Factuality-Expression-on-User-Trust" class="headerlink" title="162. Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust"></a>162. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hide_or_Highlight__Understanding_the_Impact_of_Factuality_Expression_on_User_Trust.pdf">Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IBM Research</span></p>
<p>本文提出并比较了五种在AI生成内容中表达事实性的方法（透明、注意、遮蔽、模糊、基线），其中遮蔽和模糊策略通过隐藏或弱化低事实性内容，有效提升了用户对AI系统的信任且不损害答案质量。实验证明，隐藏低事实性内容或将其改为模糊表达更能增强用户信任，优于高亮显示策略。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="163-SEADialogues-A-Multilingual-Culturally-Grounded-Multi-turn-Dialogue-Dataset-on-Southeast-Asian-Languages"><a href="#163-SEADialogues-A-Multilingual-Culturally-Grounded-Multi-turn-Dialogue-Dataset-on-Southeast-Asian-Languages" class="headerlink" title="163. SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages"></a>163. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SEADialogues__A_Multilingual_Culturally_Grounded_Multi-turn_Dialogue_Dataset_on_Southeast_Asian_Lang.pdf">SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">MBZUAI</span></p>
<p>本论文提出了SEADialogUES，这是一个面向东南亚八种语言、六国多文化背景的多轮、多语种、具备个性化设定的对话数据集。其方法包括模板生成、文化元素词汇化、利用大语言模型自动生成对话，并结合人工与LLM自动评测，验证数据的文化相关性、流畅性和自然度。结论显示，SEADialogUES能有效促进文化敏感、个性化对话系统的发展，且高质量的文化数据集有助于提升开源大模型性能，弥补与专有模型的差距。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="164-Membership-and-Memorization-in-LLM-Knowledge-Distillation"><a href="#164-Membership-and-Memorization-in-LLM-Knowledge-Distillation" class="headerlink" title="164. Membership and Memorization in LLM Knowledge Distillation"></a>164. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Membership_and_Memorization_in_LLM_Knowledge_Distillation.pdf">Membership and Memorization in LLM Knowledge Distillation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文系统分析了六种大型语言模型（LLM）知识蒸馏（KD）技术在隐私保护方面的风险，主要关注成员推断攻击和数据记忆泄漏。通过多模型、多任务实验，作者发现所有现有 KD 方法都会将教师模型的隐私风险传递给学生模型，但不同技术泄漏程度不同，并提出了按模型块进行精细化隐私风险分析框架。结论表明，无论采用哪种蒸馏方法，学生模型都无法彻底保护教师模型的训练数据隐私，且隐私泄漏在不同技术和模型块之间差异显著。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="165-Whisfusion-Parallel-ASR-Decoding-via-a-Diffusion-Transformer"><a href="#165-Whisfusion-Parallel-ASR-Decoding-via-a-Diffusion-Transformer" class="headerlink" title="165. Whisfusion: Parallel ASR Decoding via a Diffusion Transformer"></a>165. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Whisfusion__Parallel_ASR_Decoding_via_a_Diffusion_Transformer.pdf">Whisfusion: Parallel ASR Decoding via a Diffusion Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Seoul National University</span></p>
<p>该论文提出Whisfusion框架，将预训练Whisper语音编码器与文本扩散解码器通过轻量级Cross-Attention适配器融合，实现了非自回归、全并行的ASR解码流程，并设计了批量并行多步解码策略（PDD）。实验表明，Whisfusion在LibriSpeech上相较于Whisper-tiny提升了识别准确率（8.3% vs 9.7% WER），且在长语音段落上解码速度提升最高达2.6倍，展示了扩散式非自回归解码在高吞吐低延迟ASR系统中的可行性和高效性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="166-Trustworthy-Medical-Imaging-with-Large-Language-Models-A-Study-of-Hallucinations-Across-Modalities"><a href="#166-Trustworthy-Medical-Imaging-with-Large-Language-Models-A-Study-of-Hallucinations-Across-Modalities" class="headerlink" title="166. Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities"></a>166. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Trustworthy_Medical_Imaging_with_Large_Language_Models__A_Study_of_Hallucinations_Across_Modalities.pdf">Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Akron</span></p>
<p>本论文系统评估了大语言模型（LLM）在医学影像中的幻觉现象，涵盖影像解释（image-to-text）和影像生成（text-to-image）任务，通过定量实验分析了模型在X射线、CT和MRI等多模态影像报告生成及合成图像时存在的事实不一致和解剖错误。结论表明，当前LLM在医学影像任务中存在关键可靠性漏洞，需加强医学约束和鲁棒性以提升临床安全与信任。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="167-MultiMedEdit-A-Scenario-Aware-Benchmark-for-Evaluating-Knowledge-Editing-in-Medical-VQA"><a href="#167-MultiMedEdit-A-Scenario-Aware-Benchmark-for-Evaluating-Knowledge-Editing-in-Medical-VQA" class="headerlink" title="167. MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA"></a>167. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MultiMedEdit__A_Scenario-Aware_Benchmark_for_Evaluating_Knowledge_Editing_in_Medical_VQA.pdf">MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Aeronautics and Astronautics</span></p>
<p>该论文提出了MultiMedEdit，这是首个专为临床多模态任务设计的医学知识编辑评测基准，涵盖理解与推理两类任务，并建立了可靠性、泛化性和局部性三维度的评估框架，支持跨范式方法比较。实验表明，现有知识编辑方法在复杂医学场景下表现出泛化能力弱、长期编辑易遗忘和稳定性不足等问题。结论：MultiMedEdit揭示了当前方法的局限性，为未来开发更可靠医学知识编辑技术奠定了基础。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="168-MASteer-Multi-Agent-Adaptive-Steer-Strategy-for-End-to-End-LLM-Trustworthiness-Repair"><a href="#168-MASteer-Multi-Agent-Adaptive-Steer-Strategy-for-End-to-End-LLM-Trustworthiness-Repair" class="headerlink" title="168. MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair"></a>168. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MASteer__Multi-Agent_Adaptive_Steer_Strategy_for_End-to-End_LLM_Trustworthiness_Repair.pdf">MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出MASteer，首个基于表征工程的端到端大语言模型（LLM）可信度修复框架，包含多智能体样本生成（AutoTester）和自适应修复策略构建（AutoRepairer），实现自动化问题分析、样本构建、策略选取与推理时动态修复。实验证明MASteer在多项信任任务上显著提升LLM的真实性、公平性与安全性，且不损害模型通用能力，具备高效、可扩展和定制化的实际价值。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="169-BoRA-Towards-More-Expressive-Low-Rank-Adaptation-with-Block-Diversity"><a href="#169-BoRA-Towards-More-Expressive-Low-Rank-Adaptation-with-Block-Diversity" class="headerlink" title="169. BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity"></a>169. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BoRA__Towards_More_Expressive_Low-Rank_Adaptation_with_Block_Diversity.pdf">BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huazhong University of Science and Technology</span></p>
<p>该论文提出了Block-Diversified Low-Rank Adaptation (BoRA)方法，通过将低秩矩阵分块并引入块对角矩阵，有效提升了LoRA权重的秩和表达能力。实验结果表明，BoRA在参数数量接近的情况下，能比LoRA及其多种变体在多任务和多模型基准测试上提升2-4%的准确率。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="170-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance"><a href="#170-AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance" class="headerlink" title="170. AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance"></a>170. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AMFT__Aligning_LLM_Reasoners_by_Meta-Learning_the_Optimal_Imitation-Exploration_Balance.pdf">AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了一种单阶段大型语言模型（LLM）对齐算法AMFT，利用元学习控制器动态优化模仿（SFT）与探索（RL）之间的权重，通过元梯度联合熵启发式，实现基于隐式和显式奖励信号的自适应训练流程。实验在数学推理、多模态视觉推理和视觉-语言导航任务上均取得了SOTA表现，显著提升了模型的泛化能力和样本效率，避免灾难性遗忘和策略坍塌。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="171-CLAP-Coreference-Linked-Augmentation-for-Passage-Retrieval"><a href="#171-CLAP-Coreference-Linked-Augmentation-for-Passage-Retrieval" class="headerlink" title="171. CLAP: Coreference-Linked Augmentation for Passage Retrieval"></a>171. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CLAP__Coreference-Linked_Augmentation_for_Passage_Retrieval.pdf">CLAP: Coreference-Linked Augmentation for Passage Retrieval</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Adelaide</span></p>
<p>该论文提出了CLAP，一种基于大语言模型的检索增强框架，通过语义分块、指代消解和局部伪查询生成三步流程，将长文本结构化为多角度、无歧义的检索信号，并融合全局与细粒度相关性分数。在多种主流稠密和稀疏检索器及多领域数据集上，CLAP显著提升了检索性能，尤其在领域外泛化场景下表现优异，部分情况下超过了二阶段重排序器。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="172-CROP-Integrating-Topological-and-Spatial-Structures-via-Cross-View-Prefixes-for-Molecular-LLMs"><a href="#172-CROP-Integrating-Topological-and-Spatial-Structures-via-Cross-View-Prefixes-for-Molecular-LLMs" class="headerlink" title="172. CROP: Integrating Topological and Spatial Structures via Cross-View Prefixes for Molecular LLMs"></a>172. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CROP__Integrating_Topological_and_Spatial_Structures_via_Cross-View_Prefixes_for_Molecular_LLMs.pdf">CROP: Integrating Topological and Spatial Structures via Cross-View Prefixes for Molecular LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本文提出了一种创新的分子多模态大语言模型架构CROP，通过SMILES引导的联合重采样，将分子图（拓扑结构）和分子图像（空间结构）信息融合为固定长度前缀，有效增强LLM对分子的理解能力。实验证明，CROP在分子描述生成、IUPAC命名预测和分子性质预测等任务上显著优于现有方法，验证了多视图集成和高效结构信息利用的有效性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="173-DocRefine-An-Intelligent-Framework-for-Scientific-Document-Understanding-and-Content-Optimization-based-on-Multimodal-Large-Model-Agents"><a href="#173-DocRefine-An-Intelligent-Framework-for-Scientific-Document-Understanding-and-Content-Optimization-based-on-Multimodal-Large-Model-Agents" class="headerlink" title="173. DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents"></a>173. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DocRefine__An_Intelligent_Framework_for_Scientific_Document_Understanding_and_Content_Optimization_b.pdf">DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shangqiu University</span></p>
<p>该论文提出DocRefine框架，利用GPT-4o等先进视觉语言大模型（LVLMs），通过六大多智能体协作，实现对科学PDF文档的结构分析、多模态内容理解、指令分解、内容优化、自动总结和一致性验证。实验结果显示，DocRefine在DocEditBench数据集上显著优于现有方法，在语义一致性、布局保真和指令遵循方面均取得了领先成绩，有效提升了复杂科学文档的自动化处理能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="174-DATASETRESEARCH-Benchmarking-Agent-Systems-for-Demand-Driven-Dataset-Discovery"><a href="#174-DATASETRESEARCH-Benchmarking-Agent-Systems-for-Demand-Driven-Dataset-Discovery" class="headerlink" title="174. DATASETRESEARCH: Benchmarking Agent Systems for Demand-Driven Dataset Discovery"></a>174. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DatasetResearch__Benchmarking_Agent_Systems_for_Demand-Driven_Dataset_Discovery.pdf">DATASETRESEARCH: Benchmarking Agent Systems for Demand-Driven Dataset Discovery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了DATASETRESEARCH，这是首个系统评估AI智能体基于用户需求发现和合成数据集能力的综合基准，包括208个真实世界任务，覆盖知识型与推理型任务。通过三维评测框架，对搜索、合成和深度研究智能体在元数据对齐、Few-shot和微调等下游任务中的表现进行量化，发现当前最先进系统在高难度子集上最高仅达22%，揭示了现有方法在通用数据发现上的巨大差距。结论：现有智能体在知识任务中搜索表现优异，在推理任务中合成方法更强，但在极端“角落案例”上均表现不佳，凸显AI数据发现仍有广阔提升空间。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="175-Large-Language-Models-Do-Not-Simulate-Human-Psychology"><a href="#175-Large-Language-Models-Do-Not-Simulate-Human-Psychology" class="headerlink" title="175. Large Language Models Do Not Simulate Human Psychology"></a>175. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Large_Language_Models_Do_Not_Simulate_Human_Psychology.pdf">Large Language Models Do Not Simulate Human Psychology</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bielefeld University</span></p>
<p>本文通过理论分析和实证研究，评估了当前大模型（如GPT-4、Llama-3.1 70b及CENTAUR）是否能模拟人类心理学。作者发现，尽管在部分常规任务中LLM与人类响应高度一致，但对语义变化敏感性不足，难以准确模拟人类心理反应，结论是LLM不能替代真实人类参与者，需谨慎用于心理学实验。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="176-When-Prompt-Engineering-Meets-Software-Engineering-CNL-P-as-Natural-and-Robust-APIs-for-Human-AI-Interaction"><a href="#176-When-Prompt-Engineering-Meets-Software-Engineering-CNL-P-as-Natural-and-Robust-APIs-for-Human-AI-Interaction" class="headerlink" title="176. When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust &quot;APIs&quot; for Human-AI Interaction"></a>176. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/When_Prompt_Engineering_Meets_Software_Engineering__CNL-P_as_Natural_and_Robust__APIs''_for_Human-AI.pdf">When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust &quot;APIs&quot; for Human-AI Interaction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CSIRO’s Data61</span></p>
<p>本文提出了CNL-P（Controlled Natural Language for Prompt），融合了提示工程（PE）和软件工程（SE）最佳实践，通过精确定义语法结构和严格的语义规范，消除自然语言提示中的歧义，实现用户意图的准确表达。作者开发了NL到CNL-P自动转换工具和CNL-P linting静态分析工具，并通过大量实验验证，CNL-P可提升LLM的输出质量，其语法和语义无需额外说明即可被主流大模型有效理解，且linting工具能够高效发现和定位结构化提示中的语法与语义错误。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="177-Model-Agnostic-Sentiment-Distribution-Stability-Analysis-for-Robust-LLM-Generated-Texts-Detection"><a href="#177-Model-Agnostic-Sentiment-Distribution-Stability-Analysis-for-Robust-LLM-Generated-Texts-Detection" class="headerlink" title="177. Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection"></a>177. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Model-Agnostic_Sentiment_Distribution_Stability_Analysis_for_Robust_LLM-Generated_Texts_Detection.pdf">Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science, Shanghai Jiao Tong University</span></p>
<p>该论文提出了SentiDetect，一种基于情感分布稳定性分析的模型无关LLM生成文本检测框架。方法包括低情感重写、情感分布特征提取及稳定性分析，通过定义情感分布一致性和保持性指标，无需模型参数或监督训练即可区分LLM生成文本和人类写作。实验证明SentiDetect在五个数据集和多种LLM（如Gemini-1.5-Pro、Claude-3、GPT-4-0613、LLaMa-3.3）上F1分数最高提升超16%，且对同义改写和对抗攻击等具有更强鲁棒性。结论：SentiDetect显著提升LLM生成文本检测的准确性和鲁棒性，适用于多领域。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="178-BASIC-Boosting-Visual-Alignment-with-Intrinsic-Refined-Embeddings-in-Multimodal-Large-Language-Models"><a href="#178-BASIC-Boosting-Visual-Alignment-with-Intrinsic-Refined-Embeddings-in-Multimodal-Large-Language-Models" class="headerlink" title="178. BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models"></a>178. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BASIC__Boosting_Visual_Alignment_with_Intrinsic_Refined_Embeddings_in_Multimodal_Large_Language_Mode.pdf">BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本文提出BASIC方法，通过利用大型语言模型（LLM）浅层中的细化视觉嵌入作为监督信号，直接引导视觉投影器生成更优的初始视觉嵌入，实现视觉与文本模态的更细致对齐。该方法通过方向对齐和语义分布匹配两大优化目标，无需额外监督模型或人工标注，显著提升了多模态大模型（MLLMs）在多项主流视觉理解基准上的表现，验证了直接视觉监督的有效性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="179-MeteorPred-A-Meteorological-Multimodal-Large-Model-and-Dataset-for-Severe-Weather-Event-Prediction"><a href="#179-MeteorPred-A-Meteorological-Multimodal-Large-Model-and-Dataset-for-Severe-Weather-Event-Prediction" class="headerlink" title="179. MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction"></a>179. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MeteorPred__A_Meteorological_Multimodal_Large_Model_and_Dataset_for_Severe_Weather_Event_Prediction.pdf">MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Automation, Chinese Academy of Sciences</span></p>
<p>该论文提出了MP-Bench大规模气象多模态数据集，并基于此开发了气象多模态大模型（MMLM），该模型能够直接处理4D气象数据输入，并集成了三种自适应特征融合模块（DTGF、TGS、TGCA），分别提升模型在时间、空间和垂直压力层维度上的特征抽取与融合能力。实验表明MMLM在多项严酷天气事件预测与问答任务上显著优于现有开源和闭源方法，为自动化AI驱动的天气预报系统提供了关键进展。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="180-Highlight-All-the-Phrases-Enhancing-LLM-Transparency-through-Visual-Factuality-Indicators"><a href="#180-Highlight-All-the-Phrases-Enhancing-LLM-Transparency-through-Visual-Factuality-Indicators" class="headerlink" title="180. Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators"></a>180. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Highlight_All_the_Phrases__Enhancing_LLM_Transparency_through_Visual_Factuality_Indicators.pdf">Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IBM Research, Cambridge, MA, USA</span></p>
<p>本论文提出并系统评估了多种用于在LLM问答场景中可视化展示事实性分数的设计策略，包括全短语高亮、阈值高亮和数值标注，粒度分为词级和短语级。通过两轮用户实验发现，基于事实性分数对所有短语进行高亮的可视化设计最受用户偏好、信任度最高且能显著提升用户校验LLM输出准确性的便利性，建议开发者优先采用该设计以提升用户对LLM输出的信任与辨别能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="181-BiXSE-Improving-Dense-Retrieval-via-Probabilistic-Graded-Relevance-Distillation"><a href="#181-BiXSE-Improving-Dense-Retrieval-via-Probabilistic-Graded-Relevance-Distillation" class="headerlink" title="181. BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation"></a>181. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BiXSE__Improving_Dense_Retrieval_via_Probabilistic_Graded_Relevance_Distillation.pdf">BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universite de Montréal</span></p>
<p>该论文提出BiXSE方法，用二元交叉熵损失直接对LLM生成的连续型分级相关性标签进行训练，实现了更细粒度的监督信息注入到密集检索模型。实验表明，BiXSE在多种检索与句子嵌入基准上均优于标准InfoNCE对比学习目标，并对标签噪声表现出更强鲁棒性，是高效且可扩展的LLM知识蒸馏方案。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="182-Zero-Direction-Probing-A-Linear-Algebraic-Framework-for-Deep-Analysis-of-Large-Language-Model-Drift"><a href="#182-Zero-Direction-Probing-A-Linear-Algebraic-Framework-for-Deep-Analysis-of-Large-Language-Model-Drift" class="headerlink" title="182. Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift"></a>182. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Zero-Direction_Probing__A_Linear-Algebraic_Framework_for_Deep_Analysis_of_Large-Language-Model_Drift.pdf">Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AI Analytics</span></p>
<p>本文提出了Zero-Direction Probing (ZDP)理论框架，利用变换器层激活矩阵的左右零空间（null space）及其Fisher信息几何来刻画大语言模型（LLM）表示漂移，无需标签或输出，仅基于线性代数和信息几何工具。核心贡献包括：证明了漂移与零空间能量之间的Variance–Leak定理，提出Fisher Null-Conservation定理、低秩LoRA更新的Rank–Leak界、基于随机矩阵理论的无参数漂移阈值，以及在线零空间跟踪与漂移保证。结论指出：只要“监听沉默”（监控零空间），就能实现可验证的模型表示变化检测，理论结果直接面向实践，且所有定理经验证均可迁移。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="183-Many-Turn-Jailbreaking"><a href="#183-Many-Turn-Jailbreaking" class="headerlink" title="183. Many-Turn Jailbreaking"></a>183. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Many-Turn_Jailbreaking.pdf">Many-Turn Jailbreaking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Santa Barbara</span></p>
<p>该论文首次系统性研究了大语言模型（LLMs）在多轮对话场景下的越狱攻击问题，提出“Many-Turn Jailbreaking”概念，并创建了MTJ-Bench多轮越狱评测基准，对14个开源和1个闭源模型进行了实证测试。结果显示，一旦模型在首轮被越狱后，后续相关或无关问题均易被持续攻破，安全隐患远超以往单轮越狱，亟需社区关注和防护。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="184-MDK12-Bench-A-Comprehensive-Evaluation-of-Multimodal-Large-Language-Models-on-Multidisciplinary-Exams"><a href="#184-MDK12-Bench-A-Comprehensive-Evaluation-of-Multimodal-Large-Language-Models-on-Multidisciplinary-Exams" class="headerlink" title="184. MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams"></a>184. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MDK12-Bench__A_Comprehensive_Evaluation_of_Multimodal_Large_Language_Models_on_Multidisciplinary_Exa.pdf">MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai AI Laboratory</span></p>
<p>本文提出MDK12-Bench，一个大规模多学科、多模态基准，涵盖6个学科、141K K-12真实考试题，并以六层知识点体系组织。方法包括多维度动态评测框架和知识点参考增强生成（KP-RAG），系统评估MLLM在难度、时间、上下文和知识利用方面的泛化能力。实验发现，现有MLLM对动态扰动与新颖任务敏感，在复杂推理和知识整合上存在明显短板，KP-RAG对困难题提升有限。结论表明，MDK12-Bench有助于精准诊断多模态大模型的优势与不足，为未来更强泛化和推理能力的AI发展提供方向。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="185-Remote-Sensing-Image-Intelligent-Interpretation-with-the-Language-Centered-Perspective-Principles-Methods-and-Challenges"><a href="#185-Remote-Sensing-Image-Intelligent-Interpretation-with-the-Language-Centered-Perspective-Principles-Methods-and-Challenges" class="headerlink" title="185. Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges"></a>185. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Remote_Sensing_Image_Intelligent_Interpretation_with_the_Language-Centered_Perspective__Principles,_.pdf">Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Central South University</span></p>
<p>本文提出将遥感图像智能解读从视觉中心范式转向语言中心范式，借鉴全球工作空间理论（GWT），以大语言模型（LLM）为认知核心，实现感知、任务、知识、动作空间的统一融合。提出了基于语言的遥感解释系统理论框架，系统梳理多模态统一表达、知识关联与推理决策等技术难题，并展望自适应多模态对齐、动态知识约束下的任务理解、可信推理和自主交互等未来方向。结论认为，语言中心范式能突破传统视觉模型的认知瓶颈，推动遥感解释向具备认知进化能力的智能体发展。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="186-Technical-Report-Full-Stack-Fine-Tuning-for-the-Q-Programming-Language"><a href="#186-Technical-Report-Full-Stack-Fine-Tuning-for-the-Q-Programming-Language" class="headerlink" title="186. Technical Report: Full-Stack Fine-Tuning for the Q Programming Language"></a>186. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Technical_Report__Full-Stack_Fine-Tuning_for_the_Q_Programming_Language.pdf">Technical Report: Full-Stack Fine-Tuning for the Q Programming Language</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Morgan Stanley</span></p>
<p>本论文提出了一套完整的开源方法，实现了对大型语言模型（LLM）在Q编程语言上的适应，包括数据集构建、预训练、监督微调和强化学习等流程，显著提升了模型在Q-LeetCode基准上的代码生成能力。最终，所训练的Qwen-2.5系列模型在Q任务上均超越了GPT-4.1，最大模型比Claude Opus-4提升29.5%，为低资源领域LLM适配提供了可复用蓝图。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="187-LSDTs-LLM-Augmented-Semantic-Digital-Twins-for-Adaptive-Knowledge-Intensive-Infrastructure-Planning"><a href="#187-LSDTs-LLM-Augmented-Semantic-Digital-Twins-for-Adaptive-Knowledge-Intensive-Infrastructure-Planning" class="headerlink" title="187. LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning"></a>187. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LSDTs__LLM-Augmented_Semantic_Digital_Twins_for_Adaptive_Knowledge-Intensive_Infrastructure_Planning.pdf">LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland, College Park</span></p>
<p>本论文提出LSDTs框架，将大语言模型（LLM）与数字孪生（DT）结合，实现对基础设施规划中非结构化法规与技术文档的自动知识抽取和结构化建模，关键技术包括LLM驱动的本体提取、RDF语义图生成、推理与仿真集成，以及动态情境下的自适应优化。实验以马里兰州离岸风电场为例，展示该方法能自动集成法规约束、生成合规布局、并在极端天气情境下支持仿真和自动调优。结论认为，LSDTs可提升基础设施规划的数据驱动、法规合规与适应性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="188-PROPS-Progressively-Private-Self-alignment-of-Large-Language-Models"><a href="#188-PROPS-Progressively-Private-Self-alignment-of-Large-Language-Models" class="headerlink" title="188. PROPS: Progressively Private Self-alignment of Large Language Models"></a>188. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PROPS__Progressively_Private_Self-alignment_of_Large_Language_Models.pdf">PROPS: Progressively Private Self-alignment of Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Arizona</span></p>
<p>该论文提出了PROPS（Progressively Private Self-alignment），一种多阶段的隐私保护大语言模型（LLM）对齐算法。方法流程包括：将数据集分割为多个阶段，首阶段用随机响应机制扰动偏好标签并对模型进行初步对齐，后续阶段利用前一阶段模型对新数据进行偏好预测，并结合最大似然估计整合模型预测和扰动标签，从而进一步提升对齐性能和隐私保护。实验表明，在同等隐私预算下，PROPS在多模型和数据集上对齐质量显著优于DP-SGD和随机响应方法，尤其在高隐私需求场景下表现突出。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="189-Story-Ribbons-Reimagining-Storyline-Visualizations-with-Large-Language-Models"><a href="#189-Story-Ribbons-Reimagining-Storyline-Visualizations-with-Large-Language-Models" class="headerlink" title="189. Story Ribbons: Reimagining Storyline Visualizations with Large Language Models"></a>189. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Story_Ribbons__Reimagining_Storyline_Visualizations_with_Large_Language_Models.pdf">Story Ribbons: Reimagining Storyline Visualizations with Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harvard University</span></p>
<p>本文提出了一种基于大语言模型（LLM）的自动小说结构化信息抽取管道，结合交互式可视化系统STORY RIBBONS，实现小说&#x2F;剧本中角色、地点、主题等多维故事要素的自动提取与可视化分析。实验和用户研究表明，该系统显著降低了文学文本分析的门槛，帮助用户发现新颖故事洞察，但当前LLM在文学细节理解与歧义消解上仍有限。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="190-Fed-MobiLLM-Efficient-Federated-LLM-Fine-Tuning-over-Heterogeneous-Mobile-Devices-via-Server-Assisted-Side-Tuning"><a href="#190-Fed-MobiLLM-Efficient-Federated-LLM-Fine-Tuning-over-Heterogeneous-Mobile-Devices-via-Server-Assisted-Side-Tuning" class="headerlink" title="190. Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning"></a>190. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fed_MobiLLM__Efficient_Federated_LLM_Fine-Tuning_over_Heterogeneous_Mobile_Devices_via_Server_Assist.pdf">Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Houston</span></p>
<p>Fed MobiLLM提出了一种异步、服务器辅助的side-tuning范式，实现了在异构移动设备上的大语言模型（LLM）高效联邦微调。方法上，移动端仅保留冻结的backbone，执行前向传播并上传中间激活，服务器异步训练统一的side-network，并通过分层激活采样和跨结构特征对齐支持不同设备模型的高效协同。结论证明，该方法大幅降低了设备端内存和计算、通信消耗，并实现更快收敛，在IID和non-IID数据分布下均能保持强劲性能，适用于实际分布式移动场景。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="191-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy"><a href="#191-Democratizing-Diplomacy-A-Harness-for-Evaluating-Any-Large-Language-Model-on-Full-Press-Diplomacy" class="headerlink" title="191. Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy"></a>191. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Democratizing_Diplomacy__A_Harness_for_Evaluating_Any_Large_Language_Model_on_Full-Press_Diplomacy.pdf">Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Good Start Labs</span></p>
<p>该论文提出了首个无需微调即可让任意本地大型语言模型（LLM）完整参与Full-Press Diplomacy（全交流外交棋局）的评测框架。方法包括数据驱动的文本化博弈状态表示、自动化对局流程、批量模型对比、Critical State Analysis关键局势分析等，可高效分析模型的策略推理、外交表现、承诺与背叛等行为特征。结论表明：即使是24B参数的小型通用模型在未专门训练下也能顺利完成博弈且展现出自然涌现的战略能力，框架极大降低了战略推理类LLM研究门槛。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="192-AURA-A-Fine-Grained-Benchmark-and-Decomposed-Metric-for-Audio-Visual-Reasoning"><a href="#192-AURA-A-Fine-Grained-Benchmark-and-Decomposed-Metric-for-Audio-Visual-Reasoning" class="headerlink" title="192. AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning"></a>192. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AURA__A_Fine-Grained_Benchmark_and_Decomposed_Metric_for_Audio-Visual_Reasoning.pdf">AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland, College Park</span></p>
<p>本文提出AURA基准，专为评估音视频大模型（AV-LLMs）和全模态大模型（OLMs）在六类细粒度跨模态推理任务（如因果推理、音色&#x2F;音高推理、同步性分析等）上的推理能力。AURA结合自动化QA生成流程与新颖分解评测指标AuraScore（包括事实一致性和核心推理两项），揭示主流多模态模型在准确率与推理逻辑之间存在显著差距，推动更深入的多模态模型理解和评测。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="193-Freeze-and-Reveal-Exposing-Modality-Bias-in-Vision-Language-Models"><a href="#193-Freeze-and-Reveal-Exposing-Modality-Bias-in-Vision-Language-Models" class="headerlink" title="193. Freeze and Reveal: Exposing Modality Bias in Vision-Language Models"></a>193. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Freeze_and_Reveal__Exposing_Modality_Bias_in_Vision-Language_Models.pdf">Freeze and Reveal: Exposing Modality Bias in Vision-Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IIIT Hyderabad</span></p>
<p>本文提出了针对视觉-语言模型（VLM）中性别偏见的模态定向去偏框架，通过分别对视觉编码器和文本编码器应用Counterfactual Data Augmentation（CDA）、Task Vector方法以及新颖的数据高效去偏技术DAUDoS以量化和降低性别偏见。实验证明，CLIP模型的视觉编码器偏见更大，PaliGemma2则以文本编码器为主，通过针对性去偏可有效缩小性别差距且保持模型性能，为多模态系统公平性改进提供了方法论和评估工具。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="194-Event-Aware-Sentiment-Factors-from-LLM-Augmented-Financial-Tweets-A-Transparent-Framework-for-Interpretable-Quant-Trading"><a href="#194-Event-Aware-Sentiment-Factors-from-LLM-Augmented-Financial-Tweets-A-Transparent-Framework-for-Interpretable-Quant-Trading" class="headerlink" title="194. Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading"></a>194. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Event-Aware_Sentiment_Factors_from_LLM-Augmented_Financial_Tweets__A_Transparent_Framework_for_Inter.pdf">Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cambridge</span></p>
<p>本文提出了一种利用大型语言模型（LLM）对金融相关推文进行多标签事件分类，并结合情感强度量化构建可解释的量化交易因子。实证结果表明，LLM标注的特定事件（如谣言&#x2F;炒作、散户热议等）情感因子在多个持仓周期内具有显著且稳定的负Alpha和高信息系数，验证了社交媒体语义结构对金融预测的有效性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="195-Generative-AI-for-Strategic-Plan-Development"><a href="#195-Generative-AI-for-Strategic-Plan-Development" class="headerlink" title="195. Generative AI for Strategic Plan Development"></a>195. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Generative_AI_for_Strategic_Plan_Development.pdf">Generative AI for Strategic Plan Development</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Johns Hopkins University</span></p>
<p>本文提出了一个模块化认知模型，利用生成式人工智能（GAI）和大型语言模型（LLM）辅助政府机构战略规划开发。通过对BERTopic和非负矩阵分解（NMF）两种主题建模技术在战略计划愿景要素提取任务中的表现进行比较，结果表明两者均能覆盖全部愿景要素，但BERTopic的相关性更强，表现最佳。结论指出主题建模可有效辅助战略愿景要素的生成，BERTopic优于NMF，未来需扩展至更多模块并优化模型以实现实际应用。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="196-LET-US-Long-Event-Text-Understanding-of-Scenes"><a href="#196-LET-US-Long-Event-Text-Understanding-of-Scenes" class="headerlink" title="196. LET-US: Long Event-Text Understanding of Scenes"></a>196. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LET-US__Long_Event-Text_Understanding_of_Scenes.pdf">LET-US: Long Event-Text Understanding of Scenes</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出LET-US框架，针对事件相机产生的稀疏长序列数据，通过跨模态语义引导压缩和层次聚类，动态选取关键信息段，实现事件流与文本的对齐和理解。通过两阶段优化（先RGB视觉-文本预训练，再事件流-文本微调）以及自建百万级事件-文本对齐数据集，LET-US显著提升了长时事件流的推理、分类、定位和描述任务性能。实验结果表明，LET-US在多项事件流理解任务上均优于现有主流MLLM方法，能够高效处理百万级时间戳的长事件流。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="197-ALOPE-Adaptive-Layer-Optimization-for-Translation-Quality-Estimation-using-Large-Language-Models"><a href="#197-ALOPE-Adaptive-Layer-Optimization-for-Translation-Quality-Estimation-using-Large-Language-Models" class="headerlink" title="197. ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models"></a>197. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ALOPE__Adaptive_Layer_Optimization_for_Translation_Quality_Estimation_using_Large_Language_Models.pdf">ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Surrey</span></p>
<p>本论文提出ALOPE框架，通过在大型语言模型（LLM）的Transformer不同层插入低秩适配器（LoRA）和回归头，实现对机器翻译质量的无参考评估。该方法系统探索中间层表示并引入动态加权与多头回归策略，显著提升了低资源语言对翻译质量估算的相关性分数，优于标准微调和现有主流模型。结论指出，Transformer中间层（尤其是TL-7）具备更优的跨语言表征能力，ALOPE在GPU资源和效果上表现优秀，适合实际部署。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="198-Grounding-Natural-Language-for-Multi-agent-Decision-Making-with-Multi-agentic-LLMs"><a href="#198-Grounding-Natural-Language-for-Multi-agent-Decision-Making-with-Multi-agentic-LLMs" class="headerlink" title="198. Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs"></a>198. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Grounding_Natural_Language_for_Multi-agent_Decision-Making_with_Multi-agentic_LLMs.pdf">Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">UC Davis</span></p>
<p>本文提出了一个多智能体大语言模型（LLM）系统框架，将先进的提示工程、多模态信息处理、检索增强生成（RAG）、记忆结构和机制设计与多智能体决策算法结合。通过在经典博弈（如囚徒困境、鸡游戏等）上的实验，作者发现经微调的多智能体LLM能够促进个体和整体的高效、可解释、鲁棒的协作决策，并有效应对社会困境、信息不完全、目标不一致等复杂情况。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="199-Grounding-Multilingual-Multimodal-LLMs-With-Cultural-Knowledge"><a href="#199-Grounding-Multilingual-Multimodal-LLMs-With-Cultural-Knowledge" class="headerlink" title="199. Grounding Multilingual Multimodal LLMs With Cultural Knowledge"></a>199. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Grounding_Multilingual_Multimodal_LLMs_With_Cultural_Knowledge.pdf">Grounding Multilingual Multimodal LLMs With Cultural Knowledge</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carnegie Mellon University</span></p>
<p>本文提出CulturalGround数据集和CulturalPangea模型，通过利用Wikidata构建覆盖39种语言、42个国家、2200万条文化视觉问答的数据集，采用多阶段数据生成、LLM润色、VLM筛选，提升多模态大语言模型的文化知识和多语言能力。实验表明，CulturalPangea在多项文化相关多语言多模态基准测试上超越现有开源模型约5%，且主流视觉-语言任务性能不降，显著缩小LLM文化偏差。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="200-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems"><a href="#200-A-Comprehensive-Survey-of-Self-Evolving-AI-Agents-A-New-Paradigm-Bridging-Foundation-Models-and-Lifelong-Agentic-Systems" class="headerlink" title="200. A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems"></a>200. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Comprehensive_Survey_of_Self-Evolving_AI_Agents__A_New_Paradigm_Bridging_Foundation_Models_and_Lif.pdf">A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Glasgow</span></p>
<p>本论文系统综述了自进化AI代理的新范式，提出了统一概念框架，涵盖模型离线训练、在线适应、多智能体协同到多智能体自进化（MASE）。方法包括对LLM、提示、记忆、工具、工作流及多智能体通信优化技术的梳理，并讨论评估、安全与伦理问题。结论认为自进化AI代理能实现持续适应、增强自主性和长期学习，为动态环境下的AI系统发展提供理论与技术基础。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="201-Urbanite-A-Dataflow-Based-Framework-for-Human-AI-Interactive-Alignment-in-Urban-Visual-Analytics"><a href="#201-Urbanite-A-Dataflow-Based-Framework-for-Human-AI-Interactive-Alignment-in-Urban-Visual-Analytics" class="headerlink" title="201. Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics"></a>201. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Urbanite__A_Dataflow-Based_Framework_for_Human-AI_Interactive_Alignment_in_Urban_Visual_Analytics.pdf">Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Chicago</span></p>
<p>Urbanite提出了一种基于数据流的数据分析框架，通过集成大语言模型（LLM），支持用户以自然语言指定分析意图，进而自动生成、解释和迭代城市视觉分析的数据流流程。该方法显著降低了城市数据可视化分析的技术门槛，实现了人机协作、任务可追溯与多层级对齐，经专家实证和案例分析验证有效，能准确将高层次意图转化为可执行分析流程。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="202-Rethinking-Domain-Specific-LLM-Benchmark-Construction-A-Comprehensiveness-Compactness-Approach"><a href="#202-Rethinking-Domain-Specific-LLM-Benchmark-Construction-A-Comprehensiveness-Compactness-Approach" class="headerlink" title="202. Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach"></a>202. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Rethinking_Domain-Specific_LLM_Benchmark_Construction__A_Comprehensiveness-Compactness_Approach.pdf">Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong Polytechnic University</span></p>
<p>本文提出了COMP-COMP框架，用于领域特定大语言模型（LLM）基准测试集的自动构建，强调语义空间内的全面性（comprehensiveness）和紧凑性（compactness）原则，通过高斯核密度估计和相关性分析迭代收集与筛选知识语料和问答数据，实现高效覆盖与低冗余的基准集设计。实验证明，该方法可显著减少基准问题和语料数量（问题减少98.3%，语料减少53.6%），同时提升LLM领域任务的评测效果，并在学术领域构建了大规模基准XUBench，具备跨领域扩展性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="203-ObfusQAte-A-Proposed-Framework-to-Evaluate-LLM-Robustness-on-Obfuscated-Factual-Question-Answering"><a href="#203-ObfusQAte-A-Proposed-Framework-to-Evaluate-LLM-Robustness-on-Obfuscated-Factual-Question-Answering" class="headerlink" title="203. ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering"></a>203. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ObfusQAte__A_Proposed_Framework_to_Evaluate_LLM_Robustness_on_Obfuscated_Factual_Question_Answering.pdf">ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Indian Institute of Technology Patna</span></p>
<p>本文提出ObfusQAte技术和ObfusQA框架，通过多层次的语义混淆（命名实体间接、干扰项间接、上下文过载）系统性构造问题，全面评估主流大语言模型在应对被混淆事实问答任务时的鲁棒性。实验发现，现有LLM在复杂混淆下准确率显著下降，暴露出对间接表达和背景噪声适应能力不足，提出混淆型问题是测试与提升LLM真实推理能力的重要方向。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="204-HealthBranches-Synthesizing-Clinically-Grounded-Question-Answering-Datasets-via-Decision-Pathways"><a href="#204-HealthBranches-Synthesizing-Clinically-Grounded-Question-Answering-Datasets-via-Decision-Pathways" class="headerlink" title="204. HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways"></a>204. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HealthBranches__Synthesizing_Clinically-Grounded_Question_Answering_Datasets_via_Decision_Pathways.pdf">HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Calabria</span></p>
<p>该论文提出HealthBranches数据集，通过半自动流程从医学决策路径生成临床真实场景的医学问答数据，涵盖17类临床领域共4063个案例。方法结合结构化知识抽取、LLM辅助生成和人工审核，支持多步推理评估与RAG检索增强。结果显示结构化推理路径显著提升LLM在医学问答任务中的准确性和解释性，为医学领域可信LLM开发与评估奠定基础。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="205-Fine-Tuning-Large-Language-Models-Using-EEG-Microstate-Features-for-Mental-Workload-Assessment"><a href="#205-Fine-Tuning-Large-Language-Models-Using-EEG-Microstate-Features-for-Mental-Workload-Assessment" class="headerlink" title="205. Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment"></a>205. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fine-Tuning_Large_Language_Models_Using_EEG_Microstate_Features_for_Mental_Workload_Assessment.pdf">Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science, Technological University Dublin</span></p>
<p>本论文提出了利用EEG微状态特征对大型语言模型（LLM）进行微调的方法，以提升对认知负荷状态（休息与负荷）的判别能力。研究流程涵盖数据采集与预处理、微状态分割与回拟合、特征提取与提示工程、模型选择与微调，最终通过监督学习训练LLM基于EEG微状态特征区分用户的认知负荷状态。结果显示，经过EEG微状态特征微调后的LLM模型，其认知负荷判别准确率由4.5%大幅提升至97%，显著优于未微调模型，证明EEG微状态特征可有效增强LLM对大脑认知活动的感知与推断能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="206-MAQUA-Adaptive-Question-Asking-for-Multidimensional-Mental-Health-Screening-using-Item-Response-Theory"><a href="#206-MAQUA-Adaptive-Question-Asking-for-Multidimensional-Mental-Health-Screening-using-Item-Response-Theory" class="headerlink" title="206. MAQUA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory"></a>206. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MAQuA__Adaptive_Question-Asking_for_Multidimensional_Mental_Health_Screening_using_Item_Response_The.pdf">MAQUA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stony Brook University</span></p>
<p>本论文提出了MAQUA框架，将多输出语言建模、因子分析与多维项目反应理论（MIRT）结合，实现多维心理健康自适应问诊。该方法通过多任务建模与信息增益驱动的问题选择，显著减少所需问题数（最多减少85%），提升多维心理健康筛查效率。结论表明，MAQUA在保证准确性的同时大幅降低用户负担，适合大模型驱动的临床应用。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="207-LLM-based-Agents-for-Automated-Confounder-Discovery-and-Subgroup-Analysis-in-Causal-Inference"><a href="#207-LLM-based-Agents-for-Automated-Confounder-Discovery-and-Subgroup-Analysis-in-Causal-Inference" class="headerlink" title="207. LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference"></a>207. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LLM-based_Agents_for_Automated_Confounder_Discovery_and_Subgroup_Analysis_in_Causal_Inference.pdf">LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Sun Yat-Sen University</span></p>
<p>本文提出了一种基于大语言模型（LLM）的智能体框架，通过将LLM Agent与因果机器学习流程结合，实现自动化混杂因子发现与亚组分析。方法采用Mixture of Experts结构和迭代不确定性评估，利用RAG和知识库加强LLM推理能力，显著提升了治疗效应估计的鲁棒性并降低专家工作量。实验表明，该框架能有效缩小置信区间宽度，发现隐匿混杂因素，提高因果推断的可信度和可扩展性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="208-Can-Smaller-Large-Language-Models-Evaluate-Research-Quality"><a href="#208-Can-Smaller-Large-Language-Models-Evaluate-Research-Quality" class="headerlink" title="208. Can Smaller Large Language Models Evaluate Research Quality?"></a>208. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Can_Smaller_Large_Language_Models_Evaluate_Research_Quality_.pdf">Can Smaller Large Language Models Evaluate Research Quality?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Wolverhampton</span></p>
<p>本论文系统评估了可下载的中型开源大语言模型Gemma-3-27b-it在学术论文研究质量评分任务上的能力，与专家评分及主流云端LLM（如ChatGPT 4o&#x2F;4o-mini）进行了广泛对比。结果表明，Gemma-3-27b-it的评分与专家评分在全部学科方向均呈显著正相关，且相关强度达到ChatGPT 4o的83.8%、4o-mini的94.7%；但模型报告结构较为统一，重复评分提升有限。结论：Gemma等中型离线LLM可用于研究质量自动评估，支持高安全或低成本场景，且这种能力不是超大模型的特有涌现性质。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="209-AutoAssert-1-A-LoRA-Fine-Tuned-LLM-Model-for-Efficient-Automated-Assertion-Generation"><a href="#209-AutoAssert-1-A-LoRA-Fine-Tuned-LLM-Model-for-Efficient-Automated-Assertion-Generation" class="headerlink" title="209. AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation"></a>209. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AutoAssert_1__A_LoRA_Fine-Tuned_LLM_Model_for_Efficient_Automated_Assertion_Generation.pdf">AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Computing Technology, Chinese Academy of Sciences</span></p>
<p>本文提出了一种基于LoRA高效微调技术和Unsloth平台的轻量级大语言模型，用于自动从Verilog硬件描述语言代码中生成验证断言。实验表明，该方法在大幅降低微调计算资源消耗的同时，能够生成语法和语义均准确的断言，性能优于传统规则方法，适用于资源受限环境下的自动化测试与维护。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="210-Hallucination-as-a-Computational-Boundary-A-Hierarchy-of-Inevitability-and-the-Oracle-Escape"><a href="#210-Hallucination-as-a-Computational-Boundary-A-Hierarchy-of-Inevitability-and-the-Oracle-Escape" class="headerlink" title="210. Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape"></a>210. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Hallucination_as_a_Computational_Boundary__A_Hierarchy_of_Inevitability_and_the_Oracle_Escape.pdf">Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Changzhou University</span></p>
<p>本文将大语言模型（LLM）形式化为概率图灵机，提出了幻觉不可避免的计算层级（对角化、不可计算性、信息论边界），并通过“学习者泵引理”给出理论证明。论文首次从理论上证明了检索增强生成（RAG）作为外部“oracle”可实现绝对逃逸，并提出基于神经博弈理论的持续学习作为内部适应路径，实验证明RAG-CL混合策略在准确性和健壮性上优于单独策略。结论认为：幻觉是LLM的根本限制，但通过外部检索或内部持续学习可系统性规避，提出“计算类别对齐”作为AI安全的新原则。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="211-Efficient-Edge-LLMs-Deployment-via-Hessian-Aware-Quantization-and-CPU–GPU-Collaborative"><a href="#211-Efficient-Edge-LLMs-Deployment-via-Hessian-Aware-Quantization-and-CPU–GPU-Collaborative" class="headerlink" title="211. Efficient Edge LLMs Deployment via Hessian-Aware Quantization and CPU–GPU Collaborative"></a>211. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Efficient_Edge_LLMs_Deployment_via_HessianAware_Quantization_and_CPU_GPU_Collaborative.pdf">Efficient Edge LLMs Deployment via Hessian-Aware Quantization and CPU–GPU Collaborative</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong Polytechnic University</span></p>
<p>本文提出了一种基于Hessian感知量化（HAQ）与CPU-GPU协同推理的高效边缘端Mixture-of-Experts（MoE）大语言模型部署方案。方法首先通过自适应激活平滑及Hessian矩阵权重量化，实现激活与权重的联合8比特量化，显著缓解离群值导致的精度损失，并结合专家激活路径统计，设计专家级协同卸载与缓存机制，有效分配GPU与CPU资源，降低显存压力与推理延迟。实验表明，该方法在OPT系列和Mixtral-8×7B模型上，量化模型推理精度接近全精度，显存占用降低约60%，推理延迟和系统稳定性大幅提升，为MoE LLM在实际边缘环境下高效稳定部署提供了实用技术路径。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="212-DocR1-Evidence-Page-Guided-GRPO-for-Multi-Page-Document-Understanding"><a href="#212-DocR1-Evidence-Page-Guided-GRPO-for-Multi-Page-Document-Understanding" class="headerlink" title="212. DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding"></a>212. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DocR1__Evidence_Page-Guided_GRPO_for_Multi-Page_Document_Understanding.pdf">DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>DocR1是一种专为多页文档理解设计的多模态大语言模型（MLLM），通过提出Evidence Page-Guided GRPO（EviGRPO）强化学习框架，引入证据感知奖励机制，实现从粗到细的人类式推理：先检索相关页面，再生成答案。结合两阶段标注流程和课程学习策略，DocR1在多个多页文档理解基准上取得了最先进性能，同时保持了单页任务上的强表现。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="213-MCITlib-Multimodal-Continual-Instruction-Tuning-Library-and-Benchmark"><a href="#213-MCITlib-Multimodal-Continual-Instruction-Tuning-Library-and-Benchmark" class="headerlink" title="213. MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark"></a>213. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MCITlib__Multimodal_Continual_Instruction_Tuning_Library_and_Benchmark.pdf">MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Advanced Interdisciplinary Sciences, UCAS</span></p>
<p>MCITlib提出了一个面向多模态大语言模型的持续指令微调开源库，涵盖8种代表性算法，并在严格筛选的两个基准数据集上系统评测各种方法，有效解决信息泄露和公平比较难题。结论显示，不同方法在缓解遗忘和提升多任务性能上表现有差异，DISCO方法表现最佳，但存在参数膨胀问题，库将持续扩展以促进多模态持续学习领域研究。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="214-“Pull-or-Not-to-Pull-”-Investigating-Moral-Biases-in-Leading-Large-Language-Models-Across-Ethical-Dilemmas"><a href="#214-“Pull-or-Not-to-Pull-”-Investigating-Moral-Biases-in-Leading-Large-Language-Models-Across-Ethical-Dilemmas" class="headerlink" title="214. “Pull or Not to Pull?”: Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas"></a>214. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/_Pull_or_Not_to_Pull_''__Investigating_Moral_Biases_in_Leading_Large_Language_Models_Across_Ethical_.pdf">“Pull or Not to Pull?”: Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of New South Wales</span></p>
<p>本文系统评估了14种主流大语言模型（LLM）在27类“电车难题”下的道德决策与解释，涵盖十种伦理哲学视角，采用二阶段分步提示和多指标（决策果断性、解释一致性、与人类道德一致性、情境偏差敏感性）分析。结果显示，推理增强型模型更具决策果断性和结构化解释，但与人类共识的对齐度并不总是更高，且在某些伦理框架下（如亲情、自利、法律）偏差明显。研究发现“公平”“利他”“美德伦理”提示在决策果断、解释一致和人类一致性间达成较好平衡，建议将道德推理纳入LLM对齐评估主轴。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="215-Selection-and-Exploitation-of-High-Quality-Knowledge-from-Large-Language-Models-for-Recommendation"><a href="#215-Selection-and-Exploitation-of-High-Quality-Knowledge-from-Large-Language-Models-for-Recommendation" class="headerlink" title="215. Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation"></a>215. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Selection_and_Exploitation_of_High-Quality_Knowledge_from_Large_Language_Models_for_Recommendation.pdf">Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou Technology</span></p>
<p>论文提出了KSER框架，通过知识过滤模块（ESFNet）和嵌入空间对齐模块，有效地从大型语言模型（LLMs）中筛选和利用高质量知识用于推荐系统。实验结果表明，KSER不仅能提升推荐模型性能，还能通过extractor-only训练策略降低计算成本，验证了知识过滤与对齐模块的有效性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="216-What-One-Cannot-Two-Can-Two-Layer-Transformers-Provably-Represent-Induction-Heads-on-Any-Order-Markov-Chains"><a href="#216-What-One-Cannot-Two-Can-Two-Layer-Transformers-Provably-Represent-Induction-Heads-on-Any-Order-Markov-Chains" class="headerlink" title="216. What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains"></a>216. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/What_One_Cannot,_Two_Can__Two-Layer_Transformers_Provably_Represent_Induction_Heads_on_Any-Order_Mar.pdf">What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Massachusetts Institute of Technology</span></p>
<p>该论文提出并严格证明了：仅用两层、每层一个注意力头的Transformer即可表征任意阶的Markov链的条件k-gram模型（即归纳头任务），突破了此前需三层结构的理论下界。作者还对一阶Markov链上该结构的梯度下降学习动态进行了分析，说明浅层Transformer模型在结构化序列建模任务上也具备强大的in-context learning能力。结论表明Transformer的层数与可表达的Markov阶数关系达到了最紧致的已知表征，深化了对其in-context learning本质的理解。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="217-Adapting-LLMs-to-Time-Series-Forecasting-via-Temporal-Heterogeneity-Modeling-and-Semantic-Alignment"><a href="#217-Adapting-LLMs-to-Time-Series-Forecasting-via-Temporal-Heterogeneity-Modeling-and-Semantic-Alignment" class="headerlink" title="217. Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment"></a>217. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Adapting_LLMs_to_Time_Series_Forecasting_via_Temporal_Heterogeneity_Modeling_and_Semantic_Alignment.pdf">Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tianjin University</span></p>
<p>该论文提出了TALON框架，通过异质时序编码器和语义对齐模块，将大语言模型（LLM）适配于时间序列预测。方法包括对多变量时间序列进行结构分段、专家模型动态路由，以及用自然语言统计特征引导LLM嵌入，并通过对比学习实现时序与语言表示的细粒度对齐。实验显示TALON在七个真实世界数据集上均显著优于现有LLM及深度学习方法，MSE提升最高达11%，具备高效率和强泛化能力，尤其在零样本和跨域迁移环境下表现突出。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="218-Multi-Dimensional-Summarization-Agents-with-Context-Aware-Reasoning-over-Enterprise-Tables"><a href="#218-Multi-Dimensional-Summarization-Agents-with-Context-Aware-Reasoning-over-Enterprise-Tables" class="headerlink" title="218. Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables"></a>218. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-Dimensional_Summarization_Agents_with_Context-Aware_Reasoning_over_Enterprise_Tables.pdf">Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Amazon</span></p>
<p>该论文提出了一种基于多智能体的框架，结合大语言模型（LLM），实现对企业表格多维结构化数据的上下文感知自动摘要。方法流程包括切片、度量变异检测、上下文增强和LLM驱动生成，各智能体分工协作，提升了生成摘要的事实一致性、覆盖率和业务相关性。结果表明，该方法在Kaggle企业数据集上显著优于传统平铺式LLM和模板化NLG方法，在数据对齐、关键信息展现和决策洞察方面表现更佳。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="219-Dynamic-Benchmark-Construction-for-Evaluating-Large-Language-Models-on-Real-World-Codes"><a href="#219-Dynamic-Benchmark-Construction-for-Evaluating-Large-Language-Models-on-Real-World-Codes" class="headerlink" title="219. Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes"></a>219. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Dynamic_Benchmark_Construction_for_Evaluating_Large_Language_Models_on_Real-World_Codes.pdf">Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>该论文提出了CODE2BENCH，一个用于动态构建真实世界代码基准的自动化管道，结合了自动代码抓取、Scope Graph依赖分析和基于性质的自动化测试，有效解决了数据污染和测试覆盖率低的问题。实验表明现有主流大语言模型在自包含复杂逻辑任务和跨语言泛化上表现较弱，但在常用库交互任务表现较好，CODE2BENCH能更真实揭示模型在复杂代码生成中的不足。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="220-Improved-Personalized-Headline-Generation-via-Denoising-Fake-Interests-from-Implicit-Feedback"><a href="#220-Improved-Personalized-Headline-Generation-via-Denoising-Fake-Interests-from-Implicit-Feedback" class="headerlink" title="220. Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback"></a>220. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Improved_Personalized_Headline_Generation_via_Denoising_Fake_Interests_from_Implicit_Feedback.pdf">Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Henan Institute of Advanced Technology, Zhengzhou University</span></p>
<p>本文提出了一种新的个性化新闻标题生成框架PHG-DIF，通过双阶段过滤（基于新闻和时间）去除用户历史点击流中的噪声点击，并融合即时、演化和稳定兴趣建模模块，动态刻画用户多维兴趣，结合Breaking-News感知生成器提升事实准确性与个性化的平衡。同时，作者构建了包含用户停留时长标注的大规模基准数据集DT-PENS。实验结果表明，该方法显著减少了点击噪声对个性化生成的负面影响，提升了标题质量，在DT-PENS上取得了SOTA表现。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="221-A-Stable-and-Principled-Loss-Function-for-Direct-Language-Model-Alignment"><a href="#221-A-Stable-and-Principled-Loss-Function-for-Direct-Language-Model-Alignment" class="headerlink" title="221. A Stable and Principled Loss Function for Direct Language Model Alignment"></a>221. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Stable_and_Principled_Loss_Function_for_Direct_Language_Model_Alignment.pdf">A Stable and Principled Loss Function for Direct Language Model Alignment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出了一种新的稳定偏好优化（SPO）损失函数，用于直接对齐大型语言模型（LLM）与人类偏好。该方法通过理论分析和梯度对比，优化模型的logits差值至有限目标，避免了DPO方法下的不稳定和“奖励黑客”问题。实验表明SPO在Qwen2.5-7B和Llama-3-8B模型上均优于DPO，显著提升模型对齐质量，具有更好的稳定性和泛化能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="222-DySK-Attn-A-Framework-for-Efficient-Real-Time-Knowledge-Updating-in-Large-Language-Models-via-Dynamic-Sparse-Knowledge-Attention"><a href="#222-DySK-Attn-A-Framework-for-Efficient-Real-Time-Knowledge-Updating-in-Large-Language-Models-via-Dynamic-Sparse-Knowledge-Attention" class="headerlink" title="222. DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention"></a>222. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/DySK-Attn__A_Framework_for_Efficient,_Real-Time_Knowledge_Updating_in_Large_Language_Models_via_Dyna.pdf">DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Department of Computer Science, San Francisco State University</span></p>
<p>该论文提出DySK-Attn框架，通过动态知识图谱与稀疏知识注意力机制，实现大语言模型实时高效地融合外部最新知识。方法包括两阶段检索（粗检索+稀疏注意力选取top-k知识点）、知识向量融合进LLM生成过程，并支持知识图谱毫秒级实时更新。实验表明，DySK-Attn在时效性问答任务上显著优于RAG和模型编辑等主流方法，兼具计算效率和事实准确性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="223-Schema-Lineage-Extraction-at-Scale-Multilingual-Pipelines-Composite-Evaluation-and-Language-Model-Benchmarks"><a href="#223-Schema-Lineage-Extraction-at-Scale-Multilingual-Pipelines-Composite-Evaluation-and-Language-Model-Benchmarks" class="headerlink" title="223. Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks"></a>223. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Schema_Lineage_Extraction_at_Scale__Multilingual_Pipelines,_Composite_Evaluation,_and_Language-Model.pdf">Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Microsoft</span></p>
<p>该论文提出了一个新框架，实现对多语言企业数据管道脚本中细粒度schema lineage的自动抽取，涵盖源schema、源表、转换逻辑和聚合操作，并提出了SLiCE评测指标对结构和语义正确性进行复合评价。实验证明，随着模型规模和提示策略的提升（如链式思维），开源32B模型的性能可媲美GPT-4o等专有大模型，为企业级schema感知智能体的经济部署提供可行路径。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="224-LL3M-Large-Language-3D-Modelers"><a href="#224-LL3M-Large-Language-3D-Modelers" class="headerlink" title="224. LL3M: Large Language 3D Modelers"></a>224. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LL3M__Large_Language_3D_Modelers.pdf">LL3M: Large Language 3D Modelers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Chicago</span></p>
<p>该论文提出LL3M，一种多智能体系统，利用多种预训练大语言模型（LLM）协作生成可编辑的3D资产，通过自动编写解释性强的Blender Python代码实现3D建模和编辑。LL3M创新性地将形状生成任务转化为代码编写流程，包含规划、检索、编写、批评、验证和用户反馈六类智能体，并引入BlenderRAG数据库提升高级建模能力，实现高质量、可迭代、易于人机协作的3D资产生成和细化。结论表明，LL3M无需3D数据集训练即可实现高保真、可解释的3D资产生成，并支持用户多轮自然语言编辑，显著优于现有方法。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="225-Multi-head-Transformers-Provably-Learn-Symbolic-Multi-step-Reasoning-via-Gradient-Descent"><a href="#225-Multi-head-Transformers-Provably-Learn-Symbolic-Multi-step-Reasoning-via-Gradient-Descent" class="headerlink" title="225. Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent"></a>225. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-head_Transformers_Provably_Learn_Symbolic_Multi-step_Reasoning_via_Gradient_Descent.pdf">Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carnegie Mellon University</span></p>
<p>本文提出并理论证明了单层多头Transformer可通过梯度下降机制学习并泛化符号化多步链式推理任务，尤其在树结构上的路径查找，包括单步和复杂两阶段（先反向再正向）推理。构建性和优化分析揭示多头注意力机制可自主分工并协调完成递归路径遍历和阶段控制，且该能力能泛化到未见结构，表明Transformer学到算法规则而非仅记忆训练样本。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="226-Human-Alignment-and-Calibration-of-Inference-Time-Uncertainty-in-Large-Language-Models"><a href="#226-Human-Alignment-and-Calibration-of-Inference-Time-Uncertainty-in-Large-Language-Models" class="headerlink" title="226. Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models"></a>226. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Human-Alignment_and_Calibration_of_Inference-Time_Uncertainty_in_Large_Language_Models.pdf">Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Vanderbilt University</span></p>
<p>本文系统评估了大语言模型（LLM）推理时的不确定性度量方法，重点分析其与人类群体不确定性和模型传统校准之间的一致性。通过多种熵类不确定性指标（如choice entropy、top-k entropy、top-p entropy等）在大规模人类调查数据集和标准MMLU基准上进行对比，发现这些度量与人类不确定性高度相关，并且在模型校准方面具有统计显著性。结论指出，熵类推理时不确定性度量能较好反映人类不确定性并具备一定模型校准能力，未来可拓展至开放式问题和增强人机交互信任。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="227-Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning"><a href="#227-Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning" class="headerlink" title="227. Capabilities of GPT-5 on Multimodal Medical Reasoning"></a>227. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Capabilities_of_GPT-5_on_Multimodal_Medical_Reasoning.pdf">Capabilities of GPT-5 on Multimodal Medical Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Emory University</span></p>
<p>本论文系统评估了GPT-5在医学领域多模态推理任务上的能力，采用统一的零样本链式思维(CoT)协议，涵盖文本问答和视觉问答，对比GPT-5及多个变体与GPT-4o以及人类专家在MedQA、MedXpertQA（文本和多模态）、MMLU医学子集、USMLE自测和VQA-RAD等基准上的表现。实验结果表明：GPT-5在所有问答基准上均实现了最先进的准确率，尤其在多模态推理任务上显著超越GPT-4o和人类专家，提升幅度达20%以上，展示了其作为临床决策支持核心组件的潜力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="228-PYVERITAS-On-Verifying-Python-via-LLM-Based-Transpilation-and-Bounded-Model-Checking-for-C"><a href="#228-PYVERITAS-On-Verifying-Python-via-LLM-Based-Transpilation-and-Bounded-Model-Checking-for-C" class="headerlink" title="228. PYVERITAS: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C"></a>228. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/PyVeritas__On_Verifying_Python_via_LLM-Based_Transpilation_and_Bounded_Model_Checking_for_C.pdf">PYVERITAS: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Oxford</span></p>
<p>该论文提出了PYVERITAS框架，利用大语言模型（LLM）将Python代码高层次转译为C代码，并结合CBMC有界模型检测与MaxSAT故障定位工具实现对Python程序的形式化验证与错误定位。实验表明，部分专用代码大模型（如QWEN2.5-CODER）能在80-90%的案例中生成语义等价且可验证的C代码，实现对小型但非平凡Python程序的有效验证和可解释的错误诊断。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="229-Can-LLMs-Detect-Their-Confabulations-Estimating-Reliability-in-Uncertainty-Aware-Language-Models"><a href="#229-Can-LLMs-Detect-Their-Confabulations-Estimating-Reliability-in-Uncertainty-Aware-Language-Models" class="headerlink" title="229. Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models"></a>229. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Can_LLMs_Detect_Their_Confabulations__Estimating_Reliability_in_Uncertainty-Aware_Language_Models.pdf">Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KTH Royal Institute of Technology</span></p>
<p>本文提出了一种基于不确定性引导的探测方法，通过计算LLM输出的token级别的aleatoric和epistemic不确定性，聚合高不确定性的隐藏状态，提升对响应可靠性的预测能力。实验证明，该方法在面对正确和误导性上下文时，能更有效检测LLM输出的不可靠内容，优于直接不确定性信号，揭示了上下文对模型过度自信错误的影响。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="230-Optimal-Transport-Regularization-for-Speech-Text-Alignment-in-Spoken-Language-Models"><a href="#230-Optimal-Transport-Regularization-for-Speech-Text-Alignment-in-Spoken-Language-Models" class="headerlink" title="230. Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models"></a>230. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Optimal_Transport_Regularization_for_Speech_Text_Alignment_in_Spoken_Language_Models.pdf">Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mashang Consumer Finance Co., Ltd.</span></p>
<p>本文提出了Optimal Transport Regularization (OTReg)方法，将语音-文本对齐建模为最优传输问题，通过无参数正则化损失提升Spoken Language Models（SLMs）的训练效果和泛化能力。OTReg无需额外标签或参数，直接对齐语音与文本嵌入，实验显示该方法有效缓解模态间差距，提升多语言自动语音识别（ASR）任务的跨域泛化性能。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="231-TBAC-UniImage-Unified-Understanding-and-Generation-by-Ladder-Side-Diffusion-Tuning"><a href="#231-TBAC-UniImage-Unified-Understanding-and-Generation-by-Ladder-Side-Diffusion-Tuning" class="headerlink" title="231. TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning"></a>231. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TBAC-UniImage__Unified_Understanding_and_Generation_by_Ladder-Side_Diffusion_Tuning.pdf">TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Basic Algorithm Center, PCG, Tencent</span></p>
<p>该论文提出了TBAC-UniImage，一种通过Ladder-Side Diffusion Tuning机制深度融合多模态大语言模型（MLLM）与Diffusion Transformer（DiT）的统一多模态理解与生成模型。方法采用分层条件机制，将MLLM不同层的隐藏状态作为条件输入到DiT各层，提升理解与生成的协同能力。实验证明，该模型在GenEval、DPG-Bench和TIIF-Bench等多个基准上具有领先的统一理解与生成性能，尤其在复杂指令跟随和多模态任务上表现优异。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="232-Dual-Information-Speech-Language-Models-for-Emotional-Conversations"><a href="#232-Dual-Information-Speech-Language-Models-for-Emotional-Conversations" class="headerlink" title="232. Dual Information Speech Language Models for Emotional Conversations"></a>232. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Dual_Information_Speech_Language_Models_for_Emotional_Conversations.pdf">Dual Information Speech Language Models for Emotional Conversations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mashang Consumer Finance Co., Ltd.</span></p>
<p>该论文提出了一种高效扩展现有大型语言模型（LLM）为语音语言模型（SLM）的方法，通过设计异构双适配器结构并采用弱监督训练策略，实现了对语音中的副语言和语言信息的结构化解耦表示。实验结果表明，该方法仅需训练适配器即可在情感对话场景中高效融合副语言与语言信息，性能优越且理解上下文能力强。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="233-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches"><a href="#233-HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches" class="headerlink" title="233. HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches"></a>233. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/HierSearch__A_Hierarchical_Enterprise_Deep_Search_Framework_Integrating_Local_and_Web_Searches.pdf">HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Gaoling School of Artificial Intelligence, Renmin University of China</span></p>
<p>该论文提出了HierSearch，一种层次化多智能体深度搜索框架，集成了本地与Web知识检索，并采用层次化强化学习（HRL）训练本地与Web深度搜索代理，再由高层规划代理整合证据并给出最终答案。通过设计知识精炼模块，过滤低层代理返回的无关与幻觉证据，方法在六大领域基准上优于平坦RL和多种深度搜索及多源RAG基线，提升了多源检索与推理能力，泛化性强。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="234-Robust-Anomaly-Detection-in-O-RAN-Leveraging-LLMs-against-Data-Manipulation-Attacks"><a href="#234-Robust-Anomaly-Detection-in-O-RAN-Leveraging-LLMs-against-Data-Manipulation-Attacks" class="headerlink" title="234. Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks"></a>234. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Robust_Anomaly_Detection_in_O-RAN__Leveraging_LLMs_against_Data_Manipulation_Attacks.pdf">Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University</span></p>
<p>本文提出在5G O-RAN架构下，利用大型语言模型（LLM）进行异常检测，对抗通过Unicode字符（hypoglyphs）实施的数据操纵攻击。方法包括将LLM集成至O-RAN的Near-RT RIC xApp，采用窗口化消息处理流程，对传统ML模型易崩溃的情况进行对比，结果显示LLM不仅能稳定处理被篡改的消息且检测延迟低于0.07秒，适用于实时场景，但检测准确率仍需优化。结论：LLM对数据操纵攻击展现出更高鲁棒性，为O-RAN安全部署提供了可行方案。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="235-From-Natural-Language-to-Solver-Ready-Power-System-Optimization-An-LLM-Assisted-Validation-in-the-Loop-Framework"><a href="#235-From-Natural-Language-to-Solver-Ready-Power-System-Optimization-An-LLM-Assisted-Validation-in-the-Loop-Framework" class="headerlink" title="235. From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework"></a>235. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Natural_Language_to_Solver-Ready_Power_System_Optimization__An_LLM-Assisted,_Validation-in-the-.pdf">From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Southern California</span></p>
<p>该论文提出了一种以大语言模型（LLM）为核心的智能代理系统，能将电力系统优化的自然语言描述自动转换为紧凑、求解器可用的数学模型，并在回路中进行验证和修复。方法流程包括领域感知的参数提取、结构化模板生成、迭代校验与修复，以及GNN辅助的分支策略提升MILP求解效率。实验显示，该方法在单元组合问题上实现了100%有效建模，提升了运行时效率并保证了解的可行性与最优性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="236-MuaLLM-A-Multimodal-Large-Language-Model-Agent-for-Circuit-Design-Assistance-with-Hybrid-Contextual-Retrieval-Augmented-Generation"><a href="#236-MuaLLM-A-Multimodal-Large-Language-Model-Agent-for-Circuit-Design-Assistance-with-Hybrid-Contextual-Retrieval-Augmented-Generation" class="headerlink" title="236. MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation"></a>236. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MuaLLM__A_Multimodal_Large_Language_Model_Agent_for_Circuit_Design_Assistance_with_Hybrid_Contextual.pdf">MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Utah</span></p>
<p>本论文提出了MuaLLM，一种开源多模态大语言模型（LLM）代理，集成了基于ReAct（Reason + Act）迭代推理流程和混合检索增强生成（RAG）框架，专为电路设计文献检索与多步推理任务设计。MuaLLM通过结合稀疏&#x2F;密集检索、向量数据库、智能工具链（如自动文献抓取、数据库动态更新、原理图到网表转换）以及多模态能力（文本+图像），实现了高效、可扩展、上下文感知的电路设计问答和分析。实验结果显示，MuaLLM在自建RAG-250和Reas-100数据集上分别达到90.1%召回率和86.8%推理准确率，同时在大语境下推理速度提升1.6倍、成本降低10倍，具备高效、可扩展的电路设计辅助能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="237-BlindGuard-Safeguarding-LLM-based-Multi-Agent-Systems-under-Unknown-Attacks"><a href="#237-BlindGuard-Safeguarding-LLM-based-Multi-Agent-Systems-under-Unknown-Attacks" class="headerlink" title="237. BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks"></a>237. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/BlindGuard__Safeguarding_LLM-based_Multi-Agent_Systems_under_Unknown_Attacks.pdf">BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Jilin University</span></p>
<p>本文提出BlindGuard，一种针对LLM驱动的多智能体系统（MAS）在未知攻击下的无监督防御方法。通过设计层次化代理编码器融合个体、邻域和全局信息，并引入语义腐化引导的攻击检测器，BlindGuard无需攻击标签即可检测和隔离恶意代理，有效应对提示注入、内存污染和工具攻击等多种场景。实验表明，BlindGuard在多种MAS拓扑和攻击类型下均具有强泛化性和优越防御性能，推动了LLM多智能体系统的安全防护发展。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="238-TeamMedAgents-Enhancing-Medical-Decision-Making-of-LLMs-Through-Structured-Teamwork"><a href="#238-TeamMedAgents-Enhancing-Medical-Decision-Making-of-LLMs-Through-Structured-Teamwork" class="headerlink" title="238. TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork"></a>238. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TeamMedAgents__Enhancing_Medical_Decision-Making_of_LLMs_Through_Structured_Teamwork.pdf">TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois, Chicago</span></p>
<p>该论文提出了TeamMedAgents，一个将人类组织心理学中的团队协作理论系统化引入大语言模型（LLM）驱动的多智能体医学决策系统。方法通过模块化实现团队领导、互相监控、团队导向、共享心理模型、闭环沟通和互信六大团队机制，并在八个医学基准上通过系统消融实验验证其提升效果。结论指出，针对任务类型选择不同协作机制比全面集成更优，实验证明该方法在七个医学任务上超越现有多智能体框架，最大提高达17.6%。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="239-Assessing-LLM-Text-Detection-in-Educational-Contexts-Does-Human-Contribution-Affect-Detection"><a href="#239-Assessing-LLM-Text-Detection-in-Educational-Contexts-Does-Human-Contribution-Affect-Detection" class="headerlink" title="239. Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?"></a>239. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Assessing_LLM_Text_Detection_in_Educational_Contexts__Does_Human_Contribution_Affect_Detection_.pdf">Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bielefeld University</span></p>
<p>本文提出了GEDE数据集，涵盖900余篇学生原创与12500余篇不同贡献度下的大模型生成作文，系统评测DetectGPT、Fast-DetectGPT、Ghostbuster、RoBERTa等检测器在教育场景对不同人类-LLM协作文本的识别能力。研究发现，现有检测器在区分完全人类或完全LLM生成文本时表现良好，但对中间贡献层次表现不佳，尤其容易出现高假阳性，影响教育公平性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="240-Investigating-the-Design-Space-of-Visual-Grounding-in-Multimodal-Large-Language-Model"><a href="#240-Investigating-the-Design-Space-of-Visual-Grounding-in-Multimodal-Large-Language-Model" class="headerlink" title="240. Investigating the Design Space of Visual Grounding in Multimodal Large Language Model"></a>240. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Investigating_the_Design_Space_of_Visual_Grounding_in_Multimodal_Large_Language_Model.pdf">Investigating the Design Space of Visual Grounding in Multimodal Large Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Chicago</span></p>
<p>本文系统性研究了多模态大语言模型（MLLM）在视觉指代（Visual Grounding, VG）任务上的设计空间，围绕视觉定位范式与数据设计进行全面的消融实验，提出最优方案为归一化整数格式结合左上右下坐标与one-hot监督。结论显示，采纳这些设计能使MLLM在RefCOCO&#x2F;+&#x2F;g等基准上较LLaVA-1.5显著提升（+5.6%&#x2F;+6.9%&#x2F;+7.0%），为未来MLLM在视觉指代任务的发展提供了明确指导。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="241-AdaptFlow-Adaptive-Workflow-Optimization-via-Meta-Learning"><a href="#241-AdaptFlow-Adaptive-Workflow-Optimization-via-Meta-Learning" class="headerlink" title="241. AdaptFlow: Adaptive Workflow Optimization via Meta-Learning"></a>241. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/AdaptFlow__Adaptive_Workflow_Optimization_via_Meta-Learning.pdf">AdaptFlow: Adaptive Workflow Optimization via Meta-Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本论文提出了AdaptFlow，一种基于模型无关元学习（MAML）的自然语言驱动元学习框架，用于自适应优化LLM的agentic工作流。方法包括通过K-means任务聚类、双层优化（内循环用LLM文本反馈细化工作流、外循环聚合并反思）、以及测试时基于语义描述的快速适应。实验表明，AdaptFlow在问答、代码生成和数学推理等八个基准上显著优于手工和自动工作流优化基线，实现了跨任务和模型的强泛化能力。结论：AdaptFlow可自动构建通用且可自适应的LLM工作流，提升复杂任务表现。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="242-FEAT-A-Multi-Agent-Forensic-AI-System-with-Domain-Adapted-Large-Language-Model-for-Automated-Cause-of-Death-Analysis"><a href="#242-FEAT-A-Multi-Agent-Forensic-AI-System-with-Domain-Adapted-Large-Language-Model-for-Automated-Cause-of-Death-Analysis" class="headerlink" title="242. FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis"></a>242. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FEAT__A_Multi-Agent_Forensic_AI_System_with_Domain-Adapted_Large_Language_Model_for_Automated_Cause-.pdf">FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong University</span></p>
<p>本文提出FEAT系统，一种基于多智能体和领域自适应大语言模型（Forensic-LLM）的法医自动死因分析AI框架。系统采用模块化设计，包括任务分解的Planner、证据分析的Local Solvers、反思与记忆模块以及结论整合的Global Solver，并集成外部工具推理、层次化RAG、领域微调LLM及人类反馈。实验证明FEAT在中国六省市多机构7,748例法医案件中，显著优于同类AI系统，提升长文分析和短结论的准确性，并获得高级法医专家的高一致性认可。结论：FEAT实现了法医死因分析领域的专家级决策支持系统，可大幅提升司法流程自动化与标准化水平。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="243-Not-Yet-AlphaFold-for-the-Mind-Evaluating-Centaur-as-a-Synthetic-Participant"><a href="#243-Not-Yet-AlphaFold-for-the-Mind-Evaluating-Centaur-as-a-Synthetic-Participant" class="headerlink" title="243. Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant"></a>243. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Not_Yet_AlphaFold_for_the_Mind__Evaluating_Centaur_as_a_Synthetic_Participant.pdf">Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Osnabrück University</span></p>
<p>本文评估了经过认知实验数据微调的大语言模型Centaur在认知行为模拟中的能力，重点比较其预测性能和生成性能。实验表明，尽管Centaur在已知任务中预测准确性高，但其自主生成行为与人类行为存在系统性偏差，尚不具备作为可靠“合成参与者”或认知模型的能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="244-CATP-Contextually-Adaptive-Token-Pruning-for-Efficient-and-Enhanced-Multimodal-In-Context-Learning"><a href="#244-CATP-Contextually-Adaptive-Token-Pruning-for-Efficient-and-Enhanced-Multimodal-In-Context-Learning" class="headerlink" title="244. CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning"></a>244. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/CATP__Contextually_Adaptive_Token_Pruning_for_Efficient_and_Enhanced_Multimodal_In-Context_Learning.pdf">CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Brown University</span></p>
<p>CATP是一种针对多模态大模型（LVLMs）多模态in-context learning场景设计的无训练图像token剪枝方法，通过两阶段剪枝策略，分别在投影器和解码器早期层根据语义对齐和特征多样性，以及跨层注意力动态和语义相关性筛选最关键的图像token。实验证明CATP在四种主流LVLM和八个基准上提升效率（推理延迟平均降低10.78%）同时略微提升性能（平均提升0.6%），优于现有所有剪枝方法，有效缓解多模态冗余并增强模型泛化能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="245-Pareto-Multi-Objective-Alignment-for-Language-Models"><a href="#245-Pareto-Multi-Objective-Alignment-for-Language-Models" class="headerlink" title="245. Pareto Multi-Objective Alignment for Language Models"></a>245. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Pareto_Multi-Objective_Alignment_for_Language_Models.pdf">Pareto Multi-Objective Alignment for Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ruhr University Bochum</span></p>
<p>该论文提出了PAreto Multi-Objective Alignment (PAMA)算法，专为大语言模型（LLMs）在多目标（可能相互冲突）对齐场景下高效优化而设计。PAMA将多目标强化学习人类反馈（RLHF）问题转化为具有闭式解的凸优化问题，将复杂度从O(n²d)降至O(n)，并理论证明收敛到Pareto平衡点。实验表明，PAMA在GPT-2、LLaMA-2等模型上显著优于现有基线方法，实现了高效、稳定的多目标对齐，有助于提升LLM在多样化实际需求下的灵活性和适应性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="246-Interpreting-Fedspeak-with-Confidence-A-LLM-Based-Uncertainty-Aware-Framework-Guided-by-Monetary-Policy-Transmission-Paths"><a href="#246-Interpreting-Fedspeak-with-Confidence-A-LLM-Based-Uncertainty-Aware-Framework-Guided-by-Monetary-Policy-Transmission-Paths" class="headerlink" title="246. Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths"></a>246. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Interpreting_Fedspeak_with_Confidence__A_LLM-Based_Uncertainty-Aware_Framework_Guided_by_Monetary_Po.pdf">Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>本文提出了一种结合货币政策传导机制的领域推理与动态不确定性解码模块的LLM框架，用于解析美联储“Fedspeak”并分类其货币政策立场。方法通过金融实体关系抽取、政策传导路径推理和感知不确定性量化提升模型可解释性与预测可靠性。实验结果表明，该方法在FOMC政策立场分析任务上达到当前最优表现，并能有效识别高不确定性预测，增强实际金融决策中的模型可靠性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="247-The-Escalator-Problem-Identifying-Implicit-Motion-Blindness-in-AI-for-Accessibility"><a href="#247-The-Escalator-Problem-Identifying-Implicit-Motion-Blindness-in-AI-for-Accessibility" class="headerlink" title="247. The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility"></a>247. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/The_Escalator_Problem__Identifying_Implicit_Motion_Blindness_in_AI_for_Accessibility.pdf">The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>该论文提出了“电梯问题”，揭示了多模态大语言模型（MLLMs）在辅助视觉障碍者时无法感知连续、低信号运动（如判断自动扶梯方向）的关键缺陷，即“隐性运动盲点”。作者分析了这一问题对用户信任和安全的影响，并呼吁从静态语义识别转向更强的物理感知，建议开发以用户安全和可靠性为核心的新评测方法。结论认为，现有MLLMs在动态环境中的辅助能力存在根本性不足，亟需范式转变。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="248-WeChat-YATT-A-Simple-Scalable-and-Balanced-RLHF-Trainer"><a href="#248-WeChat-YATT-A-Simple-Scalable-and-Balanced-RLHF-Trainer" class="headerlink" title="248. WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer"></a>248. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/WeChat-YATT__A_Simple,_Scalable_and_Balanced_RLHF_Trainer.pdf">WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent</span></p>
<p>该论文提出WeChat-YATT，一种简洁且可扩展的RLHF训练框架，通过并行控制器编程模型和动态资源分配策略，实现高效的人类反馈强化学习流程。实验结果表明，WeChat-YATT显著提升了硬件利用率和训练吞吐量，已成功大规模应用于微信产品模型训练。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="249-X-evolve-Solution-space-evolution-powered-by-large-language-models"><a href="#249-X-evolve-Solution-space-evolution-powered-by-large-language-models" class="headerlink" title="249. X-evolve: Solution space evolution powered by large language models"></a>249. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/_(X_)-evolve__Solution_space_evolution_powered_by_large_language_models.pdf">X-evolve: Solution space evolution powered by large language models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>该论文提出了一种创新的LLM+EA混合优化方法X-evolve，核心思想是利用大语言模型生成可调参数的程序，通过标注tunable参数定义解空间，并结合基于得分的搜索算法（X-search）高效探索该空间。方法显著降低了LLM调用成本（减少两数量级），在cap set问题、Shannon容量问题和NP难在线装箱等多个难题中显著提升了搜索效率和结果质量，部分结果刷新了世界记录。结论是X-evolve能够更高效地解决高维优化问题，极大拓展了LLM在科学发现中的应用边界。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="250-MIMIC-Multimodal-Inversion-for-Model-Interpretation-and-Conceptualization"><a href="#250-MIMIC-Multimodal-Inversion-for-Model-Interpretation-and-Conceptualization" class="headerlink" title="250. MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization"></a>250. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MIMIC__Multimodal_Inversion_for_Model_Interpretation_and_Conceptualization.pdf">MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Twente</span></p>
<p>该论文提出了一种名为MIMIC的多模态反演框架，能通过联合视觉-语言模型（VLM）反演与特征对齐目标，在不修改模型权重的前提下，将VLM内部编码的语义概念可视化为图像。方法结合了自适应交叉熵损失、基于ViT激活的特征损失、空间及先验正则化器，实现了高语义一致性和结构连贯的概念图像生成，提升了VLM模型的可解释性。实验结果表明，MIMIC能够从不同长度文本输出中反演VLM的视觉概念，生成高保真、语义对齐的图像，展示了对VLM内部表征空间的有效解释能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="251-EvoCoT-Overcoming-the-Exploration-Bottleneck-in-Reinforcement-Learning"><a href="#251-EvoCoT-Overcoming-the-Exploration-Bottleneck-in-Reinforcement-Learning" class="headerlink" title="251. EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning"></a>251. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EvoCoT__Overcoming_the_Exploration_Bottleneck_in_Reinforcement_Learning.pdf">EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出EvoCoT，一种针对大语言模型（LLM）自进化的课程学习框架，通过两阶段链式思维（CoT）优化实现自生成并验证推理路径，随后逐步缩短推理步骤以受控扩展探索空间。实验表明，EvoCoT能帮助多种LLM在稀疏奖励下解决此前未能解决的难题，提升数学推理能力，并且优于传统RLVR及有监督微调等方法，突破探索瓶颈。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="252-UniSVG-A-Unified-Dataset-for-Vector-Graphic-Understanding-and-Generation-with-Multimodal-Large-Language-Models"><a href="#252-UniSVG-A-Unified-Dataset-for-Vector-Graphic-Understanding-and-Generation-with-Multimodal-Large-Language-Models" class="headerlink" title="252. UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models"></a>252. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/UniSVG__A_Unified_Dataset_for_Vector_Graphic_Understanding_and_Generation_with_Multimodal_Large_Lang.pdf">UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出了UniSVG，这是首个面向多模态大语言模型（MLLM）训练与评测的统一SVG数据集，包含52.8万条数据，涵盖图像到SVG、文本到SVG及SVG理解等多种任务。实验表明，利用UniSVG微调的开源MLLM在SVG生成与理解任务上超过了包括GPT-4V在内的主流封闭模型，显著提升了模型的SVG处理能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="253-LoSemB-Logic-Guided-Semantic-Bridging-for-Inductive-Tool-Retrieval"><a href="#253-LoSemB-Logic-Guided-Semantic-Bridging-for-Inductive-Tool-Retrieval" class="headerlink" title="253. LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval"></a>253. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/LoSemB__Logic-Guided_Semantic_Bridging_for_Inductive_Tool_Retrieval.pdf">LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong Polytechnic University</span></p>
<p>本文提出了一种面向大语言模型工具检索任务的逻辑信息引导语义桥接框架LoSemB。该方法包含逻辑特征提取与迁移的嵌入对齐模块，有效缓解未见工具的分布偏移，并通过关系增强检索机制结合逻辑约束与图嵌入相似性，提高对未见工具的检索准确率。大量实验结果表明，LoSemB在归纳（inductive）与传导（transductive）两种设置下均显著优于现有方法，且无需对新工具重新训练，具备强泛化和实用性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="254-Semantic-Caching-for-Low-Cost-LLM-Serving-From-Offline-Learning-to-Online-Adaptation"><a href="#254-Semantic-Caching-for-Low-Cost-LLM-Serving-From-Offline-Learning-to-Online-Adaptation" class="headerlink" title="254. Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation"></a>254. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Semantic_Caching_for_Low-Cost_LLM_Serving__From_Offline_Learning_to_Online_Adaptation.pdf">Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carnegie Mellon University</span></p>
<p>本论文提出一种面向大语言模型（LLM）推理低成本服务的语义缓存框架，从离线学习到在线自适应，系统性地建模了不确定环境下的缓存淘汰问题。方法包括设计超模逆向贪心、CUCB-SC和CLCB-SC-LS等算法，分别针对已知参数、离线数据学习和在线自适应场景，理论上给出近最优逼近、有限样本次优界和次线性遗憾上界。实验表明，所提方法在各种缓存大小和查询分布下显著优于基线，缓存切换数和运行时均大幅降低。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="255-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks"><a href="#255-GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks" class="headerlink" title="255. GLiClass: Generalist Lightweight Model for Sequence Classification Tasks"></a>255. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/GLiClass__Generalist_Lightweight_Model_for_Sequence_Classification_Tasks.pdf">GLiClass: Generalist Lightweight Model for Sequence Classification Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Knowledgator Engineering</span></p>
<p>GLiClass adapts the GLiNER architecture into a uni-encoder transformer model for efficient multi-label sequence classification, enabling joint text-label encoding and rich inter-label interactions. It achieves state-of-the-art accuracy and throughput on standard benchmarks, scales favorably with label count, excels in zero-shot and few-shot settings, and introduces PPO-based reinforcement learning for data-sparse scenarios; GLiClass surpasses cross-encoder baselines in accuracy-latency trade-off while maintaining practical deployment flexibility.</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="256-Multi-Turn-Jailbreaks-Are-Simpler-Than-They-Seem"><a href="#256-Multi-Turn-Jailbreaks-Are-Simpler-Than-They-Seem" class="headerlink" title="256. Multi-Turn Jailbreaks Are Simpler Than They Seem"></a>256. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-Turn_Jailbreaks_Are_Simpler_Than_They_Seem.pdf">Multi-Turn Jailbreaks Are Simpler Than They Seem</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Imperial College London</span></p>
<p>本文提出了一种自动化多轮越狱攻击流水线，并在GPT-4、Claude、Gemini等主流大模型上系统评测，发现所谓多轮越狱的成功本质上等价于多次单轮攻击采样，并无额外复杂性优势。结论指出，现有安全评估低估了攻击成功率，多轮策略仅相当于更多尝试，且同一厂商模型脆弱性高度相关，需关注根本鲁棒性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="257-Beyond-Single-A-Data-Selection-Principle-for-LLM-Alignment-via-Fine-Grained-Preference-Signals"><a href="#257-Beyond-Single-A-Data-Selection-Principle-for-LLM-Alignment-via-Fine-Grained-Preference-Signals" class="headerlink" title="257. Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals"></a>257. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Beyond_Single__A_Data_Selection_Principle_for_LLM_Alignment_via_Fine-Grained_Preference_Signals.pdf">Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>该论文提出了一种基于细粒度偏好信号的数据选择方法用于大语言模型（LLM）对齐。核心方法包括推导多偏好直接优化（DMPO）目标，提出偏好分歧（PD）项，利用PD值选择高一致性子集用于标准偏好优化（DPO）训练，并通过奖励模型估算和长度偏置修正实现高效筛选。实验证明，该方法在UltraFeedback数据集上显著优于传统整体偏好和oracle方法，提升训练效率且无需复杂标注，实现更鲁棒的LLM对齐。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="258-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization"><a href="#258-Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization" class="headerlink" title="258. Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization"></a>258. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Klear-Reasoner__Advancing_Reasoning_Capability_via_Gradient-Preserving_Clipping_Policy_Optimization.pdf">Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou Technology</span></p>
<p>本论文提出Klear-Reasoner，通过高质量链式思维监督微调（long CoT SFT）和梯度保留剪切策略优化（GPPO）提升数学与代码推理能力。GPPO在强化学习阶段保留所有token的梯度信息，结合数据过滤、软奖励和SFT监督，有效提升模型探索性与收敛速度，最终在AIME和LiveCodeBench等多个基准上超越同规模SOTA模型。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="259-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment"><a href="#259-Learning-to-Align-Aligning-to-Learn-A-Unified-Approach-for-Self-Optimized-Alignment" class="headerlink" title="259. Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment"></a>259. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_to_Align,_Aligning_to_Learn__A_Unified_Approach_for_Self-Optimized_Alignment.pdf">Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AntGroup</span></p>
<p>本文提出了GRAO（Group Relative Alignment Optimization）框架，通过多样本生成、组内相对优势加权和参考感知参数更新，将SFT与RL的优点结合，实现高效自适应对齐。实验证明GRAO在复杂人类对齐任务上相较SFT、DPO、PPO和GRPO分别提升57.7%、17.65%、7.95%和5.18%，并显著加速收敛，适用于多种大型语言模型架构。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="260-CHIMERA-HARNESSING-MULTI-AGENT-LLMS-FOR-AUTOMATIC-INSIDER-THREAT-SIMULATION"><a href="#260-CHIMERA-HARNESSING-MULTI-AGENT-LLMS-FOR-AUTOMATIC-INSIDER-THREAT-SIMULATION" class="headerlink" title="260. CHIMERA: HARNESSING MULTI-AGENT LLMS FOR AUTOMATIC INSIDER THREAT SIMULATION"></a>260. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Chimera__Harnessing_Multi-Agent_LLMs_for_Automatic_Insider_Threat_Simulation.pdf">CHIMERA: HARNESSING MULTI-AGENT LLMS FOR AUTOMATIC INSIDER THREAT SIMULATION</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Singapore Management University</span></p>
<p>该论文提出了Chimera，一个基于多智能体大语言模型（LLM）的自动化框架，用于模拟企业内部员工的正常与恶意行为，并自动生成日志，解决高质量内部威胁检测（ITD）数据稀缺的问题。Chimera通过自定义员工角色和多阶段任务流程，模拟真实企业运作，生成覆盖15种攻击场景、6类日志模态的ChimeraLog数据集，并经专家评估验证其真实性和多样性。实验显示ChimeraLog数据更具挑战性，能促进ITD模型泛化能力提升。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="261-Semantic-Enhanced-Time-Series-Forecasting-via-Large-Language-Models"><a href="#261-Semantic-Enhanced-Time-Series-Forecasting-via-Large-Language-Models" class="headerlink" title="261. Semantic-Enhanced Time-Series Forecasting via Large Language Models"></a>261. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Semantic-Enhanced_Time-Series_Forecasting_via_Large_Language_Models.pdf">Semantic-Enhanced Time-Series Forecasting via Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology Beijing</span></p>
<p>该论文提出了SE-LLM框架，通过引入Temporal-Semantic Cross-Correlation (TSCC)模块和Time-Adapter插件，有效融合时序数据与LLM的语义空间，提升了LLM对时间序列的理解和预测能力。实验结果显示，SE-LLM在多项公开数据集上的预测性能均超越现有SOTA方法，且具备更高的泛化能力和计算效率。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="262-1-2-3-Check-Enhancing-Contextual-Privacy-in-LLM-via-Multi-Agent-Reasoning"><a href="#262-1-2-3-Check-Enhancing-Contextual-Privacy-in-LLM-via-Multi-Agent-Reasoning" class="headerlink" title="262. 1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning"></a>262. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/1-2-3_Check__Enhancing_Contextual_Privacy_in_LLM_via_Multi-Agent_Reasoning.pdf">1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carnegie Mellon University</span></p>
<p>该论文提出了一种多智能体框架，将LLM的隐私推理任务分解为事件抽取、分类、总结三个子任务，各Agent专注于特定子环节，通过控制信息流和引入校验环节，有效减轻单Agent过载问题。实验表明，该多Agent体系在ConfAIde和PrivacyLens基准上显著降低了隐私泄露（分别减少18%和19%），且保证了公共内容的完整性，优于单Agent方案，凸显了模块化信息流设计在复杂隐私场景下的优势。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="263-Designing-a-Feedback-Driven-Decision-Support-System-for-Dynamic-Student-Intervention"><a href="#263-Designing-a-Feedback-Driven-Decision-Support-System-for-Dynamic-Student-Intervention" class="headerlink" title="263. Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention"></a>263. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Designing_a_Feedback-Driven_Decision_Support_System_for_Dynamic_Student_Intervention.pdf">Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">WeAreGenius Research Institute</span></p>
<p>该论文提出了一种反馈驱动的决策支持系统，用于动态学生干预，核心方法是利用闭环架构和LightGBM回归模型，通过增量式在线学习不断吸收学生干预后的新数据，实现模型的自我迭代和优化。实验表明，系统可自动整合教师输入的新成绩，显著提升预测准确度（RMSE降低10.7%），且对学生干预效果有一致、可解释的正向调整，证明模型能泛化干预成功模式，推动教育决策智能化和人本化。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="264-QuProFS-An-Evolutionary-Training-free-Approach-to-Efficient-Quantum-Feature-Map-Search"><a href="#264-QuProFS-An-Evolutionary-Training-free-Approach-to-Efficient-Quantum-Feature-Map-Search" class="headerlink" title="264. QuProFS: An Evolutionary Training-free Approach to Efficient Quantum Feature Map Search"></a>264. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/QuProFS__An_Evolutionary_Training-free_Approach_to_Efficient_Quantum_Feature_Map_Search.pdf">QuProFS: An Evolutionary Training-free Approach to Efficient Quantum Feature Map Search</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Tokyo</span></p>
<p>该论文提出了QuProFS，一种基于进化算法的训练无关量子特征映射高效搜索方法。方法通过硬件约束感知的量子电路采样、KTA过滤和多代理训练无关评价指标（如表达性、可训练性、硬件鲁棒性等），结合非线性排序聚合与进化算子（层扩展+门剪枝）实现高效、鲁棒的量子特征映射搜索。实验结果显示，QuProFS在多种经典与量子数据集上准确率高，且架构搜索速度达到现有方法2倍，真实量子硬件上表现鲁棒，显著降低了计算成本。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="265-SEF-MK-Speaker-Embedding-Free-Voice-Anonymization-through-Multi-k-means-Quantization"><a href="#265-SEF-MK-Speaker-Embedding-Free-Voice-Anonymization-through-Multi-k-means-Quantization" class="headerlink" title="265. SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means Quantization"></a>265. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/SEF-MK__Speaker-Embedding-Free_Voice_Anonymization_through_Multi-k-means_Quantization.pdf">SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means Quantization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Duke Kunshan University</span></p>
<p>该论文提出了一种全新的说话人嵌入无关（Speaker-Embedding-Free, SEF）语音匿名化方法SEF-MK，利用WavLM自监督表示、多个k-means量化模型和Conformer-HiFi-GAN解码器。方法流程包括：首先使用WavLM提取语音特征，然后在多个基于不同说话人子集训练的k-means模型中随机选择一个对特征量化，最后通过Conformer和HiFi-GAN重构匿名化语音。实验表明，多k-means模型能更好地保留语义和情感内容，同时在应对攻击者时提升了攻击多样性但也加大隐私风险，整体在隐私保护和语音可用性之间取得新的平衡。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="266-A-Stage-Aware-Mixture-of-Experts-Framework-for-Neurodegenerative-Disease-Progression-Modelling"><a href="#266-A-Stage-Aware-Mixture-of-Experts-Framework-for-Neurodegenerative-Disease-Progression-Modelling" class="headerlink" title="266. A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling"></a>266. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Stage-Aware_Mixture_of_Experts_Framework_for_Neurodegenerative_Disease_Progression_Modelling.pdf">A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University College London</span></p>
<p>该论文提出了一种阶段感知的专家混合（MoE）框架，结合病理学模型、非均匀图神经扩散模型（IGND）和本地化神经反应模块，并采用时序注意机制动态调节各专家对疾病进展的贡献。结果显示该模型在阿尔茨海默病tau蛋白传播的长期预测中优于传统方法，揭示了疾病不同阶段主导机制的变化，并提供了更具解释性的病理学见解。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="267-Multi-Level-Service-Performance-Forecasting-via-Spatiotemporal-Graph-Neural-Networks"><a href="#267-Multi-Level-Service-Performance-Forecasting-via-Spatiotemporal-Graph-Neural-Networks" class="headerlink" title="267. Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks"></a>267. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-Level_Service_Performance_Forecasting_via_Spatiotemporal_Graph_Neural_Networks.pdf">Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent Author</span></p>
<p>该论文提出了一种基于时空图神经网络（STGNN）的分布式多级服务性能预测方法，通过将系统在不同时间片的状态建模为图结构，结合服务节点的运行特征和服务调用关系，利用图卷积网络提取结构依赖，结合门控循环单元（GRU）建模时序动态，并引入时间编码机制提升对非平稳序列的表达能力，整体端到端训练实现高精度性能指标回归。实验结果表明，该方法在大规模真实数据集上相较主流基线在MAE、RMSE和R²等指标上表现优异，具备较强的泛化性和鲁棒性，能够适应不同窗口长度和并发负载，适用于智能运维和资源调度等实际场景。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="268-Improving-Real-Time-Concept-Drift-Detection-using-a-Hybrid-Transformer-Autoencoder-Framework"><a href="#268-Improving-Real-Time-Concept-Drift-Detection-using-a-Hybrid-Transformer-Autoencoder-Framework" class="headerlink" title="268. Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework"></a>268. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Improving_Real-Time_Concept_Drift_Detection_using_a_Hybrid_Transformer-Autoencoder_Framework.pdf">Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Vellore Institute of Technology (VIT-AP)</span></p>
<p>该论文提出了一种结合Transformer与Autoencoder的混合框架，用于实时检测数据流中的概念漂移。方法包括利用统计指标（PSI、JSD）、重构误差、预测不确定性、规则违规及分类器错误趋势，通过CatBoost分类器和复合Trust Score提升检测灵敏度与解释性。实验结果表明，该框架在合成航空乘客数据集上比主流方法更早、更敏感地检测到漂移，并增强了模型解释能力，适用于动态环境下的机器学习系统监控。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="269-Balancing-Privacy-and-Efficiency-Music-Information-Retrieval-via-Additive-Homomorphic-Encryption"><a href="#269-Balancing-Privacy-and-Efficiency-Music-Information-Retrieval-via-Additive-Homomorphic-Encryption" class="headerlink" title="269. Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption"></a>269. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Balancing_Privacy_and_Efficiency__Music_Information_Retrieval_via_Additive_Homomorphic_Encryption.pdf">Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent Researcher</span></p>
<p>该论文提出了一种基于加法同态加密（AHE）的音乐信息检索方法，通过结构化的分块内积与加权层次内积实现对高维音乐向量的高效隐私保护相似性搜索。实验结果显示，相较于全同态加密（FHE），AHE方案在检索速度和内存占用方面更优，有效平衡了安全性与实用性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="270-TurboBias-Universal-ASR-Context-Biasing-powered-by-GPU-accelerated-Phrase-Boosting-Tree"><a href="#270-TurboBias-Universal-ASR-Context-Biasing-powered-by-GPU-accelerated-Phrase-Boosting-Tree" class="headerlink" title="270. TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree"></a>270. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TurboBias__Universal_ASR_Context-Biasing_powered_by_GPU-accelerated_Phrase-Boosting_Tree.pdf">TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">NVIDIA</span></p>
<p>本论文提出了一种通用ASR上下文偏置框架GPU-PB，基于GPU加速的短语提升树和改进的权重分布，可高效支持CTC、RNN-T和Attention Encoder-Decoder等主流声学模型，并适用于贪婪和束搜索解码。实验证明，该方法在提升关键词识别准确率（F-score）和整体识别性能（WER）方面均优于现有开源方法，且推理速度损耗极低，支持高达2万条短语的扩展。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="271-TLCCSP-A-Scalable-Framework-for-Enhancing-Time-Series-Forecasting-with-Time-Lagged-Cross-Correlations"><a href="#271-TLCCSP-A-Scalable-Framework-for-Enhancing-Time-Series-Forecasting-with-Time-Lagged-Cross-Correlations" class="headerlink" title="271. TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations"></a>271. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/TLCCSP__A_Scalable_Framework_for_Enhancing_Time_Series_Forecasting_with_Time-Lagged_Cross-Correlatio.pdf">TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, Beijing Normal University</span></p>
<p>该论文提出了TLCCSP框架，通过设计Sequence Shifted Dynamic Time Warping (SSDTW) 算法自动发现时间序列间的时滞相关性，并引入对比学习编码器高效近似SSDTW距离，极大提升了多领域（气象、金融、房地产）时间序列预测的准确性和可扩展性。实验表明，TLCCSP在多个数据集上显著降低了预测误差，且对比学习方法将计算耗时减少约99%，有效支持大规模实时预测。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="272-UniMove-A-Unified-Model-for-Multi-city-Human-Mobility-Prediction"><a href="#272-UniMove-A-Unified-Model-for-Multi-city-Human-Mobility-Prediction" class="headerlink" title="272. UniMove: A Unified Model for Multi-city Human Mobility Prediction"></a>272. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/UniMove__A_Unified_Model_for_Multi-city_Human_Mobility_Prediction.pdf">UniMove: A Unified Model for Multi-city Human Mobility Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本论文提出UniMove，一个用于多城市人类移动性预测的统一模型。UniMove采用轨迹-位置双塔结构，通过位置塔生成通用空间表示，并加入Deep &amp; Cross Net以增强特征表达，同时轨迹塔采用MoE Transformer模块自适应选择专家网络处理不同城市的异质移动模式。实验结果表明，UniMove能在多城市数据联合训练中实现数据互补，准确率提升超过10.2%，尤其对数据稀疏城市提升显著，具备更强泛化能力和训练效率。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="273-Simulating-Biological-Intelligence-Active-Inference-with-Experiment-Informed-Generative-Model"><a href="#273-Simulating-Biological-Intelligence-Active-Inference-with-Experiment-Informed-Generative-Model" class="headerlink" title="273. Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model"></a>273. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Simulating_Biological_Intelligence__Active_Inference_with_Experiment-Informed_Generative_Model.pdf">Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Turner Institute for Brain and Mental Health, School of Psychological Sciences, Monash University</span></p>
<p>本论文提出了一种基于主动推断（Active Inference）理论的生成模型框架，模拟生物神经网络决策过程，通过结合POMDP结构和实验数据，系统对比了多种决策算法（包括记忆增强的反事实学习、动态规划和经典主动推断）在Pong环境下的学习表现。结果显示，具有记忆机制的反事实学习方法在动态环境中的决策效果优于传统规划算法，揭示了记忆在智能体目标导向行为中的关键作用，并为可解释、类脑AI系统的设计提供了理论基础。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="274-Neural-Beam-Field-for-Spatial-Beam-RSRP-Prediction"><a href="#274-Neural-Beam-Field-for-Spatial-Beam-RSRP-Prediction" class="headerlink" title="274. Neural Beam Field for Spatial Beam RSRP Prediction"></a>274. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Neural_Beam_Field_for_Spatial_Beam_RSRP_Prediction.pdf">Neural Beam Field for Spatial Beam RSRP Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiamen University</span></p>
<p>本文提出了Neural Beam Field (NBF)，一种结合Transformer神经网络和物理模型的混合框架，用于高效且可解释的空间波束RSRP预测。方法核心包括多路径条件功率剖面(MCPP)的引入，结合先验物理建模与神经网络学习，并通过预训练-校准策略提升收敛与泛化能力。实验结果表明，NBF较传统表格法和纯黑盒神经网络在准确率、训练效率和模型紧凑性方面均有显著提升，适用于密集无线网络中的智能波束管理。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="275-Machine-Learning-Algorithms-for-Improving-Exact-Classical-Solvers-in-Mixed-Integer-Continuous-Optimization"><a href="#275-Machine-Learning-Algorithms-for-Improving-Exact-Classical-Solvers-in-Mixed-Integer-Continuous-Optimization" class="headerlink" title="275. Machine Learning Algorithms for Improving Exact Classical Solvers in Mixed Integer Continuous Optimization"></a>275. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Machine_Learning_Algorithms_for_Improving_Exact_Classical_Solvers_in_Mixed_Integer_Continuous_Optimi.pdf">Machine Learning Algorithms for Improving Exact Classical Solvers in Mixed Integer Continuous Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universität Wien</span></p>
<p>本文系统综述了机器学习（ML）和强化学习（RL）技术在提升整数与混合整数非线性规划（INLP、MINLP）等精确优化算法中的应用，特别聚焦于与分支定界（Branch-and-Bound, BB）框架的集成，包括分支决策、切割选择、节点排序和参数控制等环节。通过将监督学习、模仿学习和强化学习嵌入传统优化流程，显著加速求解过程且不损失全局最优性。结论指出：学习增强的优化方法在大规模或结构复杂问题上能提高收敛速度，但在数据稀缺和结构规则时仍需依赖传统算法，两者融合是未来智能优化发展的关键。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="276-A-Score-based-Diffusion-Model-Approach-for-Adaptive-Learning-of-Stochastic-Partial-Differential-Equation-Solutions"><a href="#276-A-Score-based-Diffusion-Model-Approach-for-Adaptive-Learning-of-Stochastic-Partial-Differential-Equation-Solutions" class="headerlink" title="276. A Score-based Diffusion Model Approach for Adaptive Learning of Stochastic Partial Differential Equation Solutions"></a>276. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/A_Score-based_Diffusion_Model_Approach_for_Adaptive_Learning_of_Stochastic_Partial_Differential_Equa.pdf">A Score-based Diffusion Model Approach for Adaptive Learning of Stochastic Partial Differential Equation Solutions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of South Carolina</span></p>
<p>本文提出了一种基于分数(score)-驱动扩散模型的自适应学习框架，用于递归贝叶斯推断下的随机偏微分方程(SPDEs)解的动态估计。方法将物理知识编码到扩散模型的分数函数，通过数值仿真和观测数据的似然校正，实现模型状态的迭代优化，并提出无需训练的Ensemble Score Filter以提升高维实时推断效率。大量数值实验（如Burgers方程、Navier-Stokes方程、Allen-Cahn方程）显示，在稀疏和有噪声观测条件下，该方法能准确、鲁棒地恢复隐藏物理状态和提升解的精度。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="277-Mode-Aware-Non-Linear-Tucker-Autoencoder-for-Tensor-based-Unsupervised-Learning"><a href="#277-Mode-Aware-Non-Linear-Tucker-Autoencoder-for-Tensor-based-Unsupervised-Learning" class="headerlink" title="277. Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning"></a>277. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Mode-Aware_Non-Linear_Tucker_Autoencoder_for_Tensor-based_Unsupervised_Learning.pdf">Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">东南大学</span></p>
<p>该论文提出了一种新颖的无监督学习框架——Mode-Aware Non-linear Tucker Autoencoder (MA-NTAE)，通过递归的Pick–Unfold–Encode–Fold操作实现高阶张量的灵活非线性压缩，结合Tucker分解与现代自编码器技术，显著提升了高维数据的压缩、重构和聚类表现。实验结果显示，MA-NTAE在多个合成和真实数据集上的重构误差、聚类指标均优于典型的深度自编码器（DAE）和张量分解神经网络（TFNN），且模型参数量小、训练效率高，尤其适用于高阶高维张量场景。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="278-Structured-Superposition-of-Autoencoders-for-UEP-Codes-at-Intermediate-Blocklengths"><a href="#278-Structured-Superposition-of-Autoencoders-for-UEP-Codes-at-Intermediate-Blocklengths" class="headerlink" title="278. Structured Superposition of Autoencoders for UEP Codes at Intermediate Blocklengths"></a>278. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Structured_Superposition_of_Autoencoders_for_UEP_Codes_at_Intermediate_Blocklengths.pdf">Structured Superposition of Autoencoders for UEP Codes at Intermediate Blocklengths</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Novi Sad</span></p>
<p>该论文提出了一种结构化自编码器（AE）架构，将编码和解码过程分为多个AE子模块，并结合叠加编码与SIC解码思想，实现了中等区块长度下的比特级不等错误保护（UEP）码设计。数值结果显示，该结构在可达错误概率区域优于传统叠加随机编码与SIC解码方法，且可高效扩展至更大区块长度，适用于资源受限场景。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="279-Lightning-Prediction-under-Uncertainty-DeepLight-with-Hazy-Loss"><a href="#279-Lightning-Prediction-under-Uncertainty-DeepLight-with-Hazy-Loss" class="headerlink" title="279. Lightning Prediction under Uncertainty: DeepLight with Hazy Loss"></a>279. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Lightning_Prediction_under_Uncertainty__DeepLight_with_Hazy_Loss.pdf">Lightning Prediction under Uncertainty: DeepLight with Hazy Loss</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bangladesh University of Engineering and Technology</span></p>
<p>本文提出了DeepLight，一种新颖的深度学习架构，通过多分支ConvLSTM和双编码器，融合雷达反射率、云属性和历史闪电数据进行闪电预测。创新性地引入Hazy Loss邻域感知损失函数，有效处理闪电的时空不确定性，提升预测准确性。实验表明，DeepLight在多个预测时长上较现有方法Equitable Threat Score提升8%–30%，Hazy Loss对多模型均有显著增益。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="280-Leveraging-GNN-to-Enhance-MEF-Method-in-Predicting-ENSO"><a href="#280-Leveraging-GNN-to-Enhance-MEF-Method-in-Predicting-ENSO" class="headerlink" title="280. Leveraging GNN to Enhance MEF Method in Predicting ENSO"></a>280. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Leveraging_GNN_to_Enhance_MEF_Method_in_Predicting_ENSO.pdf">Leveraging GNN to Enhance MEF Method in Predicting ENSO</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shiraz University of Technology</span></p>
<p>本文提出了一种基于图神经网络（GNN）增强的多模态ENSO预测（MEF）框架，将3D卷积神经网络（3D-CNN）与时间序列模块输出的80个集成成员通过构建相似性加权图进行结构化分析，并利用社区检测方法选取最优子集，最终提升预测的鲁棒性和准确性，尤其在长期预测场景下表现更优。实验结果表明，GNN方法可有效过滤不稳定噪声成员，实现更高的相关性和一致性输出，且该方法对模型结构无依赖，易于应用于其它集成预测系统。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="281-ProteoKnight-Convolution-based-phage-virion-protein-classification-and-uncertainty-analysis"><a href="#281-ProteoKnight-Convolution-based-phage-virion-protein-classification-and-uncertainty-analysis" class="headerlink" title="281. ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis"></a>281. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/ProteoKnight__Convolution-based_phage_virion_protein_classification_and_uncertainty_analysis.pdf">ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">BRAC University</span></p>
<p>本论文提出ProteoKnight方法，采用基于DNA-Walk的Knight编码，将蛋白质序列转化为图像，并利用预训练卷积神经网络（CNN）进行二分类和多分类预测，同时引入Monte Carlo Dropout方法对模型预测进行不确定性分析。结果显示，Knight编码结合高效CNN模型可在二分类任务中达到约90.8%的准确率，并揭示了蛋白质类别和序列长度对预测置信度的影响。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="282-Finite-Time-Convergence-Analysis-of-ODE-based-Generative-Models-for-Stochastic-Interpolants"><a href="#282-Finite-Time-Convergence-Analysis-of-ODE-based-Generative-Models-for-Stochastic-Interpolants" class="headerlink" title="282. Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants"></a>282. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Finite-Time_Convergence_Analysis_of_ODE-based_Generative_Models_for_Stochastic_Interpolants.pdf">Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IIIS, Tsinghua University</span></p>
<p>该论文系统分析了基于常微分方程（ODE）的生成模型在随机插值框架下的有限时间收敛性，通过理论推导给出了前向Euler方法和Heun’s方法在总变差距离上的误差界，并分析了迭代复杂度。实验结果验证了理论收敛速率，为实际生成过程的数值实现提供了更精确的误差控制和复杂度优化。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="283-Fairness-of-Automatic-Speech-Recognition-Looking-Through-a-Philosophical-Lens"><a href="#283-Fairness-of-Automatic-Speech-Recognition-Looking-Through-a-Philosophical-Lens" class="headerlink" title="283. Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens"></a>283. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fairness_of_Automatic_Speech_Recognition__Looking_Through_a_Philosophical_Lens.pdf">Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Cornell University</span></p>
<p>本文通过哲学视角分析自动语音识别（ASR）系统的偏见与公平性，提出ASR对非标准方言的系统性误识不仅是技术局限，更是对边缘化语言群体的不尊重。作者区分了道德中性的分类（discriminate1）与有害歧视（discriminate2），并揭示ASR偏见的三大独特伦理维度：时间负担（temporal taxation）、对话流中断和对身份认同的影响。结论认为，解决ASR偏见不仅需技术改进，更需承认多样化语音形式的合法性，推动技术与伦理并重，实现对语言多样性的尊重与包容。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="284-An-effective-potential-for-generative-modelling-with-active-matter"><a href="#284-An-effective-potential-for-generative-modelling-with-active-matter" class="headerlink" title="284. An effective potential for generative modelling with active matter"></a>284. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/An_effective_potential_for_generative_modelling_with_active_matter.pdf">An effective potential for generative modelling with active matter</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Queen Mary University of London</span></p>
<p>本文提出了一种基于主动粒子过程的生成扩散模型，通过在位置坐标上施加有效的时间相关势能，实现了扩散模型的时间反演采样。关键技术包括Fox和UCN近似，有效势能由标准score函数及其高阶导数决定，数值实验验证了方法的准确性。结论表明，所提方法能精确再现目标分布，且具备实验实现潜力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="285-OFAL-An-Oracle-Free-Active-Learning-Framework"><a href="#285-OFAL-An-Oracle-Free-Active-Learning-Framework" class="headerlink" title="285. OFAL: An Oracle-Free Active Learning Framework"></a>285. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/OFAL__An_Oracle-Free_Active_Learning_Framework.pdf">OFAL: An Oracle-Free Active Learning Framework</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Amirkabir University of Technology</span></p>
<p>该论文提出了OFAL，一种无需人工标注者的主动学习框架，通过神经网络的不确定性量化（利用Monte Carlo Dropout近似贝叶斯神经网络）与变分自编码器（VAE），将模型高置信的无标签样本转换为具有高信息量的不确定样本，从而提升模型性能。实验表明，在MNIST数据集上，通过多轮生成不确定样本并与原样本共同训练，模型准确率提升2.7%，且OFAL可与其它主动学习采样方法结合，在无需额外标注成本下进一步提升效果。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="286-Learned-Regularization-for-Microwave-Tomography"><a href="#286-Learned-Regularization-for-Microwave-Tomography" class="headerlink" title="286. Learned Regularization for Microwave Tomography"></a>286. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learned_Regularization_for_Microwave_Tomography.pdf">Learned Regularization for Microwave Tomography</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本论文提出了一种物理信息引导的混合重建框架，将单步扩散模型（Single-Step Diffusion Regularization, SSD-Reg）作为Plug-and-Play正则项嵌入到微波层析成像（MWT）迭代重建流程中，有效结合了精确的物理建模与生成式先验。该方法无需配对数据，通过Fréchet可微前向模型和单步扩散正则项，引入强结构先验，显著提升了重建的准确性、稳定性和收敛速度。大量仿真和真实数据实验表明，该方法在精度、抗噪性和效率方面均优于现有方法，展现出较强的临床应用潜力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="287-C-MAG-Cascade-Multimodal-Attributed-Graphs-for-Supply-Chain-Link-Prediction"><a href="#287-C-MAG-Cascade-Multimodal-Attributed-Graphs-for-Supply-Chain-Link-Prediction" class="headerlink" title="287. C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction"></a>287. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/C-MAG__Cascade_Multimodal_Attributed_Graphs_for_Supply_Chain_Link_Prediction.pdf">C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AAITC, Lenovo</span></p>
<p>该论文提出了C-MAG（Cascade Multimodal Attributed Graphs）方法，通过两阶段级联架构将文本和图像等多模态信息聚合进制造商节点，再通过异构图神经网络实现制造商-产品关系的高精度预测。实验结果显示，C-MAG在ROC-AUC和PR-AUC等指标上优于传统方法，且对视觉噪声具有鲁棒性，为供应链知识图谱的多模态集成和链路预测提供了新的技术方案。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="288-From-Source-to-Target-Leveraging-Transfer-Learning-for-Predictive-Process-Monitoring-in-Organizations"><a href="#288-From-Source-to-Target-Leveraging-Transfer-Learning-for-Predictive-Process-Monitoring-in-Organizations" class="headerlink" title="288. From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations"></a>288. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/From_Source_to_Target__Leveraging_Transfer_Learning_for_Predictive_Process_Monitoring_in_Organizatio.pdf">From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Friedrich-Alexander-Universität Erlangen-Nürnberg</span></p>
<p>该论文提出了一种基于迁移学习的预测流程监控（PPM）技术，通过预训练嵌入模型进行活动编码和相对跨域时间戳编码，将源业务流程的LSTM预测模型移植到目标流程，无需对目标数据进行微调。实验证明，该方法可在组织内外实现跨流程知识转移，有效提升目标流程的预测性能，帮助缺乏数据或算力的组织实现高效流程监控和决策支持。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="289-On-Understanding-of-the-Dynamics-of-Model-Capacity-in-Continual-Learning"><a href="#289-On-Understanding-of-the-Dynamics-of-Model-Capacity-in-Continual-Learning" class="headerlink" title="289. On Understanding of the Dynamics of Model Capacity in Continual Learning"></a>289. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/On_Understanding_of_the_Dynamics_of_Model_Capacity_in_Continual_Learning.pdf">On Understanding of the Dynamics of Model Capacity in Continual Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Capital One</span></p>
<p>本文提出了“连续学习有效模型容量（CLEMC）”的动态理论框架，系统刻画了神经网络在连续学习中的稳定性-可塑性平衡点随任务演化的动态变化。方法上，作者基于动态规划和最优控制，推导出模型容量随任务分布、权重更新、优化过程变化的递推方程，并从理论上证明了无论模型结构或优化方法如何，只要新任务的分布不断变化，容量（即模型对新任务的表征能力）都会不可避免地发散，导致遗忘现象不断累积。通过FNN、CNN、GNN和大规模Transformer等多种架构及合成&#x2F;真实数据集实验，实证验证了理论结论的普适性。结论：连续学习中模型容量动态变化、不可避免地随着任务变化发散，需采用动态模型和容量约束方法才能有效应对。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="290-Learning-to-Select-MCP-Algorithms-From-Traditional-ML-to-Dual-Channel-GAT-MLP"><a href="#290-Learning-to-Select-MCP-Algorithms-From-Traditional-ML-to-Dual-Channel-GAT-MLP" class="headerlink" title="290. Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP"></a>290. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Learning_to_Select_MCP_Algorithms__From_Traditional_ML_to_Dual-Channel_GAT-MLP.pdf">Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shantou University</span></p>
<p>该论文针对最大团问题（MCP）的算法选择，提出了一个结合传统机器学习和图神经网络（GNN）的学习型框架。作者通过构建包含多样化图实例和结构特征的数据集，评估了SVM、随机森林等传统分类器，并提出了融合局部结构编码（GAT）和全局特征建模（MLP）的双通道GAT-MLP模型。实验结果显示，GAT-MLP在准确率和F1分数上均优于传统方法，能有效自动选择最优MCP算法，显著降低计算成本，验证了双通道架构在组合算法选择上的有效性与泛化能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="291-MemoryKT-An-Integrative-Memory-and-Forgetting-Method-for-Knowledge-Tracing"><a href="#291-MemoryKT-An-Integrative-Memory-and-Forgetting-Method-for-Knowledge-Tracing" class="headerlink" title="291. MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing"></a>291. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/MemoryKT__An_Integrative_Memory-and-Forgetting_Method_for_Knowledge_Tracing.pdf">MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China Normal University</span></p>
<p>本文提出了一种基于时序变分自编码器（VAE）的知识追踪（KT）模型MemoryKT，将记忆的编码、存储和检索三阶段过程进行建模，并在时序流程中嵌入个性化遗忘模块，动态调节学生记忆存储强度。实验结果显示，MemoryKT在四个公开数据集上显著优于现有主流KT方法，提升了模型的性能与个性化建模能力。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="292-Fast-and-Generalizable-parameter-embedded-Neural-Operators-for-Lithium-Ion-Battery-Simulation"><a href="#292-Fast-and-Generalizable-parameter-embedded-Neural-Operators-for-Lithium-Ion-Battery-Simulation" class="headerlink" title="292. Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation"></a>292. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Fast_and_Generalizable_parameter-embedded_Neural_Operators_for_Lithium-Ion_Battery_Simulation.pdf">Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">RWTH Aachen University</span></p>
<p>本文提出并系统比较了三种算子学习神经网络（DeepONet、FNO和新提出的参数嵌入Fourier神经算子PE-FNO）在锂离子电池仿真中的应用。PE-FNO在保持高准确率的同时实现了对材料参数广泛变化的泛化，并在速度上比传统多线程仿真器快约两百倍，适合实时电池管理和大规模推断任务。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="293-FNBT-Full-Negation-Belief-Transformation-for-Open-World-Information-Fusion-Based-on-Dempster-Shafer-Theory-of-Evidence"><a href="#293-FNBT-Full-Negation-Belief-Transformation-for-Open-World-Information-Fusion-Based-on-Dempster-Shafer-Theory-of-Evidence" class="headerlink" title="293. FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence"></a>293. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/FNBT__Full_Negation_Belief_Transformation_for_Open-World_Information_Fusion_Based_on_Dempster-Shafer.pdf">FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, South China Normal University</span></p>
<p>本文提出了一种基于Dempster-Shafer证据理论的全否定信念变换（FNBT）方法，用于解决实际应用中因异构帧导致的信息融合问题。FNBT通过引入开放世界判据、帧扩展和全否定机制，将原始概率分配映射到扩展帧，使传统组合规则在开放世界下有效，理论证明其具备质量函数不变性、可继承性和本质冲突消除等性质。实验结果显示，FNBT在多个真实分类任务上显著优于现有方法，特别适用于数据孤岛和异构数据源融合场景。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="294-Multi-modal-Adaptive-Mixture-of-Experts-for-Cold-start-Recommendation"><a href="#294-Multi-modal-Adaptive-Mixture-of-Experts-for-Cold-start-Recommendation" class="headerlink" title="294. Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation"></a>294. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Multi-modal_Adaptive_Mixture_of_Experts_for_Cold-start_Recommendation.pdf">Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">VNU University of Engineering and Technology</span></p>
<p>本论文提出了MAMEX框架，通过多模态专家网络和可学习门控机制对冷启动推荐系统中的多模态信息（如图像、文本）进行自适应融合，动态平衡各模态贡献，并引入正则项防止模态塌缩。实验结果表明，MAMEX在多个Amazon数据集上显著优于现有方法，提升了Recall和NDCG指标，验证了模型的有效性和适应性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="295-Deep-Learning-Based-Analysis-of-Power-Consumption-in-Gasoline-Electric-and-Hybrid-Vehicles"><a href="#295-Deep-Learning-Based-Analysis-of-Power-Consumption-in-Gasoline-Electric-and-Hybrid-Vehicles" class="headerlink" title="295. Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles"></a>295. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Deep_Learning-Based_Analysis_of_Power_Consumption_in_Gasoline,_Electric,_and_Hybrid_Vehicles.pdf">Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Research Council Canada</span></p>
<p>本文提出了一种基于深度学习（TCN、LSTM、Transformer）与传统机器学习（RF）的数据驱动方法，利用车辆动力系统动态特征，预测汽油车（ICE）、电动车（EV）、混合动力车（HEV）的瞬时与累计能耗，并收集了三类车辆的真实道路数据。结果表明，深度学习模型在ICE上的预测精度极高（MAE和RMSE约为10^-3），在EV和HEV上Transformer和LSTM表现最佳，累计能耗误差低于4.1%和2.1%，且不确定性分析显示EV和HEV预测波动更大，突显了复杂动力系统对能耗建模的挑战。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="296-Topological-Feature-Compression-for-Molecular-Graph-Neural-Networks"><a href="#296-Topological-Feature-Compression-for-Molecular-Graph-Neural-Networks" class="headerlink" title="296. Topological Feature Compression for Molecular Graph Neural Networks"></a>296. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Topological_Feature_Compression_for_Molecular_Graph_Neural_Networks.pdf">Topological Feature Compression for Molecular Graph Neural Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Imperial College London</span></p>
<p>本论文提出了一种结合高阶拓扑特征压缩的分子图神经网络（PACTNet），通过ECC算法将高阶细胞复形的拓扑特征与传统分子图特征融合，有效捕捉全球几何信息且保持特征的可解释性和高效计算。通过在多个分子性质预测基准数据集上的实验证明，PACTNet在大多数任务中实现了统计学显著优于现有主流GNN模型的表现，兼顾了准确率、鲁棒性和计算效率。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="297-Frequency-Domain-Analysis-of-Time-Dependent-Multiomic-Data-in-Progressive-Neurodegenerative-Diseases-A-Proposed-Quantum-Classical-Hybrid-Approach-with-Quaternionic-Extensions"><a href="#297-Frequency-Domain-Analysis-of-Time-Dependent-Multiomic-Data-in-Progressive-Neurodegenerative-Diseases-A-Proposed-Quantum-Classical-Hybrid-Approach-with-Quaternionic-Extensions" class="headerlink" title="297. Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions"></a>297. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Frequency-Domain_Analysis_of_Time-Dependent_Multiomic_Data_in_Progressive_Neurodegenerative_Diseases.pdf">Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Massachusetts General Hospital</span></p>
<p>本文提出了一种将多组学和神经影像时序数据通过傅里叶&#x2F;Laplace变换转入频域，并利用哈密顿量建模、变分量子特征求解器（VQE）和四元数扩展进行高维特征提取和疾病动力学建模的方法。结论指出该理论框架为未来用量子混合计算和超复数方法分析神经变性病提供了数学基础，有望提升高危患者预测和精准医疗决策，但需进一步实证验证。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="298-Generative-Inversion-for-Property-Targeted-Materials-Design-Application-to-Shape-Memory-Alloys"><a href="#298-Generative-Inversion-for-Property-Targeted-Materials-Design-Application-to-Shape-Memory-Alloys" class="headerlink" title="298. Generative Inversion for Property-Targeted Materials Design: Application to Shape Memory Alloys"></a>298. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Generative_Inversion_for_Property-Targeted_Materials_Design__Application_to_Shape_Memory_Alloys.pdf">Generative Inversion for Property-Targeted Materials Design: Application to Shape Memory Alloys</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong University</span></p>
<p>本文提出了一种基于生成对抗网络（GAN）反演的新型高性能形状记忆合金（SMA）逆向设计框架，结合预训练GAN和性质预测模型，通过梯度优化在潜空间生成满足指定性能指标的合金成分与工艺参数。实验合成和表征的五种NiTi基合金均达到高转变温度和大机械功输出，A1合金性能优异，验证了框架的有效性和广泛适用性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="299-Disentangling-Multiplex-Spatial-Temporal-Transition-Graph-Representation-Learning-for-Socially-Enhanced-POI-Recommendation"><a href="#299-Disentangling-Multiplex-Spatial-Temporal-Transition-Graph-Representation-Learning-for-Socially-Enhanced-POI-Recommendation" class="headerlink" title="299. Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation"></a>299. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/Disentangling_Multiplex_Spatial-Temporal_Transition_Graph_Representation_Learning_for_Socially_Enhan.pdf">Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China Normal University</span></p>
<p>该论文提出了DiMuST方法，通过构建社会异质图和多路空间-时间转移图，利用Disentangled Variational Multiplex Graph Auto-Encoder (DAE)实现空间-时间信息的共享与私有分布解耦，并通过Product of Experts机制融合共享特征，对私有特征进行对比约束去噪。该方法在两个真实数据集上显著优于现有POI推荐方法，有效提升了推荐准确率和模型鲁棒性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="300-EMPATHIA-Multi-Faceted-Human-AI-Collaboration-for-Refugee-Integration"><a href="#300-EMPATHIA-Multi-Faceted-Human-AI-Collaboration-for-Refugee-Integration" class="headerlink" title="300. EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration"></a>300. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-12/EMPATHIA__Multi-Faceted_Human-AI_Collaboration_for_Refugee_Integration.pdf">EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Texas A&amp;M University</span></p>
<p>该论文提出EMPATHIA框架，通过多智能体系统（情感、文化、伦理三代理）及selector–validator架构，实现难民安置的人机协同决策，兼顾情感、文化和伦理价值。实验证明，在UN Kakuma数据集上，系统能以87.4%一致率给出可解释推荐，有效提升安置公平性和可扩展性。</p>
<img src="https://images.unsplash.com/photo-1506784983877-45594efa4cbe?auto=format&fit=crop&w=600&q=80" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>今日论文报纸</div>
      <div>http://example.com/2025/08/2025-08-13_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Ywfhhh</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月12日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/2025-08-14_article/" title="今日论文报纸">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">今日论文报纸</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-12_article/" title="今日论文报纸">
                        <span class="hidden-mobile">今日论文报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
