

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="具身ADT">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;28报纸">
<meta property="og:url" content="http://ywfhhh.github.io/2025/08/2025-08-28_article/index.html">
<meta property="og:site_name" content="最新前沿论文分享">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Attackers_Strike_Back_Not_Anymore_%E2%80%94_An_Ensemble_of_RL_Defenders_Awakens_for_APT_Detection.jpg">
<meta property="article:published_time" content="2025-08-28T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-27T18:14:48.451Z">
<meta property="article:author" content="具身ADT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Attackers_Strike_Back_Not_Anymore_%E2%80%94_An_Ensemble_of_RL_Defenders_Awakens_for_APT_Detection.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/28报纸 - 最新前沿论文分享</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ywfhhh.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<!-- hexo injector head_end start -->
<script src="/js/custom.js"></script>
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文库</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/28报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-28 00:00" pubdate>
          星期四, 八月 28日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          21k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/28报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/28报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-28<br>📄 <strong>发现论文数量</strong>：137  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19072v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      1. Attackers Strike Back? Not Anymore — An Ensemble of RL Defenders Awakens for APT Detection
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Attackers_Strike_Back__Not_Anymore_--_An_Ensemble_of_RL_Defenders_Awakens_for_APT_Detection.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">New York University</span></p>
<p>本文提出了一种集成深度学习、强化学习（RL）和主动学习的多智能体框架，用于高级持续威胁（APT）检测。通过自编码器进行行为特征编码，结合Q-Learning、PPO、DQN等多种RL代理，并在不确定性样本上采用主动学习和专家反馈，最后通过加权集成投票提升检测鲁棒性。实验表明，多智能体主动-对抗RL集成模型在40个真实数据集上显著优于传统方法，极大提升了APT检测的准确率和适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Attackers_Strike_Back_Not_Anymore_%E2%80%94_An_Ensemble_of_RL_Defenders_Awakens_for_APT_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19014v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      2. MAB Optimizer for Estimating Math Question Difficulty via Inverse CV without NLP
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/MAB_Optimizer_for_Estimating_Math_Question_Difficulty_via_Inverse_CV_without_NLP.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ITMO University</span></p>
<p>本文提出了一种基于强化学习多臂老虎机(MAB)的自监督难度估计方法（APME），仅利用解题者的得分与耗时数据，通过优化逆变异系数(CV)实现数学习题难度排序，无需NLP特征或专家标注。实验覆盖多个真实教育数据集（SKYBEN, TIMSS, IIT JEE），结果显示模型具有高准确性（平均R² 0.9213, RMSE 0.0584），适应性强，优于传统NLP和回归方法，尤其适用于符号领域如代数。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/MAB_Optimizer_for_Estimating_Math_Question_Difficulty_via_Inverse_CV_without_NLP.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19152v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      3. Playstyle and Artificial Intelligence: An Initial Blueprint Through the Lens of Video Games
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Playstyle_and_Artificial_Intelligence__An_Initial_Blueprint_Through_the_Lens_of_Video_Games.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Yang Ming Chiao Tung University</span></p>
<p>本论文提出以playstyle（玩法风格）作为智能体决策行为分析的新视角，系统性梳理其哲学基础、形式化定义及度量方法，并设计以分层离散状态（Hierarchical State Discretization, HSD）为核心的无监督度量系统。论文进一步结合强化学习与模仿学习，实现特定风格AI训练与人类风格建模，并提出多种平衡性与多样性度量指标。实验和应用涵盖主流电子游戏、AI对抗与人类行为建模。结论指出，playstyle是理解智能体自主性、价值表达、甚至“数字灵魂”建构的重要路径，相关方法对游戏设计、AI个性化及泛化智能具有广泛意义。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Playstyle_and_Artificial_Intelligence_An_Initial_Blueprint_Through_the_Lens_of_Video_Games.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19132v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      4. Active Query Selection for Crowd-Based Reinforcement Learning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Active_Query_Selection_for_Crowd-Based_Reinforcement_Learning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Bristol</span></p>
<p>本论文提出一种结合主动学习与人群建模的反馈聚合框架，扩展了Advise算法以支持多训练者反馈，并在线估算每位训练者的可靠性，利用基于熵的查询选择机制主动请求对高不确定性状态-动作对的反馈。实验结果表明，该方法在经典网格世界任务和糖尿病血糖控制等真实场景中提升了学习速度与反馈效率，尤其在决策受限场景中表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Active_Query_Selection_for_Crowd-Based_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18914v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      5. FormaRL: Enhancing Autoformalization with no Labeled Data
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/FormaRL__Enhancing_Autoformalization_with_no_Labeled_Data.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出了FormaRL，一种基于强化学习（RL）的自动形式化训练框架，通过结合Lean编译器的语法检查和大语言模型（LLM）的语义一致性检查进行奖励设计，仅需极少量无标注数据。方法采用GRPO算法优化模型，并构建了uproof高级数学基准集。实验表明，FormaRL在无需标注数据的情况下显著提升了自动形式化在高级数学领域的泛化能力，性能大幅超越传统监督微调基线，尤其在小数据条件下更具优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/FormaRL_Enhancing_Autoformalization_with_no_Labeled_Data.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18884v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      6. HAEPO: History-Aggregated Exploratory Policy Optimization
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/HAEPO__History-Aggregated_Exploratory_Policy_Optimization.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Birla Institute of Technology and Science, Pilani</span></p>
<p>本文提出了一种新的强化学习策略优化方法HAEPO，通过将每条轨迹的对数似然累加并基于Plackett-Luce归一化赋权，实现轨迹级的探索与稳定性提升，并结合熵正则和软KL约束避免收敛过快或策略崩塌。实验证明HAEPO在多臂赌博机、随机游走、CartPole和LLM微调任务上收敛更快、探索更充分且表现稳定，优于或可与PPO、GRPO、DPO等主流方法媲美。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/HAEPO_History-Aggregated_Exploratory_Policy_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18839v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      7. DRMD: Deep Reinforcement Learning for Malware Detection under Concept Drift
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/DRMD__Deep_Reinforcement_Learning_for_Malware_Detection_under_Concept_Drift.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">King’s College London</span></p>
<p>本文提出了一种新颖的恶意软件检测方法，将其建模为一步马尔可夫决策过程（MDP），并利用深度强化学习（PPO）训练DRMD智能体，联合优化分类、拒绝高风险样本和主动学习策略，从而提升在现实概念漂移场景下的检测稳定性。实验表明，DRMD在两个Android恶意软件数据集和多种特征空间下，较主流基线方法在时间自适应评估中平均AUT指标提升5-14分，展现出更强的鲁棒性和适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/DRMD_Deep_Reinforcement_Learning_for_Malware_Detection_under_Concept_Drift.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18708v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      8. Skill-Aligned Fairness in Multi-Agent Learning for Collaboration in Healthcare
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Skill-Aligned_Fairness_in_Multi-Agent_Learning_for_Collaboration_in_Healthcare.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Cornell Tech</span></p>
<p>本文提出了FairSkillMARL框架，将公平性定义为工作负载平衡与技能-任务匹配的双重目标，并开发了面向医疗场景的多智能体仿真环境MARLHospital，实现对异质团队在能量约束下的协作建模。实验表明，单纯的工作量均衡会导致技能-任务不匹配，FairSkillMARL在复杂医疗任务中能更好捕捉公平性并提升团队效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Skill-Aligned_Fairness_in_Multi-Agent_Learning_for_Collaboration_in_Healthcare.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18768v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      9. Efficient Best-of-Both-Worlds Algorithms for Contextual Combinatorial Semi-Bandits
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Efficient_Best-of-Both-Worlds_Algorithms_for_Contextual_Combinatorial_Semi-Bandits.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">EPFL</span></p>
<p>本文提出了首个针对上下文组合半赌博问题的“Best-of-Both-Worlds”算法，在对抗性和被污染的随机环境下分别实现了O(√T)和O(ln T)的最优遗憾界。方法基于Shannon熵正则化的FTRL框架，并通过KKT条件将高维投影简化为一维根查找，大幅提升每轮计算效率。实验表明，该算法兼具统计和计算效率，适用于大规模实时应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Efficient_Best-of-Both-Worlds_Algorithms_for_Contextual_Combinatorial_Semi-Bandits.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18741v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      10. Stability and Generalization for Bellman Residuals
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Stability_and_Generalization_for_Bellman_Residuals.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Washington</span></p>
<p>本文系统分析了Bellman Residual Minimization（BRM）在离线强化学习与逆强化学习中的统计行为，利用PL-强凹几何和Lyapunov势函数，提出对邻域数据集的SGDA算法进行耦合分析，得到了O(1&#x2F;n)的平均稳定性界和O(1&#x2F;n)的泛化界。结论显示，SGDA在神经网络参数化下无需额外正则化或独立采样假设即可实现全局收敛和最优泛化，显著提升了BRM的样本复杂度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Stability_and_Generalization_for_Bellman_Residuals.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18610v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      11. Scalable Fairness Shaping with LLM-Guided Multi-Agent Reinforcement Learning for Peer-to-Peer Electricity Markets
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Scalable_Fairness_Shaping_with_LLM-Guided_Multi-Agent_Reinforcement_Learning_for_Peer-to-Peer_Electr.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Michigan-Dearborn</span></p>
<p>本文提出了FairMarket-RL框架，通过将大语言模型（LLM）作为公平性评价“裁判”，引导多智能体强化学习（MARL）在部分可观测、离散价格-数量动作空间下进行P2P电力市场竞价，并实时将三种公平性指标（FTG、FBS、FPP）反馈到奖励函数，实现经济激励与社区公平目标的融合。实验表明，该方法可在保证经济效率的同时，提升交易公平性、促进本地能源流通并维持电网及各参与方的利润，且在不同规模与真实数据集上表现稳健。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Scalable_Fairness_Shaping_with_LLM-Guided_Multi-Agent_Reinforcement_Learning_for_Peer-to-Peer_Electricity_Markets.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19236v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      12. MEMORYVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/MemoryVLA__Perceptual-Cognitive_Memory_in_Vision-Language-Action_Models_for_Robotic_Manipulation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出MemoryVLA，一种受认知科学启发的机器人操作认知-记忆-行动框架，通过引入类海马体的感知-认知记忆库，结合工作记忆，显式建模时序依赖。方法包括利用预训练视觉语言模型(VLM)生成感知和认知token，存储于记忆库，并通过门控机制检索与融合历史信息，最后以记忆增强的扩散模型生成动作序列。实验覆盖3种机器人、150+任务，MemoryVLA在仿真和真实场景均超越主流基线CogACT与π0，特别在长时序任务上提升显著，并展现出强鲁棒性与泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/MEMORYVLA_Perceptual-Cognitive_Memory_in_Vision-Language-Action_Models_for_Robotic_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19204v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      13. LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/LSD-3D__Large-Scale_3D_Driving_Scene_Generation_with_Geometry_Grounding.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Princeton University</span></p>
<p>该论文提出LSD-3D方法，通过几何引导的蒸馏采样（GGDS），结合代理几何生成、环境建模和2D扩散模型，实现大规模、可控、真实且因果一致的3D自动驾驶场景生成。实验结果表明，该方法在新视角合成质量和多样性上优于现有视频扩散与3D生成方法，支持实时仿真与多维场景定制。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/LSD-3D_Large-Scale_3D_Driving_Scene_Generation_with_Geometry_Grounding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19199v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      14. Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Planning-Query-Guided_Model_Generation_for_Model-Based_Deformable_Object_Manipulation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stevens Institute of Technology</span></p>
<p>该论文提出了一种针对可变形物体操作任务的模型生成方法，通过扩散模型根据规划查询（起点与终点点云）自动生成区域分辨率优化的动力学模型。方法采用两阶段优化流程，先以动力学模型准确性为先验，再以闭环任务性能直接优化模型简化。实验结果表明，该方法在树枝操控任务中能显著提升规划速度（提升2倍），且任务性能损失很小，优于全分辨率和静态分辨率基线。结论：模型生成器能实现任务相关的区域简化，在保持性能的同时大幅降低计算量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Planning-Query-Guided_Model_Generation_for_Model-Based_Deformable_Object_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19191v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      15. AutoRing: Imitation Learning-based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/AutoRing__Imitation_Learning--based_Autonomous_Intraocular_Foreign_Body_Removal_Manipulation_with_Ey.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本论文提出AutoRing框架，实现了基于模仿学习的眼科手术机器人自主异物移除操作。方法核心为RCM动态校准与动作分块Transformer（RCM-ACT）架构，利用专家演示的立体视觉与机械臂运动学数据训练端到端策略，通过软件空间锚定解决运动缩放和远程运动中心（RCM）变化带来的不确定性，支持显著的空间精度和鲁棒性。实验表明，AutoRing在未校准显微镜条件下，无需显式深度感知即可实现0.12mm均值误差的高精度抓取，显著优于传统方法并首次完成全流程自主操作。结论：AutoRing为复杂眼内手术自动化提供了可行路径，但仍需进一步改进以支持人机协作与安全策略。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/AutoRing_Imitation_Learning-based_Autonomous_Intraocular_Foreign_Body_Removal_Manipulation_with_Eye_Surgical_Robot.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19186v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      16. Real-Time Model Checking for Closed-Loop Robot Reactive Planning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Real-Time_Model_Checking_for_Closed-Loop_Robot_Reactive_Planning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Glasgow</span></p>
<p>本文提出了一种基于模型检测（model checking）的实时多步规划与避障方法，实现了在低功耗自主机器人上的实时闭环反应式规划。方法核心包括设计轻量级模型检测算法，利用2D LiDAR数据空间离散化，基于线性时序逻辑（LTL）自动生成多步避障序列，并通过与传统单步反应基线方法的实物对比实验与理论分析，证明该方法能高效、安全地避障，避免陷入死角，并满足严格的实时性和低内存要求。实验结果表明，模型检测方法在避障效率、碰撞次数和处理延迟方面显著优于基线控制器。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Real-Time_Model_Checking_for_Closed-Loop_Robot_Reactive_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19153v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      17. QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/QuadKAN__KAN-Enhanced_Quadruped_Motion_Control_via_End-to-End_Reinforcement_Learning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Berkeley</span></p>
<p>该论文提出QuadKAN框架，将Kolmogorov–Arnold Networks (KANs)引入四足机器人运动控制，通过融合本体感知与深度视觉，实现基于端到端强化学习的跨模态策略。方法包括KAN参数化的样条编码器和决策融合头，并采用多模态延迟随机化(MMDR)和PPO优化。实验结果显示QuadKAN在多种地形和障碍环境下相较于当前主流基线，在回报、避障和稳定性上均取得显著提升，证明了KAN结构和跨模态融合的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/QuadKAN_KAN-Enhanced_Quadruped_Motion_Control_via_End-to-End_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19131v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      18. ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/ZeST__an_LLM-based_Zero-Shot_Traversability_Navigation_for_Unknown_Environments.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois at Urbana-Champaign</span></p>
<p>该论文提出ZeST，一种基于多模态大语言模型（LLM）视觉推理能力的零样本可通行性（traversability）导航方法，通过视觉分割（SAM或SLIC）对环境图像进行区域划分，并利用LLM结合机器人参数和场景上下文，对每个区域预测可通行性分布，进而生成全局可通行性概率地图。ZeST通过NIG分布建模不确定性，结合基于CVaR的风险评估和RRT*路径规划，实现无需实地危险数据采集的安全高效自主导航。实验证明，ZeST在室内外复杂环境中零样本导航表现优于NoMaD和CoNVOI等方法，显著提升了导航成功率和安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/ZeST_an_LLM-based_Zero-Shot_Traversability_Navigation_for_Unknown_Environments.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19042v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      19. A Concurrent Modular Agent: Framework for Autonomous LLM Agents
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/A_Concurrent_Modular_Agent__Framework_for_Autonomous_LLM_Agents.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Alternative Machine Inc.</span></p>
<p>该论文提出Concurrent Modular Agent (CMA)框架，将多个基于大语言模型（LLM）的模块通过异步消息传递和共享向量数据库进行协同，实现分布式智能体系统。作者在Plantbot（混合植物机器人）和ALTER3（人形机器人）上进行实证，展示了该框架能自主产生自我意识、行为适应和高鲁棒性，验证了分布式模块化智能体架构在机器人领域的实用性和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/A_Concurrent_Modular_Agent_Framework_for_Autonomous_LLM_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19002v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      20. HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/HuBE__Cross-Embodiment_Human-like_Behavior_Execution_for_Humanoid_Robots.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong Polytechnic University</span></p>
<p>该论文提出了HuBE，一个双层闭环框架，通过集成机器人状态、目标姿态和情境信息，实现类人行为的生成，兼顾动作相似性和情境适应性。方法包括基于情境语义增强的数据集HPose、骨骼缩放数据增强及端到端动作生成与执行流程。实验证明该方法在多平台上实现了高动作相似性、情境适应性和跨平台兼容性，显著优于现有方法，为可转移的类人行为执行奠定基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/HuBE_Cross-Embodiment_Human-like_Behavior_Execution_for_Humanoid_Robots.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19164v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      21. REAL-TIME TESTING OF SATELLITE ATTITUDE CONTROL WITH A REACTION WHEEL HARDWARE-IN-THE-LOOP PLATFORM
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Real-time_Testing_of_Satellite_Attitude_Control_With_a_Reaction_Wheel_Hardware-In-the-Loop_Platform.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Florida Institute of Technology</span></p>
<p>本文提出并实现了一种基于Hardware-in-the-Loop (HIL)的自适应卫星姿态控制系统测试平台，能够对反作用轮（RW）健康状态进行估计。平台集成了实际反作用轮硬件、嵌入式控制器（NVIDIA Jetson Nano）、CAN总线通信、模拟卫星仿真器和故障注入机制，验证了自适应控制器在硬件约束和传感器噪声下的有效性。结论显示，所提测试平台能够有效验证姿态控制算法的硬件可行性及其容错能力，对未来更真实的物理测试和端到端系统验证具有重要意义。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/REAL-TIME_TESTING_OF_SATELLITE_ATTITUDE_CONTROL_WITH_A_REACTION_WHEEL_HARDWARE-IN-THE-LOOP_PLATFORM.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19074v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      22. An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/An_LLM-powered_Natural-to-Robotic_Language_Translation_Framework_with_Correctness_Guarantees.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-Sen University</span></p>
<p>本文提出NRTrans框架，通过大语言模型（LLM）结合自定义机器人技能语言（RSL），实现从自然语言任务到机器人控制程序的自动翻译。其关键方法包括RSL编译器用于验证程序正确性，以及基于错误反馈的程序微调机制，显著提升轻量级LLM在机器人控制代码生成中的可靠性和成功率。实验表明，该框架在多种任务和模型下均优于现有方法，尤其在受限设备上也能实现高成功率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/An_LLM-powered_Natural-to-Robotic_Language_Translation_Framework_with_Correctness_Guarantees.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18820v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      23. AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/AS2FM__Enabling_Statistical_Model_Checking_of_ROS_2_Systems_for_Robust_Autonomy.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bosch Research, Robert Bosch GmbH</span></p>
<p>本文提出了一种基于统计模型检测（SMC）的方法，通过扩展SCXML格式以支持ROS 2通信和行为树（BT）特性，并开发AS2FM工具，将完整的机器人系统模型转换为JANI格式，实现对自主机器人系统的形式化验证。实验证明，该方法能高效发现ROS 2机器人系统中的设计缺陷，并在模型规模扩展时表现出优越的线性可扩展性，提升了系统鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/AS2FM_Enabling_Statistical_Model_Checking_of_ROS_2_Systems_for_Robust_Autonomy.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18802v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      24. HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/HyperTASR__Hypernetwork-Driven_Task-Aware_Scene_Representations_for_Robust_Manipulation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Hong Kong</span></p>
<p>该论文提出HyperTASR，一种基于超网络的任务感知场景表示框架，通过动态生成编码参数，根据任务目标与执行阶段自适应地调整视觉特征，显著提升多任务机器人操作效率与泛化能力。实验证明，HyperTASR在仿真与现实环境下均能有效聚焦任务相关场景信息，带来新的单视角操作性能最优，并具备更强的跨任务泛化特性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/HyperTASR_Hypernetwork-Driven_Task-Aware_Scene_Representations_for_Robust_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18797v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      25. CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/CausalMACE__Causality_Empowered_Multi-Agents_in_Minecraft_Cooperative_Tasks.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>该论文提出了CausalMACE框架，通过引入因果推理模块与全局任务图，实现了LLM驱动的多智能体在Minecraft中的协作任务规划与执行。方法包括任务分解、因果依赖建图、任务分配与自反机制，显著提升了多智能体协作效率和任务完成率。实验结果显示本方法在多智能体和单智能体任务上均优于现有主流基线，结构化因果规划有效提升了任务执行的准确性和效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/CausalMACE_Causality_Empowered_Multi-Agents_in_Minecraft_Cooperative_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18937v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      26. VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/VisionSafeEnhanced_VPC__Cautious_Predictive_Control_with_Visibility_Constraints_under_Uncertainty_fo.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tongji University</span></p>
<p>本论文提出了VisionSafeEnhanced VPC，一种融合高斯过程回归(GPR)的不确定性自适应视觉预测控制框架，通过联合轨迹优化与基于控制屏障函数(CBF)的安全约束，实现了在不确定环境下自主腹腔镜保持视野安全。方法通过GPR对系统残差和外部扰动进行混合（确定性+随机）建模，并将其不确定性传播到CBF和机会约束中以保证概率安全，降低无谓运动。实验表明该方法在商用手术机器人平台上实现了99.9%以上的目标可见性，显著提升了鲁棒性和精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/VisionSafeEnhanced_VPC_Cautious_Predictive_Control_with_Visibility_Constraints_under_Uncertainty_for_Autonomous_Robotic_Surgery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18817v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      27. Learning Real-World Acrobatic Flight from Human Preferences
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Learning_Real-World_Acrobatic_Flight_from_Human_Preferences.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Zurich</span></p>
<p>该论文提出了一种基于人类偏好的概率化强化学习方法REC Preference PPO，用于训练无人机执行复杂特技飞行动作，无需人工奖励函数。方法通过集成奖励模型量化人类偏好中的不确定性，并在仿真和真实无人机上验证，结果显示其性能达到手工奖励的88.4%，优于传统偏好RL，并能学习新颖技能。结论认为，该方法显著提升了复杂机器人任务中偏好学习的效能，尤其适用于奖励难以设计的场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Learning_Real-World_Acrobatic_Flight_from_Human_Preferences.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18696v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      28. ColorGS: High-fidelity Surgical Scene Reconstruction with Colored Gaussian Splatting
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/ColorGS__High-fidelity_Surgical_Scene_Reconstruction_with_Colored_Gaussian_Splatting.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Aeronautics and Astronautics</span></p>
<p>该论文提出ColorGS框架，通过引入Colored Gaussian Primitives实现空间自适应的颜色编码，并结合增强型形变建模（EDM），提升了手术场景中组织重建的颜色表达和全局形变一致性。在DaVinci机器人手术视频和医学基准数据集上，ColorGS表现优异，PSNR提升至39.85，SSIM达97.25%，实现实时高保真重建，显著优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/ColorGS_High-fidelity_Surgical_Scene_Reconstruction_with_Colored_Gaussian_Splatting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18694v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      29. AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting Variability with a Field Robot
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/AgriChrono__A_Multi-modal_Dataset_Capturing_Crop_Growth_and_Lighting_Variability_with_a_Field_Robot.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea University</span></p>
<p>该论文提出了AgriChrono，一种集成多种传感器（RGB、深度、LiDAR、IMU）的田间机器人数据采集平台及其大规模多模态数据集，专为记录真实农业环境中作物生长和光照变化而设计。通过对现有3D重建模型在该数据集上的基准测试，发现当前方法在真实动态场景下表现有限，AgriChrono为提升模型泛化和鲁棒性提供了关键资源。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/AgriChrono_A_Multi-modal_Dataset_Capturing_Crop_Growth_and_Lighting_Variability_with_a_Field_Robot.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18691v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      30. Deep Sensorimotor Control by Imitating Predictive Models of Human Motion
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Deep_Sensorimotor_Control_by_Imitating_Predictive_Models_of_Human_Motion.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">UC Berkeley</span></p>
<p>本文提出了一种通过模仿人类运动预测模型来训练机器人传感-运动策略的新方法。其核心做法是用Transformer架构在大规模人类与物体交互数据上训练未来关键点运动预测模型，并在机器人上用强化学习优化跟踪该预测模型的奖励信号，从而显著减少对复杂稠密奖励设计的依赖，避免了传统运动重定向和对抗损失的弊端。实验证明该方法在多种机器手和操作任务上均优于现有基线方法，且能不依赖人工工程奖励实现高效学习。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Deep_Sensorimotor_Control_by_Imitating_Predictive_Models_of_Human_Motion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18705v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      31. Enhancing Video-Based Robot Failure Detection Using Task Knowledge
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Enhancing_Video-Based_Robot_Failure_Detection_Using_Task_Knowledge.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fraunhofer Institute for Intelligent Analysis and Information Systems</span></p>
<p>本文提出了一种基于视频的机器人任务失败检测方法，通过利用机器人动作的时序边界和任务相关物体信息对视频帧进行选择和预处理。采用MViT-B等视频分类模型，并引入可变帧率的数据增强技术，在ARMBench等数据集上实现了F1得分从77.9提升到81.4，无需额外计算开销。实验表明，结合任务知识进行帧选择和预处理能有效提升机器人失败检测性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Enhancing_Video-Based_Robot_Failure_Detection_Using_Task_Knowledge.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18627v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      32. Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Integration_of_Robot_and_Scene_Kinematics_for_Sequential_Mobile_Manipulation_Planning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)</span></p>
<p>本文提出了一种顺序移动操作规划（SMMP）框架，通过将环境和机器人运动学统一为增强配置空间（A-Space），实现了机器人与多种可关节物体的协调全身动作规划。方法包括基于AKR的运动建模、三层级规划（任务规划、优化运动规划、计划细化），显著提升长时序复杂操作任务的成功率和通用性。实验表明，该方法在仿真和真实机器人上均能高效执行多步、复杂场景下的移动操作任务，任务成功率比传统方法提升84.6%，并具备良好的可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Integration_of_Robot_and_Scene_Kinematics_for_Sequential_Mobile_Manipulation_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19242v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      33. Autoregressive Universal Video Segmentation Model
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Autoregressive_Universal_Video_Segmentation_Model.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">NVIDIA</span></p>
<p>该论文提出了AUSM，一种统一的自回归视频分割模型，通过将视频分割任务重新表述为语言建模中的顺序掩码预测，融合了prompted和unprompted视频分割。AUSM采用基于State Space Models的架构，包含History Marker和History Compressor模块，实现高效的时空信息压缩和并行训练，支持任意长度的视频流处理。实验表明AUSM在多个基准（如DAVIS17、YouTube-VOS、MOSE、YouTube-VIS、OVIS）上性能优越，并且训练速度较现有方法提升至2.5倍。结论是AUSM实现了高效统一的视频分割，推动了视频感知任务的范式转变。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Autoregressive_Universal_Video_Segmentation_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19209v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      34. OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/OmniHuman-1.5__Instilling_an_Active_Mind_in_Avatars_via_Cognitive_Simulation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Intelligent Creation Lab, ByteDance</span></p>
<p>本论文提出OmniHuman-1.5框架，通过结合多模态大语言模型（MLLM）实现角色动画的高层语义引导，模拟人类认知中的“System 1”（反应型）与“System 2”（推理型）双系统。方法包括MLLM驱动的多步推理生成动作计划，以及多模态Diffusion Transformer融合语音、图像和文本信号，采用伪末帧策略解决模态冲突。实验表明，该方法在自然度、动作一致性和多场景泛化能力上均优于现有技术，生成的动画动作更具语义连贯性和表达力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/OmniHuman-1.5_Instilling_an_Active_Mind_in_Avatars_via_Cognitive_Simulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19195v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      35. All-in-One Slider for Attribute Manipulation in Diffusion Models
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/All-in-One_Slider_for_Attribute_Manipulation_in_Diffusion_Models.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Jiaotong University</span></p>
<p>本文提出了All-in-One Slider框架，核心为Attribute Sparse Autoencoder（稀疏自编码器），通过对扩散模型的文本嵌入空间进行稀疏分解，实现了多种图像属性的可解释、连续且细粒度操控。该方法无需为每个属性单独训练模块，支持多属性组合编辑及对未见属性的零样本泛化，在保持身份一致性的同时提升了属性操控的灵活性和扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/All-in-One_Slider_for_Attribute_Manipulation_in_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19188v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      36. FASTMESH: Efficient Artistic Mesh Generation via Component Decoupling
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/FastMesh_Efficient_Artistic_Mesh_Generation_via_Component_Decoupling.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>FASTMESH提出了一种高效的3D艺术网格生成框架，通过将顶点和面片的生成解耦，分别采用自回归和双向Transformer模型，减少冗余显著提升生成效率。该方法结合精度增强器与预测过滤策略，最终在Toys4K等数据集上实现了高质量、8倍加速的网格生成，优于现有主流方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/FASTMESH_Efficient_Artistic_Mesh_Generation_via_Component_Decoupling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19167v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      37. Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Beyond_flattening__a_geometrically_principled_positional_encoding_for_vision_transformers_with_Weier.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Jiangnan University</span></p>
<p>本文提出了Weierstrass Elliptic Function Positional Encoding (WEF-PE)，通过在复平面映射图像二维坐标，利用椭圆函数的双周期和非线性几何特性，替代传统一维可学习位置编码，实现空间距离与编码距离的单调对应。理论证明其距离衰减性质，并在CIFAR-100和VTAB-1k等基准上实验证明从头训练与微调均优于现有方法，增强了Vision Transformer对空间结构的感知能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Beyond_flattening_a_geometrically_principled_positional_encoding_for_vision_transformers_with_Weierstrass_elliptic_functions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19104v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      38. Composition and Alignment of Diffusion Models using Constrained Learning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Composition_and_Alignment_of_Diffusion_Models_using_Constrained_Learning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Pennsylvania</span></p>
<p>本文提出了一种基于约束优化的统一框架，实现扩散模型的对齐（alignment）和组合（composition），通过引入奖励约束和KL散度约束，同时理论分析了优化问题的解，并开发了拉格朗日原始-对偶训练算法。实验显示在图像生成任务中，该方法能有效同时满足多重约束，生成质量优于等权重基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Composition_and_Alignment_of_Diffusion_Models_using_Constrained_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19030v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      39. GReAT: leveraging geometric artery data to improve wall shear stress assessment
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/GReAT__leveraging_geometric_artery_data_to_improve_wall_shear_stress_assessment.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University of Munich</span></p>
<p>本文提出了一种自监督预训练框架GReAT，利用大规模血管几何模型（MedShapeNet-Blood-Vessel，8449例）通过热核特征（heat kernel signature）进行几何表示学习，再将其融合进Transformer架构中，提升了在小样本（49例）冠状动脉壁面切应力（WSS）三分类分割任务中的表现。实验显示，GReAT在少量数据情况下能显著提升WSS分割准确率，且其几何特征学习可在不依赖标签的情况下迁移到下游生物医学任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/GReAT_leveraging_geometric_artery_data_to_improve_wall_shear_stress_assessment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18971v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      40. Can we make NeRF-based visual localization privacy-preserving?
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Can_we_make_NeRF-based_visual_localization_privacy-preserving_.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Faculty of Electrical Engineering, Czech Technical University in Prague</span></p>
<p>本文提出了一种NeRF变体ppNeSF（Privacy-Preserving Neural Segmentation Field），通过用分割标签代替RGB监督，实现了视觉定位的隐私保护。方法采用自监督学习在2D&#x2F;3D联合特征空间中通过最优传输生成分割标签，并对几何和分割场进行优化，同时引入层次化分割和不确定性建模提升表现。实验表明：ppNeSF能在不暴露细粒度隐私信息的前提下，取得媲美甚至超过非隐私保护方法的定位精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Can_we_make_NeRF-based_visual_localization_privacy-preserving.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19154v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      41. RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/RDDM__Practicing_RAW_Domain_Diffusion_Model_for_Real-world_Image_Restoration.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huawei Noah’s Ark Lab</span></p>
<p>本文提出了RDDM，一种端到端的RAW域扩散模型，能直接从传感器RAW数据恢复高保真、高感知质量的图像。核心方法包括RAW域VAE（RVAE）以适应RAW数据分布，配置化多Bayer LoRA模块兼容多种RAW模式，以及可扩展的数据合成管线用于生成训练样本。实验结果表明，RDDM在多个指标上优于最先进的sRGB域扩散方法，生成的图像更清晰、细节更丰富并且兼容多种实际应用场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/RDDM_Practicing_RAW_Domain_Diffusion_Model_for_Real-world_Image_Restoration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19112v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      42. Random forest-based out-of-distribution detection for robust lung cancer segmentation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Random_forest-based_out-of-distribution_detection_for_robust_lung_cancer_segmentation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Memorial Sloan Kettering Cancer Center</span></p>
<p>本文提出了一种基于随机森林（RF-Deep）的分布外（OOD）检测方法，通过提取经SimMIM自监督预训练的Swin Transformer编码器深度特征，检测CT肺癌分割任务中的OOD样本。实验表明，RF-Deep在多种近域和远域OOD场景下均显著优于MaxSoftmax等主流方法，提高了自动分割模型在实际临床中的可靠性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Random_forest-based_out-of-distribution_detection_for_robust_lung_cancer_segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19003v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      43. RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/RoofSeg__An_edge-aware_transformer-based_network_for_end-to-end_roof_plane_segmentation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Remote Sensing and Information Engineering, Wuhan University</span></p>
<p>本文提出了RoofSeg，一种基于transformer的端到端屋顶平面分割网络。方法采用PointNet++提取多尺度点云特征，通过transformer编码解码器和可学习平面查询实现平面实例掩码预测，并引入Edge-Aware Mask Module（EAMM）利用点到平面距离增强边缘特征判别力，同时设计自适应加权损失与几何约束保证分割精度。实验结果表明，RoofSeg在三大基准数据集上显著优于现有方法，准确分割屋顶平面且边缘精确、误分类点极少、几何一致性高。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/RoofSeg_An_edge-aware_transformer-based_network_for_end-to-end_roof_plane_segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18960v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      44. Enhancing compact convolutional transformers with super attention
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Enhancing_compact_convolutional_transformers_with_super_attention.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">C.V. Raman Global University</span></p>
<p>该论文提出了一种结合token mixing、序列池化和卷积tokenizer的新型紧凑型卷积Transformer架构，并引入super attention机制以提升在固定上下文长度任务中的性能和推理效率。经CIFAR100基准测试，模型在无需数据增强、位置嵌入或学习率调度的情况下，相较于传统Scaled Dot Product Attention（SDPA）Transformer，在top-1%和top-5%验证准确率上分别提升了约10%，且参数量更少、收敛更快，训练过程更稳定，泛化能力更强。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Enhancing_compact_convolutional_transformers_with_super_attention.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18959v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      45. Generative AI in Map-Making: A Technical Exploration and Its Implications for Cartographers
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Generative_AI_in_Map-Making__A_Technical_Exploration_and_Its_Implications_for_Cartographers.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Cartography and Geoinformation, ETH Zurich</span></p>
<p>本文提出了一种结合ControlNet和Stable Diffusion的生成式AI方法，通过融合向量数据和文本提示，实现对地图空间布局和语义的精准控制，可自动生成现代与历史风格地图。用户研究显示，该方法在现代地图生成上表现优异，提升了制图效率和可用性，未来需进一步提升历史地图的准确性和风格一致性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Generative_AI_in_Map-Making_A_Technical_Exploration_and_Its_Implications_for_Cartographers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18939v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      46. Preliminary Study on Space Utilization and Emergent Behaviors of Group vs. Single Pedestrians in Real-World Trajectories
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Preliminary_Study_on_Space_Utilization_and_Emergent_Behaviors_of_Group_vs._Single_Pedestrians_in_Rea.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Osaka</span></p>
<p>本文提出了基于真实轨迹数据的群体与单体行人空间利用和行为差异分析框架，采用Transformer架构的配对分类模型对行人进行分组识别，并结合时序分段和空间&#x2F;行为度量（如凸包面积、最小包围圆、速度和方向变化等）系统分析其空间占用和互动行为。实验表明群体行人在空间占用和行为模式上具备更强的空间主导效应，影响其他行人的运动路径和速度，对公共空间设计与仿真有参考价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Preliminary_Study_on_Space_Utilization_and_Emergent_Behaviors_of_Group_vs._Single_Pedestrians_in_Real-World_Trajectories.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18896v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      47. DQEN: Dual Query Enhancement Network for DETR-based HOI Detection
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/DQEN__Dual_Query_Enhancement_Network_for_DETR-based_HOI_Detection.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ningbo University</span></p>
<p>本文提出了DQEN（Dual Query Enhancement Network），一种针对DETR框架的端到端人-物交互（HOI）检测新方法，通过对象查询增强模块（OQE）和交互查询增强模块（IQE）分别利用对象感知特征和CLIP语义特征对查询进行优化，并增加辅助动词预测单元以提升交互识别能力。实验结果表明，DQEN在HICO-Det和V-COCO数据集上实现了业界竞争性的检测性能，尤其提升了对稀有类别和零样本场景的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/DQEN_Dual_Query_Enhancement_Network_for_DETR-based_HOI_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18886v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      48. Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via Text-Guided Attribute-Disentangled Prompt Learning for Vision–Language Models
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Toward_Robust_Medical_Fairness__Debiased_Dual-Modal_Alignment_via_Text-Guided_Attribute-Disentangled.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, Northwestern Polytechnical University</span></p>
<p>该论文提出DualFairVL框架，通过文本引导的正交锚点、跨模态注意力和超网络对视觉-语言模型中的敏感与目标属性进行显式解耦，实现双模态公平对齐与鲁棒泛化。实验表明，该方法在五个医学影像分布内与三个跨域任务中，显著提升公平性与准确率，优于全参数和高效微调基线，参数仅3.6M。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Toward_Robust_Medical_Fairness_Debiased_Dual-Modal_Alignment_via_Text-Guided_Attribute-Disentangled_Prompt_Learning_for_Vision%E2%80%93Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18829v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      49. Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Deep_Pre-trained_Time_Series_Features_for_Tree_Species_Classification_in_the_Dutch_Forest_Inventory.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wageningen University</span></p>
<p>该论文提出利用预训练的遥感时间序列Transformer模型（Presto）对多源卫星数据进行特征提取，并通过微调结合多层感知机（MLP）进行荷兰林木树种分类，显著优于传统的基于手工特征和随机森林（RF）的方法。结论显示，深度学习特征在各种数据集上均大幅提升分类准确率，尤其在数据平衡和样本量大的情况下提升最显著，为国家级林业资源普查自动化和大规模应用奠定技术基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Deep_Pre-trained_Time_Series_Features_for_Tree_Species_Classification_in_the_Dutch_Forest_Inventory.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18949v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      50. Energy-Based Flow Matching for Generating 3D Molecular Structure
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Energy-Based_Flow_Matching_for_Generating_3D_Molecular_Structure.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KTH Royal Institute of Technology</span></p>
<p>该论文提出了一种基于能量的流匹配方法（IDFlow），将重构误差作为能量函数，通过深度神经网络实现对分子三维结构的生成和迭代精细化。方法在分子对接和蛋白质主链生成任务上，通过对比实验展示了在结构生成准确性和多样性方面超过现有流匹配和扩散模型的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Energy-Based_Flow_Matching_for_Generating_3D_Molecular_Structure.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18904v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      51. Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Event-Enriched_Image_Analysis_Grand_Challenge_at_ACM_Multimedia_2025.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science, VNU-HCM, Vietnam</span></p>
<p>本文提出了EVENTA 2025大赛，首次构建了面向事件语义的大规模图像检索与描述基准，通过引入上下文、时序、语义等多模态信息，推动图像从表层识别走向事件级理解。结果显示检索增强生成（RAG）和多模态融合等方法显著提升了事件丰富型描述和检索的表现，但在事实核查和叙事一致性方面仍有挑战，未来需结合外部知识和更优评测机制。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Event-Enriched_Image_Analysis_Grand_Challenge_at_ACM_Multimedia_2025.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18825v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      52. Embedding Font Impression Word Tags Based on Co-occurrence
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Embedding_Font_Impression_Word_Tags_Based_on_Co-occurrence.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kyushu University</span></p>
<p>本文提出了一种基于共现关系的字体印象词标签嵌入方法，通过构建印象标签共现图并采用谱嵌入，将字体印象词映射为反映字体形状特征的向量。实验结果显示，该方法在字体印象建模、缺失&#x2F;噪声标签识别和印象导向的字体生成任务中，均优于BERT和CLIP等主流文本嵌入模型，尤其在印象标签稀缺时表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Embedding_Font_Impression_Word_Tags_Based_on_Co-occurrence.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18799v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      53. Robust and Label-Efficient Deep Waste Detection
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Robust_and_Label-Efficient_Deep_Waste_Detection.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mohamed Bin Zayed University of Artificial Intelligence</span></p>
<p>该论文提出了一种用于工业废弃物检测的深度学习框架，结合了先进的Open-Vocabulary Object Detection（OVOD）模型基线、LLM优化的文本提示、transformer检测器的监督微调，以及基于集成的软伪标签半监督学习流程。实验结果表明，优化的文本提示和模型微调显著提升了检测性能，而集成软伪标签能在无监督数据上训练出高质量模型，提升各类别检测表现，甚至超越全监督训练。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Robust_and_Label-Efficient_Deep_Waste_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18772v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      54. Beyond the Textual: Generating Coherent Visual Options for MCQs
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Beyond_the_Textual__Generating_Coherent_Visual_Options_for_MCQs.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Normal University</span></p>
<p>该论文提出了Cross-modal Options Synthesis (CmOS) 框架，结合多模态链式推理(MCoT)和检索增强生成(RAG)，实现了从文本和图像输入自动生成带有高质量视觉干扰项的多项选择题。实验表明，该方法在内容判别、题目生成和视觉选项生成任务上均优于现有方法，推动了教育领域多模态资源的自动化生成与个性化学习。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Beyond_the_Textual_Generating_Coherent_Visual_Options_for_MCQs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18733v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      55. Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Drawing2CAD__Sequence-to-Sequence_Learning_for_CAD_Generation_from_Vectorized_Drawings.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hangzhou Dianzi University</span></p>
<p>该论文提出了Drawing2CAD框架，将CAD生成任务重定义为序列到序列学习问题，直接从2D矢量工程图（SVG）生成3D参数化CAD模型。方法包括：网络友好的向量图元表示、命令与参数解耦的双解码器Transformer架构及支持参数柔性调整的软目标分布损失。实验证明，矢量输入在命令准确率、参数精度和模型有效性方面均优于像素输入，并发布了大规模配对数据集CAD-VGDrawing。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Drawing2CAD_Sequence-to-Sequence_Learning_for_CAD_Generation_from_Vectorized_Drawings.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18695v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      56. A Novel Deep Hybrid Framework with Ensemble-Based Feature Optimization for Robust Real-Time Human Activity Recognition
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/A_Novel_Deep_Hybrid_Framework_with_Ensemble-Based_Feature_Optimization_for_Robust_Real-Time_Human_Ac.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HITEC University</span></p>
<p>本文提出了一种新颖的混合深度学习框架，将定制的InceptionV3网络用于帧级空间特征提取，LSTM用于建模时序依赖，并结合了带有自适应动态适应和注意力机制的遗传算法（ADFSA）进行特征优化选择，实现了在视频人类活动识别中的高效、紧凑特征抽取。实验结果表明，该方法在UCF-YouTube数据集上将特征维数从128降至7，准确率达99.65%，并显著提升推理速度和资源效率，适用于实时和边缘设备部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/A_Novel_Deep_Hybrid_Framework_with_Ensemble-Based_Feature_Optimization_for_Robust_Real-Time_Human_Activity_Recognition.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18693v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      57. Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Feature-Space_Planes_Searcher__A_Universal_Domain_Adaptation_Framework_for_Interpretability_and_Comp.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology</span></p>
<p>该论文提出了Feature-space Planes Searcher (FPS)，一种通用的领域自适应框架。FPS在冻结预训练特征编码器的基础上，仅优化分类器的决策边界，利用特征空间中的几何结构（类内聚类与类间分离），结合熵最小化、类别熵均衡和随机池化一致性正则，减小源域与目标域的决策边界错位，从而高效实现无标签目标域的迁移。大规模实验表明，FPS在Office-31、Office-Home、VisDA-2017等视觉基准以及蛋白结构预测、遥感分类、地震检测等多领域任务中均取得了与当前最佳方法相当或更优的性能，同时显著提升了计算效率和可解释性。结论认为，FPS为领域自适应和迁移学习提供了高效、解释性强且具通用性的范式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Feature-Space_Planes_Searcher_A_Universal_Domain_Adaptation_Framework_for_Interpretability_and_Computational_Efficiency.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18664v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      58. SFormer: SNR-guided Transformer for Underwater Image Enhancement from the Frequency Domain
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/SFormer__SNR-guided_Transformer_for_Underwater_Image_Enhancement_from_the_Frequency_Domain.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Macau</span></p>
<p>该论文提出了一种基于SNR先验的Transformer网络SFormer用于水下图像增强，创新性地在频率域利用SNR指导，通过FAST模块实现傅里叶域幅值-相位交互与SNR注意力，FAT模块自适应融合低高频信息，嵌入U型结构，显著提升水下图像色彩、细节与对比度。实验结果表明，该方法在PSNR、SSIM等指标上超越现有主流方法，具备更优的视觉效果和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/SFormer_SNR-guided_Transformer_for_Underwater_Image_Enhancement_from_the_Frequency_Domain.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18634v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      59. OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/OwlCap__Harmonizing_Motion-Detail_for_Video_Captioning_via_HMD-270K_and_Caption_Set_Equivalence_Rewa.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huazhong University of Science and Technology</span></p>
<p>本文提出OwlCap模型，通过构建包含运动与细节均衡描述的大规模视频字幕数据集HMD-270K，并采用两阶段流程（运动-细节融合与细粒度检验）提升数据质量；在优化方面，结合基于GRPO的Caption Set Equivalence Reward（CSER）强化学习训练，促进字幕的完整性和正确性。实验表明，OwlCap在VDC与DREAM-1K等主流基准上均显著优于现有方法，实现了视频字幕运动与细节的双重平衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/OwlCap_Harmonizing_Motion-Detail_for_Video_Captioning_via_HMD-270K_and_Caption_Set_Equivalence_Reward.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18608v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      60. eSkinHealth: A Multimodal Dataset for Neglected Tropical Skin Diseases
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/eSkinHealth__A_Multimodal_Dataset_for_Neglected_Tropical_Skin_Diseases.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tulane University</span></p>
<p>本文提出eSkinHealth，一个专注于西非被忽视热带皮肤病的多模态皮肤病数据集，包含5,623张图片、47种疾病，并配有AI-专家协作注释流程，结合了多模态大模型（MLLM）和SAM分割模型，在皮肤科医生指导下高效生成标注，包括病例元数据、语义分割掩码、实例级视觉描述和临床概念。实验结果表明，现有先进视觉模型在该数据集上表现有限，数据集和注释框架将推动更公平、准确、可解释的AI辅助皮肤病诊断工具的开发。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/eSkinHealth_A_Multimodal_Dataset_for_Neglected_Tropical_Skin_Diseases.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18597v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      61. SemLayoutDiff: Semantic Layout Generation with Diffusion Model for Indoor Scene Synthesis
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/SemLayoutDiff__Semantic_Layout_Generation_with_Diffusion_Model_for_Indoor_Scene_Synthesis.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Simon Fraser University</span></p>
<p>该论文提出SemLayoutDiff，一种基于多类别扩散模型的室内三维场景生成方法，核心流程包括：利用2D语义布局图（融合建筑结构元素如门窗、地板）作为场景表达，经离散扩散模型生成语义分布，再借助交叉注意力网络预测家具的空间属性，最终通过检索和布置三维对象重建完整场景。实验显示该方法在多房型统一建模、建筑约束适应性、空间合理性及分布匹配等方面明显优于DiffuScene和MiDiffusion等主流方法，生成场景更加真实、可导航且布局合理。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/SemLayoutDiff_Semantic_Layout_Generation_with_Diffusion_Model_for_Indoor_Scene_Synthesis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18649v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      62. PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/PRISM__Robust_VLM_Alignment_with_Principled_Reasoning_for_Integrated_Safety_in_Multimodality.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Wisconsin-Madison</span></p>
<p>本文提出PRISM框架，通过引入结构化的安全链式推理（PRISM-CoT数据集）与基于蒙特卡洛树搜索生成的偏好优化数据（PRISM-DPO），提升视觉语言模型（VLM）的多模态安全对齐能力。该方法显著降低多种静态与自适应攻击下的攻击成功率，并在保障安全的同时保持甚至提升模型的有用性，验证了结构化推理结合偏好优化是实现高鲁棒性与实用性VLM安全对齐的有效方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/PRISM_Robust_VLM_Alignment_with_Principled_Reasoning_for_Integrated_Safety_in_Multimodality.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18633v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      63. ROSE: Remove Objects with Side Effects in Videos
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/ROSE__Remove_Objects_with_Side_Effects_in_Videos.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出ROSE框架，基于扩散Transformer模型进行视频对象移除，同时有效处理如阴影、反射、镜像、光照和半透明等对象的环境副作用。方法通过3D渲染引擎自动生成大规模多样化的配对合成数据，并引入差异掩码监督以显式定位副作用区域。实验表明ROSE在合成和真实视频中均优于现有方法，能泛化到复杂真实场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/ROSE_Remove_Objects_with_Side_Effects_in_Videos.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18613v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      64. ModAn-MulSupCon: Modality- and Anatomy-Aware Multi-Label Supervised Contrastive Pretraining for Medical Imaging
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/ModAn-MulSupCon__Modality-and_Anatomy-Aware_Multi-Label_Supervised_Contrastive_Pretraining_for_Medic.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tohoku University Hospital</span></p>
<p>本文提出了一种结合模态和解剖部位元信息的多标签有监督对比预训练方法ModAn-MulSupCon，将医学影像的模态与解剖区域编码为多热向量，并采用Jaccard加权多标签对比损失对ResNet-18进行预训练。实验证明，该方法在允许全参数微调下，能有效提升下游医学图像分类任务的准确率，优于SimCLR和ImageNet等主流预训练方式，但冻结编码器时表现在部分任务不如SimCLR。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/ModAn-MulSupCon_Modality-_and_Anatomy-Aware_Multi-Label_Supervised_Contrastive_Pretraining_for_Medical_Imaging.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19229v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      65. STEPWISER: Stepwise Generative Judges for Wiser Reasoning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/StepWiser__Stepwise_Generative_Judges_for_Wiser_Reasoning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">FAIR at Meta</span></p>
<p>STEPWISER提出了一种针对大语言模型多步推理过程的奖励判别方法，通过自监督分割推理链为“chunks-of-thought”，并利用蒙特卡洛采样对每一步进行质量评估，获得标签后采用强化学习训练生成式判别模型（Judge）以CoT方式推理每步是否正确。实验证明STEPWISER在ProcessBench等多项数学推理任务中显著优于传统判别模型和现有方法，并能用于训练时提升推理能力及推理时动态纠错。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/STEPWISER_Stepwise_Generative_Judges_for_Wiser_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19228v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      66. Predicting the Order of Upcoming Tokens Improves Language Modeling
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Predicting_the_Order_of_Upcoming_Tokens_Improves_Language_Modeling.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">MBZUAI</span></p>
<p>本文提出了一种新的大语言模型辅助训练目标——Token Order Prediction（TOP），以学习-to-rank损失对未来token顺序进行预测，并与传统的Next-Token Prediction（NTP）联合训练。实验表明，在340M、1.8B和7B参数规模下，TOP在八个主流NLP基准测试中整体优于NTP和Multi-Token Prediction（MTP），且参数效率更高。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Predicting_the_Order_of_Upcoming_Tokens_Improves_Language_Modeling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19227v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      67. Generative Interfaces for Language Models
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Generative_Interfaces_for_Language_Models.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>本论文提出了一种新方法，令大语言模型（LLM）根据用户查询主动生成适应性强的交互式用户界面（UI），而不仅仅返回传统线性文本回复。该方法利用结构化的界面表示和迭代优化流程，将自然语言查询转化为任务定制的UI，并通过多维度人机体验评测系统，系统性比较生成式界面与传统聊天界面的效果。实验结果显示，生成式界面在功能、互动性及用户满意度方面显著优于传统聊天界面，用户偏好率超过70%。结论表明，生成式界面能提升复杂任务的交互效率和用户体验，特别是在结构化、信息密集领域。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Generative_Interfaces_for_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19201v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      68. Understanding Tool-Integrated Reasoning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Understanding_Tool-Integrated_Reasoning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent</span></p>
<p>本论文首次提出并严格证明了工具集成推理（TIR）能够突破大型语言模型（LLM）在纯文本推理下的能力上限，通过引入外部工具（如Python解释器）极大扩展了模型的可达与可行解空间，并提出了优势塑造策略优化（ASPO）算法以稳定引导模型工具使用行为。实验表明，TIR模型在数学推理任务中显著优于纯文本模型，这一优势不仅体现在计算密集型问题，对抽象推理同样有效，并揭示了模型如何与工具协同形成新型推理策略。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Understanding_Tool-Integrated_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19163v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      69. MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/MATRIX__Multi-Agent_simulaTion_fRamework_for_safe_Interactions_and_conteXtual_clinical_conversationa.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ufonia Limited</span></p>
<p>该论文提出了MATRIX框架，用于临床对话型大模型的系统性、安全性评估，包括基于安全工程推导的场景-行为-失效模式分类库、LLM驱动的对话安全判别器BehvJudge（超越临床医生的危险识别能力），及高保真度的患者模拟代理PatBot，并在2,100条对话、10个临床领域和14种风险情景下对五种LLM进行了验证。实验结果表明，部分LLM在安全失效检测上已超越人类专家，MATRIX可为医疗AI提供可扩展、合规的自动化安全评估方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/MATRIX_Multi-Agent_simulaTion_fRamework_for_safe_Interactions_and_conteXtual_clinical_conversational_evaluation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19096v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      70. Trustworthy Agents for Electronic Health Records through Confidence Estimation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Trustworthy_Agents_for_Electronic_Health_Records_through_Confidence_Estimation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kyung Hee University</span></p>
<p>本文提出了Hallucination-Controlled Accuracy at k% (HCAcc@k%)，一种用于量化医疗场景下准确性与可靠性权衡的新型评估指标，并引入了结合逐步置信度估计的TrustEHRAgent。结果显示，TrustEHRAgent在高可靠性要求下（如HCAcc@70%）在MIMIC-III和eICU数据集上分别比基线方法提升44.23和25.34个百分点，能够在不确定时合理拒答，提升临床AI安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Trustworthy_Agents_for_Electronic_Health_Records_through_Confidence_Estimation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19076v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      71. HIPLAN: Hierarchical Planning for LLM Agents with Adaptive Global-Local Guidance
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/HiPlan__Hierarchical_Planning_for_LLM-Based_Agents_with_Adaptive_Global-Local_Guidance.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Science Library, Chinese Academy of Sciences</span></p>
<p>该论文提出了HIPLAN，一种为大语言模型（LLM）智能体设计的分层规划框架，通过自适应结合全局里程碑行动指引和局部逐步提示，增强其在复杂、长时序任务中的决策能力。方法上，HIPLAN离线构建专家演示的里程碑库，线上利用语义检索获取相似任务的里程碑和轨迹片段，分别用于生成全局任务分解和动态局部提示，实现全局方向保持与实时局部修正的协同。实验结果显示，HIPLAN在ALFWorld和WebShop两个挑战性基准上显著优于多种强基线，尤其提升了成功率、鲁棒性和决策效率，验证了其分层规划和经验复用机制的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/HIPLAN_Hierarchical_Planning_for_LLM_Agents_with_Adaptive_Global-Local_Guidance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19008v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      72. Sense of Self and Time in Borderline Personality: A Comparative Robustness Study with Generative AI
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Sense_of_Self_and_Time_in_Borderline_Personality._A_Comparative_Robustness_Study_with_Generative_AI.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute IDEAS, Warsaw</span></p>
<p>本论文系统比较了三种主流大型语言模型（GPT-4o、Gemini 2.5 Pro、Claude Opus 4）在对边缘型人格障碍（BPD）患者生命故事访谈的现象学主题分析中的表现。通过专家盲评和非盲评，评估模型输出与人类分析的语义一致性、有效性和细节丰富度。结果显示，Gemini模型的分析最接近人类，且在某些情况下难以被专家区分为AI生成；此外，AI模型能发现人类遗漏的主题，有助于提升分析敏感性和减少解释偏差，但整体仍略逊于人类专家。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Sense_of_Self_and_Time_in_Borderline_Personality_A_Comparative_Robustness_Study_with_Generative_AI.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19004v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      73. AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/AI_Models_Exceed_Individual_Human_Accuracy_in_Predicting_Everyday_Social_Norms.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute for Futures Studies, Stockholm, Sweden</span></p>
<p>本文系统评估了多种大型语言模型（如GPT-4.5、GPT-5、Claude Sonnet 4和Gemini 2.5 Pro）通过统计学习预测美国日常社会规范共识的能力，利用555个场景的人类评判数据，采用模型预测与人类个体表现的直接对比。结果显示，所有模型在预测社会规范共识上准确度超过绝大多数人类个体，部分模型表现已达到甚至超越所有参与者，但模型仍存在一致性的系统性误差，表明统计学习虽可高度建模文化知识，但仍有体验和情境推理的瓶颈。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/AI_Models_Exceed_Individual_Human_Accuracy_in_Predicting_Everyday_Social_Norms.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18983v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      74. Enabling MoE on the Edge via Importance-Driven Expert Scheduling
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Enabling_MoE_on_the_Edge_via_Importance-Driven_Expert_Scheduling.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本论文提出了一种适用于边缘设备的Mixture-of-Experts (MoE)大模型推理加速方法，通过引入基于专家重要性的动态专家调度机制，将低重要度的激活专家替换为已缓存在GPU中的功能相近专家，并对高分专家进行预取，结合CPU-GPU负载流水线优化，有效减少内存占用和PCIe数据传输。实验表明，该方法在保证几乎无损准确率的前提下，将解码延迟降低了48%，专家缓存命中率提升至60%以上，显著提升LLM在资源受限环境下的推理效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Enabling_MoE_on_the_Edge_via_Importance-Driven_Expert_Scheduling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19097v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      75. Reasoning LLMs in the Medical Domain: A Literature Survey
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Reasoning_LLMs_in_the_Medical_Domain__A_Literature_Survey.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fraunhofer IAIS</span></p>
<p>本文系统综述了大语言模型（LLM）在医疗领域的推理能力发展，重点分析了链式思维（Chain-of-Thought）和多智能体协作等关键技术，以及强化学习（RL）如DeepSeek-R1和Med-R1在提升模型推理泛化和可靠性方面的突破。结论认为，尽管LLM在医学推理、决策支持和多模态数据融合方面取得了显著进展，但在可解释性、安全性、偏见消除和临床实际部署等方面仍面临诸多挑战，需持续优化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Reasoning_LLMs_in_the_Medical_Domain_A_Literature_Survey.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19087v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      76. APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/APT-LLM__Exploiting_Arbitrary-Precision_Tensor_Core_Computing_for_LLM_Acceleration.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本文提出APT-LLM，一种针对大语言模型（LLM）在GPU上实现任意精度高效推理的加速方案。方法上，创新性地引入了bipolar-INT数据格式，实现了与有符号INT的高效无损转换，并提出了基于比特级分解重组的矩阵乘法（MatMul）方法，结合分层内存管理和自适应核映射优化GPU Tensor Core利用率。实验表明，APT-LLM在主流GPU上对LLM推理最高可达3.99×加速（相较FP16），且不损失精度，适合资源受限环境高效部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/APT-LLM_Exploiting_Arbitrary-Precision_Tensor_Core_Computing_for_LLM_Acceleration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19069v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      77. Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An Exploration of Scaling Laws by Difficulty
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Can_Structured_Templates_Facilitate_LLMs_in_Tackling_Harder_Tasks____An_Exploration_of_Scaling_Laws_.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, Beihang University</span></p>
<p>本文提出了Structured Solution Template (SST) 框架，通过结构化解题模板和难度递进课程训练，显式教授LLM程序化推理能力。核心流程包括三阶段：加权结构化模板微调、推理时动态模板注入以及整合计划-执行-自纠的课程训练，实验验证SST在GSM8K、AIME24、Dynamic En等多项基准上显著提升复杂推理任务的准确率和效率，尤其适用于难度较高的问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Can_Structured_Templates_Facilitate_LLMs_in_Tackling_Harder_Tasks_An_Exploration_of_Scaling_Laws_by_Difficulty.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19035v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      78. Investigating Advanced Reasoning of Large Language Models via Black-Box Interaction
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Investigating_Advanced_Reasoning_of_Large_Language_Models_via_Black-Box_Interaction.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Aeronautics and Astronautics</span></p>
<p>本论文提出黑盒交互（black-box interaction）新范式，通过让大语言模型（LLM）与隐藏规则环境多轮交互、推理输入输出对，系统评测其综合推理能力，并构建了包含6类任务96个黑盒的ORACLE基准，覆盖代码推理、物理系统、加密、谜题和博弈策略。对19个主流LLM实验证明，SOTA模型在简单黑盒可达70%+准确率，但在高难任务上普遍表现不足，显示出高效探索策略与高层次规划能力的缺失。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Investigating_Advanced_Reasoning_of_Large_Language_Models_via_Black-Box_Interaction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19029v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      79. When recalling in-context, Transformers are not SSMs
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/When_recalling_in-context,_Transformers_are_not_SSMs.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Max Planck Institute for Intelligent Systems</span></p>
<p>本论文系统分析了变换器（Transformer）与现代递归模型（如SSM、Mamba、Hyena）在多查询联想回忆（MQAR）任务上的表现。通过精细的超参数调优与结构消融，作者发现递归模型对优化（尤其是学习率）高度敏感，宽度扩展带来显著提升，而Transformer在单层情况下表现受限但训练中展现出类似归纳头的行为；Mamba等递归模型表现更鲁棒，且架构组件如卷积和门控并非优异表现的唯一原因。结论是：递归模型与Transformer在表达能力和优化稳定性上有本质差异，需进一步研究其优化与泛化机制。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/When_recalling_in-context_Transformers_are_not_SSMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18992v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      80. Automatic Prompt Optimization with Prompt Distillation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Automatic_Prompt_Optimization_with_Prompt_Distillation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ITMO University</span></p>
<p>该论文提出了DistillPrompt，一种新颖的非梯度自动提示优化方法，通过多阶段集成任务特定信息（包括候选生成、示例嵌入、指令压缩、候选聚合和迭代优化），系统性探索提示空间，优化大语言模型（LLM）的输入提示。实验表明DistillPrompt在多项文本分类和生成任务上显著优于现有非梯度自动提示方法，平均提升20.12%，展示了其高效和通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Automatic_Prompt_Optimization_with_Prompt_Distillation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18905v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      81. Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Interactive_Evaluation_of_Large_Language_Models_for_Multi-Requirement_Software_Engineering_Tasks.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National and Kapodistrian University of Athens</span></p>
<p>该论文提出了一种针对复杂软件工程任务的依赖驱动交互式评估框架，通过以需求依赖图建模任务、引入LLM“面试官”角色生成结构化反馈，对“面试者”模型进行多轮互动指导与修正，显著提升了对LLM代码生成能力的细粒度诊断和实用性评估。实验结果显示，许多静态评测中的模型失败可通过有针对性的反馈有效恢复，揭示了静态评测低估LLM能力的问题，同时也发现部分模型在多轮反馈下仍难以修正错误，反映出模型在解决复杂依赖任务和吸收反馈能力上的局限性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Interactive_Evaluation_of_Large_Language_Models_for_Multi-Requirement_Software_Engineering_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18880v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      82. Judicial Requirements for Generative AI in Legal Reasoning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Judicial_Requirements_for_Generative_AI_in_Legal_Reasoning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tampere University</span></p>
<p>本文系统梳理了生成式大语言模型（LLM）在司法决策中的核心能力需求，采用IRAC（问题-规则-应用-结论）模型分析法律推理，并聚焦于规则选择与规则应用阶段。论文评估了检索增强生成（RAG）、多智能体系统和神经符号AI等技术在满足法律裁决透明性、公正性和可解释性方面的潜力及局限。结论认为，当前AI在法律领域最有效的角色是处理简单高量案件的助手和复杂案件中专家的“陪练”，尚难以替代人类在复杂法律推理中的裁量权和透明度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Judicial_Requirements_for_Generative_AI_in_Legal_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18870v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      83. ReflectivePrompt: Reflective Evolution in Autoprompting Algorithms
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/ReflectivePrompt__Reflective_evolution_in_autoprompting_algorithms.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ITMO University</span></p>
<p>本文提出了ReflectivePrompt，一种结合反思进化算法与大语言模型的自动化提示优化方法。该方法在交叉和精英变异操作前引入短期和长期反思机制，通过积累和动态更新进化过程中的知识，有效提升了提示词的多样性和质量。实验在33个分类和文本生成数据集上，分别使用t-lite-instruct-0.1和gemma3-27b-it模型，显示ReflectivePrompt在F1分数和METEOR分数上平均优于现有进化算法类自动提示方法（如EvoPrompt、SPELL等），尤其在BBH基准测试集上提升显著（如F1提升6.59%，METEOR提升33.34%），验证了该方法在自动化提示优化领域的有效性和竞争力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/ReflectivePrompt_Reflective_Evolution_in_Autoprompting_Algorithms.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18847v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      84. ConfTuner: Training Large Language Models to Express Their Confidence Verbally
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/ConfTuner__Training_Large_Language_Models_to_Express_Their_Confidence_Verbally.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Singapore</span></p>
<p>该论文提出ConfTuner，一种针对大语言模型（LLM）进行高效微调的方法，通过“tokenized Brier score”损失函数将模型输出的语言化置信度与其真实正确概率对齐，无需真实置信度标签或代理估计。实验显示ConfTuner显著提升LLM在多任务和多模型上的置信度表达校准效果，并帮助系统实现更可靠的自我纠错和模型级联，提升可信度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/ConfTuner_Training_Large_Language_Models_to_Express_Their_Confidence_Verbally.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18812v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      85. STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/STARec__An_Efficient_Agent_Framework_for_Recommender_Systems_via_Autonomous_Deliberate_Reasoning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Gaoling School of Artificial Intelligence, Renmin University of China</span></p>
<p>该论文提出STARec框架，将每个用户建模为具备快慢双系统认知的自主LLM智能体，通过快速响应和链式思考实现个性化推荐与自我反思记忆更新。核心方法为锚定强化训练，先用有慢思考能力的大模型进行结构化知识蒸馏获得基础能力，再通过基于排名奖励的GRPO强化学习，实现策略动态自适应。实验在MovieLens 1M和Amazon CDs数据集上，STARec模型即使只用0.4%的训练数据，在推荐准确性和个性化上均大幅超越传统和最新基线，且对数据稀疏和冷启动非常鲁棒。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/STARec_An_Efficient_Agent_Framework_for_Recommender_Systems_via_Autonomous_Deliberate_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18784v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      86. Insights into User Interface Innovations from a Design Thinking Workshop at deRSE25
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Insights_into_User_Interface_Innovations_from_a_Design_Thinking_Workshop_at_deRSE25.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ludwig-Maximilians-Universitat München</span></p>
<p>本文通过在deRSE25会议举办的设计思维工作坊，采用人本中心设计（HCD）流程，系统收集并分析科研用户对现有大语言模型（LLM）界面使用体验、需求与痛点。基于参与者反馈，提出并原型化了基于白板的动态UI方案，实现灵活上下文管理、动态分支和用户权重控制等功能，结论认为未来LLM发展应更加重视以用户为中心的界面创新。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Insights_into_User_Interface_Innovations_from_a_Design_Thinking_Workshop_at_deRSE25.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18929v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      87. Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Diverse_And_Private_Synthetic_Datasets_Generation_for_RAG_evaluation__A_multi-agent_framework.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AMADEUS France</span></p>
<p>本文提出了一种面向RAG（检索增强生成）系统评估的多智能体合成问答数据集生成框架，包括多样性智能体（利用聚类技术提升语义多样性）、隐私智能体（自动检测并伪匿名化敏感信息）、与问答策划智能体（生成高质量QA对）。实验结果表明，该框架生成的数据集在多样性和隐私保护上均优于现有方法，为RAG系统的安全与合规评估提供了有效工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Diverse_And_Private_Synthetic_Datasets_Generation_for_RAG_evaluation_A_multi-agent_framework.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18877v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      88. Optimization of Latent-Space Compression using Game-Theoretic Techniques for Transformer-Based Vector Search
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Optimization_of_Latent-Space_Compression_using_Game-Theoretic_Techniques_for_Transformer-Based_Vecto.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Engineering, KIIT Deemed to be University</span></p>
<p>本文提出了一种基于博弈论的潜空间压缩优化框架，采用深度自编码器和HNSW索引，对Transformer生成的向量进行压缩以提升语义检索效率与准确性。方法通过将压缩与检索建模为零和博弈，实现了检索精度和存储效率的平衡，实验对比FAISS显示，本方法在语义相似度和效用分数上大幅提升，检索时间略有增加。结论：该方法实现了高效、近无损的语义向量检索，适用于大模型检索场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Optimization_of_Latent-Space_Compression_using_Game-Theoretic_Techniques_for_Transformer-Based_Vector_Search.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18850v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      89. ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/ClusterFusion__Expanding_Operator_Fusion_Scope_for_LLM_Inference_via_Cluster-Level_Collective_Primit.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出ClusterFusion框架，通过设计两种集群级通信原语ClusterReduce和ClusterGather，在NVIDIA Hopper GPU上实现LLM推理阶段QKV Projection、Attention和Output Projection算子的高效集成融合。其方法利用集群级并行和片上共享内存，显著减少全局内存流量和内核启动开销，实验结果表明在多种主流模型和配置下端到端推理延迟平均提升1.61倍。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/ClusterFusion_Expanding_Operator_Fusion_Scope_for_LLM_Inference_via_Cluster-Level_Collective_Primitive.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18780v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      90. Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Harnessing_Rule-Based_Reinforcement_Learning_for_Enhanced_Grammatical_Error_Correction.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wangxuan Institute of Computer Technology, Peking University</span></p>
<p>本文提出了一种结合规则驱动的强化学习（RL）与大语言模型（LLM）的方法来提升中文语法纠错（GEC）任务性能。方法包括两阶段的监督微调（SFT）与基于格式与正确性双重奖励的GRPO强化学习，极大提升了LLM在GEC中的推理与纠错能力，实验在FCGEC等数据集上取得了最新最优成绩，显著提高了召回率，展现了RL范式在GEC任务中的可控性与泛化性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Harnessing_Rule-Based_Reinforcement_Learning_for_Enhanced_Grammatical_Error_Correction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18758v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      91. Text to Query Plans for Question Answering on Large Tables
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Text_to_Query_Plans_for_Question_Answering_on_Large_Tables.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CSIRO Data61</span></p>
<p>本文提出了一种名为Tree-Driven Sequential Operation QA System (TSO)的新框架，将自然语言查询直接转化为查询计划序列，利用LLM和ReAct框架，结合多层次向量索引，突破SQL和LLM上下文长度限制，支持对超大表格数据的复杂分析和灵活扩展。实验结果表明，该方法在Spider和超大农业科学数据集上均取得优异表现，显著提升了大规模结构化数据的自然语言问答能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Text_to_Query_Plans_for_Question_Answering_on_Large_Tables.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18749v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      92. Reflection-Enhanced Meta-Optimization: Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Reflection-Enhanced_Meta-Optimization_Integrating_TextGrad-style_Prompt_Optimization_with_Memory-Dri.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tongji University</span></p>
<p>本文提出了Reflection-Enhanced Meta-Optimization (REMO) 框架，结合了TextGrad式局部梯度优化、记忆增强的Reflection RAG模块（“错误笔记本”）和基于大模型的自适应优化器，实现优化历史的积累与复用。实验证明，REMO在GSM8K数学推理任务上显著提升了泛化能力和鲁棒性，有效缓解了过拟合问题，但计算开销增加。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Reflection-Enhanced_Meta-Optimization_Integrating_TextGrad-style_Prompt_Optimization_with_Memory-Driven_Self-Evolution.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18740v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      93. M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/M3HG__Multimodal,_Multi-scale,_and_Multi-type_Node_Heterogeneous_Graph_for_Emotion_Cause_Triplet_Ext.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tongji University</span></p>
<p>本论文提出M3HG模型，利用多模态异构图（融合文本、音频、视频）和多尺度语义信息（句内、句间）自动抽取对话中的情感-原因三元组。实验结果显示，M3HG在新构建的多场景多模态数据集MECAD和现有ECF数据集上，均显著优于当前主流方法，特别是多模态和复杂对话场景下的情感原因识别。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/M3HG_Multimodal_Multi-scale_and_Multi-type_Node_Heterogeneous_Graph_for_Emotion_Cause_Triplet_Extraction_in_Conversations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18724v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      94. Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Bias_Mitigation_Agent__Optimizing_Source_Selection_for_Fair_and_Balanced_Knowledge_Retrieval.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Salesforce</span></p>
<p>本文提出了一种创新的Bias Mitigation Agent多智能体架构，用于提升基于大型语言模型（LLM）的Agentic AI系统中知识检索的公平性与可信度。该方法在LLM驱动的agentic工作流中引入知识检索、偏见检测和源选择等多智能体协作，通过零样本和少样本两种源选择策略，动态甄别与过滤带偏见的外部知识源。实验表明，系统在不损失信息相关性的前提下，相较于朴素检索基线，最大可实现81.82%的偏见率降低，显著提升了输出的公正性和用户信任度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Bias_Mitigation_Agent_Optimizing_Source_Selection_for_Fair_and_Balanced_Knowledge_Retrieval.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18684v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      95. FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/FALCON__Autonomous_Cyber_Threat_Intelligence_Mining_with_LLMs_for_IDS_Rule_Generation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Alabama</span></p>
<p>该论文提出FALCON框架，利用大型语言模型（LLM）驱动的智能体系统，自动从网络威胁情报（CTI）生成可部署的入侵检测系统（IDS）规则，并通过内置多阶段验证（包括语法、语义和性能）确保规则质量。实验表明，FALCON在Snort和YARA平台上的规则生成准确率高达95%，与人工分析结果高度一致，证明LLM驱动的数据挖掘可实现高效、可解释的自动化网络威胁防御。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/FALCON_Autonomous_Cyber_Threat_Intelligence_Mining_with_LLMs_for_IDS_Rule_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18673v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      96. Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Tailored_Teaching_with_Balanced_Difficulty__Elevating_Reasoning_in_Multimodal_Chain-of-Thought_via_P.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Aeronautics and Astronautics</span></p>
<p>本文提出了一种名为CAMS（Complexity-Guided Active Multimodal CoT Sampling）的新型多模态Chain-of-Thought（MCoT）提示样例选择框架。方法结合主动学习的不确定性量化与样本复杂度评估，动态平衡选择易与难的样本以构建定制化提示课程，有效提升多模态大模型在复杂推理任务中的稳定性和准确率。实验在五个基准数据集和多种主流多模态大模型上验证了CAMS的一致性提升作用，显著减少了随机选样带来的性能波动，并优于现有多种基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Tailored_Teaching_with_Balanced_Difficulty_Elevating_Reasoning_in_Multimodal_Chain-of-Thought_via_Prompt_Curriculum.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18665v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      97. Membership Inference Attacks on LLM-based Recommender Systems
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Membership_Inference_Attacks_on_LLM-based_Recommender_Systems.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland, Baltimore County</span></p>
<p>本文提出并系统评估了针对基于大语言模型（LLM）的推荐系统的四种成员推断攻击方法：直接询问、幻觉、相似性和投毒攻击，重点针对利用In-Context Learning定制推荐的场景。实验表明直接询问和投毒攻击在多个主流LLM和公开推荐数据集上能有效推断用户成员身份，显示出现实的隐私风险。结论：LLM推荐系统面临显著的成员推断威胁，需设计针对性防护措施。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Membership_Inference_Attacks_on_LLM-based_Recommender_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18663v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      98. FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/FFT-MoE__Efficient_Federated_Fine-Tuning_for_Foundation_Models_via_Large-scale_Sparse_MoE_under_Hete.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing University of Posts and Telecommunications</span></p>
<p>本文提出FFT-MoE框架，通过在联邦学习环境下用大规模稀疏Mixture-of-Experts（MoE）适配器替代传统LoRA模块，实现基础模型在异构边缘设备上的高效个性化微调。方法包括每个客户端训练轻量化门控网络以动态激活部分专家，结合异构感知辅助损失实现专家负载均衡，提升了在非IID数据和设备异构下的泛化能力和收敛速度。实验结果表明，FFT-MoE在多种数据分布和基线方法下均取得更优的准确率和训练效率，验证了其在高异质性场景下的稳健性和高效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/FFT-MoE_Efficient_Federated_Fine-Tuning_for_Foundation_Models_via_Large-scale_Sparse_MoE_under_Heterogeneous_Edge.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18756v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      99. UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/UltraMemV2__Memory_Networks_Scaling_to_120B_Parameters_with_Superior_Long-Context_Learning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ByteDance Seed</span></p>
<p>UltraMemV2提出了一种全新的内存层架构，通过在每个Transformer块中集成UltraMemV2层，引入简化的隐式值扩展、PEER的FFN值处理、优化初始化和计算比例重平衡等关键技术，实现稀疏激活下高效扩展参数。实验表明UltraMemV2在同等计算和参数下达到8专家MoE模型性能，且在长上下文记忆、多轮对话和上下文学习任务上明显优于MoE，激活密度比稀疏参数总量更关键。结论是UltraMemV2为高效稀疏计算提供了新范式，尤其适合大规模长上下文任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/UltraMemV2_Memory_Networks_Scaling_to_120B_Parameters_with_Superior_Long-Context_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18743v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      100. CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/CAC-CoT__Connector-Aware_Compact_Chain-of-Thought_for_Efficient_Reasoning_Data_Synthesis_Across_Dual.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">DATUMO</span></p>
<p>该论文提出了CAC-CoT（Connector-Aware Compact Chain-of-Thought）方法，通过在推理数据中显式注入“连接词”并控制推理链长度，引导大语言模型在复杂任务中产生简洁且结构化的推理链。实验结果表明，CAC-CoT在GSM8K和GPQA等System-2（分析型）任务上表现良好，同时在S1-Bench等System-1（直觉型）任务上显著提升推理效率，平均推理长度仅为基线的三分之一，兼顾准确率与计算经济性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/CAC-CoT_Connector-Aware_Compact_Chain-of-Thought_for_Efficient_Reasoning_Data_Synthesis_Across_Dual-System_Cognitive_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18739v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      101. Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Beyond_Quality__Unlocking_Diversity_in_Ad_Headline_Generation_with_Large_Language_Models.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiaohongshu Inc.</span></p>
<p>本文提出DIVER框架，通过语义和风格感知的数据生成流程，结合大语言模型的多阶段多目标优化（包括冷启动监督微调和多目标强化学习），实现广告标题生成的高质量与多样性平衡。实验和线上部署结果显示，DIVER在广告系统中显著提升了标题多样性与用户点击率，兼顾质量和业务效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Beyond_Quality_Unlocking_Diversity_in_Ad_Headline_Generation_with_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18736v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      102. Rethinking Caching for LLM Serving Systems: Beyond Traditional Heuristics
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Rethinking_Caching_for_LLM_Serving_Systems__Beyond_Traditional_Heuristics.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">DGIST</span></p>
<p>本文提出SISO，一种针对大语言模型(LLM)推理服务的高效语义缓存系统。SISO通过三大创新：以中心点（centroid）为单位的语义缓存，基于长期语义局部性的缓存替换策略，以及动态相似度阈值调整，实现了更高效的缓存空间利用和系统响应。实验表明，SISO相比主流系统GPTCache和vLLM，平均命中率提升1.71倍，在多种数据集和负载下更好地满足服务SLO，且仅以小幅度精度损失换取显著性能提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Rethinking_Caching_for_LLM_Serving_Systems_Beyond_Traditional_Heuristics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18722v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      103. VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/VistaWise__Building_Cost-Effective_Agent_with_Cross-Modal_Knowledge_Graph_for_Minecraft.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>该论文提出VistaWise，一种面向Minecraft虚拟环境的成本高效体智能体框架，通过微调小规模的目标检测模型和检索外部文本知识，将视觉与文本信息融合进跨模态知识图谱，并采用基于检索的图池化策略高效提取任务相关信息，配合桌面级技能库实现纯视觉输入下的复杂任务决策与操作。实验表明，VistaWise在极大降低开发成本的同时，在多项任务上取得了SOTA性能，显著优于同类非API方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/VistaWise_Building_Cost-Effective_Agent_with_Cross-Modal_Knowledge_Graph_for_Minecraft.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18689v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      104. AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/AppAgent-Pro__A_Proactive_GUI_Agent_System_for_Multidomain_Information_Integration_and_User_Assistan.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本文提出了一种主动型多模态GUI智能体系统AppAgent-Pro，能够基于用户指令主动推测潜在需求，并通过深度执行模式在多个移动应用中整合和挖掘信息。结果表明，该系统显著提升了信息获取的效率、个性化和深度，为智能、上下文感知的人机交互奠定了基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/AppAgent-Pro_A_Proactive_GUI_Agent_System_for_Multidomain_Information_Integration_and_User_Assistance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18676v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      105. Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Utilizing_Training_Data_to_Improve_LLM_Reasoning_for_Tabular_Understanding.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Urbana-Champaign</span></p>
<p>本文提出了一种名为LRTab的新型基于prompt的推理方法，用于提升大语言模型在表格理解任务中的推理能力。LRTab通过在训练阶段利用标注数据生成并纠正Chain-of-Thought（CoT）推理错误，提取可解释的Prompt Conditions，并在推理阶段通过检索相关Prompt Conditions为模型提供额外上下文。实验表明，LRTab在WikiTQ和TabFact数据集上取得了新的SOTA，兼具可解释性和高效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Utilizing_Training_Data_to_Improve_LLM_Reasoning_for_Tabular_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18672v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      106. Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Optimal_Sparsity_of_Mixture-of-Experts_Language_Models_for_Reasoning_Tasks.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Science Tokyo</span></p>
<p>本论文系统研究了Mixture-of-Experts（MoE）稀疏性对大语言模型在记忆和推理任务上的影响。通过在固定算力下对专家数、激活参数和top-k路由进行多维度控制实验，发现增加总参数数目能提升记忆任务表现但对推理任务有饱和甚至退化效应，只调整top-k路由在激活参数固定时作用有限，经典超参数如学习率和初始化也影响泛化差距。无论采用后训练强化学习（GRPO）或增强推理时算力，都无法消除过度稀疏模型在推理任务上的性能损失。结论表明，MoE模型在记忆任务中稀疏度越高越好，但在推理任务中需适当提升激活参数甚至向密集配置倾斜，且这一权衡无法通过后期优化弥补，强调了在预训练阶段合理分配稀疏参数对推理能力的重要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Optimal_Sparsity_of_Mixture-of-Experts_Language_Models_for_Reasoning_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18669v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      107. MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/MUA-RL__Multi-turn_User-interacting_Agent_Reinforcement_Learning_for_agentic_tool_use.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Meituan Inc.</span></p>
<p>该论文提出了MUA-RL，一种用于多轮用户交互式智能体工具使用的强化学习框架，首次在训练过程中融合了由LLM模拟的动态用户角色，并采用Group Relative Policy Optimization（GRPO）算法实现高效策略优化。通过轻量冷启动与多轮实时交互增强，MUA-RL显著提升了智能体在动态任务中的工具调用和沟通能力，并在多个主流多轮工具使用基准上实现了优于或媲美大规模开源模型的表现，表现出极强的泛化能力和稳定性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/MUA-RL_Multi-turn_User-interacting_Agent_Reinforcement_Learning_for_agentic_tool_use.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18653v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      108. The Sound of Risk: A Multimodal Physics-Informed Acoustic Model for Forecasting Market Volatility and Enhancing Market Interpretability
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/The_Sound_of_Risk__A_Multimodal_Physics-Informed_Acoustic_Model_for_Forecasting_Market_Volatility_an.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">SoundAI Technology, Beijing, China</span></p>
<p>该论文提出了PIAM物理引导声学模型，通过融合深度学习声学特征（从原始电话会议音频中提取高维动态情绪）与大语言模型（LLM）文本情感分析，将多模态情感映射到三维ASL空间，并用于预测市场波动率。实验表明，该多模态方法在预测未来30天市场波动率时解释了43.8%的方差，显著优于仅用财务数据的基线模型，但对股票收益没有预测能力，情绪信号主要反映市场不确定性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/The_Sound_of_Risk_A_Multimodal_Physics-Informed_Acoustic_Model_for_Forecasting_Market_Volatility_and_Enhancing_Market_Interpretability.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18651v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      109. Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Breaking_the_Trade-Off_Between_Faithfulness_and_Expressiveness_for_Large_Language_Models.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>本文提出了一种新型解码方法Collaborative Decoding (CoDe)，通过自适应双流融合和知识感知重排序模块，动态整合大模型的参数知识和外部知识，从而打破LLM在知识增强场景下的忠实性与表达性矛盾。实验证明，CoDe在多个LLM和多项基准上显著提升了响应的忠实性和表达性，具有较强的通用性和有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Breaking_the_Trade-Off_Between_Faithfulness_and_Expressiveness_for_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18646v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      110. Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Beyond_Benchmark__LLMs_Evaluation_with_an_Anthropomorphic_and_Value-oriented_Roadmap.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">China Mobile Communications Group Co.,Ltd.</span></p>
<p>本论文提出了一种全新的人本主义大模型（LLM）评价框架，将评价维度拓展为智商（IQ）、情商（EQ）、职业能力（PQ）和价值导向（VQ），涵盖基础智能、对齐能力、专业知识与社会经济伦理环境影响。通过分析200余种基准，作者构建了六大模块的评价体系，并指出当前评测的碎片化及与实际应用脱节问题，最终为LLM开发和部署提供了面向未来的全生命周期评价路线图。结论认为该框架能有效弥补传统技术指标的不足，有助于指导负责任的AI应用落地。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Beyond_Benchmark_LLMs_Evaluation_with_an_Anthropomorphic_and_Value-oriented_Roadmap.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18587v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      111. A Case Study on the Effectiveness of LLMs in Verification with Proof Assistants
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/A_Case_Study_on_the_Effectiveness_of_LLMs_in_Verification_with_Proof_Assistants.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Toronto</span></p>
<p>该论文研究了大语言模型（LLMs）在程序验证中自动生成证明的能力，方法是在两个真实的Rocq（Coq）项目（hs-to-coq和Verdi）中，通过提供不同依赖和上下文信息，分析多种主流LLMs生成证明的有效性、证明质量和项目间差异。结果显示，充分的上下文和外部依赖显著提升LLM生成证明的成功率，LLMs在小规模证明上表现优异，但在大型证明中仍有一定成功概率，同时能生成简洁且创新的证明，但偶尔会产生无意义的“卡壳”式证明。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/A_Case_Study_on_the_Effectiveness_of_LLMs_in_Verification_with_Proof_Assistants.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18642v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      112. RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/RLMR__Reinforcement_Learning_with_Mixed_Rewards_for_Creative_Writing.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent Hunyuan Team</span></p>
<p>该论文提出了一种用于大语言模型创意写作优化的动态混合奖励强化学习框架（RLMR），结合了主观写作质量奖励模型和客观约束验证模型，通过动态调整约束奖励权重，确保违反约束的样本在GRPO中被惩罚。实验结果显示，RLMR在写作质量和指令遵循方面均优于单一奖励及固定权重混合方法，显著提升了模型在多维创意写作任务中的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/RLMR_Reinforcement_Learning_with_Mixed_Rewards_for_Creative_Writing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18635v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      113. STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/STRATA-TS__Selective_Knowledge_Transfer_for_Urban_Time_Series_Forecasting_with_Retrieval-Guided_Reas.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>STRATA-TS提出了结合目标感知检索与LLM推理的城市时序预测框架，通过Patch-based Temporal Encoder检索与目标语义和动态一致的源序列，并在Retrieval-Guided Reasoning模块中利用大语言模型进行结构化因果推理，最后蒸馏至轻量开源模型以提升推理效率。实验证明，在新加坡、诺丁汉和格拉斯哥停车数据上，STRATA-TS显著优于当前主流预测和迁移方法，并提供可解释的知识迁移路径。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/STRATA-TS_Selective_Knowledge_Transfer_for_Urban_Time_Series_Forecasting_with_Retrieval-Guided_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18609v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      114. Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Scaling_Laws_for_Task-Stratified_Knowledge_in_Post-Training_Quantized_Large_Language_Models.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences</span></p>
<p>本文提出并实证验证了任务分层的后训练量化（PTQ）大语言模型（LLM）知识能力缩放定律，将知识能力细分为记忆和利用两类，并构建统一的量化框架，将模型规模、有效比特宽度、校准集大小和分组大小纳入多重幂律建模。核心结论是：知识记忆能力对这些PTQ参数变化更为敏感，而知识利用能力表现出更强鲁棒性，为发展知识感知量化策略提供了理论依据。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Scaling_Laws_for_Task-Stratified_Knowledge_in_Post-Training_Quantized_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18598v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      115. WHAT DO LANGUAGE MODELS MODEL? TRANSFORMERS, AUTOMATA, AND THE FORMAT OF THOUGHT.
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/What_do_language_models_model__Transformers,_automata,_and_the_format_of_thought.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Australian National University</span></p>
<p>本文通过对Transformer架构的分析，论证大语言模型（LLM）并非模拟了人类的语言能力，而是高效地建模了其训练语料库。作者指出，认知科学认为人类语言处理依赖超线性计算格式，而Transformers至多支持线性格式，不能实现人类语法处理的复杂结构。通过引入自动机理论，尤其是Liu等人提出的shortcut automata观点，作者认为LLMs更像是在高效模拟生成语料的有限状态自动机，通过对序列的线性转换与状态转移完成文本预测任务。结论认为，LLMs对语料自身的变换能力反映了语料的强大生成规则，而非对人类思维机制的直接建模，强调了LLMs与人类语言生产机制的本质差异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/WHAT_DO_LANGUAGE_MODELS_MODEL_TRANSFORMERS_AUTOMATA_AND_THE_FORMAT_OF_THOUGHT..jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18588v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      116. History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/History_Rhymes__Accelerating_LLM_Reinforcement_Learning_with_RhymeRL.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University</span></p>
<p>该论文提出RhymeRL系统，通过利用历史rollout相似性，在大模型强化学习训练中加速RL流程。方法包括HistoSpec历史驱动的推测解码和HistoPipe分布感知调度，有效减少GPU闲置与rollout瓶颈。实验表明，在不影响准确性和RL范式的前提下，RhymeRL在多种大模型和真实生产环境下实现2.6倍训练性能提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/History_Rhymes_Accelerating_LLM_Reinforcement_Learning_with_RhymeRL.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18579v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      117. DrugReasoner: Interpretable Drug Approval Prediction with a Reasoning-augmented Language Model
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/DrugReasoner__Interpretable_Drug_Approval_Prediction_with_a_Reasoning-augmented_Language_Model.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Regenerative Medicine Research Center, Isfahan University of Medical Sciences</span></p>
<p>该论文提出了DrugReasoner，一种基于LLaMA架构、采用群体相对策略优化（GRPO）强化学习微调的大语言模型，用于通过分子特征和与结构相似药物的比较推理预测小分子药物的获批概率。DrugReasoner能生成带有逐步推理和置信度分数的解释性预测，性能优于传统机器学习和ChemAP模型，提升了预测准确性和可解释性，适用于药物研发早期决策。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/DrugReasoner_Interpretable_Drug_Approval_Prediction_with_a_Reasoning-augmented_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19151v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      118. Saddle Hierarchy in Dense Associative Memory
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Saddle_Hierarchy_in_Dense_Associative_Memory.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Scuola Normale Superiore di Pisa</span></p>
<p>本论文提出了基于三层Boltzmann机和Potts隐藏单元的稠密联想记忆（DAM）模型，并通过统计力学分析推导了刻画DAM稳定点的鞍点方程，揭示了DAM权重在不同网络规模下的鞍点层次结构。作者提出了一种新型正则化方法显著提升了DAM训练的稳定性，并通过理论及实证分析展示了模型在监督及非监督分类任务中的可解释性和高效性。结论表明，利用鞍点层次和网络增长算法，DAM训练效率大幅提升，且模型具备良好的可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Saddle_Hierarchy_in_Dense_Associative_Memory.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19016v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      119. Working My Way Back to You: Resource-Centric Next-Activity Prediction
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Working_My_Way_Back_to_You__Resource-Centric_Next-Activity_Prediction.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Utrecht University</span></p>
<p>本文提出了一种面向资源的下一活动预测方法，重点研究不同编码策略（如2-gram活动转移和活动重复特征）和预测模型（Random Forest、LightGBM、LSTM、Transformer）在实际业务流程数据集上的效果。实验表明，基于资源的2-gram转移与活动重复特征编码，结合Random Forest模型，能显著提升预测准确率，优于传统的基于流程实例的预测基线。这种方法有助于改善资源分配和流程管理，展示了资源视角预测的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Working_My_Way_Back_to_You_Resource-Centric_Next-Activity_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19015v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      120. Learning with springs and sticks
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Learning_with_springs_and_sticks.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Department of Computer Science. University of Toronto</span></p>
<p>本文提出了一种基于弹簧和杆的物理系统（SS模型），用于以物理过程模拟机器学习回归任务。该模型通过弹簧势能编码均方误差损失，并利用耗散动力学收敛到最小能量状态，实现任意连续函数的分段线性逼近，性能与单层MLP相当。结论指出，系统学习能力存在最小能量需求（热力学学习障碍），该障碍与模型表达能力、环境温度和摩擦系数成正比。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Learning_with_springs_and_sticks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19011v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      121. STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/STDiff__A_State_Transition_Diffusion_Framework_for_Time_Series_Imputation_in_Industrial_Systems.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Aalborg University</span></p>
<p>该论文提出STDiff框架，将工业系统时序数据的缺失值填补任务重新定义为条件状态转移建模，并采用条件去噪扩散模型递归生成缺失数据。方法通过结合控制和外部输入，实现对系统状态的动态一致性填补，经在污水处理厂数据集验证，对比多种基线模型，STDiff在长间隙和高动态场景下表现最优。结论是：显式建模系统动态转移比传统窗口方法更准确且更具物理合理性，适用于工业时序数据场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/STDiff_A_State_Transition_Diffusion_Framework_for_Time_Series_Imputation_in_Industrial_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.19006v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      122. Is attention truly all we need? An empirical study of asset pricing in pretrained RNN sparse and global attention models
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Is_attention_truly_all_we_need__An_empirical_study_of_asset_pricing_in_pretrained_RNN_sparse_and_glo.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of York</span></p>
<p>本论文提出了两种创新的预训练RNN注意力模型：RNN全局自注意力模型（self att）和RNN滑动窗口稀疏注意力模型（sparse att），并首次系统性地将包括主流注意力机制（加性注意力、Luong三种注意力、全局自注意力、滑动窗口稀疏注意力）应用于资产定价任务。通过MLP自编码器进行特征降维和缺失值处理，结合RNN结构建模时间依赖性，并在三种极端市场环境下实证检验。结论显示，self att和sparse att模型均在收益和风险对冲上表现优异，特别是在新冠期间年化Sortino比率分别达到2.0和1.80，且sparse att在不同市值股票上表现更稳定。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Is_attention_truly_all_we_need_An_empirical_study_of_asset_pricing_in_pretrained_RNN_sparse_and_global_attention_models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18982v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      123. PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/PAX-TS__Model-agnostic_multi-granular_explanations_for_time_series_forecasting_via_localized_perturb.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stockholm University</span></p>
<p>该论文提出了一种面向时间序列预测的、模型无关的可解释性方法PAX-TS，通过对输入数据进行局部扰动，生成多粒度、可用于单变量和多变量时间序列的解释，包含时间步、统计量和跨通道相关性等维度。实验表明，PAX-TS能有效揭示预测模型的行为模式，在解释粒度和适用性上优于ShapTime和TS-MULE，并可用于实际问题分析。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/PAX-TS_Model-agnostic_multi-granular_explanations_for_time_series_forecasting_via_localized_perturbations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18953v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      124. Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Novel_Approaches_to_Artificial_Intelligence_Development_Based_on_the_Nearest_Neighbor_Method.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Oil and Gas «Gubkin University»</span></p>
<p>该论文提出一种结合k近邻算法与基于Kohonen自组织映射的层次聚类结构的人工智能新方法，旨在克服神经网络如幻觉效应、训练与微调高开销、灾难性遗忘等局限。通过在图像识别和文本翻译任务中的实验，证实了该方法能够大幅提升检索效率、保持高可解释性，并在可靠性要求高的场景下有效减少幻觉现象，仅以较小精度损失换取极大推理加速。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Novel_Approaches_to_Artificial_Intelligence_Development_Based_on_the_Nearest_Neighbor_Method.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18933v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      125. VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/VISION__Robust_and_Interpretable_Code_Vulnerability_Detection_Leveraging_Counterfactual_Augmentation.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland College Park</span></p>
<p>该论文提出了VISION框架，通过利用大语言模型(LLM)生成的反事实数据对原始源代码进行最小语义修改，实现标签翻转，从而平衡数据集并减少图神经网络(GNN)在代码漏洞检测中的伪相关学习。框架包括反事实生成、GNN模型对比训练、基于图的可解释性分析与可视化模块，显著提升了在CWE-20数据集上的准确率、对比精度与最弱组鲁棒性，并通过解释性指标证明模型决策更依赖真实漏洞信号。结论表明，反事实增强能有效提升代码漏洞检测的泛化能力和可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/VISION_Robust_and_Interpretable_Code_Vulnerability_Detection_Leveraging_Counterfactual_Augmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18922v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      126. HierCVAE: Hierarchical Attention-Driven Conditional Variational Autoencoders for Multi-Scale Temporal Modeling
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/HierCVAE__Hierarchical_Attention-Driven_Conditional_Variational_Autoencoders_for_Multi-Scale_Tempora.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Westlake University</span></p>
<p>该论文提出HierCVAE，一种结合了三层次分层注意力机制（局部、全局、跨时序）和多模态条件编码（时序、统计、趋势特征）的条件变分自编码器，用于捕捉多尺度时序依赖并量化预测不确定性。实验表明，HierCVAE在能源消耗等多时序建模任务中，预测准确率比最先进方法提升15-40%，并提供了良好的不确定性校准，尤其适用于长期预测和多变量复杂依赖场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/HierCVAE_Hierarchical_Attention-Driven_Conditional_Variational_Autoencoders_for_Multi-Scale_Temporal_Modeling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18954v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      127. On the Generalisation of Koopman Representations for Chaotic System Control
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/On_the_Generalisation_of_Koopman_Representations_for_Chaotic_System_Control.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Groningen</span></p>
<p>该论文提出了一种三阶段方法：首先通过Koopman自编码器学习混沌动力系统的线性化嵌入，然后利用Transformer在嵌入空间进行下一步状态预测预训练，最后微调用于安全关键控制任务。结果证明Koopman嵌入在下游安全函数预测任务上优于物理先验和标准PCA基线，且即使Transformer权重冻结也无性能损失，说明其表征具有强泛化性和可迁移性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/On_the_Generalisation_of_Koopman_Representations_for_Chaotic_System_Control.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18921v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      128. Forecasting Probability Distributions of Financial Returns with Deep Neural Networks
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Forecasting_Probability_Distributions_of_Financial_Returns_with_Deep_Neural_Networks.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">TripleSun</span></p>
<p>该论文提出使用1D卷积神经网络（CNN）和长短期记忆网络（LSTM）预测金融收益的概率分布参数，包括正态、学生t及偏斜学生t分布，采用自定义负对数似然损失函数直接优化分布参数。实证结果表明，LSTM结合偏斜学生t分布在六大股票指数上的分布预测和风险管理（VaR与ES）表现优于经典GARCH模型，准确捕捉收益分布的厚尾与偏斜特征。结论认为深度神经网络为金融风险评估和组合管理提供了有效新工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Forecasting_Probability_Distributions_of_Financial_Returns_with_Deep_Neural_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18907v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      129. SegReConcat: A Data Augmentation Method for Voice Anonymization Attack
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/SegReConcat__A_Data_Augmentation_Method_for_Voice_Anonymization_Attack.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Singapore Institute of Technology</span></p>
<p>本文提出了一种名为SegReConcat的数据增强方法，用于攻击端提升自动说话人验证（ASV）系统对匿名语音的去匿名能力。该方法将匿名语音在词级别切分后，采用随机或相似性驱动的重组策略打乱原始语序，再与原音拼接作为训练数据，迫使模型关注短时说话人特征。实验表明，SegReConcat在VoicePrivacy Attacker Challenge 2024框架下对七种匿名系统中的五种显著提升去匿名攻击性能，最高可降低平均等错误率（EER）达11%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/SegReConcat_A_Data_Augmentation_Method_for_Voice_Anonymization_Attack.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18891v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      130. pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/pyFAST__A_Modular_PyTorch_Framework_for_Time_Series_Modeling_with_Multi-source_and_Sparse_Data.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">College of Computer Engineering, Jimei University</span></p>
<p>该论文提出了pyFAST，一个基于PyTorch的模块化时间序列建模框架，显著地将数据处理与模型计算解耦，支持稀疏和多源数据的对齐无关融合。方法上集成了LLM启发式架构、动态填充、归一化和灵活掩码建模，包含多种深度学习与统计模型组件，适用于复杂真实世界场景。结论表明，pyFAST为时间序列研究和应用提供了高效、可扩展和灵活的平台。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/pyFAST_A_Modular_PyTorch_Framework_for_Time_Series_Modeling_with_Multi-source_and_Sparse_Data.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18803v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      131. A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT Networks
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/A_Survey_on_Cloud-Edge-Terminal_Collaborative_Intelligence_in_AIoT_Networks.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, China University of Mining and Technology (Beijing)</span></p>
<p>本文系统综述了云-边-端协同智能（CETCI）在AIoT网络中的最新进展，提出了涵盖架构范式、使能技术、资源管理、协作学习、安全与隐私等多维度的统一框架。通过对任务卸载、资源分配、联邦学习、分布式深度学习、模型演进和强化学习优化等核心技术的归纳分析，全面揭示了CETCI提升AIoT系统实时性、效率和安全性的机制与挑战。结论指出，CETCI可有效优化分布式AIoT系统性能，但在可扩展性、异构性和安全隐私等方面仍面临重大挑战，未来应关注6G、数字孪生、大模型与量子计算等新兴方向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/A_Survey_on_Cloud-Edge-Terminal_Collaborative_Intelligence_in_AIoT_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18785v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      132. EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/EMind__A_Foundation_Model_for_Multi-task_Electromagnetic_Signals_Understanding.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Microelectronics of the Chinese Academy of Sciences</span></p>
<p>本论文提出了EMind，一种针对电磁信号的多任务基础模型，采用了Transformer编码器-解码器结构和自监督Masked Auto-Encoding（MAE）大规模预训练，结合长度自适应多信号打包和硬件感知训练策略，能够高效处理多源异构电磁信号。实验结果表明，EMind在自动调制分类、雷达参数估计、无线干扰识别、信号去噪和盲源分离等多个下游任务上实现了广泛泛化和SOTA性能，推动电磁智能从专用模型向统一通用模型迈进。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/EMind_A_Foundation_Model_for_Multi-task_Electromagnetic_Signals_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18742v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      133. Constraint Matters: Multi-Modal Representation for Reducing Mixed-Integer Linear programming
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Constraint_Matters__Multi-Modal_Representation_for_Reducing_Mixed-Integer_Linear_programming.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southeast University</span></p>
<p>本论文提出了一种基于约束的混合整数线性规划（MILP）模型压缩方法，核心流程包括：通过多模态表示结合实例级和抽象层次MILP信息，利用图神经网络及预训练语言模型，识别并固定一部分关键紧约束，从而有效减少可行域。实验证明，该方法在多个大规模MILP基准和真实场景下，相比SOTA方法，提升了解空间质量超过50%，计算时间降低约17%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Constraint_Matters_Multi-Modal_Representation_for_Reducing_Mixed-Integer_Linear_programming.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18737v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      134. FLAegis: A Two-Layer Defense Framework for Federated Learning Against Poisoning Attacks
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/FLAegis__A_Two-Layer_Defense_Framework_for_Federated_Learning_Against_Poisoning_Attacks.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Murcia</span></p>
<p>本文提出了一种名为FLAegis的联邦学习防御框架，通过两阶段流程提升对中毒攻击的鲁棒性。该方法首先利用SAX符号化处理和谱聚类检测并隔离恶意客户端，然后采用FFT频域鲁棒聚合函数进一步减少未检测到的恶意客户端对全局模型的影响。实验结果表明，FLAegis在多种中毒攻击下均优于当前主流防御方法，在恶意客户端比例较高时依旧能保持较高准确率和检测精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/FLAegis_A_Two-Layer_Defense_Framework_for_Federated_Learning_Against_Poisoning_Attacks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18671v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      135. Auditing Approximate Machine Unlearning for Differentially Private Models
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Auditing_Approximate_Machine_Unlearning_for_Differentially_Private_Models.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland, Baltimore County</span></p>
<p>本文提出了一种用于审计近似机器反遗忘算法隐私风险的体系，特别关注于差分隐私模型下未被删除和保留样本的隐私保护效果。方法上，作者提出了针对未学习和保留样本的两项隐私审计标准，并设计了高效的数据增强型似然比攻击（A-LiRA），显著降低了现有攻击的时间成本。实验结果表明，现有的近似机器反遗忘方法可能无法充分保护未学习样本的隐私，甚至会增加保留样本的隐私风险，需发展更强的差分隐私反遗忘算法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Auditing_Approximate_Machine_Unlearning_for_Differentially_Private_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18730v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      136. Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Beyond_Tokens__Enhancing_RTL_Quality_Estimation_via_Structural_Graph_Learning.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong</span></p>
<p>本论文提出了StructRTL结构感知图自监督学习框架，通过在控制数据流图（CDFG）上进行结构感知掩码节点建模和边预测两项预训练任务，显著提升了RTL设计质量（面积与延迟）估算的准确性。结合了知识蒸馏策略，将后映射网表的低层次知识迁移到CDFG预测器，StructRTL在各项指标上超过了现有基线方法，实现了新的最优性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Beyond_Tokens_Enhancing_RTL_Quality_Estimation_via_Structural_Graph_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18638v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      137. Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance
</a> 
<a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-27/arxiv/Biologically_Disentangled_Multi-Omic_Modeling_Reveals_Mechanistic_Insights_into_Pan-Cancer_Immunothe.pdf">
    [PDF]
</a> 
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Massachusetts Institute of Technology</span></p>
<p>该论文提出了Biologically Disentangled Variational Autoencoder (BDVAE)模型，通过多模态编码器结合变分推断，整合转录组和基因组多组学数据，实现了对免疫、基因组和代谢过程的可解释潜空间解耦。实验表明，BDVAE可准确预测免疫检查点阻断（ICI）疗效（AUC-ROC&#x3D;0.94），并揭示免疫抑制、代谢转变和神经信号等关键耐药机制，潜空间特征与生存结局及临床亚型相关，支持精准肿瘤免疫治疗策略。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-28/Biologically_Disentangled_Multi-Omic_Modeling_Reveals_Mechanistic_Insights_into_Pan-Cancer_Immunotherapy_Resistance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/28报纸</div>
      <div>http://ywfhhh.github.io/2025/08/2025-08-28_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>具身ADT</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月28日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-27_article/" title="2025/08/27报纸">
                        <span class="hidden-mobile">2025/08/27报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
