

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="具身ADT">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;25报纸">
<meta property="og:url" content="http://ywfhhh.github.io/2025/08/2025-08-25_article/index.html">
<meta property="og:site_name" content="最新前沿论文分享">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Distributed_Detection_of_Adversarial_Attacks_in_Multi-Agent_Reinforcement_Learning_with_Continuous_Action_Space.jpg">
<meta property="article:published_time" content="2025-08-25T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-22T20:05:51.238Z">
<meta property="article:author" content="具身ADT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Distributed_Detection_of_Adversarial_Attacks_in_Multi-Agent_Reinforcement_Learning_with_Continuous_Action_Space.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/25报纸 - 最新前沿论文分享</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ywfhhh.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<!-- hexo injector head_end start -->
<script src="/js/custom.js"></script>
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文库</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/25报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-25 00:00" pubdate>
          星期一, 八月 25日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          16k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/25报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/25报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-25<br>📄 <strong>发现论文数量</strong>：107  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Distributed-Detection-of-Adversarial-Attacks-in-Multi-Agent-Reinforcement-Learning-with-Continuous-Action-Space"><a href="#1-Distributed-Detection-of-Adversarial-Attacks-in-Multi-Agent-Reinforcement-Learning-with-Continuous-Action-Space" class="headerlink" title="1. Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Distributed_Detection_of_Adversarial_Attacks_in_Multi-Agent_Reinforcement_Learning_with_Continuous_A.pdf">Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KTH Royal Institute of Technology</span></p>
<p>本文提出了一种用于协作型多智能体强化学习（c-MARL）连续动作空间系统中检测对抗性攻击的分布式方法。该方法通过神经网络将他智能体的行为建模为参数化高斯分布，计算归一化对数似然得分，并利用CUSUM检测均值偏移，实现实时异常检测。实验结果表明，该方法在多种基准环境和多种攻击下均能高效检测受害智能体，且优于离散动作空间下的对比方法。结论是，该方法在检测高影响攻击时表现优越，仅极低影响攻击可能逃逸，并且计算复杂度低，适用于多受害智能体场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Distributed_Detection_of_Adversarial_Attacks_in_Multi-Agent_Reinforcement_Learning_with_Continuous_Action_Space.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-Understanding-Action-Effects-through-Instrumental-Empowerment-in-Multi-Agent-Reinforcement-Learning"><a href="#2-Understanding-Action-Effects-through-Instrumental-Empowerment-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="2. Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Understanding_Action_Effects_through_Instrumental_Empowerment_in_Multi-Agent_Reinforcement_Learning.pdf">Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Istituto Dalle Molle di Studi sull’Intelligenza Artificiale (IDSIA)</span></p>
<p>本文提出了一种新的因果动作归因方法Intended Cooperation Values (ICVs)，基于信息论Shapley值，仅利用策略分布分析，量化个体动作对多智能体系统中他人决策确定性和偏好一致性的影响。实验表明，ICV无需奖励信号即可有效解释多智能体合作与影响机制，提升了MARL系统的可解释性和团队行为分析能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Understanding_Action_Effects_through_Instrumental_Empowerment_in_Multi-Agent_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-NiceWebRL-a-Python-library-for-human-subject-experiments-with-reinforcement-learning-environments"><a href="#3-NiceWebRL-a-Python-library-for-human-subject-experiments-with-reinforcement-learning-environments" class="headerlink" title="3. NiceWebRL: a Python library for human subject experiments with reinforcement learning environments"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/NiceWebRL__a_Python_library_for_human_subject_experiments_with_reinforcement_learning_environments.pdf">NiceWebRL: a Python library for human subject experiments with reinforcement learning environments</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University</span></p>
<p>NiceWebRL 是一个 Python 库，利用 Jax 和 NiceGUI，让研究者能够将任意 Jax 环境转化为支持单&#x2F;多智能体的在线人类实验接口，极大简化了人类与 AI 算法的对比和协作实验流程。通过三个案例研究，论文展示了该工具在开发类人AI、人类兼容AI和人类辅助AI中的实际应用，验证了其在认知建模、多智能体协作、LLM 辅助决策等方向的实用性和通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/NiceWebRL_a_Python_library_for_human_subject_experiments_with_reinforcement_learning_environments.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-A-Dynamical-Systems-Framework-for-Reinforcement-Learning-Safety-and-Robustness-Verification"><a href="#4-A-Dynamical-Systems-Framework-for-Reinforcement-Learning-Safety-and-Robustness-Verification" class="headerlink" title="4. A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/A_Dynamical_Systems_Framework_for_Reinforcement_Learning_Safety_and_Robustness_Verification.pdf">A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">City University of London</span></p>
<p>本文提出了一种基于动力系统理论的新框架，将强化学习（RL）智能体及其环境视为离散时间自治动力系统，利用有限时间Lyapunov指数（FTLE）识别和可视化Lagrangian Coherent Structures（LCS），从而洞察策略的安全性和鲁棒性。通过引入平均边界排斥（MBR）、聚合伪吸引子强度（ASAS）和时序感知伪吸引子强度（TASAS）等量化指标，该方法能对策略安全裕度和鲁棒性进行定量评估，并在多种离散和连续控制环境中验证了其实用性，能有效发现基于奖励难以暴露的策略缺陷。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/A_Dynamical_Systems_Framework_for_Reinforcement_Learning_Safety_and_Robustness_Verification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-Adversarial-Agent-Behavior-Learning-in-Autonomous-Driving-Using-Deep-Reinforcement-Learning"><a href="#5-Adversarial-Agent-Behavior-Learning-in-Autonomous-Driving-Using-Deep-Reinforcement-Learning" class="headerlink" title="5. Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Adversarial_Agent_Behavior_Learning_in_Autonomous_Driving_Using_Deep_Reinforcement_Learning.pdf">Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland, College Park</span></p>
<p>该论文提出了一种基于深度强化学习的对抗性智能体行为建模方法，采用PPO算法训练主车（ego-agent），并通过TD3算法结合对抗性奖励机制训练周围车辆为对抗性智能体，生成可导致主车失败的场景。实验表明，在对抗性环境下训练的主车更为鲁棒，获得更高的奖励和更优的生存表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Adversarial_Agent_Behavior_Learning_in_Autonomous_Driving_Using_Deep_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="6-Mag-Match-Magnetic-Vector-Field-Features-for-Map-Matching-and-Registration"><a href="#6-Mag-Match-Magnetic-Vector-Field-Features-for-Map-Matching-and-Registration" class="headerlink" title="6. Mag-Match: Magnetic Vector Field Features for Map Matching and Registration"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Mag-Match__Magnetic_Vector_Field_Features_for_Map_Matching_and_Registration.pdf">Mag-Match: Magnetic Vector Field Features for Map Matching and Registration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Technology Sydney</span></p>
<p>本文提出了Mag-Match，一种用于3D磁场地图匹配和注册的关键点检测与描述方法。该方法基于递归无散度高斯过程推断磁场及其高阶导数，在磁场曲率高且噪声低区域提取旋转不变的关键点描述符（HOV），通过最近邻与改进MSAC实现地图间的SE(3)配准。实验表明Mag-Match在无重力对齐情况下优于传统SIFT，能稳健实现多机器人&#x2F;多会话场景下的地图匹配与注册。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Mag-Match_Magnetic_Vector_Field_Features_for_Map_Matching_and_Registration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-Neural-Robot-Dynamics"><a href="#7-Neural-Robot-Dynamics" class="headerlink" title="7. Neural Robot Dynamics"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Neural_Robot_Dynamics.pdf">Neural Robot Dynamics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">NVIDIA</span></p>
<p>本文提出了NeRD（Neural Robot Dynamics），一种面向机器人多关节刚体的通用可泛化神经动力学模拟器。NeRD采用混合预测框架，将传统模拟器中的动力学与接触解算器替换为基于Transformer的神经网络，并提出机器人中心的空间不变状态表征，实现对不同任务、环境及控制器的泛化。结论表明，NeRD在多种机器人和任务中能够进行稳定准确的长时序预测，支持策略学习，并可通过少量真实数据高效微调实现仿真到现实的迁移。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Neural_Robot_Dynamics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-LLM-Driven-Self-Refinement-for-Embodied-Drone-Task-Planning"><a href="#8-LLM-Driven-Self-Refinement-for-Embodied-Drone-Task-Planning" class="headerlink" title="8. LLM-Driven Self-Refinement for Embodied Drone Task Planning"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/LLM-Driven_Self-Refinement_for_Embodied_Drone_Task_Planning.pdf">LLM-Driven Self-Refinement for Embodied Drone Task Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Central South University</span></p>
<p>该论文提出SRDrone系统，实现工业级无人机任务规划的自我优化。方法包括连续状态评估（CMSR算法）用于自主任务结果判断和故障解释，以及分层行为树（BT）修改模型，将LLM反思反馈转化为结构化规划优化。实验证明SRDrone在仿真和真实无人机任务中成功率显著高于现有方法，并具备强适应性和可扩展性，能在无需人工干预下自主适应复杂环境。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/LLM-Driven_Self-Refinement_for_Embodied_Drone_Task_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-Lang2Lift-A-Framework-for-Language-Guided-Pallet-Detection-and-Pose-Estimation-Integrated-in-Autonomous-Outdoor-Forklift-Operation"><a href="#9-Lang2Lift-A-Framework-for-Language-Guided-Pallet-Detection-and-Pose-Estimation-Integrated-in-Autonomous-Outdoor-Forklift-Operation" class="headerlink" title="9. Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Lang2Lift__A_Framework_for_Language-Guided_Pallet_Detection_and_Pose_Estimation_Integrated_in_Autono.pdf">Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AIT Austrian Institute of Technology GmbH</span></p>
<p>本论文提出Lang2Lift框架，通过集成Florence-2和SAM2等视觉语言基础模型实现自然语言引导的托盘检测与6D位姿估计，随后结合FoundationPose进行几何精化和时序跟踪，并将结果输入到运动规划模块，实现全流程户外自动驾驶叉车操作。实验表明，该系统在多种恶劣环境下实现0.76 mIoU分割精度和稳定的实时性能，显著提升了无人叉车的实用性与部署灵活性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Lang2Lift_A_Framework_for_Language-Guided_Pallet_Detection_and_Pose_Estimation_Integrated_in_Autonomous_Outdoor_Forklift_Operation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-JEDI-linear-Fast-and-Efficient-Graph-Neural-Networks-for-Jet-Tagging-on-FPGAs"><a href="#10-JEDI-linear-Fast-and-Efficient-Graph-Neural-Networks-for-Jet-Tagging-on-FPGAs" class="headerlink" title="10. JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/JEDI-linear__Fast_and_Efficient_Graph_Neural_Networks_for_Jet_Tagging_on_FPGAs.pdf">JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Imperial College London</span></p>
<p>本文提出了JEDI-linear，一种线性复杂度的GNN架构，通过移除显式的两两粒子交互，采用全局特征聚合及细粒度混合量化和分布式算法，实现了高效、低延迟且无DSP资源消耗的FPGA实时推理，适用于高能物理实验中的喷注识别。实验表明，JEDI-linear在准确率、资源利用和推理延迟上均优于现有方法，首次满足了CERN HL-LHC CMS一级触发系统的资源和时延约束。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/JEDI-linear_Fast_and_Efficient_Graph_Neural_Networks_for_Jet_Tagging_on_FPGAs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-RCDINO-Enhancing-Radar-Camera-3D-Object-Detection-with-DINOv2-Semantic-Features"><a href="#11-RCDINO-Enhancing-Radar-Camera-3D-Object-Detection-with-DINOv2-Semantic-Features" class="headerlink" title="11. RCDINO: Enhancing Radar-Camera 3D Object Detection with DINOv2 Semantic Features"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/RCDINO__Enhancing_Radar-Camera_3D_Object_Detection_with_DINOv2_Semantic_Features.pdf">RCDINO: Enhancing Radar-Camera 3D Object Detection with DINOv2 Semantic Features</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Moscow Institute of Physics and Technology</span></p>
<p>该论文提出了RCDINO，一种基于多模态Transformer的雷达-摄像头3D目标检测模型，通过轻量级适配器将预训练的DINOv2语义特征与视觉主干特征融合，增强视觉表示并提升检测性能。实验证明该方法在nuScenes数据集上对比主流模型获得了SOTA表现，NDS提升0.4%，mAP提升0.7%，并展现出较强的泛化能力和实际适用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/RCDINO_Enhancing_Radar-Camera_3D_Object_Detection_with_DINOv2_Semantic_Features.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-Search-Based-Credit-Assignment-for-Offline-Preference-Based-Reinforcement-Learning"><a href="#12-Search-Based-Credit-Assignment-for-Offline-Preference-Based-Reinforcement-Learning" class="headerlink" title="12. Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Search-Based_Credit_Assignment_for_Offline_Preference-Based_Reinforcement_Learning.pdf">Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本论文提出了一种搜索式偏好加权（SPW）方法，通过将偏好轨迹中的每一步与专家演示中的最相似状态-动作对进行匹配，计算重要性权重，并将其用于偏好学习，实现了细粒度的奖励归因。实验结果表明，SPW在机器人操作任务中，在仅有少量专家演示和偏好反馈条件下，显著优于现有纯偏好、模仿及混合基线方法，提升了任务成功率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Search-Based_Credit_Assignment_for_Offline_Preference-Based_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-AeroDuo-Aerial-Duo-for-UAV-based-Vision-and-Language-Navigation"><a href="#13-AeroDuo-Aerial-Duo-for-UAV-based-Vision-and-Language-Navigation" class="headerlink" title="13. AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/AeroDuo__Aerial_Duo_for_UAV-based_Vision_and_Language_Navigation.pdf">AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>该论文提出了DuAl-VLN任务和AeroDuo框架，实现了双高度无人机协同执行视觉-语言导航。高空无人机采用多模态大语言模型（Pilot-LLM）进行目标区域推理，低空无人机采用轻量级多阶段策略进行导航与目标定位。实验表明，AeroDuo协同方式在HaL-13k数据集上导航成功率较单无人机方法提升9.71%，有效提升了复杂环境下的导航效率和自主管理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/AeroDuo_Aerial_Duo_for_UAV-based_Vision_and_Language_Navigation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-面向具身操作的视觉-语言-动作模型综述"><a href="#14-面向具身操作的视觉-语言-动作模型综述" class="headerlink" title="14. 面向具身操作的视觉-语言-动作模型综述"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Survey_of_Vision-Language-Action_Models_for_Embodied_Manipulation.pdf">面向具身操作的视觉-语言-动作模型综述</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">中国科学院自动化研究所</span></p>
<p>本文系统梳理了视觉-语言-动作（VLA）模型在具身操作领域的发展脉络，详述了VLA模型架构、训练数据、预训练方法、后训练方法和模型评估等五大核心模块。作者提出VLA模型通过融合大模型技术实现视觉感知、语义推理与动作生成一体化，极大提升了机器人在开放环境下的泛化性和任务执行能力，并指出泛化能力、精细操作与推理实时性是未来VLA发展的关键挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/%E9%9D%A2%E5%90%91%E5%85%B7%E8%BA%AB%E6%93%8D%E4%BD%9C%E7%9A%84%E8%A7%86%E8%A7%89-%E8%AF%AD%E8%A8%80-%E5%8A%A8%E4%BD%9C%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="15-Visual-Autoregressive-Modeling-for-Instruction-Guided-Image-Editing"><a href="#15-Visual-Autoregressive-Modeling-for-Instruction-Guided-Image-Editing" class="headerlink" title="15. Visual Autoregressive Modeling for Instruction-Guided Image Editing"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Visual_Autoregressive_Modeling_for_Instruction-Guided_Image_Editing.pdf">Visual Autoregressive Modeling for Instruction-Guided Image Editing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本论文提出了VAREdit，一种基于视觉自回归（VAR）建模的指令引导图像编辑框架，将图像编辑重构为多尺度条件生成任务，并创新性地引入了Scale-Aligned Reference (SAR)模块，在自注意力第一层注入尺度匹配的源图信息以解决尺度不匹配问题。实验结果表明，VAREdit在编辑精准度和生成效率上均显著优于主流扩散模型，编辑保持性提升30%以上，推理速度提升2倍以上。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Visual_Autoregressive_Modeling_for_Instruction-Guided_Image_Editing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-Fine-grained-Multi-class-Nuclei-Segmentation-with-Molecular-empowered-All-in-SAM-Model"><a href="#16-Fine-grained-Multi-class-Nuclei-Segmentation-with-Molecular-empowered-All-in-SAM-Model" class="headerlink" title="16. Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Fine-grained_Multi-class_Nuclei_Segmentation_with_Molecular-empowered_All-in-SAM_Model.pdf">Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Vanderbilt University</span></p>
<p>本文提出了分子赋能的All-in-SAM模型，通过结合分子影像辅助弱标注、SAM模型自适应微调及MOCL分子导向纠错学习，实现多类细胞核的高精度分割。实验结果表明，该方法在不同数据量和标注质量下均优于现有主流方法，有效降低标注成本并提升病理图像自动分析的准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Fine-grained_Multi-class_Nuclei_Segmentation_with_Molecular-empowered_All-in-SAM_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-Probability-Density-from-Latent-Diffusion-Models-for-Out-of-Distribution-Detection"><a href="#17-Probability-Density-from-Latent-Diffusion-Models-for-Out-of-Distribution-Detection" class="headerlink" title="17. Probability Density from Latent Diffusion Models for Out-of-Distribution Detection"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Probability_Density_from_Latent_Diffusion_Models_for_Out-of-Distribution_Detection.pdf">Probability Density from Latent Diffusion Models for Out-of-Distribution Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Tartu</span></p>
<p>本文提出在预训练ResNet-18的表示空间上训练变分扩散模型（VDM），并结合概率流ODE计算精确对数似然，实现密度驱动的OOD检测。实验表明，利用表示空间的密度估计方法在OpenOOD基准上与主流检测方法表现相当，其中Top-K扩散损失（TKDL）方法更具稳定性和鲁棒性。结论：在表示空间进行密度估计可有效提升OOD检测性能，值得进一步研究。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Probability_Density_from_Latent_Diffusion_Models_for_Out-of-Distribution_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-End-to-End-Analysis-of-Charge-Stability-Diagrams-with-Transformers"><a href="#18-End-to-End-Analysis-of-Charge-Stability-Diagrams-with-Transformers" class="headerlink" title="18. End-to-End Analysis of Charge Stability Diagrams with Transformers"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/End-to-End_Analysis_of_Charge_Stability_Diagrams_with_Transformers.pdf">End-to-End Analysis of Charge Stability Diagrams with Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Department of Engineering Science, University of Oxford</span></p>
<p>本文提出了TRACS，一种基于Transformers的端到端模型，结合卷积神经网络(CNN)和Transformer架构，实现对半导体量子点阵列电荷稳定性图（CSD）的自动分析，包括三重点的识别和连通性提取。实验结果表明，TRACS在三种不同量子比特器件架构上均优于传统CNN模型YOLO，无需重新训练，具备更高的泛化性和推理速度，有助于自动化量子器件调控和扩展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/End-to-End_Analysis_of_Charge_Stability_Diagrams_with_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation"><a href="#19-CineScale-Free-Lunch-in-High-Resolution-Cinematic-Visual-Generation" class="headerlink" title="19. CineScale: Free Lunch in High-Resolution Cinematic Visual Generation"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/CineScale__Free_Lunch_in_High-Resolution_Cinematic_Visual_Generation.pdf">CineScale: Free Lunch in High-Resolution Cinematic Visual Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>该论文提出了CineScale，一种扩展自FreeScale的新型推理范式，专为扩展预训练扩散模型在高分辨率下的视觉生成能力设计。方法包含自级联上采样、受限膨胀卷积、多尺度频率融合和对DiT架构的NTK-RoPE等适配技术，并支持文本到图像、文本到视频、图像到视频和视频到视频生成。实验显示，CineScale在无需微调即可实现8k图像生成，仅需极少LoRA微调即可实现4k视频生成，生成质量优于现有方法。结论是CineScale显著拓展了主流扩散模型的高分辨率生成能力，提升了视觉质量和任务范围。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/CineScale_Free_Lunch_in_High-Resolution_Cinematic_Visual_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass"><a href="#20-SceneGen-Single-Image-3D-Scene-Generation-in-One-Feedforward-Pass" class="headerlink" title="20. SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/SceneGen__Single-Image_3D_Scene_Generation_in_One_Feedforward_Pass.pdf">SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出SceneGen框架，实现从单张场景图像及对象掩码直接生成多个3D资产的几何、纹理和空间关系，无需优化或检索。SceneGen采用视觉和几何编码器提取局部与全局特征，通过特征聚合模块和位置预测头，在单次前馈中完成3D场景生成，且可自然扩展到多图像输入。实验表明，SceneGen在几何、纹理质量及效率上均优于现有方法，能高效生成物理合理且高质量的3D场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/SceneGen_Single-Image_3D_Scene_Generation_in_One_Feedforward_Pass.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-WAVER-WAVE-YOUR-WAY-TO-LIFELIKE-VIDEO-GENERATION"><a href="#21-WAVER-WAVE-YOUR-WAY-TO-LIFELIKE-VIDEO-GENERATION" class="headerlink" title="21. WAVER: WAVE YOUR WAY TO LIFELIKE VIDEO GENERATION"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Waver__Wave_Your_Way_to_Lifelike_Video_Generation.pdf">WAVER: WAVE YOUR WAY TO LIFELIKE VIDEO GENERATION</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bytedance</span></p>
<p>本文提出了Waver，一个基于混合流Rectified Flow Transformer的统一图像与视频生成基础模型，采用了Task-Unified DiT与Cascade Refiner架构，支持文本到视频、图片到视频和文本到图片等多任务，结合高质量数据过滤管线与多阶段训练优化策略，实现了高分辨率（本地720p至1080p）视频的高效生成。实验证明Waver在复杂运动场景下运动幅度和时序一致性表现优异，在多个公开和自建基准上排名领先，超越开源模型并达到商用水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/WAVER_WAVE_YOUR_WAY_TO_LIFELIKE_VIDEO_GENERATION.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-“Does-the-cafe-entrance-look-accessible-Where-is-the-door-”-Towards-Geospatial-AI-Agents-for-Visual-Inquiries"><a href="#22-“Does-the-cafe-entrance-look-accessible-Where-is-the-door-”-Towards-Geospatial-AI-Agents-for-Visual-Inquiries" class="headerlink" title="22. “Does the cafe entrance look accessible? Where is the door?” Towards Geospatial AI Agents for Visual Inquiries"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/_Does_the_cafe_entrance_look_accessible__Where_is_the_door___Towards_Geospatial_AI_Agents_for_Visual.pdf">“Does the cafe entrance look accessible? Where is the door?” Towards Geospatial AI Agents for Visual Inquiries</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Washington</span></p>
<p>本文提出了Geo-Visual Agents的愿景，即多模态AI体能够融合大规模地理空间图像（如街景、航拍和用户上传照片）与GIS数据，实时理解和回答复杂的视觉-空间地理问题。方法上，结合计算机视觉、场景理解、空间推理与多模态大模型，实现如Accessibility Scout和StreetViewAI等原型系统，支持个性化、交互式地理空间视觉问答。结论认为，这一方向可极大提升导航与无障碍体验，但在数据融合、空间推理、个性化和语音输出等方面仍面临诸多挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/%E2%80%9CDoes_the_cafe_entrance_look_accessible_Where_is_the_door_%E2%80%9D_Towards_Geospatial_AI_Agents_for_Visual_Inquiries.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-LLM-empowered-Dynamic-Prompt-Routing-for-Vision-Language-Models-Tuning-under-Long-Tailed-Distributions"><a href="#23-LLM-empowered-Dynamic-Prompt-Routing-for-Vision-Language-Models-Tuning-under-Long-Tailed-Distributions" class="headerlink" title="23. LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning under Long-Tailed Distributions"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/LLM-empowered_Dynamic_Prompt_Routing_for_Vision-Language_Models_Tuning_under_Long-Tailed_Distributio.pdf">LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning under Long-Tailed Distributions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shandong University</span></p>
<p>该论文提出了MDPR（Multi-dimensional Dynamic Prompt Routing）框架，用于解决视觉-语言模型（VLM）在长尾分布下微调时的类别偏置问题。方法上，MDPR通过大语言模型（LLM）构建五维语义知识库，并采用动态路由机制对类别语义进行图像感知聚合，结合正则化提升尾类表现。实验结果表明，MDPR在CIFAR-100-LT、ImageNet-LT和Places-LT等长尾数据集上显著提升了尾类识别准确率，同时保持头类和中类的稳健性，达到SOTA或高度竞争的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/LLM-empowered_Dynamic_Prompt_Routing_for_Vision-Language_Models_Tuning_under_Long-Tailed_Distributions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-GRAFT-GRAPH-AND-TABLE-REASONING-FOR-TEXTUAL-ALIGNMENT-—-A-BENCHMARK-FOR-STRUCTURED-INSTRUCTION-FOLLOWING-AND-VISUAL-REASONING"><a href="#24-GRAFT-GRAPH-AND-TABLE-REASONING-FOR-TEXTUAL-ALIGNMENT-—-A-BENCHMARK-FOR-STRUCTURED-INSTRUCTION-FOLLOWING-AND-VISUAL-REASONING" class="headerlink" title="24. GRAFT: GRAPH AND TABLE REASONING FOR TEXTUAL ALIGNMENT — A BENCHMARK FOR STRUCTURED INSTRUCTION FOLLOWING AND VISUAL REASONING"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/GRAFT__GRaPH_and_Table_Reasoning_for_Textual_Alignment_--_A_Benchmark_for_Structured_Instruction_Fol.pdf">GRAFT: GRAPH AND TABLE REASONING FOR TEXTUAL ALIGNMENT — A BENCHMARK FOR STRUCTURED INSTRUCTION FOLLOWING AND VISUAL REASONING</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ServiceNow</span></p>
<p>本文提出了GRAFT基准，专为评估多模态模型在结构化视觉推理、指令跟随和视觉-文本对齐任务中的能力。GRAFT利用Python可控生成的图表和表格图像，配以多步视觉推理问题和结构化答案（JSON&#x2F;YAML），并通过多LLM裁判机制确保高质量评测。结论指出，当前主流多模态模型在输出格式和完整性上表现良好，但在复杂视觉推理和语义对齐方面仍有显著提升空间，GRAFT为未来多模态问答模型的评估与发展提供了统一且细粒度的平台。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/GRAFT_GRAPH_AND_TABLE_REASONING_FOR_TEXTUAL_ALIGNMENT_%E2%80%94_A_BENCHMARK_FOR_STRUCTURED_INSTRUCTION_FOLLOWING_AND_VISUAL_REASONING.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-MapKD-Unlocking-Prior-Knowledge-with-Cross-Modal-Distillation-for-Efficient-Online-HD-Map-Construction"><a href="#25-MapKD-Unlocking-Prior-Knowledge-with-Cross-Modal-Distillation-for-Efficient-Online-HD-Map-Construction" class="headerlink" title="25. MapKD: Unlocking Prior Knowledge with Cross-Modal Distillation for Efficient Online HD Map Construction"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/MapKD__Unlocking_Prior_Knowledge_with_Cross-Modal_Distillation_for_Efficient_Online_HD_Map_Construct.pdf">MapKD: Unlocking Prior Knowledge with Cross-Modal Distillation for Efficient Online HD Map Construction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本文提出MapKD，一种新颖的三阶段教师-教练-学生跨模态知识蒸馏框架，用于高效的在线高精地图构建。其核心流程包含：以多模态融合模型为教师、通过模拟LiDAR的教练桥接模态差异、并以轻量级视觉学生为目标，并设计了Token-Guided Patch Distillation（TGPD）和Masked Semantic Response Distillation（MSRD）两种针对性蒸馏策略，实现鸟瞰图特征和语义响应的对齐。实验结果表明，MapKD无需依赖昂贵的HD地图先验，仅用摄像头输入即可显著提升学生模型性能（mIoU提升6.68，mAP提升10.94），且推理速度更快，具有良好的实用性和部署价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/MapKD_Unlocking_Prior_Knowledge_with_Cross-Modal_Distillation_for_Efficient_Online_HD_Map_Construction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding"><a href="#26-When-and-What-Diffusion-Grounded-VideoLLM-with-Entity-Aware-Segmentation-for-Long-Video-Understanding" class="headerlink" title="26. When and What: Diffusion-Grounded VideoLLM with Entity-Aware Segmentation for Long Video Understanding"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/When_and_What__Diffusion-Grounded_VideoLLM_with_Entity_Aware_Segmentation_for_Long_Video_Understandi.pdf">When and What: Diffusion-Grounded VideoLLM with Entity-Aware Segmentation for Long Video Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出Grounded-VideoDiT，一种面向长视频理解的Video-LLM，结合扩散模型的时序特征提取（Diffusion Temporal Latent, DTL）、语义分割驱动的实体级表示和混合token输入结构，实现了事件精准定位和实体追踪。实验表明该方法在Charades-STA、NExT-GQA等多个长视频基准上取得了最优或高度竞争的性能，显著提升了时序和实体对齐能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/When_and_What_Diffusion-Grounded_VideoLLM_with_Entity-Aware_Segmentation_for_Long_Video_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-Deep-Equilibrium-Convolutional-Sparse-Coding-for-Hyperspectral-Image-Denoising"><a href="#27-Deep-Equilibrium-Convolutional-Sparse-Coding-for-Hyperspectral-Image-Denoising" class="headerlink" title="27. Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Deep_Equilibrium_Convolutional_Sparse_Coding_for_Hyperspectral_Image_Denoising.pdf">Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Science and Technology</span></p>
<p>该论文提出了Deep Equilibrium Convolutional Sparse Coding (DECSC) 框架，结合深度平衡模型（DEQ）与卷积稀疏编码（CSC），用于高光谱图像去噪。方法通过分解全局空间-谱一致性与局部空间-谱细节，并引入Swin Transformer和细节增强模块，提升了对非局部自相似性与细节的建模能力。实验表明，DECSC相比现有方法在各种合成与真实噪声条件下均取得了更优的去噪表现，具有更强的鲁棒性与物理可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Deep_Equilibrium_Convolutional_Sparse_Coding_for_Hyperspectral_Image_Denoising.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-D3FNet-A-Differential-Attention-Fusion-Network-for-Fine-Grained-Road-Structure-Extraction-in-Remote-Perception-Systems"><a href="#28-D3FNet-A-Differential-Attention-Fusion-Network-for-Fine-Grained-Road-Structure-Extraction-in-Remote-Perception-Systems" class="headerlink" title="28. D3FNet: A Differential Attention Fusion Network for Fine-Grained Road Structure Extraction in Remote Perception Systems"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/D3FNet__A_Differential_Attention_Fusion_Network_for_Fine-Grained_Road_Structure_Extraction_in_Remote.pdf">D3FNet: A Differential Attention Fusion Network for Fine-Grained Road Structure Extraction in Remote Perception Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Budapest University of Technology and Economics</span></p>
<p>该论文提出了D3FNet，一种基于D-LinkNet的Dilated Dual-Stream Differential Attention Fusion Network，用于高分辨率遥感影像中细粒度窄路结构提取。方法创新包括差分注意力膨胀提取模块（DADE）强化道路特征，双流解码融合机制（DDFM）实现结构与语义信息融合，并采用多尺度膨胀策略（1,3,5,9）提升连续性。实验结果表明D3FNet在DeepGlobe和CHN6-CUG数据集上对窄路段提取性能优越，IoU和Recall均高于主流模型，且能有效恢复遮挡和低对比度路段的连通性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/D3FNet_A_Differential_Attention_Fusion_Network_for_Fine-Grained_Road_Structure_Extraction_in_Remote_Perception_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-Aligning-Moments-in-Time-using-Video-Queries"><a href="#29-Aligning-Moments-in-Time-using-Video-Queries" class="headerlink" title="29. Aligning Moments in Time using Video Queries"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Aligning_Moments_in_Time_using_Video_Queries.pdf">Aligning Moments in Time using Video Queries</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Indian Institute of Technology Jodhpur</span></p>
<p>本文提出了MATR（Moment Alignment TRansformer），一种针对视频到视频时刻检索（Vid2VidMR）任务的Transformer架构，通过显式的双阶段序列对齐机制和自监督预训练方法，实现了查询视频与目标视频在时序和语义上的精确对齐，并结合前景&#x2F;背景分类及边界预测头，实现时刻精确定位。实验结果表明，MATR在ActivityNet-VRL和新提出的SportsMoments数据集上对比现有方法，在mIoU和R@1等指标上均取得了显著提升，验证了其方法的有效性和优越性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Aligning_Moments_in_Time_using_Video_Queries.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-Task-Generalized-Adaptive-Cross-Domain-Learning-for-Multimodal-Image-Fusion"><a href="#30-Task-Generalized-Adaptive-Cross-Domain-Learning-for-Multimodal-Image-Fusion" class="headerlink" title="30. Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Task-Generalized_Adaptive_Cross-Domain_Learning_for_Multimodal_Image_Fusion.pdf">Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanchang Hangkong University</span></p>
<p>该论文提出了AdaSFFuse框架，采用自适应近似小波变换（AdaWAT）进行多模态图像的高低频特征解耦，并通过空间-频率Mamba块实现跨域高效融合。实验结果表明，AdaSFFuse在红外-可见、多曝光、多焦点和医学图像融合等任务上表现优异，兼具高效性和泛化能力，成为多模态融合领域新基准。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Task-Generalized_Adaptive_Cross-Domain_Learning_for_Multimodal_Image_Fusion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-Thinning-a-medical-image-segmentation-model-via-dual-level-multiscale-fusion"><a href="#31-Thinning-a-medical-image-segmentation-model-via-dual-level-multiscale-fusion" class="headerlink" title="31. Thinning a medical image segmentation model via dual-level multiscale fusion"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/LGMSNet__Thinning_a_medical_image_segmentation_model_via_dual-level_multiscale_fusion.pdf">Thinning a medical image segmentation model via dual-level multiscale fusion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本论文提出了LGMSNet，一种结合本地多尺度卷积（LMS块）与稀疏Transformer-卷积混合（GMS块）的轻量级医学图像分割网络。该方法有效缓解了通道冗余和全局上下文建模不足的问题；在六个公开数据集上取得了SOTA分割精度和参数效率，尤其在四个未见数据集上表现出优异的跨域泛化能力，适用于资源受限医疗场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Thinning_a_medical_image_segmentation_model_via_dual-level_multiscale_fusion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-A-Curated-Dataset-and-Deep-Learning-Approach-for-Minor-Dent-Detection-in-Vehicles"><a href="#32-A-Curated-Dataset-and-Deep-Learning-Approach-for-Minor-Dent-Detection-in-Vehicles" class="headerlink" title="32. A Curated Dataset and Deep Learning Approach for Minor Dent Detection in Vehicles"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/A_Curated_Dataset_and_Deep_Learning_Approach_for_Minor_Dent_Detection_in_Vehicles.pdf">A Curated Dataset and Deep Learning Approach for Minor Dent Detection in Vehicles</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Sciences and Technology (NUST)</span></p>
<p>本文提出了一种基于YOLOv8m及其改进变体（YOLOv8m-t4和YOLOv8m-t42）的深度学习检测流程，针对汽车表面微小凹陷，构建并公开了多视角、自然光照条件下的高质量标注数据集，并在训练中采用实时数据增强方法优化模型泛化能力。实验结果显示，YOLOv8m-t42模型在精度、召回率和F1分数上优于YOLOv8m-t4，具备更高的实际应用可靠性，适合工业和保险自动化场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/A_Curated_Dataset_and_Deep_Learning_Approach_for_Minor_Dent_Detection_in_Vehicles.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-From-Linearity-to-Non-Linearity-How-Masked-Autoencoders-Capture-Spatial-Correlations"><a href="#33-From-Linearity-to-Non-Linearity-How-Masked-Autoencoders-Capture-Spatial-Correlations" class="headerlink" title="33. From Linearity to Non-Linearity: How Masked Autoencoders Capture Spatial Correlations"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/From_Linearity_to_Non-Linearity__How_Masked_Autoencoders_Capture_Spatial_Correlations.pdf">From Linearity to Non-Linearity: How Masked Autoencoders Capture Spatial Correlations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Pennsylvania</span></p>
<p>本论文系统分析了Masked Autoencoders（MAE）在视觉领域如何通过调整遮罩比例和patch大小来捕获输入图像的空间相关性。方法上，作者对线性MAE推导了解析特征表达，说明关键超参数如何影响模型对图像空间长短程相关性的学习，并拓展至非线性MAE，通过分析输入输出Jacobian，揭示了MAE自适应学习空间相关特征的机制。结论指出，MAE能有效适应不同下游任务的数据空间特性，但其超参数需针对数据集仔细调优。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/From_Linearity_to_Non-Linearity_How_Masked_Autoencoders_Capture_Spatial_Correlations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-Image-Conditioned-3D-Gaussian-Splat-Quantization"><a href="#34-Image-Conditioned-3D-Gaussian-Splat-Quantization" class="headerlink" title="34. Image-Conditioned 3D Gaussian Splat Quantization"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Image-Conditioned_3D_Gaussian_Splat_Quantization.pdf">Image-Conditioned 3D Gaussian Splat Quantization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, San Diego</span></p>
<p>该论文提出了ICGS-Quantizer，一种图像条件化的3D高斯点云量化方法，通过联合高斯及其属性的矢量量化和跨场景共享码本，将3D场景压缩至KB级存储；解码端可利用当前采集图像进行条件化重建，自适应场景照明与外观变化。实验表明，该方法在压缩率和场景自适应性方面显著优于现有方法，能高保真恢复和动态更新3D场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Image-Conditioned_3D_Gaussian_Splat_Quantization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-Transfer-learning-optimization-based-on-evolutionary-selective-fine-tuning"><a href="#35-Transfer-learning-optimization-based-on-evolutionary-selective-fine-tuning" class="headerlink" title="35. Transfer learning optimization based on evolutionary selective fine tuning"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Transfer_learning_optimization_based_on_evolutionary_selective_fine_tuning.pdf">Transfer learning optimization based on evolutionary selective fine tuning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Engineering</span></p>
<p>本文提出了BioTune，一种基于进化算法的自适应选择性微调方法，通过进化搜索自动决定哪些层进行微调和优化学习率，实现了迁移学习的高效参数调整。实验证明BioTune在九个图像分类数据集上相较于现有微调方法提升了准确率，并显著降低了可训练参数比例和计算成本，适用于不同领域的迁移学习任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Transfer_learning_optimization_based_on_evolutionary_selective_fine_tuning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-VideoEraser-Concept-Erasure-in-Text-to-Video-Diffusion-Models"><a href="#36-VideoEraser-Concept-Erasure-in-Text-to-Video-Diffusion-Models" class="headerlink" title="36. VideoEraser: Concept Erasure in Text-to-Video Diffusion Models"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/VideoEraser__Concept_Erasure_in_Text-to-Video_Diffusion_Models.pdf">VideoEraser: Concept Erasure in Text-to-Video Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出了VideoEraser，一种无需训练、即插即用的文本到视频扩散模型概念擦除框架。方法包括选择性提示嵌入调整（SPEA）和对抗鲁棒噪声引导（ARNG），用于在不损害无关内容生成能力的前提下，抑制模型生成不良或敏感概念。实验显示，VideoEraser在目标内容擦除、视频质量、鲁棒性和泛化性等方面均优于现有方法，在多种主流T2V扩散模型及四类任务（物体、艺术风格、名人、显式内容擦除）上均取得SOTA表现。结论是VideoEraser能有效、安全地防止T2V模型生成不良内容，适用于实际监管和合规场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/VideoEraser_Concept_Erasure_in_Text-to-Video_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-First-RAG-Second-SEG-A-Training-Free-Paradigm-for-Camouflaged-Object-Detection"><a href="#37-First-RAG-Second-SEG-A-Training-Free-Paradigm-for-Camouflaged-Object-Detection" class="headerlink" title="37. First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/First_RAG,_Second_SEG__A_Training-Free_Paradigm_for_Camouflaged_Object_Detection.pdf">First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Aeronautics and Astronautics</span></p>
<p>本文提出了一种无训练的伪装目标检测（COD）新范式RAG-SEG，将COD任务分为两阶段：首先通过RAG（检索增强生成）自动生成粗略掩码作为提示，随后利用SAM2进行精细分割。方法通过DINOv2提取特征构建向量数据库，FAISS聚类压缩检索空间，推理时用检索结果生成伪标签，再经SAM2优化分割，彻底免除传统训练。实验表明，RAG-SEG在标准COD数据集上与SOTA方法性能相当甚至更优，并能高效运行在普通笔记本电脑上，兼具实用性与低算力消耗。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/First_RAG_Second_SEG_A_Training-Free_Paradigm_for_Camouflaged_Object_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-BasketLiDAR-The-First-LiDAR-Camera-Multimodal-Dataset-for-Professional-Basketball-MOT"><a href="#38-BasketLiDAR-The-First-LiDAR-Camera-Multimodal-Dataset-for-Professional-Basketball-MOT" class="headerlink" title="38. BasketLiDAR: The First LiDAR-Camera Multimodal Dataset for Professional Basketball MOT"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/BasketLiDAR__The_First_LiDAR-Camera_Multimodal_Dataset_for_Professional_Basketball_MOT.pdf">BasketLiDAR: The First LiDAR-Camera Multimodal Dataset for Professional Basketball MOT</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Keio University</span></p>
<p>该论文首次构建了面向专业篮球多目标跟踪（MOT）的LiDAR-多摄像头同步多模态数据集BasketLiDAR，并提出了一种融合LiDAR点云与多视角摄像头图像的高效MOT方法。实验表明，该方法在遮挡场景下显著提升了跟踪准确率与ID一致性，并大幅降低了计算成本，实现了实时高精度运动员3D跟踪。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/BasketLiDAR_The_First_LiDAR-Camera_Multimodal_Dataset_for_Professional_Basketball_MOT.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-DESIGNCLIP-Multimodal-Learning-with-CLIP-for-Design-Patent-Understanding"><a href="#39-DESIGNCLIP-Multimodal-Learning-with-CLIP-for-Design-Patent-Understanding" class="headerlink" title="39. DESIGNCLIP: Multimodal Learning with CLIP for Design Patent Understanding"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/DesignCLIP__Multimodal_Learning_with_CLIP_for_Design_Patent_Understanding.pdf">DESIGNCLIP: Multimodal Learning with CLIP for Design Patent Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Chicago</span></p>
<p>该论文提出了DESIGNCLIP，一个针对设计专利多模态分析的CLIP改进框架，通过引入类别感知采样与对比损失，有效缓解专利类别长尾分布问题，并结合多视角图像对比学习与多任务预训练（包括专利分类、图像-文本和多视角对比学习）。实验显示，DESIGNCLIP在专利分类、检索和多模态检索等任务上均优于现有SOTA方法，提升了专利领域的多模态理解与泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/DESIGNCLIP_Multimodal_Learning_with_CLIP_for_Design_Patent_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-RATopo-Improving-Lane-Topology-Reasoning-via-Redundancy-Assignment"><a href="#40-RATopo-Improving-Lane-Topology-Reasoning-via-Redundancy-Assignment" class="headerlink" title="40. RATopo: Improving Lane Topology Reasoning via Redundancy Assignment"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/RATopo__Improving_Lane_Topology_Reasoning_via_Redundancy_Assignment.pdf">RATopo: Improving Lane Topology Reasoning via Redundancy Assignment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本文提出了一种名为RATopo的模型无关型冗余分配策略，通过重构Transformer解码器（交换交叉注意力和自注意力顺序）并引入多路并行交叉注意力模块，实现对车道拓扑推理任务的数量丰富和几何多样的有效监督。实验结果显示，RATopo能无缝集成到现有DETR风格的拓扑推理框架，显著提升OpenLane-V2等数据集上的车道-车道与车道-交通元素拓扑推理性能，具备良好的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/RATopo_Improving_Lane_Topology_Reasoning_via_Redundancy_Assignment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-Explainable-Knowledge-Distillation-for-Efficient-Medical-Image-Classification"><a href="#41-Explainable-Knowledge-Distillation-for-Efficient-Medical-Image-Classification" class="headerlink" title="41. Explainable Knowledge Distillation for Efficient Medical Image Classification"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Explainable_Knowledge_Distillation_for_Efficient_Medical_Image_Classification.pdf">Explainable Knowledge Distillation for Efficient Medical Image Classification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Jamia Millia Islamia</span></p>
<p>本文提出一种结合知识蒸馏与可解释性AI的框架，将高容量教师模型（如DenseNet-201、Visformer-S、AutoFormerV2-T）知识压缩到轻量化的OFA-595学生模型，采用混合损失函数（硬标签与软目标）并通过Score-CAM实现模型解释。实验结果表明，该学生模型在COVID-QU-Ex和LCS25000医学影像数据集上保持高准确率和F1分数，参数量和推理时间显著降低，适用于资源受限临床环境。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Explainable_Knowledge_Distillation_for_Efficient_Medical_Image_Classification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-Pathology-Informed-Latent-Diffusion-Model-for-Anomaly-Detection-in-Lymph-Node-Metastasis"><a href="#42-Pathology-Informed-Latent-Diffusion-Model-for-Anomaly-Detection-in-Lymph-Node-Metastasis" class="headerlink" title="42. Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph Node Metastasis"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Pathology-Informed_Latent_Diffusion_Model_for_Anomaly_Detection_in_Lymph_Node_Metastasis.pdf">Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph Node Metastasis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea University</span></p>
<p>该论文提出了一种结合潜在扩散模型（LDM）和视觉语言模型（VLM）的无监督异常检测方法AnoPILaD，通过利用病理学关键词引导重建过程，实现对淋巴结转移病理图像异常区域的高效检测。实验结果表明，该方法在不同器官和数据域下均优于现有密度和重建类方法，具有较强的泛化性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Pathology-Informed_Latent_Diffusion_Model_for_Anomaly_Detection_in_Lymph_Node_Metastasis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation"><a href="#43-Collaborative-Multi-Modal-Coding-for-High-Quality-3D-Generation" class="headerlink" title="43. Collaborative Multi-Modal Coding for High-Quality 3D Generation"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Collaborative_Multi-Modal_Coding_for_High-Quality_3D_Generation.pdf">Collaborative Multi-Modal Coding for High-Quality 3D Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>该论文提出了TriMM，一种协同多模态编码及三平面潜变量扩散模型，能够将RGB、RGBD及点云等多模态特征映射到统一三平面隐空间，通过结合2D图像和3D结构的混合监督及专门的重建损失，高效提升3D生成的几何和纹理质量。实验表明，TriMM在小规模数据集下实现了与大规模训练模型相媲美的3D生成效果，并验证了多模态数据可显著提升模型性能与泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Collaborative_Multi-Modal_Coding_for_High-Quality_3D_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-STAGNet-A-Spatio-Temporal-Graph-and-LSTM-Framework-for-Accident-Anticipation"><a href="#44-STAGNet-A-Spatio-Temporal-Graph-and-LSTM-Framework-for-Accident-Anticipation" class="headerlink" title="44. STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/STAGNet__A_Spatio-Temporal_Graph_and_LSTM_Framework_for_Accident_Anticipation.pdf">STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Moratuwa</span></p>
<p>该论文提出了STAGNet框架，将SlowFast网络提取的全局时空特征与LSTM进行帧级聚合，并在对象级图中采用GATv2注意力机制，专注于关键交通目标及其动态关系，实现了仅依赖行车记录仪视频的交通事故早期预测。实验表明，STAGNet在多个公开数据集上平均精度和事故预测提前量均优于现有方法，且跨数据集泛化能力更强。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/STAGNet_A_Spatio-Temporal_Graph_and_LSTM_Framework_for_Accident_Anticipation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-TPA-Temporal-Prompt-Alignment-for-Fetal-Congenital-Heart-Defect-Classification"><a href="#45-TPA-Temporal-Prompt-Alignment-for-Fetal-Congenital-Heart-Defect-Classification" class="headerlink" title="45. TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/TPA__Temporal_Prompt_Alignment_for_Fetal_Congenital_Heart_Defect_Classification.pdf">TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</span></p>
<p>本文提出了Temporal Prompt Alignment (TPA)方法，通过结合视觉-语言基础模型、可训练时序特征提取器与提示对齐对比学习，实现了对胎儿超声心脏视频先天性心脏病（CHD）的多类别分类。TPA在视频级别引入了条件变分自编码风格调制（CVAESM）模块以量化不确定性，从而提升模型在临床中的可靠性。实验表明，TPA在私有CHD数据集和EchoNet-Dynamic公开数据集上均取得了当前最佳的分类性能，并显著优化了模型置信度校准。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/TPA_Temporal_Prompt_Alignment_for_Fetal_Congenital_Heart_Defect_Classification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-Comp-X-On-Defining-an-Interactive-Learned-Image-Compression-Paradigm-With-Expert-driven-LLM-Agent"><a href="#46-Comp-X-On-Defining-an-Interactive-Learned-Image-Compression-Paradigm-With-Expert-driven-LLM-Agent" class="headerlink" title="46. Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Comp-X__On_Defining_an_Interactive_Learned_Image_Compression_Paradigm_With_Expert-driven_LLM_Agent.pdf">Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本论文提出Comp-X：一种由大语言模型（LLM）智能体驱动的交互式图像压缩新范式，集成多功能统一编解码框架，支持人机感知、变量压缩和空间比特分配。核心方法包括：1）多功能统一图像编解码器，2）结合专家反馈的增强式上下文学习（ICL-EF）提升LLM理解压缩知识与用户需求的能力，3）首个交互式图像压缩评测基准IIC-Bench。实验表明，Comp-X能高效理解用户自然语言指令，实现自动参数选择和自适应压缩，指令解析成功率超80%，压缩性能与现有顶尖方法相当，展示了在智能图像压缩和通用人工智能方向的前景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Comp-X_On_Defining_an_Interactive_Learned_Image_Compression_Paradigm_With_Expert-driven_LLM_Agent.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-Pretrained-Diffusion-Models-Are-Inherently-Skipped-Step-Samplers"><a href="#47-Pretrained-Diffusion-Models-Are-Inherently-Skipped-Step-Samplers" class="headerlink" title="47. Pretrained Diffusion Models Are Inherently Skipped-Step Samplers"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Pretrained_Diffusion_Models_Are_Inherently_Skipped-Step_Samplers.pdf">Pretrained Diffusion Models Are Inherently Skipped-Step Samplers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wuhan University</span></p>
<p>该论文提出了一种跳步采样机制，证明预训练扩散模型本身即可实现跳过多个中间去噪步骤以加速生成，无需重新训练或复杂修改，并与DDIM方法结合进一步提升采样效率和生成质量。实验在OpenAI ADM、Stable Diffusion和Open Sora等主流扩散模型上，涵盖图像和视频生成，结果显示在大幅减少采样步数的同时，生成质量和多样性优于主流基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Pretrained_Diffusion_Models_Are_Inherently_Skipped-Step_Samplers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-Center-Oriented-Prototype-Contrastive-Clustering"><a href="#48-Center-Oriented-Prototype-Contrastive-Clustering" class="headerlink" title="48. Center-Oriented Prototype Contrastive Clustering"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Center-Oriented_Prototype_Contrastive_Clustering.pdf">Center-Oriented Prototype Contrastive Clustering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Information Science and Technology</span></p>
<p>该论文提出一种中心导向原型对比聚类方法（CPCC），结合软原型对比模块和双一致性学习模块，软原型通过样本属于聚类中心的概率加权计算原型，减少类间冲突和原型漂移；双一致性学习通过对同一样本不同变换和不同样本邻域的一致性约束，保证特征变换不变的语义信息和紧凑的类内分布。实验结果表明，该方法在五个数据集上聚类表现优于现有主流方法，并提升了训练稳定性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Center-Oriented_Prototype_Contrastive_Clustering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-GenTune-Toward-Traceable-Prompts-to-Improve-Controllability-of-Image-Refinement-in-Environment-Design"><a href="#49-GenTune-Toward-Traceable-Prompts-to-Improve-Controllability-of-Image-Refinement-in-Environment-Design" class="headerlink" title="49. GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/GenTune__Toward_Traceable_Prompts_to_Improve_Controllability_of_Image_Refinement_in_Environment_Desi.pdf">GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Taiwan University</span></p>
<p>该论文提出了GenTune系统，通过可追溯的文本标签与图像元素映射，结合语义引导的逐步细粒度图像修饰方法，提升环境设计中AI生成图像的可理解性与可控性。实验显示，GenTune显著提升了设计师对生成内容的理解、精修效率及结果一致性，并在真实工作流中有效改善了创意沟通与设计效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/GenTune_Toward_Traceable_Prompts_to_Improve_Controllability_of_Image_Refinement_in_Environment_Design.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-See-it-Say-it-Sorted-Agentic-System-for-Compositional-Diagram-Generation"><a href="#50-See-it-Say-it-Sorted-Agentic-System-for-Compositional-Diagram-Generation" class="headerlink" title="50. See it. Say it. Sorted: Agentic System for Compositional Diagram Generation"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/See_it._Say_it._Sorted__Agentic_System_for_Compositional_Diagram_Generation.pdf">See it. Say it. Sorted: Agentic System for Compositional Diagram Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Yale University</span></p>
<p>该论文提出了一种训练无关的智能体系统，结合视觉语言模型（VLM）和大语言模型（LLM），通过迭代Critic–Candidates–Judge流程将手绘草图和文本说明转换为可编辑SVG图形。方法强调高层次的结构推理和多策略SVG生成，实验结果表明在结构重建和细节保留方面显著优于前沿图像生成LLM模型。结论：系统能稳定、准确地生成可控且可扩展的结构化图表，可集成到实际设计工具中。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/See_it._Say_it._Sorted_Agentic_System_for_Compositional_Diagram_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-MeSS-City-Mesh-Guided-Outdoor-Scene-Generation-with-Cross-View-Consistent-Diffusion"><a href="#51-MeSS-City-Mesh-Guided-Outdoor-Scene-Generation-with-Cross-View-Consistent-Diffusion" class="headerlink" title="51. MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/MeSS__City_Mesh-Guided_Outdoor_Scene_Generation_with_Cross-View_Consistent_Diffusion.pdf">MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">TU Munich</span></p>
<p>该论文提出MeSS，一种利用城市网格模型作为几何先验，通过多阶段图像扩散模型（Cascaded Outpainting ControlNets、Appearance Guided Inpainting和Global Consistency Alignment）生成高质量、跨视角一致的3D高斯场景。方法在保持几何对齐与外观一致性的同时，显著提升了城市级室外场景生成的细节和风格多样性，实验表明其在视觉质量和一致性上均优于同类方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/MeSS_City_Mesh-Guided_Outdoor_Scene_Generation_with_Cross-View_Consistent_Diffusion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-Zero-shot-Volumetric-CT-Super-Resolution-using-3D-Gaussian-Splatting-with-Upsampled-2D-X-ray-Projection-Priors"><a href="#52-Zero-shot-Volumetric-CT-Super-Resolution-using-3D-Gaussian-Splatting-with-Upsampled-2D-X-ray-Projection-Priors" class="headerlink" title="52. Zero-shot Volumetric CT Super-Resolution using 3D Gaussian Splatting with Upsampled 2D X-ray Projection Priors"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Zero-shot_Volumetric_CT_Super-Resolution_using_3D_Gaussian_Splatting_with_Upsampled_2D_X-ray_Project.pdf">Zero-shot Volumetric CT Super-Resolution using 3D Gaussian Splatting with Upsampled 2D X-ray Projection Priors</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea University</span></p>
<p>该论文提出了一种零样本3D CT超分辨率重建框架，利用基于扩散模型的高分辨率2D X射线投影作为先验，结合3D Gaussian Splatting（3DGS）与提出的负Alpha混合（NAB-GS）机制，实现3D CT体数据的高质量超分重建。方法流程为：首先对低分辨率CT体数据生成多角度2D投影，利用无监督扩散模型和逐投影自适应采样（PAS）策略提升2D投影分辨率，然后用NAB-GS在3DGS框架下进行残差学习重建高分辨率3D CT体。实验结果显示，该方法在无高分辨率CT体训练数据的情况下，在定量和定性指标上均优于现有方法，并能更好地恢复解剖细节。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Zero-shot_Volumetric_CT_Super-Resolution_using_3D_Gaussian_Splatting_with_Upsampled_2D_X-ray_Projection_Priors.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation"><a href="#53-Mobile-Agent-v3-Foundamental-Agents-for-GUI-Automation" class="headerlink" title="53. Mobile-Agent-v3: Foundamental Agents for GUI Automation"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Mobile-Agent-v3__Foundamental_Agents_for_GUI_Automation.pdf">Mobile-Agent-v3: Foundamental Agents for GUI Automation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tongyi Lab, Alibaba Group</span></p>
<p>该论文提出了GUI-Owl，一种基于Qwen2.5-VL的多模态GUI自动化基础模型，通过大规模交互数据后训练，统一实现感知、定位、推理、规划和动作执行，并以此为核心构建了多智能体框架Mobile-Agent-v3，实现跨平台（桌面与移动）环境下的GUI自动化任务。论文创新包括自进化轨迹数据生产、高质量环境基础设施、多样化能力构建及可扩展强化学习训练，并在十项主流基准上达到了开源新SOTA，显著优于现有开源和专有模型。结论是GUI-Owl及Mobile-Agent-v3具备强泛化、适应性和多智能体协作能力，推动了GUI自动化智能体的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Mobile-Agent-v3_Foundamental_Agents_for_GUI_Automation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="54-Intern-S1-A-Scientific-Multimodal-Foundation-Model"><a href="#54-Intern-S1-A-Scientific-Multimodal-Foundation-Model" class="headerlink" title="54. Intern-S1: A Scientific Multimodal Foundation Model"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Intern-S1__A_Scientific_Multimodal_Foundation_Model.pdf">Intern-S1: A Scientific Multimodal Foundation Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai AI Laboratory</span></p>
<p>Intern-S1提出了一种专为科学领域设计的多模态Mixture-of-Experts大模型（28B激活参数，241B总参数），通过高比例科学数据（2.5T tokens）持续预训练、创新的多模态结构（视觉编码器、动态分词器、时序信号编码器）、以及Mixture-of-Rewards框架实现千任务并行在线强化学习，极大提升了科学领域的专业推理能力。实验表明，Intern-S1在科学专业任务（如分子合成、反应预测、晶体稳定性判断）上超越了开源和闭源SOTA模型，并在通用推理任务上保持一流水平，实现了科学AI基础模型的重大突破。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Intern-S1_A_Scientific_Multimodal_Foundation_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-Language-Guided-Tuning-Enhancing-Numeric-Optimization-with-Textual-Feedback"><a href="#55-Language-Guided-Tuning-Enhancing-Numeric-Optimization-with-Textual-Feedback" class="headerlink" title="55. Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Language-Guided_Tuning__Enhancing_Numeric_Optimization_with_Textual_Feedback.pdf">Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>该论文提出了Language-Guided Tuning (LGT) 框架，利用多智能体大语言模型通过自然语言推理来优化模型架构、特征工程、训练策略和超参数，采用文本梯度与数值梯度结合实现多维度自适应优化。实验结果显示，LGT在六个数据集上的表现显著优于传统方法，提高了准确率并降低了误差，同时优化过程具备高可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Language-Guided_Tuning_Enhancing_Numeric_Optimization_with_Textual_Feedback.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-Dissecting-Tool-Integrated-Reasoning-An-Empirical-Study-and-Analysis"><a href="#56-Dissecting-Tool-Integrated-Reasoning-An-Empirical-Study-and-Analysis" class="headerlink" title="56. Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Dissecting_Tool-Integrated_Reasoning__An_Empirical_Study_and_Analysis.pdf">Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Artificial Intelligence Laboratory</span></p>
<p>该论文提出了REASONZOO基准，系统性地评估工具集成推理（TIR）对大语言模型在九类复杂推理任务上的泛化和效率提升，并创新性引入PAC和AUC-PCC两项推理效率指标。实验结果显示，TIR方法（如PoT、MT-TIR、TIT）能广泛提升推理准确率和效率，显著减少冗余计算与“过度思考”现象，推动LLM在复杂推理领域的表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Dissecting_Tool-Integrated_Reasoning_An_Empirical_Study_and_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-EcomMMMU-Strategic-Utilization-of-Visuals-for-Robust-Multimodal-E-Commerce-Models"><a href="#57-EcomMMMU-Strategic-Utilization-of-Visuals-for-Robust-Multimodal-E-Commerce-Models" class="headerlink" title="57. EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/EcomMMMU__Strategic_Utilization_of_Visuals_for_Robust_Multimodal_E-Commerce_Models.pdf">EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Ohio State University</span></p>
<p>本文提出EcomMMMU数据集，包含406,190个样本和8,989,510张图片，专为电商多模态多任务理解设计，并构建视觉显著子集（VSS）系统性评估多模态大语言模型（MLLMs）对视觉内容的有效利用。分析发现，产品图片并非始终提升任务表现，有时反而带来干扰。为此，作者提出SUMEI方法，通过视觉效用预测，筛选“有用”图片用于下游任务，大幅提升多模态模型电商应用表现和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/EcomMMMU_Strategic_Utilization_of_Visuals_for_Robust_Multimodal_E-Commerce_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-StreamMem-Query-Agnostic-KV-Cache-Memory-for-Streaming-Video-Understanding"><a href="#58-StreamMem-Query-Agnostic-KV-Cache-Memory-for-Streaming-Video-Understanding" class="headerlink" title="58. StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/StreamMem__Query-Agnostic_KV_Cache_Memory_for_Streaming_Video_Understanding.pdf">StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Meta AI</span></p>
<p>StreamMem提出了一种针对流式视频理解的训练无关、查询无关KV缓存压缩机制。其通过利用视觉token与通用查询token之间的注意力分数进行关键内容保留，并结合帧过滤与加权合并，持续保持固定KV缓存，实现高效、准确的视频问答，在多个长视频理解和流式问答基准上取得了领先性能。结论指出，StreamMem无需微调即可在多种MLLM模型与内存受限环境下高效工作，显著提升了流式视频理解能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/StreamMem_Query-Agnostic_KV_Cache_Memory_for_Streaming_Video_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning"><a href="#59-End-to-End-Agentic-RAG-System-Training-for-Traceable-Diagnostic-Reasoning" class="headerlink" title="59. End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/End-to-End_Agentic_RAG_System_Training_for_Traceable_Diagnostic_Reasoning.pdf">End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出Deep-DxSearch，一种结合大型医学检索语料库和强化学习的端到端Agentic RAG系统，针对医疗诊断任务优化LLM的检索-推理流程。方法包括构建多源医学知识库、设计五步动作空间并用软奖励RL进行联合训练，实现检索与推理的协同优化。实验结果显示，该系统在多中心数据集上显著提升诊断准确率，超越GPT-4o、DeepSeek-R1等强基线，具备更强泛化性和解释性。结论：Deep-DxSearch通过端到端RL训练，将医学外部知识与推理深度融合，显著提高诊断准确性和可靠性，在常见及罕见疾病场景均优于现有方法，为安全关键医疗领域提供了更可追溯的诊断支持。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/End-to-End_Agentic_RAG_System_Training_for_Traceable_Diagnostic_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-Measuring-the-environmental-impact-of-delivering-AI-at-Google-Scale"><a href="#60-Measuring-the-environmental-impact-of-delivering-AI-at-Google-Scale" class="headerlink" title="60. Measuring the environmental impact of delivering AI at Google Scale"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Measuring_the_environmental_impact_of_delivering_AI_at_Google_Scale.pdf">Measuring the environmental impact of delivering AI at Google Scale</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Google, Mountain View, CA, USA</span></p>
<p>本论文提出并实施了一个全面的方法学，首次在大规模生产环境中测量AI推理服务的能耗、碳排放和用水，包括AI加速器、主机系统、闲置机器及数据中心能耗等全栈指标。通过对Google Gemini AI助手的详细监测，结果显示单次文本推理的能耗远低于以往公开估算，并且通过软硬件协同优化，Gemini产品在一年内实现了能耗33倍和碳排放44倍的显著降低。结论表明，全面、标准化的环境影响测量对于推动AI服务全栈效率提升至关重要。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Measuring_the_environmental_impact_of_delivering_AI_at_Google_Scale.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-Communication-Efficient-LLM-Pre-training-with-SparseLoCo"><a href="#61-Communication-Efficient-LLM-Pre-training-with-SparseLoCo" class="headerlink" title="61. Communication Efficient LLM Pre-training with SparseLoCo"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Communication_Efficient_LLM_Pre-training_with_SparseLoCo.pdf">Communication Efficient LLM Pre-training with SparseLoCo</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Templar AI</span></p>
<p>本文提出SparseLoCo算法，通过将TOP-k稀疏化和量化结合于分布式大语言模型（LLM）预训练，实现极高压缩比（1–3%稀疏度与2位量化），显著降低跨数据中心的通信成本。该方法利用局部误差反馈近似全局动量，并通过稀疏聚合提升模型性能。实验表明，在多种通信受限环境下，SparseLoCo在通信效率和最终性能上均优于现有方法（如DiLoCo和DeMo），并处于损失与通信量的帕累托前沿。结论：SparseLoCo有效减少通信开销且性能优于主流低通信量训练方法，适用于全球分布式LLM大规模预训练。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Communication_Efficient_LLM_Pre-training_with_SparseLoCo.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-Benchmarking-Computer-Science-Survey-Generation"><a href="#62-Benchmarking-Computer-Science-Survey-Generation" class="headerlink" title="62. Benchmarking Computer Science Survey Generation"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Benchmarking_Computer_Science_Survey_Generation.pdf">Benchmarking Computer Science Survey Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了SurGE——一个面向计算机科学领域的科学综述自动生成基准，包括高质量专家综述、百万级学术论文检索库以及自动化多维评估框架。通过对多种大语言模型驱动的检索-生成方法进行系统评测，发现即使先进自反框架也难以生成高质量综述，凸显任务复杂性和未来研究需求。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Benchmarking_Computer_Science_Survey_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-Trained-Miniatures-Low-cost-High-Efficacy-SLMs-for-Sales-Marketing"><a href="#63-Trained-Miniatures-Low-cost-High-Efficacy-SLMs-for-Sales-Marketing" class="headerlink" title="63. Trained Miniatures: Low cost, High Efficacy SLMs for Sales &amp; Marketing"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Trained_Miniatures__Low_cost,_High_Efficacy_SLMs_for_Sales_&_Marketing.pdf">Trained Miniatures: Low cost, High Efficacy SLMs for Sales &amp; Marketing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">SuperAGI Research</span></p>
<p>本文提出“Trained Miniatures”方法，采用大型语言模型（LLM）生成领域特定数据并经人工审核后，利用全参数微调或LoRA等高效参数微调方法，训练小型语言模型（SLM）以满足销售与市场营销场景的高效低成本需求。实验结果表明，4B–12B参数规模的开源SLM在邮件营销等关键业务指标上可接近LLM表现，LoRA微调在大幅降低算力和部署成本的同时，性能损失极小，适合大规模和资源敏感应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Trained_Miniatures_Low_cost_High_Efficacy_SLMs_for_Sales_%26_Marketing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-Think-in-Blocks-Adaptive-Reasoning-from-Direct-Response-to-Deep-Reasoning"><a href="#64-Think-in-Blocks-Adaptive-Reasoning-from-Direct-Response-to-Deep-Reasoning" class="headerlink" title="64. Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Think_in_Blocks__Adaptive_Reasoning_from_Direct_Response_to_Deep_Reasoning.pdf">Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了Think in Blocks框架，通过将推理过程划分为可调节的块，实现了大语言模型（LLM）对于不同任务复杂度的自适应推理深度。方法包括三阶段训练：监督微调、Direct Preference Optimization（DPO）和强化学习（RL），使模型能先预测所需推理块数，再按块生成推理内容。结论显示，该方法在减少平均答案长度25.1%的同时，几乎不降低准确率，提升了模型的推理效率和灵活性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Think_in_Blocks_Adaptive_Reasoning_from_Direct_Response_to_Deep_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-Subjective-Behaviors-and-Preferences-in-LLM-Language-of-Browsing"><a href="#65-Subjective-Behaviors-and-Preferences-in-LLM-Language-of-Browsing" class="headerlink" title="65. Subjective Behaviors and Preferences in LLM: Language of Browsing"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Subjective_Behaviors_and_Preferences_in_LLM__Language_of_Browsing.pdf">Subjective Behaviors and Preferences in LLM: Language of Browsing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Adobe Research</span></p>
<p>论文提出了HeTLM（Heterogeneity aware Training of Language Model）方法，通过将用户的网页浏览行为序列进行嵌入编码和聚类，采用Actor-Critic框架对小型语言模型进行集群细化训练，使每个集群模型更好地捕捉用户的主观偏好和行为模式。实验结果显示，采用HeTLM后的小型LM在页面生成和结果预测任务上整体优于更大单一LM，同时降低了用户级性能方差，提高了个性化对齐。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Subjective_Behaviors_and_Preferences_in_LLM_Language_of_Browsing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-Reliable-Unlearning-Harmful-Information-in-LLMs-with-Metamorphosis-Representation-Projection"><a href="#66-Reliable-Unlearning-Harmful-Information-in-LLMs-with-Metamorphosis-Representation-Projection" class="headerlink" title="66. Reliable Unlearning Harmful Information in LLMs with Metamorphosis Representation Projection"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Reliable_Unlearning_Harmful_Information_in_LLMs_with_Metamorphosis_Representation_Projection.pdf">Reliable Unlearning Harmful Information in LLMs with Metamorphosis Representation Projection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出了一种针对大型语言模型（LLMs）持续性有害信息“遗忘”的新方法——Metamorphosis Representation Projection（MRP）。该方法通过在隐藏状态空间中应用不可逆的投影矩阵，将不良信息从特定网络层的表示中正交去除，同时最大程度保留有用知识，有效防止遗忘信息的恢复并支持多轮连续遗忘。实验结果显示，MRP在连续遗忘和抵抗再学习攻击方面大幅优于现有方法，能够在仅更新极少参数的情况下实现高效、安全的知识移除而不损害模型通用性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Reliable_Unlearning_Harmful_Information_in_LLMs_with_Metamorphosis_Representation_Projection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-GraSP-A-Unified-Graph-Based-Framework-for-Scalable-Generation-Quality-Tagging-and-Management-of-Synthetic-Data-for-SFT-and-DPO"><a href="#67-GraSP-A-Unified-Graph-Based-Framework-for-Scalable-Generation-Quality-Tagging-and-Management-of-Synthetic-Data-for-SFT-and-DPO" class="headerlink" title="67. GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/GraSP__A_Unified_Graph-Based_Framework_for_Scalable_Generation,_Quality_Tagging,_and_Management_of_S.pdf">GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ServiceNow Inc.</span></p>
<p>GraSP提出了一个基于图结构的可扩展合成数据生成框架，采用低代码YAML配置和模块化DAG编排，支持复杂多分支、多模态输入、代理执行等流程，自动实现高质量数据标注与过滤。实验结果显示GraSP极大提升了大语言模型训练中数据生成的速度和质量，促进了透明、高效的数据管理。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/GraSP_A_Unified_Graph-Based_Framework_for_Scalable_Generation_Quality_Tagging_and_Management_of_Synthetic_Data_for_SFT_and_DPO.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-When-Audio-and-Text-Disagree-Benchmarking-Text-Bias-in-Large-Audio-Language-Models-under-Cross-Modal-Inconsistencies"><a href="#68-When-Audio-and-Text-Disagree-Benchmarking-Text-Bias-in-Large-Audio-Language-Models-under-Cross-Modal-Inconsistencies" class="headerlink" title="68. When Audio and Text Disagree: Benchmarking Text Bias in Large Audio-Language Models under Cross-Modal Inconsistencies"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/When_Audio_and_Text_Disagree__Revealing_Text_Bias_in_Large_Audio-Language_Models.pdf">When Audio and Text Disagree: Benchmarking Text Bias in Large Audio-Language Models under Cross-Modal Inconsistencies</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Singapore</span></p>
<p>该论文提出了MCR-BENCH基准，用于评估大型音频-语言模型（LALMs）在音频与文本信息不一致时的表现，涵盖音频问答、语音情感识别和人声分类等任务。实验发现当前主流LALMs普遍对文本输入有显著偏好，导致在音频为主的任务中性能大幅下降，仅有限的微调和提示工程能部分缓解该问题。结论：现有模型在多模态冲突下可靠性不足，亟需新的训练范式与融合机制以提升音频-语言模型的稳健性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/When_Audio_and_Text_Disagree_Benchmarking_Text_Bias_in_Large_Audio-Language_Models_under_Cross-Modal_Inconsistencies.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-Unveiling-Trust-in-Multimodal-Large-Language-Models-Evaluation-Analysis-and-Mitigation"><a href="#69-Unveiling-Trust-in-Multimodal-Large-Language-Models-Evaluation-Analysis-and-Mitigation" class="headerlink" title="69. Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Unveiling_Trust_in_Multimodal_Large_Language_Models__Evaluation,_Analysis,_and_Mitigation.pdf">Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了MultiTrust-X，一个三维度全面评估多模态大语言模型（MLLMs）可信度的基准，包括真实、鲁棒性、安全、公平和隐私五大方面，并涵盖多模态和跨模态风险，以及数据、架构、训练和推理等多种缓解方法。实验表明，当前开源MLLMs在可信度和通用能力上仍存在显著差距，且多模态训练和推理会放大风险。作者提出基于链式推理的安全对齐方法RESA，有效提升模型安全性并保持性能，在MultiTrust-X上取得开源模型最佳表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Unveiling_Trust_in_Multimodal_Large_Language_Models_Evaluation_Analysis_and_Mitigation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-DeepThink3D-Enhancing-Large-Language-Models-with-Programmatic-Reasoning-in-Complex-3D-Situated-Reasoning-Tasks"><a href="#70-DeepThink3D-Enhancing-Large-Language-Models-with-Programmatic-Reasoning-in-Complex-3D-Situated-Reasoning-Tasks" class="headerlink" title="70. DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/DeepThink3D__Enhancing_Large_Language_Models_with_Programmatic_Reasoning_in_Complex_3D_Situated_Reas.pdf">DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本文提出DeepThink3D框架，利用大语言模型（LLM）结合程序化API调用，采用两阶段优化策略（监督微调SFT与直接偏好优化DPO）提升LLM在复杂3D场景中的推理和执行能力。方法通过自动生成多步复杂问题扩展训练集，显著增强模型在3D环境下的结构化推理能力和代码可执行性。实验表明DeepThink3D在SQA3D基准上准确率领先，提升了3D情境推理的泛化性、可解释性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/DeepThink3D_Enhancing_Large_Language_Models_with_Programmatic_Reasoning_in_Complex_3D_Situated_Reasoning_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-Super-additive-Cooperation-in-Language-Model-Agents"><a href="#71-Super-additive-Cooperation-in-Language-Model-Agents" class="headerlink" title="71. Super-additive Cooperation in Language Model Agents"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Super-additive_Cooperation_in_Language_Model_Agents.pdf">Super-additive Cooperation in Language Model Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Southern Denmark</span></p>
<p>本文提出了一种基于大语言模型的多智能体虚拟锦标赛框架，通过迭代囚徒困境（IPD）模拟重复互动与组间竞争，验证了“超加性合作”理论。实验结果显示，在同时存在组内重复互动和组间竞争时，Qwen3与Phi4模型的整体及首次合作率显著高于单独机制，促进了更强的合作倾向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Super-additive_Cooperation_in_Language_Model_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-From-Bits-to-Boardrooms-A-Cutting-Edge-Multi-Agent-LLM-Framework-for-Business-Excellence"><a href="#72-From-Bits-to-Boardrooms-A-Cutting-Edge-Multi-Agent-LLM-Framework-for-Business-Excellence" class="headerlink" title="72. From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/From_Bits_to_Boardrooms__A_Cutting-Edge_Multi-Agent_LLM_Framework_for_Business_Excellence.pdf">From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">College of Artificial Intelligence and Automation, Hohai University</span></p>
<p>该论文提出BusiAgent，一种面向企业决策的多智能体大语言模型（LLM）框架，融合扩展连续时间马尔可夫决策过程（CTMDP）建模、基于熵的头脑风暴、分层Stackelberg博弈、工具集成和上下文Thompson采样，实现多角色（如CEO、CTO、CFO等）高效协作、任务分配与流程优化。实验证明，BusiAgent在跨行业复杂业务场景下显著提升了问题分析、任务分配和方案开发的质量与用户满意度，能有效整合细粒度运营洞见与宏观战略，实现AI驱动的企业决策智能化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/From_Bits_to_Boardrooms_A_Cutting-Edge_Multi-Agent_LLM_Framework_for_Business_Excellence.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-Test-time-Corpus-Feedback-From-Retrieval-to-RAG"><a href="#73-Test-time-Corpus-Feedback-From-Retrieval-to-RAG" class="headerlink" title="73. Test-time Corpus Feedback: From Retrieval to RAG"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Test-time_Corpus_Feedback__From_Retrieval_to_RAG.pdf">Test-time Corpus Feedback: From Retrieval to RAG</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">L3S Research Center</span></p>
<p>本文系统综述了检索增强生成（RAG）系统中测试时语料库反馈机制，聚焦于如何在RAG管道的查询、检索和生成阶段引入动态反馈信号以提升整体性能。总结了基于大语言模型的查询重写、检索自适应、生成时反馈等关键技术流程，并提出了开放性挑战与未来方向。结论认为反馈机制能显著提高复杂任务的检索和推理能力，推动RAG系统向更高效、动态和智能化发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Test-time_Corpus_Feedback_From_Retrieval_to_RAG.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-Foundational-Design-Principles-and-Patterns-for-Building-Robust-and-Adaptive-GenAI-Native-Systems"><a href="#74-Foundational-Design-Principles-and-Patterns-for-Building-Robust-and-Adaptive-GenAI-Native-Systems" class="headerlink" title="74. Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Foundational_Design_Principles_and_Patterns_for_Building_Robust_and_Adaptive_GenAI-Native_Systems.pdf">Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nokia Bell Labs</span></p>
<p>本论文提出了构建健壮且自适应的GenAI-native系统的基础设计原则和架构模式，围绕可靠性、卓越性、可进化性、自主性和保障五大支柱，结合传统软件工程方法与生成式AI的认知能力，提出了GenAI-native cell、organic substrate、programmable router等关键设计模式及软件栈蓝图。结论认为，融合传统与GenAI范式将推动未来软件系统更具弹性、自主进化与高效，尽管仍需进一步实证研究和技术发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Foundational_Design_Principles_and_Patterns_for_Building_Robust_and_Adaptive_GenAI-Native_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-Exploiting-Vocabulary-Frequency-Imbalance-in-Language-Model-Pre-training"><a href="#75-Exploiting-Vocabulary-Frequency-Imbalance-in-Language-Model-Pre-training" class="headerlink" title="75. Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Exploiting_Vocabulary_Frequency_Imbalance_in_Language_Model_Pre-training.pdf">Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KAIST</span></p>
<p>本论文系统研究了在语言模型预训练中扩展词表规模（24K至196K）对模型损失与表现的影响，通过Kolmogorov复杂度、分词效率和词频分布等分析，发现更大词表主要通过加剧高频词的频率不均衡、降低对高频词的不确定性，从而减少整体交叉熵损失和提升下游任务准确率。结论表明，模型损失和下游表现的提升主要来自于高频词的不确定性降低，进一步扩展词表对稀有词收益有限。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Exploiting_Vocabulary_Frequency_Imbalance_in_Language_Model_Pre-training.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-An-Empirical-Study-on-How-Video-LLMs-Answer-Video-Questions"><a href="#76-An-Empirical-Study-on-How-Video-LLMs-Answer-Video-Questions" class="headerlink" title="76. An Empirical Study on How Video-LLMs Answer Video Questions"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/An_Empirical_Study_on_How_Video-LLMs_Answer_Video_Questions.pdf">An Empirical Study on How Video-LLMs Answer Video Questions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University</span></p>
<p>本文系统性分析了 Video-LLMs 在视频问答任务中的内部工作机制，提出并应用三类注意力剔除（时序、空间、语言到视频）方法，分别在全局和细粒度层面剖析了不同注意力类型和模型层对视频理解的影响。结论显示，视频信息主要在早期层被提取，高层负责推理，且模型对语言引导的视频检索依赖远大于时空自注意力，提出的精简策略可有效降低计算量且几乎不损失性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/An_Empirical_Study_on_How_Video-LLMs_Answer_Video_Questions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-RETAIL-Towards-Real-world-Travel-Planning-for-Large-Language-Models"><a href="#77-RETAIL-Towards-Real-world-Travel-Planning-for-Large-Language-Models" class="headerlink" title="77. RETAIL: Towards Real-world Travel Planning for Large Language Models"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/RETAIL__Towards_Real-world_Travel_Planning_for_Large_Language_Models.pdf">RETAIL: Towards Real-world Travel Planning for Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>该论文提出了RETAIL数据集，专注于真实场景下的旅游规划，覆盖隐式与显式用户需求、环境感知与计划修订，并包含丰富POI信息，提升规划可行性。基于此，作者提出了TGMA主题引导多智能体框架，通过决策支持和多智能体协作体系，有效提升了大模型在真实旅游规划中的表现，实验结果显著优于现有基线。结论表明，TGMA框架和RETAIL数据集为大模型在复杂实际规划任务中提供了新的思路和资源。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/RETAIL_Towards_Real-world_Travel_Planning_for_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-IPIGuard-A-Novel-Tool-Dependency-Graph-Based-Defense-Against-Indirect-Prompt-Injection-in-LLM-Agents"><a href="#78-IPIGuard-A-Novel-Tool-Dependency-Graph-Based-Defense-Against-Indirect-Prompt-Injection-in-LLM-Agents" class="headerlink" title="78. IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/IPIGuard__A_Novel_Tool_Dependency_Graph-Based_Defense_Against_Indirect_Prompt_Injection_in_LLM_Agent.pdf">IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出了IPIGUARD，一种以工具依赖图（Tool Dependency Graph, TDG）为核心的任务执行范式，通过将LLM代理的动作规划与外部数据交互解耦，显式约束工具调用行为，有效防御间接提示注入（IPI）攻击。方法包括TDG构建、参数估算、节点扩展和虚假工具调用机制，实现了更强的安全性与较高的任务完成率。实验表明，IPIGUARD在多个主流LLM和多种攻击场景下均显著降低攻击成功率且保持高效用。结论：IPIGUARD显著提升了LLM代理系统对IPI攻击的鲁棒性，兼顾安全性与实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/IPIGuard_A_Novel_Tool_Dependency_Graph-Based_Defense_Against_Indirect_Prompt_Injection_in_LLM_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-EXBIGBANG-A-Dynamic-Approach-for-Explainable-Persona-Classification-Through-Contextualized-Hybrid-Transformer-Analysis"><a href="#79-EXBIGBANG-A-Dynamic-Approach-for-Explainable-Persona-Classification-Through-Contextualized-Hybrid-Transformer-Analysis" class="headerlink" title="79. EXBIGBANG: A Dynamic Approach for Explainable Persona Classification Through Contextualized Hybrid Transformer Analysis"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/ExBigBang__A_Dynamic_Approach_for_Explainable_Persona_Classification_through_Contextualized_Hybrid_T.pdf">EXBIGBANG: A Dynamic Approach for Explainable Persona Classification Through Contextualized Hybrid Transformer Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Macquarie University</span></p>
<p>本文提出了ExBigBang框架，利用混合文本-表格Transformer（BERT为核心）结合领域知识、元数据和用户画像，实现动态、可解释的用户Persona分类。通过循环反馈的用户画像与分类流程，模型能反映用户行为变化，并借助XAI（如SHAP）揭示特征贡献。结果表明，该方法在社交媒体数据上性能显著优于传统与深度学习基线，且对特征影响机制进行了可解释性分析。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/EXBIGBANG_A_Dynamic_Approach_for_Explainable_Persona_Classification_Through_Contextualized_Hybrid_Transformer_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-DiagECG-An-LLM-Driven-Framework-for-Diagnostic-Reasoning-via-Discretized-ECG-Tokenization"><a href="#80-DiagECG-An-LLM-Driven-Framework-for-Diagnostic-Reasoning-via-Discretized-ECG-Tokenization" class="headerlink" title="80. DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/DiagECG__An_LLM-Driven_Framework_for_Diagnostic_Reasoning_via_Discretized_ECG_Tokenization.pdf">DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences</span></p>
<p>DiagECG提出了一种将12导联ECG信号离散化为符号token，并将其集成到大语言模型（LLM）词汇表的方法，通过lead-wise编码器和FSQ量化模块，将ECG连续信号转化为可供LLM处理的离散符号，支持统一的时序与文本建模。方法包括自动回归式ECG预训练和轻量化指令微调（LoRA），实现了强大的开放式问答和诊断报告生成能力。实验结果显示，DiagECG在多项ECG问答和报告生成任务上达到SOTA性能，具备良好的跨数据集泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/DiagECG_An_LLM-Driven_Framework_for_Diagnostic_Reasoning_via_Discretized_ECG_Tokenization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-Coarse-to-Fine-Grounded-Memory-for-LLM-Agent-Planning"><a href="#81-Coarse-to-Fine-Grounded-Memory-for-LLM-Agent-Planning" class="headerlink" title="81. Coarse-to-Fine Grounded Memory for LLM Agent Planning"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Coarse-to-Fine_Grounded_Memory_for_LLM_Agent_Planning.pdf">Coarse-to-Fine Grounded Memory for LLM Agent Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institution of Automation, Chinese Academy of Sciences</span></p>
<p>该论文提出了Coarse-to-Fine Grounded Memory (CFGM) 框架，通过引入粗到细粒度的记忆机制，利用LLM的内在知识对环境信息、经验轨迹和当前状态进行分层知识萃取和规划辅助。在AlfWorld、WebShop和ScienceWorld等复杂交互环境中，CFGM显著提升了LLM Agent的任务规划能力和泛化性，实验结果显示其在多个基准上均优于现有增强记忆的LLM agent系统。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Coarse-to-Fine_Grounded_Memory_for_LLM_Agent_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-Deep-Think-with-Confidence"><a href="#82-Deep-Think-with-Confidence" class="headerlink" title="82. Deep Think with Confidence"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Deep_Think_with_Confidence.pdf">Deep Think with Confidence</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Meta AI</span></p>
<p>该论文提出了Deep Think with Confidence (DeepConf)，一种通过利用大语言模型内部置信度信号，在推理生成过程中动态过滤低质量推理路径的方法。方法无需额外训练或超参数调整，能在离线和在线场景下通过局部置信度度量实现早停和置信度加权投票，有效减少计算开销并提升推理准确率。实验表明，DeepConf在AIME 2025等高难度推理基准上最高可达99.9%准确率，并显著减少84.7%的token生成。结论是DeepConf能以更高效方式提升LLM推理性能，并易于集成现有服务框架。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Deep_Think_with_Confidence.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-SPARK-Query-Aware-Unstructured-Sparsity-with-Recoverable-KV-Cache-Channel-Pruning"><a href="#83-SPARK-Query-Aware-Unstructured-Sparsity-with-Recoverable-KV-Cache-Channel-Pruning" class="headerlink" title="83. SPARK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/SparK__Query-Aware_Unstructured_Sparsity_with_Recoverable_KV_Cache_Channel_Pruning.pdf">SPARK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Advanced Micro Devices (china) Co., Ltd.</span></p>
<p>该论文提出了SPARK方法，通过无需训练的查询感知无结构稀疏性，对LLM推理中的KV缓存进行细粒度通道剪枝，并引入统计恢复机制在注意力计算时动态重建被剪枝的通道。实验表明，SPARK在高压缩比下能大幅降低KV缓存内存消耗，同时保持模型性能损失低于5%，优于现有结构化通道剪枝方法，兼容主流的缓存压缩及量化技术。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/SPARK_Query-Aware_Unstructured_Sparsity_with_Recoverable_KV_Cache_Channel_Pruning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-R-ConstraintBench-Evaluating-LLMs-on-NP-Complete-Scheduling"><a href="#84-R-ConstraintBench-Evaluating-LLMs-on-NP-Complete-Scheduling" class="headerlink" title="84. R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/R-ConstraintBench__Evaluating_LLMs_on_NP-Complete_Scheduling.pdf">R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Labelbox</span></p>
<p>本文提出R-ConstraintBench，一个可扩展的基准框架，系统评估大语言模型（LLM）在NP完全资源约束项目调度问题（RCPSP）上的推理能力。方法包括多阶段有向无环图（DAG）生成、逐步增加约束（资源停机、时间窗口、互斥），并在现实数据中心迁移场景中测试多种LLM，通过可行性和错误分析发现：模型在仅有前序约束时表现接近满分，但当多种约束互相作用时，可行性骤降，主要瓶颈在约束交互而非图的深度；且在合成数据上表现优异的模型在实际领域迁移时性能会明显下降，显示出有限的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/R-ConstraintBench_Evaluating_LLMs_on_NP-Complete_Scheduling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-SemToken-Semantic-Aware-Tokenization-for-Efficient-Long-Context-Language-Modeling"><a href="#85-SemToken-Semantic-Aware-Tokenization-for-Efficient-Long-Context-Language-Modeling" class="headerlink" title="85. SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/SemToken__Semantic-Aware_Tokenization_for_Efficient_Long-Context_Language_Modeling.pdf">SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Yale University</span></p>
<p>SemToken提出了一种基于语义的动态分词框架，通过语义嵌入、局部聚类以及自适应粒度分配，减少冗余token并提升长上下文大语言模型的推理效率。实验表明，该方法在保持甚至提升下游任务表现的同时，实现了高达2.4倍的token压缩和1.9倍推理加速，兼容现有加速方法且无需模型重训练。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/SemToken_Semantic-Aware_Tokenization_for_Efficient_Long-Context_Language_Modeling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-Multiple-Memory-Systems-for-Enhancing-the-Long-term-Memory-of-Agent"><a href="#86-Multiple-Memory-Systems-for-Enhancing-the-Long-term-Memory-of-Agent" class="headerlink" title="86. Multiple Memory Systems for Enhancing the Long-term Memory of Agent"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Multiple_Memory_Systems_for_Enhancing_the_Long-term_Memory_of_Agent.pdf">Multiple Memory Systems for Enhancing the Long-term Memory of Agent</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tianjin University</span></p>
<p>该论文提出了多重记忆系统（MMS），结合认知心理学理论，将短期记忆内容处理为高质量的长期记忆片段，包括关键词、认知视角、情节记忆和语义记忆，并构建了检索记忆单元与情境记忆单元。实验结果显示，MMS在LoCoMo数据集的多种任务上显著提升了长程记忆的召回率和生成质量，同时具有更低的资源消耗和更强的鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Multiple_Memory_Systems_for_Enhancing_the_Long-term_Memory_of_Agent.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-M-𝐿𝐿𝑀3REC-A-Motivation-Aware-User-Item-Interaction-Framework-for-Enhancing-Recommendation-Accuracy-with-LLMs"><a href="#87-M-𝐿𝐿𝑀3REC-A-Motivation-Aware-User-Item-Interaction-Framework-for-Enhancing-Recommendation-Accuracy-with-LLMs" class="headerlink" title="87. M-𝐿𝐿𝑀3REC: A Motivation-Aware User-Item Interaction Framework for Enhancing Recommendation Accuracy with LLMs"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/M-$LLM^3$REC__A_Motivation-Aware_User-Item_Interaction_Framework_for_Enhancing_Recommendation_Accura.pdf">M-𝐿𝐿𝑀3REC: A Motivation-Aware User-Item Interaction Framework for Enhancing Recommendation Accuracy with LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Sydney</span></p>
<p>该论文提出了M-𝐿𝐿𝑀3REC推荐框架，利用大语言模型（LLM）通过三个模块（MOPE用户动机提取、MOTE商品特征编码、MAR动机-特征匹配）深度建模用户动机与商品语义，实现冷启动和数据稀疏场景下的高准确个性化推荐。实验表明，该方法在多个领域数据集上显著优于现有主流模型，提升了推荐的泛化能力和解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/M-%F0%9D%90%BF%F0%9D%90%BF%F0%9D%91%803REC_A_Motivation-Aware_User-Item_Interaction_Framework_for_Enhancing_Recommendation_Accuracy_with_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-Conflict-Aware-Soft-Prompting-for-Retrieval-Augmented-Generation"><a href="#88-Conflict-Aware-Soft-Prompting-for-Retrieval-Augmented-Generation" class="headerlink" title="88. Conflict-Aware Soft Prompting for Retrieval-Augmented Generation"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Conflict-Aware_Soft_Prompting_for_Retrieval-Augmented_Generation.pdf">Conflict-Aware Soft Prompting for Retrieval-Augmented Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sungkyunkwan University</span></p>
<p>该论文提出了CARE方法，通过引入context assessor模块和软提示（soft prompting）来解决检索增强生成（RAG）中外部检索内容与LLM内部知识冲突问题。方法包括重构预训练和冲突感知微调，以动态生成反映可靠性的记忆嵌入，有效引导LLM在外部或内部知识间做出合理推理。实验表明，CARE在问答和事实核查任务上提升了平均性能5%，并能在不微调基础LLM的前提下保持其泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Conflict-Aware_Soft_Prompting_for_Retrieval-Augmented_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-LLM4Sweat-A-Trustworthy-Large-Language-Model-for-Hyperhidrosis-Support"><a href="#89-LLM4Sweat-A-Trustworthy-Large-Language-Model-for-Hyperhidrosis-Support" class="headerlink" title="89. LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/LLM4Sweat__A_Trustworthy_Large_Language_Model_for_Hyperhidrosis_Support.pdf">LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Purdue University</span></p>
<p>该论文提出LLM4Sweat框架，通过三阶段流程（数据增强、领域微调、专家闭环评估）将开源大模型适应于多任务（诊断、治疗建议、心理支持）罕见病超汗症，利用前沿LLM生成医学合理的合成问答数据，专家反馈进一步提升模型表现。实验结果显示LLM4Sweat在诊断和治疗任务上显著优于基线模型，兼具准确性和临床可信度，具备在其他罕见病领域扩展的通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/LLM4Sweat_A_Trustworthy_Large_Language_Model_for_Hyperhidrosis_Support.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-SafeLLM-Unlearning-Harmful-Outputs-from-Large-Language-Models-against-Jailbreak-Attacks"><a href="#90-SafeLLM-Unlearning-Harmful-Outputs-from-Large-Language-Models-against-Jailbreak-Attacks" class="headerlink" title="90. SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/SafeLLM__Unlearning_Harmful_Outputs_from_Large_Language_Models_against_Jailbreak_Attacks.pdf">SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Queen’s University</span></p>
<p>本论文提出SafeLLM，一种基于unlearning的防御框架，通过三阶段流程：动态有害输出检测（结合外部分类器与模型自评）、基于FFN激活的有害内容追踪、受约束优化抑制有害行为，实现LLM对有害知识的精准定位与不可逆遗忘，同时保持语言流畅和通用能力。实验表明，SafeLLM在多种主流LLM与越狱攻击基准下显著降低攻击成功率，优于常规SFT与DPO等防御方法，并对未见攻击具备更强鲁棒性，且对通用任务无明显性能损失，展示了token级unlearning在LLM安全中的应用前景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/SafeLLM_Unlearning_Harmful_Outputs_from_Large_Language_Models_against_Jailbreak_Attacks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-PuzzleClone-An-SMT-Powered-Framework-for-Synthesizing-Verifiable-Data"><a href="#91-PuzzleClone-An-SMT-Powered-Framework-for-Synthesizing-Verifiable-Data" class="headerlink" title="91. PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/PuzzleClone__An_SMT-Powered_Framework_for_Synthesizing_Verifiable_Data.pdf">PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HiThink Research</span></p>
<p>本论文提出了PuzzleClone，一个利用可满足性模理论（SMT）自动合成大规模、可验证逻辑与数学推理数据的框架。方法包括将种子谜题编码为结构化逻辑规范，通过变量和约束系统化随机化生成多样变体并用SMT求解器验证答案，最终构建出覆盖不同难度的8.3万+谜题数据集。实验表明，PuzzleClone不仅对主流大语言模型提出挑战，且其数据用于SFT和RL微调后，显著提升了模型在逻辑与数学推理基准上的性能，验证了框架的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/PuzzleClone_An_SMT-Powered_Framework_for_Synthesizing_Verifiable_Data.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-XDR-LVLM-An-Explainable-Vision-Language-Large-Model-for-Diabetic-Retinopathy-Diagnosis"><a href="#92-XDR-LVLM-An-Explainable-Vision-Language-Large-Model-for-Diabetic-Retinopathy-Diagnosis" class="headerlink" title="92. XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/XDR-LVLM__An_Explainable_Vision-Language_Large_Model_for_Diabetic_Retinopathy_Diagnosis.pdf">XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">SANNO University</span></p>
<p>本文提出了一种面向糖尿病视网膜病变（DR）诊断的解释型视觉-语言大模型XDR-LVLM，结合医学视觉编码器、LVLM Core、多任务提示工程和多阶段微调，能够对眼底图像进行精确诊断，并生成包含病变解释的自然语言报告。实验结果在DDR数据集上实现了84.55%的Balanced Accuracy和79.92%的F1分数，报告在流畅性、准确性和临床实用性方面均获得高分，显著提升了模型的可解释性与临床应用价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/XDR-LVLM_An_Explainable_Vision-Language_Large_Model_for_Diabetic_Retinopathy_Diagnosis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-A-Robust-BERT-Based-Deep-Learning-Model-for-Automated-Cancer-Type-Extraction-from-Unstructured-Pathology-Reports"><a href="#93-A-Robust-BERT-Based-Deep-Learning-Model-for-Automated-Cancer-Type-Extraction-from-Unstructured-Pathology-Reports" class="headerlink" title="93. A Robust BERT-Based Deep Learning Model for Automated Cancer Type Extraction from Unstructured Pathology Reports"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/A_Robust_BERT-Based_Deep_Learning_Model_for_Automated_Cancer_Type_Extraction_from_Unstructured_Patho.pdf">A Robust BERT-Based Deep Learning Model for Automated Cancer Type Extraction from Unstructured Pathology Reports</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of New South Wales</span></p>
<p>该论文提出了一种基于RoBERTa的深度学习模型，通过OCR与领域特定微调，实现了从未结构化病理报告中自动提取癌症类型和亚型。结果显示，微调后的RoBERTa模型在准确率和F1-BERT分数方面均显著优于通用RoBERTa和Mistral 7B模型，验证了领域适应与微调在医学文本信息抽取中的必要性和有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/A_Robust_BERT-Based_Deep_Learning_Model_for_Automated_Cancer_Type_Extraction_from_Unstructured_Pathology_Reports.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="94-Discovering-Hidden-Algebraic-Structures-via-Transformers-with-Rank-Aware-Beam-GRPO"><a href="#94-Discovering-Hidden-Algebraic-Structures-via-Transformers-with-Rank-Aware-Beam-GRPO" class="headerlink" title="94. Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Discovering_Hidden_Algebraic_Structures_via_Transformers_with_Rank-Aware_Beam_GRPO.pdf">Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">California Institute of Technology</span></p>
<p>本文提出了一种结合合成数据生成、轻量级Transformer训练、束搜索和Beam Grouped Relative Policy Optimization (BGRPO)强化学习的方法，用于解决多元多项式分解这一NP难题。实验表明该方法在精度和推理计算效率上均有显著提升，且模型在多项式简化任务中部分超过Mathematica，展示了神经网络发现隐藏代数结构的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Discovering_Hidden_Algebraic_Structures_via_Transformers_with_Rank-Aware_Beam_GRPO.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-Tutorial-on-the-Probabilistic-Unification-of-Estimation-Theory-Machine-Learning-and-Generative-AI"><a href="#95-Tutorial-on-the-Probabilistic-Unification-of-Estimation-Theory-Machine-Learning-and-Generative-AI" class="headerlink" title="95. Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Tutorial_on_the_Probabilistic_Unification_of_Estimation_Theory,_Machine_Learning,_and_Generative_AI.pdf">Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Vaasa</span></p>
<p>该论文提出了一个统一的概率框架，将经典估计理论、统计推断与现代机器学习（包括深度学习与大语言模型）连接起来。通过分析最大似然估计、贝叶斯推断和注意力机制等技术在处理不确定性中的作用，论文展示了各种AI方法均源自共同的概率原理，并通过图像分类和语言生成等示例证明了概率视角对理解AI模型的有效性。结论指出：无论是估计隐藏因果、学习输入输出映射还是建模语言结构，均可归于概率推断，未来AI发展应关注数学基础与可扩展学习系统的结合。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Tutorial_on_the_Probabilistic_Unification_of_Estimation_Theory_Machine_Learning_and_Generative_AI.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-Foundation-Models-for-Cross-Domain-EEG-Analysis-Application-A-Survey"><a href="#96-Foundation-Models-for-Cross-Domain-EEG-Analysis-Application-A-Survey" class="headerlink" title="96. Foundation Models for Cross-Domain EEG Analysis Application: A Survey"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Foundation_Models_for_Cross-Domain_EEG_Analysis_Application__A_Survey.pdf">Foundation Models for Cross-Domain EEG Analysis Application: A Survey</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northwestern Polytechnical University</span></p>
<p>本文首次系统性梳理了预训练于大规模非EEG数据的基础模型在EEG分析中的跨领域应用，提出了基于输出模态的分类法，涵盖EEG原生解码、EEG-文本、EEG-视觉、EEG-音频及多模态融合五大类任务。结论指出，基础模型显著提升了EEG解码的泛化能力和跨模态表现，但在可解释性、跨主体泛化和实际部署等方面仍面临挑战，未来需加强神经语义对齐与多模态融合。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Foundation_Models_for_Cross-Domain_EEG_Analysis_Application_A_Survey.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-Effect-Identification-and-Unit-Categorization-in-the-Multi-Score-Regression-Discontinuity-Design-with-Application-to-LED-Manufacturing"><a href="#97-Effect-Identification-and-Unit-Categorization-in-the-Multi-Score-Regression-Discontinuity-Design-with-Application-to-LED-Manufacturing" class="headerlink" title="97. Effect Identification and Unit Categorization in the Multi-Score Regression Discontinuity Design with Application to LED Manufacturing"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Effect_Identification_and_Unit_Categorization_in_the_Multi-Score_Regression_Discontinuity_Design_wit.pdf">Effect Identification and Unit Categorization in the Multi-Score Regression Discontinuity Design with Application to LED Manufacturing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AMS Osram</span></p>
<p>本文提出了一套多评分回归断点设计（MRD）的理论工具，针对多阈值决策环境下单位行为类型（如complier、nevertaker、alwaystaker、defier、indecisive）进行了系统定义与归类，推导了多维切点下complier效应的可识别性条件，并提出通过分解复杂规则、去除非合规单位提升因果效应估计准确率。理论框架在LED制造真实数据和模拟数据上验证，表明该方法能有效降低估计方差，为制造决策优化提供支持。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Effect_Identification_and_Unit_Categorization_in_the_Multi-Score_Regression_Discontinuity_Design_with_Application_to_LED_Manufacturing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-Amortized-In-Context-Mixed-Effect-Transformer-Models-A-Zero-Shot-Approach-for-Pharmacokinetics"><a href="#98-Amortized-In-Context-Mixed-Effect-Transformer-Models-A-Zero-Shot-Approach-for-Pharmacokinetics" class="headerlink" title="98. Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot Approach for Pharmacokinetics"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Amortized_In-Context_Mixed_Effect_Transformer_Models__A_Zero-Shot_Approach_for_Pharmacokinetics.pdf">Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot Approach for Pharmacokinetics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Potsdam</span></p>
<p>本论文提出了AICMET模型，将分层潜变量Transformer与药动学分腔体机理先验结合，通过大规模合成数据预训练，实现基于Transformer的零样本个体预测与群体生成。结果显示AICMET在药物浓度预测和个体变异量化方面优于传统NLME和神经ODE方法，实现更高预测准确率并大幅缩短模型开发周期。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Amortized_In-Context_Mixed_Effect_Transformer_Models_A_Zero-Shot_Approach_for_Pharmacokinetics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-CITE-A-Comprehensive-Benchmark-for-Heterogeneous-Text-Attributed-Graphs-on-Catalytic-Materials"><a href="#99-CITE-A-Comprehensive-Benchmark-for-Heterogeneous-Text-Attributed-Graphs-on-Catalytic-Materials" class="headerlink" title="99. CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on Catalytic Materials"></a>99. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/CITE__A_Comprehensive_Benchmark_for_Heterogeneous_Text-Attributed_Graphs_on_Catalytic_Materials.pdf">CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on Catalytic Materials</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Computer Network Information Center, China Academy of Sciences</span></p>
<p>该论文提出了CITE数据集，这是首个聚焦催化材料领域的异构文本属性图大规模基准，包含四类节点（论文、作者、期刊、关键词）和丰富文本属性，支持复杂关系建模。论文系统比较了同质图模型、异质图模型、LLM和LLM+图模型在节点分类等任务上的表现，发现异质图模型在结构复杂和语义丰富场景下表现更强，LLM相关方法在多标签一致性上存在挑战，并强调高连接节点和丰富语义对图表示学习的重要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/CITE_A_Comprehensive_Benchmark_for_Heterogeneous_Text-Attributed_Graphs_on_Catalytic_Materials.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-EvoFormer-Learning-Dynamic-Graph-Level-Representations-with-Structural-and-Temporal-Bias-Correction"><a href="#100-EvoFormer-Learning-Dynamic-Graph-Level-Representations-with-Structural-and-Temporal-Bias-Correction" class="headerlink" title="100. EvoFormer: Learning Dynamic Graph-Level Representations with Structural and Temporal Bias Correction"></a>100. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/EvoFormer__Learning_Dynamic_Graph-Level_Representations_with_Structural_and_Temporal_Bias_Correction.pdf">EvoFormer: Learning Dynamic Graph-Level Representations with Structural and Temporal Bias Correction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xidian University</span></p>
<p>本文提出EvoFormer，一种结合结构感知Transformer模块和演化敏感时序模块的动态图级表示学习方法。该方法利用结构角色位置编码和图级时序分段，解决了随机游走采样导致的结构访问偏差和突发结构变化感知不足的问题。实验结果表明，EvoFormer在图相似性排名、时序异常检测和时序分割等任务上实现了最新最优性能，有效提升了动态图结构和演化特征的表达能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/EvoFormer_Learning_Dynamic_Graph-Level_Representations_with_Structural_and_Temporal_Bias_Correction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="101-GRASPED-Graph-Anomaly-Detection-using-Autoencoder-with-Spectral-Encoder-and-Decoder"><a href="#101-GRASPED-Graph-Anomaly-Detection-using-Autoencoder-with-Spectral-Encoder-and-Decoder" class="headerlink" title="101. GRASPED: Graph Anomaly Detection using Autoencoder with Spectral Encoder and Decoder"></a>101. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/GRASPED__Graph_Anomaly_Detection_using_Autoencoder_with_Spectral_Encoder_and_Decoder_(Full_Version).pdf">GRASPED: Graph Anomaly Detection using Autoencoder with Spectral Encoder and Decoder</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fraunhofer Institute for Applied and Integrated Security (AISEC)</span></p>
<p>该论文提出了GRASPED，一种基于图自编码器（GAE）的无监督异常检测模型，方法使用图小波卷积构建的多尺度谱域编码器与图反卷积网络（GDN）解码器，实现节点属性与结构信息的多分辨率捕获和高效重构。实验结果显示，GRASPED在多个真实数据集上的节点异常检测性能优于当前主流模型，并表现出对参数和不同异常类型的鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/GRASPED_Graph_Anomaly_Detection_using_Autoencoder_with_Spectral_Encoder_and_Decoder.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="102-Bayesian-Inference-and-Learning-in-Nonlinear-Dynamical-Systems-A-Framework-for-Incorporating-Explicit-and-Implicit-Prior-Knowledge"><a href="#102-Bayesian-Inference-and-Learning-in-Nonlinear-Dynamical-Systems-A-Framework-for-Incorporating-Explicit-and-Implicit-Prior-Knowledge" class="headerlink" title="102. Bayesian Inference and Learning in Nonlinear Dynamical Systems: A Framework for Incorporating Explicit and Implicit Prior Knowledge"></a>102. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Bayesian_Inference_and_Learning_in_Nonlinear_Dynamical_Systems__A_Framework_for_Incorporating_Explic.pdf">Bayesian Inference and Learning in Nonlinear Dynamical Systems: A Framework for Incorporating Explicit and Implicit Prior Knowledge</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Leibniz Universität Hannover</span></p>
<p>该论文提出了一个通用的系统辨识工具，用于在非线性动力学系统中融合显式（如已知物理方程）和隐式（如平滑或对称性假设）先验知识，实现对未知模型部分的贝叶斯推断与学习。方法核心包括：基于可交换接口变量和灵活基函数展开的建模结构，通过封闭形式的参数边缘化推导，结合序贯蒙特卡洛（SMC）与粒子MCMC，实现在线和离线场景下的高效推断与学习，避免模型反演或人工变换。实验在仿真与真实系统上验证了方法的适用性与数据高效性，能在有限数据下准确学习复杂系统的未知动力学。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Bayesian_Inference_and_Learning_in_Nonlinear_Dynamical_Systems_A_Framework_for_Incorporating_Explicit_and_Implicit_Prior_Knowledge.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="103-Evaluating-Knowledge-Graph-Complexity-via-Semantic-Spectral-and-Structural-Metrics-for-Link-Prediction"><a href="#103-Evaluating-Knowledge-Graph-Complexity-via-Semantic-Spectral-and-Structural-Metrics-for-Link-Prediction" class="headerlink" title="103. Evaluating Knowledge Graph Complexity via Semantic, Spectral, and Structural Metrics for Link Prediction"></a>103. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Evaluating_Knowledge_Graph_Complexity_via_Semantic,_Spectral,_and_Structural_Metrics_for_Link_Predic.pdf">Evaluating Knowledge Graph Complexity via Semantic, Spectral, and Structural Metrics for Link Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universiti Brunei Darussalam</span></p>
<p>本文提出了一个综合框架，通过语义、谱和结构指标系统性评估知识图谱（KG）复杂性，并分析这些指标在链路预测任务中的表现。研究发现，谱指标CSG在KG环境下对参数高度敏感且与模型性能相关性弱，而语义和结构指标（如关系熵、最大关系多样性、平均度等）能更稳健地反映任务难度。结论是，KG链路预测中应优先采用语义和结构复杂性指标而非谱分离指标。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Evaluating_Knowledge_Graph_Complexity_via_Semantic_Spectral_and_Structural_Metrics_for_Link_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="104-Learning-ECG-Representations-via-Poly-Window-Contrastive-Learning"><a href="#104-Learning-ECG-Representations-via-Poly-Window-Contrastive-Learning" class="headerlink" title="104. Learning ECG Representations via Poly-Window Contrastive Learning"></a>104. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Learning_ECG_Representations_via_Poly-Window_Contrastive_Learning.pdf">Learning ECG Representations via Poly-Window Contrastive Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Emory University</span></p>
<p>该论文提出了一种多窗口对比学习（Poly-Window Contrastive Learning）框架，通过从每条ECG信号中提取多个时间窗口，利用统计方式聚合正样本对，实现对心电信号的稳定生理特征表征学习。实验在PTB-XL数据集上表明，该方法在多标签分类性能（AUROC和F1）上超越传统双视图方法，同时训练收敛速度提高至原来的四分之一，计算效率提升14.8%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Learning_ECG_Representations_via_Poly-Window_Contrastive_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="105-Frequency-adaptive-tensor-neural-networks-for-high-dimensional-multi-scale-problems"><a href="#105-Frequency-adaptive-tensor-neural-networks-for-high-dimensional-multi-scale-problems" class="headerlink" title="105. Frequency-adaptive tensor neural networks for high-dimensional multi-scale problems"></a>105. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/Frequency-adaptive_tensor_neural_networks_for_high-dimensional_multi-scale_problems.pdf">Frequency-adaptive tensor neural networks for high-dimensional multi-scale problems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Academy of Mathematics and Systems Science, Chinese Academy of Sciences</span></p>
<p>本文提出了频率自适应张量神经网络（TNNs）方法，结合张量分解和离散傅里叶变换（DFT），通过分析TNNs的频率原理（频谱偏置），设计了可自适应调节频率特征的TNNs算法。此方法有效提升了TNNs在高维多尺度 PDE 问题中的高频特征表达能力，大幅降低了解的L2误差。结论表明，频率自适应TNNs在多种高维多尺度方程（如泊松方程、热方程、波方程、Helmholtz方程）上均表现出高效、鲁棒的逼近能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/Frequency-adaptive_tensor_neural_networks_for_high-dimensional_multi-scale_problems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="106-MMQ-Multimodal-Mixture-of-Quantization-Tokenization-for-Semantic-ID-Generation-and-User-Behavioral-Adaptation"><a href="#106-MMQ-Multimodal-Mixture-of-Quantization-Tokenization-for-Semantic-ID-Generation-and-User-Behavioral-Adaptation" class="headerlink" title="106. MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation"></a>106. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/MMQ__Multimodal_Mixture-of-Quantization_Tokenization_for_Semantic_ID_Generation_and_User_Behavioral_.pdf">MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Alibaba Group</span></p>
<p>该论文提出MMQ框架，通过多专家结构结合模态共享与模态特异专家训练，实现多模态内容（文本和图像）到语义ID的高效离散化，并以正交约束提升表示多样性。第二阶段采用行为感知微调，使语义ID动态适应推荐目标，显著提升推荐系统在检索和排序任务中的表现，尤其在长尾物品推荐和工业级场景中有较大优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/MMQ_Multimodal_Mixture-of-Quantization_Tokenization_for_Semantic_ID_Generation_and_User_Behavioral_Adaptation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="107-SleepDIFFormer-Sleep-Stage-Classification-via-Multivariate-Differential-Transformer"><a href="#107-SleepDIFFormer-Sleep-Stage-Classification-via-Multivariate-Differential-Transformer" class="headerlink" title="107. SleepDIFFormer: Sleep Stage Classification via Multivariate Differential Transformer"></a>107. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-22/arxiv/SleepDIFFormer__Sleep_Stage_Classification_via_Multivariate_Differential_Transformer.pdf">SleepDIFFormer: Sleep Stage Classification via Multivariate Differential Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiamen University Malaysia</span></p>
<p>SleepDIFFormer提出了一种基于多变量差分Transformer架构（MDTA）的方法，通过联合EEG和EOG信号，采用差分自注意力和跨注意力机制抑制信号与注意力噪声，并结合多层特征对齐实现跨域泛化。实验在五个公开睡眠分期数据集上取得了当前最佳性能，并通过消融分析验证了各模块的贡献，提升了模型在未见数据集上的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-25/SleepDIFFormer_Sleep_Stage_Classification_via_Multivariate_Differential_Transformer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/25报纸</div>
      <div>http://ywfhhh.github.io/2025/08/2025-08-25_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>具身ADT</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月25日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/2025-08-26_article/" title="2025/08/26报纸">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">2025/08/26报纸</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-22_article/" title="2025/08/22报纸">
                        <span class="hidden-mobile">2025/08/22报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
