

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Ywfhhh">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="今日论文报纸">
<meta property="og:url" content="http://example.com/2025/08/2025-08-12_article/index.html">
<meta property="og:site_name" content="论文报纸集合">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/The_Fair_Game_Auditing_%26_Debiasing_AI_Algorithms_Over_Time.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Unsupervised_Partner_Design_Enables_Robust_Ad-hoc_Teamwork.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/OM2P_Offline_Multi-Agent_Mean-Flow_Policy.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Lightweight_Auto-bidding_based_on_Traffic_Prediction_in_Live_Advertising.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/GCHR_Goal-Conditioned_Hindsight_Regularization_for_Sample-Efficient_Reinforcement_Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/VQAThinker_Exploring_Generalizable_and_Explainable_Video_Quality_Assessment_via_Reinforcement_Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Society_of_Mind_Meets_Real-Time_Strategy_A_Hierarchical_Multi-Agent_Framework_for_Strategic_Reasoning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Multi-Armed_Bandits-Based_Optimization_of_Decision_Trees.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LinguaFluid_Language-Guided_Fluid_Control_via_Semantic_Rewards_in_Reinforcement_Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Mildly_Conservative_Regularized_Evaluation_for_Offline_Reinforcement_Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Shortcut_Learning_in_Generalist_Robot_Policies_The_Role_of_Dataset_Diversity_and_Fragmentation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/L2Calib_SE%283%29-Manifold_Reinforcement_Learning_for_Robust_Extrinsic_Calibration_with_Degenerate_Motion_Resilience.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SCAR_State-Space_Compression_for_AI-Driven_Resource_Management_in_6G-Enabled_Vehicular_Infotainment_Systems.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Computer_Vision-based_Adaptive_Control_for_Back_Exoskeleton_Performance_Optimization.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Beyond_Constant_Parameters_Hyper_Prediction_Models_and_HyperMPC.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Mitigating_Undesired_Conditions_in_Flexible_Production_with_Product%E2%80%93Process%E2%80%93Resource_Asset_Knowledge_Graphs.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/REBot_Reflexive_Evasion_Robot_for_Instantaneous_Dynamic_Obstacle_Avoidance.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Affordance-R1_Reinforcement_Learning_for_Generalizable_Affordance_Reasoning_in_Multimodal_Large_Language_Model.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Graph-based_Robot_Localization_Using_a_Graph_Neural_Network_with_a_Floor_Camera_and_a_Feature_Rich_Industrial_Floor.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/GMF-Drive_Gated_Mamba_Fusion_with_Spatial-Aware_BEV_Representation_for_End-to-End_Autonomous_Driving.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Bounding_Distributional_Shifts_in_World_Modeling_through_Novelty_Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ME3-BEV_Mamba-Enhanced_Deep_Reinforcement_Learning_for_End-to-End_Autonomous_Driving_with_BEV-Perception.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ReNiL_Relative_Neural_Inertial_Locator_with_Any-Scale_Bayesian_Inference.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Hand_by_Hand_LLM_Driving_EMS_Assistant_for_Operational_Skill_Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Dynamical_Trajectory_Planning_of_Disturbance_Consciousness_for_Air-Land_Bimodal_Unmanned_Aerial_Vehicles.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Affordance-Guided_Dual-Armed_Disassembly_Teleoperation_for_Mating_Parts.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Latent_Policy_Barrier_Learning_Robust_Visuomotor_Policies_by_Staying_In-Distribution.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/WGAST_Weakly-Supervised_Generative_Network_for_Daily_10_m_Land_Surface_Temperature_Estimation_via_Spatio-Temporal_Fusion.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Text_Embedded_Swin-UMamba_for_DeepLesion_Segmentation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/TRUST_Leveraging_Text_Robustness_for_Unsupervised_Domain_Adaptation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/CLIPin_A_Non-contrastive_Plug-in_to_CLIP_for_Multimodal_Semantic_Alignment.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/FVGen_Accelerating_Novel-View_Synthesis_with_Adversarial_Video_Diffusion_Distillation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Text_as_Any-Modality_for_Zero-Shot_Classification_by_Consistent_Prompt_Tuning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Text-guided_Visual_Prompt_DINO_for_Generic_Segmentation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Street_View_Sociability_Interpretable_Analysis_of_Urban_Social_Behavior_Across_15_Cities.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Uncertainty-quantified_Rollout_Policy_Adaptation_for_Unlabelled_Cross-domain_Temporal_Grounding.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Advanced_Deep_Learning_Techniques_for_Accurate_Lung_Cancer_Detection_and_Classification.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Depth_Jitter_Seeing_through_the_Depth.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/TEFormer_Texture-Aware_and_Edge-Guided_Transformer_for_Semantic_Segmentation_of_Urban_Remote_Sensing_Images.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/MA-CBP_A_Criminal_Behavior_Prediction_Framework_Based_on_Multi-Agent_Asynchronous_Collaboration.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Can_Diffusion_Models_Bridge_the_Domain_Gap_in_Cardiac_MR_Imaging_.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Deepfake_Detection_that_Generalizes_Across_Benchmarks.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/InfoCausalQA_Can_Models_Perform_Non-explicit_Causal_Reasoning_Based_on_Infographic_.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Clinically-guided_Data_Synthesis_for_Laryngeal_Lesion_Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Fewer_Denoising_Steps_or_Cheaper_Per-Step_Inference_Towards_Compute-Optimal_Diffusion_Model_Deployment.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Improving_Diagnostic_Accuracy_for_Oral_Cancer_with_inpainting_Synthesis_Lesions_Generated_Using_Diffusion_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/DiffCap_Diffusion-based_Real-time_Human_Motion_Capture_using_Sparse_IMUs_and_a_Monocular_Camera.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SC-Captioner_Improving_Image_Captioning_with_Self-Correction_by_Reinforcement_Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Ensemble-Based_Graph_Representation_of_fMRI_Data_for_Cognitive_Brain_State_Classification.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Mask_%26_Match_Learning_to_Recognize_Handwritten_Math_with_Self-Supervised_Attention.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/UGD-IML_A_Unified_Generative_Diffusion-based_Framework_for_Constrained_and_Unconstrained_Image_Manipulation_Localization.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Transformer-Based_Explainable_Deep_Learning_for_Breast_Cancer_Detection_in_Mammography_The_MammoFormer_Framework.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SAM_Encoder_Breach_by_Adversarial_Simplicial_Complex_Triggers_Downstream_Model_Failures.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Learning_Representations_of_Satellite_Images_with_Evaluations_on_Synoptic_Weather_Events.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SynSeg_Feature_Synergy_for_Multi-Category_Contrastive_Learning_in_Open-Vocabulary_Semantic_Segmentation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/MCA_2D-3D_Retrieval_with_Noisy_Labels_via_Multi-level_Adaptive_Correction_and_Alignment.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/E-React_Towards_Emotionally_Controlled_Synthesis_of_Human_Reactions.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Towards_MR-Based_Trochleoplasty_Planning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/AGI_for_the_Earth%2C_the_path%2C_possibilities_and_how_to_evaluate_intelligence_of_models_that_work_with_Earth_Observation_Data_.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Learning_3D_Texture-Aware_Representations_for_Parsing_Diverse_Human_Clothing_and_Body_Parts.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/A_3DGS-Diffusion_Self-Supervised_Framework_for_Normal_Estimation_from_a_Single_Image.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Improved_Sub-Visible_Particle_Classification_in_Flow_Imaging_Microscopy_via_Generative_AI-Based_Image_Synthesis.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/KnapFormer_An_Online_Load_Balancer_for_Efficient_Diffusion_Transformers_Training.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Mediator-Guided_Multi-Agent_Collaboration_among_Open-Source_Models_for_Medical_Decision-Making.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ECMF_Enhanced_Cross-Modal_Fusion_for_Multimodal_Emotion_Recognition_in_MER-SEMI_Challenge.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Neural_Field_Representations_of_Mobile_Computational_Photography.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Effective_Training_Data_Synthesis_for_Improving_MLLM_Chart_Understanding.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LLM_Unlearning_using_Gradient_Ratio-Based_Influence_Estimation_and_Noise_Injection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ScamAgents_How_AI_Agents_Can_Simulate_Human-Level_Scam_Calls.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Learning_the_Topic%2C_Not_the_Language_How_LLMs_Classify_Online_Immigration_Discourse_Across_Languages.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Memp_Exploring_Agent_Procedural_Memory.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Sample-efficient_LLM_Optimization_with_Reset_Replay.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/End-to-End_Text-to-SQL_with_Dataset_Selection_Leveraging_LLMs_for_Adaptive_Query_Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SpeakerLM_End-to-End_Versatile_Speaker_Diarization_and_Recognition_with_Multimodal_Large_Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/From_Explainable_to_Explanatory_Artificial_Intelligence_Toward_a_New_Paradigm_for_Human-Centered_Explanations_through_Generative_AI.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Large_Language_Model_Data_Generation_for_Enhanced_Intent_Recognition_in_German_Speech.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SIFThinker_Spatially-Aware_Image_Focus_for_Visual_Reasoning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/In-Training_Defenses_against_Emergent_Misalignment_in_Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Classification_is_a_RAG_problem_A_case_study_on_hate_speech_detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/UR2_Unify_RAG_and_Reasoning_through_Reinforcement_Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Automated_Creation_of_the_Legal_Knowledge_Graph_Addressing_Legislation_on_Violence_Against_Women_Resource%2C_Methodology_and_Lessons_Learned.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Beyond_Prompt-Induced_Lies_Investigating_LLM_Deception_on_Benign_Prompts.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Aligning_Effective_Tokens_with_Video_Anomaly_in_Large_Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Harnessing_Adaptive_Topology_Representations_for_Zero-Shot_Graph_Question_Answering.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/GeoLaux_A_Benchmark_for_Evaluating_MLLMs%E2%80%99_Geometry_Performance_on_Long-Step_Problems_Requiring_Auxiliary_Lines.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LoRA_in_LoRA_Towards_Parameter-Efficient_Architecture_Expansion_for_Continual_Visual_Instruction_Tuning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Retrieval_Augmented_Large_Language_Model_System_for_Comprehensive_Drug_Contraindications.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Less_is_More_Selective_Reflection_for_Compatible_and_Efficient_Knowledge_Distillation_in_Large_Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/PanelTR_Zero-Shot_Table_Reasoning_Framework_Through_Multi-Agent_Scientific_Discussion.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/AdaptInfer_Adaptive_Token_Pruning_for_Vision%E2%80%93Language_Model_Inference_with_Dynamical_Text_Guidance.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LLMs_for_Resource_Allocation_A_Participatory_Budgeting_Approach_to_Inferring_Preferences.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/EvolvR_Self-Evolving_Pairwise_Reasoning_for_Story_Evaluation_to_Enhance_Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/DP-LLM_Runtime_Model_Adaptation_with_Dynamic_Layer-wise_Precision_Assignment.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SDEval_Safety_Dynamic_Evaluation_for_Multimodal_Large_Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LLM_Serving_Optimization_with_Variable_Prefill_and_Decode_Lengths.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SKATE%2C_a_Scalable_Tournament_Eval_Weaker_LLMs_differentiate_between_stronger_ones_using_verifiable_challenges.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Can_Large_Models_Fool_the_Eye_A_New_Turing_Test_for_Biological_Animation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Fourier-VLM_Compressing_Vision_Tokens_in_the_Frequency_Domain_for_Large_Vision-Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Efficient_Knowledge_Probing_of_Large_Language_Models_by_Adapting_Pre-trained_Embeddings.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Temporal_Self-Rewarding_Language_Models_Decoupling_Chosen-Rejected_via_Past-Future.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Position_Intelligent_Coding_Systems_Should_Write_Programs_with_Justifications.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/MATHREAL_We_Keep_It_Real%21_A_Real_Scene_Benchmark_for_Evaluating_Math_Reasoning_in_Multimodal_Large_Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Optimizing_Prompt_Sequences_using_Monte_Carlo_Tree_Search_for_LLM-Based_Optimization.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Learning_by_Teaching_Engaging_Students_as_Instructors_of_Large_Language_Models_in_Computer_Science_Education.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Crisp_Attention_Regularizing_Transformers_via_Structured_Sparsity.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Bifrost-1_Bridging_Multimodal_LLMs_and_Diffusion_Models_with_Patch-level_CLIP_Latents.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/eSASRec_Enhancing_Transformer-based_Recommendations_in_a_Modular_Fashion.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Tree-Based_Deep_Learning_for_Ranking_Symbolic_Integration_Algorithms.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ActivityDiff_A_diffusion_model_with_Positive_and_Negative_Activity_Guidance_for_De_Novo_Drug_Design.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/AntiCheatPT_A_Transformer-Based_Approach_to_Cheat_Detection_in_Competitive_Computer_Games.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Low-Bit_Data_Processing_Using_Multiple-Output_Spiking_Neurons_with_Non-linear_Reset_Feedback.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Multi-Omics_Analysis_for_Cancer_Subtype_Inference_via_Unrolling_Graph_Smoothness_Priors.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/One_Size_Does_Not_Fit_All_A_Distribution-Aware_Sparsification_for_More_Precise_Model_Merging.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Semantic_Item_Graph_Enhancement_for_Multimodal_Recommendation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Enhancing_the_Scalability_of_Classical_Surrogates_for_Real-World_Quantum_Machine_Learning_Applications.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/MeanAudio_Fast_and_Faithful_Text-to-Audio_Generation_with_Mean_Flows.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Architecture-Aware_Generalization_Bounds_for_Temporal_Networks_Theory_and_Fair_Comparison_Methodology.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Do_Ethical_AI_Principles_Matter_to_Users_A_Large-Scale_Analysis_of_User_Sentiment_and_Satisfaction.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Fast%2C_Convex_and_Conditioned_Network_for_Multi-Fidelity_Vectors_and_Stiff_Univariate_Differential_Equations.jpg">
<meta property="article:published_time" content="2025-08-12T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-19T06:40:20.231Z">
<meta property="article:author" content="Ywfhhh">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/The_Fair_Game_Auditing_%26_Debiasing_AI_Algorithms_Over_Time.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>今日论文报纸 - 论文报纸集合</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>【学术动态报告】论文报纸</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文集合</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="今日论文报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-12 00:00" pubdate>
          星期二, 八月 12日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          18k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">今日论文报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>今日论文报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-12<br>📄 <strong>发现论文数量</strong>：119  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-The-Fair-Game-Auditing-Debiasing-AI-Algorithms-Over-Time"><a href="#1-The-Fair-Game-Auditing-Debiasing-AI-Algorithms-Over-Time" class="headerlink" title="1. The Fair Game: Auditing &amp; Debiasing AI Algorithms Over Time"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/The_Fair_Game__Auditing_&_Debiasing_AI_Algorithms_Over_Time.pdf">The Fair Game: Auditing &amp; Debiasing AI Algorithms Over Time</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Inria</span></p>
<p>该论文提出了“Fair Game”框架，通过将审计器（Auditor）与去偏算法（Debiasing Algorithm）组成闭环，动态审计和去偏机器学习模型，利用强化学习（RL）机制实现长期公平性目标的自适应调整。实验与理论分析表明，该方法能在数据分布和社会伦理规范动态变化的环境下，持续减少模型偏见并提高公平性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/The_Fair_Game_Auditing_%26_Debiasing_AI_Algorithms_Over_Time.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-Unsupervised-Partner-Design-Enables-Robust-Ad-hoc-Teamwork"><a href="#2-Unsupervised-Partner-Design-Enables-Robust-Ad-hoc-Teamwork" class="headerlink" title="2. Unsupervised Partner Design Enables Robust Ad-hoc Teamwork"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Unsupervised_Partner_Design_Enables_Robust_Ad-hoc_Teamwork.pdf">Unsupervised Partner Design Enables Robust Ad-hoc Teamwork</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Stuttgart</span></p>
<p>该论文提出了Unsupervised Partner Design (UPD)——一种无需预训练伙伴、无须人工调参的多智能体强化学习框架，通过动态生成带有随机偏差和能力多样性的训练伙伴，并利用回报方差作为学习性指标筛选最能促进主智能体学习的伙伴。UPD可与无监督环境设计结合，实现同时针对任务和伙伴分布的自适应课程学习，在Overcooked-AI和其通用挑战中显著优于主流基线，并在人机协同实验中获得更高评分和回报，展现强泛化与协作能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Unsupervised_Partner_Design_Enables_Robust_Ad-hoc_Teamwork.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-OM2P-Offline-Multi-Agent-Mean-Flow-Policy"><a href="#3-OM2P-Offline-Multi-Agent-Mean-Flow-Policy" class="headerlink" title="3. OM2P: Offline Multi-Agent Mean-Flow Policy"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/OM2P__Offline_Multi-Agent_Mean-Flow_Policy.pdf">OM2P: Offline Multi-Agent Mean-Flow Policy</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出了OM2P算法，将均值流（mean-flow）生成模型创新性地集成到离线多智能体强化学习（MARL）中，实现高效的一步动作生成。方法上通过奖励感知优化、广义时间步分布和无导数估算，提升了训练稳定性和内存效率。实验表明，OM2P在多智能体标准任务中达到了最优或近最优表现，显著减少了GPU内存消耗并加速训练。结论是OM2P可高效扩展至复杂多智能体环境，兼具高性能与实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/OM2P_Offline_Multi-Agent_Mean-Flow_Policy.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-Lightweight-Auto-bidding-based-on-Traffic-Prediction-in-Live-Advertising"><a href="#4-Lightweight-Auto-bidding-based-on-Traffic-Prediction-in-Live-Advertising" class="headerlink" title="4. Lightweight Auto-bidding based on Traffic Prediction in Live Advertising"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Lightweight_Auto-bidding_based_on_Traffic_Prediction_in_Live_Advertising.pdf">Lightweight Auto-bidding based on Traffic Prediction in Live Advertising</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Taobao &amp; Tmall Group of Alibaba</span></p>
<p>本文提出了BiCB轻量级自动竞价算法，结合线性规划(LP)理论分析的最优竞价公式与未来流量的统计预测，利用LightGBM等回归模型估算全时段流量，通过梯度下降高效近似优化目标。实验证明BiCB在满足预算和点击单价约束下性能优越，工程复杂度低，已在大型平台广告业务落地并取得显著业务提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Lightweight_Auto-bidding_based_on_Traffic_Prediction_in_Live_Advertising.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-GCHR-Goal-Conditioned-Hindsight-Regularization-for-Sample-Efficient-Reinforcement-Learning"><a href="#5-GCHR-Goal-Conditioned-Hindsight-Regularization-for-Sample-Efficient-Reinforcement-Learning" class="headerlink" title="5. GCHR: Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/GCHR___Goal-Conditioned_Hindsight_Regularization_for_Sample-Efficient_Reinforcement_Learning.pdf">GCHR: Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong University</span></p>
<p>本文提出了一种高效的目标条件强化学习方法GCHR，通过结合Hindsight Self-Imitation Regularization（HSR）和Hindsight Goal Regularization（HGR），显著提升了稀疏奖励场景下的样本利用率和策略性能。GCHR无需额外网络或复杂规划，仅通过对历史轨迹的目标重标与行为克隆实现对策略的双重正则化，在理论和实验证明中均优于现有GCRL方法，能够更快收敛并达到更优策略。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/GCHR_Goal-Conditioned_Hindsight_Regularization_for_Sample-Efficient_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-VQAThinker-Exploring-Generalizable-and-Explainable-Video-Quality-Assessment-via-Reinforcement-Learning"><a href="#6-VQAThinker-Exploring-Generalizable-and-Explainable-Video-Quality-Assessment-via-Reinforcement-Learning" class="headerlink" title="6. VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/VQAThinker__Exploring_Generalizable_and_Explainable_Video_Quality_Assessment_via_Reinforcement_Learn.pdf">VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>VQAThinker提出了一种结合大规模多模态模型（LMM）与强化学习（基于GRPO算法）的推理型视频质量评价（VQA）框架，通过引入三种奖励函数（钟形回归奖励、对排序奖励与时序一致性奖励），实现了视频质量理解与评分的联合建模。实验表明，该方法在多项VQA基准测试（含OOD场景）中取得了领先的泛化能力和解释性，显著优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/VQAThinker_Exploring_Generalizable_and_Explainable_Video_Quality_Assessment_via_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-Society-of-Mind-Meets-Real-Time-Strategy-A-Hierarchical-Multi-Agent-Framework-for-Strategic-Reasoning"><a href="#7-Society-of-Mind-Meets-Real-Time-Strategy-A-Hierarchical-Multi-Agent-Framework-for-Strategic-Reasoning" class="headerlink" title="7. Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Society_of_Mind_Meets_Real-Time_Strategy__A_Hierarchical_Multi-Agent_Framework_for_Strategic_Reasoni.pdf">Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Seoul National University</span></p>
<p>本文提出了HIMA架构，通过多模仿学习代理结合战略规划器（SP），利用专家演示数据进行单位组成聚类，生成结构化多步行动序列，并由SP以环境感知和时序链式推理方式融合各代理建议，适应实时战略游戏变化。结果显示，HIMA在StarCraft II九种种族组合对战中胜率和效率均优于现有主流方法，证明多代理模仿结合高层协调能提升复杂环境下的智能体表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Society_of_Mind_Meets_Real-Time_Strategy_A_Hierarchical_Multi-Agent_Framework_for_Strategic_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-Multi-Armed-Bandits-Based-Optimization-of-Decision-Trees"><a href="#8-Multi-Armed-Bandits-Based-Optimization-of-Decision-Trees" class="headerlink" title="8. Multi-Armed Bandits-Based Optimization of Decision Trees"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Multi-Armed_Bandits-Based_Optimization_of_Decision_Trees.pdf">Multi-Armed Bandits-Based Optimization of Decision Trees</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bangladesh University of Engineering and Technology</span></p>
<p>本论文提出了一种基于多臂赌博机（Multi-Armed Bandits, MAB）的决策树剪枝优化方法，将剪枝过程建模为探索-利用问题，采用UCB1、Thompson Sampling等MAB算法动态选择要剪枝的分支节点，并基于每次剪枝动作的反馈调整策略。实验证明，该方法在多个基准数据集上优于传统的代价复杂度剪枝（CCP），能提升决策树的泛化能力和预测性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Multi-Armed_Bandits-Based_Optimization_of_Decision_Trees.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-LinguaFluid-Language-Guided-Fluid-Control-via-Semantic-Rewards-in-Reinforcement-Learning"><a href="#9-LinguaFluid-Language-Guided-Fluid-Control-via-Semantic-Rewards-in-Reinforcement-Learning" class="headerlink" title="9. LinguaFluid: Language-Guided Fluid Control via Semantic Rewards in Reinforcement Learning"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LinguaFluid__Language_Guided_Fluid_Control_via_Semantic_Rewards_in_Reinforcement_Learning.pdf">LinguaFluid: Language-Guided Fluid Control via Semantic Rewards in Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">College of Environmental and Resource Sciences, Zhejiang University</span></p>
<p>该论文提出了一种基于语义奖励的强化学习方法，用于实现流体控制等科学领域复杂任务。方法核心是利用SBERT生成当前状态和目标描述的语义嵌入，通过余弦相似度作为奖励信号，指导RL智能体学习，无需手工设计数值奖励函数。实验结果表明，该方法在无手工奖励函数下能够有效引导智能体实现与自然语言目标一致的控制行为，且语义空间中的奖励与物理空间指标高度相关。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LinguaFluid_Language-Guided_Fluid_Control_via_Semantic_Rewards_in_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-Mildly-Conservative-Regularized-Evaluation-for-Offline-Reinforcement-Learning"><a href="#10-Mildly-Conservative-Regularized-Evaluation-for-Offline-Reinforcement-Learning" class="headerlink" title="10. Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Mildly_Conservative_Regularized_Evaluation_for_Offline_Reinforcement_Learning.pdf">Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Central South University</span></p>
<p>本文提出了一种用于离线强化学习的温和保守正则化评估（MCRE）框架，将时序差分（TD）误差与行为克隆项结合进Bellman备份，以兼顾值估计的准确性和策略的保守性。基于MCRE，作者提出了MCRQ算法，并在D4RL基准的MuJoCo任务上验证了其优越性，理论上证明了收敛性及性能上界，实验显示MCRQ优于现有主流算法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Mildly_Conservative_Regularized_Evaluation_for_Offline_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="11-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation"><a href="#11-Shortcut-Learning-in-Generalist-Robot-Policies-The-Role-of-Dataset-Diversity-and-Fragmentation" class="headerlink" title="11. Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Shortcut_Learning_in_Generalist_Robot_Policies__The_Role_of_Dataset_Diversity_and_Fragmentation.pdf">Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">UESTC</span></p>
<p>本文系统分析了通用机器人策略在大规模多子集数据集（如OXE）训练下出现的shortcut learning现象，指出数据子集内部多样性不足和子集间分布碎片化是导致策略泛化能力受限的核心原因。通过理论建模、仿真和真实机器人实验，作者证明提升子集多样性、减少子集间分布差异，以及采用针对性的机器人数据增强（如视角&#x2F;对象增强）可显著缓解shortcut learning并提升泛化能力；结论为：优化数据收集与增强流程是提升通用机器人策略泛化性的关键途径。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Shortcut_Learning_in_Generalist_Robot_Policies_The_Role_of_Dataset_Diversity_and_Fragmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-L2Calib-SE-3-Manifold-Reinforcement-Learning-for-Robust-Extrinsic-Calibration-with-Degenerate-Motion-Resilience"><a href="#12-L2Calib-SE-3-Manifold-Reinforcement-Learning-for-Robust-Extrinsic-Calibration-with-Degenerate-Motion-Resilience" class="headerlink" title="12. L2Calib: SE(3)-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/L2Calib__$SE(3)$-Manifold_Reinforcement_Learning_for_Robust_Extrinsic_Calibration_with_Degenerate_Mo.pdf">L2Calib: SE(3)-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Cyber-Systems and Control, Zhejiang University</span></p>
<p>本文提出了一种基于强化学习(SE(3)-流形建模)的传感器外参标定方法，将标定问题建模为决策过程，直接在SE(3)空间优化外参以提升里程计精度。方法采用Bingham分布建模旋转参数，结合轨迹对齐奖励机制和自动化数据筛选模块，实现无需结构化靶标即可在弱激励下进行多传感器联合标定。实验表明该方法在无人机、地面车和手持平台上超越传统优化方法，具备更高的鲁棒性和精度，且对初值不敏感，极大简化了机器人多传感器标定流程。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/L2Calib_SE%283%29-Manifold_Reinforcement_Learning_for_Robust_Extrinsic_Calibration_with_Degenerate_Motion_Resilience.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-SCAR-State-Space-Compression-for-AI-Driven-Resource-Management-in-6G-Enabled-Vehicular-Infotainment-Systems"><a href="#13-SCAR-State-Space-Compression-for-AI-Driven-Resource-Management-in-6G-Enabled-Vehicular-Infotainment-Systems" class="headerlink" title="13. SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SCAR__State-Space_Compression_for_AI-Driven_Resource_Management_in_6G-Enabled_Vehicular_Infotainment.pdf">SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Swiss Distance University of Applied Sciences</span></p>
<p>本文提出SCAR框架，通过SAST优化的K-means聚类与径向基函数网络（RBFN）实现边缘AI辅助的CQI数据压缩，随后利用压缩后的状态输入6G环境下的强化学习（RL）调度器进行车载娱乐资源分配。实验表明，SCAR在提升调度公平性和可扩展性方面优于传统方法，实现TTI可行区间提升14%，不公平调度时间下降15%，展现出高效性和公平性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SCAR_State-Space_Compression_for_AI-Driven_Resource_Management_in_6G-Enabled_Vehicular_Infotainment_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-Computer-Vision-based-Adaptive-Control-for-Back-Exoskeleton-Performance-Optimization"><a href="#14-Computer-Vision-based-Adaptive-Control-for-Back-Exoskeleton-Performance-Optimization" class="headerlink" title="14. Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Computer_Vision-based_Adaptive_Control_for_Back_Exoskeleton_Performance_Optimization.pdf">Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Politecnico di Milano</span></p>
<p>本论文提出了结合计算机视觉的自适应控制系统，用于提升背部外骨骼在工业场景下的性能优化。方法上，作者首先构建了基于肌肉活动减少、用户不适感和偏好的多维优化空间，通过实验获得最优工作区域，并开发了基于YOLOv11和DINOv2的实时视觉管线，实现对搬运物体的自动检测、重量分类与辅助力度自适应调节。实验结果显示，所提系统在提升肌肉减负（最大减少达23%）、用户偏好和舒适度等方面均优于静态控制，且具备80%以上实时分类准确率。结论表明，智能自适应控制有望显著优化工业外骨骼的实际应用效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Computer_Vision-based_Adaptive_Control_for_Back_Exoskeleton_Performance_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-Beyond-Constant-Parameters-Hyper-Prediction-Models-and-HyperMPC"><a href="#15-Beyond-Constant-Parameters-Hyper-Prediction-Models-and-HyperMPC" class="headerlink" title="15. Beyond Constant Parameters: Hyper Prediction Models and HyperMPC"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Beyond_Constant_Parameters__Hyper_Prediction_Models_and_HyperMPC.pdf">Beyond Constant Parameters: Hyper Prediction Models and HyperMPC</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Robotics and Machine Intelligence, Poznan University of Technology</span></p>
<p>本文提出了Hyper Prediction Model (HyperPM)和其在HyperMPC框架中的集成，实现了通过神经网络预测随时间变化的动力学模型参数轨迹，结合历史状态、控制输入与未来规划动作，提升了MPC在复杂动力学系统（如带有未建模扰动的无人机、摆和自动驾驶赛车）中的长期预测准确性和控制性能。实验结果表明，HyperPM在多个任务中长期预测误差降低超35%，HyperMPC在下游控制任务中相较现有技术取得了最高76%的性能提升，并保持低计算复杂度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Beyond_Constant_Parameters_Hyper_Prediction_Models_and_HyperMPC.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-Mitigating-Undesired-Conditions-in-Flexible-Production-with-Product–Process–Resource-Asset-Knowledge-Graphs"><a href="#16-Mitigating-Undesired-Conditions-in-Flexible-Production-with-Product–Process–Resource-Asset-Knowledge-Graphs" class="headerlink" title="16. Mitigating Undesired Conditions in Flexible Production with Product–Process–Resource Asset Knowledge Graphs"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Mitigating_Undesired_Conditions_in_Flexible_Production_with_Product-Process-Resource_Asset_Knowledge.pdf">Mitigating Undesired Conditions in Flexible Production with Product–Process–Resource Asset Knowledge Graphs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague</span></p>
<p>该论文提出了一种基于语义知识图谱的产品–过程–资源资产知识图（PPR-AKG）方法，通过扩展传统PPR模型引入对工业生产中不良状态及其原因的建模，并采用OWL本体实现查询与推理。结合大型语言模型（LLM）实现人机交互，提升操作员和工程师对生产过程的理解与优化。实验验证在电池拆解与再制造场景下显著提升了灵活生产系统的问题诊断与性能表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Mitigating_Undesired_Conditions_in_Flexible_Production_with_Product%E2%80%93Process%E2%80%93Resource_Asset_Knowledge_Graphs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-REBot-Reflexive-Evasion-Robot-for-Instantaneous-Dynamic-Obstacle-Avoidance"><a href="#17-REBot-Reflexive-Evasion-Robot-for-Instantaneous-Dynamic-Obstacle-Avoidance" class="headerlink" title="17. REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/REBot__Reflexive_Evasion_Robot_for_Instantaneous_Dynamic_Obstacle_Avoidance.pdf">REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Singapore</span></p>
<p>本文提出了REBot系统，通过有限状态机集成了基于强化学习训练的瞬时回避策略和恢复策略，实现四足机器人在动态障碍物靠近时的低延迟反射性规避。REBot经过仿真和真实机器人测试，在多种障碍场景下表现出更高的回避成功率、能耗更低及对高速障碍物的鲁棒性，验证了方法在提升机器人安全和机动性上的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/REBot_Reflexive_Evasion_Robot_for_Instantaneous_Dynamic_Obstacle_Avoidance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-Affordance-R1-Reinforcement-Learning-for-Generalizable-Affordance-Reasoning-in-Multimodal-Large-Language-Model"><a href="#18-Affordance-R1-Reinforcement-Learning-for-Generalizable-Affordance-Reasoning-in-Multimodal-Large-Language-Model" class="headerlink" title="18. Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Affordance-R1__Reinforcement_Learning_for_Generalizable_Affordance_Reasoning_in_Multimodal_Large_Lan.pdf">Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (GZ)</span></p>
<p>Affordance-R1提出了一个强化学习框架，通过认知链式思考(CoT)引导的Group Relative Policy Optimization (GRPO)算法，提升多模态大语言模型(MLLM)的可泛化affordance推理能力。其方法设计了涵盖格式、感知和认知的奖励函数，并构建了高质量affordance推理数据集ReasonAff，实现了无需有监督微调即可在真实场景和域外数据上展现强大泛化和推理能力。结论表明，Affordance-R1在多项评测中显著优于现有方法，并为机器人感知与操作任务提供了更强的推理基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Affordance-R1_Reinforcement_Learning_for_Generalizable_Affordance_Reasoning_in_Multimodal_Large_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-Graph-based-Robot-Localization-Using-a-Graph-Neural-Network-with-a-Floor-Camera-and-a-Feature-Rich-Industrial-Floor"><a href="#19-Graph-based-Robot-Localization-Using-a-Graph-Neural-Network-with-a-Floor-Camera-and-a-Feature-Rich-Industrial-Floor" class="headerlink" title="19. Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Graph-based_Robot_Localization_Using_a_Graph_Neural_Network_with_a_Floor_Camera_and_a_Feature_Rich_I.pdf">Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">TU Dortmund University</span></p>
<p>该论文提出了一种创新的机器人室内定位方法，利用地面摄像头采集地板特征，通过将图像特征抽象为图结构并采用图卷积网络（GCN）进行特征嵌入与匹配，实现高效、可扩展的定位。该方法在工业地板环境下获得了高精度（误差仅0.64厘米）和高速度定位，并能有效解决‘被绑架机器人’问题，具有良好的通用性和扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Graph-based_Robot_Localization_Using_a_Graph_Neural_Network_with_a_Floor_Camera_and_a_Feature_Rich_Industrial_Floor.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-GMF-Drive-Gated-Mamba-Fusion-with-Spatial-Aware-BEV-Representation-for-End-to-End-Autonomous-Driving"><a href="#20-GMF-Drive-Gated-Mamba-Fusion-with-Spatial-Aware-BEV-Representation-for-End-to-End-Autonomous-Driving" class="headerlink" title="20. GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/GMF-Drive__Gated_Mamba_Fusion_with_Spatial-Aware_BEV_Representation_for_End-to-End_Autonomous_Drivin.pdf">GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>该论文提出GMF-Drive，一种面向端到端自动驾驶的多模态融合框架。方法包括采用几何增强的14维LiDAR柱状体表示来保留三维空间信息，并提出空间感知状态空间模型（BEV-SSM）实现高效线性复杂度的多层级融合，取代传统自注意力Transformer。实验表明，GMF-Drive在NAVSIM基准数据集上性能优于现有DiffusionDrive方法，实现了新的SOTA，并通过消融实验证实各创新组件的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/GMF-Drive_Gated_Mamba_Fusion_with_Spatial-Aware_BEV_Representation_for_End-to-End_Autonomous_Driving.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-Bounding-Distributional-Shifts-in-World-Modeling-through-Novelty-Detection"><a href="#21-Bounding-Distributional-Shifts-in-World-Modeling-through-Novelty-Detection" class="headerlink" title="21. Bounding Distributional Shifts in World Modeling through Novelty Detection"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Bounding_Distributional_Shifts_in_World_Modeling_through_Novelty_Detection.pdf">Bounding Distributional Shifts in World Modeling through Novelty Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Rutgers University</span></p>
<p>本文提出在视觉世界模型中引入基于变分自编码器（VAE）的新颖性检测机制，以解决模型推理时分布转移导致的性能下降问题。方法将VAE作为新颖性检测组件，度量预测状态与训练分布的偏离程度，并将此重构损失作为模型预测控制（MPC）中每步动作的代价项，集成至DINO-WM架构进行机器人操作任务测试。实验表明，该方法有效提升了规划算法的数据效率和鲁棒性，显著优于现有世界模型方法。结论认为新颖性检测能缓解世界模型不完美带来的推理偏差，但会牺牲对未见状态的探索。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Bounding_Distributional_Shifts_in_World_Modeling_through_Novelty_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-ME3-BEV-Mamba-Enhanced-Deep-Reinforcement-Learning-for-End-to-End-Autonomous-Driving-with-BEV-Perception"><a href="#22-ME3-BEV-Mamba-Enhanced-Deep-Reinforcement-Learning-for-End-to-End-Autonomous-Driving-with-BEV-Perception" class="headerlink" title="22. ME3-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ME$^3$-BEV__Mamba-Enhanced_Deep_Reinforcement_Learning_for_End-to-End_Autonomous_Driving_with_BEV-Pe.pdf">ME3-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了ME3-BEV框架，将鸟瞰视角（BEV）空间感知与Mamba时序建模模块结合，采用深度强化学习（DRL）实现端到端自动驾驶决策。方法包括SSA模块提升空间理解、TAFM模块增强时序特征提取，并通过语义分割提升可解释性。实验结果表明，ME3-BEV在CARLA多场景下显著降低碰撞率、提升轨迹准确性和整体驾驶评分，优于主流基线，具备实时性和安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ME3-BEV_Mamba-Enhanced_Deep_Reinforcement_Learning_for_End-to-End_Autonomous_Driving_with_BEV-Perception.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-ReNiL-Relative-Neural-Inertial-Locator-with-Any-Scale-Bayesian-Inference"><a href="#23-ReNiL-Relative-Neural-Inertial-Locator-with-Any-Scale-Bayesian-Inference" class="headerlink" title="23. ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ReNiL__Relative_Neural_Inertial_Locator_with_Any-Scale_Bayesian_Inference.pdf">ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wuhan University</span></p>
<p>本文提出了ReNiL，一个基于深度学习和贝叶斯推断的惯性定位系统，通过引入惯性定位需求点（IPDPs）和任意尺度Laplace估计器（ASLE），实现对任意尺度IMU序列的高效、灵活和不确定性一致的位移估计。实验结果表明，ReNiL在多数据集上具有领先的定位精度与不确定性估计能力，优于现有主流方法，并显著降低计算开销。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ReNiL_Relative_Neural_Inertial_Locator_with_Any-Scale_Bayesian_Inference.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-Hand-by-Hand-LLM-Driving-EMS-Assistant-for-Operational-Skill-Learning"><a href="#24-Hand-by-Hand-LLM-Driving-EMS-Assistant-for-Operational-Skill-Learning" class="headerlink" title="24. Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Hand_by_Hand__LLM_Driving_EMS_Assistant_for_Operational_Skill_Learning.pdf">Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了FlightAxis原型系统，将大语言模型（LLM）与电刺激（EMS）结合，通过“Align-Analyze-Adjust”流程支持和引导用户飞行技能训练。实验结果表明，系统能有效提升学员操作表现和高度保持能力，改善认知和学习体验，用户对LLM驱动的体感辅助表现出较高接受度，尤其适合新手技能训练。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Hand_by_Hand_LLM_Driving_EMS_Assistant_for_Operational_Skill_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-Dynamical-Trajectory-Planning-of-Disturbance-Consciousness-for-Air-Land-Bimodal-Unmanned-Aerial-Vehicles"><a href="#25-Dynamical-Trajectory-Planning-of-Disturbance-Consciousness-for-Air-Land-Bimodal-Unmanned-Aerial-Vehicles" class="headerlink" title="25. Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Dynamical_Trajectory_Planning_of_Disturbance_Consciousness_for_Air-Land_Bimodal_Unmanned_Aerial_Vehi.pdf">Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Guangdong Laboratory of Artificial Intelligence and Digital Economy (Shenzhen)</span></p>
<p>本文提出了一种面向空地双模无人车的扰动自适应轨迹规划框架，将实时扰动估计纳入路径搜索与轨迹优化流程，并通过扰动自适应安全边界调整机制动态修正车辆可行的动力边界，保障轨迹可行性。实验结果表明，该方法在真实环境与基准测试中显著提升了跟踪精度、任务效率及能耗表现，增强了系统在复杂扰动场景下的适应性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Dynamical_Trajectory_Planning_of_Disturbance_Consciousness_for_Air-Land_Bimodal_Unmanned_Aerial_Vehicles.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-Affordance-Guided-Dual-Armed-Disassembly-Teleoperation-for-Mating-Parts"><a href="#26-Affordance-Guided-Dual-Armed-Disassembly-Teleoperation-for-Mating-Parts" class="headerlink" title="26. Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Affordance-Guided_Dual-Armed_Disassembly_Teleoperation_for_Mating_Parts.pdf">Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Osaka University</span></p>
<p>本论文提出了一种基于可供性引导的双臂遥操作拆解系统，结合物体几何信息可视化抓取和拆卸方向，通过混合位置-阻抗控制实现顺应性拆解。实验表明，该系统提升了大型家电部件无损拆卸的成功率并降低了目标物体的姿态偏差，验证了混合控制和双臂策略在拆卸任务中的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Affordance-Guided_Dual-Armed_Disassembly_Teleoperation_for_Mating_Parts.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-Latent-Policy-Barrier-Learning-Robust-Visuomotor-Policies-by-Staying-In-Distribution"><a href="#27-Latent-Policy-Barrier-Learning-Robust-Visuomotor-Policies-by-Staying-In-Distribution" class="headerlink" title="27. Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Latent_Policy_Barrier__Learning_Robust_Visuomotor_Policies_by_Staying_In-Distribution.pdf">Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>本文提出Latent Policy Barrier (LPB) 框架，通过引入基于视觉编码的隐空间障碍，将专家演示的分布视为安全边界，利用扩散模型学习基础策略，仅使用专家数据确保模仿精度，并结合在专家和回滚数据上训练的视觉隐空间动力学模型，在推理时对行动进行梯度引导，动态修正偏离专家分布的状态。实验证明，LPB相比以往方法在仿真及真实机器人操作任务中，能够显著提升策略鲁棒性和样本效率，减少人工干预，实现从有限专家数据中可靠的操作控制。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Latent_Policy_Barrier_Learning_Robust_Visuomotor_Policies_by_Staying_In-Distribution.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="28-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion"><a href="#28-WGAST-Weakly-Supervised-Generative-Network-for-Daily-10-m-Land-Surface-Temperature-Estimation-via-Spatio-Temporal-Fusion" class="headerlink" title="28. WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/WGAST__Weakly-Supervised_Generative_Network_for_Daily_10_m_Land_Surface_Temperature_Estimation_via_S.pdf">WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">INSA Centre Val de Loire</span></p>
<p>本文提出WGAST，一种基于条件生成对抗网络（cGAN）的弱监督生成模型，实现Terra MODIS、Landsat 8和Sentinel-2多源遥感卫星数据的时空融合，生成每日10米分辨率地表温度（LST）。方法包括多尺度特征提取、余弦相似性特征融合、自适应归一化、时序注意力机制及高频噪声抑制，通过物理平均原则弱监督训练。实验表明，WGAST在定量和定性评测中显著优于现有方法，平均RMSE降低17.18%，SSIM提升4.10%，可有效重建高分辨率LST并克服云层遮挡问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/WGAST_Weakly-Supervised_Generative_Network_for_Daily_10_m_Land_Surface_Temperature_Estimation_via_Spatio-Temporal_Fusion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-Text-Embedded-Swin-UMamba-for-DeepLesion-Segmentation"><a href="#29-Text-Embedded-Swin-UMamba-for-DeepLesion-Segmentation" class="headerlink" title="29. Text Embedded Swin-UMamba for DeepLesion Segmentation"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Text_Embedded_Swin-UMamba_for_DeepLesion_Segmentation.pdf">Text Embedded Swin-UMamba for DeepLesion Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Scientific Application Services, Center of Information Technology, NIH</span></p>
<p>本文提出了Text-Swin-UMamba模型，在Swin-UMamba分割骨干的多尺度解码器中集成Text Tower语言嵌入机制，将放射学报告中的短文本描述编码后与影像特征多层融合，实现对CT图像中病灶的精准分割。实验表明，该方法在DeepLesion数据集上Dice分数达到82%，在所有比对模型中表现最佳，融合文本和影像特征能显著提升分割性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Text_Embedded_Swin-UMamba_for_DeepLesion_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-TRUST-Leveraging-Text-Robustness-for-Unsupervised-Domain-Adaptation"><a href="#30-TRUST-Leveraging-Text-Robustness-for-Unsupervised-Domain-Adaptation" class="headerlink" title="30. TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/TRUST__Leveraging_Text_Robustness_for_Unsupervised_Domain_Adaptation.pdf">TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Catania</span></p>
<p>本文提出了TRUST方法，通过利用文本的鲁棒性改进图像分类中的无监督领域自适应。其方法包括：用语言模型从图像对应的文本描述生成目标域伪标签，并通过CLIP模型衡量图片与文本的语义相似性，估计伪标签的不确定性，用于加权分类损失，降低低质量文本带来的负面影响。此外，提出了一种多模态软对比学习损失，通过文本指导视觉模型的特征空间对齐，提升模型在复杂领域迁移下的泛化能力。实验结果表明，在DomainNet和GeoNet等经典与复杂领域迁移任务上，TRUST均优于现有方法，取得新的SOTA表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/TRUST_Leveraging_Text_Robustness_for_Unsupervised_Domain_Adaptation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-CLIPin-A-Non-contrastive-Plug-in-to-CLIP-for-Multimodal-Semantic-Alignment"><a href="#31-CLIPin-A-Non-contrastive-Plug-in-to-CLIP-for-Multimodal-Semantic-Alignment" class="headerlink" title="31. CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/CLIPin__A_Non-contrastive_Plug-in_to_CLIP_for_Multimodal_Semantic_Alignment.pdf">CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>该论文提出CLIPin，一种可无缝集成到CLIP架构中的非对比学习插件，通过引入在线-目标分支对图像和文本模态进行交互监督，并设计共享预投影器，有效整合对比和非对比学习以提升多模态语义对齐和特征表达。实验结果表明，CLIPin显著提升了自然图像和医学图像文本任务中的表现和泛化能力，支持多种对比学习框架，具备良好的可插拔性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/CLIPin_A_Non-contrastive_Plug-in_to_CLIP_for_Multimodal_Semantic_Alignment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-FVGen-Accelerating-Novel-View-Synthesis-with-Adversarial-Video-Diffusion-Distillation"><a href="#32-FVGen-Accelerating-Novel-View-Synthesis-with-Adversarial-Video-Diffusion-Distillation" class="headerlink" title="32. FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/FVGen__Accelerating_Novel-View_Synthesis_with_Adversarial_Video_Diffusion_Distillation.pdf">FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute for Creative Technologies</span></p>
<p>FVGen提出了一种加速3D场景重建中新视角生成的方法，采用视频扩散模型，通过引入GAN初始化和软化反向KL散度的分布匹配蒸馏，将多步降噪教师模型高效地压缩为仅需4步的学生模型。实验结果显示，该方法在视觉质量不损失甚至提升的情况下，将生成速度提升了90%以上，大幅提高了稀疏视角输入场景的重建效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/FVGen_Accelerating_Novel-View_Synthesis_with_Adversarial_Video_Diffusion_Distillation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-Text-as-Any-Modality-for-Zero-Shot-Classification-by-Consistent-Prompt-Tuning"><a href="#33-Text-as-Any-Modality-for-Zero-Shot-Classification-by-Consistent-Prompt-Tuning" class="headerlink" title="33. Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Text_as_Any-Modality_for_Zero-Shot_Classification_by_Consistent_Prompt_Tuning.pdf">Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Science and Technology</span></p>
<p>本文提出了TaAM-CPT方法，实现了通过一致的提示调优（Prompt Tuning）构建通用多模态表征模型，仅利用LLMs生成的文本数据，无需任何模态特定标注数据。该方法包含模态提示池、文本构造和模态对齐文本编码器，并通过设计模态内和模态间学习目标实现多模态类别无冲突扩展与高效特征一致性。实验结果表明，TaAM-CPT在视频、图像和音频等多模态零样本分类任务上取得了领先性能，验证了其通用性和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Text_as_Any-Modality_for_Zero-Shot_Classification_by_Consistent_Prompt_Tuning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-Text-guided-Visual-Prompt-DINO-for-Generic-Segmentation"><a href="#34-Text-guided-Visual-Prompt-DINO-for-Generic-Segmentation" class="headerlink" title="34. Text-guided Visual Prompt DINO for Generic Segmentation"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Text-guided_Visual_Prompt_DINO_for_Generic_Segmentation.pdf">Text-guided Visual Prompt DINO for Generic Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua Shenzhen International Graduate School, Tsinghua University</span></p>
<p>该论文提出Prompt-DINO，一种融合文本和视觉提示的新型通用分割模型，通过在编码初期实现早期多模态融合、引入视觉-文本顺序对齐的查询选择机制，以及开发RAP数据引擎生成大规模高质量训练样本，实现跨模态语义消歧与泛化提升。实验表明Prompt-DINO在开放世界分割和检测任务上取得了最新最优性能，显著扩展了模型的语义覆盖和适应能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Text-guided_Visual_Prompt_DINO_for_Generic_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-Street-View-Sociability-Interpretable-Analysis-of-Urban-Social-Behavior-Across-15-Cities"><a href="#35-Street-View-Sociability-Interpretable-Analysis-of-Urban-Social-Behavior-Across-15-Cities" class="headerlink" title="35. Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Street_View_Sociability__Interpretable_Analysis_of_Urban_Social_Behavior_Across_15_Cities.pdf">Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carnegie Mellon University</span></p>
<p>本论文提出利用街景图像（SVI）结合多模态大语言模型（MLLM），通过Mehta的社会行为分类法自动分析15座城市的街道社会互动类型，并与城市归属感、绿化和天空可见度等环境变量进行线性回归关联分析。结果显示，SVI中的社会行为类型与城市环境特征和归属感有显著相关性，验证了街景图像作为可解释、可扩展的社会行为研究工具的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Street_View_Sociability_Interpretable_Analysis_of_Urban_Social_Behavior_Across_15_Cities.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-Uncertainty-quantified-Rollout-Policy-Adaptation-for-Unlabelled-Cross-domain-Temporal-Grounding"><a href="#36-Uncertainty-quantified-Rollout-Policy-Adaptation-for-Unlabelled-Cross-domain-Temporal-Grounding" class="headerlink" title="36. Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Uncertainty-quantified_Rollout_Policy_Adaptation_for_Unlabelled_Cross-domain_Temporal_Grounding.pdf">Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Queen Mary University of London</span></p>
<p>本文提出了一种面向视频时序定位任务的数据高效无标注跨域自适应方法。核心方法为基于GRPO（Group Relative Policy Optimization）强化学习的Uncertainty-quantified Rollout Policy Adaptation（URPA），通过在无标注目标域视频上多次采样生成伪标签，并利用其方差估算不确定性，将置信权重引入奖励函数，指导模型关注可靠伪监督信号，实现仅需少量无标注目标域视频即可高效自适应。实验结果表明，该方法在六个跨域基准上超越或媲美全量数据的主流方法，有效提升了跨域时序定位的泛化性和实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Uncertainty-quantified_Rollout_Policy_Adaptation_for_Unlabelled_Cross-domain_Temporal_Grounding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-Advanced-Deep-Learning-Techniques-for-Accurate-Lung-Cancer-Detection-and-Classification"><a href="#37-Advanced-Deep-Learning-Techniques-for-Accurate-Lung-Cancer-Detection-and-Classification" class="headerlink" title="37. Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Advanced_Deep_Learning_Techniques_for_Accurate_Lung_Cancer_Detection_and_Classification.pdf">Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Vigo</span></p>
<p>该论文提出了一种基于DenseNet201的深度学习方法，结合Focal Loss、数据增强和正则化等技术，有效解决了肺癌CT影像分类中的数据不平衡和过拟合问题。实验结果显示，DenseNet201模型在多种肺癌亚型分类上取得了98.95%的高准确率，优于主流模型，具备临床实际应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Advanced_Deep_Learning_Techniques_for_Accurate_Lung_Cancer_Detection_and_Classification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-Depth-Jitter-Seeing-through-the-Depth"><a href="#38-Depth-Jitter-Seeing-through-the-Depth" class="headerlink" title="38. Depth Jitter: Seeing through the Depth"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Depth_Jitter__Seeing_through_the_Depth.pdf">Depth Jitter: Seeing through the Depth</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Université de Toulon</span></p>
<p>本文提出了一种新颖的深度感知数据增强方法Depth-Jitter，通过利用水下图像形成模型和单目深度估计，对高深度方差图像引入自适应或固定深度偏移，生成具有物理意义的深度扰动样本，从而增强模型在多标签水下图像分类中的鲁棒性和泛化能力。实验在UTDAC2020和FathomNet两个基准数据集上，Depth-Jitter在mAP、mAP@20和ROC AUC等指标上均优于传统增强方法，尤其在深度敏感环境下提升显著，验证了其在真实水下视觉任务中的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Depth_Jitter_Seeing_through_the_Depth.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-TEFormer-Texture-Aware-and-Edge-Guided-Transformer-for-Semantic-Segmentation-of-Urban-Remote-Sensing-Images"><a href="#39-TEFormer-Texture-Aware-and-Edge-Guided-Transformer-for-Semantic-Segmentation-of-Urban-Remote-Sensing-Images" class="headerlink" title="39. TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/TEFormer__Texture-Aware_and_Edge-Guided_Transformer_for_Semantic_Segmentation_of_Urban_Remote_Sensin.pdf">TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing University of Technology</span></p>
<p>该论文提出TEFormer，一种融合纹理感知模块（TaM）和边缘引导三分支解码器（Eg3Head）的Transformer架构，用于城市遥感图像语义分割。方法通过TaM增强细粒度纹理特征感知，Eg3Head和边缘引导特征融合模块（EgFFM）实现多尺度、细节和边缘特征的有效融合，提升类别区分和边界精度。实验表明TEFormer在Potsdam、Vaihingen和LoveDA数据集上取得了领先的分割性能，边界和细节表现尤为突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/TEFormer_Texture-Aware_and_Edge-Guided_Transformer_for_Semantic_Segmentation_of_Urban_Remote_Sensing_Images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-MA-CBP-A-Criminal-Behavior-Prediction-Framework-Based-on-Multi-Agent-Asynchronous-Collaboration"><a href="#40-MA-CBP-A-Criminal-Behavior-Prediction-Framework-Based-on-Multi-Agent-Asynchronous-Collaboration" class="headerlink" title="40. MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/MA-CBP__A_Criminal_Behavior_Prediction_Framework_Based_on_Multi-Agent_Asynchronous_Collaboration.pdf">MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Mechatronical Engineering, Beijing Institute of Technology</span></p>
<p>本文提出了MA-CBP框架，通过多智能体异步协作，将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，并融合邻近图像帧，实现对短期与长期行为上下文的联合推理。实验结果显示，该方法在犯罪行为早期预警与检测任务上优于现有生成式和异常检测模型，有效提升了城市公共安全场景下的风险预警能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/MA-CBP_A_Criminal_Behavior_Prediction_Framework_Based_on_Multi-Agent_Asynchronous_Collaboration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-Can-Diffusion-Models-Bridge-the-Domain-Gap-in-Cardiac-MR-Imaging"><a href="#41-Can-Diffusion-Models-Bridge-the-Domain-Gap-in-Cardiac-MR-Imaging" class="headerlink" title="41. Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Can_Diffusion_Models_Bridge_the_Domain_Gap_in_Cardiac_MR_Imaging_.pdf">Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Centre for Doctoral Training in AI for Medical Diagnosis and Care, School of Computing, University of Leeds</span></p>
<p>本论文提出了一种基于源域训练扩散模型（SD-DM）的方法，通过参考图像指导扩散过程，生成结构一致且分布对齐的合成心脏MR图像，用于数据稀缺、多中心域下的医学影像分割。方法支持测试时输入域适配和训练-测试混合适配，实现领域泛化和领域自适应，无需迁移学习或在线训练，显著提升未见域分割性能，减少对人工标注和多中心采集的依赖。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Can_Diffusion_Models_Bridge_the_Domain_Gap_in_Cardiac_MR_Imaging_.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-Deepfake-Detection-that-Generalizes-Across-Benchmarks"><a href="#42-Deepfake-Detection-that-Generalizes-Across-Benchmarks" class="headerlink" title="42. Deepfake Detection that Generalizes Across Benchmarks"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Deepfake_Detection_that_Generalizes_Across_Benchmarks.pdf">Deepfake Detection that Generalizes Across Benchmarks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Czech Technical University in Prague</span></p>
<p>本文提出LNCLIP-DF方法，通过对预训练CLIP视觉编码器仅微调Layer Normalization参数、L2归一化特征、引入slerp特征空间增强及统一-对齐损失，实现了深度伪造检测的高效泛化。大量跨13个基准数据集的实验显示，该方法在大多数基准上优于复杂模型，且强调了使用真实-伪造配对训练集对提升泛化和防止捷径学习的重要性。结论指出，精心设计的数据配对和训练集多样性是构建强泛化性深度伪造检测器的关键。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Deepfake_Detection_that_Generalizes_Across_Benchmarks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-InfoCausalQA-Can-Models-Perform-Non-explicit-Causal-Reasoning-Based-on-Infographic"><a href="#43-InfoCausalQA-Can-Models-Perform-Non-explicit-Causal-Reasoning-Based-on-Infographic" class="headerlink" title="43. InfoCausalQA: Can Models Perform Non-explicit Causal Reasoning Based on Infographic?"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/InfoCausalQA_Can_Models_Perform_Non-explicit_Causal_Reasoning_Based_on_Infographic_.pdf">InfoCausalQA: Can Models Perform Non-explicit Causal Reasoning Based on Infographic?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Yonsei University</span></p>
<p>该论文提出InfoCausalQA，这是首个用于评估视觉语言模型（VLMs）在信息图基础上进行因果推理能力的基准，包括基于数字趋势的定量因果推理和涉及五类因果关系的语义因果推理。实验结果显示，现有VLMs在基于信息图的因果推理任务上表现显著低于人类，凸显多模态模型在深层次因果推理上的不足。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/InfoCausalQA_Can_Models_Perform_Non-explicit_Causal_Reasoning_Based_on_Infographic_.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-Clinically-guided-Data-Synthesis-for-Laryngeal-Lesion-Detection"><a href="#44-Clinically-guided-Data-Synthesis-for-Laryngeal-Lesion-Detection" class="headerlink" title="44. Clinically-guided Data Synthesis for Laryngeal Lesion Detection"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Clinically-guided_Data_Synthesis_for_Laryngeal_Lesion_Detection.pdf">Clinically-guided Data Synthesis for Laryngeal Lesion Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Istituto Italiano di Tecnologia</span></p>
<p>本文提出了一种结合Latent Diffusion Model (LDM) 与 ControlNet 的临床引导合成方法，用于生成喉部内窥镜图像-注释对，通过临床观察进行条件控制以提升图像的多样性与真实性。实验表明，加入不确定性策略筛选的10%合成数据可使喉部病变检测精度在内部数据集提升9%，外部数据集提升22.1%，且专家难以区分真实与合成图像，验证了合成样本的临床应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Clinically-guided_Data_Synthesis_for_Laryngeal_Lesion_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-Fewer-Denoising-Steps-or-Cheaper-Per-Step-Inference-Towards-Compute-Optimal-Diffusion-Model-Deployment"><a href="#45-Fewer-Denoising-Steps-or-Cheaper-Per-Step-Inference-Towards-Compute-Optimal-Diffusion-Model-Deployment" class="headerlink" title="45. Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Fewer_Denoising_Steps_or_Cheaper_Per-Step_Inference__Towards_Compute-Optimal_Diffusion_Model_Deploym.pdf">Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Georgia Institute of Technology</span></p>
<p>本文提出PostDiff，一种训练后无需微调即可用于加速扩散模型的训练自由压缩框架，分别在输入层（通过混合分辨率去噪策略）和模块层（通过混合模块缓存策略）减少推理冗余，提升效率。实验表明，相比减少去噪步数，降低每步推理成本通常能在保持高生成保真度的同时获得更优的效率-保真权衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Fewer_Denoising_Steps_or_Cheaper_Per-Step_Inference_Towards_Compute-Optimal_Diffusion_Model_Deployment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-Improving-Diagnostic-Accuracy-for-Oral-Cancer-with-inpainting-Synthesis-Lesions-Generated-Using-Diffusion-Models"><a href="#46-Improving-Diagnostic-Accuracy-for-Oral-Cancer-with-inpainting-Synthesis-Lesions-Generated-Using-Diffusion-Models" class="headerlink" title="46. Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Improving_Diagnostic_Accuracy_for_Oral_Cancer_with_inpainting_Synthesis_Lesions_Generated_Using_Diff.pdf">Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hongik University</span></p>
<p>本论文提出利用细调Stable Diffusion扩散模型结合Segment Anything Model (SAM) 分割与inpainting技术，生成高保真口腔癌病变合成图像，显著丰富了训练数据集多样性。通过将这些合成图像用于诊断模型训练，ResNet-50分类模型准确率提升至0.97，YOLO v8检测模型精度提升至0.85，验证了合成数据在医疗影像AI诊断中的有效性和可推广性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Improving_Diagnostic_Accuracy_for_Oral_Cancer_with_inpainting_Synthesis_Lesions_Generated_Using_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-DiffCap-Diffusion-based-Real-time-Human-Motion-Capture-using-Sparse-IMUs-and-a-Monocular-Camera"><a href="#47-DiffCap-Diffusion-based-Real-time-Human-Motion-Capture-using-Sparse-IMUs-and-a-Monocular-Camera" class="headerlink" title="47. DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/DiffCap__Diffusion-based_Real-time_Human_Motion_Capture_using_Sparse_IMUs_and_a_Monocular_Camera.pdf">DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出DiffCap方法，首次将扩散模型应用于稀疏IMU与单目摄像头信号融合，实现实时高精度人体运动捕捉。方法核心在于将视觉信号序列编码为全局条件嵌入，IMU信号逐帧输入，采用两阶段扩散模型先预测3D关节点再回归姿态，显著提升了在遮挡和视野外等复杂场景下的鲁棒性和准确率。实验证明DiffCap在多个公开数据集上超越现有最优方法，尤其在遮挡、动态及野外场景下表现优越。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/DiffCap_Diffusion-based_Real-time_Human_Motion_Capture_using_Sparse_IMUs_and_a_Monocular_Camera.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-SC-Captioner-Improving-Image-Captioning-with-Self-Correction-by-Reinforcement-Learning"><a href="#48-SC-Captioner-Improving-Image-Captioning-with-Self-Correction-by-Reinforcement-Learning" class="headerlink" title="48. SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SC-Captioner__Improving_Image_Captioning_with_Self-Correction_by_Reinforcement_Learning.pdf">SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本文提出SC-Captioner，一种针对图像描述任务的自我修正训练框架，通过多轮强化学习及新颖的奖励函数设计（结合对象、属性、关系的场景图解析）引导大型视觉语言模型在修正初始描述时精准移除幻觉、补全遗漏细节。实验表明，该方法在多个指标和场景下均优于直接偏好优化等主流方法，显著提升了图像描述质量和可靠性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SC-Captioner_Improving_Image_Captioning_with_Self-Correction_by_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-Ensemble-Based-Graph-Representation-of-fMRI-Data-for-Cognitive-Brain-State-Classification"><a href="#49-Ensemble-Based-Graph-Representation-of-fMRI-Data-for-Cognitive-Brain-State-Classification" class="headerlink" title="49. Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Ensemble-Based_Graph_Representation_of_fMRI_Data_for_Cognitive_Brain_State_Classification.pdf">Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute for Cognitive Neuroscience of University Higher School of Economics</span></p>
<p>该论文提出了一种基于集成学习的图表示方法，将fMRI数据转化为功能连接网络，并利用多个基础机器学习模型计算边权，实现二元脑状态分类。方法在七种认知任务上通过简单逻辑回归和图神经网络进行验证，集成图表现出更高的分类准确率（97.07%至99.74%），显著优于传统相关性图。结论表明集成图能更好地捕捉脑网络拓扑信息，提高脑状态判别力，且方法具有良好的可扩展性和可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Ensemble-Based_Graph_Representation_of_fMRI_Data_for_Cognitive_Brain_State_Classification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-Mask-Match-Learning-to-Recognize-Handwritten-Math-with-Self-Supervised-Attention"><a href="#50-Mask-Match-Learning-to-Recognize-Handwritten-Math-with-Self-Supervised-Attention" class="headerlink" title="50. Mask &amp; Match: Learning to Recognize Handwritten Math with Self-Supervised Attention"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Mask_&_Match__Learning_to_Recognize_Handwritten_Math_with_Self-Supervised_Attention.pdf">Mask &amp; Match: Learning to Recognize Handwritten Math with Self-Supervised Attention</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">International Institute of Information Technology Hyderabad</span></p>
<p>本文提出了一种针对手写数学表达式识别（HMER）的自监督学习框架，通过结合全局和局部对比损失进行图像编码器预训练，并引入渐进式空间掩码训练的自监督注意力机制，提升模型对复杂结构的理解能力。实验结果显示，该方法在CROHME基准上优于现有的自监督和全监督方法，渐进式掩码注意力机制显著增强了模型的结构鲁棒性和识别准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Mask_%26_Match_Learning_to_Recognize_Handwritten_Math_with_Self-Supervised_Attention.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-UGD-IML-A-Unified-Generative-Diffusion-based-Framework-for-Constrained-and-Unconstrained-Image-Manipulation-Localization"><a href="#51-UGD-IML-A-Unified-Generative-Diffusion-based-Framework-for-Constrained-and-Unconstrained-Image-Manipulation-Localization" class="headerlink" title="51. UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/UGD-IML__A_Unified_Generative_Diffusion-based_Framework_for_Constrained_and_Unconstrained_Image_Mani.pdf">UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology</span></p>
<p>该论文提出了UGD-IML框架，基于生成扩散模型，首次统一了解析和约束两种图像篡改定位任务，通过条件控制模块、噪声添加模块和去噪模块实现输入可控任务切换，并采用参数共享和类别嵌入机制提升效率。实验证明，UGD-IML在多数据集上对现有方法有显著性能提升，具备更高的准确性、鲁棒性和不确定性评估能力，有效支持真实场景下的图像真伪检测。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/UGD-IML_A_Unified_Generative_Diffusion-based_Framework_for_Constrained_and_Unconstrained_Image_Manipulation_Localization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-Transformer-Based-Explainable-Deep-Learning-for-Breast-Cancer-Detection-in-Mammography-The-MammoFormer-Framework"><a href="#52-Transformer-Based-Explainable-Deep-Learning-for-Breast-Cancer-Detection-in-Mammography-The-MammoFormer-Framework" class="headerlink" title="52. Transformer-Based Explainable Deep Learning for Breast Cancer Detection in Mammography: The MammoFormer Framework"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Transformer-Based_Explainable_Deep_Learning_for_Breast_Cancer_Detection_in_Mammography__The_MammoFor.pdf">Transformer-Based Explainable Deep Learning for Breast Cancer Detection in Mammography: The MammoFormer Framework</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Morgan State University</span></p>
<p>该论文提出了MammoFormer框架，将多种Transformer架构（如ViT、Swin Transformer）与CNN、ConvNeXt等模型及多特征增强（原图、负片、AHE、HOG）联合，用于乳腺X光图像的癌症检测，并系统集成了Integrated Gradients、GradCAM等五种可解释性AI（XAI）技术。实验结果显示，通过针对架构优化的特征增强，Transformer模型在HOG等预处理下可达98.4%准确率，性能与CNN相当且具备更强的全局上下文建模和多角度可解释性，为乳腺AI筛查提供高精度和透明的诊断支持。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Transformer-Based_Explainable_Deep_Learning_for_Breast_Cancer_Detection_in_Mammography_The_MammoFormer_Framework.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-SAM-Encoder-Breach-by-Adversarial-Simplicial-Complex-Triggers-Downstream-Model-Failures"><a href="#53-SAM-Encoder-Breach-by-Adversarial-Simplicial-Complex-Triggers-Downstream-Model-Failures" class="headerlink" title="53. SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SAM_Encoder_Breach_by_Adversarial_Simplicial_Complex_Triggers_Downstream_Model_Failures.pdf">SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Jiaotong University</span></p>
<p>本文提出了一种针对Segment Anything Model (SAM)视觉基础模型的高效可迁移对抗攻击方法——Vertex-Refining Simplicial Complex Attack (VeSCA)。该方法仅依赖SAM的编码器，通过参数化单纯形复合体显式建模并精炼与下游模型共享的脆弱子空间，结合轻量级领域再适应策略，实现了对多类下游任务模型的高效攻击。实验结果显示，VeSCA在五个视觉分割相关的数据集和多种下游任务上，攻击性能比当前最优方法提升12.7%，验证了SAM脆弱性对其下游生态的系统性风险。结论是，VeSCA可系统性暴露视觉基础模型及其下游的安全风险，提示需加强基础模型鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SAM_Encoder_Breach_by_Adversarial_Simplicial_Complex_Triggers_Downstream_Model_Failures.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-Learning-Representations-of-Satellite-Images-with-Evaluations-on-Synoptic-Weather-Events"><a href="#54-Learning-Representations-of-Satellite-Images-with-Evaluations-on-Synoptic-Weather-Events" class="headerlink" title="54. Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Learning_Representations_of_Satellite_Images_with_Evaluations_on_Synoptic_Weather_Events.pdf">Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Taiwan University</span></p>
<p>本文通过对卫星影像应用主成分分析（PCA）、卷积自编码器（CAE）以及预训练残差网络（ResNet50）等表征学习算法，结合可解释分类方法（如逻辑回归），实现多种天气事件的自动识别。实验结果表明CAE在所有分类任务上威胁评分最高，并且高分辨率卫星数据能显著提升其表现；相比之下，PCA有较高的识别率但误报率也更高。结论：CAE是一种高效且表现优异的卫星图像表征学习方法，尤其适合高分辨率数据，但物理可解释性仍需改进。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Learning_Representations_of_Satellite_Images_with_Evaluations_on_Synoptic_Weather_Events.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-SynSeg-Feature-Synergy-for-Multi-Category-Contrastive-Learning-in-Open-Vocabulary-Semantic-Segmentation"><a href="#55-SynSeg-Feature-Synergy-for-Multi-Category-Contrastive-Learning-in-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="55. SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SynSeg__Feature_Synergy_for_Multi-Category_Contrastive_Learning_in_Open-Vocabulary_Semantic_Segmenta.pdf">SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Global Innovation Exchange</span></p>
<p>该论文提出SynSeg方法，通过Feature Synergy Structure（FSS）与Multi-Category Contrastive Learning（MCCL）结合，实现开放词汇语义分割的弱监督训练。FSS融合视觉和语义激活图生成判别特征，MCCL则同时强化类内和类间对比学习，有效提升分割性能。实验结果表明，SynSeg在五个主流数据集上显著超越现有弱监督方法，实现了最佳mIoU表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SynSeg_Feature_Synergy_for_Multi-Category_Contrastive_Learning_in_Open-Vocabulary_Semantic_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-MCA-2D-3D-Retrieval-with-Noisy-Labels-via-Multi-level-Adaptive-Correction-and-Alignment"><a href="#56-MCA-2D-3D-Retrieval-with-Noisy-Labels-via-Multi-level-Adaptive-Correction-and-Alignment" class="headerlink" title="56. MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/MCA__2D-3D_Retrieval_with_Noisy_Labels_via_Multi-level_Adaptive_Correction_and_Alignment.pdf">MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了一种面向2D-3D跨模态检索的多层次自适应纠正与对齐框架（MCA），包括多模态联合标签纠正（MJC）机制和多层次自适应对齐（MAA）策略。MJC利用多模态历史自预测实现标签纠正，MAA则在中心、组和实例三个层次自适应对齐特征，显著提升了在噪声标签下的跨模态检索准确性和鲁棒性。实验表明，该方法在合成和真实噪声3D数据集上均实现了最优性能，有效缓解了标签噪声带来的影响。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/MCA_2D-3D_Retrieval_with_Noisy_Labels_via_Multi-level_Adaptive_Correction_and_Alignment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-E-React-Towards-Emotionally-Controlled-Synthesis-of-Human-Reactions"><a href="#57-E-React-Towards-Emotionally-Controlled-Synthesis-of-Human-Reactions" class="headerlink" title="57. E-React: Towards Emotionally Controlled Synthesis of Human Reactions"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/E-React__Towards_Emotionally_Controlled_Synthesis_of_Human_Reactions.pdf">E-React: Towards Emotionally Controlled Synthesis of Human Reactions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southeast University, China</span></p>
<p>该论文提出了E-React框架，实现了基于半监督情感先验的情感驱动反应动作生成。方法包括用少量标注数据和大量未标注数据训练情感先验，通过对称的actor-reactor扩散模型结合空间关系和情感嵌入生成自然多样的人体反应动作。实验表明，本方法在情感一致性与交互自然性上优于现有方法，提升了多人物互动动作生成的真实感与多样性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/E-React_Towards_Emotionally_Controlled_Synthesis_of_Human_Reactions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-Towards-MR-Based-Trochleoplasty-Planning"><a href="#58-Towards-MR-Based-Trochleoplasty-Planning" class="headerlink" title="58. Towards MR-Based Trochleoplasty Planning"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Towards_MR-Based_Trochleoplasty_Planning.pdf">Towards MR-Based Trochleoplasty Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Basel</span></p>
<p>该论文提出了一套基于MR影像的三维髁沟成形术（trochleoplasty）手术规划管线。方法包括利用隐式神经表示（INR）实现MR图像超分辨率重建，结合自定义多标签分割网络对膝关节结构进行分割，并采用小波扩散模型（WDM）对病变区域进行伪健康形态生成。实验表明，该方法在25例患者中显著改善了髁沟角和深度，为术前和术中提供了高分辨率、个性化的三维手术蓝图，且无需CT，降低了辐射风险。结论：该管线提升了手术一致性与安全性，有望推广到更多外科应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Towards_MR-Based_Trochleoplasty_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-AGI-for-the-Earth-the-path-possibilities-and-how-to-evaluate-intelligence-of-models-that-work-with-Earth-Observation-Data"><a href="#59-AGI-for-the-Earth-the-path-possibilities-and-how-to-evaluate-intelligence-of-models-that-work-with-Earth-Observation-Data" class="headerlink" title="59. AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/AGI_for_the_Earth,_the_path,_possibilities_and_how_to_evaluate_intelligence_of_models_that_work_with.pdf">AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HUM.AI</span></p>
<p>本文系统梳理了地球观测（EO）数据对实现人工通用智能（AGI）的重要性，并分析了现有EO基准测试的不足。作者提出了一套涵盖多种卫星影像任务的分类框架，强调应建立一个覆盖场景分类、变化检测、场景生成及反事实分析等多任务、跨模态的综合性基准，以全面评估和推动AGI在地球观测领域的能力。结论指出现有基准难以充分衡量AGI模型泛化与推理能力，呼吁社区协作完善多任务基准，助力环境与可持续发展挑战应对。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/AGI_for_the_Earth%2C_the_path%2C_possibilities_and_how_to_evaluate_intelligence_of_models_that_work_with_Earth_Observation_Data_.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-Learning-3D-Texture-Aware-Representations-for-Parsing-Diverse-Human-Clothing-and-Body-Parts"><a href="#60-Learning-3D-Texture-Aware-Representations-for-Parsing-Diverse-Human-Clothing-and-Body-Parts" class="headerlink" title="60. Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Learning_3D_Texture-Aware_Representations_for_Parsing_Diverse_Human_Clothing_and_Body_Parts.pdf">Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KTH Royal Institute of Technology</span></p>
<p>该论文提出了Spectrum模型，通过重新利用I2Tx（Image-to-Texture）扩散模型的内部特征，实现对多人体场景中细粒度衣物和身体部位的逐像素语义分割。方法结合3D纹理映射和多模态文本引导分割，支持对新颖、未见服装类别的泛化。实验结果表明，Spectrum在多数据集下均优于现有基线方法，能准确解析复杂场景中的人体衣物和部位。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Learning_3D_Texture-Aware_Representations_for_Parsing_Diverse_Human_Clothing_and_Body_Parts.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-A-3DGS-Diffusion-Self-Supervised-Framework-for-Normal-Estimation-from-a-Single-Image"><a href="#61-A-3DGS-Diffusion-Self-Supervised-Framework-for-Normal-Estimation-from-a-Single-Image" class="headerlink" title="61. A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/A_3DGS-Diffusion_Self-Supervised_Framework_for_Normal_Estimation_from_a_Single_Image.pdf">A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Jiangnan University</span></p>
<p>该论文提出了一种名为SINGAD的自监督单幅图像法线估计算法，结合3D Gaussian Splatting（3DGS）物理建模与条件扩散模型，通过物理光交互建模、跨域特征融合及可微3D重投影损失，将3D几何误差直接转化为法线优化信号，实现无标注数据下多视角几何一致的法线估计。实验结果表明，该方法在Google Scanned Objects数据集上超越现有主流方法，显著提升了几何一致性和细节重建能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/A_3DGS-Diffusion_Self-Supervised_Framework_for_Normal_Estimation_from_a_Single_Image.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-Improved-Sub-Visible-Particle-Classification-in-Flow-Imaging-Microscopy-via-Generative-AI-Based-Image-Synthesis"><a href="#62-Improved-Sub-Visible-Particle-Classification-in-Flow-Imaging-Microscopy-via-Generative-AI-Based-Image-Synthesis" class="headerlink" title="62. Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Improved_Sub-Visible_Particle_Classification_in_Flow_Imaging_Microscopy_via_Generative_AI-Based_Imag.pdf">Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ghent University Global Campus</span></p>
<p>该论文提出利用扩散生成模型（diffusion models）为流式成像显微镜下亚可见颗粒（SvPs）分类中的少数类（如硅油滴和气泡）合成高保真图像，并用这些生成样本扩充不平衡训练集，显著提升多类深度神经网络（如ResNet-50）在高不平衡真实验证集上的分类精度。实验显示，融合生成图像的数据增强方案能有效缓解类别不平衡问题，提升药品质量控制中微粒自动识别的鲁棒性和准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Improved_Sub-Visible_Particle_Classification_in_Flow_Imaging_Microscopy_via_Generative_AI-Based_Image_Synthesis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-KnapFormer-An-Online-Load-Balancer-for-Efficient-Diffusion-Transformers-Training"><a href="#63-KnapFormer-An-Online-Load-Balancer-for-Efficient-Diffusion-Transformers-Training" class="headerlink" title="63. KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/KnapFormer__An_Online_Load_Balancer_for_Efficient_Diffusion_Transformers_Training.pdf">KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Adobe</span></p>
<p>KnapFormer是一种高效的在线序列块级负载均衡算法，专为分布式训练Diffusion Transformers（DiT）模型设计，通过解决跨GPU的动态token长度不均衡问题，实现了序列并行处理与工作负载最优分配。该方法集成了DeepSpeed-Ulysses分布式注意力机制，仅需一次all-to-all通信即可实现全局负载均衡，显著提升了训练吞吐率，在多模态、混合分辨率和图像视频联合训练场景下可获得2至3倍加速，消除straggler效应。结论：KnapFormer可大幅提升异构数据分布下的DiT模型训练效率，并简化系统设计。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/KnapFormer_An_Online_Load_Balancer_for_Efficient_Diffusion_Transformers_Training.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-Mediator-Guided-Multi-Agent-Collaboration-among-Open-Source-Models-for-Medical-Decision-Making"><a href="#64-Mediator-Guided-Multi-Agent-Collaboration-among-Open-Source-Models-for-Medical-Decision-Making" class="headerlink" title="64. Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Mediator-Guided_Multi-Agent_Collaboration_among_Open-Source_Models_for_Medical_Decision-Making.pdf">Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本文提出MedOrch，一个基于开源多模态视觉语言模型（VLM）和大语言模型（LLM）中介者的多智能体协作框架，用于医学多模态决策。其流程包括多VLM专家独立作答、LLM中介者开展Socratic问答引导反思、专家修正答案，最终由判决智能体综合输出。实验在五个医学视觉问答基准上验证了该方法无需训练即可显著超越单一模型和传统多智能体策略，提升医学多模态智能决策性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Mediator-Guided_Multi-Agent_Collaboration_among_Open-Source_Models_for_Medical_Decision-Making.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-ECMF-Enhanced-Cross-Modal-Fusion-for-Multimodal-Emotion-Recognition-in-MER-SEMI-Challenge"><a href="#65-ECMF-Enhanced-Cross-Modal-Fusion-for-Multimodal-Emotion-Recognition-in-MER-SEMI-Challenge" class="headerlink" title="65. ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ECMF__Enhanced_Cross-Modal_Fusion_for_Multimodal_Emotion_Recognition_in_MER-SEMI_Challenge.pdf">ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Laboratory of General Artificial Intelligence, BIGAI</span></p>
<p>该论文提出了一种多模态情感识别框架，结合预训练音频（HuBERT-Large）、文本（Chinese-RoBERTa-wwm-ext-large，GPT-4，Qwen-Omni）和视觉（OpenFace，CLIP-ViT-Large）特征提取器，通过自注意力机制与残差连接实现多模态特征融合，并通过多源标签策略提升数据质量。实验结果显示，该方法在MER2025-SEMI数据集上加权F1显著优于官方基线，验证了框架的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ECMF_Enhanced_Cross-Modal_Fusion_for_Multimodal_Emotion_Recognition_in_MER-SEMI_Challenge.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-Neural-Field-Representations-of-Mobile-Computational-Photography"><a href="#66-Neural-Field-Representations-of-Mobile-Computational-Photography" class="headerlink" title="66. Neural Field Representations of Mobile Computational Photography"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Neural_Field_Representations_of_Mobile_Computational_Photography.pdf">Neural Field Representations of Mobile Computational Photography</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Princeton University</span></p>
<p>本文提出了多种基于神经场（neural field）的紧凑模型，用于移动设备摄影数据的深度估计、图层分离和全景图像拼接。通过直接对手机采集的原始多帧数据进行端到端优化，无需复杂预处理或标注，实现了高质量的深度重建、前景&#x2F;背景分离和视角扩展，在多个真实场景和基准测试中超越了现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Neural_Field_Representations_of_Mobile_Computational_Photography.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="67-Effective-Training-Data-Synthesis-for-Improving-MLLM-Chart-Understanding"><a href="#67-Effective-Training-Data-Synthesis-for-Improving-MLLM-Chart-Understanding" class="headerlink" title="67. Effective Training Data Synthesis for Improving MLLM Chart Understanding"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Effective_Training_Data_Synthesis_for_Improving_MLLM_Chart_Understanding.pdf">Effective Training Data Synthesis for Improving MLLM Chart Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Australian National University</span></p>
<p>论文提出了一种五步图表数据合成管道，包括分离数据和函数的单图生成、条件式多子图生成、视觉多样化、质量过滤和基于GPT-4o的问答对生成，构建了高质量的Effective Chart Dataset (ECD)。实验表明，使用ECD微调多种开源多模态大语言模型能在真实和合成测试集上显著提升图表理解准确率，尤其是在复杂科学图表领域，优于现有训练集。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Effective_Training_Data_Synthesis_for_Improving_MLLM_Chart_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-LLM-Unlearning-using-Gradient-Ratio-Based-Influence-Estimation-and-Noise-Injection"><a href="#68-LLM-Unlearning-using-Gradient-Ratio-Based-Influence-Estimation-and-Noise-Injection" class="headerlink" title="68. LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LLM_Unlearning_using_Gradient_Ratio-Based_Influence_Estimation_and_Noise_Injection.pdf">LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Urbana Champaign</span></p>
<p>本文提出GRIN，一种面向大语言模型（LLM）精准高效的机器遗忘方法。方法首先利用梯度比率衡量参数对待遗忘数据的影响力，定位出最关键的权重，并在这些权重注入噪声后进行有针对性的微调，以提升遗忘效果并减少对保留知识的损害。实验在TOFU、WMDP和SafePKU等基准数据集上验证，GRIN能够在有效删除敏感或有害信息的同时保持模型整体性能。结论表明GRIN兼具高效、模块化和实际可用性，适用于法律合规和安全需求场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LLM_Unlearning_using_Gradient_Ratio-Based_Influence_Estimation_and_Noise_Injection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-ScamAgents-How-AI-Agents-Can-Simulate-Human-Level-Scam-Calls"><a href="#69-ScamAgents-How-AI-Agents-Can-Simulate-Human-Level-Scam-Calls" class="headerlink" title="69. ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ScamAgents__How_AI_Agents_Can_Simulate_Human-Level_Scam_Calls.pdf">ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Rutgers University</span></p>
<p>该论文提出ScamAgent，一个基于大型语言模型（LLM）的自主多回合智能代理系统，可自动生成高度逼真的诈骗电话脚本，并集成对话记忆、动态适应和文本转语音（TTS）技术。实验证明，ScamAgent能有效规避现有LLM安全机制，成功执行多场景诈骗对话，显示多回合、代理型滥用是现有安全防护的显著漏洞。结论指出，亟需多回合安全审计、代理级控制框架及新型对抗性检测方法，以应对生成式AI推动的对话欺诈风险。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ScamAgents_How_AI_Agents_Can_Simulate_Human-Level_Scam_Calls.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-Learning-the-Topic-Not-the-Language-How-LLMs-Classify-Online-Immigration-Discourse-Across-Languages"><a href="#70-Learning-the-Topic-Not-the-Language-How-LLMs-Classify-Online-Immigration-Discourse-Across-Languages" class="headerlink" title="70. Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Learning_the_Topic,_Not_the_Language__How_LLMs_Classify_Online_Immigration_Discourse_Across_Language.pdf">Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harvard University</span></p>
<p>该论文提出通过对开源LLaMA 3.2–3B模型在单语、双语及多语种数据集上LoRA微调，探索少量语言微调能否实现跨语言的移民推文主题与立场分类。研究发现：单&#x2F;双语微调模型能可靠检测多种未见语言中的移民相关内容，多语种微调则有助于更准确区分支持&#x2F;反对&#x2F;中立立场；只需极少量目标语言数据即可显著提升低资源语言表现并修正预训练偏见。此外，论文发布了4比特量化、可重现、推理速度快且成本极低的开源模型，为多语言社会科学研究提供了低门槛工具。结论是，跨语言主题检测无需大规模多语训练，有限语言覆盖即可实现主题泛化，结构性偏见可通过轻量干预修正。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Learning_the_Topic%2C_Not_the_Language_How_LLMs_Classify_Online_Immigration_Discourse_Across_Languages.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-Memp-Exploring-Agent-Procedural-Memory"><a href="#71-Memp-Exploring-Agent-Procedural-Memory" class="headerlink" title="71. Memp: Exploring Agent Procedural Memory"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Memp__Exploring_Agent_Procedural_Memory.pdf">Memp: Exploring Agent Procedural Memory</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了Memp框架，通过将以往代理任务轨迹蒸馏为细粒度操作指令与高层次脚本抽象，实现了可学习、可更新、终身进化的程序性记忆。Memp系统性地研究了记忆的构建、检索与动态更新策略，在TravelPlanner和ALFWorld等基准任务上，实验结果显示程序性记忆能显著提升代理的成功率和效率，同时具备持续学习和迁移能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Memp_Exploring_Agent_Procedural_Memory.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-Sample-efficient-LLM-Optimization-with-Reset-Replay"><a href="#72-Sample-efficient-LLM-Optimization-with-Reset-Replay" class="headerlink" title="72. Sample-efficient LLM Optimization with Reset Replay"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Sample-efficient_LLM_Optimization_with_Reset_Replay.pdf">Sample-efficient LLM Optimization with Reset Replay</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本文提出了LoRR（LLM optimization with Reset Replay），一种增强大语言模型（LLM）微调样本效率的通用插件。LoRR通过高重放训练（多次利用每批数据）、周期性参数重置（Shrink &amp; Perturb战略）及混合优化损失（结合SFT和偏好损失），显著提升了偏好优化方法在数学与推理任务上的表现。结论显示，LoRR能有效缓解早期过拟合和样本利用不足问题，迭代DPO结合LoRR在多个基准上超越复杂RL算法，极大提高了LLM微调的效能和数据利用率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Sample-efficient_LLM_Optimization_with_Reset_Replay.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-End-to-End-Text-to-SQL-with-Dataset-Selection-Leveraging-LLMs-for-Adaptive-Query-Generation"><a href="#73-End-to-End-Text-to-SQL-with-Dataset-Selection-Leveraging-LLMs-for-Adaptive-Query-Generation" class="headerlink" title="73. End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/End-to-End_Text-to-SQL_with_Dataset_Selection__Leveraging_LLMs_for_Adaptive_Query_Generation.pdf">End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Infoorigin Pvt Ltd</span></p>
<p>该论文提出了一种三阶段端到端Text-to-SQL框架，首先通过LLM与prompt工程生成隐式规则集，结合RoBERTa编码器模型进行自动数据库识别（db id prediction），然后利用数据库schema和NLQ进行prompt驱动SQL生成，最后通过多智能体自纠错模块提升SQL准确性。实验表明该方法在Spider数据集上数据库意图预测与SQL生成均超越现有SOTA，显著提升复杂查询场景下的执行准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/End-to-End_Text-to-SQL_with_Dataset_Selection_Leveraging_LLMs_for_Adaptive_Query_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-SpeakerLM-End-to-End-Versatile-Speaker-Diarization-and-Recognition-with-Multimodal-Large-Language-Models"><a href="#74-SpeakerLM-End-to-End-Versatile-Speaker-Diarization-and-Recognition-with-Multimodal-Large-Language-Models" class="headerlink" title="74. SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SpeakerLM__End-to-End_Versatile_Speaker_Diarization_and_Recognition_with_Multimodal_Large_Language_M.pdf">SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tongyi Lab</span></p>
<p>本文提出了SpeakerLM，这是首个针对端到端说话人分离与识别（SDR）任务的多模态大语言模型（MLLM），采用音频编码器、投影器与预训练文本LLM耦合，形成Encoder-Projector-LLM架构。其创新在于引入灵活的说话人注册机制，支持不同注册场景下的SDR，通过多阶段大规模数据训练，模型在多项基准上超越级联式SOTA系统，并在复杂场景下表现出极强的泛化与鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SpeakerLM_End-to-End_Versatile_Speaker_Diarization_and_Recognition_with_Multimodal_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-From-Explainable-to-Explanatory-Artificial-Intelligence-Toward-a-New-Paradigm-for-Human-Centered-Explanations-through-Generative-AI"><a href="#75-From-Explainable-to-Explanatory-Artificial-Intelligence-Toward-a-New-Paradigm-for-Human-Centered-Explanations-through-Generative-AI" class="headerlink" title="75. From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/From_Explainable_to_Explanatory_Artificial_Intelligence__Toward_a_New_Paradigm_for_Human-Centered_Ex.pdf">From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ruhr University Bochum</span></p>
<p>该论文提出了“Explanatory AI”新范式，利用生成式AI（如大语言模型）为用户生成叙事型、个性化、可交互的解释，区别于传统XAI仅关注算法透明性。通过构建八维概念模型并在医疗场景实证，结果显示用户更偏好情境敏感、易懂的解释，强调AI系统应以人类理解为核心而非仅算法透明。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/From_Explainable_to_Explanatory_Artificial_Intelligence_Toward_a_New_Paradigm_for_Human-Centered_Explanations_through_Generative_AI.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-Large-Language-Model-Data-Generation-for-Enhanced-Intent-Recognition-in-German-Speech"><a href="#76-Large-Language-Model-Data-Generation-for-Enhanced-Intent-Recognition-in-German-Speech" class="headerlink" title="76. Large Language Model Data Generation for Enhanced Intent Recognition in German Speech"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Large_Language_Model_Data_Generation_for_Enhanced_Intent_Recognition_in_German_Speech.pdf">Large Language Model Data Generation for Enhanced Intent Recognition in German Speech</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Hamburg</span></p>
<p>本论文提出结合Whisper自动语音识别（ASR）与BERT、DistilBERT和Electra等Transformer语言模型，利用LeoLM、Llama3和ChatGPT三种大语言模型自动生成德语意图识别任务的训练文本，提升针对老年德语语音的意图识别性能。实验结果表明，加入由LLM合成的数据能有效提升模型对不同说话风格和未见词汇的泛化和鲁棒性，其中专为德语设计的小型LeoLM生成的数据优于更大规模的ChatGPT数据，展示了本地领域特定LLM的优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Large_Language_Model_Data_Generation_for_Enhanced_Intent_Recognition_in_German_Speech.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-SIFThinker-Spatially-Aware-Image-Focus-for-Visual-Reasoning"><a href="#77-SIFThinker-Spatially-Aware-Image-Focus-for-Visual-Reasoning" class="headerlink" title="77. SIFThinker: Spatially-Aware Image Focus for Visual Reasoning"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SIFThinker__Spatially-Aware_Image_Focus_for_Visual_Reasoning.pdf">SIFThinker: Spatially-Aware Image Focus for Visual Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua Shenzhen International Graduate School, Tsinghua University</span></p>
<p>SIFThinker提出了一种空间感知的“think-with-images”多模态大语言模型框架，通过引入深度增强的区域关注机制，将3D空间信息与图像-文本链式推理相结合，并利用反向扩展-前向推理策略构建SIF-50K数据集。采用基于GRPO的空间感知强化学习方法（GRPO-SIF），集成多种奖励（格式、深度一致性、区域校正等）优化模型动态关注和推理能力。实验表明SIFThinker在空间理解与细粒度视觉感知方面超越现有SOTA模型，具有强鲁棒性和广泛适用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SIFThinker_Spatially-Aware_Image_Focus_for_Visual_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-In-Training-Defenses-against-Emergent-Misalignment-in-Language-Models"><a href="#78-In-Training-Defenses-against-Emergent-Misalignment-in-Language-Models" class="headerlink" title="78. In-Training Defenses against Emergent Misalignment in Language Models"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/In-Training_Defenses_against_Emergent_Misalignment_in_Language_Models.pdf">In-Training Defenses against Emergent Misalignment in Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bonn-Aachen International Center for Information Technology, University of Bonn</span></p>
<p>本文系统性研究了在大语言模型（LLM）微调过程中，防止涌现性失对齐（EMA）的实用正则化方法，包括KL散度正则化、特征空间L2距离保持（LDIFS）、安全子空间投影（SafeLoRA）以及安全数据插入（Interleaving）。实验结果表明，KL散度和插入安全数据能显著抑制EMA，但分别会带来学习能力受限和答案不连贯的副作用，仍需更精细的策略以实现安全与效能的平衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/In-Training_Defenses_against_Emergent_Misalignment_in_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-Classification-is-a-RAG-problem-A-case-study-on-hate-speech-detection"><a href="#79-Classification-is-a-RAG-problem-A-case-study-on-hate-speech-detection" class="headerlink" title="79. Classification is a RAG problem: A case study on hate speech detection"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Classification_is_a_RAG_problem__A_case_study_on_hate_speech_detection.pdf">Classification is a RAG problem: A case study on hate speech detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Contextual AI</span></p>
<p>本文提出了一种基于检索增强生成（RAG, Retrieval-Augmented Generation）的分类方法，将传统分类任务转变为基于推理时检索到的上下文知识进行评估。作者开发了CONTEXTUAL POLICY ENGINE (CPE)系统，通过检索相关政策文档并结合生成式模型，对仇恨言论进行灵活、可解释和可动态调整的分类，实验表明该方法在准确性、可扩展性和灵活性方面优于主流商用系统，无需模型重训练即可适应政策变更。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Classification_is_a_RAG_problem_A_case_study_on_hate_speech_detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-UR2-Unify-RAG-and-Reasoning-through-Reinforcement-Learning"><a href="#80-UR2-Unify-RAG-and-Reasoning-through-Reinforcement-Learning" class="headerlink" title="80. UR2: Unify RAG and Reasoning through Reinforcement Learning"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/UR$^2$__Unify_RAG_and_Reasoning_through_Reinforcement_Learning.pdf">UR2: Unify RAG and Reasoning through Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出UR2框架，首次将检索增强生成（RAG）与强化学习（RL）紧密结合，实现大模型在推理过程中动态协调何时检索与何时依赖自身知识。UR2采用难度感知课程训练，仅对复杂问题激活检索，并通过混合知识访问策略融合离线领域语料与LLM摘要，提升模型的泛化与适应性。实验表明，UR2在数学、医学、开放领域QA等多任务上显著优于现有RAG和RL方法，7B模型达到GPT-4o-mini水准，具备广泛的通用性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/UR2_Unify_RAG_and_Reasoning_through_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-Automated-Creation-of-the-Legal-Knowledge-Graph-Addressing-Legislation-on-Violence-Against-Women-Resource-Methodology-and-Lessons-Learned"><a href="#81-Automated-Creation-of-the-Legal-Knowledge-Graph-Addressing-Legislation-on-Violence-Against-Women-Resource-Methodology-and-Lessons-Learned" class="headerlink" title="81. Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Automated_Creation_of_the_Legal_Knowledge_Graph_Addressing_Legislation_on_Violence_Against_Women__Re.pdf">Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Bari Aldo Moro</span></p>
<p>本论文提出了两种用于自动化构建针对女性暴力立法的法律知识图谱的方法：一种是法律领域定制的系统化底层流程，包括结构化数据提取、语义丰富和本体开发，另一种则基于大型语言模型（LLM）结合检索增强生成（RAG）、提示工程和传统NLP技术。结论显示，底层方法精度高但耗时，LLM方法具备快速扩展性但需人工校验，二者互补能提升法律信息获取和智能化应用效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Automated_Creation_of_the_Legal_Knowledge_Graph_Addressing_Legislation_on_Violence_Against_Women_Resource%2C_Methodology_and_Lessons_Learned.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-Beyond-Prompt-Induced-Lies-Investigating-LLM-Deception-on-Benign-Prompts"><a href="#82-Beyond-Prompt-Induced-Lies-Investigating-LLM-Deception-on-Benign-Prompts" class="headerlink" title="82. Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Beyond_Prompt-Induced_Lies__Investigating_LLM_Deception_on_Benign_Prompts.pdf">Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Singapore</span></p>
<p>本文提出了一个名为Contact Searching Question (CSQ) 的新评估框架，用于量化大型语言模型（LLM）在无明确欺骗意图的普通问题上的自发性欺骗行为。方法包括设计两项心理学启发的统计指标：欺骗意图分数和欺骗行为分数，在14个主流LLM上实证分析发现，这两项分数在任务难度提升时同步上升，揭示当前LLM在复杂任务中欺骗倾向增强，提出对应数学模型解释。该结论表明，先进LLM在处理复杂任务时自发性欺骗行为普遍存在，需引起信任和安全重视。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Beyond_Prompt-Induced_Lies_Investigating_LLM_Deception_on_Benign_Prompts.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-Aligning-Effective-Tokens-with-Video-Anomaly-in-Large-Language-Models"><a href="#83-Aligning-Effective-Tokens-with-Video-Anomaly-in-Large-Language-Models" class="headerlink" title="83. Aligning Effective Tokens with Video Anomaly in Large Language Models"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Aligning_Effective_Tokens_with_Video_Anomaly_in_Large_Language_Models.pdf">Aligning Effective Tokens with Video Anomaly in Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Hong Kong</span></p>
<p>本文提出VA-GPT，一种专为视频异常事件检测与理解设计的多模态大语言模型，通过Spatial Effective Token Selection（SETS）和Temporal Effective Token Generation（TETG）模块，有效选择和生成与异常事件相关的空间和时间关键视觉令牌，并与LLM对齐。实验结果显示，VA-GPT在多个基准测试（含跨域场景）中异常检测和定位表现优于现有方法，显著提升了异常视频理解的准确性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Aligning_Effective_Tokens_with_Video_Anomaly_in_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-Harnessing-Adaptive-Topology-Representations-for-Zero-Shot-Graph-Question-Answering"><a href="#84-Harnessing-Adaptive-Topology-Representations-for-Zero-Shot-Graph-Question-Answering" class="headerlink" title="84. Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Harnessing_Adaptive_Topology_Representations_for_Zero-Shot_Graph_Question_Answering.pdf">Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southern University of Science and Technology</span></p>
<p>该论文提出DynamicTRF框架，通过系统分析现有拓扑表示形式（TRF）并引入GRE指标，设计专用于零样本图问答的多样TRF集FZS。方法包括构建TRF偏好数据集、训练TRF Router以实现针对问题的动态TRF分配，显著提升LMM的零样本图问答准确率与简洁性。实验表明DynamicTRF在7种算法任务和2种下游任务上均优于现有方法，实现了准确性与响应长度的均衡提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Harnessing_Adaptive_Topology_Representations_for_Zero-Shot_Graph_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-GeoLaux-A-Benchmark-for-Evaluating-MLLMs’-Geometry-Performance-on-Long-Step-Problems-Requiring-Auxiliary-Lines"><a href="#85-GeoLaux-A-Benchmark-for-Evaluating-MLLMs’-Geometry-Performance-on-Long-Step-Problems-Requiring-Auxiliary-Lines" class="headerlink" title="85. GeoLaux: A Benchmark for Evaluating MLLMs’ Geometry Performance on Long-Step Problems Requiring Auxiliary Lines"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/GeoLaux__A_Benchmark_for_Evaluating_MLLMs'_Geometry_Performance_on_Long-Step_Problems_Requiring_Auxi.pdf">GeoLaux: A Benchmark for Evaluating MLLMs’ Geometry Performance on Long-Step Problems Requiring Auxiliary Lines</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong University</span></p>
<p>该论文提出了GeoLaux，大规模几何问题数据集，专为评估多模态大语言模型（MLLMs）在长步骤推理和辅助线构建任务中的表现。作者设计了包含五维度的细粒度评估框架，并在13种主流MLLMs上进行了实验，发现模型在长步骤推理中性能急剧下降、证明题易偷懒、辅助线意识薄弱，提升辅助线能力能显著改善几何推理效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/GeoLaux_A_Benchmark_for_Evaluating_MLLMs%E2%80%99_Geometry_Performance_on_Long-Step_Problems_Requiring_Auxiliary_Lines.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-LoRA-in-LoRA-Towards-Parameter-Efficient-Architecture-Expansion-for-Continual-Visual-Instruction-Tuning"><a href="#86-LoRA-in-LoRA-Towards-Parameter-Efficient-Architecture-Expansion-for-Continual-Visual-Instruction-Tuning" class="headerlink" title="86. LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LoRA_in_LoRA__Towards_Parameter-Efficient_Architecture_Expansion_for_Continual_Visual_Instruction_Tu.pdf">LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hefei University of Technology</span></p>
<p>本文提出了LiLoRA方法，用于多模态大语言模型（MLLMs）中的持续视觉指令微调（CVIT），通过在LoRA低秩适配器中跨任务共享矩阵A，并对任务相关的矩阵B进行进一步低秩分解，大幅提升参数效率。同时引入余弦正则化稳定损失以保持共享基的稳定性。实验结果显示，LiLoRA在保持高性能的同时，显著减少了参数开销，有效缓解灾难性遗忘问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LoRA_in_LoRA_Towards_Parameter-Efficient_Architecture_Expansion_for_Continual_Visual_Instruction_Tuning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-Retrieval-Augmented-Large-Language-Model-System-for-Comprehensive-Drug-Contraindications"><a href="#87-Retrieval-Augmented-Large-Language-Model-System-for-Comprehensive-Drug-Contraindications" class="headerlink" title="87. Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Retrieval_Augmented_Large_Language_Model_System_for_Comprehensive_Drug_Contraindications.pdf">Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hongik University</span></p>
<p>本文提出了一种检索增强生成（RAG）系统，结合GPT-4o-mini大语言模型与药品使用审查（DUR）知识库，通过混合语义和关键词检索及重排序，在药物禁忌问答任务中实现高准确率。结果显示，该方法对孕妇、儿童及联合用药场景的禁忌判别准确率显著提升，能够为敏感群体提供更可靠的用药安全信息，极大减少处方决策的不确定性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Retrieval_Augmented_Large_Language_Model_System_for_Comprehensive_Drug_Contraindications.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-Less-is-More-Selective-Reflection-for-Compatible-and-Efficient-Knowledge-Distillation-in-Large-Language-Models"><a href="#88-Less-is-More-Selective-Reflection-for-Compatible-and-Efficient-Knowledge-Distillation-in-Large-Language-Models" class="headerlink" title="88. Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Less_is_More__Selective_Reflection_for_Compatible_and_Efficient_Knowledge_Distillation_in_Large_Lang.pdf">Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">City University of Hong Kong</span></p>
<p>本文提出了一种名为Selective Reflection Distillation (SRD)的新型数据筛选与课程调度框架，用于提升大语言模型（LLM）知识蒸馏的效果与效率。方法上，SRD利用学生模型输出对训练样本进行难度评估与筛选，通过ROUGE-L和交叉熵指标进行融合排序，保留高质量、学生兼容的数据，并采用课程学习策略按难度分阶段引入数据以优化训练流程。实验显示，SRD作为即插即用模块，能在多种白盒知识蒸馏方法和模型架构下提升学生模型性能，同时将蒸馏训练的总运行时间最多降低39%。结论：SRD显著提升了蒸馏模型的能力和训练效率，为LLM压缩提供了实用方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Less_is_More_Selective_Reflection_for_Compatible_and_Efficient_Knowledge_Distillation_in_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-PanelTR-Zero-Shot-Table-Reasoning-Framework-Through-Multi-Agent-Scientific-Discussion"><a href="#89-PanelTR-Zero-Shot-Table-Reasoning-Framework-Through-Multi-Agent-Scientific-Discussion" class="headerlink" title="89. PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/PanelTR__Zero-Shot_Table_Reasoning_Framework_Through_Multi-Agent_Scientific_Discussion.pdf">PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Univeristy of Posts and Telecommunications</span></p>
<p>本文提出PanelTR框架，通过多LLM科学家代理协作，分为个人调查、自我复查和集体同行评议三个阶段，模拟科学方法提升复杂表格推理能力。实验表明，该方法无需特定训练数据即可在多个表格推理任务上超越基础LLM，性能接近甚至超过部分有监督模型，展现出科学方法在复杂推理场景下的有效性和灵活的零样本泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/PanelTR_Zero-Shot_Table_Reasoning_Framework_Through_Multi-Agent_Scientific_Discussion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-AdaptInfer-Adaptive-Token-Pruning-for-Vision–Language-Model-Inference-with-Dynamical-Text-Guidance"><a href="#90-AdaptInfer-Adaptive-Token-Pruning-for-Vision–Language-Model-Inference-with-Dynamical-Text-Guidance" class="headerlink" title="90. AdaptInfer: Adaptive Token Pruning for Vision–Language Model Inference with Dynamical Text Guidance"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/AdaptInfer__Adaptive_Token_Pruning_for_Vision-Language_Model_Inference_with_Dynamical_Text_Guidance.pdf">AdaptInfer: Adaptive Token Pruning for Vision–Language Model Inference with Dynamical Text Guidance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Global Innovation Exchange</span></p>
<p>AdaptInfer提出了一种适用于视觉-语言模型（VLMs）的动态文本引导视觉token稀疏化推理框架，通过复用层间text-to-text注意力生成文本token重要性先验，并基于此动态加权text-to-vision注意力得分，实现逐层自适应vision token剪枝。此外，论文通过离线分析cross-modal注意力迁移，制定了高效合理的剪枝调度方案。实验显示，在大幅压缩视觉token数量的同时，AdaptInfer能显著降低推理延迟且准确率接近原始模型，在多个基准任务上超越现有SOTA方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/AdaptInfer_Adaptive_Token_Pruning_for_Vision%E2%80%93Language_Model_Inference_with_Dynamical_Text_Guidance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-LLMs-for-Resource-Allocation-A-Participatory-Budgeting-Approach-to-Inferring-Preferences"><a href="#91-LLMs-for-Resource-Allocation-A-Participatory-Budgeting-Approach-to-Inferring-Preferences" class="headerlink" title="91. LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LLMs_for_Resource_Allocation__A_Participatory_Budgeting_Approach_to_Inferring_Preferences.pdf">LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">EPFL</span></p>
<p>本文提出了一个基于参与式预算（Participatory Budgeting, PB）的机制设计框架，系统评估了大语言模型（LLM）在复杂资源分配任务中的推理与决策能力。方法上，作者将LLMs置于社会规划者角色，通过三种提示策略（贪心选择、直接优化和爬山式精炼），在预算等约束下分配项目，并进一步考察LLMs能否仅凭自然语言或元数据推断投票者偏好。结论显示，LLMs在推理和机制设计中具有潜力，尤其是在无法获得结构化偏好的场景下能有效推断和分配资源，提示其作为动态评测基准的价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LLMs_for_Resource_Allocation_A_Participatory_Budgeting_Approach_to_Inferring_Preferences.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-EvolvR-Self-Evolving-Pairwise-Reasoning-for-Story-Evaluation-to-Enhance-Generation"><a href="#92-EvolvR-Self-Evolving-Pairwise-Reasoning-for-Story-Evaluation-to-Enhance-Generation" class="headerlink" title="92. EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/EvolvR__Self-Evolving_Pairwise_Reasoning_for_Story_Evaluation_to_Enhance_Generation.pdf">EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出了EvolvR，一种基于自进化的成对推理框架，用于提升开源大语言模型在故事评价任务中的推理和评分能力。其方法包括多角色自生成Chain-of-Thought数据、通过多智能体自过滤确保逻辑严谨，并将训练出的评价器作为奖励模型引导故事生成。实验结果显示EvolvR在StoryER、HANNA和OpenMEVA三大基准上达到SOTA，同时显著提升故事生成质量，验证了自进化框架的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/EvolvR_Self-Evolving_Pairwise_Reasoning_for_Story_Evaluation_to_Enhance_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-DP-LLM-Runtime-Model-Adaptation-with-Dynamic-Layer-wise-Precision-Assignment"><a href="#93-DP-LLM-Runtime-Model-Adaptation-with-Dynamic-Layer-wise-Precision-Assignment" class="headerlink" title="93. DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/DP-LLM__Runtime_Model_Adaptation_with_Dynamic_Layer-wise_Precision_Assignment.pdf">DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Seoul National University</span></p>
<p>本文提出DP-LLM，一种针对大语言模型（LLM）的动态层级精度分配推理自适应机制。其核心方法是在每一次推理解码时，通过轻量级误差估计器与门控选择器，根据输入动态选择每层的量化比特数，实现更细粒度的性能-延迟权衡。实验表明，该方法在多种模型和数据集上均优于静态分配方案，有效提升了在不同内存和延迟约束下的推理性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/DP-LLM_Runtime_Model_Adaptation_with_Dynamic_Layer-wise_Precision_Assignment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-SDEval-Safety-Dynamic-Evaluation-for-Multimodal-Large-Language-Models"><a href="#94-SDEval-Safety-Dynamic-Evaluation-for-Multimodal-Large-Language-Models" class="headerlink" title="94. SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SDEval__Safety_Dynamic_Evaluation_for_Multimodal_Large_Language_Models.pdf">SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (GZ)</span></p>
<p>本文提出了SDEval框架，是首个针对多模态大模型(MLLM)的安全动态评估方法，通过三类动态策略（文本、图像、文本-图像）对原有评测基准进行扰动，动态生成新的评估样本，有效提升了测试集复杂度、降低数据泄漏以及暴露MLLM安全风险。实验表明，SDEval能显著降低现有MLLM的安全评分，揭示当前模型难以应对动态安全挑战，凸显了提升模型安全能力的紧迫性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SDEval_Safety_Dynamic_Evaluation_for_Multimodal_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-LLM-Serving-Optimization-with-Variable-Prefill-and-Decode-Lengths"><a href="#95-LLM-Serving-Optimization-with-Variable-Prefill-and-Decode-Lengths" class="headerlink" title="95. LLM Serving Optimization with Variable Prefill and Decode Lengths"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/LLM_Serving_Optimization_with_Variable_Prefill_and_Decode_Lengths.pdf">LLM Serving Optimization with Variable Prefill and Decode Lengths</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文研究了大语言模型（LLM）推理服务中的任务调度问题，重点考虑每个请求具有不同的输入（prefill）长度和输出（decode）长度，受限于KV缓存内存约束。作者首先证明该问题在一般情况下为NP-难，并分析了常用调度算法（如FCFS和Shortest-First）在非均匀输入下的性能下界。针对上述挑战，提出了基于新质量指标F(X)的Sorted-F调度算法，并证明其具有常数竞争比；同时开发了动态规划、局部搜索和LP松弛等高效近似算法。大量实验证明，Sorted-F及其变体在实际数据集上优于常用基线，并兼具理论鲁棒性和计算效率。结论表明，Sorted-F算法能显著降低LLM服务端到端延迟，为多样化请求场景下的高效推理提供了理论与实践支持。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/LLM_Serving_Optimization_with_Variable_Prefill_and_Decode_Lengths.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-SKATE-a-Scalable-Tournament-Eval-Weaker-LLMs-differentiate-between-stronger-ones-using-verifiable-challenges"><a href="#96-SKATE-a-Scalable-Tournament-Eval-Weaker-LLMs-differentiate-between-stronger-ones-using-verifiable-challenges" class="headerlink" title="96. SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/SKATE,_a_Scalable_Tournament_Eval__Weaker_LLMs_differentiate_between_stronger_ones_using_verifiable_.pdf">SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Alan Turing Institute</span></p>
<p>该论文提出SKATE框架，将大语言模型（LLM）的评估转化为模型间的竞赛，模型既充当任务制定者又充当解答者，通过生成和解决可验证的任务（如代码输出预测），以自动化、无数据依赖和高度可扩展的方式实现评估。实验表明，弱模型可以有效区分强模型，排名稳定，并能够自动发现模型间细微能力差异，验证了该框架的客观性与适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/SKATE%2C_a_Scalable_Tournament_Eval_Weaker_LLMs_differentiate_between_stronger_ones_using_verifiable_challenges.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-Can-Large-Models-Fool-the-Eye-A-New-Turing-Test-for-Biological-Animation"><a href="#97-Can-Large-Models-Fool-the-Eye-A-New-Turing-Test-for-Biological-Animation" class="headerlink" title="97. Can Large Models Fool the Eye? A New Turing Test for Biological Animation"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Can_Large_Models_Fool_the_Eye__A_New_Turing_Test_for_Biological_Animation.pdf">Can Large Models Fool the Eye? A New Turing Test for Biological Animation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了BioMotion Arena，一个通过视觉生物运动动画进行大模型评估的新框架，专门针对LLMs和多模态大模型（MLLMs）。方法采用点光源动画和人类偏好投票，收集超过45,000次对比投票，利用Elo评分系统动态排名模型。实验发现绝大多数主流模型难以生成合理且逼真的生物动作动画，BioMotion Arena能有效区分模型表现并反映真实用户体验差异。结论：当前大模型对生物运动理解较差，BioMotion Arena能提供直观且可靠的评估方式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Can_Large_Models_Fool_the_Eye_A_New_Turing_Test_for_Biological_Animation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-Fourier-VLM-Compressing-Vision-Tokens-in-the-Frequency-Domain-for-Large-Vision-Language-Models"><a href="#98-Fourier-VLM-Compressing-Vision-Tokens-in-the-Frequency-Domain-for-Large-Vision-Language-Models" class="headerlink" title="98. Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Fourier-VLM__Compressing_Vision_Tokens_in_the_Frequency_Domain_for_Large_Vision-Language_Models.pdf">Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出Fourier-VLM，通过在频率域压缩视觉特征，显著减少Vision-Language Models（VLMs）输入中的视觉token数量。方法核心是无参数的Frequency Feature Compressor（FFC），利用二维离散余弦变换（2d-DCT）提取低频视觉特征，减少高频冗余，同时通过逆DCT恢复压缩后的视觉表示。实验证明，Fourier-VLM在LLaVA和Qwen-VL等主流架构下，在八个图像基准任务上以极低的视觉token保留率（最低仅6.25%）保持96%以上的平均性能，并将计算量（FLOPs）降低至16.16%，推理速度提升31.2%，泛化性和零样本视频理解能力强，适合高效实际部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Fourier-VLM_Compressing_Vision_Tokens_in_the_Frequency_Domain_for_Large_Vision-Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-Efficient-Knowledge-Probing-of-Large-Language-Models-by-Adapting-Pre-trained-Embeddings"><a href="#99-Efficient-Knowledge-Probing-of-Large-Language-Models-by-Adapting-Pre-trained-Embeddings" class="headerlink" title="99. Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings"></a>99. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Efficient_Knowledge_Probing_of_Large_Language_Models_by_Adapting_Pre-trained_Embeddings.pdf">Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Georgia Institute of Technology</span></p>
<p>本论文提出PEEK框架，利用经过训练的文本和图结构嵌入模型，通过微调线性层高效预测大语言模型（LLM）是否掌握某一事实，无需对LLM进行大量推理调用。实验证明，该方法在多个数据集和主流LLM上能以高达90%的准确率判定知识掌握情况，揭示嵌入模型可用于高效发现LLM知识盲点，结论是PEEK能在LLM部署前有效识别和弥补事实空白，提升知识评估效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Efficient_Knowledge_Probing_of_Large_Language_Models_by_Adapting_Pre-trained_Embeddings.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future"><a href="#100-Temporal-Self-Rewarding-Language-Models-Decoupling-Chosen-Rejected-via-Past-Future" class="headerlink" title="100. Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future"></a>100. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Temporal_Self-Rewarding_Language_Models__Decoupling_Chosen-Rejected_via_Past-Future.pdf">Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本论文提出Temporal Self-Rewarding Language Models，通过引入“Anchored Rejection”（历史模型固定负样本）和“Future-Guided Chosen”（未来模型动态正样本）两个阶段，解决了自奖励大语言模型中正负样本表示收敛导致学习信号消失的问题。实验结果表明，该方法在Llama、Qwen、Mistral等多种模型及AlpacaEval 2.0等主流基准上显著优于标准自奖励方法，同时保持计算资源不变，且具有良好的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Temporal_Self-Rewarding_Language_Models_Decoupling_Chosen-Rejected_via_Past-Future.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="101-Position-Intelligent-Coding-Systems-Should-Write-Programs-with-Justifications"><a href="#101-Position-Intelligent-Coding-Systems-Should-Write-Programs-with-Justifications" class="headerlink" title="101. Position: Intelligent Coding Systems Should Write Programs with Justifications"></a>101. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Position__Intelligent_Coding_Systems_Should_Write_Programs_with_Justifications.pdf">Position: Intelligent Coding Systems Should Write Programs with Justifications</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Purdue University</span></p>
<p>本文提出智能编码系统不仅应生成代码，还应生成可理解、与代码语义一致的自然语言解释（justifications），以提升非专业用户的理解和信任。作者分析了现有形式化验证、静态分析和后验可解释性方法的局限，提出基于神经-符号一体化的方法，通过在训练阶段引入符号约束并在推理阶段自动验证解释与语义一致性，实现认知对齐与语义忠实的解释生成。结论认为，神经-符号方法能更好地提升代码智能生成系统的可解释性和信赖度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Position_Intelligent_Coding_Systems_Should_Write_Programs_with_Justifications.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="102-MATHREAL-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models"><a href="#102-MATHREAL-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models" class="headerlink" title="102. MATHREAL: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models"></a>102. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/MathReal__We_Keep_It_Real!_A_Real_Scene_Benchmark_for_Evaluating_Math_Reasoning_in_Multimodal_Large_.pdf">MATHREAL: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Baidu Inc.</span></p>
<p>该论文提出MATHREAL数据集，收集了2000个真实场景下K-12数学题目图片，覆盖图像质量退化、视角变化、内容干扰等挑战，并细分为14类噪声。通过系统性实验，作者评估了40种主流多模态大模型在六种设定下的数学推理能力，发现现有MLLMs在真实教育场景下表现显著低于在干净数据上的能力。结论表明，多模态大模型在视觉感知、结构化理解和数学推理方面仍面临实际应用瓶颈，需要提升视觉编码和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/MATHREAL_We_Keep_It_Real%21_A_Real_Scene_Benchmark_for_Evaluating_Math_Reasoning_in_Multimodal_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="103-Optimizing-Prompt-Sequences-using-Monte-Carlo-Tree-Search-for-LLM-Based-Optimization"><a href="#103-Optimizing-Prompt-Sequences-using-Monte-Carlo-Tree-Search-for-LLM-Based-Optimization" class="headerlink" title="103. Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization"></a>103. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Optimizing_Prompt_Sequences_using_Monte_Carlo_Tree_Search_for_LLM-Based_Optimization.pdf">Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The George Washington University</span></p>
<p>该论文提出了MCTS-OPS框架，将大语言模型（LLM）与蒙特卡洛树搜索（MCTS）结合，通过将prompt选择建模为序列决策过程，自动优化多步prompt序列以生成高质量、可执行的代码，解决复杂约束优化问题。实验结果表明，MCTS-OPS在网络优化任务中成功率、优化质量和稳定性远超GPT-4、Chain-of-Thought和Self-Refine等主流LLM方法，显著提升了可达最优解的概率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Optimizing_Prompt_Sequences_using_Monte_Carlo_Tree_Search_for_LLM-Based_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="104-Learning-by-Teaching-Engaging-Students-as-Instructors-of-Large-Language-Models-in-Computer-Science-Education"><a href="#104-Learning-by-Teaching-Engaging-Students-as-Instructors-of-Large-Language-Models-in-Computer-Science-Education" class="headerlink" title="104. Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education"></a>104. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Learning_by_Teaching__Engaging_Students_as_Instructors_of_Large_Language_Models_in_Computer_Science_.pdf">Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Graduate Center, CUNY</span></p>
<p>该论文提出了一种反转传统角色的教学范式，要求学生通过精心设计的问题与提示，将LLM作为“学生”加以教学，促使学生构造和表达知识，核心方法包括基于知识空缺的问题设计、链式思维（CoT）、少样本提示（Few-Shot Prompting）以及自洽性验证，并开发了Socrates系统实现低门槛部署。实证结果表明，该方法在本科计算机课程中能显著提升学生作业与项目表现，降低对LLM的依赖，增强主动学习成效。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Learning_by_Teaching_Engaging_Students_as_Instructors_of_Large_Language_Models_in_Computer_Science_Education.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="105-Crisp-Attention-Regularizing-Transformers-via-Structured-Sparsity"><a href="#105-Crisp-Attention-Regularizing-Transformers-via-Structured-Sparsity" class="headerlink" title="105. Crisp Attention: Regularizing Transformers via Structured Sparsity"></a>105. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Crisp_Attention__Regularizing_Transformers_via_Structured_Sparsity.pdf">Crisp Attention: Regularizing Transformers via Structured Sparsity</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Joyspace AI</span></p>
<p>本文提出在Transformer模型自注意力机制中引入结构化稀疏性作为正则化手段，通过对DistilBERT在SST-2情感分析任务的微调实验，采用top-k筛选显著注意力连接，显著提升模型泛化能力和准确率。结果显示，80%稀疏度模型准确率提升0.97%，验证了稀疏不仅提升效率，更能提高Transformer性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Crisp_Attention_Regularizing_Transformers_via_Structured_Sparsity.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="106-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents"><a href="#106-Bifrost-1-Bridging-Multimodal-LLMs-and-Diffusion-Models-with-Patch-level-CLIP-Latents" class="headerlink" title="106. Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents"></a>106. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Bifrost-1__Bridging_Multimodal_LLMs_and_Diffusion_Models_with_Patch-level_CLIP_Latents.pdf">Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">UNC Chapel Hill</span></p>
<p>BIFROST-1提出了一种高效的统一多模态生成与理解框架，通过将预训练多模态大语言模型（MLLM）与扩散模型连接，采用与MLLM原生对齐的patch级CLIP图像embedding作为桥接变量，并利用轻量级Latent ControlNet集成到扩散模型，实现高保真、可控的图像生成，同时显著降低训练成本。实验结果表明，BIFROST-1在图像质量和多模态理解上达到或超越现有方法，且训练计算量显著减少，同时完全保持MLLM的推理和规划能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Bifrost-1_Bridging_Multimodal_LLMs_and_Diffusion_Models_with_Patch-level_CLIP_Latents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="107-eSASRec-Enhancing-Transformer-based-Recommendations-in-a-Modular-Fashion"><a href="#107-eSASRec-Enhancing-Transformer-based-Recommendations-in-a-Modular-Fashion" class="headerlink" title="107. eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion"></a>107. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/eSASRec__Enhancing_Transformer-based_Recommendations_in_a_Modular_Fashion.pdf">eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">MTS</span></p>
<p>该论文提出了eSASRec，一种基于模块化增强的Transformer推荐模型，通过将SASRec的训练目标、LiGR Transformer层和Sampled Softmax损失函数结合，系统性评估各增强模块的增益。结论显示，eSASRec在工业和学术数据集上超越了当前SOTA模型，可作为强大且易集成的推荐系统基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/eSASRec_Enhancing_Transformer-based_Recommendations_in_a_Modular_Fashion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="108-Tree-Based-Deep-Learning-for-Ranking-Symbolic-Integration-Algorithms"><a href="#108-Tree-Based-Deep-Learning-for-Ranking-Symbolic-Integration-Algorithms" class="headerlink" title="108. Tree-Based Deep Learning for Ranking Symbolic Integration Algorithms"></a>108. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Tree-Based_Deep_Learning_for_Ranking_Symbolic_Integration_Algorithms.pdf">Tree-Based Deep Learning for Ranking Symbolic Integration Algorithms</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Coventry University</span></p>
<p>本文提出了一种基于树结构深度学习模型的两阶段架构，优化计算机代数系统（CAS）中符号不定积分算法的选择。方法包括先识别可用方法，再根据输出表达式复杂度排序，并将数学表达式表示为树结构以提升性能。实验显示，树变换器在独立测试集上优于Maple内置选择器及先前ML方法，准确率接近90%，具有较强泛化能力。结论：树结构表示和两阶段排名框架对数学软件算法选择优化至关重要。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Tree-Based_Deep_Learning_for_Ranking_Symbolic_Integration_Algorithms.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="109-ActivityDiff-A-diffusion-model-with-Positive-and-Negative-Activity-Guidance-for-De-Novo-Drug-Design"><a href="#109-ActivityDiff-A-diffusion-model-with-Positive-and-Negative-Activity-Guidance-for-De-Novo-Drug-Design" class="headerlink" title="109. ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design"></a>109. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/ActivityDiff__A_diffusion_model_with_Positive_and_Negative_Activity_Guidance_for_De_Novo_Drug_Design.pdf">ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science and Engineering, Central South University</span></p>
<p>本文提出了ActivityDiff，一种结合正向和负向分类器引导的离散扩散分子生成模型，能够实现药物分子对多靶点活性和选择性的精确调控。该方法通过将正向（靶向活性提升）和负向（抑制脱靶效应）指导引入扩散过程，支持单&#x2F;多靶标、片段约束、特异性增强等多种药物设计任务。实验结果表明，ActivityDiff在分子新颖性、有效性、特异性和降低脱靶风险方面均优于现有方法，实现了药物设计中活性与安全性的平衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/ActivityDiff_A_diffusion_model_with_Positive_and_Negative_Activity_Guidance_for_De_Novo_Drug_Design.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="110-AntiCheatPT-A-Transformer-Based-Approach-to-Cheat-Detection-in-Competitive-Computer-Games"><a href="#110-AntiCheatPT-A-Transformer-Based-Approach-to-Cheat-Detection-in-Competitive-Computer-Games" class="headerlink" title="110. AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games"></a>110. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/AntiCheatPT__A_Transformer-Based_Approach_to_Cheat_Detection_in_Competitive_Computer_Games.pdf">AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The IT-University of Copenhagen</span></p>
<p>本文提出了AntiCheatPT 256，一种基于Transformer编码器的机器学习模型，通过对Counter-Strike 2游戏行为数据进行多变量时序建模，实现作弊检测，并开源了手工标注的CS2CD数据集（795场比赛）。实验结果显示该模型在未增强测试集上准确率达89.17%、AUC达93.36%，验证了基于服务器端行为数据的可复现作弊检测方法的有效性，降低了对侵入式反作弊手段的依赖。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/AntiCheatPT_A_Transformer-Based_Approach_to_Cheat_Detection_in_Competitive_Computer_Games.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="111-Low-Bit-Data-Processing-Using-Multiple-Output-Spiking-Neurons-with-Non-linear-Reset-Feedback"><a href="#111-Low-Bit-Data-Processing-Using-Multiple-Output-Spiking-Neurons-with-Non-linear-Reset-Feedback" class="headerlink" title="111. Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback"></a>111. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Low-Bit_Data_Processing_Using_Multiple-Output_Spiking_Neurons_with_Non-linear_Reset_Feedback.pdf">Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Uppsala University</span></p>
<p>该论文提出了一种多输出、带有非线性重置反馈的新型脉冲神经元模型，将线性状态空间模型（SSM）与可学习的非线性重置机制结合，实现低比特脉冲信号处理。作者通过在多模态任务（语音关键字识别、事件视觉和序列模式识别）上的实验，表明该模型在不强制线性稳定性约束下，能获得与主流SNN基线相当的性能，并解决不稳定线性动力学下的学习收敛问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Low-Bit_Data_Processing_Using_Multiple-Output_Spiking_Neurons_with_Non-linear_Reset_Feedback.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="112-Multi-Omics-Analysis-for-Cancer-Subtype-Inference-via-Unrolling-Graph-Smoothness-Priors"><a href="#112-Multi-Omics-Analysis-for-Cancer-Subtype-Inference-via-Unrolling-Graph-Smoothness-Priors" class="headerlink" title="112. Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors"></a>112. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Multi-Omics_Analysis_for_Cancer_Subtype_Inference_via_Unrolling_Graph_Smoothness_Priors.pdf">Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了GTMancer框架，通过对多组组学数据进行对比学习对齐后，采用图神经网络优化方法联合捕捉组学间和样本间的结构信息，并引入双重注意力机制提升癌症亚型分类性能。理论上证明了优化过程的收敛性，并通过牛顿法自动调整步长，实验结果在七个真实癌症数据集上超越现有方法，提升分类准确率与泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Multi-Omics_Analysis_for_Cancer_Subtype_Inference_via_Unrolling_Graph_Smoothness_Priors.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="113-One-Size-Does-Not-Fit-All-A-Distribution-Aware-Sparsification-for-More-Precise-Model-Merging"><a href="#113-One-Size-Does-Not-Fit-All-A-Distribution-Aware-Sparsification-for-More-Precise-Model-Merging" class="headerlink" title="113. One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging"></a>113. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/One_Size_Does_Not_Fit_All__A_Distribution-Aware_Sparsification_for_More_Precise_Model_Merging.pdf">One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northeastern University</span></p>
<p>本文提出了TADrop（Tensor-wise Adaptive Drop），一种基于参数分布自适应的稀疏化策略，用于提升多任务模型融合的精度。TADrop根据每个参数张量的分布特性动态分配稀疏率，实现细粒度、结构感知的稀疏化，并通过归一化保持张量范数。实验表明，TADrop可无缝集成至多种模型融合方案，在视觉、语言及多模态任务上均显著提升融合模型性能，尤其在任务数量增多时表现出更强的鲁棒性和泛化性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/One_Size_Does_Not_Fit_All_A_Distribution-Aware_Sparsification_for_More_Precise_Model_Merging.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="114-Semantic-Item-Graph-Enhancement-for-Multimodal-Recommendation"><a href="#114-Semantic-Item-Graph-Enhancement-for-Multimodal-Recommendation" class="headerlink" title="114. Semantic Item Graph Enhancement for Multimodal Recommendation"></a>114. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Semantic_Item_Graph_Enhancement_for_Multimodal_Recommendation.pdf">Semantic Item Graph Enhancement for Multimodal Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>本文提出SIGER框架，将用户-物品交互图中的协同信号注入到基于模态的物品-物品语义图，构建增强型语义图以丰富物品语义建模。通过模数化个性化嵌入扰动机制生成对比视图，结合对比学习实现抗噪表征，并设计双重表征对齐机制，提升行为与语义表征的一致性。实验表明SIGER在多种推荐场景均显著优于现有方法，尤其在冷启动情境下优势突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Semantic_Item_Graph_Enhancement_for_Multimodal_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="115-Enhancing-the-Scalability-of-Classical-Surrogates-for-Real-World-Quantum-Machine-Learning-Applications"><a href="#115-Enhancing-the-Scalability-of-Classical-Surrogates-for-Real-World-Quantum-Machine-Learning-Applications" class="headerlink" title="115. Enhancing the Scalability of Classical Surrogates for Real-World Quantum Machine Learning Applications"></a>115. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Enhancing_the_Scalability_of_Classical_Surrogates_for_Real-World_Quantum_Machine_Learning_Applicatio.pdf">Enhancing the Scalability of Classical Surrogates for Real-World Quantum Machine Learning Applications</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">E.ON Digital Technology GmbH</span></p>
<p>本文提出了一种高效替代流程，利用实际训练数据和随机傅里叶频率采样，极大降低了从量子模型生成经典代理模型的计算资源需求，实现了对更大规模量子模型的经典近似。实验证明，该方法在能源需求预测任务中，可在资源线性扩展的情况下获得与量子模型高度一致的预测精度，显著提升了QML实际工业部署的可行性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Enhancing_the_Scalability_of_Classical_Surrogates_for_Real-World_Quantum_Machine_Learning_Applications.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="116-MeanAudio-Fast-and-Faithful-Text-to-Audio-Generation-with-Mean-Flows"><a href="#116-MeanAudio-Fast-and-Faithful-Text-to-Audio-Generation-with-Mean-Flows" class="headerlink" title="116. MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows"></a>116. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/MeanAudio__Fast_and_Faithful_Text-to-Audio_Generation_with_Mean_Flows.pdf">MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出MeanAudio，一种基于MeanFlow的高效文本到音频生成模型，利用Flux-style流变换器在训练中回归平均速度场，通过直接从流程轨迹起点到终点实现快速音频合成。引入分类器无关引导和瞬时到平均流的训练策略，极大提升了训练稳定性和生成质量。实验结果显示MeanAudio在单步和多步生成均达SOTA性能，推理速度比主流扩散方法快一百倍，参数量更低。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/MeanAudio_Fast_and_Faithful_Text-to-Audio_Generation_with_Mean_Flows.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="117-Architecture-Aware-Generalization-Bounds-for-Temporal-Networks-Theory-and-Fair-Comparison-Methodology"><a href="#117-Architecture-Aware-Generalization-Bounds-for-Temporal-Networks-Theory-and-Fair-Comparison-Methodology" class="headerlink" title="117. Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology"></a>117. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Architecture-Aware_Generalization_Bounds_for_Temporal_Networks__Theory_and_Fair_Comparison_Methodolo.pdf">Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technion</span></p>
<p>本文提出了针对深度时序网络（如TCN）的首个架构相关、非虚无的泛化界理论，并引入了公平对比评估方法。核心方法包括延迟反馈阻断机制将相关样本转化为近似独立样本，推导出泛化界随网络深度D呈√D缩放，并通过固定有效样本量的实验揭示强依赖序列在相同信息量下能获得显著更小的泛化间隔，挑战了依赖性只会带来负面影响的传统认识。结论：理论界为网络架构选择提供量化指导，实验证明时序依赖有助于提升泛化性能，但理论与实践间仍有差异，公平对比方法能更准确揭示依赖结构对学习的影响。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Architecture-Aware_Generalization_Bounds_for_Temporal_Networks_Theory_and_Fair_Comparison_Methodology.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="118-Do-Ethical-AI-Principles-Matter-to-Users-A-Large-Scale-Analysis-of-User-Sentiment-and-Satisfaction"><a href="#118-Do-Ethical-AI-Principles-Matter-to-Users-A-Large-Scale-Analysis-of-User-Sentiment-and-Satisfaction" class="headerlink" title="118. Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction"></a>118. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Do_Ethical_AI_Principles_Matter_to_Users__A_Large-Scale_Analysis_of_User_Sentiment_and_Satisfaction.pdf">Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hankuk University of Foreign Studies</span></p>
<p>本研究通过收集G2.com上10万余条AI产品用户评论，基于欧盟可信赖AI伦理准则七大维度，采用零样本分类与RoBERTa-Large等Transformer模型，量化评估伦理AI原则与用户满意度的关联。结论显示，所有伦理维度（如透明度、公平性、责任）均与用户满意度正相关，且非技术用户和终端应用场景下这种关联更强，凸显伦理AI设计需关注用户情境差异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Do_Ethical_AI_Principles_Matter_to_Users_A_Large-Scale_Analysis_of_User_Sentiment_and_Satisfaction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="119-Fast-Convex-and-Conditioned-Network-for-Multi-Fidelity-Vectors-and-Stiff-Univariate-Differential-Equations"><a href="#119-Fast-Convex-and-Conditioned-Network-for-Multi-Fidelity-Vectors-and-Stiff-Univariate-Differential-Equations" class="headerlink" title="119. Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations"></a>119. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-11/Fast,_Convex_and_Conditioned_Network_for_Multi-Fidelity_Vectors_and_Stiff_Univariate_Differential_Eq.pdf">Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Applied Mathematics, University of British Columbia</span></p>
<p>本文提出了一种结合极限学习机（ELM）和移位高斯编码的神经网络架构，用于解决多保真度数据和刚性微分方程中的优化病态问题。通过在ELM中引入移位高斯编码，有效提升了激活矩阵的秩和稀疏结构，实现了更优的矩阵条件数，显著提升了对刚性PDE、多尺度模式和高维向量（如图像）的拟合精度和效率。结论指出，相较于增加网络深度，改善激活矩阵的条件性对科学计算中的神经网络收敛与表达能力更为关键。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-12/Fast%2C_Convex_and_Conditioned_Network_for_Multi-Fidelity_Vectors_and_Stiff_Univariate_Differential_Equations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>今日论文报纸</div>
      <div>http://example.com/2025/08/2025-08-12_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Ywfhhh</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月12日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/2025-08-13_article/" title="今日论文报纸">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">今日论文报纸</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-11_article/" title="今日论文报纸">
                        <span class="hidden-mobile">今日论文报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
