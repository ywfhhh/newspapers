

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Ywfhhh">
  <meta name="keywords" content="">
  
    <meta name="description" content="æ¯æ—¥æœ€æ–°è®ºæ–‡åˆ†äº«">
<meta property="og:type" content="article">
<meta property="og:title" content="ä»Šæ—¥è®ºæ–‡æŠ¥çº¸">
<meta property="og:url" content="http://example.com/2025/08/2025-08-18_article/index.html">
<meta property="og:site_name" content="è®ºæ–‡æŠ¥çº¸é›†åˆ">
<meta property="og:description" content="æ¯æ—¥æœ€æ–°è®ºæ–‡åˆ†äº«">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Non-Stationary%20Restless%20Multi-Armed%20Bandits%20with%20Provable%20Guarantee.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Variance%20Reduced%20Policy%20Gradient%20Method%20for%20Multi-Objective%20Reinforcement%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Pass%40k%20Training%20for%20Adaptively%20Balancing%20Exploration%20and%20Exploitation%20of%20Large%20Reasoning%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/REFN%3A%20A%20Reinforcement-Learning-From-Network%20Framework%20against%201-day/n-day%20Exploitations.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Nonlocal%20Monte%20Carlo%20via%20Reinforcement%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Stabilizing%20Long-term%20Multi-turn%20Reinforcement%20Learning%20with%20Gated%20Rewards.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Few-shot%20Vision-based%20Human%20Activity%20Recognition%20with%20MLLM-based%20Visual%20Reinforcement%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Agent%20Trust%20Region%20Policy%20Optimisation%3A%20A%20Joint%20Constraint%20Approach.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Integrating%20Reinforcement%20Learning%20with%20Visual%20Generative%20Models%3A%20Foundations%20and%20Advances.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Curriculum%20Learning%20Approach%20to%20Reinforcement%20Learning%3A%20Leveraging%20RAG%20for%20Multimodal%20Question%20Answering.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Agent%20Reinforcement%20Learning%20for%20Adaptive%20Resource%20Orchestration%20in%20Cloud-Native%20Clusters.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/TLE-Based%20A2C%20Agent%20for%20Terrestrial%20Coverage%20Orbital%20Path%20Planning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Multimodal%20Neural%20Network%20for%20Recognizing%20Subjective%20Self-Disclosure%20Towards%20Social%20Robots.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/UI-Venus%20Technical%20Report%3A%20Building%20High-performance%20UI%20Agents%20with%20RFT.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Scaling%20Up%20without%20Fading%20Out%3A%20Goal-Aware%20Sparse%20GNN%20for%20RL-based%20Generalized%20Planning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Synthesis%20of%20Deep%20Neural%20Networks%20with%20Safe%20Robust%20Adaptive%20Control%20for%20Reliable%20Operation%20of%20Wheeled%20Mobile%20Robots.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning%20Task%20Execution%20Hierarchies%20for%20Redundant%20Robots.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MLM%3A%20Learning%20Multi-task%20Loco-Manipulation%20Whole-Body%20Control%20for%20Quadruped%20Robot%20with%20Arm.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Projected%20Coupled%20Diffusion%20for%20Test-Time%20Constrained%20Joint%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MASH%3A%20Cooperative-Heterogeneous%20Multi-Agent%20Reinforcement%20Learning%20for%20Single%20Humanoid%20Robot%20Locomotion.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/KDPE%3A%20A%20Kernel%20Density%20Estimation%20Strategy%20for%20Diffusion%20Policy%20Trajectory%20Selection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning%20State-Space%20Models%20of%20Dynamic%20Systems%20from%20Arbitrary%20Data%20using%20Joint%20Embedding%20Predictive%20Architectures.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Efficient%20Methods%20for%20Accurate%20Sparse%20Trajectory%20Recovery%20and%20Map%20Matching.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/CorrectNav%3A%20Self-Correction%20Flywheel%20Empowers%20Vision-Language-Action%20Navigation%20Model.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Probabilistic%20Latency%20Analysis%20of%20the%20Data%20Distribution%20Service%20in%20ROS%202.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Super%20LiDAR%20Reflectance%20for%20Robotic%20Perception.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ReconVLA%3A%20Reconstructive%20Vision-Language-Action%20Model%20as%20Effective%20Robot%20Perceiver.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Large%20Model%20Empowered%20Embodied%20AI%3A%20A%20Survey%20on%20Decision-Making%20and%20Embodied%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Semantic-Aware%20Framework%20for%20Safe%20and%20Intent-Integrative%20Assistance%20in%20Upper-Limb%20Exoskeletons.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/JRDB-Reasoning%3A%20A%20Difficulty-Graded%20Benchmark%20for%20Visual%20Reasoning%20in%20Robotics.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GNN-based%20Unified%20Deep%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Quantum%20Visual%20Fields%20with%20Neural%20Amplitude%20Encoding.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MAESTRO%3A%20Masked%20AutoEncoders%20for%20Multimodal%2C%20Multitemporal%2C%20and%20Multispectral%20Earth%20Observation%20Data.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ToonComposer%3A%20Streamlining%20Cartoon%20Production%20with%20Generative%20Post-Keyframing.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Puppeteer%3A%20Rig%20and%20Animate%20Your%203D%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/STREAM3R%3A%20Scalable%20Sequential%203D%20Reconstruction%20with%20Causal%20Transformer.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Medico%202025%3A%20Visual%20Question%20Answering%20for%20Gastrointestinal%20Imaging.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AEGIS%3A%20Authenticity%20Evaluation%20Benchmark%20for%20AI-Generated%20Video%20Sequences.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Natively%20Trainable%20Sparse%20Attention%20for%20Hierarchical%20Point%20Cloud%20Datasets.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Exploiting%20Discriminative%20Codebook%20Prior%20for%20Autoregressive%20Image%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Continuous%20Bangla%20Sign%20Language%20Translation%3A%20Mitigating%20the%20Expense%20of%20Gloss%20Annotation%20with%20the%20Assistance%20of%20Graph.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Hybrid%20Generative%20Fusion%20for%20Efficient%20and%20Privacy-Preserving%20Face%20Recognition%20Dataset%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Geospatial%20Diffusion%20for%20Land%20Cover%20Imperviousness%20Change%20Forecasting.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Processing%20and%20acquisition%20traces%20in%20visual%20encoders%3A%20What%20does%20CLIP%20know%20about%20your%20camera%3F.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Object%20Fidelity%20Diffusion%20for%20Remote%20Sensing%20Image%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/VIDEO-BLADE%3A%20BLOCK-SPARSE%20ATTENTION%20MEETS%20STEP%20DISTILLATION%20FOR%20EFFICIENT%20VIDEO%20GENERATION.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From%20Diagnosis%20to%20Improvement%3A%20Probing%20Spatio-Physical%20Reasoning%20in%20Vision%20Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Privacy-enhancing%20Sclera%20Segmentation%20Benchmarking%20Competition%3A%20SSBC%202025.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/NextStep-1%3A%20Toward%20Autoregressive%20Image%20Generation%20with%20Continuous%20Tokens%20at%20Scale.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Novel%20View%20Synthesis%20using%20DDIM%20Inversion.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Conditional%20Information%20Bottleneck%20for%20Multimodal%20Fusion%3A%20Overcoming%20Shortcut%20Learning%20in%20Sarcasm%20Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ChatENV%3A%20An%20Interactive%20Vision-Language%20Model%20for%20Sensor-Guided%20Environmental%20Monitoring%20and%20Scenario%20Simulation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Fourier-Guided%20Attention%20Upsampling%20for%20Image%20Super-Resolution.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Towards%20Agentic%20AI%20for%20Multimodal-Guided%20Video%20Object%20Segmentation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GCRPNet%3A%20Graph-Enhanced%20Contextual%20and%20Regional%20Perception%20Network%20For%20Salient%20Object%20Detection%20in%20Optical%20Remote%20Sensing%20Images.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PASS%3A%20Probabilistic%20Agentic%20Supernet%20Sampling%20for%20Interpretable%20and%20Adaptive%20Chest%20X-Ray%20Reasoning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/X-Node%3A%20Self-Explanation%20is%20All%20We%20Need.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Label%20Plant%20Species%20Prediction%20with%20Metadata-Enhanced%20Multi-Head%20Vision%20Transformers.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From%20Images%20to%20Perception%3A%20Emergence%20of%20Perceptual%20Properties%20by%20Reconstructing%20Images.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MM-Food-100K%3A%20A%20100%2C000-Sample%20Multimodal%20Food%20Intelligence%20Dataset%20with%20Verifiable%20Provenance.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Adapting%20SAM%20via%20Cross-Entropy%20Masking%20for%20Class%20Imbalance%20in%20Remote%20Sensing%20Change%20Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PTQAT%3A%20A%20Hybrid%20Parameter-Efficient%20Quantization%20Algorithm%20for%203D%20Perception%20Tasks.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/EgoMusic-driven%20Human%20Dance%20Motion%20Estimation%20with%20Skeleton%20Mamba.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/TweezeEdit%3A%20Consistent%20and%20Efficient%20Image%20Editing%20with%20Path%20Regularization.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/STAMP%3A%20Multi-pattern%20Attention-aware%20Multiple%20Instance%20Learning%20for%20STAS%20Diagnosis%20in%20Multi-center%20Histopathology%20Images.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/NanoControl%3A%20A%20Lightweight%20Framework%20for%20Precise%20and%20Efficient%20Control%20in%20Diffusion%20Transformer.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Towards%20Spatially%20Consistent%20Image%20Generation%3A%20On%20Incorporating%20Intrinsic%20Scene%20Properties%20into%20Diffusion%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SynBrain%3A%20Enhancing%20Visual-to-fMRI%20Synthesis%20via%20Probabilistic%20Representation%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/DINOMotion%3A%20advanced%20robust%20tissue%20motion%20tracking%20with%20DINOv2%20in%202D-Cine%20MRI-guided%20radiotherapy.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SC-Lane%3A%20Slope-aware%20and%20Consistent%20Road%20Height%20Estimation%20Framework%20for%203D%20Lane%20Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Translation%20of%20Text%20Embedding%20via%20Delta%20Vector%20to%20Suppress%20Strongly%20Entangled%20Content%20in%20Text-to-Image%20Diffusion%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PQ-DAF%3A%20Pose-driven%20Quality-controlled%20Data%20Augmentation%20for%20Data-scarce%20Driver%20Distraction%20Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Unlocking%20Robust%20Semantic%20Segmentation%20Performance%20via%20Label-only%20Elastic%20Deformations%20against%20Implicit%20Label%20Noise.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AtomDiffuser%3A%20Time-Aware%20Degradation%20Modeling%20for%20Drift%20and%20Beam%20Damage%20in%20STEM%20Imaging.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving%20OCR%20for%20Historical%20Texts%20of%20Multiple%20Languages.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Efficient%20Image%20Denoising%20Using%20Global%20and%20Local%20Circulant%20Representation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving%20Learning%20of%20New%20Diseases%20through%20Knowledge-Enhanced%20Initialization%20for%20Federated%20Adapter%20Tuning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Sub-Pixel%20Multimodal%20Optical%20Remote%20Sensing%20Images%20Matching%20Method.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Deep%20Learning%20for%20Crack%20Detection%3A%20A%20Review%20of%20Learning%20Paradigms%2C%20Generalizability%2C%20and%20Datasets.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Performance%20of%20GPT-5%20in%20Brain%20Tumor%20MRI%20Reasoning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Survey%20on%20Diffusion%20Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Reinforced%20Language%20Models%20for%20Sequential%20Decision%20Making.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Memory-Augmented%20Transformers%3A%20A%20Systematic%20Review%20from%20Neuroscience%20Principles%20to%20Technical%20Solutions.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/The%20Knowledge-Reasoning%20Dissociation%3A%20Fundamental%20Limitations%20of%20LLMs%20in%20Clinical%20Natural%20Language%20Inference.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Modeling%20Human%20Responses%20to%20Multimodal%20AI%20Content.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GenOM%3A%20Ontology%20Matching%20with%20Description%20Generation%20and%20Large%20Language%20Model.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning%20from%20Natural%20Language%20Feedback%20for%20Personalized%20Question%20Answering.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FROGENT%3A%20An%20End-to-End%20Full-process%20Drug%20Design%20Agent.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/EgoCross%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Cross-Domain%20Egocentric%20Video%20Question%20Answering.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Advancing%20Autonomous%20Incident%20Response%3A%20Leveraging%20LLMs%20and%20Cyber%20Threat%20Intelligence.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MSRS%3A%20Adaptive%20Multi-Subspace%20Representation%20Steering%20for%20Attribute%20Alignment%20in%20Large%20Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MCP2OSC%3A%20Parametric%20Control%20by%20Natural%20Language.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Facilitating%20the%20Adoption%20of%20Causal%20Inference%20Methods%20Through%20LLM-Empowered%20Co-Pilot.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/When%20Language%20Overrules%3A%20Revealing%20Text%20Dominance%20in%20Multimodal%20Large%20Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Unified%20Multi-Agent%20Framework%20for%20Universal%20Multimodal%20Understanding%20and%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SEQ-GPT%3A%20LLM-assisted%20Spatial%20Query%20via%20Example.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/HumanSense%3A%20From%20Multimodal%20Perception%20to%20Empathetic%20Context-Aware%20Responses%20through%20Reasoning%20MLLMs.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving%20Value-based%20Process%20Verifier%20via%20Low-Cost%20Variance%20Reduction.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Reverse%20Physician-AI%20Relationship%3A%20Full-process%20Clinical%20Diagnosis%20Driven%20by%20a%20Large%20Language%20Model.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FIRESPARQL%3A%20A%20LLM-based%20Framework%20for%20SPARQL%20Query%20Generation%20over%20Scholarly%20Knowledge%20Graphs.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/WE-MATH%202.0%3A%20A%20Versatile%20MathBook%20System%20for%20Incentivizing%20Visual%20Mathematical%20Reasoning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SC2Arena%20and%20StarEvolve%3A%20Benchmark%20and%20Self-Improvement%20Framework%20for%20LLMs%20in%20Complex%20Decision-Making%20Tasks.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AnalogSeeker%3A%20An%20Open-source%20Foundation%20Language%20Model%20for%20Analog%20Circuit%20Design.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Layer-Wise%20Perturbations%20via%20Sparse%20Autoencoders%20for%20Adversarial%20Text%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/LeanRAG%3A%20Knowledge-Graph-Based%20Generation%20with%20Semantic%20Aggregation%20and%20Hierarchical%20Retrieval.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Contrast%20Sensitivity%20Function%20of%20Multimodal%20Vision-Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/What%20to%20Ask%20Next%3F%20Probing%20the%20Imaginative%20Reasoning%20of%20LLMs%20with%20TurtleSoup%20Puzzles.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ReviewRL%3A%20Towards%20Automated%20Scientific%20Review%20with%20RL.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Yet%20another%20algorithmic%20bias%3A%20A%20Discursive%20Analysis%20of%20Large%20Language%20Models%20Reinforcing%20Dominant%20Discourses%20on%20Gender%20and%20Race.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Promoting%20Efficient%20Reasoning%20with%20Verifiable%20Stepwise%20Reward.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Why%20Cannot%20Large%20Language%20Models%20Ever%20Make%20True%20Correct%20Reasoning%3F.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Flexible%20Personalized%20Split%20Federated%20Learning%20for%20On-Device%20Fine-Tuning%20of%20Foundation%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MRFD%3A%20Multi-Region%20Fusion%20Decoding%20with%20Self-Consistency%20for%20Mitigating%20Hallucinations%20in%20LVLMs.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Generalizable%20Federated%20Learning%20using%20Client%20Adaptive%20Focal%20Modulation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From%20Black%20Box%20to%20Transparency%3A%20Enhancing%20Automated%20Interpreting%20Assessment%20with%20Explainable%20AI%20in%20College%20Classrooms.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Who%20Benefits%20from%20AI%20Explanations%3F%20Towards%20Accessible%20and%20Interpretable%20Systems.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Insights%20from%20the%20Algonauts%202025%20Winners.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/APFL%3A%20Analytic%20Personalized%20Federated%20Learning%20via%20Dual-Stream%20Least%20Squares.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FreeGAD%3A%20A%20Training-Free%20yet%20Effective%20Approach%20for%20Graph%20Anomaly%20Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Deep%20Learning%20in%20Classical%20and%20Quantum%20Physics.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Self-Supervised%20Temporal%20Super-Resolution%20of%20Energy%20Data%20using%20Generative%20Adversarial%20Transformers.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FAKE%20SPEECH%20WILD%3A%20DETECTING%20DEEPFAKE%20SPEECH%20ON%20SOCIAL%20MEDIA%20PLATFORM.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Contrastive%20ECOC%3A%20Learning%20Output%20Codes%20for%20Adversarial%20Defense.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GraphFedMIG%3A%20Tackling%20Class%20Imbalance%20in%20Federated%20Graph%20Learning%20via%20Mutual%20Information-Guided%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/HiRef%3A%20Leveraging%20Hierarchical%20Ontology%20and%20Network%20Refinement%20for%20Robust%20Medication%20Recommendation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Mitigating%20Exponential%20Mixed%20Frequency%20Growth%20through%20Frequency%20Selection%20and%20Dimensional%20Separation%20in%20Quantum%20Machine%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/RealAC%3A%20A%20Domain-Agnostic%20Framework%20for%20Realistic%20and%20Actionable%20Counterfactual%20Explanations.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Semantic%20Communication%20with%20Distribution%20Learning%20through%20Sequential%20Observations.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/eMamba%3A%20Efficient%20Acceleration%20Framework%20for%20Mamba%20Models%20in%20Edge%20Computing.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Layer-Wise%20Analysis%20of%20Self-Supervised%20Representations%20for%20Age%20and%20Gender%20Classification%20in%20Children%E2%80%99s%20Speech.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Vision-Language%20Pre-training%20Model-Guided%20Approach%20for%20Mitigating%20Backdoor%20Attacks%20in%20Federated%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Pruning%20and%20Malicious%20Injection%3A%20A%20Retraining-Free%20Backdoor%20Attack%20on%20Transformer%20Models.jpg">
<meta property="article:published_time" content="2025-08-18T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-18T09:40:54.428Z">
<meta property="article:author" content="Ywfhhh">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Non-Stationary%20Restless%20Multi-Armed%20Bandits%20with%20Provable%20Guarantee.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>ä»Šæ—¥è®ºæ–‡æŠ¥çº¸ - è®ºæ–‡æŠ¥çº¸é›†åˆ</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- ä¸»é¢˜ä¾èµ–çš„å›¾æ ‡åº“ï¼Œä¸è¦è‡ªè¡Œä¿®æ”¹ -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ã€å­¦æœ¯åŠ¨æ€æŠ¥å‘Šã€‘è®ºæ–‡æŠ¥çº¸</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>è®ºæ–‡é›†åˆ</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="ä»Šæ—¥è®ºæ–‡æŠ¥çº¸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-18 00:00" pubdate>
          æ˜ŸæœŸä¸€, å…«æœˆ 18æ—¥ 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          20k å­—
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> æ¬¡
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">ä»Šæ—¥è®ºæ–‡æŠ¥çº¸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>ä»Šæ—¥è®ºæ–‡æŠ¥çº¸</h1></center>

<p>ğŸ“… <strong>æ—¥æœŸ</strong>ï¼š2025-08-18<br>ğŸ“„ <strong>å‘ç°è®ºæ–‡æ•°é‡</strong>ï¼š132  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Non-Stationary-Restless-Multi-Armed-Bandits-with-Provable-Guarantee"><a href="#1-Non-Stationary-Restless-Multi-Armed-Bandits-with-Provable-Guarantee" class="headerlink" title="1. Non-Stationary Restless Multi-Armed Bandits with Provable Guarantee"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Non-Stationary_Restless_Multi-Armed_Bandits_with_Provable_Guarantee.pdf">Non-Stationary Restless Multi-Armed Bandits with Provable Guarantee</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">National Yang Ming Chiao Tung University</span></p>
<p>æœ¬æ–‡æå‡ºNS-Whittleç®—æ³•ï¼Œé’ˆå¯¹å¸¦æœ‰æœ‰é™å˜åŒ–é¢„ç®—çš„éå¹³ç¨³Restless Multi-Armed Banditsï¼ˆRMABsï¼‰ï¼Œå°†æ»‘åŠ¨çª—å£å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ä¸Šç½®ä¿¡ç•Œï¼ˆUCBï¼‰æœºåˆ¶ç»“åˆï¼Œå®ç°å¯¹è½¬ç§»åŠ¨æ€åŠå…¶å˜åŒ–çš„è‡ªé€‚åº”å­¦ä¹ ã€‚æ–¹æ³•é‡‡ç”¨æ¯è‡‚ç‹¬ç«‹çš„æ»‘åŠ¨çª—å£å’ŒWhittleæŒ‡æ•°ï¼Œæ˜¾è‘—é™ä½ç»´åº¦å’Œè®¡ç®—å¤æ‚åº¦ï¼Œå¹¶é¦–æ¬¡åœ¨éå¹³ç¨³RMABä¸‹ç»™å‡ºOe(NÂ²B^{1&#x2F;4}T^{3&#x2F;4})çš„ç†è®ºåæ‚”ç•Œé™ã€‚ç»“è®ºè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…¼é¡¾ç†è®ºä¸¥è°¨æ€§å’Œå®é™…å¯æ‰©å±•æ€§ï¼Œä¸ºåŠ¨æ€èµ„æºå—é™ç¯å¢ƒä¸‹çš„åºè´¯å†³ç­–æä¾›äº†é«˜æ•ˆåŸºç¡€ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Non-Stationary%20Restless%20Multi-Armed%20Bandits%20with%20Provable%20Guarantee.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-Variance-Reduced-Policy-Gradient-Method-for-Multi-Objective-Reinforcement-Learning"><a href="#2-Variance-Reduced-Policy-Gradient-Method-for-Multi-Objective-Reinforcement-Learning" class="headerlink" title="2. Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Variance_Reduced_Policy_Gradient_Method_for_Multi-Objective_Reinforcement_Learning.pdf">Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">ETH Zurich</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†MO-TSIVR-PGç®—æ³•ï¼Œé€šè¿‡åœ¨å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ (MORL)ä¸­å¯¹ç­–ç•¥æ¢¯åº¦æ–¹æ³•å¼•å…¥æ–¹å·®é™ä½æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†æ ·æœ¬æ•ˆç‡ï¼Œæ”¯æŒå¤§è§„æ¨¡å’Œè¿ç»­çŠ¶æ€-åŠ¨ä½œç©ºé—´ã€‚ç†è®ºåˆ†æå’Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨æ”¶æ•›é€Ÿåº¦å’Œæ ·æœ¬å¤æ‚åº¦ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”åœ¨å¤šç›®æ ‡ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Variance%20Reduced%20Policy%20Gradient%20Method%20for%20Multi-Objective%20Reinforcement%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-Pass-k-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models"><a href="#3-Pass-k-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models" class="headerlink" title="3. Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Pass@k_Training_for_Adaptively_Balancing_Exploration_and_Exploitation_of_Large_Reasoning_Models.pdf">Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Renmin University of China</span></p>
<p>æœ¬æ–‡æå‡ºäº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†ä»»åŠ¡ä¸­çš„RLVRè®­ç»ƒä¸­ï¼Œé‡‡ç”¨Pass@kä½œä¸ºå¥–åŠ±æŒ‡æ ‡çš„æ–¹æ³•ï¼ˆPass@k Trainingï¼‰ï¼Œé€šè¿‡å…¨é‡‡æ ·ã€bootstrapé‡‡æ ·å’Œè§£ææ¨å¯¼ä¸‰ç§æµç¨‹ï¼Œæœ‰æ•ˆæå‡æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å¹¶ä¿æŒå…¶åˆ©ç”¨èƒ½åŠ›ã€‚ç»“è®ºæ˜¾ç¤ºï¼ŒPass@k Trainingä¸ä»…æå‡äº†æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„Pass@kè¡¨ç°ï¼Œè¿˜èƒ½é€šè¿‡åç»­Pass@1è®­ç»ƒå°†æ¢ç´¢æ”¶ç›Šè½¬åŒ–ä¸ºæ›´å¼ºçš„Pass@1è¡¨ç°ï¼Œå…·å¤‡è‰¯å¥½çš„æ³›åŒ–æ€§å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Pass%40k%20Training%20for%20Adaptively%20Balancing%20Exploration%20and%20Exploitation%20of%20Large%20Reasoning%20Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-REFN-A-Reinforcement-Learning-From-Network-Framework-against-1-day-n-day-Exploitations"><a href="#4-REFN-A-Reinforcement-Learning-From-Network-Framework-against-1-day-n-day-Exploitations" class="headerlink" title="4. REFN: A Reinforcement-Learning-From-Network Framework against 1-day&#x2F;n-day Exploitations"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/REFN__A_Reinforcement-Learning-From-Network_Framework_against_1-day_n-day_Exploitations.pdf">REFN: A Reinforcement-Learning-From-Network Framework against 1-day&#x2F;n-day Exploitations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Artificial Intelligence, Hubei University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºREFNæ¡†æ¶ï¼Œé€šè¿‡ç½‘ç»œé©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè‡ªåŠ¨ç”Ÿæˆå¹¶éƒ¨ç½²ç½‘ç»œè¿‡æ»¤å™¨ï¼Œæœ‰æ•ˆé˜²å¾¡å¤§è§„æ¨¡1-day&#x2F;n-dayæ¼æ´åˆ©ç”¨ã€‚REFNé‡‡ç”¨Agentic-RAGçŸ¥è¯†è’¸é¦ã€RL-from-VNFç®¡é“å’Œåœ¨çº¿AgenticéªŒè¯ï¼Œæ˜¾è‘—æå‡è¿‡æ»¤è§„åˆ™ç”Ÿæˆçš„å‡†ç¡®æ€§ã€éƒ¨ç½²æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒREFNåœ¨å‡†ç¡®ç‡å’ŒF1åˆ†æ•°æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡ä¿®å¤æ—¶é—´ç¼©çŸ­è‡³3.65å°æ—¶ï¼Œèƒ½é«˜æ•ˆæ‰©å±•è‡³ä¸‡å°è®¾å¤‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/REFN%3A%20A%20Reinforcement-Learning-From-Network%20Framework%20against%201-day/n-day%20Exploitations.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-Nonlocal-Monte-Carlo-via-Reinforcement-Learning"><a href="#5-Nonlocal-Monte-Carlo-via-Reinforcement-Learning" class="headerlink" title="5. Nonlocal Monte Carlo via Reinforcement Learning"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Nonlocal_Monte_Carlo_via_Reinforcement_Learning.pdf">Nonlocal Monte Carlo via Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Peter GrÃ¼nberg Institut (PGI-14), Forschungszentrum JÃ¼lich GmbH</span></p>
<p>è¯¥è®ºæ–‡æå‡ºRLNMCæ–¹æ³•ï¼Œå°†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸Nonequilibrium Nonlocal Monte Carloï¼ˆNMCï¼‰ç®—æ³•ç»“åˆï¼Œé€šè¿‡è®­ç»ƒå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç­–ç•¥è‡ªåŠ¨å‘ç°ç»„åˆä¼˜åŒ–ä¸­çš„éå±€éƒ¨è·³è·ƒæ­¥éª¤ã€‚å®éªŒè¡¨æ˜ï¼ŒRLNMCåœ¨å…¸å‹éš¾è§£4-SATåŸºå‡†é—®é¢˜ä¸Šï¼Œç›¸è¾ƒäºæ ‡å‡†MCMCå’Œä¼ ç»ŸNMCæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ®‹å·®èƒ½é‡ã€è§£çš„å¤šæ ·æ€§å’Œæ±‚è§£æ—¶é—´ç­‰æŒ‡æ ‡ä¸Šå–å¾—æ˜æ˜¾æå‡ï¼Œå¹¶å±•ç°å‡ºè¾ƒå¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Nonlocal%20Monte%20Carlo%20via%20Reinforcement%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-Stabilizing-Long-term-Multi-turn-Reinforcement-Learning-with-Gated-Rewards"><a href="#6-Stabilizing-Long-term-Multi-turn-Reinforcement-Learning-with-Gated-Rewards" class="headerlink" title="6. Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Stabilizing_Long-term_Multi-turn_Reinforcement_Learning_with_Gated_Rewards.pdf">Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Harbin Institute of Technology (Shenzhen)</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†SWE-oriented RL Frameworkå’ŒGated Reward Accumulation (G-RA)æ–¹æ³•ï¼Œé’ˆå¯¹è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­çš„å¤šè½®é•¿æ—¶åºå¼ºåŒ–å­¦ä¹ ï¼Œç»“åˆå¤šå±‚å¥–åŠ±è®¾è®¡åŠdockerç¯å¢ƒæ”¯æŒï¼Œå®ç°äº†å¥–åŠ±çš„é—¨æ§ç´¯ç§¯ï¼Œä»…åœ¨é«˜å±‚(é•¿æœŸ)å¥–åŠ±è¾¾åˆ°é˜ˆå€¼æ—¶ç´¯ç§¯å³æ—¶å¥–åŠ±ï¼Œä»è€Œå¹³è¡¡ç¨€ç–é•¿æœŸç›®æ ‡ä¸å¯†é›†å³æ—¶åé¦ˆã€‚å®éªŒè¡¨æ˜ï¼ŒG-RAæ˜¾è‘—æå‡äº†ä»»åŠ¡å®Œæˆç‡ä¸ä¿®æ”¹ç‡ï¼Œæœ‰æ•ˆé¿å…äº†å¥–åŠ±é”™é…å’Œç­–ç•¥é€€åŒ–ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚é•¿æ—¶åºä»»åŠ¡ä¸­ç¨³å®šRLä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Stabilizing%20Long-term%20Multi-turn%20Reinforcement%20Learning%20with%20Gated%20Rewards.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning"><a href="#7-Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning" class="headerlink" title="7. Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Few-shot_Vision-based_Human_Activity_Recognition_with_MLLM-based_Visual_Reinforcement_Learning.pdf">Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Kyushu University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºFAVORæ–¹æ³•ï¼Œå°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸åŸºäºè§†è§‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆä½¿ç”¨Group Relative Policy Optimization, GRPOå’Œå¯éªŒè¯å¥–åŠ±å‡½æ•°ï¼‰ç»“åˆï¼Œç”¨äºå°‘æ ·æœ¬äººç±»æ´»åŠ¨è¯†åˆ«ã€‚å®éªŒè¡¨æ˜FAVORåœ¨HARä»»åŠ¡çš„å°‘æ ·æœ¬å­¦ä¹ ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹æå‡æ³›åŒ–å’Œæ¨ç†èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Few-shot%20Vision-based%20Human%20Activity%20Recognition%20with%20MLLM-based%20Visual%20Reinforcement%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-Multi-Agent-Trust-Region-Policy-Optimisation-A-Joint-Constraint-Approach"><a href="#8-Multi-Agent-Trust-Region-Policy-Optimisation-A-Joint-Constraint-Approach" class="headerlink" title="8. Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Multi-Agent_Trust_Region_Policy_Optimisation__A_Joint_Constraint_Approach.pdf">Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Maryland</span></p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹å¤šæ™ºèƒ½ä½“ä¿¡ä»»åŸŸç­–ç•¥ä¼˜åŒ–ï¼ˆHATRPOï¼‰å‡åŒ€KLçº¦æŸä½æ•ˆé—®é¢˜çš„ä¸¤ç§è‡ªé€‚åº”æ‰©å±•æ–¹æ³•ï¼šHATRPO-Gï¼ˆåŸºäºæ”¹è¿›-æ•£åº¦æ¯”çš„è´ªå¿ƒåˆ†é…ï¼‰å’ŒHATRPO-Wï¼ˆåŸºäºKKTæ¡ä»¶çš„å…¨å±€ä¼˜åŒ–åˆ†é…ï¼‰ï¼Œå®ç°æ€»KLé˜ˆå€¼åœ¨æ™ºèƒ½ä½“é—´çš„çµæ´»åˆ†é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºæ–¹æ³•èƒ½æ›´å¿«æ”¶æ•›å¹¶æ˜¾è‘—æå‡æœ€ç»ˆå¥–åŠ±ï¼Œå°¤å…¶åœ¨å¼‚è´¨å’Œä¸å¹³è¡¡æ™ºèƒ½ä½“åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡22.5%çš„æ€§èƒ½æå‡ï¼Œå¹¶æœ‰æ•ˆæ•è·æ™ºèƒ½ä½“é—´ä¼˜åŠ¿å·®å¼‚ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Agent%20Trust%20Region%20Policy%20Optimisation%3A%20A%20Joint%20Constraint%20Approach.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-Integrating-Reinforcement-Learning-with-Visual-Generative-Models-Foundations-and-Advances"><a href="#9-Integrating-Reinforcement-Learning-with-Visual-Generative-Models-Foundations-and-Advances" class="headerlink" title="9. Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Integrating_Reinforcement_Learning_with_Visual_Generative_Models__Foundations_and_Advances.pdf">Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute of Artificial Intelligence (TeleAI), China Telecom</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿç»¼è¿°äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸è§†è§‰ç”Ÿæˆæ¨¡å‹ï¼ˆåŒ…æ‹¬å›¾åƒã€è§†é¢‘ã€3D&#x2F;4Då†…å®¹ç”Ÿæˆï¼‰ç»“åˆçš„æœ€æ–°è¿›å±•ï¼Œæ¶µç›–RLç†è®ºæ¼”å˜ã€ä¸»æµRLæ–¹æ³•ï¼ˆå¦‚PPOã€GRPOã€DPOï¼‰åœ¨æå‡ç”Ÿæˆæ¨¡å‹å¯æ§æ€§ã€è¯­ä¹‰å¯¹é½ã€äººç±»åå¥½åŒ¹é…ç­‰æ–¹é¢çš„åº”ç”¨æµç¨‹ã€‚ç»“è®ºæŒ‡å‡ºï¼ŒRLä¸ä»…ä½œä¸ºç”Ÿæˆæ¨¡å‹çš„å¾®è°ƒå’Œä¼˜åŒ–å±‚ï¼Œæ›´æˆä¸ºå®ç°å¤æ‚ç›®æ ‡å’Œäººç±»æ„å›¾å¯¹é½çš„ç»“æ„æ€§ç»„ä»¶ï¼Œå¯¹æœªæ¥å¤šæ¨¡æ€ã€äº¤äº’å¼ç”Ÿæˆç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Integrating%20Reinforcement%20Learning%20with%20Visual%20Generative%20Models%3A%20Foundations%20and%20Advances.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-A-Curriculum-Learning-Approach-to-Reinforcement-Learning-Leveraging-RAG-for-Multimodal-Question-Answering"><a href="#10-A-Curriculum-Learning-Approach-to-Reinforcement-Learning-Leveraging-RAG-for-Multimodal-Question-Answering" class="headerlink" title="10. A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Curriculum_Learning_Approach_to_Reinforcement_Learning__Leveraging_RAG_for_Multimodal_Question_Ans.pdf">A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Meituan</span></p>
<p>æœ¬æ–‡æå‡ºäº†ç»“åˆè§†è§‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰ã€çŸ¥è¯†è’¸é¦ã€RAGæ£€ç´¢å¢å¼ºç”Ÿæˆå’Œè¯¾ç¨‹å­¦ä¹ çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºå¤šæ¨¡æ€ã€å¤šè½®æ¬¡é—®é¢˜å›ç­”ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†é˜¶æ®µè¯¾ç¨‹å­¦ä¹ ç¨³å®šå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæœ‰æ•ˆæå‡æ¨¡å‹åœ¨å¤šæ¨¡æ€æ£€ç´¢ä¸é—®é¢˜å›ç­”ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡å’Œå¯é æ€§ï¼Œåœ¨KDD Cup 2025 CRAG-MMç«èµ›çš„å¤šé¡¹ä»»åŠ¡ä¸­å–å¾—é¢†å…ˆæˆç»©ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Curriculum%20Learning%20Approach%20to%20Reinforcement%20Learning%3A%20Leveraging%20RAG%20for%20Multimodal%20Question%20Answering.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-Multi-Agent-Reinforcement-Learning-for-Adaptive-Resource-Orchestration-in-Cloud-Native-Clusters"><a href="#11-Multi-Agent-Reinforcement-Learning-for-Adaptive-Resource-Orchestration-in-Cloud-Native-Clusters" class="headerlink" title="11. Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Multi-Agent_Reinforcement_Learning_for_Adaptive_Resource_Orchestration_in_Cloud-Native_Clusters.pdf">Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Northwestern University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰çš„è‡ªé€‚åº”èµ„æºç¼–æ’æ–¹æ³•ï¼Œé‡‡ç”¨å¼‚æ„è§’è‰²é©±åŠ¨çš„ä»£ç†åˆä½œæœºåˆ¶ï¼ˆHRACï¼‰å’Œå±€éƒ¨-å…¨å±€å¥–åŠ±å¡‘å½¢ï¼ˆLGRSï¼‰ç­–ç•¥ï¼Œå®ç°å¯¹è®¡ç®—èŠ‚ç‚¹ã€å­˜å‚¨èŠ‚ç‚¹å’Œè°ƒåº¦å™¨ç­‰å¼‚æ„ç»„ä»¶çš„åˆ†å·¥å­¦ä¹ ä¸åä½œã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨èµ„æºåˆ©ç”¨ç‡ã€è°ƒåº¦å»¶è¿Ÿå’Œç­–ç•¥æ”¶æ•›é€Ÿåº¦ç­‰æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶åœ¨å¤šç§Ÿæˆ·å…¬å¹³æ€§å’Œä¸å®Œå…¨ä¿¡æ¯åœºæ™¯ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Agent%20Reinforcement%20Learning%20for%20Adaptive%20Resource%20Orchestration%20in%20Cloud-Native%20Clusters.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="12-TLE-Based-A2C-Agent-for-Terrestrial-Coverage-Orbital-Path-Planning"><a href="#12-TLE-Based-A2C-Agent-for-Terrestrial-Coverage-Orbital-Path-Planning" class="headerlink" title="12. TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/TLE-Based_A2C_Agent_for_Terrestrial_Coverage_Orbital_Path_Planning.pdf">TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Sardar Vallabhbhai National Institute of Technology Surat</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºTLEçš„è½¨é“ä»¿çœŸç¯å¢ƒï¼Œåˆ©ç”¨Advantage Actor-Critic (A2C) å¼ºåŒ–å­¦ä¹ ç®—æ³•è‡ªåŠ¨ä¼˜åŒ–åœ°çƒè§‚æµ‹å«æ˜Ÿçš„è½¨é“å‚æ•°ï¼Œå®ç°å¯¹åœ°è¡¨ç›®æ ‡çš„ç²¾ç¡®è¦†ç›–ã€‚å®éªŒè¡¨æ˜ï¼ŒA2Cåœ¨æ”¶æ•›é€Ÿåº¦å’Œç´¯è®¡å¥–åŠ±ä¸Šå‡ä¼˜äºProximal Policy Optimization (PPO)ï¼Œèƒ½é«˜æ•ˆæ»¡è¶³ä»»åŠ¡ç›®æ ‡ï¼ŒéªŒè¯äº†å¼ºåŒ–å­¦ä¹ åœ¨ä½è½¨å«æ˜Ÿè½¨é“è§„åˆ’ä¸­çš„å®ç”¨æ€§å’Œé«˜æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/TLE-Based%20A2C%20Agent%20for%20Terrestrial%20Coverage%20Orbital%20Path%20Planning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-A-Multimodal-Neural-Network-for-Recognizing-Subjective-Self-Disclosure-Towards-Social-Robots"><a href="#13-A-Multimodal-Neural-Network-for-Recognizing-Subjective-Self-Disclosure-Towards-Social-Robots" class="headerlink" title="13. A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Multimodal_Neural_Network_for_Recognizing_Subjective_Self-Disclosure_Towards_Social_Robots.pdf">A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Amazon</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸“ä¸ºäººæœºäº¤äº’ä¸­ä¸»è§‚è‡ªæˆ‘è¡¨éœ²å»ºæ¨¡è®¾è®¡çš„å¤šæ¨¡æ€æ³¨æ„åŠ›ç¥ç»ç½‘ç»œï¼Œç»“åˆéŸ³é¢‘ï¼ˆMFCCã€wav2vec2.0ï¼‰ä¸è§†è§‰ï¼ˆOpenFaceã€InceptionV1-ResNetï¼‰ç‰¹å¾è¾“å…¥ï¼Œå¹¶åˆ›æ–°æ€§åœ°å¼•å…¥äº†å°ºåº¦ä¿æŒäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œæœ‰æ•ˆå¤„ç†äº†åˆ†ç±»ä¸å›å½’ä¹‹é—´çš„é—®é¢˜ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨è‡ªå»ºå¤§è§„æ¨¡æ•°æ®é›†ä¸ŠF1åˆ†æ•°è¾¾0.83ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œä¸ºç¤¾ä¼šæœºå™¨äººæ„ŸçŸ¥äººç±»è‡ªæˆ‘è¡¨éœ²èƒ½åŠ›æä¾›äº†é‡è¦çªç ´ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Multimodal%20Neural%20Network%20for%20Recognizing%20Subjective%20Self-Disclosure%20Towards%20Social%20Robots.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT"><a href="#14-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT" class="headerlink" title="14. UI-Venus Technical Report: Building High-performance UI Agents with RFT"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/UI-Venus_Technical_Report__Building_High-performance_UI_Agents_with_RFT.pdf">UI-Venus Technical Report: Building High-performance UI Agents with RFT</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Ant Group</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†UI-Venusï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆQwen2.5-VLï¼‰å¹¶ä»…éœ€æˆªå›¾è¾“å…¥çš„åŸç”ŸUIæ™ºèƒ½ä½“ï¼Œé€šè¿‡å¼•å…¥åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼ˆRFTï¼‰æ–¹æ³•ï¼Œå®ç°äº†UIç•Œé¢å…ƒç´ å®šä½ä¸å¯¼èˆªä»»åŠ¡çš„ä¸šç•Œæœ€ä¼˜ï¼ˆSOTAï¼‰æ€§èƒ½ã€‚æ–¹æ³•åŒ…æ‹¬é«˜è´¨é‡æ•°æ®æ¸…æ´—ã€ç²¾ç»†åŒ–å¥–åŠ±å‡½æ•°è®¾è®¡ä»¥åŠè‡ªè¿›åŒ–è½¨è¿¹å†å²å¯¹é½ä¸ç¨€ç–åŠ¨ä½œå¢å¼ºæ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚UIä»»åŠ¡ä¸­çš„æ³›åŒ–å’Œè§„åˆ’èƒ½åŠ›ã€‚ç»“è®ºæ˜¯ï¼ŒUI-Venusåœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†ä¸Šè¶…è¶Šç°æœ‰å¼€æºä¸é—­æºæ¨¡å‹ï¼ŒéªŒè¯äº†å¼ºåŒ–å­¦ä¹ ç­–ç•¥å’Œæ•°æ®å¤„ç†æµç¨‹åœ¨UI Agentè®­ç»ƒä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å·²å¼€æºç›¸å…³æ¨¡å‹åŠè¯„æµ‹ä»£ç ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/UI-Venus%20Technical%20Report%3A%20Building%20High-performance%20UI%20Agents%20with%20RFT.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-Scaling-Up-without-Fading-Out-Goal-Aware-Sparse-GNN-for-RL-based-Generalized-Planning"><a href="#15-Scaling-Up-without-Fading-Out-Goal-Aware-Sparse-GNN-for-RL-based-Generalized-Planning" class="headerlink" title="15. Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Scaling_Up_without_Fading_Out__Goal-Aware_Sparse_GNN_for_RL-based_Generalized_Planning.pdf">Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">LIG Nex1</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆç¨€ç–ã€ç›®æ ‡æ„ŸçŸ¥å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰è¡¨ç¤ºå’Œè¯¾ç¨‹å­¦ä¹ çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºè§£å†³å¤§è§„æ¨¡åŸºäºPDDLçš„æ— äººæœºä»»åŠ¡è§„åˆ’é—®é¢˜ã€‚é€šè¿‡ä»…ç¼–ç å±€éƒ¨é‚»æ¥å…³ç³»å¹¶æ˜¾å¼æ•´åˆç›®æ ‡ç©ºé—´ç‰¹å¾ï¼Œæœ‰æ•ˆæå‡äº†è®­ç»ƒæ•ˆç‡å’Œç­–ç•¥æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—é™ä½äº†GPUå†…å­˜éœ€æ±‚ï¼Œå®ç°äº†åœ¨å¤§è§„æ¨¡ç½‘æ ¼ç¯å¢ƒä¸­é«˜æ•ˆçš„ä»»åŠ¡è§„åˆ’ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚æ— äººæœºä»»åŠ¡åœºæ™¯ä¸­å…·æœ‰ä¼˜è¶Šçš„æ‰©å±•æ€§å’Œç¨³å®šæ€§ï¼Œå…‹æœäº†ä»¥å¾€å…¨è¿æ¥å›¾æ–¹æ³•çš„ç“¶é¢ˆã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Scaling%20Up%20without%20Fading%20Out%3A%20Goal-Aware%20Sparse%20GNN%20for%20RL-based%20Generalized%20Planning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-Synthesis-of-Deep-Neural-Networks-with-Safe-Robust-Adaptive-Control-for-Reliable-Operation-of-Wheeled-Mobile-Robots"><a href="#16-Synthesis-of-Deep-Neural-Networks-with-Safe-Robust-Adaptive-Control-for-Reliable-Operation-of-Wheeled-Mobile-Robots" class="headerlink" title="16. Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Synthesis_of_Deep_Neural_Networks_with_Safe_Robust_Adaptive_Control_for_Reliable_Operation_of_Wheele.pdf">Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tampere University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ä¸å®‰å…¨é²æ£’è‡ªé€‚åº”æ§åˆ¶ï¼ˆRACï¼‰çš„åˆ†å±‚æ§åˆ¶ç­–ç•¥ï¼Œå®ç°é‡å‹è½®å¼ç§»åŠ¨æœºå™¨äººï¼ˆWMRï¼‰åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å¯é è¿è¡Œã€‚æ–¹æ³•åŒ…æ‹¬ä»¥DNNä¸ºä¸»æ§åˆ¶ï¼Œåœ¨é‡åˆ°å¤–éƒ¨å¹²æ‰°æ—¶ç”±ä½å±‚å®‰å…¨æœºåˆ¶åˆ‡æ¢ä¸ºRACï¼Œç³»ç»Ÿå§‹ç»ˆå—ä¸¤çº§å®‰å…¨å±‚ç›‘æ§ï¼Œä¿éšœç²¾åº¦ä¸ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•èƒ½åœ¨æ»¡è¶³å®‰å…¨æ ‡å‡†çš„å‰æä¸‹ï¼Œå®ç°é«˜ç²¾åº¦å’Œé²æ£’æ€§ï¼Œå¹¶æœ‰æ•ˆåº”å¯¹å®é™…å·¥å†µä¸‹çš„æ‰°åŠ¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Synthesis%20of%20Deep%20Neural%20Networks%20with%20Safe%20Robust%20Adaptive%20Control%20for%20Reliable%20Operation%20of%20Wheeled%20Mobile%20Robots.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-Learning-Task-Execution-Hierarchies-for-Redundant-Robots"><a href="#17-Learning-Task-Execution-Hierarchies-for-Redundant-Robots" class="headerlink" title="17. Learning Task Execution Hierarchies for Redundant Robots"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Learning_Task_Execution_Hierarchies_for_Redundant_Robots.pdf">Learning Task Execution Hierarchies for Redundant Robots</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Padova</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œé—ä¼ ç¼–ç¨‹ï¼ˆGenetic Programming, GPï¼‰çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·è‡ªå®šä¹‰çš„é«˜å±‚ä»»åŠ¡ç›®æ ‡å’Œä»£ä»·å‡½æ•°ï¼Œè‡ªåŠ¨ä¸ºå†—ä½™æœºå™¨äººå­¦ä¹ ä»»åŠ¡ä¼˜å…ˆçº§ã€æ§åˆ¶å‚æ•°å’Œä»»åŠ¡æ¿€æ´»é€»è¾‘ï¼ˆStack of Tasks, SoTsï¼‰ã€‚æ–¹æ³•æµç¨‹åŒ…æ‹¬ï¼šç”¨æˆ·é€šè¿‡GUIè®¾å®šä»»åŠ¡ç›®æ ‡æƒé‡ï¼Œç³»ç»Ÿåœ¨ä»¿çœŸä¸­ä»¥å¼ºåŒ–å­¦ä¹ æ–¹å¼è¯„ä¼°æ¯ç»„ä»»åŠ¡æ ˆçš„è¡¨ç°ï¼Œé€šè¿‡é—ä¼ ç¼–ç¨‹æ¼”åŒ–ä¼˜åŒ–ä»»åŠ¡é¡ºåºå’Œå‚æ•°ï¼Œæœ€ç»ˆé€‰å–æœ€ä¼˜SoTè¿›è¡ŒçœŸå®æœºå™¨äººæµ‹è¯•ã€‚å®éªŒåœ¨ABBç§»åŠ¨åŒè‡‚æœºå™¨äººYuMiå¹³å°çš„ä»¿çœŸå’Œç°å®ç¯å¢ƒä¸­è¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å®ç°é›¶æ ·æœ¬è¿ç§»ã€é€‚åº”åŠ¨æ€ç¯å¢ƒã€æ— éœ€ä¸“å®¶æ‰‹å·¥è°ƒå‚ï¼Œæ˜¾è‘—æå‡äº†å†—ä½™æœºå™¨äººä»»åŠ¡ç®¡ç†çš„é€šç”¨æ€§å’Œè‡ªé€‚åº”æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning%20Task%20Execution%20Hierarchies%20for%20Redundant%20Robots.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-MLM-Learning-Multi-task-Loco-Manipulation-Whole-Body-Control-for-Quadruped-Robot-with-Arm"><a href="#18-MLM-Learning-Multi-task-Loco-Manipulation-Whole-Body-Control-for-Quadruped-Robot-with-Arm" class="headerlink" title="18. MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MLM__Learning_Multi-task_Loco-Manipulation_Whole-Body_Control_for_Quadruped_Robot_with_Arm.pdf">MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Jiao Tong University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆçœŸå®å’Œä»¿çœŸæ•°æ®çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶MLMï¼Œå®ç°äº†å¸¦æœºæ¢°è‡‚å››è¶³æœºå™¨äººå¤šä»»åŠ¡å…¨èº«è¿åŠ¨-æ“ä½œä¸€ä½“åŒ–æ§åˆ¶ã€‚æ–¹æ³•åŒ…æ‹¬å¼•å…¥åŸºäºè‡ªé€‚åº”è¯¾ç¨‹çš„è½¨è¿¹é‡‡æ ·æœºåˆ¶å’Œè½¨è¿¹-é€Ÿåº¦é¢„æµ‹ç­–ç•¥ç½‘ç»œï¼Œå®ç°å¤šä»»åŠ¡è‡ªé€‚åº”å­¦ä¹ ä¸å†å²è½¨è¿¹ä¸‹çš„é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒä¸­å®ç°å¤šä»»åŠ¡é›¶æ ·æœ¬è¿ç§»ï¼Œæå‡äº†å››è¶³æœºå™¨äººè¿åŠ¨ä¸æ“ä½œååŒçš„é€šç”¨æ€§å’Œä»»åŠ¡è¡¨ç°ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MLM%3A%20Learning%20Multi-task%20Loco-Manipulation%20Whole-Body%20Control%20for%20Quadruped%20Robot%20with%20Arm.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-Projected-Coupled-Diffusion-for-Test-Time-Constrained-Joint-Generation"><a href="#19-Projected-Coupled-Diffusion-for-Test-Time-Constrained-Joint-Generation" class="headerlink" title="19. Projected Coupled Diffusion for Test-Time Constrained Joint Generation"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Projected_Coupled_Diffusion_for_Test-Time_Constrained_Joint_Generation.pdf">Projected Coupled Diffusion for Test-Time Constrained Joint Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">National University of Singapore</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºProjected Coupled Diffusion (PCD)çš„æ–°å‹æµ‹è¯•æ—¶çº¦æŸè”åˆç”Ÿæˆæ¡†æ¶ï¼Œåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„å‰æä¸‹ï¼Œé€šè¿‡å¼•å…¥è€¦åˆå¼•å¯¼é¡¹å’Œæ¯æ­¥æŠ•å½±æ“ä½œï¼Œä½¿å¤šä¸ªé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆç›¸å…³æ ·æœ¬æ—¶èƒ½ç²¾ç¡®æ»¡è¶³å®æ—¶çº¦æŸã€‚æ–¹æ³•åœ¨å›¾åƒå¯¹ç”Ÿæˆã€æœºå™¨äººæ“ä½œå’Œå¤šæœºå™¨äººè·¯å¾„è§„åˆ’ç­‰ä»»åŠ¡ä¸­éªŒè¯ï¼Œå®éªŒè¡¨æ˜PCDèƒ½æœ‰æ•ˆæå‡æ ·æœ¬ç›¸å…³æ€§å¹¶ä¸¥æ ¼æ»¡è¶³ä»»åŠ¡çº¦æŸï¼Œä¸”è®¡ç®—å¼€é”€å¯æ§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Projected%20Coupled%20Diffusion%20for%20Test-Time%20Constrained%20Joint%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-MASH-Cooperative-Heterogeneous-Multi-Agent-Reinforcement-Learning-for-Single-Humanoid-Robot-Locomotion"><a href="#20-MASH-Cooperative-Heterogeneous-Multi-Agent-Reinforcement-Learning-for-Single-Humanoid-Robot-Locomotion" class="headerlink" title="20. MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MASH__Cooperative-Heterogeneous_Multi-Agent_Reinforcement_Learning_for_Single_Humanoid_Robot_Locomot.pdf">MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Northeastern University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†MASHæ–¹æ³•ï¼Œå°†å•ä¸ªäººå½¢æœºå™¨äººå››è‚¢ï¼ˆåŒè…¿ä¸åŒè‡‚ï¼‰å»ºæ¨¡ä¸ºç‹¬ç«‹æ™ºèƒ½ä½“ï¼Œé€šè¿‡åä½œå¼‚è´¨å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¼˜åŒ–æœºå™¨äººæ­¥æ€ä¸åŠ¨ä½œåè°ƒï¼Œé‡‡ç”¨ä¸­å¤®è¯„è®ºå‘˜å’Œå¤šæ™ºèƒ½ä½“PPOè®­ç»ƒã€‚å®éªŒè¯æ˜MASHç›¸æ¯”å•æ™ºèƒ½ä½“RLæ”¶æ•›æ›´å¿«ã€åŠ¨ä½œæ›´å¹³æ»‘ã€è‚¢ä½“åè°ƒæ€§æ›´é«˜ï¼Œå¹¶ä¸”å…·å¤‡æ›´å¼ºçš„å®é™…éƒ¨ç½²é²æ£’æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MASH%3A%20Cooperative-Heterogeneous%20Multi-Agent%20Reinforcement%20Learning%20for%20Single%20Humanoid%20Robot%20Locomotion.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-KDPE-A-Kernel-Density-Estimation-Strategy-for-Diffusion-Policy-Trajectory-Selection"><a href="#21-KDPE-A-Kernel-Density-Estimation-Strategy-for-Diffusion-Policy-Trajectory-Selection" class="headerlink" title="21. KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/KDPE__A_Kernel_Density_Estimation_Strategy_for_Diffusion_Policy_Trajectory_Selection.pdf">KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Istituto Italiano di Tecnologia</span></p>
<p>æœ¬æ–‡æå‡ºäº†KDPEï¼Œä¸€ç§åŸºäºæ ¸å¯†åº¦ä¼°è®¡ï¼ˆKDEï¼‰çš„ç­–ç•¥ï¼Œç”¨äºè¿‡æ»¤å’Œé€‰æ‹©Diffusion Policyï¼ˆDPï¼‰ç”Ÿæˆçš„æœºå™¨äººè½¨è¿¹ï¼Œé€šè¿‡å¹¶è¡Œé‡‡æ ·å¤šæ¡è½¨è¿¹å¹¶ç”¨æµå½¢æ„ŸçŸ¥æ ¸å‡½æ•°å»ºæ¨¡åŠ¨ä½œåˆ†å¸ƒï¼Œç­›é™¤å¼‚å¸¸è½¨è¿¹ï¼Œä»…ä¿ç•™æœ€å…·ä»£è¡¨æ€§çš„åŠ¨ä½œè·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKDPEåœ¨ä»¿çœŸå’ŒçœŸå®æœºå™¨äººæŠ“å–ã€åˆ†æ‹£ã€åˆ¶ä½œå’–å•¡ç­‰ä»»åŠ¡ä¸­ï¼Œèƒ½æå‡æˆåŠŸç‡å’Œé²æ£’æ€§ï¼Œå°¤å…¶åœ¨æ¼”ç¤ºè´¨é‡è¾ƒä½æˆ–ç²¾åº¦è¦æ±‚è¾ƒé«˜çš„åœºæ™¯æ•ˆæœæ›´ä¼˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/KDPE%3A%20A%20Kernel%20Density%20Estimation%20Strategy%20for%20Diffusion%20Policy%20Trajectory%20Selection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-Learning-State-Space-Models-of-Dynamic-Systems-from-Arbitrary-Data-using-Joint-Embedding-Predictive-Architectures"><a href="#22-Learning-State-Space-Models-of-Dynamic-Systems-from-Arbitrary-Data-using-Joint-Embedding-Predictive-Architectures" class="headerlink" title="22. Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Learning_State-Space_Models_of_Dynamic_Systems_from_Arbitrary_Data_using_Joint_Embedding_Predictive_.pdf">Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">RPTU University Kaiserslautern-Landau</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºJoint Embedding Predictive Architectures (JEPA) å’Œç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆneural ODEï¼‰çš„æ–°æ–¹æ³•ï¼Œå°†ä»»æ„è§‚æµ‹æ•°æ®åºåˆ—ç¼–ç ä¸ºè¿ç»­æ—¶é—´çš„æ½œåœ¨çŠ¶æ€ç©ºé—´ï¼Œå¹¶é€šè¿‡å¼•å…¥æ”¶ç¼©æ€§å’ŒLipschitzå¸¸æ•°æŸå¤±ï¼Œä¿è¯çŠ¶æ€ç©ºé—´çš„æœ‰åºç»“æ„ã€‚æ–¹æ³•åœ¨ä»¿çœŸæ‘†ç³»ç»Ÿä¸­å®ç°ä»å›¾åƒæ•°æ®ç›´æ¥å­¦ä¹ åŠ¨æ€ç³»ç»Ÿæ½œåœ¨çŠ¶æ€ç©ºé—´ï¼Œå®ç°äº†ä»æ½œåœ¨é¢„æµ‹åˆ°å›¾åƒé‡æ„ï¼Œå±•ç¤ºäº†å¯¹æœºå™¨äººæ§åˆ¶å’Œä¼°è®¡çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning%20State-Space%20Models%20of%20Dynamic%20Systems%20from%20Arbitrary%20Data%20using%20Joint%20Embedding%20Predictive%20Architectures.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-Efficient-Methods-for-Accurate-Sparse-Trajectory-Recovery-and-Map-Matching"><a href="#23-Efficient-Methods-for-Accurate-Sparse-Trajectory-Recovery-and-Map-Matching" class="headerlink" title="23. Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Efficient_Methods_for_Accurate_Sparse_Trajectory_Recovery_and_Map_Matching.pdf">Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Hong Kong Polytechnic University</span></p>
<p>æœ¬æ–‡æå‡ºäº†TRMMAå’ŒMMAä¸¤ç§é«˜æ•ˆä¸”å‡†ç¡®çš„ç¨€ç–è½¨è¿¹æ¢å¤ä¸åœ°å›¾åŒ¹é…æ–¹æ³•ã€‚MMAå°†GPSç‚¹åŒ¹é…åˆ°é“è·¯æ®µçš„ä»»åŠ¡è½¬åŒ–ä¸ºå°å€™é€‰é›†ä¸Šçš„åˆ†ç±»é—®é¢˜ï¼Œå¹¶é€šè¿‡å€™é€‰æ®µä¸ç‚¹çš„æœ‰æ•ˆåµŒå…¥æå‡åŒ¹é…ç²¾åº¦ï¼ŒTRMMAè¿›ä¸€æ­¥åœ¨MMAè¯†åˆ«çš„è·¯å¾„ä¸Šï¼Œé€šè¿‡åŒå˜æ¢å™¨ï¼ˆDual-Transformerï¼‰ç¼–ç æ•æ‰è½¨è¿¹å’Œè·¯å¾„çš„æ½œåœ¨æ¨¡å¼ï¼Œåˆ©ç”¨è§£ç è¿‡ç¨‹é¢„æµ‹ç¼ºå¤±ç‚¹çš„ä½ç½®æ¯”ä¾‹ä¸é“è·¯æ®µï¼Œå®ç°é«˜æ•ˆè½¨è¿¹æ¢å¤ã€‚å®éªŒè¡¨æ˜ï¼ŒTRMMAä¸MMAåœ¨å¤§è§„æ¨¡çœŸå®æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”æ˜¾è‘—æå‡è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Efficient%20Methods%20for%20Accurate%20Sparse%20Trajectory%20Recovery%20and%20Map%20Matching.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-CorrectNav-Self-Correction-Flywheel-Empowers-Vision-Language-Action-Navigation-Model"><a href="#24-CorrectNav-Self-Correction-Flywheel-Empowers-Vision-Language-Action-Navigation-Model" class="headerlink" title="24. CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/CorrectNav__Self-Correction_Flywheel_Empowers_Vision-Language-Action_Navigation_Model.pdf">CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Peking University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†Self-correction Flywheelï¼ˆè‡ªæˆ‘çº æ­£é£è½®ï¼‰åè®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡è‡ªåŠ¨æ”¶é›†å¯¼èˆªæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„é”™è¯¯è½¨è¿¹ï¼Œæ£€æµ‹åå·®å¹¶ç”Ÿæˆè‡ªæˆ‘çº æ­£æ•°æ®ç”¨äºæ„ŸçŸ¥å’ŒåŠ¨ä½œæ ¡æ­£ï¼Œå¤šè½®è¿­ä»£è®­ç»ƒæå‡å¯¼èˆªæ¨¡å‹CorrectNavçš„è‡ªæˆ‘çº é”™èƒ½åŠ›ã€‚å®éªŒåœ¨R2R-CEå’ŒRxR-CEç­‰åŸºå‡†ä¸Šå–å¾—äº†æ–°çš„SOTAï¼Œå¹¶åœ¨å®é™…æœºå™¨äººä¸­éªŒè¯äº†å…¶ä¼˜è¶Šçš„çº é”™ã€é¿éšœå’Œé•¿æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/CorrectNav%3A%20Self-Correction%20Flywheel%20Empowers%20Vision-Language-Action%20Navigation%20Model.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-Probabilistic-Latency-Analysis-of-the-Data-Distribution-Service-in-ROS-2"><a href="#25-Probabilistic-Latency-Analysis-of-the-Data-Distribution-Service-in-ROS-2" class="headerlink" title="25. Probabilistic Latency Analysis of the Data Distribution Service in ROS 2"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Probabilistic_Latency_Analysis_of_the_Data_Distribution_Service_in_ROS_2.pdf">Probabilistic Latency Analysis of the Data Distribution Service in ROS 2</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">DGIST</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ROS 2 DDSé€šä¿¡åœ¨æœ‰æŸæ— çº¿ç½‘ç»œä¸‹çš„æ¦‚ç‡å»¶è¿Ÿåˆ†æï¼ˆPLAï¼‰æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¦»æ•£çŠ¶æ€å»ºæ¨¡ä¸åŠ¨æ€è§„åˆ’ï¼Œç³»ç»Ÿæ€§åˆ†æä¸­é—´ä»¶å±‚å’Œä¼ è¾“å±‚äº‹ä»¶ï¼Œé¢„æµ‹æœªç¡®è®¤æ¶ˆæ¯æ¦‚ç‡åˆ†å¸ƒä¸é‡ä¼ å»¶è¿Ÿï¼Œå¹¶åœ¨270ç§åœºæ™¯ä¸‹å®éªŒéªŒè¯ï¼Œå…¶å¹³å‡è¯¯å·®ä½äº5%ã€‚ç»“è®ºæŒ‡å‡ºPLAæ¨¡å‹åœ¨å¯é æ€§ã€å»¶è¿Ÿå’Œæ€§èƒ½ä¼˜åŒ–æ–¹é¢å…·å¤‡ç†è®ºæŒ‡å¯¼æ„ä¹‰ï¼Œèƒ½æœ‰æ•ˆè¾…åŠ©æ— çº¿å·¥ä¸šæœºå™¨äººç½‘ç»œçš„QoSå‚æ•°è°ƒä¼˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Probabilistic%20Latency%20Analysis%20of%20the%20Data%20Distribution%20Service%20in%20ROS%202.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-Super-LiDAR-Reflectance-for-Robotic-Perception"><a href="#26-Super-LiDAR-Reflectance-for-Robotic-Perception" class="headerlink" title="26. Super LiDAR Reflectance for Robotic Perception"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Super_LiDAR_Reflectance_for_Robotic_Perception.pdf">Super LiDAR Reflectance for Robotic Perception</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Macau</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°æ¡†æ¶ï¼Œé€šè¿‡è‡ªç ”çš„æ·±åº¦å·ç§¯ç½‘ç»œï¼ˆå«è‡ªé€‚åº”èåˆæ¨¡å—å’ŒåŠ¨æ€è¡¥å¿æ¨¡å—ï¼‰å°†ä½æˆæœ¬éé‡å¤æ‰«æLiDARçš„ç¨€ç–åå°„ç‡æ•°æ®å®æ—¶è½¬æ¢ä¸ºé«˜å¯†åº¦ã€æ ¡å‡†åçš„åå°„ç‡å›¾åƒã€‚æ–¹æ³•åŒ…æ‹¬æ„å»ºç¨€ç–åˆ°ç¨ å¯†é…å¯¹æ ‡æ³¨æ•°æ®é›†ã€é™æ€åˆ°åŠ¨æ€åŸŸæ•°æ®å¢å¼ºï¼Œä»¥åŠåå°„ç‡è¡¥å¿ï¼Œæ˜¾è‘—æå‡äº†ä½æˆæœ¬LiDARåœ¨æœºå™¨äººé—­ç¯æ£€æµ‹å’Œè½¦é“çº¿æ£€æµ‹ç­‰æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚ç»“è®ºè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…¼å…·é«˜å‡†ç¡®æ€§å’Œå®æ—¶æ€§ï¼Œæå¤§æ‹“å±•äº†ä½æˆæœ¬LiDARåœ¨æœºå™¨äººä¸»åŠ¨è§†è§‰ä¸­çš„åº”ç”¨è¾¹ç•Œã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Super%20LiDAR%20Reflectance%20for%20Robotic%20Perception.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-ReconVLA-Reconstructive-Vision-Language-Action-Model-as-Effective-Robot-Perceiver"><a href="#27-ReconVLA-Reconstructive-Vision-Language-Action-Model-as-Effective-Robot-Perceiver" class="headerlink" title="27. ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ReconVLA__Reconstructive_Vision-Language-Action_Model_as_Effective_Robot_Perceiver.pdf">ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é‡æ„å¼è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆReconVLAï¼‰ï¼Œé‡‡ç”¨éšå¼è§†è§‰å½’å› èŒƒå¼ï¼Œé€šè¿‡æ‰©æ•£Transformeræ¨¡å—é‡å»ºæœºå™¨äººæ“ä½œç›®æ ‡çš„å‡è§†åŒºåŸŸï¼Œä¿ƒè¿›æ¨¡å‹å­¦ä¹ ç»†ç²’åº¦è§†è§‰è¡¨å¾å’Œç²¾å‡†å…³æ³¨ç›®æ ‡åŒºåŸŸï¼Œæå‡æ“æ§ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹ŸåŠçœŸå®ç¯å¢ƒä¸­å‡ä¼˜äºæ˜¾å¼å’Œé“¾å¼è§†è§‰å½’å› æ–¹æ³•ï¼Œå®ç°äº†æ›´å¼ºçš„æ³›åŒ–å’Œå¯¹æœªè§ç›®æ ‡çš„æ“æ§èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ReconVLA%3A%20Reconstructive%20Vision-Language-Action%20Model%20as%20Effective%20Robot%20Perceiver.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-Large-Model-Empowered-Embodied-AI-A-Survey-on-Decision-Making-and-Embodied-Learning"><a href="#28-Large-Model-Empowered-Embodied-AI-A-Survey-on-Decision-Making-and-Embodied-Learning" class="headerlink" title="28. Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Large_Model_Empowered_Embodied_AI__A_Survey_on_Decision-Making_and_Embodied_Learning.pdf">Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Electronic Science and Technology of China</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿç»¼è¿°äº†å¤§æ¨¡å‹èµ‹èƒ½çš„å…·èº«æ™ºèƒ½ï¼ˆEmbodied AIï¼‰ï¼Œèšç„¦äºè‡ªä¸»å†³ç­–ä¸å…·èº«å­¦ä¹ ã€‚æ–¹æ³•ä¸Šï¼Œæ–‡ç« æ¢³ç†äº†å¤§æ¨¡å‹åœ¨å±‚æ¬¡åŒ–å†³ç­–ï¼ˆåŒ…æ‹¬é«˜å±‚è§„åˆ’ã€ä½å±‚æ‰§è¡Œã€åé¦ˆå¢å¼ºï¼‰ä¸ç«¯åˆ°ç«¯å†³ç­–ï¼ˆåŸºäºVLAæ¨¡å‹ï¼‰ã€å…·èº«å­¦ä¹ ï¼ˆæ¨¡ä»¿å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ ï¼‰ã€ä¸–ç•Œæ¨¡å‹ç­‰æ–¹é¢çš„æœ€æ–°è¿›å±•ä¸æŠ€æœ¯æµç¨‹ã€‚ç»“è®ºæŒ‡å‡ºï¼Œå¤§æ¨¡å‹æå¤§æå‡äº†å…·èº«æ™ºèƒ½ç³»ç»Ÿåœ¨æ„ŸçŸ¥ã€è§„åˆ’ã€æ‰§è¡Œä¸å­¦ä¹ ç­‰ç¯èŠ‚çš„èƒ½åŠ›ï¼Œä½†åœ¨æ•°æ®ç¨€ç¼ºã€æŒç»­å­¦ä¹ ã€éƒ¨ç½²æ•ˆç‡åŠä»¿çœŸ-ç°å®è½¬ç§»ç­‰æ–¹é¢ä»æœ‰è¯¸å¤šæŒ‘æˆ˜ï¼Œæœªæ¥éœ€æŒç»­ç ”ç©¶ä»¥å®ç°æ›´å¼ºçš„é€šç”¨æ™ºèƒ½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Large%20Model%20Empowered%20Embodied%20AI%3A%20A%20Survey%20on%20Decision-Making%20and%20Embodied%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-A-Semantic-Aware-Framework-for-Safe-and-Intent-Integrative-Assistance-in-Upper-Limb-Exoskeletons"><a href="#29-A-Semantic-Aware-Framework-for-Safe-and-Intent-Integrative-Assistance-in-Upper-Limb-Exoskeletons" class="headerlink" title="29. A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Semantic-Aware_Framework_for_Safe_and_Intent-Integrative_Assistance_in_Upper-Limb_Exoskeletons.pdf">A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tsinghua University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘ä¸Šè‚¢å¤–éª¨éª¼çš„è¯­ä¹‰æ„ŸçŸ¥è¾…åŠ©æ¡†æ¶ï¼Œå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°ä»»åŠ¡è§„åˆ’ä¸­ï¼Œé€šè¿‡æå–ä»»åŠ¡è¯­ä¹‰ä¿¡æ¯å’Œäººç±»æ„å›¾ï¼Œè‡ªåŠ¨é…ç½®è¾…åŠ©å‚æ•°ï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹å®ç°å¼‚å¸¸æ£€æµ‹ä¸å®æ—¶é‡è§„åˆ’ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå®ç°äº†äººæœºè®¤çŸ¥å¯¹é½ï¼Œæé«˜äº†è¾…åŠ©ä»»åŠ¡çš„å®‰å…¨æ€§ã€é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Semantic-Aware%20Framework%20for%20Safe%20and%20Intent-Integrative%20Assistance%20in%20Upper-Limb%20Exoskeletons.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-JRDB-Reasoning-A-Difficulty-Graded-Benchmark-for-Visual-Reasoning-in-Robotics"><a href="#30-JRDB-Reasoning-A-Difficulty-Graded-Benchmark-for-Visual-Reasoning-in-Robotics" class="headerlink" title="30. JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/JRDB-Reasoning__A_Difficulty-Graded_Benchmark_for_Visual_Reasoning_in_Robotics.pdf">JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Monash University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºJRDB-ReasoningåŸºå‡†ï¼Œé¦–æ¬¡ç³»ç»Ÿå®šä¹‰å’Œé‡åŒ–è§†è§‰æ¨ç†å¤æ‚åº¦ï¼Œå¹¶å¼€å‘äº†è‡ªé€‚åº”æŸ¥è¯¢å¼•æ“ï¼Œå¯æŒ‰éœ€ç”Ÿæˆä¸åŒéš¾åº¦ã€å¤šæ­¥éª¤æ¨ç†å’Œä¸­é—´æ ‡æ³¨çš„é—®é¢˜ï¼Œæ‰©å±•JRDBæ•°æ®é›†ä»¥æ”¯æŒæœºå™¨äººåœ¨æ‹¥æŒ¤äººç¾¤ç¯å¢ƒä¸­çš„è§†è§‰æ¨ç†ä»»åŠ¡ã€‚å®éªŒæ˜¾ç¤ºï¼Œç°æœ‰VLMæ¨¡å‹åœ¨æ¨ç†éš¾åº¦æå‡æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè€Œè¯¥åŸºå‡†å’Œå·¥å…·æœ‰åŠ©äºç²¾ç»†åŒ–è¯„ä¼°æœºå™¨äººè§†è§‰æ¨ç†èƒ½åŠ›å’Œæœªæ¥æ¨¡å‹å‘å±•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/JRDB-Reasoning%3A%20A%20Difficulty-Graded%20Benchmark%20for%20Visual%20Reasoning%20in%20Robotics.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="31-GNN-based-Unified-Deep-Learning"><a href="#31-GNN-based-Unified-Deep-Learning" class="headerlink" title="31. GNN-based Unified Deep Learning"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GNN-based_Unified_Deep_Learning.pdf">GNN-based Unified Deep Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Imperial College London</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„ç»Ÿä¸€å­¦ä¹ èŒƒå¼ï¼Œå°†å¤šç§å¼‚æ„æ·±åº¦å­¦ä¹ æ¶æ„ï¼ˆå¦‚MLPã€CNNå’ŒGNNï¼‰è½¬åŒ–ä¸ºå›¾è¡¨ç¤ºï¼Œå¹¶åœ¨ç»Ÿä¸€çš„å›¾å­¦ä¹ ç©ºé—´ä¸­é€šè¿‡GNNè¿›è¡ŒååŒä¼˜åŒ–ï¼Œå®ç°å‚æ•°å…±äº«å’ŒçŸ¥è¯†è½¬ç§»ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸé¢å¯¹åˆ†å¸ƒæ¼‚ç§»å’ŒåŸŸæ–­è£‚åœºæ™¯æ—¶ï¼Œæ˜¾è‘—æå‡äº†å„ç±»æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GNN-based%20Unified%20Deep%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-Quantum-Visual-Fields-with-Neural-Amplitude-Encoding"><a href="#32-Quantum-Visual-Fields-with-Neural-Amplitude-Encoding" class="headerlink" title="32. Quantum Visual Fields with Neural Amplitude Encoding"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Quantum_Visual_Fields_with_Neural_Amplitude_Encoding.pdf">Quantum Visual Fields with Neural Amplitude Encoding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">MPI for Informatics</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹é‡å­éšå¼ç¥ç»è¡¨ç¤ºï¼ˆQINRï¼‰æ–¹æ³•Quantum Visual Field (QVF)ï¼Œç”¨äº2Då›¾åƒä¸3Då‡ ä½•åœºçš„é«˜æ•ˆè¡¨ç¤ºã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡å¯å­¦ä¹ çš„èƒ½é‡æµå½¢è¿›è¡Œç¥ç»å¹…åº¦ç¼–ç ï¼Œå°†ç»å…¸æ•°æ®åµŒå…¥é‡å­æ€å‘é‡ï¼Œå¹¶é‡‡ç”¨å…¨çº ç¼ è®¾è®¡çš„å‚æ•°åŒ–é‡å­ç”µè·¯åœ¨å®å¸Œå°”ä¼¯ç‰¹ç©ºé—´å†…å®Œæˆé‡å­æ“ä½œï¼Œä¿éšœæ•°å€¼ç¨³å®šå’Œå¿«é€Ÿæ”¶æ•›ï¼Œæ— éœ€ç»å…¸åå¤„ç†ï¼Œç›´æ¥é€šè¿‡æŠ•å½±æµ‹é‡æå–ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼ŒQVFåœ¨è§†è§‰åœºè¡¨ç¤ºç²¾åº¦å’Œé«˜é¢‘ç»†èŠ‚å­¦ä¹ ç­‰æ–¹é¢å‡ä¼˜äºç°æœ‰é‡å­åŠä¸»æµç»å…¸åŸºçº¿æ–¹æ³•ï¼Œå¹¶é¦–æ¬¡å®ç°äº†å›¾åƒä¸3Då½¢çŠ¶é›†åˆçš„è”åˆè¡¨ç¤ºåŠåº”ç”¨ï¼ˆå¦‚å›¾åƒä¿®å¤ã€3Dè¡¥å…¨å’Œæ’å€¼ï¼‰ï¼Œå±•ç¤ºäº†å…¶åœ¨é‡å­å¢å¼ºè§†è§‰è®¡ç®—é¢†åŸŸçš„å®é™…æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Quantum%20Visual%20Fields%20with%20Neural%20Amplitude%20Encoding.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data"><a href="#33-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data" class="headerlink" title="33. MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MAESTRO__Masked_AutoEncoders_for_Multimodal,_Multitemporal,_and_Multispectral_Earth_Observation_Data.pdf">MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institut national de lâ€™information gÃ©ographique et forestiÃ¨re (IGN), France</span></p>
<p>è®ºæ–‡æå‡ºMAESTROï¼Œä¸€ç§é’ˆå¯¹åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰æ•°æ®çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œæ‰©å±•äº†Masked Autoencoderï¼ˆMAEï¼‰æ¡†æ¶ä»¥æ”¯æŒå¤šæ¨¡æ€ã€å¤šæ—¶åºå’Œå¤šå…‰è°±è¾“å…¥ã€‚å…¶æ ¸å¿ƒæŠ€æœ¯åŒ…æ‹¬åŸºäºtokençš„æ—©èåˆä¸æ™šèåˆç­–ç•¥ï¼Œä»¥åŠæ–°é¢–çš„patch-group-wiseå½’ä¸€åŒ–æ–¹æ³•ï¼Œæœ‰æ•ˆæ³¨å…¥å…‰è°±å…ˆéªŒå¹¶æå‡è¡¨ç¤ºèƒ½åŠ›ã€‚å®éªŒåœ¨å››ä¸ªé¥æ„Ÿæ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•ï¼ŒMAESTROåœ¨ä¾èµ–å¤šæ—¶åºåŠ¨æ€çš„ä»»åŠ¡ä¸Šè¾¾åˆ°SOTAï¼Œå¹¶åœ¨å•æ—¶åºä»»åŠ¡ä¸Šä¿æŒç«äº‰åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MAESTRO%3A%20Masked%20AutoEncoders%20for%20Multimodal%2C%20Multitemporal%2C%20and%20Multispectral%20Earth%20Observation%20Data.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing"><a href="#34-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing" class="headerlink" title="34. ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ToonComposer__Streamlining_Cartoon_Production_with_Generative_Post-Keyframing.pdf">ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Chinese University of Hong Kong</span></p>
<p>æœ¬æ–‡æå‡ºäº†ToonComposerï¼Œä¸€ç§åŸºäºDiffusion Transformer (DiT) çš„ç”Ÿæˆå¼åå…³é”®å¸§(post-keyframing)æ¨¡å‹ï¼Œå°†å¡é€šåŠ¨ç”»åˆ¶ä½œä¸­çš„ä¸­é—´å¸§ç”Ÿæˆä¸ä¸Šè‰²æ­¥éª¤ç»Ÿä¸€ä¸ºå•ä¸€æµç¨‹ã€‚æ–¹æ³•æ ¸å¿ƒåŒ…æ‹¬ç¨€ç–è‰å›¾æ³¨å…¥æœºåˆ¶ï¼ˆsparse sketch injectionï¼‰ã€åŒºåŸŸæ§åˆ¶(region-wise control)å’Œç©ºé—´ä½ç§©é€‚é…å™¨ï¼ˆSLRAï¼‰ï¼Œå®ç°å¯¹ç¨€ç–å…³é”®å¸§è‰å›¾çš„ç²¾ç¡®æ§åˆ¶ï¼Œå¹¶é«˜æ•ˆè¿ç§»è§†é¢‘åŸºç¡€æ¨¡å‹è‡³å¡é€šåŸŸã€‚å®éªŒæ˜¾ç¤ºToonComposeråœ¨è§†è§‰è´¨é‡ã€è¿åŠ¨è¿è´¯æ€§ä¸ç”Ÿäº§æ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¤§å¹…å‡å°‘è‰ºæœ¯å®¶æ‰‹åŠ¨å·¥ä½œé‡ï¼Œæå‡å¡é€šè§†é¢‘ç”Ÿæˆçš„çµæ´»æ€§å’Œè´¨é‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ToonComposer%3A%20Streamlining%20Cartoon%20Production%20with%20Generative%20Post-Keyframing.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-Puppeteer-Rig-and-Animate-Your-3D-Models"><a href="#35-Puppeteer-Rig-and-Animate-Your-3D-Models" class="headerlink" title="35. Puppeteer: Rig and Animate Your 3D Models"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Puppeteer__Rig_and_Animate_Your_3D_Models.pdf">Puppeteer: Rig and Animate Your 3D Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Nanyang Technological University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†Puppeteerï¼Œä¸€ä¸ªé›†è‡ªåŠ¨ç»‘å®šéª¨éª¼ä¸åŠ¨ç”»äºä¸€ä½“çš„3Dæ¨¡å‹å¤„ç†æ¡†æ¶ã€‚æ–¹æ³•åŒ…æ‹¬åˆ©ç”¨è‡ªå›å½’Transformerå®ç°å…³èŠ‚åºåˆ—å»ºæ¨¡ä¸éª¨æ¶ç”Ÿæˆã€æ‹“æ‰‘æ„ŸçŸ¥æ³¨æ„åŠ›ç½‘ç»œè¿›è¡Œè’™çš®æƒé‡é¢„æµ‹ï¼Œå¹¶é€šè¿‡å¯å¾®åˆ†ä¼˜åŒ–è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡åŠ¨ç”»ã€‚å®éªŒè¡¨æ˜ï¼ŒPuppeteeråœ¨éª¨æ¶é¢„æµ‹ã€è’™çš®ç²¾å‡†åº¦å’ŒåŠ¨ç”»ç¨³å®šæ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½é«˜æ•ˆå¤„ç†å¤šæ ·3Då†…å®¹ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Puppeteer%3A%20Rig%20and%20Animate%20Your%203D%20Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-STREAM3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer"><a href="#36-STREAM3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer" class="headerlink" title="36. STREAM3R: Scalable Sequential 3D Reconstruction with Causal Transformer"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/STream3R__Scalable_Sequential_3D_Reconstruction_with_Causal_Transformer.pdf">STREAM3R: Scalable Sequential 3D Reconstruction with Causal Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Nanyang Technological University</span></p>
<p>æœ¬æ–‡æå‡ºSTREAM3Rï¼Œå°†3Dé‡å»ºä»»åŠ¡é‡æ„ä¸ºåŸºäºå› æœæ³¨æ„åŠ›çš„è§£ç å™¨Transformeré—®é¢˜ï¼Œå®ç°å¯¹æµå¼è¾“å…¥å›¾åƒçš„é«˜æ•ˆé¡ºåºå¤„ç†ã€‚é€šè¿‡ç¼“å­˜å†å²å¸§ç‰¹å¾å¹¶é€å¸§é€’å¢èåˆï¼ŒSTREAM3Rå…·å¤‡ä¼˜å¼‚çš„æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨é™æ€ä¸åŠ¨æ€åœºæ™¯çš„æ·±åº¦ä¼°è®¡åŠ3Dé‡å»ºä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”æ¨ç†é€Ÿåº¦æ˜¾è‘—æå‡ï¼Œæ¨åŠ¨å®æ—¶3Dç†è§£å‘å±•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/STREAM3R%3A%20Scalable%20Sequential%203D%20Reconstruction%20with%20Causal%20Transformer.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-Medico-2025-Visual-Question-Answering-for-Gastrointestinal-Imaging"><a href="#37-Medico-2025-Visual-Question-Answering-for-Gastrointestinal-Imaging" class="headerlink" title="37. Medico 2025: Visual Question Answering for Gastrointestinal Imaging"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Medico_2025__Visual_Question_Answering_for_Gastrointestinal_Imaging.pdf">Medico 2025: Visual Question Answering for Gastrointestinal Imaging</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">SimulaMet - Simula Metropolitan Center for Digital Engineering</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†Medico 2025æŒ‘æˆ˜ï¼Œæ—¨åœ¨æ¨åŠ¨é¢å‘èƒƒè‚ é“å½±åƒçš„å¯è§£é‡Šè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹çš„å‘å±•ï¼Œè¦æ±‚AIæ¨¡å‹ä¸ä»…å‡†ç¡®å›ç­”åŒ»å­¦å½±åƒä¸­çš„ä¸´åºŠé—®é¢˜ï¼Œè¿˜éœ€ç”Ÿæˆä¸åŒ»å­¦æ¨ç†ä¸€è‡´çš„å¤šæ¨¡æ€è§£é‡Šã€‚ç ”ç©¶åŸºäºKvasir-VQA-x1æ•°æ®é›†ï¼Œé€šè¿‡é‡åŒ–æŒ‡æ ‡å’Œä¸“å®¶å®¡æ ¸ç»“åˆè¯„ä¼°AIç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œè§£é‡Šæ€§ï¼Œä¿ƒè¿›å¯è¢«ä¸´åºŠé‡‡ç”¨çš„å¯ä¿¡åŒ»ç–—AIç³»ç»Ÿå‘å±•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Medico%202025%3A%20Visual%20Question%20Answering%20for%20Gastrointestinal%20Imaging.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-AEGIS-Authenticity-Evaluation-Benchmark-for-AI-Generated-Video-Sequences"><a href="#38-AEGIS-Authenticity-Evaluation-Benchmark-for-AI-Generated-Video-Sequences" class="headerlink" title="38. AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/AEGIS__Authenticity_Evaluation_Benchmark_for_AI-Generated_Video_Sequences.pdf">AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">National University of Singapore</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†AEGISï¼Œä¸€ä¸ªä¸“ä¸ºAIç”Ÿæˆè§†é¢‘çœŸä¼ªæ£€æµ‹è®¾è®¡çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªå¤šç§å…ˆè¿›ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Stable Video Diffusionã€CogVideoX-5Bã€KLingå’ŒSoraï¼‰çš„5,199ä¸ªé«˜çœŸå®æ€§å’Œè¯­ä¹‰å¤æ‚çš„åˆæˆè§†é¢‘ï¼Œå¹¶é…æœ‰ä¸°å¯Œçš„å¤šæ¨¡æ€æ ‡æ³¨ï¼ˆè¯­ä¹‰æè¿°ã€è¿åŠ¨ç‰¹å¾ã€ä½å±‚è§†è§‰ç‰¹å¾ï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Qwen-VLã€Video-LLaVAï¼‰åœ¨AEGISçš„é«˜éš¾åº¦æµ‹è¯•é›†ä¸Šè¡¨ç°æœ‰é™ï¼Œçªå‡ºå½“å‰æ£€æµ‹æ¨¡å‹åœ¨çœŸå®ç¯å¢ƒä¸­é¢ä¸´çš„æ³›åŒ–å’Œé²æ£’æ€§æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†AEGISåœ¨æ¨åŠ¨è§†é¢‘çœŸä¼ªæ£€æµ‹ç ”ç©¶ä¸­çš„åŸºç¡€æ€§ä½œç”¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AEGIS%3A%20Authenticity%20Evaluation%20Benchmark%20for%20AI-Generated%20Video%20Sequences.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-Natively-Trainable-Sparse-Attention-for-Hierarchical-Point-Cloud-Datasets"><a href="#39-Natively-Trainable-Sparse-Attention-for-Hierarchical-Point-Cloud-Datasets" class="headerlink" title="39. Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Natively_Trainable_Sparse_Attention_for_Hierarchical_Point_Cloud_Datasets.pdf">Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Groningen</span></p>
<p>æœ¬æ–‡æå‡ºå°†Erwinå±‚æ¬¡åŒ–Transformeræ¶æ„ä¸Native Sparse Attention(NSA)æœºåˆ¶ç»“åˆï¼Œé’ˆå¯¹ç‰©ç†ç§‘å­¦ä¸­çš„å¤§è§„æ¨¡ç‚¹äº‘æ•°æ®ï¼Œæå‡ºErwin NSAæ¨¡å‹ï¼Œé€šè¿‡å°†NSAç®—æ³•ä»åºåˆ—æ•°æ®æ‰©å±•åˆ°éåºåˆ—ç‚¹äº‘ï¼Œå¹¶æ›¿æ¢å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„å…¨å±€æ„Ÿå—é‡å»ºæ¨¡ã€‚å®éªŒåœ¨å®‡å®™å­¦æ¨¡æ‹Ÿã€åˆ†å­åŠ¨åŠ›å­¦å’Œç©ºæ°”å‹åŠ›å»ºæ¨¡ä¸‰å¤§æ•°æ®é›†ä¸Šï¼Œè¡¨æ˜Erwin NSAåœ¨ShapeNetå’Œåˆ†å­åŠ¨åŠ›å­¦ä»»åŠ¡ä¸­ä¼˜äºåŸErwinæ¨¡å‹ï¼Œåœ¨å®‡å®™å­¦ä»»åŠ¡ä¸Šè¡¨ç°æŒå¹³ï¼Œä¸”å…·æœ‰æ›´é«˜çš„è®­ç»ƒæ•ˆç‡å’Œæ›´å¥½çš„èŠ‚ç‚¹ä¿¡æ¯æµåŠ¨èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Natively%20Trainable%20Sparse%20Attention%20for%20Hierarchical%20Point%20Cloud%20Datasets.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-Exploiting-Discriminative-Codebook-Prior-for-Autoregressive-Image-Generation"><a href="#40-Exploiting-Discriminative-Codebook-Prior-for-Autoregressive-Image-Generation" class="headerlink" title="40. Exploiting Discriminative Codebook Prior for Autoregressive Image Generation"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Exploiting_Discriminative_Codebook_Prior_for_Autoregressive_Image_Generation.pdf">Exploiting Discriminative Codebook Prior for Autoregressive Image Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Hong Kong University of Science and Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†Discriminative Codebook Prior Extractor (DCPE)ï¼Œé€šè¿‡èšåˆèšç±»å’Œå®ä¾‹è·ç¦»æ›¿ä»£ä¼ ç»Ÿk-meansèšç±»ï¼Œä»åˆ†å¸ƒä¸å‡ä¸”é«˜ç»´çš„ä»£ç æœ¬ä»¤ç‰Œç‰¹å¾ç©ºé—´ä¸­æ›´æœ‰æ•ˆåœ°æŒ–æ˜ä»¤ç‰Œç›¸ä¼¼æ€§ä¿¡æ¯ï¼Œç”¨äºæå‡ç¦»æ•£ä»¤ç‰Œè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒDCPEä½œä¸ºå³æ’å³ç”¨æ–¹æ³•èƒ½åŠ é€Ÿè®­ç»ƒã€æå‡ç”Ÿæˆè´¨é‡ï¼Œå¹¶ä¼˜äºç°æœ‰k-meansæ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Exploiting%20Discriminative%20Codebook%20Prior%20for%20Autoregressive%20Image%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-Continuous-Bangla-Sign-Language-Translation-Mitigating-the-Expense-of-Gloss-Annotation-with-the-Assistance-of-Graph"><a href="#41-Continuous-Bangla-Sign-Language-Translation-Mitigating-the-Expense-of-Gloss-Annotation-with-the-Assistance-of-Graph" class="headerlink" title="41. Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Continuous_Bangla_Sign_Language_Translation__Mitigating_the_Expense_of_Gloss_Annotation_with_the_Ass.pdf">Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Dhaka</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†èåˆTransformerä¸STGCN-LSTMæ¶æ„çš„è¿ç»­æ‰‹è¯­ç¿»è¯‘æ–¹æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€æ•°æ®èåˆï¼ˆRGBè§†é¢‘ä¸éª¨éª¼å…³é”®ç‚¹ï¼‰ï¼Œå®ç°äº†æ— éœ€glossæ³¨é‡Šçš„ç«¯åˆ°ç«¯æ‰‹è¯­åˆ°æ–‡æœ¬ç¿»è¯‘ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªå…¬å¼€æ‰‹è¯­æ•°æ®é›†ï¼ˆRWTH-PHOENIX-2014Tã€CSL-Dailyã€How2Signã€BornilDB v1.0ï¼‰ä¸Šå–å¾—äº†æ–°çš„gloss-freeç¿»è¯‘æ€§èƒ½æœ€ä¼˜ï¼Œæ˜¾è‘—æå‡äº†BLEUåˆ†æ•°ï¼Œä¸ºè‹äººç¾¤ä½“çš„äº¤æµæ— éšœç¢æä¾›äº†æŠ€æœ¯åŸºç¡€ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Continuous%20Bangla%20Sign%20Language%20Translation%3A%20Mitigating%20the%20Expense%20of%20Gloss%20Annotation%20with%20the%20Assistance%20of%20Graph.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-Hybrid-Generative-Fusion-for-Efficient-and-Privacy-Preserving-Face-Recognition-Dataset-Generation"><a href="#42-Hybrid-Generative-Fusion-for-Efficient-and-Privacy-Preserving-Face-Recognition-Dataset-Generation" class="headerlink" title="42. Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Hybrid_Generative_Fusion_for_Efficient_and_Privacy-Preserving_Face_Recognition_Dataset_Generation.pdf">Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>æœ¬æ–‡æå‡ºäº†æ··åˆç”Ÿæˆèåˆæ–¹æ³•ï¼Œé€šè¿‡Mixture-of-Expertsæ•°æ®æ¸…æ´—ï¼ˆèšç±»+GPT-4oéªŒè¯ï¼‰ã€Stable Diffusionä¸Vec2Faceç”Ÿæˆæ–°èº«ä»½ï¼Œå¹¶ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ä¼˜åŒ–è®­ç»ƒæµç¨‹ï¼Œå®ç°é«˜æ•ˆã€éšç§ä¿æŠ¤çš„äººè„¸è¯†åˆ«æ•°æ®é›†æ„å»ºã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€ç”Ÿæˆçš„çº¯åˆæˆæ•°æ®é›†åœ¨å¤šè§„æ¨¡ä¸‹å‡æå‡äº†äººè„¸è¯†åˆ«æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨ç«èµ›ä¸­å–å¾—ç¬¬ä¸€åã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Hybrid%20Generative%20Fusion%20for%20Efficient%20and%20Privacy-Preserving%20Face%20Recognition%20Dataset%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-Geospatial-Diffusion-for-Land-Cover-Imperviousness-Change-Forecasting"><a href="#43-Geospatial-Diffusion-for-Land-Cover-Imperviousness-Change-Forecasting" class="headerlink" title="43. Geospatial Diffusion for Land Cover Imperviousness Change Forecasting"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Geospatial_Diffusion_for_Land_Cover_Imperviousness_Change_Forecasting.pdf">Geospatial Diffusion for Land Cover Imperviousness Change Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Oak Ridge National Laboratory</span></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰çš„åœ°è¡¨è¦†ç›–ä¸é€æ°´æ€§å˜åŒ–é¢„æµ‹æ–¹æ³•ï¼Œå°†åœŸåœ°åˆ©ç”¨&#x2F;è¦†ç›–ï¼ˆLULCï¼‰å˜åŒ–é¢„æµ‹è§†ä¸ºæ¡ä»¶æ•°æ®åˆæˆä»»åŠ¡ï¼Œåˆ©ç”¨å†å²LULCåŠè¾…åŠ©æ•°æ®ï¼ˆå¦‚è¿‡æ¸¡æ¦‚ç‡å›¾ï¼‰è¿›è¡Œå»ºæ¨¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¾å›½12ä¸ªå¤§éƒ½å¸‚åŒºåŸŸå®ç°äº†äºšå…¬é‡Œçº§åˆ«çš„é¢„æµ‹ç²¾åº¦ï¼Œä¼˜äºä¼ ç»Ÿçš„æ— å˜åŒ–åŸºçº¿æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å†å²æ•°æ®ä¸­çš„æ—¶ç©ºå‘å±•æ¨¡å¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Geospatial%20Diffusion%20for%20Land%20Cover%20Imperviousness%20Change%20Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera"><a href="#44-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera" class="headerlink" title="44. Processing and acquisition traces in visual encoders: What does CLIP know about your camera?"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Processing_and_acquisition_traces_in_visual_encoders__What_does_CLIP_know_about_your_camera_.pdf">Processing and acquisition traces in visual encoders: What does CLIP know about your camera?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The University of Osaka</span></p>
<p>æœ¬è®ºæ–‡ç³»ç»Ÿåˆ†æäº†è§†è§‰ç¼–ç å™¨ï¼ˆå¦‚CLIPï¼‰åœ¨è¡¨å¾ç©ºé—´ä¸­å¯¹å›¾ç‰‡å¤„ç†å’Œé‡‡é›†å‚æ•°ï¼ˆå¦‚JPEGå‹ç¼©ã€ç›¸æœºå‹å·ç­‰å…ƒæ•°æ®ï¼‰çš„æ•æ„Ÿæ€§ï¼Œé€šè¿‡åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡å±•ç¤ºè¿™äº›å…ƒæ•°æ®ä¼šè¢«æ¨¡å‹å­¦ä¹ å¹¶å½±å“è¯­ä¹‰é¢„æµ‹ã€‚ç»“è®ºè¡¨æ˜ï¼Œå°¤å…¶æ˜¯è§†è§‰-è¯­è¨€å¯¹æ¯”æ¨¡å‹ï¼ˆCVLï¼‰ï¼Œå…¶è¡¨å¾ç©ºé—´ä¸­å…ƒæ•°æ®ç—•è¿¹å¯èƒ½ä¼šå¹²æ‰°ä¸‹æ¸¸è¯­ä¹‰ä»»åŠ¡ï¼Œé™ä½æ¨¡å‹é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸”æ¨¡å‹è®­ç»ƒæ—¶æ˜¯å¦é‡‡ç”¨å¼ºæ•°æ®å¢å¼ºæ˜¯å½±å“å…ƒæ•°æ®æ•æ„Ÿæ€§çš„å…³é”®å› ç´ ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Processing%20and%20acquisition%20traces%20in%20visual%20encoders%3A%20What%20does%20CLIP%20know%20about%20your%20camera%3F.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-Object-Fidelity-Diffusion-for-Remote-Sensing-Image-Generation"><a href="#45-Object-Fidelity-Diffusion-for-Remote-Sensing-Image-Generation" class="headerlink" title="45. Object Fidelity Diffusion for Remote Sensing Image Generation"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Object_Fidelity_Diffusion_for_Remote_Sensing_Image_Generation.pdf">Object Fidelity Diffusion for Remote Sensing Image Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Fudan University</span></p>
<p>æœ¬æ–‡æå‡ºäº†Object Fidelity Diffusionï¼ˆOF-Diffï¼‰ï¼Œä¸€ç§ç”¨äºé¥æ„Ÿå›¾åƒç”Ÿæˆçš„åŒåˆ†æ”¯å¯æ§æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥å¢å¼ºå½¢çŠ¶ç”Ÿæˆæ¨¡å—ï¼ˆESGMï¼‰ä»å¸ƒå±€ä¸­æå–å¯¹è±¡å…ˆéªŒå½¢çŠ¶ï¼Œå¹¶ç»“åˆæ‰©æ•£ä¸€è‡´æ€§æŸå¤±ä¸DDPOå¾®è°ƒæå‡å›¾åƒå¤šæ ·æ€§ä¸è¯­ä¹‰ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒOF-Diffèƒ½åœ¨æ— éœ€çœŸå®å›¾åƒå‚è€ƒçš„æ¡ä»¶ä¸‹ç”Ÿæˆé«˜ä¿çœŸã€å¸ƒå±€ä¸å½¢çŠ¶ä¸€è‡´çš„é¥æ„Ÿå›¾åƒï¼Œæ˜¾è‘—æå‡å°ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Object%20Fidelity%20Diffusion%20for%20Remote%20Sensing%20Image%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-VIDEO-BLADE-BLOCK-SPARSE-ATTENTION-MEETS-STEP-DISTILLATION-FOR-EFFICIENT-VIDEO-GENERATION"><a href="#46-VIDEO-BLADE-BLOCK-SPARSE-ATTENTION-MEETS-STEP-DISTILLATION-FOR-EFFICIENT-VIDEO-GENERATION" class="headerlink" title="46. VIDEO-BLADE: BLOCK-SPARSE ATTENTION MEETS STEP DISTILLATION FOR EFFICIENT VIDEO GENERATION"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Video-BLADE__Block-Sparse_Attention_Meets_Step_Distillation_for_Efficient_Video_Generation.pdf">VIDEO-BLADE: BLOCK-SPARSE ATTENTION MEETS STEP DISTILLATION FOR EFFICIENT VIDEO GENERATION</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zhejiang University</span></p>
<p>æœ¬æ–‡æå‡ºäº†Video-BLADEæ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”å—ç¨€ç–æ³¨æ„åŠ›ï¼ˆASAï¼‰æœºåˆ¶ä¸åŸºäºè½¨è¿¹åˆ†å¸ƒåŒ¹é…ï¼ˆTDMï¼‰çš„ç¨€ç–æ„ŸçŸ¥æ­¥è’¸é¦è”åˆè®­ç»ƒï¼Œå®ç°é«˜æ•ˆè§†é¢‘æ‰©æ•£ç”Ÿæˆã€‚ASAåŠ¨æ€ç”Ÿæˆå†…å®¹æ„ŸçŸ¥ç¨€ç–æ©ç ï¼Œè”åˆç¨€ç–æ„ŸçŸ¥è’¸é¦æå‡æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒä¹ƒè‡³æå‡è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨CogVideoX-5Bå’ŒWan2.1-1.3Bç­‰ä¸»æµæ¨¡å‹ä¸Šï¼ŒBLADEå¯å®ç°8.89Ã—è‡³14.10Ã—åŠ é€Ÿï¼Œä¸”åœ¨VBench-2.0ç­‰åŸºå‡†ä¸Šå–å¾—æ›´é«˜åˆ†æ•°ï¼Œå…¼é¡¾æ•ˆç‡ä¸è´¨é‡ï¼Œä¸”äººç±»è¯„æµ‹åŒæ ·è®¤å¯å…¶ä¼˜å¼‚è¡¨ç°ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/VIDEO-BLADE%3A%20BLOCK-SPARSE%20ATTENTION%20MEETS%20STEP%20DISTILLATION%20FOR%20EFFICIENT%20VIDEO%20GENERATION.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-From-Diagnosis-to-Improvement-Probing-Spatio-Physical-Reasoning-in-Vision-Language-Models"><a href="#47-From-Diagnosis-to-Improvement-Probing-Spatio-Physical-Reasoning-in-Vision-Language-Models" class="headerlink" title="47. From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/From_Diagnosis_to_Improvement__Probing_Spatio-Physical_Reasoning_in_Vision_Language_Models.pdf">From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tongji University</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†ä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æ—¶ç©ºç‰©ç†æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°å…¶åœ¨åŸºäºShapeStacksåŸºå‡†çš„é™æ€å¹³è¡¡åˆ¤æ–­ä¸Šå‡†ç¡®ç‡ä½ï¼Œå­˜åœ¨ç±»äººè®¤çŸ¥åè§å’Œç¼ºä¹æ·±åº¦æ¨ç†ã€‚ä½œè€…æå‡ºé‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç»“åˆè§„åˆ™é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹Qwen2.5-VL-7Bæ¨¡å‹è¿›è¡ŒäºŒé˜¶æ®µè®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†å…¶é¢†åŸŸå†…ç‰©ç†æ¨ç†èƒ½åŠ›å¹¶è¶…è¶Šäº†ä¸»æµå•†ç”¨æ¨¡å‹ï¼Œä½†åœ¨è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ä¸Šä»è¡¨ç°æœ‰é™ï¼Œæš´éœ²å‡ºå½“å‰èŒƒå¼ä»¥æ¨¡å¼åŒ¹é…ä¸ºä¸»ã€éš¾ä»¥ä¹ å¾—å¯è¿ç§»ç‰©ç†åŸåˆ™çš„æ ¹æœ¬å±€é™ã€‚ç»“è®ºï¼šä¸»æµVLMåœ¨ç‰©ç†æ¨ç†æ–¹é¢å­˜åœ¨æ˜æ˜¾å±€é™ï¼ŒäºŒé˜¶æ®µå¾®è°ƒè™½å¯æå‡é¢†åŸŸå†…æ€§èƒ½ä½†æ³›åŒ–ä¸è¶³ï¼Œæœªæ¥åº”å‘å±•åŸºäºç‰©ç†å› æœæœºåˆ¶çš„æ–°èŒƒå¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From%20Diagnosis%20to%20Improvement%3A%20Probing%20Spatio-Physical%20Reasoning%20in%20Vision%20Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-Privacy-enhancing-Sclera-Segmentation-Benchmarking-Competition-SSBC-2025"><a href="#48-Privacy-enhancing-Sclera-Segmentation-Benchmarking-Competition-SSBC-2025" class="headerlink" title="48. Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Privacy-enhancing_Sclera_Segmentation_Benchmarking_Competition__SSBC_2025.pdf">Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Ljubljana</span></p>
<p>æœ¬è®ºæ–‡ä»‹ç»äº†SSBC 2025éšç§å¢å¼ºå‹å·©è†œåˆ†å‰²ç«èµ›ï¼Œé‡ç‚¹åœ¨äºåˆ©ç”¨åˆæˆçœ¼éƒ¨å›¾åƒè®­ç»ƒéšç§ä¿æŠ¤çš„å·©è†œåˆ†å‰²æ¨¡å‹ï¼Œå¹¶ç³»ç»Ÿæ¯”è¾ƒäº†åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®æ··åˆè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œä¸“ç”¨è®­ç»ƒç­–ç•¥ä¸‹ï¼Œä»…ç”¨åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹èƒ½å–å¾—ä¸çœŸå®æ•°æ®è®­ç»ƒç›¸è¿‘çš„åˆ†å‰²æ•ˆæœï¼ŒF1åˆ†æ•°å¯è¾¾0.8ä»¥ä¸Šï¼Œä¸”æ–¹æ³•é€‰æ‹©å¾€å¾€æ¯”çœŸå®æ•°æ®çš„åŠ å…¥æ›´å…³é”®ï¼Œçªå‡ºåˆæˆæ•°æ®åœ¨éšç§ä¿æŠ¤ç”Ÿç‰©è¯†åˆ«ä¸­çš„å¯è¡Œæ€§å’Œå‰æ™¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Privacy-enhancing%20Sclera%20Segmentation%20Benchmarking%20Competition%3A%20SSBC%202025.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale"><a href="#49-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale" class="headerlink" title="49. NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/NextStep-1__Toward_Autoregressive_Image_Generation_with_Continuous_Tokens_at_Scale.pdf">NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">StepFun</span></p>
<p>è¯¥è®ºæ–‡æå‡ºNextStep-1ï¼Œä¸€ç§ç»“åˆå¤§è§„æ¨¡Transformerä¸»å¹²å’ŒæµåŒ¹é…å¤´çš„è‡ªå›å½’æ¨¡å‹ï¼Œå®ç°æ–‡æœ¬åˆ°é«˜ä¿çœŸå›¾åƒç”Ÿæˆå’Œå¤šæ ·åŒ–å›¾åƒç¼–è¾‘ã€‚æ–¹æ³•åˆ›æ–°åœ¨äºåŒæ—¶å¤„ç†ç¦»æ•£æ–‡æœ¬å’Œè¿ç»­å›¾åƒtokenï¼Œå¹¶å¼•å…¥å¼ºæ­£åˆ™åŒ–çš„å›¾åƒtokenizerä»¥ä¿è¯åˆ†å¸ƒç¨³å®šï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨å¤šé¡¹ä¸»æµåŸºå‡†ä¸Šè¶…è¶Šç°æœ‰è‡ªå›å½’æ¨¡å‹ï¼Œéƒ¨åˆ†æŒ‡æ ‡æ¥è¿‘æ‰©æ•£æ¨¡å‹ï¼ŒéªŒè¯äº†è‡ªå›å½’èŒƒå¼åœ¨é«˜è´¨é‡å›¾åƒç”Ÿæˆä¸Šçš„å¯è¡Œæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/NextStep-1%3A%20Toward%20Autoregressive%20Image%20Generation%20with%20Continuous%20Tokens%20at%20Scale.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-Novel-View-Synthesis-using-DDIM-Inversion"><a href="#50-Novel-View-Synthesis-using-DDIM-Inversion" class="headerlink" title="50. Novel View Synthesis using DDIM Inversion"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Novel_View_Synthesis_using_DDIM_Inversion.pdf">Novel View Synthesis using DDIM Inversion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Indraprastha Institute of Information Technology, Delhi</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºDDIMå€’ç½®å’Œè½»é‡åŒ–TUNetç½‘ç»œçš„å•å¼ å›¾åƒé«˜è´¨é‡æ–°è§†è§’åˆæˆæ–¹æ³•ã€‚æµç¨‹åŒ…æ‹¬å°†è¾“å…¥å›¾åƒé€šè¿‡VAEå’ŒDDIMå€’ç½®è·å¾—æ½œå˜é‡ï¼Œä½¿ç”¨TUNetè¿›è¡Œè§†è§’è½¬æ¢ï¼Œç»“åˆå™ªå£°èåˆç­–ç•¥å¼ºåŒ–é«˜é¢‘ç»†èŠ‚ï¼Œæœ€åå€ŸåŠ©é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è§£ç ç”Ÿæˆæ–°è§†è§’å›¾åƒã€‚å®éªŒè¡¨æ˜æ–¹æ³•åœ¨MVImgNetç­‰æ•°æ®é›†ä¸Šä¼˜äºä¸»æµæ–¹æ³•ï¼Œèƒ½ç”Ÿæˆé«˜åˆ†è¾¨ç‡ä¸”å‡ ä½•ä¸€è‡´çš„æ–°è§†è§’å›¾åƒã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Novel%20View%20Synthesis%20using%20DDIM%20Inversion.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-Conditional-Information-Bottleneck-for-Multimodal-Fusion-Overcoming-Shortcut-Learning-in-Sarcasm-Detection"><a href="#51-Conditional-Information-Bottleneck-for-Multimodal-Fusion-Overcoming-Shortcut-Learning-in-Sarcasm-Detection" class="headerlink" title="51. Conditional Information Bottleneck for Multimodal Fusion: Overcoming Shortcut Learning in Sarcasm Detection"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Conditional_Information_Bottleneck_for_Multimodal_Fusion__Overcoming_Shortcut_Learning_in_Sarcasm_De.pdf">Conditional Information Bottleneck for Multimodal Fusion: Overcoming Shortcut Learning in Sarcasm Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Artificial Intelligence, Beijing Normal University</span></p>
<p>æœ¬æ–‡é’ˆå¯¹å¤šæ¨¡æ€è®½åˆºæ£€æµ‹ä»»åŠ¡ä¸­æ¨¡å‹å®¹æ˜“ä¾èµ–æ•°æ®é›†æ·å¾„ï¼ˆå¦‚è§’è‰²æ ‡ç­¾ã€ç½å¤´ç¬‘å£°ã€æƒ…æ„Ÿä¸ä¸€è‡´æ€§ï¼‰å¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ï¼Œé‡æ„äº†MUStARD++æ•°æ®é›†ï¼Œå»é™¤äº†ç›¸å…³æ·å¾„ä¿¡å·ï¼Œå¹¶æå‡ºäº†å¤šæ¨¡æ€æ¡ä»¶ä¿¡æ¯ç“¶é¢ˆï¼ˆMCIBï¼‰èåˆæ¨¡å‹ã€‚MCIBé€šè¿‡æ¡ä»¶ä¿¡æ¯ç“¶é¢ˆæœºåˆ¶å¯¹å¤šæ¨¡æ€ï¼ˆæ–‡æœ¬ã€éŸ³é¢‘ã€è§†è§‰ï¼‰ç‰¹å¾è¿›è¡Œå†—ä½™è¿‡æ»¤ä¸äº’è¡¥ä¿¡æ¯æå–ï¼Œå®ç°é«˜æ•ˆèåˆï¼Œæ˜¾è‘—æå‡äº†åœ¨è®½åˆºæ£€æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå®éªŒç»“æœåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä¼˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Conditional%20Information%20Bottleneck%20for%20Multimodal%20Fusion%3A%20Overcoming%20Shortcut%20Learning%20in%20Sarcasm%20Detection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-ChatENV-An-Interactive-Vision-Language-Model-for-Sensor-Guided-Environmental-Monitoring-and-Scenario-Simulation"><a href="#52-ChatENV-An-Interactive-Vision-Language-Model-for-Sensor-Guided-Environmental-Monitoring-and-Scenario-Simulation" class="headerlink" title="52. ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ChatENV__An_Interactive_Vision-Language_Model_for_Sensor-Guided_Environmental_Monitoring_and_Scenari.pdf">ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Mohamed bin Zayed University of Artificial Intelligence</span></p>
<p>æœ¬æ–‡æå‡ºChatENVï¼Œä¸€ä¸ªç»“åˆå«æ˜Ÿé¥æ„Ÿå›¾åƒä¸çœŸå®ç¯å¢ƒä¼ æ„Ÿå™¨æ•°æ®çš„äº¤äº’å¼è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡æ„å»ºæ¶µç›–å…¨çƒ177kå›¾åƒã€152kæ—¶åºå¯¹å’Œä¸°å¯Œæ°”è±¡&#x2F;æ’æ”¾å…ƒæ•°æ®çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨GPT-4oå’ŒGemini 2.0ç”Ÿæˆå¤šæ ·æ³¨é‡Šï¼Œæ¨¡å‹é‡‡ç”¨Qwen-2.5-VLä¸ºä¸»å¹²ï¼ŒLoRAé«˜æ•ˆå¾®è°ƒï¼Œå®ç°åœºæ™¯æè¿°ã€æ—¶ç©ºå˜åŒ–æ¨ç†åŠâ€œå‡è®¾æƒ…æ™¯â€æ¨¡æ‹Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChatENVåœ¨æ—¶åºæ¨ç†å’Œæƒ…æ™¯åˆ†æç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†ç¯å¢ƒç›‘æµ‹çš„æ™ºèƒ½åŒ–å’Œå®ç”¨æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ChatENV%3A%20An%20Interactive%20Vision-Language%20Model%20for%20Sensor-Guided%20Environmental%20Monitoring%20and%20Scenario%20Simulation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-Fourier-Guided-Attention-Upsampling-for-Image-Super-Resolution"><a href="#53-Fourier-Guided-Attention-Upsampling-for-Image-Super-Resolution" class="headerlink" title="53. Fourier-Guided Attention Upsampling for Image Super-Resolution"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Fourier-Guided_Attention_Upsampling_for_Image_Super-Resolution.pdf">Fourier-Guided Attention Upsampling for Image Super-Resolution</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Korea University of Technology and Education (KOREATECH)</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„é¢‘åŸŸå¼•å¯¼æ³¨æ„åŠ›ä¸Šé‡‡æ ·æ¨¡å—FGAï¼Œç”¨äºå•å¹…å›¾åƒè¶…åˆ†è¾¨ç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆåŸºäºå‚…é‡Œå¶ç‰¹å¾çš„å¤šå±‚æ„ŸçŸ¥æœº(MLP)è¿›è¡Œä½ç½®é¢‘ç‡ç¼–ç ï¼Œè·¨åˆ†è¾¨ç‡ç›¸å…³æ³¨æ„åŠ›å±‚è¿›è¡Œç©ºé—´è‡ªé€‚åº”å¯¹é½ï¼Œä»¥åŠé¢‘åŸŸL1æŸå¤±è¿›è¡Œé¢‘è°±ä¸€è‡´æ€§ç›‘ç£ï¼Œåœ¨å¤šç§ä¸»æµSISRéª¨å¹²ç½‘ç»œä¸­å‡æå‡äº†é‡å»ºé«˜é¢‘ç»†èŠ‚å’ŒæŠ‘åˆ¶ä¼ªå½±çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒFGAä»…å¢åŠ 0.3Må‚æ•°å³å¯åœ¨äº”ä¸ªä¸åŒè¶…åˆ†è¾¨ç‡ä¸»å¹²ä¸­è·å¾—å¹³å‡0.12â€“0.14dBçš„PSNRæå‡ï¼Œå¹¶åœ¨é«˜é¢‘å†…å®¹ä¸€è‡´æ€§ä¸Šæå‡é«˜è¾¾29%ï¼Œåœ¨çº¹ç†ä¸°å¯Œæ•°æ®é›†ä¸Šå°¤ä¸ºæ˜¾è‘—ï¼Œè¯æ˜å…¶åœ¨å‡å°‘æ··å å’Œä¿æŒç»†èŠ‚æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Fourier-Guided%20Attention%20Upsampling%20for%20Image%20Super-Resolution.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-Towards-Agentic-AI-for-Multimodal-Guided-Video-Object-Segmentation"><a href="#54-Towards-Agentic-AI-for-Multimodal-Guided-Video-Object-Segmentation" class="headerlink" title="54. Towards Agentic AI for Multimodal-Guided Video Object Segmentation"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Towards_Agentic_AI_for_Multimodal-Guided_Video_Object_Segmentation.pdf">Towards Agentic AI for Multimodal-Guided Video Object Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Deakin University</span></p>
<p>æœ¬æ–‡æå‡ºM2-Agentï¼Œä¸€ç§ç”¨äºå¤šæ¨¡æ€æŒ‡å¯¼ä¸‹è§†é¢‘ç›®æ ‡åˆ†å‰²çš„å…è®­ç»ƒæ™ºèƒ½ä½“ç³»ç»Ÿã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæ¯ä¸ªè¾“å…¥çš„åŠ¨æ€æ¨ç†æµç¨‹ï¼Œå¹¶é€šè¿‡ä¸ä¸“é—¨çš„è§†è§‰å’ŒéŸ³é¢‘å·¥å…·é›†çš„è¿­ä»£äº¤äº’ï¼Œçµæ´»è¯†åˆ«æ–‡æœ¬å’ŒéŸ³é¢‘æè¿°çš„ç›®æ ‡å¯¹è±¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒM2-Agentåœ¨Referring Video Object Segmentationï¼ˆRVOSï¼‰å’ŒReference Audio-Visual Segmentationï¼ˆRef-AVSï¼‰ä¸¤é¡¹ä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒäºç°æœ‰è®­ç»ƒè‡ªç”±æ–¹æ³•æœ‰æ˜æ˜¾æ€§èƒ½æå‡ï¼Œä¸”æ¥è¿‘æœ‰ç›‘ç£æœ€ä¼˜æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Towards%20Agentic%20AI%20for%20Multimodal-Guided%20Video%20Object%20Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-GCRPNet-Graph-Enhanced-Contextual-and-Regional-Perception-Network-For-Salient-Object-Detection-in-Optical-Remote-Sensing-Images"><a href="#55-GCRPNet-Graph-Enhanced-Contextual-and-Regional-Perception-Network-For-Salient-Object-Detection-in-Optical-Remote-Sensing-Images" class="headerlink" title="55. GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GCRPNet__Graph-Enhanced_Contextual_and_Regional_Perception_Network_For_Salient_Object_Detection_in_O.pdf">GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Hainan University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºMambaæ¶æ„çš„å…‰å­¦é¥æ„Ÿå›¾åƒæ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹æ–°æ–¹æ³•GCRPNetï¼Œé›†æˆäº†è§†è§‰çŠ¶æ€ç©ºé—´ç¼–ç å™¨ã€å·®å¼‚-ç›¸ä¼¼æ€§å¼•å¯¼åˆ†å±‚å›¾æ³¨æ„æ¨¡å—ï¼ˆDS-HGAMï¼‰å’Œå±€éƒ¨å¢å¼ºè§†è§‰çŠ¶æ€ç©ºé—´ï¼ˆLEVSSï¼‰è§£ç å™¨ã€‚DS-HGAMåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå®ç°å¤šå°ºåº¦ç‰¹å¾èåˆå’Œè·¨ç©ºé—´å»ºæ¨¡ï¼ŒLEVSSç»“åˆå¤šå°ºåº¦ååŒæ³¨æ„åŠ›å’Œå—å†…å››æ–¹å‘æ‰«ææå‡å±€éƒ¨ç»†èŠ‚è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGCRPNetåœ¨ORSSDå’ŒEORSSDæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·å¤‡æ›´å¼ºçš„é²æ£’æ€§å’Œç²¾åº¦ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GCRPNet%3A%20Graph-Enhanced%20Contextual%20and%20Regional%20Perception%20Network%20For%20Salient%20Object%20Detection%20in%20Optical%20Remote%20Sensing%20Images.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-PASS-Probabilistic-Agentic-Supernet-Sampling-for-Interpretable-and-Adaptive-Chest-X-Ray-Reasoning"><a href="#56-PASS-Probabilistic-Agentic-Supernet-Sampling-for-Interpretable-and-Adaptive-Chest-X-Ray-Reasoning" class="headerlink" title="56. PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/PASS__Probabilistic_Agentic_Supernet_Sampling_for_Interpretable_and_Adaptive_Chest_X-Ray_Reasoning.pdf">PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The University of Hong Kong</span></p>
<p>è¯¥è®ºæ–‡æå‡ºPASSæ¡†æ¶ï¼Œé€šè¿‡æ¦‚ç‡åŒ–çš„agentic superneté‡‡æ ·å®ç°èƒ¸éƒ¨Xå…‰çš„å¤šæ¨¡æ€åŒ»å­¦æ¨ç†ï¼Œæ ¸å¿ƒåŒ…æ‹¬åŠ¨æ€é‡‡æ ·å¤šå·¥å…·å·¥ä½œæµå¹¶ç”¨å¯è§£é‡Šæ¦‚ç‡æ ‡æ³¨å†³ç­–è·¯å¾„ã€‚æ–¹æ³•åŒ…å«ä¸‰é˜¶æ®µè®­ç»ƒï¼šä¸“å®¶çŸ¥è¯†çƒ­èº«ã€å¯¹æ¯”è·¯å¾„æ’åºå’Œæˆæœ¬æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼Œæœ‰æ•ˆä¼˜åŒ–å‡†ç¡®ç‡ä¸è®¡ç®—æˆæœ¬æƒè¡¡ã€‚å®éªŒè¡¨æ˜PASSåœ¨å¤šé¡¹åŒ»å­¦æ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºä¸»æµæ¨¡å‹ï¼Œåœ¨ä¿è¯é«˜å‡†ç¡®ç‡çš„åŒæ—¶å®ç°é«˜å¯è§£é‡Šæ€§å’Œæ•ˆç‡ï¼Œå¯å¹¿æ³›ç”¨äºé«˜å®‰å…¨æ€§è¦æ±‚çš„åŒ»å­¦åœºæ™¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PASS%3A%20Probabilistic%20Agentic%20Supernet%20Sampling%20for%20Interpretable%20and%20Adaptive%20Chest%20X-Ray%20Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-X-Node-Self-Explanation-is-All-We-Need"><a href="#57-X-Node-Self-Explanation-is-All-We-Need" class="headerlink" title="57. X-Node: Self-Explanation is All We Need"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/X-Node__Self-Explanation_is_All_We_Need.pdf">X-Node: Self-Explanation is All We Need</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Imperial College London</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªè§£é‡Šå‹å›¾ç¥ç»ç½‘ç»œæ¡†æ¶X-Nodeï¼Œåœ¨èŠ‚ç‚¹åˆ†ç±»è¿‡ç¨‹ä¸­æ¯ä¸ªèŠ‚ç‚¹èƒ½ç”Ÿæˆè‡ªèº«è§£é‡Šï¼Œæ–¹æ³•åŒ…æ‹¬æ„å»ºç»“æ„åŒ–ä¸Šä¸‹æ–‡å‘é‡ã€è§£é‡Šå‘é‡ç”Ÿæˆã€LLMæ–‡æœ¬è§£é‡Šå’Œè§£é‡Šåé¦ˆæ³¨å…¥GNNæµç¨‹ã€‚å®éªŒè¯æ˜X-Nodeåœ¨MedMNISTå’ŒMorphoMNISTå›¾æ•°æ®é›†ä¸Šä¿æŒç«äº‰åˆ†ç±»æ€§èƒ½å¹¶ç”Ÿæˆæ¯èŠ‚ç‚¹å¯è§£é‡Šå†³ç­–ï¼Œæœ‰æ•ˆæå‡æ¨¡å‹é€æ˜åº¦å’Œä¸´åºŠåº”ç”¨ä¿¡ä»»åº¦ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/X-Node%3A%20Self-Explanation%20is%20All%20We%20Need.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-Multi-Label-Plant-Species-Prediction-with-Metadata-Enhanced-Multi-Head-Vision-Transformers"><a href="#58-Multi-Label-Plant-Species-Prediction-with-Metadata-Enhanced-Multi-Head-Vision-Transformers" class="headerlink" title="58. Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Multi-Label_Plant_Species_Prediction_with_Metadata-Enhanced_Multi-Head_Vision_Transformers.pdf">Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Hamburg</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šå¤´Vision Transformerï¼ˆViTï¼‰çš„å¤šæ ‡ç­¾æ¤ç‰©ç§ç±»é¢„æµ‹æ–¹æ³•ï¼Œç»“åˆäº†DINOv2é¢„è®­ç»ƒæ¨¡å‹ã€ç‰©ç§-å±-ç§‘å¤šå¤´åˆ†ç±»ã€å…ƒæ•°æ®å¢å¼ºã€å¤šå°ºåº¦åˆ‡ç‰‡ã€åŠ¨æ€é˜ˆå€¼ä¼˜åŒ–å’Œé›†æˆç­–ç•¥ï¼ˆHydraæ¶æ„ï¼‰ï¼Œä»¥å®ç°å¯¹é«˜åˆ†è¾¨ç‡æ¤è¢«æ ·æ–¹å›¾åƒä¸­å¤šç§æ¤ç‰©çš„è‡ªåŠ¨è¯†åˆ«ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨PlantCLEF 2025æŒ‘æˆ˜èµ›çš„å…¬å…±æ¦œå•ä¸­å–å¾—äº†ç¬¬ä¸‰åçš„æˆç»©ï¼Œä½†åœ¨ç§æœ‰æ¦œå•ä¸Šè¡¨ç°ä¸‹é™ï¼Œè¯´æ˜æ¨¡å‹å¯¹é¢†åŸŸè½¬ç§»è¾ƒä¸ºæ•æ„Ÿä½†ä»å…·ç«äº‰åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Label%20Plant%20Species%20Prediction%20with%20Metadata-Enhanced%20Multi-Head%20Vision%20Transformers.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-From-Images-to-Perception-Emergence-of-Perceptual-Properties-by-Reconstructing-Images"><a href="#59-From-Images-to-Perception-Emergence-of-Perceptual-Properties-by-Reconstructing-Images" class="headerlink" title="59. From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/From_Images_to_Perception__Emergence_of_Perceptual_Properties_by_Reconstructing_Images.pdf">From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Valencia</span></p>
<p>æœ¬è®ºæ–‡æå‡ºå¹¶è®­ç»ƒäº†ä¸€ç§ç”Ÿç‰©å¯å‘çš„è§†è§‰ç³»ç»Ÿç¥ç»ç½‘ç»œPerceptNetï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ ï¼ˆåŒ…æ‹¬è‡ªç¼–ç ã€å»å™ªã€å»æ¨¡ç³Šå’Œç¨€ç–æ­£åˆ™åŒ–ä»»åŠ¡ï¼‰é‡å»ºè‡ªç„¶å›¾åƒï¼Œç ”ç©¶æ¨¡å‹å†…éƒ¨è¡¨å¾ä¸äººç±»æ„ŸçŸ¥ä¸€è‡´æ€§çš„æ¶Œç°ç°è±¡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ç¼–ç å™¨ï¼ˆç±»V1çš®å±‚ï¼‰é˜¶æ®µï¼Œæ¨¡å‹è¡¨å¾ä¸äººç±»ä¸»è§‚å¤±çœŸè¯„ä»·é«˜åº¦ç›¸å…³ï¼Œé€‚åº¦çš„å™ªå£°ã€æ¨¡ç³Šå’Œç¨€ç–åº¦å¯å¢å¼ºè¿™ç§ä¸€è‡´æ€§ï¼Œè¯´æ˜ç”Ÿç‰©è§†è§‰ç³»ç»Ÿå¯èƒ½æ­£æ˜¯ä¼˜åŒ–äº†å»é™¤é€‚åº¦å¤±çœŸå’Œç¨€ç–æ€§ï¼Œä¸”æ— éœ€äººå·¥æ„ŸçŸ¥æ ‡ç­¾å³å¯å­¦å¾—æ„ŸçŸ¥åº¦é‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From%20Images%20to%20Perception%3A%20Emergence%20of%20Perceptual%20Properties%20by%20Reconstructing%20Images.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-MM-Food-100K-A-100-000-Sample-Multimodal-Food-Intelligence-Dataset-with-Verifiable-Provenance"><a href="#60-MM-Food-100K-A-100-000-Sample-Multimodal-Food-Intelligence-Dataset-with-Verifiable-Provenance" class="headerlink" title="60. MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MM-Food-100K__A_100,000-Sample_Multimodal_Food_Intelligence_Dataset_with_Verifiable_Provenance.pdf">MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Codatta Community</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†MM-Food-100Kï¼Œä¸€ç§åŒ…å«10ä¸‡æ¡æ ·æœ¬çš„å¤šæ¨¡æ€é£Ÿç‰©æ™ºèƒ½æ•°æ®é›†ï¼Œé€šè¿‡ç¤¾åŒºè´¡çŒ®ä¸å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è‡ªåŠ¨è´¨é‡å®¡æ ¸ç»“åˆçš„æ•°æ®é‡‡é›†æµç¨‹ï¼Œæä¾›ä¸°å¯Œã€å¤šå±‚æ¬¡çš„æ³¨é‡ŠåŠå¯éªŒè¯æº¯æºã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºMM-Food-100Kå¾®è°ƒçš„æ¨¡å‹åœ¨é£Ÿç‰©åˆ†ç±»ä¸è¥å…»å›å½’ä»»åŠ¡ä¸Šå‡ä¼˜äºåŸå§‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸”é‡‡ç”¨åŒºå—é“¾åè®®å®ç°æ•°æ®æº¯æºä¸è´¡çŒ®è€…åˆ†æ¶¦æœºåˆ¶ï¼Œä¸ºé«˜è´¨é‡ç¤¾åŒºæ•°æ®é›†çš„å¯æŒç»­å»ºè®¾æä¾›äº†æ–°èŒƒå¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MM-Food-100K%3A%20A%20100%2C000-Sample%20Multimodal%20Food%20Intelligence%20Dataset%20with%20Verifiable%20Provenance.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-Adapting-SAM-via-Cross-Entropy-Masking-for-Class-Imbalance-in-Remote-Sensing-Change-Detection"><a href="#61-Adapting-SAM-via-Cross-Entropy-Masking-for-Class-Imbalance-in-Remote-Sensing-Change-Detection" class="headerlink" title="61. Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Adapting_SAM_via_Cross-Entropy_Masking_for_Class_Imbalance_in_Remote_Sensing_Change_Detection.pdf">Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The University of Sydney</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†åŸºäºSegment Anything Model (SAM) çš„é¥æ„Ÿå˜åŒ–æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡ç»†è‡´åœ°å¾®è°ƒFastSAMç¼–ç å™¨ï¼Œå¹¶å¼•å…¥ç©ºé—´-æ—¶åºç‰¹å¾å¢å¼ºï¼ˆSTFEï¼‰å’Œå¤šå°ºåº¦è§£ç å™¨èåˆï¼ˆMSDFï¼‰ï¼Œæœ‰æ•ˆæå‡äº†å¤šå°ºåº¦ä¸‹çš„å˜åŒ–æ£€æµ‹èƒ½åŠ›ã€‚é’ˆå¯¹å˜åŒ–æ£€æµ‹ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œåˆ›æ–°æ€§åœ°æå‡ºäº†äº¤å‰ç†µæ©ç æŸå¤±ï¼ˆCEMï¼‰ï¼Œé€šè¿‡åœ¨æŸå¤±è®¡ç®—ä¸­éšæœºä¸¢å¼ƒæœªå˜åŒ–åƒç´ ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹å˜åŒ–ç±»åˆ«çš„æ£€æµ‹èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨Levir-CDã€WHU-CDã€CLCDå’ŒS2Lookingç­‰å››ä¸ªå˜åŒ–æ£€æµ‹æ•°æ®é›†ä¸Šå‡å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨S2Lookingæ•°æ®é›†ä¸ŠF1æå‡2.5%ã€‚ç»“è®ºæ˜¯ç»“åˆSAMå¯†é›†é¢„æµ‹ç‰¹æ€§å’ŒCEMæŸå¤±çš„æ¶æ„åœ¨é¥æ„Ÿå˜åŒ–æ£€æµ‹ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Adapting%20SAM%20via%20Cross-Entropy%20Masking%20for%20Class%20Imbalance%20in%20Remote%20Sensing%20Change%20Detection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-PTQAT-A-Hybrid-Parameter-Efficient-Quantization-Algorithm-for-3D-Perception-Tasks"><a href="#62-PTQAT-A-Hybrid-Parameter-Efficient-Quantization-Algorithm-for-3D-Perception-Tasks" class="headerlink" title="62. PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/PTQAT__A_Hybrid_Parameter-Efficient_Quantization_Algorithm_for_3D_Perception_Tasks.pdf">PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Peking University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºPTQATï¼Œä¸€ç§ç»“åˆåè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰çš„é«˜æ•ˆæ··åˆé‡åŒ–ç®—æ³•ï¼Œä¸“ä¸º3Dæ„ŸçŸ¥ä»»åŠ¡è®¾è®¡ã€‚æ–¹æ³•é¦–å…ˆé€šè¿‡PTQé¢„æ£€æŸ¥å„å±‚é‡åŒ–è¯¯å·®ï¼Œé‡‡ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ç­›é€‰éœ€QATå¾®è°ƒçš„å…³é”®å±‚ï¼Œå…¶ä½™å±‚ä¿æŒå†»ç»“ï¼Œé‡ç‚¹åœ¨è¯¯å·®ä¼ æ’­é˜¶æ®µè¿›è¡Œè¡¥å¿ã€‚ç»“è®ºè¡¨æ˜ï¼ŒPTQATåœ¨3Dç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€å ç”¨é¢„æµ‹ç­‰ä»»åŠ¡ä¸Šï¼Œèƒ½ä»¥æ›´å°‘å‚æ•°å¾®è°ƒå’Œæ›´é«˜æ•ˆç‡ï¼Œå®ç°ä¸ç”šè‡³ä¼˜äºå…¨é‡QATçš„ç²¾åº¦ï¼ŒåŒæ—¶ä¾¿äºå®é™…éƒ¨ç½²ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PTQAT%3A%20A%20Hybrid%20Parameter-Efficient%20Quantization%20Algorithm%20for%203D%20Perception%20Tasks.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-EgoMusic-driven-Human-Dance-Motion-Estimation-with-Skeleton-Mamba"><a href="#63-EgoMusic-driven-Human-Dance-Motion-Estimation-with-Skeleton-Mamba" class="headerlink" title="63. EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/EgoMusic-driven_Human_Dance_Motion_Estimation_with_Skeleton_Mamba.pdf">EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">FPT Software AI Center</span></p>
<p>æœ¬æ–‡æå‡ºEgoMusic Motion Networkå’Œæ ¸å¿ƒSkeleton Mambaæ¨¡å‹ï¼Œå®ç°ä»ç¬¬ä¸€è§†è§’è§†é¢‘å’ŒéŸ³ä¹å…±åŒé©±åŠ¨çš„äººç±»èˆè¹ˆåŠ¨ä½œä¼°è®¡ã€‚æ–¹æ³•åŒ…æ‹¬æ–°å‹å¤§è§„æ¨¡EgoAIST++æ•°æ®é›†ã€å¤šæ¨¡æ€ç‰¹å¾èåˆã€æ¡ä»¶æ‰©æ•£æ¨¡å‹å’Œç©ºé—´-æ—¶é—´æ‰«æç­–ç•¥ï¼Œå®éªŒæ˜¾ç¤ºåœ¨å¤šé¡¹èˆè¹ˆåŠ¨ä½œä¼°è®¡æŒ‡æ ‡ä¸Šæ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½æ³›åŒ–è‡³å®é™…åœºæ™¯ã€‚ç»“è®ºï¼šè”åˆåˆ©ç”¨è§†è§‰å’ŒéŸ³ä¹è¾“å…¥å¯å¤§å¹…æå‡äººä½“èˆè¹ˆåŠ¨ä½œæ¨æ–­çš„å‡†ç¡®æ€§å’Œåè°ƒæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/EgoMusic-driven%20Human%20Dance%20Motion%20Estimation%20with%20Skeleton%20Mamba.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-TweezeEdit-Consistent-and-Efficient-Image-Editing-with-Path-Regularization"><a href="#64-TweezeEdit-Consistent-and-Efficient-Image-Editing-with-Path-Regularization" class="headerlink" title="64. TweezeEdit: Consistent and Efficient Image Editing with Path Regularization"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/TweezeEdit__Consistent_and_Efficient_Image_Editing_with_Path_Regularization.pdf">TweezeEdit: Consistent and Efficient Image Editing with Path Regularization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Hong Kong University of Science and Technology</span></p>
<p>æœ¬æ–‡æå‡ºTweezeEditï¼Œä¸€ç§åŸºäºä¸€è‡´æ€§æ¨¡å‹çš„æ— è°ƒä¼˜ã€æ— åæ¼”è·¯å¾„æ­£åˆ™åŒ–å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œé’ˆå¯¹æ‰©æ•£æ¨¡å‹ä¸‹ç°æœ‰æ–¹æ³•è¿‡åº¦å¯¹é½ç›®æ ‡æç¤ºã€éš¾ä»¥ä¿ç•™æºå›¾åƒè¯­ä¹‰çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å…¨ç¨‹æ­£åˆ™åŒ–å»å™ªè·¯å¾„å¹¶å¼•å…¥æ¢¯åº¦é©±åŠ¨çš„è·¯å¾„çº¦æŸï¼Œåœ¨æ— éœ€æ¨¡å‹ç»“æ„ä¿®æ”¹çš„æƒ…å†µä¸‹ï¼Œæé«˜è¯­ä¹‰ä¿ç•™å’Œç¼–è¾‘æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTweezeEditåœ¨è¯­ä¹‰ä¸€è‡´æ€§ã€ç›®æ ‡å¯¹é½åŠæ¨ç†é€Ÿåº¦ä¸Šä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ï¼Œå¯åœ¨12æ­¥å†…å®Œæˆç¼–è¾‘ï¼Œå…·å¤‡å®æ—¶åº”ç”¨æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/TweezeEdit%3A%20Consistent%20and%20Efficient%20Image%20Editing%20with%20Path%20Regularization.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-STAMP-Multi-pattern-Attention-aware-Multiple-Instance-Learning-for-STAS-Diagnosis-in-Multi-center-Histopathology-Images"><a href="#65-STAMP-Multi-pattern-Attention-aware-Multiple-Instance-Learning-for-STAS-Diagnosis-in-Multi-center-Histopathology-Images" class="headerlink" title="65. STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/STAMP__Multi-pattern_Attention-aware_Multiple_Instance_Learning_for_STAS_Diagnosis_in_Multi-center_H.pdf">STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Hunan University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºSTAMPæ–¹æ³•ï¼Œé€šè¿‡åŒåˆ†æ”¯Transformeræ¶æ„å’Œå¤šæ¨¡å¼æ³¨æ„åŠ›èšåˆæ¨¡å—ï¼Œåˆ†æå¤šä¸­å¿ƒè‚ºç™Œç»„ç»‡ç—…ç†å›¾åƒä¸­çš„STASç‰¹å¾ï¼Œå®ç°ç—…ç¶åŒºåŸŸçš„åŠ¨æ€é€‰æ‹©å’Œå™ªå£°æŠ‘åˆ¶ï¼Œæå‡å…¨å±€è¡¨å¾çš„åˆ¤åˆ«åŠ›ã€‚å®éªŒè¡¨æ˜STAMPåœ¨ä¸‰å¤§å¤šä¸­å¿ƒæ•°æ®é›†ä¸ŠAUCå‡è¶…è¿‡0.79ï¼Œè¯Šæ–­æ€§èƒ½ä¼˜äºä¸´åºŠæ°´å¹³ï¼Œå…·æœ‰ä¸´åºŠè¾…åŠ©è¯Šæ–­æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/STAMP%3A%20Multi-pattern%20Attention-aware%20Multiple%20Instance%20Learning%20for%20STAS%20Diagnosis%20in%20Multi-center%20Histopathology%20Images.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-NanoControl-A-Lightweight-Framework-for-Precise-and-Efficient-Control-in-Diffusion-Transformer"><a href="#66-NanoControl-A-Lightweight-Framework-for-Precise-and-Efficient-Control-in-Diffusion-Transformer" class="headerlink" title="66. NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/NanoControl__A_Lightweight_Framework_for_Precise_and_Efficient_Control_in_Diffusion_Transformer.pdf">NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">360 AI Research</span></p>
<p>æœ¬æ–‡æå‡ºNanoControlâ€”â€”ä¸€ç§åŸºäºDiffusion Transformerï¼ˆDiTï¼‰çš„è½»é‡çº§å¯æ§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚å…¶åˆ›æ–°ç‚¹åœ¨äºé‡‡ç”¨LoRAé£æ ¼æ§åˆ¶æ¨¡å—å’ŒKV-Context Augmentationæœºåˆ¶ï¼Œæå¤§å‡å°‘å‚æ•°ä¸è®¡ç®—é‡ï¼Œä»…å¢åŠ 0.024%å‚æ•°å’Œ0.029% GFLOPsï¼Œå®ç°é«˜æ•ˆæ¡ä»¶èåˆä¸ç²¾ç¡®å¯æ§ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒNanoControlåœ¨å¤šé¡¹æ¡ä»¶æ§åˆ¶ä»»åŠ¡ï¼ˆå¦‚è¾¹ç¼˜ã€æ·±åº¦ã€ç€è‰²ã€HEDï¼‰ä¸Šå®ç°äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„å¯æ§æ€§å’Œå›¾åƒè´¨é‡ï¼ŒåŒæ—¶å¤§å¹…é™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œéå¸¸é€‚åˆå®é™…éƒ¨ç½²ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/NanoControl%3A%20A%20Lightweight%20Framework%20for%20Precise%20and%20Efficient%20Control%20in%20Diffusion%20Transformer.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-Towards-Spatially-Consistent-Image-Generation-On-Incorporating-Intrinsic-Scene-Properties-into-Diffusion-Models"><a href="#67-Towards-Spatially-Consistent-Image-Generation-On-Incorporating-Intrinsic-Scene-Properties-into-Diffusion-Models" class="headerlink" title="67. Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Towards_Spatially_Consistent_Image_Generation__On_Incorporating_Intrinsic_Scene_Properties_into_Diff.pdf">Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">AI Institute, Seoul National University</span></p>
<p>æœ¬æ–‡æå‡ºäº†Intrinsic Latent Diffusion Model (I-LDM)ï¼Œé€šè¿‡è”åˆç”Ÿæˆå›¾ç‰‡å’Œå…¶å¯¹åº”çš„åœºæ™¯å†…åœ¨å±æ€§ï¼ˆæ·±åº¦ã€æ³•çº¿ã€åˆ†å‰²å›¾ã€çº¿æï¼‰ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„ç©ºé—´ä¸€è‡´æ€§ã€‚æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒä¼°ç®—å™¨è‡ªåŠ¨æå–åœºæ™¯å±æ€§ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›è·¨åŸŸè°ƒåº¦å’ŒVAEç¼–ç å®ç°é«˜æ•ˆååŒç”Ÿæˆã€‚å®éªŒæ˜¾ç¤ºI-LDMåœ¨å¤šç§åŸºçº¿æ¨¡å‹å’Œå¤æ‚åœºæ™¯ä¸‹å‡æ˜¾è‘—æ”¹å–„äº†ç©ºé—´å¸ƒå±€çš„è‡ªç„¶æ€§å’Œä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒäº†åŸå§‹æ¨¡å‹çš„å›¾åƒè´¨é‡å’Œæ–‡æœ¬åŒ¹é…èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Towards%20Spatially%20Consistent%20Image%20Generation%3A%20On%20Incorporating%20Intrinsic%20Scene%20Properties%20into%20Diffusion%20Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning"><a href="#68-SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning" class="headerlink" title="68. SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SynBrain__Enhancing_Visual-to-fMRI_Synthesis_via_Probabilistic_Representation_Learning.pdf">SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Artificial Intelligence Laboratory</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†SynBrainæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ¦‚ç‡å»ºæ¨¡ä¸è¯­ä¹‰æ¡ä»¶çº¦æŸï¼Œé‡‡ç”¨BrainVAEå’ŒS2N Mapperå…³é”®æ¨¡å—ï¼Œå®ç°è§†è§‰åˆºæ¿€åˆ°fMRIè„‘å“åº”çš„æ¦‚ç‡åˆ†å¸ƒå»ºæ¨¡å’Œä¸€æ­¥è¯­ä¹‰åˆ°ç¥ç»æ˜ å°„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynBrainåœ¨ä¸ªä½“ç‰¹å¼‚æ€§ç¼–ç å’Œå°‘æ ·æœ¬è‡ªé€‚åº”æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡fMRIä¿¡å·æå‡ç¥ç»è§£ç æ•ˆæœï¼ŒåŒæ—¶æ­ç¤ºè„‘åŠŸèƒ½çš„ä¸€è‡´æ€§å’Œç”Ÿç‰©ç¥ç»å˜å¼‚æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SynBrain%3A%20Enhancing%20Visual-to-fMRI%20Synthesis%20via%20Probabilistic%20Representation%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-DINOMotion-advanced-robust-tissue-motion-tracking-with-DINOv2-in-2D-Cine-MRI-guided-radiotherapy"><a href="#69-DINOMotion-advanced-robust-tissue-motion-tracking-with-DINOv2-in-2D-Cine-MRI-guided-radiotherapy" class="headerlink" title="69. DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/DINOMotion__advanced_robust_tissue_motion_tracking_with_DINOv2_in_2D-Cine_MRI-guided_radiotherapy.pdf">DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Concordia University</span></p>
<p>æœ¬æ–‡æå‡ºDINOMotionï¼Œä¸€ç§åŸºäºDINOv2åŸºç¡€è§†è§‰æ¨¡å‹å’ŒLow-Rank Adaptation (LoRA) å±‚çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äº2D-Cine MRIå¼•å¯¼æ”¾ç–—ä¸­çš„ç»„ç»‡è¿åŠ¨è¿½è¸ªã€‚è¯¥æ–¹æ³•è‡ªåŠ¨æ£€æµ‹å¯¹åº”æ ‡å¿—ç‚¹ä»¥å®ç°é«˜æ•ˆå’Œå¯è§£é‡Šçš„å›¾åƒé…å‡†ï¼Œå¹¶èƒ½å¤„ç†å¤§å¹…åº¦é”™ä½æƒ…å†µï¼Œåœ¨å¤šä¸ªå™¨å®˜çš„Diceåˆ†æ•°å’ŒHausdorffè·ç¦»è¯„æµ‹ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰è¾ƒå¼ºçš„å®æ—¶æ€§å’Œé²æ£’æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/DINOMotion%3A%20advanced%20robust%20tissue%20motion%20tracking%20with%20DINOv2%20in%202D-Cine%20MRI-guided%20radiotherapy.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-SC-Lane-Slope-aware-and-Consistent-Road-Height-Estimation-Framework-for-3D-Lane-Detection"><a href="#70-SC-Lane-Slope-aware-and-Consistent-Road-Height-Estimation-Framework-for-3D-Lane-Detection" class="headerlink" title="70. SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SC-Lane__Slope-aware_and_Consistent_Road_Height_Estimation_Framework_for_3D_Lane_Detection.pdf">SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Seoul National University</span></p>
<p>æœ¬æ–‡æå‡ºäº†SC-Laneï¼Œä¸€ç§é¢å‘å¡åº¦ä¸”å…·æœ‰æ—¶åºä¸€è‡´æ€§çš„3Dè½¦é“æ£€æµ‹é“è·¯é«˜åº¦ä¼°è®¡æ¡†æ¶ã€‚æ–¹æ³•æ ¸å¿ƒåŒ…æ‹¬å¡åº¦è‡ªé€‚åº”ç‰¹å¾æ¨¡å—ï¼ŒåŠ¨æ€èåˆå¤šå¡åº¦é«˜åº¦ç‰¹å¾ï¼Œå¹¶å¼•å…¥é«˜åº¦ä¸€è‡´æ€§æ¨¡å—ï¼Œé€šè¿‡åˆ©ç”¨å›¾åƒçº¿ç´¢è‡ªé€‚åº”åŠ æƒåŠæ—¶åºä¸€è‡´æ€§æŸå¤±ï¼Œæå‡å¤šå¸§é«˜åº¦ä¼°è®¡çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚å®éªŒé‡‡ç”¨MAEã€RMSEå’Œé˜ˆå€¼ç²¾åº¦ç­‰æ–°æ ‡å‡†ï¼ŒOpenLaneåŸºå‡†ä¸ŠSC-Laneåœ¨é«˜åº¦ä¼°è®¡å’Œ3Dè½¦é“æ£€æµ‹ä¸Šå‡è¾¾åˆ°SOTAï¼ŒF-score 64.3%ã€‚ç»“è®ºè¡¨æ˜è¯¥æ–¹æ³•æå‡äº†è½¦é“æ£€æµ‹çš„å‡†ç¡®æ€§å’Œé«˜åº¦ä¼°è®¡çš„ç¨³å®šæ€§ï¼Œå¯æ‰©å±•è‡³è‡ªåŠ¨é©¾é©¶ç›¸å…³ä»»åŠ¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SC-Lane%3A%20Slope-aware%20and%20Consistent%20Road%20Height%20Estimation%20Framework%20for%203D%20Lane%20Detection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-Translation-of-Text-Embedding-via-Delta-Vector-to-Suppress-Strongly-Entangled-Content-in-Text-to-Image-Diffusion-Models"><a href="#71-Translation-of-Text-Embedding-via-Delta-Vector-to-Suppress-Strongly-Entangled-Content-in-Text-to-Image-Diffusion-Models" class="headerlink" title="71. Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Translation_of_Text_Embedding_via_Delta_Vector_to_Suppress_Strongly_Entangled_Content_in_Text-to-Ima.pdf">Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Sungkyunkwan University</span></p>
<p>æœ¬æ–‡æå‡ºäº†é€šè¿‡åœ¨æ–‡æœ¬åµŒå…¥ç©ºé—´å¼•å…¥delta vectoræ¥ç›´æ¥æŠ‘åˆ¶æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å¼ºå…³è”è´Ÿå†…å®¹çš„æ–¹æ³•ï¼Œå¹¶è¿›ä¸€æ­¥æå‡ºäº†Selective Suppression with Delta Vectorï¼ˆSSDVï¼‰ï¼Œå°†delta vectoré›†æˆåˆ°è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°å¯¹ç‰¹å®šåŒºåŸŸè´Ÿå†…å®¹çš„ç²¾å‡†æŠ‘åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†å’Œä¸ªæ€§åŒ–æ¨¡å‹ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„æµ‹ä¸­å®ç°äº†æ›´æœ‰æ•ˆçš„å¼ºå…³è”è´Ÿå†…å®¹æŠ‘åˆ¶ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Translation%20of%20Text%20Embedding%20via%20Delta%20Vector%20to%20Suppress%20Strongly%20Entangled%20Content%20in%20Text-to-Image%20Diffusion%20Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-PQ-DAF-Pose-driven-Quality-controlled-Data-Augmentation-for-Data-scarce-Driver-Distraction-Detection"><a href="#72-PQ-DAF-Pose-driven-Quality-controlled-Data-Augmentation-for-Data-scarce-Driver-Distraction-Detection" class="headerlink" title="72. PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/PQ-DAF__Pose-driven_Quality-controlled_Data_Augmentation_for_Data-scarce_Driver_Distraction_Detectio.pdf">PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shandong University of Science and Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†PQ-DAFï¼ˆPose-driven Quality-controlled Data Augmentation Frameworkï¼‰ï¼Œç»“åˆå§¿æ€é©±åŠ¨çš„æ¸è¿›å¼æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆPCDMsï¼‰å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆCogVLMï¼‰å®ç°é«˜è´¨é‡é©¾é©¶å‘˜åˆ†å¿ƒæ£€æµ‹æ•°æ®å¢å¼ºã€‚æ–¹æ³•é€šè¿‡æå–é©¾é©¶å‘˜å…³é”®ç‚¹å§¿æ€ï¼Œåˆ©ç”¨PCDMsç”Ÿæˆç»“æ„ä¸€è‡´çš„ä¼ªæ ·æœ¬ï¼Œå¹¶ç”¨CogVLMå¯¹åˆæˆæ ·æœ¬è¿›è¡Œè¯­ä¹‰ä¸€è‡´æ€§ç­›é€‰ï¼Œå¤§å¹…æå‡å°‘æ ·æœ¬ä¸‹çš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPQ-DAFåœ¨StateFarmå’ŒAUC-DDDæ•°æ®é›†å°‘æ ·æœ¬è®¾å®šä¸‹å‡æ˜¾è‘—æå‡æ£€æµ‹å‡†ç¡®ç‡ï¼ŒéªŒè¯äº†å…¶åœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PQ-DAF%3A%20Pose-driven%20Quality-controlled%20Data%20Augmentation%20for%20Data-scarce%20Driver%20Distraction%20Detection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-Unlocking-Robust-Semantic-Segmentation-Performance-via-Label-only-Elastic-Deformations-against-Implicit-Label-Noise"><a href="#73-Unlocking-Robust-Semantic-Segmentation-Performance-via-Label-only-Elastic-Deformations-against-Implicit-Label-Noise" class="headerlink" title="73. Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Unlocking_Robust_Semantic_Segmentation_Performance_via_Label-only_Elastic_Deformations_against_Impli.pdf">Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">GIST</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºNSegment+çš„æ–°å‹æ•°æ®å¢å¼ºæ¡†æ¶ï¼Œä¸“ä¸ºè¯­ä¹‰åˆ†å‰²ä¸­çš„éšæ€§æ ‡ç­¾å™ªå£°é—®é¢˜è®¾è®¡ã€‚æ–¹æ³•æ ¸å¿ƒåœ¨äºä»…å¯¹åˆ†å‰²æ ‡ç­¾æ–½åŠ éšæœºå¼¹æ€§å½¢å˜ï¼ˆç»“åˆé«˜æ–¯å¹³æ»‘å’Œå°ºåº¦æ„ŸçŸ¥çš„å°ç›®æ ‡ä¿æŠ¤ï¼‰ï¼Œè€Œä¿æŒè¾“å…¥å›¾åƒä¸å˜ï¼Œä»è€Œæ¨¡æ‹ŸçœŸå®æ•°æ®ä¸­çš„è¾¹ç•Œæ¨¡ç³Šå’Œæ ‡æ³¨ä¸ä¸€è‡´ï¼Œæå‡æ¨¡å‹å¯¹æ ‡ç­¾ä¸ç¡®å®šæ€§çš„é²æ£’æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒNSegment+åœ¨é¥æ„Ÿå’Œè‡ªç„¶åœºæ™¯å…­å¤§ä¸»æµæ•°æ®é›†ä¸Šï¼Œå¹³å‡mIoUæå‡æ˜¾è‘—ï¼Œä¸”ä¸CutMixã€Label Smoothingç­‰ç°æœ‰å¢å¼º&#x2F;æ­£åˆ™åŒ–æ‰‹æ®µå…¼å®¹ï¼Œæ— éœ€å¢åŠ è®¡ç®—å¼€é”€ã€‚ç»“è®ºè®¤ä¸ºï¼ŒNSegment+ä¸ºè§£å†³ç°å®è¯­ä¹‰åˆ†å‰²ä¸­çš„éšæ€§æ ‡ç­¾å™ªå£°æä¾›äº†è½»é‡ã€é«˜æ•ˆä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Unlocking%20Robust%20Semantic%20Segmentation%20Performance%20via%20Label-only%20Elastic%20Deformations%20against%20Implicit%20Label%20Noise.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-AtomDiffuser-Time-Aware-Degradation-Modeling-for-Drift-and-Beam-Damage-in-STEM-Imaging"><a href="#74-AtomDiffuser-Time-Aware-Degradation-Modeling-for-Drift-and-Beam-Damage-in-STEM-Imaging" class="headerlink" title="74. AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/AtomDiffuser__Time-Aware_Degradation_Modeling_for_Drift_and_Beam_Damage_in_STEM_Imaging.pdf">AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Clemson University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºAtomDiffuserï¼Œä¸€ç§ç»“åˆæ—¶åºå·ç§¯U-Netå’Œç‰©ç†å¯å‘é™è§£å»ºæ¨¡çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿä»æ—¶åºSTEMå›¾åƒä¸­è§£è€¦å¹¶æ˜¾å¼é¢„æµ‹åŸå­çº§æ¼‚ç§»ï¼ˆé€šè¿‡ä»¿å°„å˜æ¢ï¼‰å’ŒæŸæµæŸä¼¤ï¼ˆé€šè¿‡ç©ºé—´è¡°å‡å›¾ï¼‰ï¼Œåˆ©ç”¨åˆæˆé™è§£åºåˆ—è®­ç»ƒå¹¶èƒ½æ³›åŒ–åˆ°çœŸå®å†·å†»STEMæ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒAtomDiffuseråœ¨æŸä¼¤è¯„ä¼°å’Œæ¼‚ç§»çŸ«æ­£ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰CNNæ–¹æ³•ï¼Œæ”¯æŒé«˜åˆ†è¾¨ç‡é™è§£æ¨æ–­å’ŒåŠ¨æ€ææ–™ç»“æ„æ¼”åŒ–çš„å¯è§†åŒ–ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AtomDiffuser%3A%20Time-Aware%20Degradation%20Modeling%20for%20Drift%20and%20Beam%20Damage%20in%20STEM%20Imaging.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-Improving-OCR-for-Historical-Texts-of-Multiple-Languages"><a href="#75-Improving-OCR-for-Historical-Texts-of-Multiple-Languages" class="headerlink" title="75. Improving OCR for Historical Texts of Multiple Languages"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Improving_OCR_for_Historical_Texts_of_Multiple_Languages.pdf">Improving OCR for Historical Texts of Multiple Languages</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Groningen</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†å¤šè¯­è¨€å†å²æ–‡çŒ®çš„OCRä¸æ–‡æ¡£ç‰ˆé¢åˆ†ææ–¹æ³•ã€‚é’ˆå¯¹æ­»æµ·å¤å·å¸Œä¼¯æ¥æ–‡ï¼Œé‡‡ç”¨æ•°æ®å¢å¼ºå¹¶æ¯”è¾ƒäº†Krakenä¸TrOCRæ¨¡å‹æ•ˆæœï¼›åœ¨è·å…°å†å²ä¼šè®®è®°å½•ä¸Šï¼Œç»“åˆDeepLabV3+è¯­ä¹‰åˆ†å‰²ä¸BiLSTMï¼Œå¹¶é€šè¿‡ä¼ªæ ‡ç­¾æå‡è®­ç»ƒæ•ˆæœï¼›åœ¨ç°ä»£è‹±æ–‡æ‰‹å†™è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œé‡‡ç”¨CRNNä¸ResNet34ç¼–ç å¹¶ä½¿ç”¨CTCæŸå¤±ã€‚å®éªŒè¡¨æ˜ï¼šæ•°æ®å¢å¼ºä¸è½¬ç§»å­¦ä¹ èƒ½æ˜¾è‘—æå‡å†å²æ–‡çŒ®OCRï¼Œä¼ªæ ‡ç­¾è‡ªç›‘ç£ç­–ç•¥èƒ½æœ‰æ•ˆæå‡æ–‡æ¡£ç»“æ„åˆ†æçš„æ€§èƒ½ï¼Œå¤æ‚æ¨¡å‹åœ¨å°æ•°æ®é›†ä¸Šæœªå¿…ä¼˜äºç®€å•æ¶æ„ï¼Œæœªæ¥éœ€åŠ å¼ºæ•°æ®å¤šæ ·æ€§ä¸æ¨¡å‹è®¾è®¡çš„å¹³è¡¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving%20OCR%20for%20Historical%20Texts%20of%20Multiple%20Languages.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-Efficient-Image-Denoising-Using-Global-and-Local-Circulant-Representation"><a href="#76-Efficient-Image-Denoising-Using-Global-and-Local-Circulant-Representation" class="headerlink" title="76. Efficient Image Denoising Using Global and Local Circulant Representation"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Efficient_Image_Denoising_Using_Global_and_Local_Circulant_Representation.pdf">Efficient Image Denoising Using Global and Local Circulant Representation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">South China University of Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å›¾åƒå»å™ªç®—æ³•Haar-tSVDï¼Œé€šè¿‡ç»“åˆå…¨çƒå’Œå±€éƒ¨å¾ªç¯ç»“æ„ï¼Œåˆ©ç”¨t-SVDå’ŒHaarå˜æ¢åœ¨å¾ªç¯è¡¨ç¤ºä¸‹æœ‰æ•ˆæ•è·å›¾åƒå—çš„ç›¸å…³æ€§ï¼Œå®ç°ä¸€æ­¥é«˜å¹¶è¡Œå¯æ‰©å±•çš„æ»¤æ³¢ï¼Œæ— éœ€å­¦ä¹ å±€éƒ¨åŸºã€‚æ–¹æ³•è¿˜å¼•å…¥CNNè‡ªé€‚åº”å™ªå£°ä¼°è®¡å’Œç‰¹å¾å€¼åˆ†ææå‡é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒã€è§†é¢‘ã€HSIã€MRIç­‰å¤šä»»åŠ¡ä¸­å…·æœ‰æ•ˆæœå’Œæ•ˆç‡å¹¶é‡çš„ä¼˜åŠ¿ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Efficient%20Image%20Denoising%20Using%20Global%20and%20Local%20Circulant%20Representation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-Improving-Learning-of-New-Diseases-through-Knowledge-Enhanced-Initialization-for-Federated-Adapter-Tuning"><a href="#77-Improving-Learning-of-New-Diseases-through-Knowledge-Enhanced-Initialization-for-Federated-Adapter-Tuning" class="headerlink" title="77. Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Improving_Learning_of_New_Diseases_through_Knowledge-Enhanced_Initialization_for_Federated_Adapter_T.pdf">Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR)</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFedKEIçš„è”é‚¦çŸ¥è¯†å¢å¼ºåˆå§‹åŒ–æ¡†æ¶ï¼Œç»“åˆäº†å…¨å±€èšç±»å’ŒåŒå±‚èšåˆæƒé‡å­¦ä¹ ï¼Œå®ç°äº†åŒ»ç–—é¢†åŸŸä¸­åŸºç¡€æ¨¡å‹é€‚é…å™¨è°ƒä¼˜è¿‡ç¨‹ä¸­æ–°ä»»åŠ¡çš„é«˜æ•ˆé€‚åº”ã€‚FedKEIé€šè¿‡æœåŠ¡å™¨ç«¯å¯¹ä»»åŠ¡ç‰¹å®šæ¨¡å—è¿›è¡Œèšç±»ï¼Œå¹¶é‡‡ç”¨åŒå±‚ä¼˜åŒ–ç­–ç•¥ååŒå­¦ä¹ èšåˆæƒé‡ï¼Œæœ‰æ•ˆæ•´åˆå¹¶è½¬ç§»è·¨ä»»åŠ¡ä¸è·¨å®¢æˆ·çš„çŸ¥è¯†ï¼Œä»è€Œä¸ºé€‚é…å™¨å’Œå¤´éƒ¨ç”Ÿæˆæ›´ä¼˜çš„åˆå§‹åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFedKEIåœ¨çš®è‚¤ã€èƒ¸éƒ¨Xå…‰å’Œè§†ç½‘è†œOCTç­‰å¤šæ¨¡æ€åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šï¼Œåœ¨æ–°ç–¾ç—…é€‚åº”ä»»åŠ¡ä¸­å‡è¶…è¿‡ç°æœ‰æœ€æ–°æ–¹æ³•ï¼Œæå‡äº†æ–°ä»»åŠ¡å­¦ä¹ çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving%20Learning%20of%20New%20Diseases%20through%20Knowledge-Enhanced%20Initialization%20for%20Federated%20Adapter%20Tuning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-A-Sub-Pixel-Multimodal-Optical-Remote-Sensing-Images-Matching-Method"><a href="#78-A-Sub-Pixel-Multimodal-Optical-Remote-Sensing-Images-Matching-Method" class="headerlink" title="78. A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Sub-Pixel_Multimodal_Optical_Remote_Sensing_Images_Matching_Method.pdf">A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Central South University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç›¸ä½ä¸€è‡´æ€§åŠ æƒæœ€å°ç»å¯¹åå·®ï¼ˆPCWLADï¼‰çš„äºšåƒç´ å¤šæ¨¡æ€å…‰å­¦é¥æ„Ÿå›¾åƒåŒ¹é…æ–¹æ³•ã€‚æ–¹æ³•æµç¨‹åŒ…æ‹¬ï¼šé¦–å…ˆåˆ©ç”¨ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰åœ¨æœªé™å™ªçš„ç›¸ä½ä¸€è‡´æ€§ï¼ˆPCï¼‰å›¾ä¸Šè¿›è¡Œç²—åŒ¹é…ï¼Œç„¶åé€šè¿‡äº’ç»“æ„åŠ æƒå’ŒåŠ æƒæœ€å°ç»å¯¹åå·®ï¼ˆWLADï¼‰å‡†åˆ™ï¼Œå®ç°äºšåƒç´ çº§åˆ«çš„ç²¾ç¡®åŒ¹é…ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸‰ä¸ªç±»å‹çš„å¯è§-çº¢å¤–é¥æ„Ÿæ•°æ®é›†ä¸Šï¼ŒPCWLADåœ¨åŒ¹é…æ­£ç¡®ç‡ï¼ˆCMRï¼‰å’Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰æ–¹é¢å‡ä¼˜äºç°æœ‰ä¸ƒç§ä¸»æµæ–¹æ³•ï¼Œå¹³å‡RMSEçº¦ä¸º0.4åƒç´ ï¼›ä½†æ–¹æ³•åœ¨å¤§å‡ ä½•å˜å½¢åœºæ™¯ä¸‹æ•ˆæœæœ‰é™ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Sub-Pixel%20Multimodal%20Optical%20Remote%20Sensing%20Images%20Matching%20Method.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-Deep-Learning-for-Crack-Detection-A-Review-of-Learning-Paradigms-Generalizability-and-Datasets"><a href="#79-Deep-Learning-for-Crack-Detection-A-Review-of-Learning-Paradigms-Generalizability-and-Datasets" class="headerlink" title="79. Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Deep_Learning_for_Crack_Detection__A_Review_of_Learning_Paradigms,_Generalizability,_and_Datasets.pdf">Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Georgia Institute of Technology</span></p>
<p>æœ¬è®ºæ–‡ç³»ç»Ÿå›é¡¾äº†åŸºäºæ·±åº¦å­¦ä¹ çš„è£‚ç¼æ£€æµ‹é¢†åŸŸï¼Œèšç„¦äºç›‘ç£ã€åŠç›‘ç£ã€å¼±ç›‘ç£ã€æ— ç›‘ç£ã€å°‘æ ·æœ¬ã€é¢†åŸŸè‡ªé€‚åº”åŠåŸºç¡€æ¨¡å‹ç­‰å¤šç§å­¦ä¹ èŒƒå¼çš„æŠ€æœ¯æ¼”è¿›å’Œæ³›åŒ–èƒ½åŠ›ï¼Œè¯¦è¿°äº†ä¸»æµæ–¹æ³•ï¼ˆå¦‚CNNã€Transformerã€SAMç­‰ï¼‰ã€å…³é”®æµç¨‹åŠä»£è¡¨æ€§æ•°æ®é›†ï¼ŒåŒæ—¶å‘å¸ƒäº†é«˜åˆ†è¾¨ç‡3Dæ¿€å…‰æ‰«æè£‚ç¼æ•°æ®é›†3DCrackã€‚ç»“è®ºæŒ‡å‡ºï¼Œæœªæ¥éœ€è¿›ä¸€æ­¥çªç ´æ•°æ®é«˜æ•ˆåˆ©ç”¨ã€æ¨¡å‹æ³›åŒ–ã€æ ‡å‡†åŒ–è¯„æµ‹ä¸å¤šæ¨¡æ€èåˆç­‰æŒ‘æˆ˜ï¼Œä»¥å®ç°æ›´é²æ£’ã€å¯æ‰©å±•çš„è£‚ç¼è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Deep%20Learning%20for%20Crack%20Detection%3A%20A%20Review%20of%20Learning%20Paradigms%2C%20Generalizability%2C%20and%20Datasets.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="80-Performance-of-GPT-5-in-Brain-Tumor-MRI-Reasoning"><a href="#80-Performance-of-GPT-5-in-Brain-Tumor-MRI-Reasoning" class="headerlink" title="80. Performance of GPT-5 in Brain Tumor MRI Reasoning"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Performance_of_GPT-5_in_Brain_Tumor_MRI_Reasoning.pdf">Performance of GPT-5 in Brain Tumor MRI Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Emory University School of Medicine</span></p>
<p>æœ¬è®ºæ–‡ç³»ç»Ÿè¯„ä¼°äº†GPT-4oã€GPT-5-nanoã€GPT-5-miniå’ŒGPT-5åœ¨ä¸‰ç±»è„‘è‚¿ç˜¤MRIå¤šæ¨¡æ€è§†è§‰é—®ç­”(VQA)åŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œæ–¹æ³•åŒ…æ‹¬å°†BraTSæ•°æ®é›†çš„å¤šåºåˆ—MRIä¸ç»“æ„åŒ–ä¸´åºŠç‰¹å¾è½¬åŒ–ä¸ºæ ‡å‡†åŒ–VQAé—®é¢˜ï¼Œå¹¶é‡‡ç”¨zero-shot chain-of-thoughtæç¤ºè¿›è¡Œæ¨¡å‹æ¨ç†ä¸å‡†ç¡®ç‡å¯¹æ¯”ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPT-5-miniå–å¾—æœ€é«˜å¹³å‡å‡†ç¡®ç‡(44.19%)ï¼Œä½†å„æ¨¡å‹æ€§èƒ½ç›¸è¿‘ï¼Œå°šæœªè¾¾åˆ°ä¸´åºŠå®ç”¨æ°´å¹³ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Performance%20of%20GPT-5%20in%20Brain%20Tumor%20MRI%20Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-A-Survey-on-Diffusion-Language-Models"><a href="#81-A-Survey-on-Diffusion-Language-Models" class="headerlink" title="81. A Survey on Diffusion Language Models"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Survey_on_Diffusion_Language_Models.pdf">A Survey on Diffusion Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Mohamed bin Zayed University of Artificial Intelligence</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿæ¢³ç†äº†æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰çš„åŸç†ã€å‘å±•ã€æŠ€æœ¯å’ŒæŒ‘æˆ˜ï¼Œæ¶µç›–è¿ç»­ã€ç¦»æ•£åŠæ··åˆæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ç»†è‡´åˆ†æäº†å…¶é¢„è®­ç»ƒã€åè®­ç»ƒï¼ˆåŒ…æ‹¬å¼ºåŒ–å­¦ä¹ å¯¹é½ï¼‰ã€æ¨ç†ä¼˜åŒ–ã€å¹¶è¡Œè§£ç ã€ç¼“å­˜å’Œå¤šæ¨¡æ€æ‰©å±•ç­‰å…³é”®æŠ€æœ¯æµç¨‹ã€‚ç»“è®ºæŒ‡å‡ºï¼ŒDLMsåœ¨æ¨ç†é€Ÿåº¦ã€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œå¤šæ¨¡æ€ç»Ÿä¸€æ–¹é¢å±•ç°å‡ºç‹¬ç‰¹ä¼˜åŠ¿ï¼Œä¸ä¸»æµè‡ªå›å½’æ¨¡å‹æ€§èƒ½ç›¸å½“ï¼ŒåŒæ—¶ä¹Ÿé¢ä¸´å¹¶è¡Œä¸€è‡´æ€§ã€åŸºç¡€è®¾æ–½å’Œå¯æ‰©å±•æ€§ç­‰æŒ‘æˆ˜ï¼Œæœªæ¥å…·å¤‡å¹¿é˜”ç ”ç©¶ä¸åº”ç”¨å‰æ™¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Survey%20on%20Diffusion%20Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-Reinforced-Language-Models-for-Sequential-Decision-Making"><a href="#82-Reinforced-Language-Models-for-Sequential-Decision-Making" class="headerlink" title="82. Reinforced Language Models for Sequential Decision Making"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Reinforced_Language_Models_for_Sequential_Decision_Making.pdf">Reinforced Language Models for Sequential Decision Making</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Southampton</span></p>
<p>æœ¬æ–‡æå‡ºäº†Multi-Step Group-Relative Policy Optimizationï¼ˆMS-GRPOï¼‰ç®—æ³•ï¼Œé€šè¿‡åœ¨Text-Mediated Stochastic Gameï¼ˆTMSGï¼‰å’ŒLanguage Agent Policyï¼ˆLAPï¼‰æ¡†æ¶ä¸‹ï¼Œå°†å…¨å±€å›æŠ¥åˆ†é…åˆ°æ¯ä¸ªå†³ç­–æ­¥éª¤ï¼Œå¹¶ç»“åˆç»å¯¹ä¼˜åŠ¿åŠ æƒï¼ˆAAWï¼‰é‡‡æ ·ç­–ç•¥ï¼Œä»¥æå‡å°å‹LLMçš„åºåˆ—å†³ç­–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡è¯¥æ–¹æ³•å¾®è°ƒçš„3Bå‚æ•°æ¨¡å‹åœ¨Frozen Lakeä»»åŠ¡ä¸Šå†³ç­–èƒ½åŠ›æå‡ï¼Œè¶…è¶Šäº†72Bå‚æ•°çš„å¤§æ¨¡å‹50%ï¼›åŒæ—¶æ–¹æ³•åœ¨æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¹Ÿè¡¨ç°å‡ºä¼˜åŠ¿ï¼Œä½†ä¸ä¸“ç”¨DQNæ–¹æ³•ç›¸æ¯”ä»å­˜åœ¨æ€§èƒ½å·®è·ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Reinforced%20Language%20Models%20for%20Sequential%20Decision%20Making.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-Memory-Augmented-Transformers-A-Systematic-Review-from-Neuroscience-Principles-to-Technical-Solutions"><a href="#83-Memory-Augmented-Transformers-A-Systematic-Review-from-Neuroscience-Principles-to-Technical-Solutions" class="headerlink" title="83. Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Memory-Augmented_Transformers__A_Systematic_Review_from_Neuroscience_Principles_to_Technical_Solutio.pdf">Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Huawei Technologies</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿç»¼è¿°äº†åŸºäºç¥ç»ç§‘å­¦åŸç†ï¼ˆå¦‚å¤šæ—¶é—´å°ºåº¦åŠ¨æ€è®°å¿†ã€é€‰æ‹©æ€§æ³¨æ„å’Œå·©å›ºï¼‰ä¸å·¥ç¨‹å®ç°ç›¸ç»“åˆçš„Memory-Augmented TransformersæŠ€æœ¯ï¼Œæå‡ºäº†ä¸‰ç»´åˆ†ç±»ä½“ç³»ï¼Œåˆ†æäº†æ ¸å¿ƒæ“ä½œï¼ˆè¯»å†™ã€é—å¿˜ã€å®¹é‡ç®¡ç†ï¼‰åŠå…¶å®ç°æ–¹å¼ï¼Œå±•ç¤ºäº†é¢†åŸŸä»é™æ€ç¼“å­˜å‘è‡ªé€‚åº”æ¨ç†å’Œç»ˆèº«å­¦ä¹ ç³»ç»Ÿçš„æ¼”å˜ã€‚ç»“è®ºæŒ‡å‡ºï¼Œæ··åˆå­˜å‚¨ã€è‡ªé€‚åº”åŠ¨æ€å’Œæ™ºèƒ½é—å¿˜æˆä¸ºä¸»æµï¼Œä½†åœ¨å¯æ‰©å±•æ€§ã€å¹²æ‰°åè°ƒå’Œè¯„æµ‹æ ‡å‡†åŒ–ç­‰æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œæœªæ¥éœ€å€Ÿé‰´ç”Ÿç‰©è®°å¿†æœºåˆ¶æ¨åŠ¨è®¤çŸ¥å‹AIå‘å±•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Memory-Augmented%20Transformers%3A%20A%20Systematic%20Review%20from%20Neuroscience%20Principles%20to%20Technical%20Solutions.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-The-Knowledge-Reasoning-Dissociation-Fundamental-Limitations-of-LLMs-in-Clinical-Natural-Language-Inference"><a href="#84-The-Knowledge-Reasoning-Dissociation-Fundamental-Limitations-of-LLMs-in-Clinical-Natural-Language-Inference" class="headerlink" title="84. The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/The_Knowledge-Reasoning_Dissociation__Fundamental_Limitations_of_LLMs_in_Clinical_Natural_Language_I.pdf">The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Manchester</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹ä¸´åºŠæ¨ç†çš„è‡ªç„¶è¯­è¨€æ¨æ–­ï¼ˆNLIï¼‰åŸºå‡†ï¼ŒåŒ…æ‹¬å››ç±»æ¨ç†ä»»åŠ¡ï¼ˆå› æœå½’å› ã€ç»„åˆæ€§åŸºç¡€ã€è®¤çŸ¥éªŒè¯å’Œé£é™©æŠ½è±¡ï¼‰ï¼Œå¹¶è®¾è®¡äº†GKMRVæ¢é’ˆç”¨äºåŒºåˆ†çŸ¥è¯†æ£€ç´¢ä¸æ¨ç†èƒ½åŠ›ã€‚å®éªŒå‘ç°ï¼Œå½“å‰ä¸»æµå¤§è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†æ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°æä½³ï¼Œä½†åœ¨ç»“æ„åŒ–æ¨ç†ä»»åŠ¡ä¸Šæ™®éå¤±è´¥ï¼Œç»“è®ºè®¤ä¸ºä»…é æ¨¡å‹è§„æ¨¡æ‰©å±•æ— æ³•å®ç°å¯é çš„ä¸´åºŠæ¨ç†ï¼Œéœ€è¦æ›´å¼ºçš„å¯ç»„åˆå†…éƒ¨è¡¨ç¤ºã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/The%20Knowledge-Reasoning%20Dissociation%3A%20Fundamental%20Limitations%20of%20LLMs%20in%20Clinical%20Natural%20Language%20Inference.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-Modeling-Human-Responses-to-Multimodal-AI-Content"><a href="#85-Modeling-Human-Responses-to-Multimodal-AI-Content" class="headerlink" title="85. Modeling Human Responses to Multimodal AI Content"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Modeling_Human_Responses_to_Multimodal_AI_Content.pdf">Modeling Human Responses to Multimodal AI Content</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Salesforce Research Asia</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†T-Lensç³»ç»Ÿï¼Œé€šè¿‡äººç±»ä¸­å¿ƒçš„æ–¹æ³•é¢„æµ‹ç”¨æˆ·å¯¹å¤šæ¨¡æ€AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰çš„ååº”ï¼Œæ ¸å¿ƒæ¨¡å—HR-MCPåŸºäºå¤§è§„æ¨¡äººç±»æ ‡æ³¨æ•°æ®ï¼Œç»“åˆå¤šæ¨¡æ€ä¸€è‡´æ€§å’Œæƒ…æ„Ÿä¸€è‡´æ€§ï¼Œèƒ½é‡åŒ–ä¿¡ä»»åº¦ã€å½±å“å’Œå¼€æ”¾æ€§ç­‰æŒ‡æ ‡ï¼Œå¹¶å¯æ— ç¼é›†æˆè‡³ä»»æ„LLMã€‚å®éªŒè¡¨æ˜ï¼ŒT-Lensåœ¨æ¨¡æ‹Ÿäººç±»å¯¹å¤šæ¨¡æ€ä¿¡æ¯çš„ä¿¡ä»»ä¸ä¼ æ’­å€¾å‘ä¸Šä¼˜äºç°æœ‰å¤§æ¨¡å‹åŠä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ºAIé©±åŠ¨çš„è™šå‡ä¿¡æ¯æ²»ç†æä¾›äº†æ–°å·¥å…·ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Modeling%20Human%20Responses%20to%20Multimodal%20AI%20Content.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-GenOM-Ontology-Matching-with-Description-Generation-and-Large-Language-Model"><a href="#86-GenOM-Ontology-Matching-with-Description-Generation-and-Large-Language-Model" class="headerlink" title="86. GenOM: Ontology Matching with Description Generation and Large Language Model"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GenOM__Ontology_Matching_with_Description_Generation_and_Large_Language_Model.pdf">GenOM: Ontology Matching with Description Generation and Large Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The University of Manchester</span></p>
<p>æœ¬æ–‡æå‡ºäº†GenOMï¼Œä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æ–‡æœ¬å®šä¹‰ç”Ÿæˆçš„æœ¬ä½“å¯¹é½æ¡†æ¶ã€‚å…¶æ–¹æ³•æµç¨‹åŒ…æ‹¬ï¼šä»æœ¬ä½“ä¸­æå–ç»“æ„ä¸è¯æ±‡ä¿¡æ¯ï¼Œç”¨LLMç”ŸæˆåŒ…å«é¢†åŸŸçŸ¥è¯†çš„æ–‡æœ¬å®šä¹‰ï¼Œåˆ©ç”¨åµŒå…¥æ¨¡å‹è¿›è¡Œå€™é€‰æ£€ç´¢ï¼Œå†é€šè¿‡LLMåˆ¤åˆ«å€™é€‰å¯¹çš„è¯­ä¹‰ç­‰ä»·æ€§ï¼Œå¹¶ä¸ä¼ ç»Ÿç²¾ç¡®åŒ¹é…å·¥å…·èåˆæå‡ç²¾åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒGenOMåœ¨OAEI Bio-MLåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå¤šç§ä¼ ç»Ÿä¸ç°æœ‰LLMæ–¹æ¡ˆï¼Œä¸”æ¨¡å—åŒ–è®¾è®¡å’Œå®šä¹‰å¢å¼ºç­–ç•¥æœ‰æ•ˆæå‡äº†æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GenOM%3A%20Ontology%20Matching%20with%20Description%20Generation%20and%20Large%20Language%20Model.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-Learning-from-Natural-Language-Feedback-for-Personalized-Question-Answering"><a href="#87-Learning-from-Natural-Language-Feedback-for-Personalized-Question-Answering" class="headerlink" title="87. Learning from Natural Language Feedback for Personalized Question Answering"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Learning_from_Natural_Language_Feedback_for_Personalized_Question_Answering.pdf">Learning from Natural Language Feedback for Personalized Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Massachusetts Amherst</span></p>
<p>è¯¥è®ºæ–‡æå‡ºVACæ¡†æ¶ï¼Œç”¨è‡ªç„¶è¯­è¨€åé¦ˆï¼ˆNLFï¼‰æ›¿ä»£ä¼ ç»Ÿä¸ªæ€§åŒ–é—®ç­”ä¸­ç”¨äºè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„æ ‡é‡å¥–åŠ±ä¿¡å·ã€‚VACé€šè¿‡äº¤æ›¿ä¼˜åŒ–åé¦ˆæ¨¡å‹å’Œç­–ç•¥æ¨¡å‹ï¼Œåˆ©ç”¨ç”¨æˆ·ç”»åƒå’Œé—®é¢˜å™è¿°ç”Ÿæˆä¸ªæ€§åŒ–åé¦ˆï¼Œæœ‰æ•ˆæå‡æ¨¡å‹çš„ä¸ªæ€§åŒ–å“åº”èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒVACåœ¨LaMP-QAåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶è¢«äººå·¥è¯„ä»·ä¸ºæ›´ä¼˜ï¼Œè¡¨æ˜è‡ªç„¶è¯­è¨€åé¦ˆä¸ºä¸ªæ€§åŒ–é—®ç­”ä¼˜åŒ–æä¾›äº†æ›´æœ‰æ•ˆçš„ä¿¡å·ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning%20from%20Natural%20Language%20Feedback%20for%20Personalized%20Question%20Answering.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-FROGENT-An-End-to-End-Full-process-Drug-Design-Agent"><a href="#88-FROGENT-An-End-to-End-Full-process-Drug-Design-Agent" class="headerlink" title="88. FROGENT: An End-to-End Full-process Drug Design Agent"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/FROGENT__An_End-to-End_Full-process_Drug_Design_Agent.pdf">FROGENT: An End-to-End Full-process Drug Design Agent</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Artificial Intelligence, Shenzhen University</span></p>
<p>FROGENT is an agentic framework leveraging Large Language Models (LLMs) and the Model Context Protocol (MCP) to unify and automate the entire drug discovery workflow, integrating dynamic biochemical databases, extensible tool libraries, and specialized AI models for tasks such as target identification, molecule generation, and retrosynthetic planning. Experimental results show FROGENT significantly outperforms state-of-the-art LLM agents and commercial models on eight drug discovery benchmarks, tripling hit-finding and doubling interaction profiling scores, thus streamlining and accelerating drug research.</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FROGENT%3A%20An%20End-to-End%20Full-process%20Drug%20Design%20Agent.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-EgoCross-Benchmarking-Multimodal-Large-Language-Models-for-Cross-Domain-Egocentric-Video-Question-Answering"><a href="#89-EgoCross-Benchmarking-Multimodal-Large-Language-Models-for-Cross-Domain-Egocentric-Video-Question-Answering" class="headerlink" title="89. EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/EgoCross__Benchmarking_Multimodal_Large_Language_Models_for_Cross-Domain_Egocentric_Video_Question_A.pdf">EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">East China Normal University</span></p>
<p>æœ¬æ–‡æå‡ºäº†EgoCrossåŸºå‡†ï¼Œä¸“ä¸ºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è·¨é¢†åŸŸç¬¬ä¸€è§†è§’è§†é¢‘é—®ç­”ï¼ˆEgocentricQAï¼‰ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡æ¶µç›–å¤–ç§‘æ‰‹æœ¯ã€å·¥ä¸šã€æé™è¿åŠ¨å’ŒåŠ¨ç‰©è§†è§’å››å¤§é¢†åŸŸï¼Œå…±åŒ…å«çº¦1000å¯¹é—®ç­”ï¼Œæ”¯æŒè¯†åˆ«ã€å®šä½ã€é¢„æµ‹ä¸è®¡æ•°ç­‰å¤šç±»ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨CloseQAä¸OpenQAä¸¤ç§è¯„æµ‹æ–¹å¼ã€‚å®éªŒå‘ç°ï¼Œå½“å‰ä¸»æµMLLMsåœ¨éæ—¥å¸¸é¢†åŸŸè¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚è®ºæ–‡è¿˜æ¢ç´¢äº†Promptå­¦ä¹ ã€å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ç­‰æå‡æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºå¼ºåŒ–å­¦ä¹ å¯¹æå‡è·¨åŸŸæ³›åŒ–æ•ˆæœæœ€æ˜¾è‘—ã€‚ç»“è®ºè¡¨æ˜ï¼ŒEgoCrossä¸ºæœªæ¥æ›´å…·é²æ£’æ€§å’Œé€‚åº”æ€§çš„å¤šæ¨¡æ€å¤§æ¨¡å‹ç ”ç©¶æä¾›äº†åšå®åŸºç¡€ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/EgoCross%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Cross-Domain%20Egocentric%20Video%20Question%20Answering.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-Advancing-Autonomous-Incident-Response-Leveraging-LLMs-and-Cyber-Threat-Intelligence"><a href="#90-Advancing-Autonomous-Incident-Response-Leveraging-LLMs-and-Cyber-Threat-Intelligence" class="headerlink" title="90. Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Advancing_Autonomous_Incident_Response__Leveraging_LLMs_and_Cyber_Threat_Intelligence.pdf">Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">OODRIVE-Trusted Cloud Solutions</span></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨äº‹ä»¶å“åº”æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆNLPç›¸ä¼¼æ€§æ£€ç´¢å’Œæ ‡å‡†åŒ–CTIå¹³å°æŸ¥è¯¢ï¼Œå®ç°å¯¹å®‰å…¨è­¦æŠ¥çš„ä¸Šä¸‹æ–‡ä¸°å¯Œä¸å…³è”ï¼Œå¹¶ç”Ÿæˆç²¾ç¡®å¯æ“ä½œçš„åº”å¯¹ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æé«˜å“åº”å‡†ç¡®æ€§ã€æ•ˆç‡åŠå‡è½»åˆ†æå¸ˆå·¥ä½œè´Ÿæ‹…æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè‡ªåŠ¨è¯„ä¼°ä¸ä¸“å®¶éªŒè¯å‡è¯å®ç³»ç»Ÿçš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Advancing%20Autonomous%20Incident%20Response%3A%20Leveraging%20LLMs%20and%20Cyber%20Threat%20Intelligence.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-MSRS-Adaptive-Multi-Subspace-Representation-Steering-for-Attribute-Alignment-in-Large-Language-Models"><a href="#91-MSRS-Adaptive-Multi-Subspace-Representation-Steering-for-Attribute-Alignment-in-Large-Language-Models" class="headerlink" title="91. MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MSRS__Adaptive_Multi-Subspace_Representation_Steering_for_Attribute_Alignment_in_Large_Language_Mode.pdf">MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</span></p>
<p>æœ¬æ–‡æå‡ºäº†MSRSï¼ˆMulti-Subspace Representation Steeringï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªå±æ€§åˆ†é…æ­£äº¤å­ç©ºé—´å¹¶ç»“åˆå…±äº«å­ç©ºé—´ï¼Œå®ç°å¯¹å¤§è¯­è¨€æ¨¡å‹å¤šå±æ€§çš„é«˜æ•ˆç²¾å‡†æ§åˆ¶ã€‚æ–¹æ³•åˆ©ç”¨SVDè‡ªé€‚åº”åˆ†é…å­ç©ºé—´å®¹é‡ï¼Œå¹¶å¼•å…¥åŠ¨æ€Tokené€‰æ‹©ä¸å­ç©ºé—´åŠ æƒæœºåˆ¶ï¼Œæœ‰æ•ˆå‡å°‘å±æ€§é—´å¹²æ‰°ã€‚å®éªŒè¡¨æ˜ï¼ŒMSRSåœ¨çœŸå€¼æ€§ã€å…¬å¹³æ€§ã€æŒ‡ä»¤éµå¾ªå’Œæ‹’ç»ç­‰å¤šå±æ€§ä»»åŠ¡ä¸Šï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”ä¿æŒäº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MSRS%3A%20Adaptive%20Multi-Subspace%20Representation%20Steering%20for%20Attribute%20Alignment%20in%20Large%20Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-MCP2OSC-Parametric-Control-by-Natural-Language"><a href="#92-MCP2OSC-Parametric-Control-by-Natural-Language" class="headerlink" title="92. MCP2OSC: Parametric Control by Natural Language"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MCP2OSC__Parametric_Control_by_Natural_Language.pdf">MCP2OSC: Parametric Control by Natural Language</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Los Angeles, CA</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†MCP2OSCæœåŠ¡ç«¯å’Œä¸€å¥—é’ˆå¯¹OpenSoundControl (OSC) å‚æ•°æ§åˆ¶çš„è‡ªç„¶è¯­è¨€æç¤ºè®¾è®¡å‡†åˆ™ï¼Œé€šè¿‡ä¸Claudeç­‰å¤§è¯­è¨€æ¨¡å‹é›†æˆï¼Œå®ç°äº†ç”¨è‡ªç„¶è¯­è¨€ç”Ÿæˆã€è§£é‡Šã€è°ƒè¯•å’Œç®¡ç†OSCæ¶ˆæ¯ï¼Œæ¶µç›–æ‰¹é‡ä¸æµå¼æ§åˆ¶ã€æ—¥å¿—åˆ†æã€å¯è§†åŒ–ç­‰å¤šç§å®ç”¨åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—é™ä½äº†å¤æ‚å¤šåª’ä½“è®¾å¤‡ä¸åˆ›ä½œæµç¨‹ä¸­çš„OSCå¼€å‘éš¾åº¦ï¼Œæé«˜äº†äººæœºåä½œæ•ˆç‡ï¼Œå±•ç°äº†åŸºäºLLMçš„é€šç”¨æ§åˆ¶æœºåˆ¶æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MCP2OSC%3A%20Parametric%20Control%20by%20Natural%20Language.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-Facilitating-the-Adoption-of-Causal-Inference-Methods-Through-LLM-Empowered-Co-Pilot"><a href="#93-Facilitating-the-Adoption-of-Causal-Inference-Methods-Through-LLM-Empowered-Co-Pilot" class="headerlink" title="93. Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Technical_Report__Facilitating_the_Adoption_of_Causal_Inference_Methods_Through_LLM-Empowered_Co-Pil.pdf">Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Ataraxis.ai</span></p>
<p>æœ¬æ–‡æå‡ºäº†CATE-Bç³»ç»Ÿï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆå› æœå‘ç°ç®—æ³•ï¼Œå¸®åŠ©ç”¨æˆ·å®Œæˆç»“æ„å› æœæ¨¡å‹å»ºè®¾ã€ä¸ç¡®å®šæ€§æœ€å°è°ƒæ•´é›†ï¼ˆMUASï¼‰é€‰æ‹©å’Œå›å½’æ–¹æ³•æ¨èï¼Œå®ç°ä»è§‚æµ‹æ•°æ®åˆ°å¤„ç†æ•ˆåº”ä¼°è®¡çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒCATE-Båœ¨å¤šæ•°æ®é›†ä¸Šæé«˜äº†å› æœæ¨æ–­çš„å‡†ç¡®æ€§å’Œå¥å£®æ€§ï¼Œé™ä½äº†é¢†åŸŸä¸“å®¶é—¨æ§›ï¼Œä¿ƒè¿›äº†å› æœæ¨æ–­æ–¹æ³•çš„æ™®åŠä¸æ ‡å‡†åŒ–ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Facilitating%20the%20Adoption%20of%20Causal%20Inference%20Methods%20Through%20LLM-Empowered%20Co-Pilot.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-When-Language-Overrules-Revealing-Text-Dominance-in-Multimodal-Large-Language-Models"><a href="#94-When-Language-Overrules-Revealing-Text-Dominance-in-Multimodal-Large-Language-Models" class="headerlink" title="94. When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/When_Language_Overrules__Revealing_Text_Dominance_in_Multimodal_Large_Language_Models.pdf">When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute of Computing Technology, Chinese Academy of Sciences</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šç§æ¨¡æ€ï¼ˆå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ã€æ—¶é—´åºåˆ—å’Œå›¾ç»“æ„æ•°æ®ï¼‰ä¸Šçš„æ–‡æœ¬ä¼˜åŠ¿ç°è±¡ï¼Œæå‡ºäº†æ¨¡æ€ä¼˜åŠ¿æŒ‡æ•°ï¼ˆMDIï¼‰å’Œæ³¨æ„åŠ›æ•ˆç‡æŒ‡æ•°ï¼ˆAEIï¼‰ä½œä¸ºè¡¡é‡æŒ‡æ ‡ï¼Œå¹¶é€šè¿‡å®éªŒå‘ç°æ–‡æœ¬æ¨¡æ€åœ¨ç°æœ‰æ¨¡å‹ä¸­æ™®éå ä¸»å¯¼åœ°ä½ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†åŸºäº[CLS]æ³¨æ„åŠ›å¼•å¯¼çš„éæ–‡æœ¬tokenå‹ç¼©æ–¹æ³•ï¼Œæœ‰æ•ˆç¼“è§£äº†æ³¨æ„åŠ›åˆ†é…å¤±è¡¡å¹¶ä¿ƒè¿›æ›´å…¬å¹³çš„å¤šæ¨¡æ€èåˆï¼Œç»“è®ºä¸ºå‹ç¼©éæ–‡æœ¬tokenèƒ½æ˜¾è‘—æ”¹å–„å¤šæ¨¡æ€æ¨¡å‹çš„æ¨¡æ€å‡è¡¡æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/When%20Language%20Overrules%3A%20Revealing%20Text%20Dominance%20in%20Multimodal%20Large%20Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-A-Unified-Multi-Agent-Framework-for-Universal-Multimodal-Understanding-and-Generation"><a href="#95-A-Unified-Multi-Agent-Framework-for-Universal-Multimodal-Understanding-and-Generation" class="headerlink" title="95. A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Unified_Multi-Agent_Framework_for_Universal_Multimodal_Understanding_and_Generation.pdf">A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">State Key Laboratory of General Artificial Intelligence, BIGAI</span></p>
<p>æœ¬æ–‡æå‡ºMAGUSæ¡†æ¶ï¼Œé€šè¿‡è®¤çŸ¥ä¸æ¨ç†ï¼ˆCognitionï¼‰å’Œè¡¨è¾¾ä¸ç”Ÿæˆï¼ˆDeliberationï¼‰ä¸¤é˜¶æ®µåˆ†ç¦»ï¼Œé‡‡ç”¨å¤šæ™ºèƒ½ä½“åä½œæœºåˆ¶ï¼Œå°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸æ‰©æ•£æ¨¡å‹æœ‰æœºç»“åˆï¼Œå®ç°æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰ä»»æ„æ¨¡æ€çš„ç†è§£å’Œç”Ÿæˆã€‚æ ¸å¿ƒæŠ€æœ¯åŒ…æ‹¬Growth-Aware Searchï¼ˆGASï¼‰ç®—æ³•ï¼Œæ”¯æŒå¤šè½®æ™ºèƒ½ä½“å¯¹è¯ã€ç»“æ„åŒ–ä»»åŠ¡è§„åˆ’ã€åŠ¨æ€ä¼˜åŒ–ç”Ÿæˆè´¨é‡ï¼Œæ— éœ€è”åˆè®­ç»ƒå³å¯çµæ´»æ‰©å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAGUSåœ¨å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä»»åŠ¡çš„ä¸»æµåŸºå‡†ä¸Šå‡ä¼˜äºç°æœ‰å¼ºåŸºçº¿åŠé—­æºå¤§æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰ï¼Œå…·å¤‡é«˜å¯æ‰©å±•æ€§å’Œå¼ºæŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Unified%20Multi-Agent%20Framework%20for%20Universal%20Multimodal%20Understanding%20and%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-SEQ-GPT-LLM-assisted-Spatial-Query-via-Example"><a href="#96-SEQ-GPT-LLM-assisted-Spatial-Query-via-Example" class="headerlink" title="96. SEQ-GPT: LLM-assisted Spatial Query via Example"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SEQ-GPT__LLM-assisted_Spatial_Query_via_Example.pdf">SEQ-GPT: LLM-assisted Spatial Query via Example</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Nanyang Technological University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†SEQ-GPTç³»ç»Ÿï¼Œå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç©ºé—´ç¤ºä¾‹æŸ¥è¯¢ï¼ˆSEQï¼‰ç»“åˆï¼Œå®ç°äº†ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€å’Œå®ä¾‹å¯¹ç©ºé—´ä½ç½®è¿›è¡Œçµæ´»æ£€ç´¢ã€‚æ–¹æ³•åŒ…æ‹¬å¤šæ¨¡å‹åä½œçš„æ•°æ®è§£æä¸å¯¹è¯è°ƒåº¦ã€æ•°æ®åˆæˆä¸å¾®è°ƒï¼Œå¹¶æ”¯æŒå¤šè½®äº¤äº’å’Œåé¦ˆä¿®æ­£ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚ç©ºé—´æŸ¥è¯¢çš„æ˜“ç”¨æ€§å’Œçµæ´»æ€§ã€‚å®éªŒå±•ç¤ºäº†SEQ-GPTåœ¨çœŸå®åœºæ™¯ä¸‹å¯¹å¤šåœ°ç‚¹æ£€ç´¢ä»»åŠ¡çš„æœ‰æ•ˆæ€§å’Œç”¨æˆ·ä½“éªŒæå‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SEQ-GPT%3A%20LLM-assisted%20Spatial%20Query%20via%20Example.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs"><a href="#97-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs" class="headerlink" title="97. HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/HumanSense__From_Multimodal_Perception_to_Empathetic_Context-Aware_Responses_through_Reasoning_MLLMs.pdf">HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Xiâ€™an Jiaotong University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†HumanSenseåŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨äººç±»ä¸­å¿ƒåœºæ™¯ä¸‹çš„æ„ŸçŸ¥ã€ç†è§£ä¸äº’åŠ¨èƒ½åŠ›ï¼Œæ¶µç›–è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ã€‚é€šè¿‡æ„å»ºå¤šå±‚æ¬¡ä»»åŠ¡ä½“ç³»ã€æ¨¡æ€æ¶ˆèå®éªŒåŠå¤šé˜¶æ®µæ¨¡æ€é€’è¿›å¼å¼ºåŒ–å­¦ä¹ ï¼Œå‘ç°å¤šæ¨¡æ€æ¨ç†èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨å¤æ‚äººæœºäº¤äº’åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œå¹¶é€šè¿‡æç¤ºå·¥ç¨‹æ— è®­ç»ƒæå‡éæ¨ç†æ¨¡å‹æ•ˆæœï¼Œä¸ºæœªæ¥AIäº’åŠ¨ä½“éªŒæŒ‡æ˜æ–¹å‘ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/HumanSense%3A%20From%20Multimodal%20Perception%20to%20Empathetic%20Context-Aware%20Responses%20through%20Reasoning%20MLLMs.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-Improving-Value-based-Process-Verifier-via-Low-Cost-Variance-Reduction"><a href="#98-Improving-Value-based-Process-Verifier-via-Low-Cost-Variance-Reduction" class="headerlink" title="98. Improving Value-based Process Verifier via Low-Cost Variance Reduction"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Improving_Value-based_Process_Verifier_via_Low-Cost_Variance_Reduction.pdf">Improving Value-based Process Verifier via Low-Cost Variance Reduction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Harbin Institute of Technology (Shenzhen)</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½æˆæœ¬æ–¹å·®é™ä½æ–¹æ³•ComMCSï¼Œé€šè¿‡å¯¹å½“å‰å’Œåç»­æ­¥éª¤çš„Monte Carloä¼°è®¡çº¿æ€§ç»„åˆï¼Œä¼˜åŒ–äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„value-based process verifierè®­ç»ƒæ ‡ç­¾çš„æ–¹å·®ï¼Œä¸”æ— éœ€å¢åŠ é¢å¤–çš„LLMæ¨ç†æˆæœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒComMCSåœ¨MATH-500å’ŒGSM8KåŸºå‡†æµ‹è¯•ä¸Šç›¸è¾ƒäºä¼ ç»Ÿå›å½’å’Œåˆ†å¸ƒå»ºæ¨¡æ–¹æ³•å–å¾—äº†2-3ä¸ªç™¾åˆ†ç‚¹çš„å‡†ç¡®ç‡æå‡ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving%20Value-based%20Process%20Verifier%20via%20Low-Cost%20Variance%20Reduction.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-Reverse-Physician-AI-Relationship-Full-process-Clinical-Diagnosis-Driven-by-a-Large-Language-Model"><a href="#99-Reverse-Physician-AI-Relationship-Full-process-Clinical-Diagnosis-Driven-by-a-Large-Language-Model" class="headerlink" title="99. Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model"></a>99. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Reverse_Physician-AI_Relationship__Full-process_Clinical_Diagnosis_Driven_by_a_Large_Language_Model.pdf">Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">State Key Laboratory of AI Safety, Institute of Computing Technology, CAS</span></p>
<p>æœ¬æ–‡æå‡ºäº†DxDirector-7Bï¼Œä¸€ç§å…·å¤‡ç±»äººâ€œæ…¢æ€è€ƒâ€æ·±åº¦æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†ä»å«ç³Šä¸»è¯‰å‡ºå‘è‡ªåŠ¨é©±åŠ¨å…¨æµç¨‹ä¸´åºŠè¯Šæ–­ï¼Œä»…åœ¨å¿…è¦æ—¶è¯·æ±‚åŒ»ç”ŸååŠ©ã€‚é€šè¿‡æŒç»­åŒ»å­¦é¢„è®­ç»ƒã€å…¨æµç¨‹è¯Šæ–­æŒ‡ä»¤å¾®è°ƒå’Œæ­¥çº§ç­–ç•¥åå¥½ä¼˜åŒ–ï¼Œä½¿æ¨¡å‹èƒ½è‡ªä¸»å†³ç­–ã€åˆ†æ­¥æ¨ç†å¹¶é™„æƒå¨æ–‡çŒ®æ”¯æŒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDxDirector-7Båœ¨å¤æ‚ã€ç½•è§å’ŒçœŸå®ä¸´åºŠåœºæ™¯ä¸­è¯Šæ–­å‡†ç¡®ç‡æ˜¾è‘—ä¼˜äºå‚æ•°é‡å¤§æ•°åå€çš„ç°æœ‰åŒ»ç–—&#x2F;é€šç”¨LLMï¼Œå¹¶æå¤§å‡è½»åŒ»ç”Ÿå·¥ä½œé‡ï¼Œéƒ¨åˆ†ç§‘å®¤å¯æ›¿ä»£60%-75%çš„ä¸“ç§‘åŒ»ç”Ÿè¯Šæ–­å·¥ä½œï¼Œæ ‡å¿—ç€AIä¸»å¯¼è¯Šæ–­æ–°æ—¶ä»£çš„åˆ°æ¥ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Reverse%20Physician-AI%20Relationship%3A%20Full-process%20Clinical%20Diagnosis%20Driven%20by%20a%20Large%20Language%20Model.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-FIRESPARQL-A-LLM-based-Framework-for-SPARQL-Query-Generation-over-Scholarly-Knowledge-Graphs"><a href="#100-FIRESPARQL-A-LLM-based-Framework-for-SPARQL-Query-Generation-over-Scholarly-Knowledge-Graphs" class="headerlink" title="100. FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs"></a>100. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/FIRESPARQL__A_LLM-based_Framework_for_SPARQL_Query_Generation_over_Scholarly_Knowledge_Graphs.pdf">FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Vrije Universiteit Amsterdam</span></p>
<p>æœ¬æ–‡æå‡ºFIRESPARQLæ¡†æ¶ï¼Œé€šè¿‡å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé¢†åŸŸç‰¹å®šå¾®è°ƒï¼ˆLoRAï¼‰ã€å¼•å…¥å¯é€‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å—å’Œè½»é‡çº§SPARQLçº é”™å±‚ï¼Œä»¥æå‡å¯¹å­¦æœ¯çŸ¥è¯†å›¾è°±ï¼ˆSKGï¼‰ä¸­çš„è‡ªç„¶è¯­è¨€é—®é¢˜è½¬SPARQLæŸ¥è¯¢çš„å‡†ç¡®æ€§å’Œç»“æ„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„LLaMA-3-8B-Instructæ¨¡å‹åœ¨SciQAåŸºå‡†ä¸Šå–å¾—äº†æœ€ä¼˜è¡¨ç°ï¼Œæ˜¾è‘—æå‡äº†æŸ¥è¯¢å‡†ç¡®ç‡å’Œæ‰§è¡Œæ•ˆæœï¼ŒRAGæ¨¡å—åè€Œæœªå¸¦æ¥é¢å¤–æå‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FIRESPARQL%3A%20A%20LLM-based%20Framework%20for%20SPARQL%20Query%20Generation%20over%20Scholarly%20Knowledge%20Graphs.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="101-WE-MATH-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning"><a href="#101-WE-MATH-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning" class="headerlink" title="101. WE-MATH 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning"></a>101. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/We-Math_2.0__A_Versatile_MathBook_System_for_Incentivizing_Visual_Mathematical_Reasoning.pdf">WE-MATH 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">BUPT</span></p>
<p>WE-MATH 2.0æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿï¼Œèåˆäº†ç»“æ„åŒ–æ•°å­¦çŸ¥è¯†ä½“ç³»ã€æ¨¡å‹ä¸­å¿ƒåŒ–æ•°æ®éš¾åº¦å»ºæ¨¡ä»¥åŠä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒåŒ…æ‹¬å†·å¯åŠ¨å¾®è°ƒå’ŒåŠ¨æ€éš¾åº¦é€’å¢RLä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡MLLMåœ¨è§†è§‰æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æ³›åŒ–ä¸ç¨³å¥æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä¸»æµæ•°å­¦æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†é¢†å…ˆæ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤šæ­¥æ¨ç†å’ŒçŸ¥è¯†è¦†ç›–å¹¿åº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/WE-MATH%202.0%3A%20A%20Versatile%20MathBook%20System%20for%20Incentivizing%20Visual%20Mathematical%20Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="102-SC2Arena-and-StarEvolve-Benchmark-and-Self-Improvement-Framework-for-LLMs-in-Complex-Decision-Making-Tasks"><a href="#102-SC2Arena-and-StarEvolve-Benchmark-and-Self-Improvement-Framework-for-LLMs-in-Complex-Decision-Making-Tasks" class="headerlink" title="102. SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks"></a>102. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SC2Arena_and_StarEvolve__Benchmark_and_Self-Improvement_Framework_for_LLMs_in_Complex_Decision-Makin.pdf">SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute of Automation, Chinese Academy of Sciences</span></p>
<p>è¯¥è®ºæ–‡æå‡ºSC2Arenaï¼Œä½œä¸ºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨StarCraft IIå¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„åŸºå‡†ï¼Œæ”¯æŒå…¨ç¨‹æ¸¸æˆã€å¤šç§ç§æ—å’Œå®Œæ•´ä½çº§åŠ¨ä½œç©ºé—´ï¼ŒåŒæ—¶é€šè¿‡æ–‡æœ¬ä¼˜åŒ–è§£å†³ç©ºé—´æ¨ç†ä¸ä¿¡æ¯è¿‡è½½é—®é¢˜ã€‚é…å¥—çš„StarEvolveæ¡†æ¶é‡‡ç”¨åˆ†å±‚è§„åˆ’ä¸æ‰§è¡Œã€è¿­ä»£è‡ªæˆ‘çº é”™æœºåˆ¶ï¼Œå¹¶ç»“åˆé«˜è´¨é‡æ•°æ®çš„ç›‘ç£å¾®è°ƒï¼Œå®ç°LLMæ™ºèƒ½ä½“çš„æŒç»­è‡ªæˆ‘æå‡å’Œå¼ºå¯¹æ‰‹é€‚åº”èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SC2Arena%20and%20StarEvolve%3A%20Benchmark%20and%20Self-Improvement%20Framework%20for%20LLMs%20in%20Complex%20Decision-Making%20Tasks.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="103-AnalogSeeker-An-Open-source-Foundation-Language-Model-for-Analog-Circuit-Design"><a href="#103-AnalogSeeker-An-Open-source-Foundation-Language-Model-for-Analog-Circuit-Design" class="headerlink" title="103. AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design"></a>103. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/AnalogSeeker__An_Open-source_Foundation_Language_Model_for_Analog_Circuit_Design.pdf">AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Fudan University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºAnalogSeekerï¼Œä¸€ç§é¢å‘æ¨¡æ‹Ÿç”µè·¯è®¾è®¡çš„å¼€æºåŸºç¡€å¤§è¯­è¨€æ¨¡å‹ã€‚æ–¹æ³•ä¸Šï¼Œä½œè€…é€šè¿‡æ¨¡æ‹Ÿç”µè·¯é¢†åŸŸçŸ¥è¯†æ¡†æ¶ç³»ç»Ÿæ”¶é›†é«˜è´¨é‡æ•™æå¹¶æ¸…æ´—æˆæ–‡æœ¬è¯­æ–™ï¼Œé‡‡ç”¨å¤šæ™ºèƒ½ä½“æ¡†æ¶å°†æœªæ ‡æ³¨æ–‡æœ¬è’¸é¦ä¸ºç»†ç²’åº¦é—®ç­”ï¼ˆQTSAï¼‰å¯¹ï¼Œå½¢æˆå¯å¾®è°ƒæ•°æ®é›†ï¼Œå¹¶è®¾è®¡äº†é‚»åŸŸè‡ªçº¦æŸç›‘ç£å¾®è°ƒç®—æ³•ï¼ˆNSC-SFTï¼‰ï¼Œåœ¨Qwen2.5-32B-Instructæ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œè®­ç»ƒã€‚ç»“è®ºæ˜¾ç¤ºï¼ŒAnalogSeekeråœ¨AMSBench-TQAåŸºå‡†ä¸Šè·å¾—85.04%å‡†ç¡®ç‡ï¼Œæ¯”åŸå§‹æ¨¡å‹æå‡15.67ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”åœ¨è¿ç®—æ”¾å¤§å™¨è®¾è®¡ä»»åŠ¡ä¸­è¡¨ç°æœ‰æ•ˆï¼Œå…·å¤‡é¢†åŸŸçŸ¥è¯†æ³¨å…¥å’Œè®¾è®¡è¾…åŠ©æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AnalogSeeker%3A%20An%20Open-source%20Foundation%20Language%20Model%20for%20Analog%20Circuit%20Design.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="104-Layer-Wise-Perturbations-via-Sparse-Autoencoders-for-Adversarial-Text-Generation"><a href="#104-Layer-Wise-Perturbations-via-Sparse-Autoencoders-for-Adversarial-Text-Generation" class="headerlink" title="104. Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation"></a>104. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Layer-Wise_Perturbations_via_Sparse_Autoencoders_for_Adversarial_Text_Generation.pdf">Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">hydrox.ai</span></p>
<p>è¯¥è®ºæ–‡æå‡ºSparse Feature Perturbation Framework (SFPF)ï¼Œåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„éšè—å±‚æ¿€æ´»è¿›è¡Œé‡æ„å’Œç‰¹å¾èšç±»ï¼Œè¯†åˆ«å¹¶æ‰°åŠ¨ä¸æ”»å‡»ç›¸å…³çš„å…³é”®ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆå¯ç»•è¿‡å®‰å…¨æœºåˆ¶çš„å¯¹æŠ—æ€§æ–‡æœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒSFPFç”Ÿæˆçš„å¯¹æŠ—æ€§æ–‡æœ¬èƒ½æ˜¾è‘—æå‡æ”»å‡»æˆåŠŸç‡å¹¶ç»´æŒæ–‡æœ¬è´¨é‡ï¼Œä½†æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹ç»“æ„å’Œå±‚æ¬¡ä¸Šçš„æ³›åŒ–æ€§å°šå¾…éªŒè¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Layer-Wise%20Perturbations%20via%20Sparse%20Autoencoders%20for%20Adversarial%20Text%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="105-LeanRAG-Knowledge-Graph-Based-Generation-with-Semantic-Aggregation-and-Hierarchical-Retrieval"><a href="#105-LeanRAG-Knowledge-Graph-Based-Generation-with-Semantic-Aggregation-and-Hierarchical-Retrieval" class="headerlink" title="105. LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval"></a>105. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/LeanRAG__Knowledge-Graph-Based_Generation_with_Semantic_Aggregation_and_Hierarchical_Retrieval.pdf">LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Artificial Intelligence Laboratory</span></p>
<p>LeanRAGæå‡ºäº†ä¸€ç§ç»“åˆçŸ¥è¯†å›¾è°±è¯­ä¹‰èšåˆä¸å±‚æ¬¡åŒ–æ£€ç´¢çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–°æ¡†æ¶ã€‚æ–¹æ³•ä¸Šï¼ŒLeanRAGé¦–å…ˆé€šè¿‡é«˜æ–¯æ··åˆèšç±»å’Œå¤§æ¨¡å‹ç”Ÿæˆï¼Œæ„å»ºå¤šå±‚æ¬¡ã€å…·å¤‡æ˜¾å¼å®ä½“é—´å…³ç³»çš„å¯å¯¼èˆªè¯­ä¹‰ç½‘ç»œï¼Œå†åˆ©ç”¨è‡ªåº•å‘ä¸Šçš„LCAç»“æ„åŒ–æ£€ç´¢ï¼Œæœ‰æ•ˆå®šä½ç›¸å…³ç»†ç²’åº¦å®ä½“å¹¶èšåˆè‡³é«˜å±‚æ¬¡è¯­ä¹‰ï¼Œæå‡ä¸Šä¸‹æ–‡ç›¸å…³æ€§å¹¶å¤§å¹…å‡å°‘ä¿¡æ¯å†—ä½™ã€‚ç»“è®ºæ˜¾ç¤ºï¼ŒLeanRAGåœ¨å››ä¸ªQAåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ£€ç´¢å†—ä½™é™ä½46%ï¼Œä¸”æ˜¾å¼è¯­ä¹‰å…³ç³»ä¸åŸå§‹æ–‡æœ¬å¹¶ç”¨å¯¹ç”Ÿæˆé«˜è´¨é‡ç­”æ¡ˆè‡³å…³é‡è¦ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/LeanRAG%3A%20Knowledge-Graph-Based%20Generation%20with%20Semantic%20Aggregation%20and%20Hierarchical%20Retrieval.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="106-Contrast-Sensitivity-Function-of-Multimodal-Vision-Language-Models"><a href="#106-Contrast-Sensitivity-Function-of-Multimodal-Vision-Language-Models" class="headerlink" title="106. Contrast Sensitivity Function of Multimodal Vision-Language Models"></a>106. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Contrast_Sensitivity_Function_of_Multimodal_Vision-Language_Models.pdf">Contrast Sensitivity Function of Multimodal Vision-Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Universidad de Valencia</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¿ƒç†ç‰©ç†å­¦å¯å‘çš„è¡Œä¸ºè¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥å‘å¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆå¦‚LLaVA-1.5-7Bã€Blip2-7Bç­‰ï¼‰æé—®æ¥æµ‹é‡å…¶å¯¹ä¸åŒç©ºé—´é¢‘ç‡å’Œå¯¹æ¯”åº¦å›¾åƒçš„æ•æ„Ÿæ€§ï¼Œæ‹Ÿåˆæ¨¡å‹çš„å¯¹æ¯”æ•æ„Ÿå‡½æ•°ï¼ˆCSFï¼‰ï¼Œå¹¶ä¸äººç±»è§†è§‰CSFè¿›è¡Œå¯¹æ¯”ã€‚ç»“æœæ˜¾ç¤ºï¼Œéƒ¨åˆ†æ¨¡å‹åœ¨CSFå½¢çŠ¶æˆ–ç»å¯¹å€¼ä¸Šä¸äººç±»ç›¸è¿‘ï¼Œä½†å‡æ— æ³•åŒæ—¶å¤ç°äººç±»CSFçš„å½¢çŠ¶ä¸ç¨³å®šæ€§ï¼Œä¸”å¯¹æç¤ºè¯å˜åŒ–æ•æ„Ÿï¼Œæç¤ºå†…éƒ¨ä¸€è‡´æ€§å’Œé²æ£’æ€§æœ‰å¾…æå‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Contrast%20Sensitivity%20Function%20of%20Multimodal%20Vision-Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="107-What-to-Ask-Next-Probing-the-Imaginative-Reasoning-of-LLMs-with-TurtleSoup-Puzzles"><a href="#107-What-to-Ask-Next-Probing-the-Imaginative-Reasoning-of-LLMs-with-TurtleSoup-Puzzles" class="headerlink" title="107. What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles"></a>107. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/What_to_Ask_Next__Probing_the_Imaginative_Reasoning_of_LLMs_with_TurtleSoup_Puzzles.pdf">What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Huazhong University of Science and Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†TurtleSoup-Benchï¼Œä¸€ä¸ªå¤§è§„æ¨¡ã€åŒè¯­ã€äº¤äº’å¼çš„æƒ³è±¡æ€§æ¨ç†åŸºå‡†ï¼Œç»“åˆTurtleSoupè°œé¢˜ã€Mosaic-Agentæ™ºèƒ½ä½“å’Œå¤šç»´è‡ªåŠ¨è¯„æµ‹åè®®ï¼Œä»¥ç³»ç»Ÿè¯„ä¼°å¤§æ¨¡å‹åœ¨ä¿¡æ¯ç¨€ç¼ºç¯å¢ƒä¸‹çš„å‡è®¾æ„å»ºã€æµ‹è¯•ä¸ä¿®æ­£èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰ä¸»æµå¤§è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€æ¨ç†ä¸åˆ›é€ æ€§å‡è®¾ç”Ÿæˆæ–¹é¢ä»ä¸äººç±»æœ‰æ˜æ˜¾å·®è·ï¼ŒéªŒè¯äº†è¯¥è¯„æµ‹æ¡†æ¶å¯¹æ¨¡å‹æ¨ç†è¿‡ç¨‹è¯Šæ–­çš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/What%20to%20Ask%20Next%3F%20Probing%20the%20Imaginative%20Reasoning%20of%20LLMs%20with%20TurtleSoup%20Puzzles.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="108-ReviewRL-Towards-Automated-Scientific-Review-with-RL"><a href="#108-ReviewRL-Towards-Automated-Scientific-Review-with-RL" class="headerlink" title="108. ReviewRL: Towards Automated Scientific Review with RL"></a>108. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ReviewRL__Towards_Automated_Scientific_Review_with_RL.pdf">ReviewRL: Towards Automated Scientific Review with RL</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tsinghua University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºReviewRLæ¡†æ¶ï¼Œé€šè¿‡ArXiv-MCPæ£€ç´¢å¢å¼ºå‹ä¸Šä¸‹æ–‡ç”Ÿæˆã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ã€äº‹å®å‡†ç¡®çš„å­¦æœ¯è®ºæ–‡è¯„å®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReviewRLåœ¨ICLR 2025è®ºæ–‡è¯„æµ‹ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†è¯„å®¡çš„è´¨é‡å’Œè¯„åˆ†ä¸€è‡´æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ReviewRL%3A%20Towards%20Automated%20Scientific%20Review%20with%20RL.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="109-Yet-another-algorithmic-bias-A-Discursive-Analysis-of-Large-Language-Models-Reinforcing-Dominant-Discourses-on-Gender-and-Race"><a href="#109-Yet-another-algorithmic-bias-A-Discursive-Analysis-of-Large-Language-Models-Reinforcing-Dominant-Discourses-on-Gender-and-Race" class="headerlink" title="109. Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race"></a>109. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Yet_another_algorithmic_bias__A_Discursive_Analysis_of_Large_Language_Models_Reinforcing_Dominant_Di.pdf">Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Instituto de Estudos da Linguagem (IEL), Universidade Estadual de Campinas (UNICAMP)</span></p>
<p>æœ¬è®ºæ–‡æå‡ºä»¥æ‰¹åˆ¤æ€§è¯è¯­åˆ†æä¸ºæ ¸å¿ƒçš„å®šæ€§åˆ†ææ¡†æ¶ï¼Œå¯¹ä¸»æµLLMsï¼ˆåŒ…æ‹¬GPT-4ã€LLaMaã€SabiÃ¡ç­‰ï¼‰ç”Ÿæˆçš„å…³äºä¸åŒæ€§åˆ«ä¸ç§æ—å¥³æ€§çš„çŸ­ç¯‡æ•…äº‹è¿›è¡Œæ·±å…¥å‰–æã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›LLMsåœ¨æ–‡æœ¬è¾“å‡ºä¸­ç³»ç»Ÿæ€§åœ°å¼ºåŒ–äº†è¥¿æ–¹ä¸»æµå…³äºé»‘äººå¥³æ€§çš„æŠ—äº‰ã€ç¥–å…ˆå’Œç¾¤ä½“è´£ä»»ç­‰åˆ»æ¿å°è±¡ï¼Œè€Œå¯¹ç™½äººå¥³æ€§åˆ™æ›´å¤šå¼ºè°ƒè‡ªæˆ‘å‘ç°å’Œä¸ªä½“æˆé•¿ã€‚å³ä½¿åœ¨è¢«æç¤ºä¿®æ­£åè§æ—¶ï¼Œæ¨¡å‹çš„ä¿®æ­£ä¹Ÿæµäºè¡¨é¢ï¼Œéš¾ä»¥æ¶ˆé™¤æ·±å±‚çš„ç»“æ„æ€§æ­§è§†ã€‚è¿™åæ˜ å‡ºLLMsåœ¨è¯­ä¹‰ä¸ç¤¾ä¼šå±‚é¢çš„ç†è§£ä¸åˆ›æ–°èƒ½åŠ›æœ‰é™ï¼Œç°æœ‰æ¨¡å‹éš¾ä»¥è‡ªå‘çªç ´è®­ç»ƒè¯­æ–™ä¸­å›ºæœ‰çš„æ„è¯†å½¢æ€æ¡†æ¶ã€‚è®ºæ–‡å¼ºè°ƒï¼Œè¯è¯­å±‚é¢çš„å®šæ€§ç ”ç©¶å¯¹äºå‘ç°å’Œå¹²é¢„å¤§æ¨¡å‹è¾“å‡ºä¸­çš„éšæ€§åè§è‡³å…³é‡è¦ï¼Œå‘¼åAIå¼€å‘éœ€å¼•å…¥è·¨å­¦ç§‘å’Œå¤šå…ƒä¼¦ç†è§†è§’ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Yet%20another%20algorithmic%20bias%3A%20A%20Discursive%20Analysis%20of%20Large%20Language%20Models%20Reinforcing%20Dominant%20Discourses%20on%20Gender%20and%20Race.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="110-Promoting-Efficient-Reasoning-with-Verifiable-Stepwise-Reward"><a href="#110-Promoting-Efficient-Reasoning-with-Verifiable-Stepwise-Reward" class="headerlink" title="110. Promoting Efficient Reasoning with Verifiable Stepwise Reward"></a>110. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Promoting_Efficient_Reasoning_with_Verifiable_Stepwise_Reward.pdf">Promoting Efficient Reasoning with Verifiable Stepwise Reward</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Meituan</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¯éªŒè¯çš„é€æ­¥å¥–åŠ±æœºåˆ¶ï¼ˆVSRMï¼‰ï¼Œé€šè¿‡è§„åˆ™åˆ†å‰²å’Œå¥–åŠ±ä¼ æ’­ï¼Œå¯¹å¤§è¯­è¨€æ¨ç†æ¨¡å‹çš„ä¸­é—´æ¨ç†æ­¥éª¤è¿›è¡Œå¥–åŠ±æˆ–æƒ©ç½šï¼Œé¼“åŠ±æœ‰æ•ˆæ­¥éª¤æŠ‘åˆ¶æ— æ•ˆå†—ä½™ï¼Œæ˜¾è‘—å‡å°‘è¾“å‡ºé•¿åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVSRMåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šæœ‰æ•ˆç¼“è§£æ¨¡å‹è¿‡åº¦æ¨ç†é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒç”šè‡³ç•¥å¾®æå‡åŸæœ‰æ¨ç†èƒ½åŠ›ï¼Œå®ç°é«˜æ•ˆå‡†ç¡®çš„æ¨ç†ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Promoting%20Efficient%20Reasoning%20with%20Verifiable%20Stepwise%20Reward.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="111-Why-Cannot-Large-Language-Models-Ever-Make-True-Correct-Reasoning"><a href="#111-Why-Cannot-Large-Language-Models-Ever-Make-True-Correct-Reasoning" class="headerlink" title="111. Why Cannot Large Language Models Ever Make True Correct Reasoning?"></a>111. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Why_Cannot_Large_Language_Models_Ever_Make_True_Correct_Reasoning_.pdf">Why Cannot Large Language Models Ever Make True Correct Reasoning?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Saitama University</span></p>
<p>æœ¬æ–‡ä»é€»è¾‘å­¦å’Œæ¨ç†æ­£ç¡®æ€§çš„è§’åº¦ï¼Œç³»ç»Ÿåˆ†æäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰€è°“â€œæ¨ç†èƒ½åŠ›â€çš„æœ¬è´¨ï¼ŒæŒ‡å‡ºLLMä»…èƒ½æ¨¡æ‹Ÿæ¨ç†å½¢å¼ä½†æ— æ³•å®ç°çœŸæ­£çš„æ­£ç¡®æ¨ç†ã€‚ä½œè€…é€šè¿‡å®šä¹‰çœŸæ­£æ­£ç¡®æ¨ç†çš„ä¸‰å¤§é€»è¾‘åŸºç¡€ï¼Œé˜è¿°äº†LLMç”±äºå…¶åŸºäºæ¦‚ç‡ä¸ç»Ÿè®¡çš„ç”ŸæˆåŸç†ï¼Œæ— æ³•å†…åµŒé€»è¾‘æœ‰æ•ˆæ€§è¯„ä»·æ ‡å‡†å’ŒåŠ¨æ€è¯„ä¼°æœºåˆ¶ï¼Œå› æ­¤æ°¸è¿œæ— æ³•è·å¾—100%æ­£ç¡®çš„æ¨ç†èƒ½åŠ›ã€‚ç»“è®ºè®¤ä¸ºï¼ŒLLMåªèƒ½ç»™å‡ºè¡¨é¢ä¸Šä¼¼ä¹åˆç†çš„æ¨ç†ç»“æœï¼Œè¿½æ±‚å…¶çœŸæ­£æ¨ç†èƒ½åŠ›è€Œå¿½è§†æ­£ç¡®æ€§æ ‡å‡†æ˜¯é”™è¯¯ä¸”æ— æœ›çš„æ–¹å‘ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Why%20Cannot%20Large%20Language%20Models%20Ever%20Make%20True%20Correct%20Reasoning%3F.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="112-Flexible-Personalized-Split-Federated-Learning-for-On-Device-Fine-Tuning-of-Foundation-Models"><a href="#112-Flexible-Personalized-Split-Federated-Learning-for-On-Device-Fine-Tuning-of-Foundation-Models" class="headerlink" title="112. Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models"></a>112. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Flexible_Personalized_Split_Federated_Learning_for_On-Device_Fine-Tuning_of_Foundation_Models.pdf">Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Duke Kunshan University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºFlexP-SFLæ¡†æ¶ï¼Œç»“åˆSplit Federated Learningå’Œä¸ªæ€§åŒ–æœºåˆ¶ï¼Œå®ç°åŸºç¡€æ¨¡å‹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„ä¸ªæ€§åŒ–é«˜æ•ˆå¾®è°ƒã€‚æ–¹æ³•å…è®¸æ¯ä¸ªå®¢æˆ·ç«¯æŒ‰è‡ªèº«ç®—åŠ›çµæ´»åˆ†é…æœ¬åœ°è®­ç»ƒå±‚ï¼ŒæœåŠ¡å™¨å¤„ç†å…±äº«éƒ¨åˆ†ï¼Œå¹¶é€šè¿‡KLå¯¹é½ç­–ç•¥å¹³è¡¡ä¸ªæ€§åŒ–ä¸æ³›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¯­è¨€æ¨¡å‹ä¸Šæ˜¾è‘—æå‡ä¸ªæ€§åŒ–å‡†ç¡®ç‡ï¼Œå‡å°‘é€šä¿¡ä¸è®­ç»ƒæ—¶é—´ï¼Œæ”¯æŒå¼‚æ„è®¾å¤‡é«˜æ•ˆåä½œã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Flexible%20Personalized%20Split%20Federated%20Learning%20for%20On-Device%20Fine-Tuning%20of%20Foundation%20Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="113-MRFD-Multi-Region-Fusion-Decoding-with-Self-Consistency-for-Mitigating-Hallucinations-in-LVLMs"><a href="#113-MRFD-Multi-Region-Fusion-Decoding-with-Self-Consistency-for-Mitigating-Hallucinations-in-LVLMs" class="headerlink" title="113. MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs"></a>113. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MRFD__Multi-Region_Fusion_Decoding_with_Self-Consistency_for_Mitigating_Hallucinations_in_LVLMs.pdf">MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of California, Merced</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§è®­ç»ƒæ— å…³çš„è§£ç æ–¹æ³•MRFDï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶é€‰å–å›¾åƒå¤šä¸ªæ˜¾è‘—åŒºåŸŸï¼Œåˆ†åˆ«ç”ŸæˆåŒºåŸŸå“åº”ï¼Œå¹¶åˆ©ç”¨Jensen-Shannon Divergenceè¯„ä¼°åŒºåŸŸé—´è‡ªæ´½æ€§ï¼Œè·å¾—å¯é æ€§æƒé‡åèåˆå„åŒºåŸŸé¢„æµ‹ã€‚æœ€ç»ˆé‡‡ç”¨é“¾å¼æ€ç»´å¯å‘çš„åŒºåŸŸæç¤ºï¼Œæå‡LVLMså¯¹å›¾åƒäº‹å®çš„æ•æ‰ä¸å“åº”çš„çœŸå®åº¦ã€‚å®éªŒè¡¨æ˜MRFDèƒ½åœ¨å¤šä¸ªLVLMå’Œå…¬å¼€åŸºå‡†ä¸Šæ˜¾è‘—é™ä½å¹»è§‰ç°è±¡ï¼Œæå‡å“åº”çš„äº‹å®æ€§ï¼Œæ— éœ€æ¨¡å‹å‚æ•°æ›´æ–°ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MRFD%3A%20Multi-Region%20Fusion%20Decoding%20with%20Self-Consistency%20for%20Mitigating%20Hallucinations%20in%20LVLMs.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="114-Generalizable-Federated-Learning-using-Client-Adaptive-Focal-Modulation"><a href="#114-Generalizable-Federated-Learning-using-Client-Adaptive-Focal-Modulation" class="headerlink" title="114. Generalizable Federated Learning using Client Adaptive Focal Modulation"></a>114. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Generalizable_Federated_Learning_using_Client_Adaptive_Focal_Modulation.pdf">Generalizable Federated Learning using Client Adaptive Focal Modulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">MBZUAI</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†AdaptFEDï¼Œä¸€ç§åœ¨è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰åœºæ™¯ä¸‹åˆ©ç”¨ä¸­å¿ƒåŒ–å¯å­¦ä¹ ç”Ÿæˆå™¨å’Œä»»åŠ¡æ„ŸçŸ¥å®¢æˆ·ç«¯åµŒå…¥ï¼Œå®ç°ç„¦ç‚¹è°ƒåˆ¶å±‚ä¸ªæ€§åŒ–çš„Transformeræ¡†æ¶ã€‚æ–¹æ³•é€šè¿‡ä½ç§©è¶…ç½‘ç»œæ¡ä»¶åŒ–å’Œé«˜æ•ˆçš„å‚æ•°èšåˆï¼Œæå‡äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€ã€éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆNon-IIDï¼‰åŠæºæ•°æ®ä¸å¯ç”¨ï¼ˆSource-Free Domain Adaptationï¼‰ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒAdaptFEDåœ¨è§†è§‰ã€æ—¶é—´åºåˆ—å’Œå¤šè¯­è¨€ä»»åŠ¡ä¸Šå‡ä¼˜äºå½“å‰ä¸»æµæ–¹æ³•ï¼Œç†è®ºä¸Šä¹Ÿç»™å‡ºäº†æ›´ç´§çš„ä¸ªæ€§åŒ–æ³›åŒ–ç•Œã€‚ç»“è®ºï¼šAdaptFEDæå‡äº†ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ çš„æ•ˆæœä¸æ‰©å±•æ€§ï¼Œé€‚ç”¨äºå¤šé¢†åŸŸå’Œèµ„æºå—é™åœºæ™¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Generalizable%20Federated%20Learning%20using%20Client%20Adaptive%20Focal%20Modulation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="115-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms"><a href="#115-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms" class="headerlink" title="115. From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms"></a>115. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/From_Black_Box_to_Transparency__Enhancing_Automated_Interpreting_Assessment_with_Explainable_AI_in_C.pdf">From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Jiao Tong University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆç‰¹å¾å·¥ç¨‹ã€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ•°æ®å¢å¼ºå’Œå¯è§£é‡Šæœºå™¨å­¦ä¹ ï¼ˆå¦‚SHAPåˆ†æï¼‰çš„å¤šç»´è‡ªåŠ¨å£è¯‘è´¨é‡è¯„ä»·æ¡†æ¶ã€‚æ–¹æ³•åœ¨è‹±ä¸­å£è¯‘æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œé‡ç‚¹æå‡å¯¹è¯­è¨€ä½¿ç”¨ã€æµç•…åº¦å’Œä¿¡æ¯å®Œæ•´æ€§çš„å¯è§£é‡Šé¢„æµ‹èƒ½åŠ›ï¼Œç»“æœæ˜¾ç¤ºVAEå¢å¼ºæ˜¾è‘—æå‡æ¨¡å‹è¡¨ç°ï¼Œä¸”å„ç»´åº¦çš„å…³é”®ç‰¹å¾å‡è¢«ç²¾ç¡®è¯†åˆ«ï¼Œä¸ºæ•™å­¦åé¦ˆå’Œè‡ªæˆ‘è°ƒèŠ‚å­¦ä¹ æä¾›äº†é€æ˜ã€å¯æ‰©å±•çš„è‡ªåŠ¨è¯„åˆ†å·¥å…·ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From%20Black%20Box%20to%20Transparency%3A%20Enhancing%20Automated%20Interpreting%20Assessment%20with%20Explainable%20AI%20in%20College%20Classrooms.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="116-Who-Benefits-from-AI-Explanations-Towards-Accessible-and-Interpretable-Systems"><a href="#116-Who-Benefits-from-AI-Explanations-Towards-Accessible-and-Interpretable-Systems" class="headerlink" title="116. Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems"></a>116. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Who_Benefits_from_AI_Explanations__Towards_Accessible_and_Interpretable_Systems.pdf">Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Ontario Tech University</span></p>
<p>æœ¬æ–‡é‡‡ç”¨æ–‡çŒ®ç»¼è¿°å’Œå››æ­¥æ³•æ¡ˆä¾‹ç ”ç©¶ï¼Œç³»ç»Ÿè¯„ä¼°ç°æœ‰å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æŠ€æœ¯åœ¨æ— éšœç¢é¢†åŸŸçš„ç¼ºå£ï¼Œå°¤å…¶å…³æ³¨è§†è§‰éšœç¢ç”¨æˆ·ã€‚ç ”ç©¶è®¾è®¡å¹¶æµ‹è¯•äº†é¢å‘ç›²äººäº¤é€šç®¡ç†è€…çš„XAIåŸå‹ï¼Œé‡‡ç”¨LIMEå’ŒSHAPè§£é‡Šæ–¹æ³•ï¼Œç»“æœå‘ç°ç®€åŒ–ã€å¤šæ¨¡æ€è§£é‡Šæ›´æ˜“äºéè§†è§‰ç”¨æˆ·ç†è§£ï¼Œå¼ºè°ƒéœ€ä¸ºä¸åŒèƒ½åŠ›ç”¨æˆ·è®¾è®¡å¤šæ ·åŒ–è§£é‡Šæ–¹å¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Who%20Benefits%20from%20AI%20Explanations%3F%20Towards%20Accessible%20and%20Interpretable%20Systems.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="117-Insights-from-the-Algonauts-2025-Winners"><a href="#117-Insights-from-the-Algonauts-2025-Winners" class="headerlink" title="117. Insights from the Algonauts 2025 Winners"></a>117. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Insights_from_the_Algonauts_2025_Winners.pdf">Insights from the Algonauts 2025 Winners</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Medical AI Research Center (MedARC)</span></p>
<p>æœ¬è®ºæ–‡æ€»ç»“äº†Algonauts 2025ç«èµ›ä¸­çš„é¡¶å°–å›¢é˜Ÿæ–¹æ³•ï¼Œä¸»è¦é‡‡ç”¨é¢„è®­ç»ƒå¤šæ¨¡æ€ç‰¹å¾æå–å™¨ï¼ˆè§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ï¼‰ï¼Œç»“åˆtransformerã€RNNæˆ–çº¿æ€§å·ç§¯æ¶æ„ï¼Œé€šè¿‡æ¨¡å‹é›†æˆé¢„æµ‹è‡ªç„¶ç”µå½±åˆºæ¿€ä¸‹çš„äººç±»è„‘åŒºfMRIååº”ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ¶æ„å¤æ‚åº¦å½±å“æœ‰é™ï¼Œæ¨¡å‹é›†æˆä¸å¤šæ¨¡æ€ç‰¹å¾èåˆæ˜¯æå‡è„‘ç¼–ç å‡†ç¡®æ€§çš„å…³é”®ï¼Œæœªæ¥çªç ´éœ€è·³å‡ºç°æœ‰èŒƒå¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Insights%20from%20the%20Algonauts%202025%20Winners.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="118-APFL-Analytic-Personalized-Federated-Learning-via-Dual-Stream-Least-Squares"><a href="#118-APFL-Analytic-Personalized-Federated-Learning-via-Dual-Stream-Least-Squares" class="headerlink" title="118. APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares"></a>118. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/APFL__Analytic_Personalized_Federated_Learning_via_Dual-Stream_Least_Squares.pdf">APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">CSU, China</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†APFLæ–¹æ³•ï¼Œåˆ©ç”¨å†»ç»“çš„åŸºç¡€æ¨¡å‹ï¼ˆå¦‚ViTï¼‰è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶é€šè¿‡åŒæµè§£ææ¨¡å‹ï¼ˆä¸»æµç”¨äºå…¨å±€æ³›åŒ–ï¼Œç»†åŒ–æµç”¨äºæœ¬åœ°ä¸ªæ€§åŒ–ï¼‰å®ç°ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ ï¼Œé‡‡ç”¨è§£ææœ€å°äºŒä¹˜æ³•é¿å…æ¢¯åº¦æ›´æ–°å¯¹éIIDæ•°æ®åˆ†å¸ƒçš„æ•æ„Ÿæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºAPFLåœ¨å¤šä¸ªæ•°æ®é›†å’Œä¸åŒéIIDç¨‹åº¦ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç†è®ºåˆ†æè¯æ˜å…¶å…·å¤‡å¼‚è´¨æ€§ä¸å˜æ€§ã€éšç§ä¿æŠ¤åŠé«˜æ•ˆç‡ï¼Œç»“è®ºä¸ºAPFLæœ‰æ•ˆæå‡ä¸ªæ€§åŒ–æ¨¡å‹æ€§èƒ½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/APFL%3A%20Analytic%20Personalized%20Federated%20Learning%20via%20Dual-Stream%20Least%20Squares.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="119-FreeGAD-A-Training-Free-yet-Effective-Approach-for-Graph-Anomaly-Detection"><a href="#119-FreeGAD-A-Training-Free-yet-Effective-Approach-for-Graph-Anomaly-Detection" class="headerlink" title="119. FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection"></a>119. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/FreeGAD__A_Training-Free_yet_Effective_Approach_for_Graph_Anomaly_Detection.pdf">FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Guangxi University</span></p>
<p>æœ¬æ–‡æå‡ºFreeGADï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„å›¾å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œé‡‡ç”¨äº²å’ŒåŠ›é—¨æ§æ®‹å·®ç¼–ç å™¨ç”Ÿæˆå¼‚å¸¸æ„ŸçŸ¥è¡¨ç¤ºï¼Œå¹¶é€šè¿‡é”šç‚¹èŠ‚ç‚¹ç­›é€‰å’Œå¼•å¯¼ç»Ÿè®¡åå·®è®¡ç®—å¼‚å¸¸åˆ†æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFreeGADåœ¨å¤šä¸ªçœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šå®ç°äº†ä¼˜è¶Šçš„æ£€æµ‹æ€§èƒ½ã€æé«˜æ•ˆç‡å’Œè‰¯å¥½æ‰©å±•æ€§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FreeGAD%3A%20A%20Training-Free%20yet%20Effective%20Approach%20for%20Graph%20Anomaly%20Detection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="120-Deep-Learning-in-Classical-and-Quantum-Physics"><a href="#120-Deep-Learning-in-Classical-and-Quantum-Physics" class="headerlink" title="120. Deep Learning in Classical and Quantum Physics"></a>120. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Deep_Learning_in_Classical_and_Quantum_Physics.pdf">Deep Learning in Classical and Quantum Physics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">ICFO â€“ Institut de CiÃ¨ncies FotÃ²niques</span></p>
<p>æœ¬è®ºæ–‡ç³»ç»Ÿæ¢³ç†äº†æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰å’Œæœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼‰çš„åŸºç¡€ç†è®ºã€å…³é”®ç®—æ³•ï¼ˆå¦‚ç¥ç»ç½‘ç»œã€è‡ªåŠ¨å¾®åˆ†ã€æŸå¤±å‡½æ•°è®¾è®¡ç­‰ï¼‰ï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†å…¶åœ¨ç»å…¸ç‰©ç†å’Œé‡å­ç‰©ç†ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬å˜åˆ†é‡å­æœ¬å¾æ±‚è§£å™¨ï¼ˆVQEï¼‰ã€ç¥ç»ç½‘ç»œé‡å­æ€ï¼ˆNQSï¼‰ã€é‡å­æ€å±‚æç­‰ã€‚ä½œè€…ä»¥ç‰©ç†è§†è§’é˜é‡Šäº†æ·±åº¦å­¦ä¹ åœ¨é«˜ç»´ç‰©ç†ç³»ç»Ÿä¸­çš„å»ºæ¨¡ä¼˜åŠ¿åŠå…¶å±€é™ï¼ŒæŒ‡å‡ºæ·±åº¦å­¦ä¹ å¯è¾…åŠ©ç‰©ç†ç ”ç©¶å‘ç°æ¨¡å¼ï¼Œä½†å°šä¸èƒ½æ›¿ä»£ç†è§£ç‰©ç†æœºåˆ¶ã€‚ç»“è®ºè®¤ä¸ºæ·±åº¦å­¦ä¹ ä¸ºç‰©ç†å­¦å°¤å…¶æ˜¯é‡å­ç§‘å­¦æä¾›äº†å¼ºå¤§å·¥å…·ï¼Œä½†å…¶ç§‘å­¦è§£é‡Šæ€§ä¸çŸ¥è¯†è·å–ä»éœ€è¿›ä¸€æ­¥çªç ´ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Deep%20Learning%20in%20Classical%20and%20Quantum%20Physics.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="121-Self-Supervised-Temporal-Super-Resolution-of-Energy-Data-using-Generative-Adversarial-Transformers"><a href="#121-Self-Supervised-Temporal-Super-Resolution-of-Energy-Data-using-Generative-Adversarial-Transformers" class="headerlink" title="121. Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformers"></a>121. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Self-Supervised_Temporal_Super-Resolution_of_Energy_Data_using_Generative_Adversarial_Transformer.pdf">Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Karlsruhe Institute of Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ— éœ€é«˜åˆ†è¾¨ç‡æ•°æ®çš„è‡ªç›‘ç£æ—¶é—´åºåˆ—è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œç»“åˆäº†Transformerçš„åºåˆ—å»ºæ¨¡èƒ½åŠ›å’ŒGANçš„å¯¹æŠ—è®­ç»ƒæ¡†æ¶ï¼Œå¹¶é€šè¿‡ç‰¹å¾ç©ºé—´æŸå¤±å®ç°ä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡æ•°æ®çš„ä¸€è‡´æ˜ å°„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§èƒ½æºæ•°æ®é›†ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿæ’å€¼ä¸ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒRMSEé™ä½9%ã€åœ¨MPCåº”ç”¨åœºæ™¯ç²¾åº¦æå‡13%ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Self-Supervised%20Temporal%20Super-Resolution%20of%20Energy%20Data%20using%20Generative%20Adversarial%20Transformers.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="122-FAKE-SPEECH-WILD-DETECTING-DEEPFAKE-SPEECH-ON-SOCIAL-MEDIA-PLATFORM"><a href="#122-FAKE-SPEECH-WILD-DETECTING-DEEPFAKE-SPEECH-ON-SOCIAL-MEDIA-PLATFORM" class="headerlink" title="122. FAKE SPEECH WILD: DETECTING DEEPFAKE SPEECH ON SOCIAL MEDIA PLATFORM"></a>122. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Fake_Speech_Wild__Detecting_Deepfake_Speech_on_Social_Media_Platform.pdf">FAKE SPEECH WILD: DETECTING DEEPFAKE SPEECH ON SOCIAL MEDIA PLATFORM</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Communication University of China</span></p>
<p>æœ¬æ–‡æå‡ºäº†Fake Speech Wild (FSW) æ•°æ®é›†ï¼Œæ”¶é›†äº†æ¥è‡ªå››ä¸ªç¤¾äº¤åª’ä½“å¹³å°ã€å…±254å°æ—¶çš„çœŸå®ä¸æ·±åº¦ä¼ªé€ è¯­éŸ³æ•°æ®ã€‚é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯å’Œå¤šç§æ•°æ®å¢å¼ºæ–¹æ³•ï¼ˆå¦‚MUSAN &amp; RIRå™ªå£°ã€Rawboostï¼‰ï¼Œè”åˆè®­ç»ƒå¤šæºæ•°æ®é›†åï¼Œæ˜¾è‘—æå‡äº†æ·±åº¦ä¼ªé€ è¯­éŸ³æ£€æµ‹æ¨¡å‹åœ¨è·¨é¢†åŸŸåœºæ™¯ä¸‹çš„é²æ£’æ€§ï¼Œæœ€ç»ˆåœ¨å…¨éƒ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°3.54%çš„å¹³å‡ç­‰é”™è¯¯ç‡ï¼ˆEERï¼‰ã€‚ç»“è®ºï¼šè”åˆå¤šåŸŸæ•°æ®å’Œæ•°æ®å¢å¼ºæ˜¾è‘—æå‡äº†æ£€æµ‹æ€§èƒ½ï¼ŒFSWæ•°æ®é›†ä¸ºçœŸå®ç¯å¢ƒä¸‹ä¼ªé€ è¯­éŸ³æ£€æµ‹æä¾›äº†é‡è¦åŸºå‡†ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FAKE%20SPEECH%20WILD%3A%20DETECTING%20DEEPFAKE%20SPEECH%20ON%20SOCIAL%20MEDIA%20PLATFORM.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="123-Contrastive-ECOC-Learning-Output-Codes-for-Adversarial-Defense"><a href="#123-Contrastive-ECOC-Learning-Output-Codes-for-Adversarial-Defense" class="headerlink" title="123. Contrastive ECOC: Learning Output Codes for Adversarial Defense"></a>123. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Contrastive_ECOC__Learning_Output_Codes_for_Adversarial_Defense.pdf">Contrastive ECOC: Learning Output Codes for Adversarial Defense</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">National Central University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸‰ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„è‡ªåŠ¨åŒ–ç¼–ç æœ¬å­¦ä¹ ï¼ˆACLï¼‰æ–¹æ³•ï¼Œç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ä¸­çš„è‡ªåŠ¨ç”ŸæˆError Correcting Output Codesï¼ˆECOCï¼‰ç¼–ç æœ¬ï¼Œæœ‰æ•ˆæå‡ç¥ç»ç½‘ç»œå¯¹æŠ—æ”»å‡»ï¼ˆå¦‚FGSMå’ŒPGDï¼‰çš„é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€ææ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ•°æ®é›†ç‰¹å®šçš„ç¼–ç æœ¬ï¼Œåœ¨å¤šä¸ªå¼€æ”¾æ•°æ®é›†ä¸Šå¯¹æŠ—æ”»å‡»ä¸‹çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ‰‹å·¥æˆ–éšæœºç¼–ç æœ¬ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Contrastive%20ECOC%3A%20Learning%20Output%20Codes%20for%20Adversarial%20Defense.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="124-GraphFedMIG-Tackling-Class-Imbalance-in-Federated-Graph-Learning-via-Mutual-Information-Guided-Generation"><a href="#124-GraphFedMIG-Tackling-Class-Imbalance-in-Federated-Graph-Learning-via-Mutual-Information-Guided-Generation" class="headerlink" title="124. GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation"></a>124. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GraphFedMIG__Tackling_Class_Imbalance_in_Federated_Graph_Learning_via_Mutual_Information-Guided_Gene.pdf">GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Chongqing University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºGraphFedMIGæ¡†æ¶ï¼Œé€šè¿‡å°†è”é‚¦å›¾å­¦ä¹ ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜è½¬åŒ–ä¸ºç”Ÿæˆå¼æ•°æ®å¢å¼ºä»»åŠ¡ï¼Œé‡‡ç”¨åˆ†å±‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œäº’ä¿¡æ¯å¼•å¯¼çš„æ¨¡å‹èšåˆæœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒGraphFedMIGåœ¨å››ä¸ªçœŸå®æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†å°‘æ•°ç±»çš„è¯†åˆ«èƒ½åŠ›å¹¶æ•´ä½“ä¼˜äºå½“å‰ä¸»æµæ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GraphFedMIG%3A%20Tackling%20Class%20Imbalance%20in%20Federated%20Graph%20Learning%20via%20Mutual%20Information-Guided%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="125-HiRef-Leveraging-Hierarchical-Ontology-and-Network-Refinement-for-Robust-Medication-Recommendation"><a href="#125-HiRef-Leveraging-Hierarchical-Ontology-and-Network-Refinement-for-Robust-Medication-Recommendation" class="headerlink" title="125. HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation"></a>125. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/HiRef__Leveraging_Hierarchical_Ontology_and_Network_Refinement_for_Robust_Medication_Recommendation.pdf">HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Korea University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºHiRefæ¡†æ¶ï¼Œå°†åŒ»ç–—æœ¬ä½“çš„å±‚æ¬¡è¯­ä¹‰åµŒå…¥ï¼ˆä½¿ç”¨è¶…æ›²ç©ºé—´ï¼‰ä¸ç”µå­ç—…å†ï¼ˆEHRï¼‰ä¸­ç¨€ç–ä¼˜åŒ–çš„å…±ç°å›¾ç»“æ„ç›¸ç»“åˆï¼Œé€šè¿‡è‡ªé€‚åº”èåˆæœºåˆ¶æå‡è¯ç‰©æ¨èæ¨¡å‹å¯¹ç½•è§å’Œæœªè§åŒ»ç–—ç¼–ç çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒHiRefåœ¨MIMIC-III&#x2F;IVæ•°æ®é›†ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨æœªè§ç¼–ç åœºæ™¯ä¸‹å‡†ç¡®ç‡é«˜ï¼Œä¸”å…·å¤‡æ›´å¥½çš„é²æ£’æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/HiRef%3A%20Leveraging%20Hierarchical%20Ontology%20and%20Network%20Refinement%20for%20Robust%20Medication%20Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="126-Mitigating-Exponential-Mixed-Frequency-Growth-through-Frequency-Selection-and-Dimensional-Separation-in-Quantum-Machine-Learning"><a href="#126-Mitigating-Exponential-Mixed-Frequency-Growth-through-Frequency-Selection-and-Dimensional-Separation-in-Quantum-Machine-Learning" class="headerlink" title="126. Mitigating Exponential Mixed Frequency Growth through Frequency Selection and Dimensional Separation in Quantum Machine Learning"></a>126. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Mitigating_Exponential_Mixed_Frequency_Growth_through_Frequency_Selection_and_Dimensional_Separation.pdf">Mitigating Exponential Mixed Frequency Growth through Frequency Selection and Dimensional Separation in Quantum Machine Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute for Informatics, LMU Munich</span></p>
<p>æœ¬æ–‡é’ˆå¯¹é‡å­æœºå™¨å­¦ä¹ ä¸­å¤šç»´å‚…é‡Œå¶é¢‘è°±å¯¼è‡´å‚æ•°åŒæŒ‡æ•°çº§å¢é•¿ã€éš¾ä»¥åœ¨å½“å‰ç¡¬ä»¶ä¸Šæ‹Ÿåˆå®é™…é—®é¢˜çš„ç“¶é¢ˆï¼Œæå‡ºäº†é¢‘ç‡é€‰æ‹©ä¸ç»´åº¦åˆ†ç¦»ä¸¤é¡¹å…³é”®æŠ€æœ¯ï¼Œé€šè¿‡ä»…ä¿ç•™å¿…è¦é¢‘ç‡å’Œæ··åˆé¢‘ç‡ç»„ï¼Œæå¤§å‡å°‘æ‰€éœ€æ¨¡å‹å‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•èƒ½åœ¨å™ªå£°æ¨¡æ‹Ÿå™¨åŠçœŸå®é‡å­ç¡¬ä»¶ä¸Šå®ç°é«˜ç²¾åº¦å›å½’æ‹Ÿåˆï¼Œçªç ´äº†é»˜è®¤æ¨¡å‹é¢‘è°±å¯†é›†å¯¼è‡´çš„æ‹Ÿåˆå¤±è´¥ï¼Œç†è®ºä¸å®è·µä¸Šå‡æœ‰æ•ˆç¼“è§£äº†å‚æ•°çˆ†ç‚¸é—®é¢˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Mitigating%20Exponential%20Mixed%20Frequency%20Growth%20through%20Frequency%20Selection%20and%20Dimensional%20Separation%20in%20Quantum%20Machine%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="127-RealAC-A-Domain-Agnostic-Framework-for-Realistic-and-Actionable-Counterfactual-Explanations"><a href="#127-RealAC-A-Domain-Agnostic-Framework-for-Realistic-and-Actionable-Counterfactual-Explanations" class="headerlink" title="127. RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations"></a>127. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/RealAC__A_Domain-Agnostic_Framework_for_Realistic_and_Actionable_Counterfactual_Explanations.pdf">RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Arizona State University</span></p>
<p>æœ¬æ–‡æå‡ºäº†RealACï¼Œä¸€ä¸ªé¢å‘é¢†åŸŸæ— å…³ã€èƒ½å¤Ÿç”Ÿæˆç°å®ä¸”å¯æ“ä½œåäº‹å®è§£é‡Šçš„æ¡†æ¶ã€‚RealACé€šè¿‡æœ€å°åŒ–åäº‹å®ä¸åŸå§‹æ ·æœ¬ä¹‹é—´æ‰€æœ‰ç‰¹å¾å¯¹çš„äº’ä¿¡æ¯å·®å¼‚ï¼Œåœ¨æ— éœ€é¢†åŸŸçŸ¥è¯†çš„æƒ…å†µä¸‹è‡ªåŠ¨ä¿æŒå¤æ‚çš„ç‰¹å¾ä¾èµ–å…³ç³»ï¼Œå¹¶ç»“åˆäºŒè¿›åˆ¶æ©ç æœºåˆ¶æ”¯æŒç”¨æˆ·æŒ‡å®šä¸å¯å˜ç‰¹å¾ï¼Œå®ç°ä¸ªæ€§åŒ–å¯è¡Œæ€§çº¦æŸã€‚å®éªŒè¡¨æ˜ï¼ŒRealACåœ¨å› æœè¾¹ç¼˜åˆ†æ•°ã€ä¾èµ–ä¿æŒåˆ†æ•°å’ŒIM1ç­‰æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•åŠå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åäº‹å®ï¼Œå…¼é¡¾äº†åäº‹å®çš„ç°å®æ€§ä¸å¯æ“ä½œæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/RealAC%3A%20A%20Domain-Agnostic%20Framework%20for%20Realistic%20and%20Actionable%20Counterfactual%20Explanations.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="128-Semantic-Communication-with-Distribution-Learning-through-Sequential-Observations"><a href="#128-Semantic-Communication-with-Distribution-Learning-through-Sequential-Observations" class="headerlink" title="128. Semantic Communication with Distribution Learning through Sequential Observations"></a>128. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Semantic_Communication_with_Distribution_Learning_through_Sequential_Observations.pdf">Semantic Communication with Distribution Learning through Sequential Observations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Dalhousie University</span></p>
<p>æœ¬æ–‡æå‡ºäº†è¯­ä¹‰é€šä¿¡ä¸­çš„åˆ†å¸ƒå­¦ä¹ ç†è®ºæ¡†æ¶ï¼Œç ”ç©¶åœ¨å…ˆéªŒåˆ†å¸ƒæœªçŸ¥ä¸”ç¼–ç æ–¹æ¡ˆä¸ä¿¡é“å›ºå®šæ—¶æ¥æ”¶ç«¯å¦‚ä½•é€šè¿‡è§‚å¯Ÿåºåˆ—å­¦ä¹ è¯­ä¹‰åˆ†å¸ƒã€‚æ–¹æ³•ä¸Šï¼Œä½œè€…æå‡ºä»¥æœ‰æ•ˆä¼ è¾“çŸ©é˜µçš„æ»¡ç§©æ€§ä¸ºå¯å­¦ä¹ æ€§æ¡ä»¶ï¼Œæ¨å¯¼å‡ºä¼°è®¡è¯¯å·®å’Œæ€§èƒ½æŸå¤±çš„æ”¶æ•›ç•Œï¼Œå¹¶ç†è®ºè¯æ˜ä¼°è®¡è¯¯å·®ä¸æ ·æœ¬æ•°æˆO(1&#x2F;âˆšT)è¡°å‡ï¼›ç»“è®ºè¡¨æ˜ï¼Œè‰¯å¥½æ¡ä»¶æ•°çš„ç³»ç»Ÿèƒ½å®ç°é«˜æ•ˆå­¦ä¹ å’Œé€‚åº”ï¼Œå‹ç¼©ä¼˜åŒ–å‹ç¼–ç åˆ™æ˜¾è‘—é™ä½é•¿æœŸå­¦ä¹ èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Semantic%20Communication%20with%20Distribution%20Learning%20through%20Sequential%20Observations.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="129-eMamba-Efficient-Acceleration-Framework-for-Mamba-Models-in-Edge-Computing"><a href="#129-eMamba-Efficient-Acceleration-Framework-for-Mamba-Models-in-Edge-Computing" class="headerlink" title="129. eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing"></a>129. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/eMamba__Efficient_Acceleration_Framework_for_Mamba_Models_in_Edge_Computing.pdf">eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Ulsan</span></p>
<p>æœ¬æ–‡æå‡ºäº†eMambaï¼Œä¸€ç§é¢å‘è¾¹ç¼˜è®¡ç®—çš„Mambaæ¨¡å‹ç«¯åˆ°ç«¯ç¡¬ä»¶åŠ é€Ÿæ¡†æ¶ã€‚æ–¹æ³•åŒ…æ‹¬ç”¨ç¡¬ä»¶å‹å¥½çš„å½’ä¸€åŒ–ã€åˆ†æ®µçº¿æ€§è¿‘ä¼¼åŠé‡åŒ–æŠ€æœ¯æ›¿ä»£åŸæœ‰å¤æ‚æ“ä½œï¼Œå¹¶ç»“åˆè¿‘ä¼¼æ„ŸçŸ¥çš„ç¥ç»æ¶æ„æœç´¢ä¼˜åŒ–å¯è®­ç»ƒå‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨Fashion-MNISTã€CIFAR-10å’ŒMARSç­‰è§†è§‰ä»»åŠ¡ä¸­ï¼ŒeMambaåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶å¤§å¹…å‡å°‘å‚æ•°é‡ï¼Œå¹¶åœ¨FPGAå’ŒASICå®ç°ä¸­è·å¾—æ›´ä½å»¶è¿Ÿã€æ›´é«˜ååç‡å’Œæ˜¾è‘—çš„èƒ½è€—é™ä½ï¼Œå¯å¹¿æ³›é€‚ç”¨äºè¾¹ç¼˜AIåº”ç”¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/eMamba%3A%20Efficient%20Acceleration%20Framework%20for%20Mamba%20Models%20in%20Edge%20Computing.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="130-Layer-Wise-Analysis-of-Self-Supervised-Representations-for-Age-and-Gender-Classification-in-Childrenâ€™s-Speech"><a href="#130-Layer-Wise-Analysis-of-Self-Supervised-Representations-for-Age-and-Gender-Classification-in-Childrenâ€™s-Speech" class="headerlink" title="130. Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Childrenâ€™s Speech"></a>130. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Layer-Wise_Analysis_of_Self-Supervised_Representations_for_Age_and_Gender_Classification_in_Children.pdf">Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Childrenâ€™s Speech</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">NIT Sikkim</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å„¿ç«¥è¯­éŸ³å¹´é¾„å’Œæ€§åˆ«åˆ†ç±»çš„å±‚çº§ç‰¹å¾åˆ†ææ–¹æ³•ï¼Œåˆ©ç”¨å››ç§Wav2Vec2è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹æå–å±‚çº§ç‰¹å¾ï¼Œå¹¶é€šè¿‡CNNåˆ†ç±»å™¨å’Œä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰è¿›è¡Œé™ç»´å’Œæ€§èƒ½ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹æ—©æœŸå±‚èƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰è¯´è¯äººç‰¹å¾ï¼Œé™ç»´åä¸ä»…æå‡äº†åˆ†ç±»å‡†ç¡®ç‡ä¹Ÿæé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œæ­ç¤ºäº†ç‰¹å¾åœ¨æ¨¡å‹æ·±åº¦ä¸­çš„åˆ†å¸ƒè§„å¾‹ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Layer-Wise%20Analysis%20of%20Self-Supervised%20Representations%20for%20Age%20and%20Gender%20Classification%20in%20Children%E2%80%99s%20Speech.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="131-A-Vision-Language-Pre-training-Model-Guided-Approach-for-Mitigating-Backdoor-Attacks-in-Federated-Learning"><a href="#131-A-Vision-Language-Pre-training-Model-Guided-Approach-for-Mitigating-Backdoor-Attacks-in-Federated-Learning" class="headerlink" title="131. A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning"></a>131. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Vision-Language_Pre-training_Model-Guided_Approach_for_Mitigating_Backdoor_Attacks_in_Federated_Le.pdf">A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Beijing Institute of Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºCLIP-Fedæ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„è·¨æ¨¡æ€é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œå®ç°è”é‚¦å­¦ä¹ ä¸­çš„å‰èšåˆæ¶æ„æ¨¡å‹è¿‡æ»¤å’Œåèšåˆåé—¨å‡€åŒ–ã€‚é€šè¿‡å¤šæ¨¡æ€å¤§æ¨¡å‹å’Œé¢‘åŸŸåˆ†ææ„å»ºæœåŠ¡å™¨æ•°æ®é›†ï¼Œå¹¶ç»“åˆåŸå‹å¯¹æ¯”æŸå¤±å’ŒçŸ¥è¯†è’¸é¦ï¼Œçº æ­£ç‰¹å¾åç§»ã€å‰Šå¼±åé—¨å…³è”ã€‚å®éªŒè¡¨æ˜ï¼ŒCLIP-Fedåœ¨å¤šç§æ•°æ®åˆ†å¸ƒå’Œæ”»å‡»ç±»å‹ä¸‹æ˜¾è‘—é™ä½ASRå¹¶æå‡ä¸»ä»»åŠ¡å‡†ç¡®ç‡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A%20Vision-Language%20Pre-training%20Model-Guided%20Approach%20for%20Mitigating%20Backdoor%20Attacks%20in%20Federated%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="132-Pruning-and-Malicious-Injection-A-Retraining-Free-Backdoor-Attack-on-Transformer-Models"><a href="#132-Pruning-and-Malicious-Injection-A-Retraining-Free-Backdoor-Attack-on-Transformer-Models" class="headerlink" title="132. Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models"></a>132. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Pruning_and_Malicious_Injection__A_Retraining-Free_Backdoor_Attack_on_Transformer_Models.pdf">Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Louisiana State University</span></p>
<p>æœ¬æ–‡æå‡ºäº†HPMIï¼ˆHead-wise Pruning and Malicious Injectionï¼‰ï¼Œä¸€ç§æ— éœ€é¢å¤–é‡è®­ç»ƒä¸”ä¸æ”¹å˜æ¨¡å‹æ¶æ„çš„å˜æ¢å™¨åé—¨æ”»å‡»æ–¹æ³•ã€‚é€šè¿‡å‰ªé™¤å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ä¸­æœ€ä¸é‡è¦çš„å¤´ï¼Œå†æ³¨å…¥é¢„è®­ç»ƒçš„æ¶æ„å¤´ï¼Œå®ç°åé—¨æ¤å…¥ï¼Œç†è®ºä¸å®éªŒå‡è¡¨æ˜åœ¨ä¿æŒå¹²å‡€æ•°æ®å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œèƒ½ä»¥è¶…99.55%çš„æˆåŠŸç‡ç»•è¿‡å››ç§ä¸»æµé˜²å¾¡æœºåˆ¶ã€‚ç»“è®ºæ˜¾ç¤ºHPMIå¯¹ç°æœ‰é˜²å¾¡æ‰‹æ®µæœ‰å¾ˆå¼ºçš„éšè”½æ€§å’Œé²æ£’æ€§ï¼Œä¸”å¯¹æ¨¡å‹æ€§èƒ½å½±å“æå°ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Pruning%20and%20Malicious%20Injection%3A%20A%20Retraining-Free%20Backdoor%20Attack%20on%20Transformer%20Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>ä»Šæ—¥è®ºæ–‡æŠ¥çº¸</div>
      <div>http://example.com/2025/08/2025-08-18_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>ä½œè€…</div>
          <div>Ywfhhh</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>å‘å¸ƒäº</div>
          <div>2025å¹´8æœˆ18æ—¥</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>è®¸å¯åè®®</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - ç½²å">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-15_article/" title="ä»Šæ—¥è®ºæ–‡æŠ¥çº¸">
                        <span class="hidden-mobile">ä»Šæ—¥è®ºæ–‡æŠ¥çº¸</span>
                        <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"æ¬¢è¿äº¤æµè®¨è®º...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>ç›®å½•</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        æ€»è®¿é—®é‡ 
        <span id="busuanzi_value_site_pv"></span>
         æ¬¡
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        æ€»è®¿å®¢æ•° 
        <span id="busuanzi_value_site_uv"></span>
         äºº
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- ä¸»é¢˜çš„å¯åŠ¨é¡¹ï¼Œå°†å®ƒä¿æŒåœ¨æœ€åº•éƒ¨ -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div>
  </noscript>
</body>
</html>
