

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head><!-- hexo injector head_begin start -->
<script>
  function download(url) {
    // 检查 URL 是否有效
    if (!url) {
      console.error('下载 URL 无效');
      return;
    }

    // 创建一个隐藏隐藏的 <a> 元素用于下载
    const a = document.createElement('a');
    a.href = url;
    a.download = url.split('/').pop(); // 设置下载的文件名（默认为 URL 的最后一部分）
    document.body.appendChild(a); // 添加到 DOM
    a.click(); // 触发点击事件
    document.body.removeChild(a); // 下载完成后移除 <a> 元素
  }
</script>
<!-- hexo injector head_begin end -->
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="具身ADT">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;18报纸">
<meta property="og:url" content="http://ywfhhh.github.io/2025/08/2025-08-18_article/index.html">
<meta property="og:site_name" content="最新前沿论文分享">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Non-Stationary_Restless_Multi-Armed_Bandits_with_Provable_Guarantee.jpg">
<meta property="article:published_time" content="2025-08-18T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-22T07:06:21.727Z">
<meta property="article:author" content="具身ADT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Non-Stationary_Restless_Multi-Armed_Bandits_with_Provable_Guarantee.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/18报纸 - 最新前沿论文分享</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ywfhhh.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文库</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/18报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-18 00:00" pubdate>
          星期一, 八月 18日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          20k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/18报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/18报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-18<br>📄 <strong>发现论文数量</strong>：132  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Non-Stationary-Restless-Multi-Armed-Bandits-with-Provable-Guarantee"><a href="#1-Non-Stationary-Restless-Multi-Armed-Bandits-with-Provable-Guarantee" class="headerlink" title="1. Non-Stationary Restless Multi-Armed Bandits with Provable Guarantee"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Non-Stationary_Restless_Multi-Armed_Bandits_with_Provable_Guarantee.pdf">Non-Stationary Restless Multi-Armed Bandits with Provable Guarantee</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Yang Ming Chiao Tung University</span></p>
<p>本文提出NS-Whittle算法，针对带有有限变化预算的非平稳Restless Multi-Armed Bandits（RMABs），将滑动窗口强化学习（RL）与上置信界（UCB）机制结合，实现对转移动态及其变化的自适应学习。方法采用每臂独立的滑动窗口和Whittle指数，显著降低维度和计算复杂度，并首次在非平稳RMAB下给出Oe(N²B^{1&#x2F;4}T^{3&#x2F;4})的理论后悔界限。结论表明，该方法兼顾理论严谨性和实际可扩展性，为动态资源受限环境下的序贯决策提供了高效基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Non-Stationary_Restless_Multi-Armed_Bandits_with_Provable_Guarantee.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-Variance-Reduced-Policy-Gradient-Method-for-Multi-Objective-Reinforcement-Learning"><a href="#2-Variance-Reduced-Policy-Gradient-Method-for-Multi-Objective-Reinforcement-Learning" class="headerlink" title="2. Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Variance_Reduced_Policy_Gradient_Method_for_Multi-Objective_Reinforcement_Learning.pdf">Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ETH Zurich</span></p>
<p>本论文提出了MO-TSIVR-PG算法，通过在多目标强化学习(MORL)中对策略梯度方法引入方差降低技术，显著提升了样本效率，支持大规模和连续状态-动作空间。理论分析和实验结果表明，该算法在收敛速度和样本复杂度上均优于现有方法，且在多目标任务中表现优异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Variance_Reduced_Policy_Gradient_Method_for_Multi-Objective_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-Pass-k-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models"><a href="#3-Pass-k-Training-for-Adaptively-Balancing-Exploration-and-Exploitation-of-Large-Reasoning-Models" class="headerlink" title="3. Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Pass@k_Training_for_Adaptively_Balancing_Exploration_and_Exploitation_of_Large_Reasoning_Models.pdf">Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Renmin University of China</span></p>
<p>本文提出了在大型语言模型（LLMs）推理任务中的RLVR训练中，采用Pass@k作为奖励指标的方法（Pass@k Training），通过全采样、bootstrap采样和解析推导三种流程，有效提升模型的探索能力并保持其利用能力。结论显示，Pass@k Training不仅提升了模型在下游任务中的Pass@k表现，还能通过后续Pass@1训练将探索收益转化为更强的Pass@1表现，具备良好的泛化性和实际应用价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Pass%40k_Training_for_Adaptively_Balancing_Exploration_and_Exploitation_of_Large_Reasoning_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-REFN-A-Reinforcement-Learning-From-Network-Framework-against-1-day-n-day-Exploitations"><a href="#4-REFN-A-Reinforcement-Learning-From-Network-Framework-against-1-day-n-day-Exploitations" class="headerlink" title="4. REFN: A Reinforcement-Learning-From-Network Framework against 1-day&#x2F;n-day Exploitations"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/REFN__A_Reinforcement-Learning-From-Network_Framework_against_1-day_n-day_Exploitations.pdf">REFN: A Reinforcement-Learning-From-Network Framework against 1-day&#x2F;n-day Exploitations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, Hubei University</span></p>
<p>该论文提出REFN框架，通过网络驱动的强化学习（RL）训练大型语言模型（LLMs），自动生成并部署网络过滤器，有效防御大规模1-day&#x2F;n-day漏洞利用。REFN采用Agentic-RAG知识蒸馏、RL-from-VNF管道和在线Agentic验证，显著提升过滤规则生成的准确性、部署效率和可扩展性。实验表明，REFN在准确率和F1分数方面均优于现有方法，平均修复时间缩短至3.65小时，能高效扩展至万台设备。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/REFN_A_Reinforcement-Learning-From-Network_Framework_against_1-day_n-day_Exploitations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-Nonlocal-Monte-Carlo-via-Reinforcement-Learning"><a href="#5-Nonlocal-Monte-Carlo-via-Reinforcement-Learning" class="headerlink" title="5. Nonlocal Monte Carlo via Reinforcement Learning"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Nonlocal_Monte_Carlo_via_Reinforcement_Learning.pdf">Nonlocal Monte Carlo via Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peter Grünberg Institut (PGI-14), Forschungszentrum Jülich GmbH</span></p>
<p>该论文提出RLNMC方法，将深度强化学习与Nonequilibrium Nonlocal Monte Carlo（NMC）算法结合，通过训练图神经网络（GNN）策略自动发现组合优化中的非局部跳跃步骤。实验表明，RLNMC在典型难解4-SAT基准问题上，相较于标准MCMC和传统NMC方法，能够在残差能量、解的多样性和求解时间等指标上取得明显提升，并展现出较好的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Nonlocal_Monte_Carlo_via_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-Stabilizing-Long-term-Multi-turn-Reinforcement-Learning-with-Gated-Rewards"><a href="#6-Stabilizing-Long-term-Multi-turn-Reinforcement-Learning-with-Gated-Rewards" class="headerlink" title="6. Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Stabilizing_Long-term_Multi-turn_Reinforcement_Learning_with_Gated_Rewards.pdf">Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology (Shenzhen)</span></p>
<p>该论文提出了SWE-oriented RL Framework和Gated Reward Accumulation (G-RA)方法，针对软件工程任务中的多轮长时序强化学习，结合多层奖励设计及docker环境支持，实现了奖励的门控累积，仅在高层(长期)奖励达到阈值时累积即时奖励，从而平衡稀疏长期目标与密集即时反馈。实验表明，G-RA显著提升了任务完成率与修改率，有效避免了奖励错配和策略退化，验证了其在复杂长时序任务中稳定RL优化的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Stabilizing_Long-term_Multi-turn_Reinforcement_Learning_with_Gated_Rewards.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning"><a href="#7-Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning" class="headerlink" title="7. Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Few-shot_Vision-based_Human_Activity_Recognition_with_MLLM-based_Visual_Reinforcement_Learning.pdf">Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kyushu University</span></p>
<p>该论文提出FAVOR方法，将多模态大语言模型（MLLM）与基于视觉的强化学习（使用Group Relative Policy Optimization, GRPO和可验证奖励函数）结合，用于少样本人类活动识别。实验表明FAVOR在HAR任务的少样本学习中显著优于传统监督微调方法，尤其在数据稀缺情况下提升泛化和推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Few-shot_Vision-based_Human_Activity_Recognition_with_MLLM-based_Visual_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-Multi-Agent-Trust-Region-Policy-Optimisation-A-Joint-Constraint-Approach"><a href="#8-Multi-Agent-Trust-Region-Policy-Optimisation-A-Joint-Constraint-Approach" class="headerlink" title="8. Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Multi-Agent_Trust_Region_Policy_Optimisation__A_Joint_Constraint_Approach.pdf">Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland</span></p>
<p>本文提出了针对多智能体信任域策略优化（HATRPO）均匀KL约束低效问题的两种自适应扩展方法：HATRPO-G（基于改进-散度比的贪心分配）和HATRPO-W（基于KKT条件的全局优化分配），实现总KL阈值在智能体间的灵活分配。实验结果表明，提出方法能更快收敛并显著提升最终奖励，尤其在异质和不平衡智能体场景下表现优异，超过22.5%的性能提升，并有效捕获智能体间优势差异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Agent_Trust_Region_Policy_Optimisation_A_Joint_Constraint_Approach.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-Integrating-Reinforcement-Learning-with-Visual-Generative-Models-Foundations-and-Advances"><a href="#9-Integrating-Reinforcement-Learning-with-Visual-Generative-Models-Foundations-and-Advances" class="headerlink" title="9. Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Integrating_Reinforcement_Learning_with_Visual_Generative_Models__Foundations_and_Advances.pdf">Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Artificial Intelligence (TeleAI), China Telecom</span></p>
<p>本文系统综述了强化学习（RL）与视觉生成模型（包括图像、视频、3D&#x2F;4D内容生成）结合的最新进展，涵盖RL理论演变、主流RL方法（如PPO、GRPO、DPO）在提升生成模型可控性、语义对齐、人类偏好匹配等方面的应用流程。结论指出，RL不仅作为生成模型的微调和优化层，更成为实现复杂目标和人类意图对齐的结构性组件，对未来多模态、交互式生成系统具有重要意义。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Integrating_Reinforcement_Learning_with_Visual_Generative_Models_Foundations_and_Advances.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-A-Curriculum-Learning-Approach-to-Reinforcement-Learning-Leveraging-RAG-for-Multimodal-Question-Answering"><a href="#10-A-Curriculum-Learning-Approach-to-Reinforcement-Learning-Leveraging-RAG-for-Multimodal-Question-Answering" class="headerlink" title="10. A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Curriculum_Learning_Approach_to_Reinforcement_Learning__Leveraging_RAG_for_Multimodal_Question_Ans.pdf">A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Meituan</span></p>
<p>本文提出了结合视觉大语言模型（VLLM）、知识蒸馏、RAG检索增强生成和课程学习的强化学习训练方法，用于多模态、多轮次问题回答。该方法通过分阶段课程学习稳定强化学习训练，有效提升模型在多模态检索与问题回答任务中的准确率和可靠性，在KDD Cup 2025 CRAG-MM竞赛的多项任务中取得领先成绩。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Curriculum_Learning_Approach_to_Reinforcement_Learning_Leveraging_RAG_for_Multimodal_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-Multi-Agent-Reinforcement-Learning-for-Adaptive-Resource-Orchestration-in-Cloud-Native-Clusters"><a href="#11-Multi-Agent-Reinforcement-Learning-for-Adaptive-Resource-Orchestration-in-Cloud-Native-Clusters" class="headerlink" title="11. Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Multi-Agent_Reinforcement_Learning_for_Adaptive_Resource_Orchestration_in_Cloud-Native_Clusters.pdf">Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northwestern University</span></p>
<p>本文提出了一种基于多智能体强化学习（MARL）的自适应资源编排方法，采用异构角色驱动的代理合作机制（HRAC）和局部-全局奖励塑形（LGRS）策略，实现对计算节点、存储节点和调度器等异构组件的分工学习与协作。实验表明，该方法在资源利用率、调度延迟和策略收敛速度等方面优于传统方法，并在多租户公平性和不完全信息场景下表现出良好的稳定性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Agent_Reinforcement_Learning_for_Adaptive_Resource_Orchestration_in_Cloud-Native_Clusters.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="12-TLE-Based-A2C-Agent-for-Terrestrial-Coverage-Orbital-Path-Planning"><a href="#12-TLE-Based-A2C-Agent-for-Terrestrial-Coverage-Orbital-Path-Planning" class="headerlink" title="12. TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/TLE-Based_A2C_Agent_for_Terrestrial_Coverage_Orbital_Path_Planning.pdf">TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sardar Vallabhbhai National Institute of Technology Surat</span></p>
<p>本文提出了一种基于TLE的轨道仿真环境，利用Advantage Actor-Critic (A2C) 强化学习算法自动优化地球观测卫星的轨道参数，实现对地表目标的精确覆盖。实验表明，A2C在收敛速度和累计奖励上均优于Proximal Policy Optimization (PPO)，能高效满足任务目标，验证了强化学习在低轨卫星轨道规划中的实用性和高效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/TLE-Based_A2C_Agent_for_Terrestrial_Coverage_Orbital_Path_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-A-Multimodal-Neural-Network-for-Recognizing-Subjective-Self-Disclosure-Towards-Social-Robots"><a href="#13-A-Multimodal-Neural-Network-for-Recognizing-Subjective-Self-Disclosure-Towards-Social-Robots" class="headerlink" title="13. A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Multimodal_Neural_Network_for_Recognizing_Subjective_Self-Disclosure_Towards_Social_Robots.pdf">A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Amazon</span></p>
<p>本文提出了一种专为人机交互中主观自我表露建模设计的多模态注意力神经网络，结合音频（MFCC、wav2vec2.0）与视觉（OpenFace、InceptionV1-ResNet）特征输入，并创新性地引入了尺度保持交叉熵损失函数，有效处理了分类与回归之间的问题。结果显示，该模型在自建大规模数据集上F1分数达0.83，显著优于基线方法，为社会机器人感知人类自我表露能力提供了重要突破。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Multimodal_Neural_Network_for_Recognizing_Subjective_Self-Disclosure_Towards_Social_Robots.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT"><a href="#14-UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT" class="headerlink" title="14. UI-Venus Technical Report: Building High-performance UI Agents with RFT"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/UI-Venus_Technical_Report__Building_High-performance_UI_Agents_with_RFT.pdf">UI-Venus Technical Report: Building High-performance UI Agents with RFT</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ant Group</span></p>
<p>本论文提出了UI-Venus，一种基于多模态大语言模型（Qwen2.5-VL）并仅需截图输入的原生UI智能体，通过引入基于GRPO的强化学习微调（RFT）方法，实现了UI界面元素定位与导航任务的业界最优（SOTA）性能。方法包括高质量数据清洗、精细化奖励函数设计以及自进化轨迹历史对齐与稀疏动作增强框架，显著提升了复杂UI任务中的泛化和规划能力。结论是，UI-Venus在多个标准基准上超越现有开源与闭源模型，验证了强化学习策略和数据处理流程在UI Agent训练中的有效性，并已开源相关模型及评测代码。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/UI-Venus_Technical_Report_Building_High-performance_UI_Agents_with_RFT.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-Scaling-Up-without-Fading-Out-Goal-Aware-Sparse-GNN-for-RL-based-Generalized-Planning"><a href="#15-Scaling-Up-without-Fading-Out-Goal-Aware-Sparse-GNN-for-RL-based-Generalized-Planning" class="headerlink" title="15. Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Scaling_Up_without_Fading_Out__Goal-Aware_Sparse_GNN_for_RL-based_Generalized_Planning.pdf">Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">LIG Nex1</span></p>
<p>本文提出了一种结合稀疏、目标感知图神经网络（GNN）表示和课程学习的强化学习（RL）方法，专门用于解决大规模基于PDDL的无人机任务规划问题。通过仅编码局部邻接关系并显式整合目标空间特征，有效提升了训练效率和策略泛化能力，显著降低了GPU内存需求，实现了在大规模网格环境中高效的任务规划。实验证明，该方法在复杂无人机任务场景中具有优越的扩展性和稳定性，克服了以往全连接图方法的瓶颈。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Scaling_Up_without_Fading_Out_Goal-Aware_Sparse_GNN_for_RL-based_Generalized_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-Synthesis-of-Deep-Neural-Networks-with-Safe-Robust-Adaptive-Control-for-Reliable-Operation-of-Wheeled-Mobile-Robots"><a href="#16-Synthesis-of-Deep-Neural-Networks-with-Safe-Robust-Adaptive-Control-for-Reliable-Operation-of-Wheeled-Mobile-Robots" class="headerlink" title="16. Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Synthesis_of_Deep_Neural_Networks_with_Safe_Robust_Adaptive_Control_for_Reliable_Operation_of_Wheele.pdf">Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tampere University</span></p>
<p>本文提出了一种结合深度神经网络（DNN）与安全鲁棒自适应控制（RAC）的分层控制策略，实现重型轮式移动机器人（WMR）在复杂环境下的可靠运行。方法包括以DNN为主控制，在遇到外部干扰时由低层安全机制切换为RAC，系统始终受两级安全层监控，保障精度与稳定性。实验结果表明，所提方法能在满足安全标准的前提下，实现高精度和鲁棒性，并有效应对实际工况下的扰动。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Synthesis_of_Deep_Neural_Networks_with_Safe_Robust_Adaptive_Control_for_Reliable_Operation_of_Wheeled_Mobile_Robots.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-Learning-Task-Execution-Hierarchies-for-Redundant-Robots"><a href="#17-Learning-Task-Execution-Hierarchies-for-Redundant-Robots" class="headerlink" title="17. Learning Task Execution Hierarchies for Redundant Robots"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Learning_Task_Execution_Hierarchies_for_Redundant_Robots.pdf">Learning Task Execution Hierarchies for Redundant Robots</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Padova</span></p>
<p>该论文提出了一种结合强化学习（RL）和遗传编程（Genetic Programming, GP）的框架，能够根据用户自定义的高层任务目标和代价函数，自动为冗余机器人学习任务优先级、控制参数和任务激活逻辑（Stack of Tasks, SoTs）。方法流程包括：用户通过GUI设定任务目标权重，系统在仿真中以强化学习方式评估每组任务栈的表现，通过遗传编程演化优化任务顺序和参数，最终选取最优SoT进行真实机器人测试。实验在ABB移动双臂机器人YuMi平台的仿真和现实环境中表明，该方法能实现零样本迁移、适应动态环境、无需专家手工调参，显著提升了冗余机器人任务管理的通用性和自适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning_Task_Execution_Hierarchies_for_Redundant_Robots.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-MLM-Learning-Multi-task-Loco-Manipulation-Whole-Body-Control-for-Quadruped-Robot-with-Arm"><a href="#18-MLM-Learning-Multi-task-Loco-Manipulation-Whole-Body-Control-for-Quadruped-Robot-with-Arm" class="headerlink" title="18. MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MLM__Learning_Multi-task_Loco-Manipulation_Whole-Body_Control_for_Quadruped_Robot_with_Arm.pdf">MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了一种结合真实和仿真数据的强化学习框架MLM，实现了带机械臂四足机器人多任务全身运动-操作一体化控制。方法包括引入基于自适应课程的轨迹采样机制和轨迹-速度预测策略网络，实现多任务自适应学习与历史轨迹下的预测。实验表明，该方法可以在仿真和真实环境中实现多任务零样本迁移，提升了四足机器人运动与操作协同的通用性和任务表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MLM_Learning_Multi-task_Loco-Manipulation_Whole-Body_Control_for_Quadruped_Robot_with_Arm.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-Projected-Coupled-Diffusion-for-Test-Time-Constrained-Joint-Generation"><a href="#19-Projected-Coupled-Diffusion-for-Test-Time-Constrained-Joint-Generation" class="headerlink" title="19. Projected Coupled Diffusion for Test-Time Constrained Joint Generation"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Projected_Coupled_Diffusion_for_Test-Time_Constrained_Joint_Generation.pdf">Projected Coupled Diffusion for Test-Time Constrained Joint Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Singapore</span></p>
<p>本文提出了一种名为Projected Coupled Diffusion (PCD)的新型测试时约束联合生成框架，在无需重新训练的前提下，通过引入耦合引导项和每步投影操作，使多个预训练扩散模型在生成相关样本时能精确满足实时约束。方法在图像对生成、机器人操作和多机器人路径规划等任务中验证，实验表明PCD能有效提升样本相关性并严格满足任务约束，且计算开销可控。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Projected_Coupled_Diffusion_for_Test-Time_Constrained_Joint_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-MASH-Cooperative-Heterogeneous-Multi-Agent-Reinforcement-Learning-for-Single-Humanoid-Robot-Locomotion"><a href="#20-MASH-Cooperative-Heterogeneous-Multi-Agent-Reinforcement-Learning-for-Single-Humanoid-Robot-Locomotion" class="headerlink" title="20. MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MASH__Cooperative-Heterogeneous_Multi-Agent_Reinforcement_Learning_for_Single_Humanoid_Robot_Locomot.pdf">MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northeastern University</span></p>
<p>该论文提出了MASH方法，将单个人形机器人四肢（双腿与双臂）建模为独立智能体，通过协作异质多智能体深度强化学习（MARL）优化机器人步态与动作协调，采用中央评论员和多智能体PPO训练。实验证明MASH相比单智能体RL收敛更快、动作更平滑、肢体协调性更高，并且具备更强的实际部署鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MASH_Cooperative-Heterogeneous_Multi-Agent_Reinforcement_Learning_for_Single_Humanoid_Robot_Locomotion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-KDPE-A-Kernel-Density-Estimation-Strategy-for-Diffusion-Policy-Trajectory-Selection"><a href="#21-KDPE-A-Kernel-Density-Estimation-Strategy-for-Diffusion-Policy-Trajectory-Selection" class="headerlink" title="21. KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/KDPE__A_Kernel_Density_Estimation_Strategy_for_Diffusion_Policy_Trajectory_Selection.pdf">KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Istituto Italiano di Tecnologia</span></p>
<p>本文提出了KDPE，一种基于核密度估计（KDE）的策略，用于过滤和选择Diffusion Policy（DP）生成的机器人轨迹，通过并行采样多条轨迹并用流形感知核函数建模动作分布，筛除异常轨迹，仅保留最具代表性的动作路径。实验结果表明，KDPE在仿真和真实机器人抓取、分拣、制作咖啡等任务中，能提升成功率和鲁棒性，尤其在演示质量较低或精度要求较高的场景效果更优。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/KDPE_A_Kernel_Density_Estimation_Strategy_for_Diffusion_Policy_Trajectory_Selection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-Learning-State-Space-Models-of-Dynamic-Systems-from-Arbitrary-Data-using-Joint-Embedding-Predictive-Architectures"><a href="#22-Learning-State-Space-Models-of-Dynamic-Systems-from-Arbitrary-Data-using-Joint-Embedding-Predictive-Architectures" class="headerlink" title="22. Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Learning_State-Space_Models_of_Dynamic_Systems_from_Arbitrary_Data_using_Joint_Embedding_Predictive_.pdf">Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">RPTU University Kaiserslautern-Landau</span></p>
<p>本文提出了一种基于Joint Embedding Predictive Architectures (JEPA) 和神经常微分方程（neural ODE）的新方法，将任意观测数据序列编码为连续时间的潜在状态空间，并通过引入收缩性和Lipschitz常数损失，保证状态空间的有序结构。方法在仿真摆系统中实现从图像数据直接学习动态系统潜在状态空间，实现了从潜在预测到图像重构，展示了对机器人控制和估计的广泛应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning_State-Space_Models_of_Dynamic_Systems_from_Arbitrary_Data_using_Joint_Embedding_Predictive_Architectures.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-Efficient-Methods-for-Accurate-Sparse-Trajectory-Recovery-and-Map-Matching"><a href="#23-Efficient-Methods-for-Accurate-Sparse-Trajectory-Recovery-and-Map-Matching" class="headerlink" title="23. Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Efficient_Methods_for_Accurate_Sparse_Trajectory_Recovery_and_Map_Matching.pdf">Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hong Kong Polytechnic University</span></p>
<p>本文提出了TRMMA和MMA两种高效且准确的稀疏轨迹恢复与地图匹配方法。MMA将GPS点匹配到道路段的任务转化为小候选集上的分类问题，并通过候选段与点的有效嵌入提升匹配精度，TRMMA进一步在MMA识别的路径上，通过双变换器（Dual-Transformer）编码捕捉轨迹和路径的潜在模式，利用解码过程预测缺失点的位置比例与道路段，实现高效轨迹恢复。实验表明，TRMMA与MMA在大规模真实数据集上均优于现有方法，且显著提升训练和推理效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Efficient_Methods_for_Accurate_Sparse_Trajectory_Recovery_and_Map_Matching.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-CorrectNav-Self-Correction-Flywheel-Empowers-Vision-Language-Action-Navigation-Model"><a href="#24-CorrectNav-Self-Correction-Flywheel-Empowers-Vision-Language-Action-Navigation-Model" class="headerlink" title="24. CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/CorrectNav__Self-Correction_Flywheel_Empowers_Vision-Language-Action_Navigation_Model.pdf">CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>该论文提出了Self-correction Flywheel（自我纠正飞轮）后训练范式，通过自动收集导航模型在训练集上的错误轨迹，检测偏差并生成自我纠正数据用于感知和动作校正，多轮迭代训练提升导航模型CorrectNav的自我纠错能力。实验在R2R-CE和RxR-CE等基准上取得了新的SOTA，并在实际机器人中验证了其优越的纠错、避障和长指令执行能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/CorrectNav_Self-Correction_Flywheel_Empowers_Vision-Language-Action_Navigation_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-Probabilistic-Latency-Analysis-of-the-Data-Distribution-Service-in-ROS-2"><a href="#25-Probabilistic-Latency-Analysis-of-the-Data-Distribution-Service-in-ROS-2" class="headerlink" title="25. Probabilistic Latency Analysis of the Data Distribution Service in ROS 2"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Probabilistic_Latency_Analysis_of_the_Data_Distribution_Service_in_ROS_2.pdf">Probabilistic Latency Analysis of the Data Distribution Service in ROS 2</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">DGIST</span></p>
<p>本文提出了一种针对ROS 2 DDS通信在有损无线网络下的概率延迟分析（PLA）模型。该方法通过离散状态建模与动态规划，系统性分析中间件层和传输层事件，预测未确认消息概率分布与重传延迟，并在270种场景下实验验证，其平均误差低于5%。结论指出PLA模型在可靠性、延迟和性能优化方面具备理论指导意义，能有效辅助无线工业机器人网络的QoS参数调优。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Probabilistic_Latency_Analysis_of_the_Data_Distribution_Service_in_ROS_2.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-Super-LiDAR-Reflectance-for-Robotic-Perception"><a href="#26-Super-LiDAR-Reflectance-for-Robotic-Perception" class="headerlink" title="26. Super LiDAR Reflectance for Robotic Perception"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Super_LiDAR_Reflectance_for_Robotic_Perception.pdf">Super LiDAR Reflectance for Robotic Perception</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Macau</span></p>
<p>该论文提出了一种创新框架，通过自研的深度卷积网络（含自适应融合模块和动态补偿模块）将低成本非重复扫描LiDAR的稀疏反射率数据实时转换为高密度、校准后的反射率图像。方法包括构建稀疏到稠密配对标注数据集、静态到动态域数据增强，以及反射率补偿，显著提升了低成本LiDAR在机器人闭环检测和车道线检测等感知任务中的实用性。结论表明，该方法兼具高准确性和实时性，极大拓展了低成本LiDAR在机器人主动视觉中的应用边界。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Super_LiDAR_Reflectance_for_Robotic_Perception.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-ReconVLA-Reconstructive-Vision-Language-Action-Model-as-Effective-Robot-Perceiver"><a href="#27-ReconVLA-Reconstructive-Vision-Language-Action-Model-as-Effective-Robot-Perceiver" class="headerlink" title="27. ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ReconVLA__Reconstructive_Vision-Language-Action_Model_as_Effective_Robot_Perceiver.pdf">ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>本论文提出了一种重构式视觉-语言-动作模型（ReconVLA），采用隐式视觉归因范式，通过扩散Transformer模块重建机器人操作目标的凝视区域，促进模型学习细粒度视觉表征和精准关注目标区域，提升操控精度。实验结果表明，该方法在模拟及真实环境中均优于显式和链式视觉归因方法，实现了更强的泛化和对未见目标的操控能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ReconVLA_Reconstructive_Vision-Language-Action_Model_as_Effective_Robot_Perceiver.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-Large-Model-Empowered-Embodied-AI-A-Survey-on-Decision-Making-and-Embodied-Learning"><a href="#28-Large-Model-Empowered-Embodied-AI-A-Survey-on-Decision-Making-and-Embodied-Learning" class="headerlink" title="28. Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Large_Model_Empowered_Embodied_AI__A_Survey_on_Decision-Making_and_Embodied_Learning.pdf">Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>本文系统综述了大模型赋能的具身智能（Embodied AI），聚焦于自主决策与具身学习。方法上，文章梳理了大模型在层次化决策（包括高层规划、低层执行、反馈增强）与端到端决策（基于VLA模型）、具身学习（模仿学习与强化学习）、世界模型等方面的最新进展与技术流程。结论指出，大模型极大提升了具身智能系统在感知、规划、执行与学习等环节的能力，但在数据稀缺、持续学习、部署效率及仿真-现实转移等方面仍有诸多挑战，未来需持续研究以实现更强的通用智能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Large_Model_Empowered_Embodied_AI_A_Survey_on_Decision-Making_and_Embodied_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-A-Semantic-Aware-Framework-for-Safe-and-Intent-Integrative-Assistance-in-Upper-Limb-Exoskeletons"><a href="#29-A-Semantic-Aware-Framework-for-Safe-and-Intent-Integrative-Assistance-in-Upper-Limb-Exoskeletons" class="headerlink" title="29. A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Semantic-Aware_Framework_for_Safe_and_Intent-Integrative_Assistance_in_Upper-Limb_Exoskeletons.pdf">A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了一种面向上肢外骨骼的语义感知辅助框架，将大语言模型（LLM）集成到任务规划中，通过提取任务语义信息和人类意图，自动配置辅助参数，并利用扩散模型实现异常检测与实时重规划。实验表明，该方法有效实现了人机认知对齐，提高了辅助任务的安全性、适应性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Semantic-Aware_Framework_for_Safe_and_Intent-Integrative_Assistance_in_Upper-Limb_Exoskeletons.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-JRDB-Reasoning-A-Difficulty-Graded-Benchmark-for-Visual-Reasoning-in-Robotics"><a href="#30-JRDB-Reasoning-A-Difficulty-Graded-Benchmark-for-Visual-Reasoning-in-Robotics" class="headerlink" title="30. JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/JRDB-Reasoning__A_Difficulty-Graded_Benchmark_for_Visual_Reasoning_in_Robotics.pdf">JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University</span></p>
<p>本论文提出JRDB-Reasoning基准，首次系统定义和量化视觉推理复杂度，并开发了自适应查询引擎，可按需生成不同难度、多步骤推理和中间标注的问题，扩展JRDB数据集以支持机器人在拥挤人群环境中的视觉推理任务。实验显示，现有VLM模型在推理难度提升时性能显著下降，而该基准和工具有助于精细化评估机器人视觉推理能力和未来模型发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/JRDB-Reasoning_A_Difficulty-Graded_Benchmark_for_Visual_Reasoning_in_Robotics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="31-GNN-based-Unified-Deep-Learning"><a href="#31-GNN-based-Unified-Deep-Learning" class="headerlink" title="31. GNN-based Unified Deep Learning"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GNN-based_Unified_Deep_Learning.pdf">GNN-based Unified Deep Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Imperial College London</span></p>
<p>本论文提出了一种基于图神经网络（GNN）的统一学习范式，将多种异构深度学习架构（如MLP、CNN和GNN）转化为图表示，并在统一的图学习空间中通过GNN进行协同优化，实现参数共享和知识转移。实验表明，该方法在医学图像领域面对分布漂移和域断裂场景时，显著提升了各类模型的泛化能力和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GNN-based_Unified_Deep_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-Quantum-Visual-Fields-with-Neural-Amplitude-Encoding"><a href="#32-Quantum-Visual-Fields-with-Neural-Amplitude-Encoding" class="headerlink" title="32. Quantum Visual Fields with Neural Amplitude Encoding"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Quantum_Visual_Fields_with_Neural_Amplitude_Encoding.pdf">Quantum Visual Fields with Neural Amplitude Encoding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">MPI for Informatics</span></p>
<p>本文提出了一种新型量子隐式神经表示（QINR）方法Quantum Visual Field (QVF)，用于2D图像与3D几何场的高效表示。其核心创新在于通过可学习的能量流形进行神经幅度编码，将经典数据嵌入量子态向量，并采用全纠缠设计的参数化量子电路在实希尔伯特空间内完成量子操作，保障数值稳定和快速收敛，无需经典后处理，直接通过投影测量提取信号。实验表明，QVF在视觉场表示精度和高频细节学习等方面均优于现有量子及主流经典基线方法，并首次实现了图像与3D形状集合的联合表示及应用（如图像修复、3D补全和插值），展示了其在量子增强视觉计算领域的实际潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Quantum_Visual_Fields_with_Neural_Amplitude_Encoding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data"><a href="#33-MAESTRO-Masked-AutoEncoders-for-Multimodal-Multitemporal-and-Multispectral-Earth-Observation-Data" class="headerlink" title="33. MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MAESTRO__Masked_AutoEncoders_for_Multimodal,_Multitemporal,_and_Multispectral_Earth_Observation_Data.pdf">MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institut national de l’information géographique et forestière (IGN), France</span></p>
<p>论文提出MAESTRO，一种针对地球观测（EO）数据的自监督学习方法，扩展了Masked Autoencoder（MAE）框架以支持多模态、多时序和多光谱输入。其核心技术包括基于token的早融合与晚融合策略，以及新颖的patch-group-wise归一化方法，有效注入光谱先验并提升表示能力。实验在四个遥感数据集上验证了方法，MAESTRO在依赖多时序动态的任务上达到SOTA，并在单时序任务上保持竞争力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MAESTRO_Masked_AutoEncoders_for_Multimodal_Multitemporal_and_Multispectral_Earth_Observation_Data.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing"><a href="#34-ToonComposer-Streamlining-Cartoon-Production-with-Generative-Post-Keyframing" class="headerlink" title="34. ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ToonComposer__Streamlining_Cartoon_Production_with_Generative_Post-Keyframing.pdf">ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong</span></p>
<p>本文提出了ToonComposer，一种基于Diffusion Transformer (DiT) 的生成式后关键帧(post-keyframing)模型，将卡通动画制作中的中间帧生成与上色步骤统一为单一流程。方法核心包括稀疏草图注入机制（sparse sketch injection）、区域控制(region-wise control)和空间低秩适配器（SLRA），实现对稀疏关键帧草图的精确控制，并高效迁移视频基础模型至卡通域。实验显示ToonComposer在视觉质量、运动连贯性与生产效率上均显著优于现有方法，大幅减少艺术家手动工作量，提升卡通视频生成的灵活性和质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ToonComposer_Streamlining_Cartoon_Production_with_Generative_Post-Keyframing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-Puppeteer-Rig-and-Animate-Your-3D-Models"><a href="#35-Puppeteer-Rig-and-Animate-Your-3D-Models" class="headerlink" title="35. Puppeteer: Rig and Animate Your 3D Models"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Puppeteer__Rig_and_Animate_Your_3D_Models.pdf">Puppeteer: Rig and Animate Your 3D Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>该论文提出了Puppeteer，一个集自动绑定骨骼与动画于一体的3D模型处理框架。方法包括利用自回归Transformer实现关节序列建模与骨架生成、拓扑感知注意力网络进行蒙皮权重预测，并通过可微分优化自动生成高质量动画。实验表明，Puppeteer在骨架预测、蒙皮精准度和动画稳定性方面优于现有方法，能高效处理多样3D内容。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Puppeteer_Rig_and_Animate_Your_3D_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-STREAM3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer"><a href="#36-STREAM3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer" class="headerlink" title="36. STREAM3R: Scalable Sequential 3D Reconstruction with Causal Transformer"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/STream3R__Scalable_Sequential_3D_Reconstruction_with_Causal_Transformer.pdf">STREAM3R: Scalable Sequential 3D Reconstruction with Causal Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>本文提出STREAM3R，将3D重建任务重构为基于因果注意力的解码器Transformer问题，实现对流式输入图像的高效顺序处理。通过缓存历史帧特征并逐帧递增融合，STREAM3R具备优异的扩展性和泛化能力，在静态与动态场景的深度估计及3D重建任务上均优于现有方法，且推理速度显著提升，推动实时3D理解发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/STREAM3R_Scalable_Sequential_3D_Reconstruction_with_Causal_Transformer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-Medico-2025-Visual-Question-Answering-for-Gastrointestinal-Imaging"><a href="#37-Medico-2025-Visual-Question-Answering-for-Gastrointestinal-Imaging" class="headerlink" title="37. Medico 2025: Visual Question Answering for Gastrointestinal Imaging"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Medico_2025__Visual_Question_Answering_for_Gastrointestinal_Imaging.pdf">Medico 2025: Visual Question Answering for Gastrointestinal Imaging</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">SimulaMet - Simula Metropolitan Center for Digital Engineering</span></p>
<p>该论文提出了Medico 2025挑战，旨在推动面向胃肠道影像的可解释视觉问答（VQA）模型的发展，要求AI模型不仅准确回答医学影像中的临床问题，还需生成与医学推理一致的多模态解释。研究基于Kvasir-VQA-x1数据集，通过量化指标和专家审核结合评估AI系统的准确性和解释性，促进可被临床采用的可信医疗AI系统发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Medico_2025_Visual_Question_Answering_for_Gastrointestinal_Imaging.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-AEGIS-Authenticity-Evaluation-Benchmark-for-AI-Generated-Video-Sequences"><a href="#38-AEGIS-Authenticity-Evaluation-Benchmark-for-AI-Generated-Video-Sequences" class="headerlink" title="38. AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/AEGIS__Authenticity_Evaluation_Benchmark_for_AI-Generated_Video_Sequences.pdf">AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Singapore</span></p>
<p>该论文提出了AEGIS，一个专为AI生成视频真伪检测设计的大规模基准数据集，包含来自多种先进生成模型（如Stable Video Diffusion、CogVideoX-5B、KLing和Sora）的5,199个高真实性和语义复杂的合成视频，并配有丰富的多模态标注（语义描述、运动特征、低层视觉特征）。实验表明，现有视觉语言模型（如Qwen-VL、Video-LLaVA）在AEGIS的高难度测试集上表现有限，突出当前检测模型在真实环境中面临的泛化和鲁棒性挑战，强调了AEGIS在推动视频真伪检测研究中的基础性作用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AEGIS_Authenticity_Evaluation_Benchmark_for_AI-Generated_Video_Sequences.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-Natively-Trainable-Sparse-Attention-for-Hierarchical-Point-Cloud-Datasets"><a href="#39-Natively-Trainable-Sparse-Attention-for-Hierarchical-Point-Cloud-Datasets" class="headerlink" title="39. Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Natively_Trainable_Sparse_Attention_for_Hierarchical_Point_Cloud_Datasets.pdf">Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Groningen</span></p>
<p>本文提出将Erwin层次化Transformer架构与Native Sparse Attention(NSA)机制结合，针对物理科学中的大规模点云数据，提出Erwin NSA模型，通过将NSA算法从序列数据扩展到非序列点云，并替换局部注意力机制，实现了更高效的全局感受野建模。实验在宇宙学模拟、分子动力学和空气压力建模三大数据集上，表明Erwin NSA在ShapeNet和分子动力学任务中优于原Erwin模型，在宇宙学任务上表现持平，且具有更高的训练效率和更好的节点信息流动能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Natively_Trainable_Sparse_Attention_for_Hierarchical_Point_Cloud_Datasets.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-Exploiting-Discriminative-Codebook-Prior-for-Autoregressive-Image-Generation"><a href="#40-Exploiting-Discriminative-Codebook-Prior-for-Autoregressive-Image-Generation" class="headerlink" title="40. Exploiting Discriminative Codebook Prior for Autoregressive Image Generation"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Exploiting_Discriminative_Codebook_Prior_for_Autoregressive_Image_Generation.pdf">Exploiting Discriminative Codebook Prior for Autoregressive Image Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>该论文提出了Discriminative Codebook Prior Extractor (DCPE)，通过聚合聚类和实例距离替代传统k-means聚类，从分布不均且高维的代码本令牌特征空间中更有效地挖掘令牌相似性信息，用于提升离散令牌自回归图像生成模型的训练。实验表明，DCPE作为即插即用方法能加速训练、提升生成质量，并优于现有k-means方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Exploiting_Discriminative_Codebook_Prior_for_Autoregressive_Image_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-Continuous-Bangla-Sign-Language-Translation-Mitigating-the-Expense-of-Gloss-Annotation-with-the-Assistance-of-Graph"><a href="#41-Continuous-Bangla-Sign-Language-Translation-Mitigating-the-Expense-of-Gloss-Annotation-with-the-Assistance-of-Graph" class="headerlink" title="41. Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Continuous_Bangla_Sign_Language_Translation__Mitigating_the_Expense_of_Gloss_Annotation_with_the_Ass.pdf">Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Dhaka</span></p>
<p>该论文提出了融合Transformer与STGCN-LSTM架构的连续手语翻译方法，通过多模态数据融合（RGB视频与骨骼关键点），实现了无需gloss注释的端到端手语到文本翻译。该方法在多个公开手语数据集（RWTH-PHOENIX-2014T、CSL-Daily、How2Sign、BornilDB v1.0）上取得了新的gloss-free翻译性能最优，显著提升了BLEU分数，为聋人群体的交流无障碍提供了技术基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Continuous_Bangla_Sign_Language_Translation_Mitigating_the_Expense_of_Gloss_Annotation_with_the_Assistance_of_Graph.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-Hybrid-Generative-Fusion-for-Efficient-and-Privacy-Preserving-Face-Recognition-Dataset-Generation"><a href="#42-Hybrid-Generative-Fusion-for-Efficient-and-Privacy-Preserving-Face-Recognition-Dataset-Generation" class="headerlink" title="42. Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Hybrid_Generative_Fusion_for_Efficient_and_Privacy-Preserving_Face_Recognition_Dataset_Generation.pdf">Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>本文提出了混合生成融合方法，通过Mixture-of-Experts数据清洗（聚类+GPT-4o验证）、Stable Diffusion与Vec2Face生成新身份，并用课程学习策略优化训练流程，实现高效、隐私保护的人脸识别数据集构建。实验表明，所生成的纯合成数据集在多规模下均提升了人脸识别模型性能，并在竞赛中取得第一名。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Hybrid_Generative_Fusion_for_Efficient_and_Privacy-Preserving_Face_Recognition_Dataset_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-Geospatial-Diffusion-for-Land-Cover-Imperviousness-Change-Forecasting"><a href="#43-Geospatial-Diffusion-for-Land-Cover-Imperviousness-Change-Forecasting" class="headerlink" title="43. Geospatial Diffusion for Land Cover Imperviousness Change Forecasting"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Geospatial_Diffusion_for_Land_Cover_Imperviousness_Change_Forecasting.pdf">Geospatial Diffusion for Land Cover Imperviousness Change Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Oak Ridge National Laboratory</span></p>
<p>本文提出一种基于生成扩散模型（Diffusion Models）的地表覆盖不透水性变化预测方法，将土地利用&#x2F;覆盖（LULC）变化预测视为条件数据合成任务，利用历史LULC及辅助数据（如过渡概率图）进行建模。实验表明，该方法在美国12个大都市区域实现了亚公里级别的预测精度，优于传统的无变化基线模型，能够有效捕捉历史数据中的时空发展模式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Geospatial_Diffusion_for_Land_Cover_Imperviousness_Change_Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera"><a href="#44-Processing-and-acquisition-traces-in-visual-encoders-What-does-CLIP-know-about-your-camera" class="headerlink" title="44. Processing and acquisition traces in visual encoders: What does CLIP know about your camera?"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Processing_and_acquisition_traces_in_visual_encoders__What_does_CLIP_know_about_your_camera_.pdf">Processing and acquisition traces in visual encoders: What does CLIP know about your camera?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Osaka</span></p>
<p>本论文系统分析了视觉编码器（如CLIP）在表征空间中对图片处理和采集参数（如JPEG压缩、相机型号等元数据）的敏感性，通过分类和检索任务展示这些元数据会被模型学习并影响语义预测。结论表明，尤其是视觉-语言对比模型（CVL），其表征空间中元数据痕迹可能会干扰下游语义任务，降低模型鲁棒性和泛化能力，且模型训练时是否采用强数据增强是影响元数据敏感性的关键因素。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP_know_about_your_camera.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-Object-Fidelity-Diffusion-for-Remote-Sensing-Image-Generation"><a href="#45-Object-Fidelity-Diffusion-for-Remote-Sensing-Image-Generation" class="headerlink" title="45. Object Fidelity Diffusion for Remote Sensing Image Generation"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Object_Fidelity_Diffusion_for_Remote_Sensing_Image_Generation.pdf">Object Fidelity Diffusion for Remote Sensing Image Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本文提出了Object Fidelity Diffusion（OF-Diff），一种用于遥感图像生成的双分支可控扩散模型，通过引入增强形状生成模块（ESGM）从布局中提取对象先验形状，并结合扩散一致性损失与DDPO微调提升图像多样性与语义一致性。实验表明，OF-Diff能在无需真实图像参考的条件下生成高保真、布局与形状一致的遥感图像，显著提升小目标检测性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Object_Fidelity_Diffusion_for_Remote_Sensing_Image_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-VIDEO-BLADE-BLOCK-SPARSE-ATTENTION-MEETS-STEP-DISTILLATION-FOR-EFFICIENT-VIDEO-GENERATION"><a href="#46-VIDEO-BLADE-BLOCK-SPARSE-ATTENTION-MEETS-STEP-DISTILLATION-FOR-EFFICIENT-VIDEO-GENERATION" class="headerlink" title="46. VIDEO-BLADE: BLOCK-SPARSE ATTENTION MEETS STEP DISTILLATION FOR EFFICIENT VIDEO GENERATION"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Video-BLADE__Block-Sparse_Attention_Meets_Step_Distillation_for_Efficient_Video_Generation.pdf">VIDEO-BLADE: BLOCK-SPARSE ATTENTION MEETS STEP DISTILLATION FOR EFFICIENT VIDEO GENERATION</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了Video-BLADE框架，通过自适应块稀疏注意力（ASA）机制与基于轨迹分布匹配（TDM）的稀疏感知步蒸馏联合训练，实现高效视频扩散生成。ASA动态生成内容感知稀疏掩码，联合稀疏感知蒸馏提升推理速度，同时保持乃至提升视频生成质量。实验表明，在CogVideoX-5B和Wan2.1-1.3B等主流模型上，BLADE可实现8.89×至14.10×加速，且在VBench-2.0等基准上取得更高分数，兼顾效率与质量，且人类评测同样认可其优异表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/VIDEO-BLADE_BLOCK-SPARSE_ATTENTION_MEETS_STEP_DISTILLATION_FOR_EFFICIENT_VIDEO_GENERATION.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-From-Diagnosis-to-Improvement-Probing-Spatio-Physical-Reasoning-in-Vision-Language-Models"><a href="#47-From-Diagnosis-to-Improvement-Probing-Spatio-Physical-Reasoning-in-Vision-Language-Models" class="headerlink" title="47. From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/From_Diagnosis_to_Improvement__Probing_Spatio-Physical_Reasoning_in_Vision_Language_Models.pdf">From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tongji University</span></p>
<p>本文系统分析了主流视觉语言模型（VLM）在时空物理推理任务中的表现，发现其在基于ShapeStacks基准的静态平衡判断上准确率低，存在类人认知偏见和缺乏深度推理。作者提出采用监督微调（SFT）结合规则驱动的强化学习（RL）对Qwen2.5-VL-7B模型进行二阶段训练，显著提升了其领域内物理推理能力并超越了主流商用模型，但在跨场景泛化能力上仍表现有限，暴露出当前范式以模式匹配为主、难以习得可迁移物理原则的根本局限。结论：主流VLM在物理推理方面存在明显局限，二阶段微调虽可提升领域内性能但泛化不足，未来应发展基于物理因果机制的新范式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From_Diagnosis_to_Improvement_Probing_Spatio-Physical_Reasoning_in_Vision_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-Privacy-enhancing-Sclera-Segmentation-Benchmarking-Competition-SSBC-2025"><a href="#48-Privacy-enhancing-Sclera-Segmentation-Benchmarking-Competition-SSBC-2025" class="headerlink" title="48. Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Privacy-enhancing_Sclera_Segmentation_Benchmarking_Competition__SSBC_2025.pdf">Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Ljubljana</span></p>
<p>本论文介绍了SSBC 2025隐私增强型巩膜分割竞赛，重点在于利用合成眼部图像训练隐私保护的巩膜分割模型，并系统比较了合成数据和真实数据混合训练的模型性能。结果表明，专用训练策略下，仅用合成数据训练的模型能取得与真实数据训练相近的分割效果，F1分数可达0.8以上，且方法选择往往比真实数据的加入更关键，突出合成数据在隐私保护生物识别中的可行性和前景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Privacy-enhancing_Sclera_Segmentation_Benchmarking_Competition_SSBC_2025.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale"><a href="#49-NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale" class="headerlink" title="49. NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/NextStep-1__Toward_Autoregressive_Image_Generation_with_Continuous_Tokens_at_Scale.pdf">NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">StepFun</span></p>
<p>该论文提出NextStep-1，一种结合大规模Transformer主干和流匹配头的自回归模型，实现文本到高保真图像生成和多样化图像编辑。方法创新在于同时处理离散文本和连续图像token，并引入强正则化的图像tokenizer以保证分布稳定，实验结果显示其在多项主流基准上超越现有自回归模型，部分指标接近扩散模型，验证了自回归范式在高质量图像生成上的可行性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/NextStep-1_Toward_Autoregressive_Image_Generation_with_Continuous_Tokens_at_Scale.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-Novel-View-Synthesis-using-DDIM-Inversion"><a href="#50-Novel-View-Synthesis-using-DDIM-Inversion" class="headerlink" title="50. Novel View Synthesis using DDIM Inversion"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Novel_View_Synthesis_using_DDIM_Inversion.pdf">Novel View Synthesis using DDIM Inversion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Indraprastha Institute of Information Technology, Delhi</span></p>
<p>该论文提出了一种基于DDIM倒置和轻量化TUNet网络的单张图像高质量新视角合成方法。流程包括将输入图像通过VAE和DDIM倒置获得潜变量，使用TUNet进行视角转换，结合噪声融合策略强化高频细节，最后借助预训练扩散模型解码生成新视角图像。实验表明方法在MVImgNet等数据集上优于主流方法，能生成高分辨率且几何一致的新视角图像。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Novel_View_Synthesis_using_DDIM_Inversion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-Conditional-Information-Bottleneck-for-Multimodal-Fusion-Overcoming-Shortcut-Learning-in-Sarcasm-Detection"><a href="#51-Conditional-Information-Bottleneck-for-Multimodal-Fusion-Overcoming-Shortcut-Learning-in-Sarcasm-Detection" class="headerlink" title="51. Conditional Information Bottleneck for Multimodal Fusion: Overcoming Shortcut Learning in Sarcasm Detection"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Conditional_Information_Bottleneck_for_Multimodal_Fusion__Overcoming_Shortcut_Learning_in_Sarcasm_De.pdf">Conditional Information Bottleneck for Multimodal Fusion: Overcoming Shortcut Learning in Sarcasm Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, Beijing Normal University</span></p>
<p>本文针对多模态讽刺检测任务中模型容易依赖数据集捷径（如角色标签、罐头笑声、情感不一致性）导致泛化能力差的问题，重构了MUStARD++数据集，去除了相关捷径信号，并提出了多模态条件信息瓶颈（MCIB）融合模型。MCIB通过条件信息瓶颈机制对多模态（文本、音频、视觉）特征进行冗余过滤与互补信息提取，实现高效融合，显著提升了在讽刺检测任务中的表现和泛化能力，实验结果在多个基准数据集上达到最优。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Conditional_Information_Bottleneck_for_Multimodal_Fusion_Overcoming_Shortcut_Learning_in_Sarcasm_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-ChatENV-An-Interactive-Vision-Language-Model-for-Sensor-Guided-Environmental-Monitoring-and-Scenario-Simulation"><a href="#52-ChatENV-An-Interactive-Vision-Language-Model-for-Sensor-Guided-Environmental-Monitoring-and-Scenario-Simulation" class="headerlink" title="52. ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ChatENV__An_Interactive_Vision-Language_Model_for_Sensor-Guided_Environmental_Monitoring_and_Scenari.pdf">ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mohamed bin Zayed University of Artificial Intelligence</span></p>
<p>本文提出ChatENV，一个结合卫星遥感图像与真实环境传感器数据的交互式视觉-语言模型。通过构建涵盖全球177k图像、152k时序对和丰富气象&#x2F;排放元数据的大规模数据集，并利用GPT-4o和Gemini 2.0生成多样注释，模型采用Qwen-2.5-VL为主干，LoRA高效微调，实现场景描述、时空变化推理及“假设情景”模拟。实验结果表明，ChatENV在时序推理和情景分析等任务上表现优异，显著提升了环境监测的智能化和实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ChatENV_An_Interactive_Vision-Language_Model_for_Sensor-Guided_Environmental_Monitoring_and_Scenario_Simulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-Fourier-Guided-Attention-Upsampling-for-Image-Super-Resolution"><a href="#53-Fourier-Guided-Attention-Upsampling-for-Image-Super-Resolution" class="headerlink" title="53. Fourier-Guided Attention Upsampling for Image Super-Resolution"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Fourier-Guided_Attention_Upsampling_for_Image_Super-Resolution.pdf">Fourier-Guided Attention Upsampling for Image Super-Resolution</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea University of Technology and Education (KOREATECH)</span></p>
<p>本文提出了一种轻量级的频域引导注意力上采样模块FGA，用于单幅图像超分辨率。该方法通过结合基于傅里叶特征的多层感知机(MLP)进行位置频率编码，跨分辨率相关注意力层进行空间自适应对齐，以及频域L1损失进行频谱一致性监督，在多种主流SISR骨干网络中均提升了重建高频细节和抑制伪影的能力。实验表明，FGA仅增加0.3M参数即可在五个不同超分辨率主干中获得平均0.12–0.14dB的PSNR提升，并在高频内容一致性上提升高达29%，在纹理丰富数据集上尤为显著，证明其在减少混叠和保持细节方面的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Fourier-Guided_Attention_Upsampling_for_Image_Super-Resolution.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-Towards-Agentic-AI-for-Multimodal-Guided-Video-Object-Segmentation"><a href="#54-Towards-Agentic-AI-for-Multimodal-Guided-Video-Object-Segmentation" class="headerlink" title="54. Towards Agentic AI for Multimodal-Guided Video Object Segmentation"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Towards_Agentic_AI_for_Multimodal-Guided_Video_Object_Segmentation.pdf">Towards Agentic AI for Multimodal-Guided Video Object Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Deakin University</span></p>
<p>本文提出M2-Agent，一种用于多模态指导下视频目标分割的免训练智能体系统。该方法利用大型语言模型（LLM）生成每个输入的动态推理流程，并通过与专门的视觉和音频工具集的迭代交互，灵活识别文本和音频描述的目标对象。实验结果表明，M2-Agent在Referring Video Object Segmentation（RVOS）和Reference Audio-Visual Segmentation（Ref-AVS）两项任务上，相较于现有训练自由方法有明显性能提升，且接近有监督最优方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Towards_Agentic_AI_for_Multimodal-Guided_Video_Object_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-GCRPNet-Graph-Enhanced-Contextual-and-Regional-Perception-Network-For-Salient-Object-Detection-in-Optical-Remote-Sensing-Images"><a href="#55-GCRPNet-Graph-Enhanced-Contextual-and-Regional-Perception-Network-For-Salient-Object-Detection-in-Optical-Remote-Sensing-Images" class="headerlink" title="55. GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GCRPNet__Graph-Enhanced_Contextual_and_Regional_Perception_Network_For_Salient_Object_Detection_in_O.pdf">GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hainan University</span></p>
<p>该论文提出了一种基于Mamba架构的光学遥感图像显著性目标检测新方法GCRPNet，集成了视觉状态空间编码器、差异-相似性引导分层图注意模块（DS-HGAM）和局部增强视觉状态空间（LEVSS）解码器。DS-HGAM利用图神经网络实现多尺度特征融合和跨空间建模，LEVSS结合多尺度协同注意力和块内四方向扫描提升局部细节表示。实验结果表明，GCRPNet在ORSSD和EORSSD数据集上显著优于现有方法，具备更强的鲁棒性和精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GCRPNet_Graph-Enhanced_Contextual_and_Regional_Perception_Network_For_Salient_Object_Detection_in_Optical_Remote_Sensing_Images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-PASS-Probabilistic-Agentic-Supernet-Sampling-for-Interpretable-and-Adaptive-Chest-X-Ray-Reasoning"><a href="#56-PASS-Probabilistic-Agentic-Supernet-Sampling-for-Interpretable-and-Adaptive-Chest-X-Ray-Reasoning" class="headerlink" title="56. PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/PASS__Probabilistic_Agentic_Supernet_Sampling_for_Interpretable_and_Adaptive_Chest_X-Ray_Reasoning.pdf">PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Hong Kong</span></p>
<p>该论文提出PASS框架，通过概率化的agentic supernet采样实现胸部X光的多模态医学推理，核心包括动态采样多工具工作流并用可解释概率标注决策路径。方法包含三阶段训练：专家知识热身、对比路径排序和成本感知强化学习，有效优化准确率与计算成本权衡。实验表明PASS在多项医学推理基准上显著优于主流模型，在保证高准确率的同时实现高可解释性和效率，可广泛用于高安全性要求的医学场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PASS_Probabilistic_Agentic_Supernet_Sampling_for_Interpretable_and_Adaptive_Chest_X-Ray_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-X-Node-Self-Explanation-is-All-We-Need"><a href="#57-X-Node-Self-Explanation-is-All-We-Need" class="headerlink" title="57. X-Node: Self-Explanation is All We Need"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/X-Node__Self-Explanation_is_All_We_Need.pdf">X-Node: Self-Explanation is All We Need</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Imperial College London</span></p>
<p>该论文提出了一种自解释型图神经网络框架X-Node，在节点分类过程中每个节点能生成自身解释，方法包括构建结构化上下文向量、解释向量生成、LLM文本解释和解释反馈注入GNN流程。实验证明X-Node在MedMNIST和MorphoMNIST图数据集上保持竞争分类性能并生成每节点可解释决策，有效提升模型透明度和临床应用信任度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/X-Node_Self-Explanation_is_All_We_Need.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-Multi-Label-Plant-Species-Prediction-with-Metadata-Enhanced-Multi-Head-Vision-Transformers"><a href="#58-Multi-Label-Plant-Species-Prediction-with-Metadata-Enhanced-Multi-Head-Vision-Transformers" class="headerlink" title="58. Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Multi-Label_Plant_Species_Prediction_with_Metadata-Enhanced_Multi-Head_Vision_Transformers.pdf">Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Hamburg</span></p>
<p>本论文提出了一种基于多头Vision Transformer（ViT）的多标签植物种类预测方法，结合了DINOv2预训练模型、物种-属-科多头分类、元数据增强、多尺度切片、动态阈值优化和集成策略（Hydra架构），以实现对高分辨率植被样方图像中多种植物的自动识别。实验结果显示，该方法在PlantCLEF 2025挑战赛的公共榜单中取得了第三名的成绩，但在私有榜单上表现下降，说明模型对领域转移较为敏感但仍具竞争力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Multi-Label_Plant_Species_Prediction_with_Metadata-Enhanced_Multi-Head_Vision_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-From-Images-to-Perception-Emergence-of-Perceptual-Properties-by-Reconstructing-Images"><a href="#59-From-Images-to-Perception-Emergence-of-Perceptual-Properties-by-Reconstructing-Images" class="headerlink" title="59. From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/From_Images_to_Perception__Emergence_of_Perceptual_Properties_by_Reconstructing_Images.pdf">From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Valencia</span></p>
<p>本论文提出并训练了一种生物启发的视觉系统神经网络PerceptNet，通过自监督学习（包括自编码、去噪、去模糊和稀疏正则化任务）重建自然图像，研究模型内部表征与人类感知一致性的涌现现象。实验表明，在编码器（类V1皮层）阶段，模型表征与人类主观失真评价高度相关，适度的噪声、模糊和稀疏度可增强这种一致性，说明生物视觉系统可能正是优化了去除适度失真和稀疏性，且无需人工感知标签即可学得感知度量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From_Images_to_Perception_Emergence_of_Perceptual_Properties_by_Reconstructing_Images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-MM-Food-100K-A-100-000-Sample-Multimodal-Food-Intelligence-Dataset-with-Verifiable-Provenance"><a href="#60-MM-Food-100K-A-100-000-Sample-Multimodal-Food-Intelligence-Dataset-with-Verifiable-Provenance" class="headerlink" title="60. MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MM-Food-100K__A_100,000-Sample_Multimodal_Food_Intelligence_Dataset_with_Verifiable_Provenance.pdf">MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Codatta Community</span></p>
<p>该论文提出了MM-Food-100K，一种包含10万条样本的多模态食物智能数据集，通过社区贡献与大型视觉语言模型自动质量审核结合的数据采集流程，提供丰富、多层次的注释及可验证溯源。实验表明，基于MM-Food-100K微调的模型在食物分类与营养回归任务上均优于原始视觉语言模型，且采用区块链协议实现数据溯源与贡献者分润机制，为高质量社区数据集的可持续建设提供了新范式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MM-Food-100K_A_100_000-Sample_Multimodal_Food_Intelligence_Dataset_with_Verifiable_Provenance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-Adapting-SAM-via-Cross-Entropy-Masking-for-Class-Imbalance-in-Remote-Sensing-Change-Detection"><a href="#61-Adapting-SAM-via-Cross-Entropy-Masking-for-Class-Imbalance-in-Remote-Sensing-Change-Detection" class="headerlink" title="61. Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Adapting_SAM_via_Cross-Entropy_Masking_for_Class_Imbalance_in_Remote_Sensing_Change_Detection.pdf">Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Sydney</span></p>
<p>该论文提出了基于Segment Anything Model (SAM) 的遥感变化检测框架，通过细致地微调FastSAM编码器，并引入空间-时序特征增强（STFE）和多尺度解码器融合（MSDF），有效提升了多尺度下的变化检测能力。针对变化检测中的类别不平衡问题，创新性地提出了交叉熵掩码损失（CEM），通过在损失计算中随机丢弃未变化像素，显著提升了模型对变化类别的检测能力。实验结果表明，所提方法在Levir-CD、WHU-CD、CLCD和S2Looking等四个变化检测数据集上均取得了优于现有方法的性能，尤其在S2Looking数据集上F1提升2.5%。结论是结合SAM密集预测特性和CEM损失的架构在遥感变化检测任务上优于现有主流方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Adapting_SAM_via_Cross-Entropy_Masking_for_Class_Imbalance_in_Remote_Sensing_Change_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-PTQAT-A-Hybrid-Parameter-Efficient-Quantization-Algorithm-for-3D-Perception-Tasks"><a href="#62-PTQAT-A-Hybrid-Parameter-Efficient-Quantization-Algorithm-for-3D-Perception-Tasks" class="headerlink" title="62. PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/PTQAT__A_Hybrid_Parameter-Efficient_Quantization_Algorithm_for_3D_Perception_Tasks.pdf">PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>该论文提出PTQAT，一种结合后训练量化（PTQ）和量化感知训练（QAT）的高效混合量化算法，专为3D感知任务设计。方法首先通过PTQ预检查各层量化误差，采用均方误差（MSE）筛选需QAT微调的关键层，其余层保持冻结，重点在误差传播阶段进行补偿。结论表明，PTQAT在3D目标检测、语义分割、占用预测等任务上，能以更少参数微调和更高效率，实现与甚至优于全量QAT的精度，同时便于实际部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PTQAT_A_Hybrid_Parameter-Efficient_Quantization_Algorithm_for_3D_Perception_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-EgoMusic-driven-Human-Dance-Motion-Estimation-with-Skeleton-Mamba"><a href="#63-EgoMusic-driven-Human-Dance-Motion-Estimation-with-Skeleton-Mamba" class="headerlink" title="63. EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/EgoMusic-driven_Human_Dance_Motion_Estimation_with_Skeleton_Mamba.pdf">EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">FPT Software AI Center</span></p>
<p>本文提出EgoMusic Motion Network和核心Skeleton Mamba模型，实现从第一视角视频和音乐共同驱动的人类舞蹈动作估计。方法包括新型大规模EgoAIST++数据集、多模态特征融合、条件扩散模型和空间-时间扫描策略，实验显示在多项舞蹈动作估计指标上明显优于现有方法，并能泛化至实际场景。结论：联合利用视觉和音乐输入可大幅提升人体舞蹈动作推断的准确性和协调性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/EgoMusic-driven_Human_Dance_Motion_Estimation_with_Skeleton_Mamba.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-TweezeEdit-Consistent-and-Efficient-Image-Editing-with-Path-Regularization"><a href="#64-TweezeEdit-Consistent-and-Efficient-Image-Editing-with-Path-Regularization" class="headerlink" title="64. TweezeEdit: Consistent and Efficient Image Editing with Path Regularization"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/TweezeEdit__Consistent_and_Efficient_Image_Editing_with_Path_Regularization.pdf">TweezeEdit: Consistent and Efficient Image Editing with Path Regularization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>本文提出TweezeEdit，一种基于一致性模型的无调优、无反演路径正则化图像编辑框架，针对扩散模型下现有方法过度对齐目标提示、难以保留源图像语义的问题。该方法通过全程正则化去噪路径并引入梯度驱动的路径约束，在无需模型结构修改的情况下，提高语义保留和编辑效率。实验结果显示，TweezeEdit在语义一致性、目标对齐及推理速度上优于现有主流方法，可在12步内完成编辑，具备实时应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/TweezeEdit_Consistent_and_Efficient_Image_Editing_with_Path_Regularization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-STAMP-Multi-pattern-Attention-aware-Multiple-Instance-Learning-for-STAS-Diagnosis-in-Multi-center-Histopathology-Images"><a href="#65-STAMP-Multi-pattern-Attention-aware-Multiple-Instance-Learning-for-STAS-Diagnosis-in-Multi-center-Histopathology-Images" class="headerlink" title="65. STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/STAMP__Multi-pattern_Attention-aware_Multiple_Instance_Learning_for_STAS_Diagnosis_in_Multi-center_H.pdf">STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hunan University</span></p>
<p>该论文提出STAMP方法，通过双分支Transformer架构和多模式注意力聚合模块，分析多中心肺癌组织病理图像中的STAS特征，实现病灶区域的动态选择和噪声抑制，提升全局表征的判别力。实验表明STAMP在三大多中心数据集上AUC均超过0.79，诊断性能优于临床水平，具有临床辅助诊断潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/STAMP_Multi-pattern_Attention-aware_Multiple_Instance_Learning_for_STAS_Diagnosis_in_Multi-center_Histopathology_Images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-NanoControl-A-Lightweight-Framework-for-Precise-and-Efficient-Control-in-Diffusion-Transformer"><a href="#66-NanoControl-A-Lightweight-Framework-for-Precise-and-Efficient-Control-in-Diffusion-Transformer" class="headerlink" title="66. NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/NanoControl__A_Lightweight_Framework_for_Precise_and_Efficient_Control_in_Diffusion_Transformer.pdf">NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">360 AI Research</span></p>
<p>本文提出NanoControl——一种基于Diffusion Transformer（DiT）的轻量级可控文本到图像生成方法。其创新点在于采用LoRA风格控制模块和KV-Context Augmentation机制，极大减少参数与计算量，仅增加0.024%参数和0.029% GFLOPs，实现高效条件融合与精确可控生成。实验表明，NanoControl在多项条件控制任务（如边缘、深度、着色、HED）上实现了优于现有方法的可控性和图像质量，同时大幅降低模型复杂度，非常适合实际部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/NanoControl_A_Lightweight_Framework_for_Precise_and_Efficient_Control_in_Diffusion_Transformer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-Towards-Spatially-Consistent-Image-Generation-On-Incorporating-Intrinsic-Scene-Properties-into-Diffusion-Models"><a href="#67-Towards-Spatially-Consistent-Image-Generation-On-Incorporating-Intrinsic-Scene-Properties-into-Diffusion-Models" class="headerlink" title="67. Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Towards_Spatially_Consistent_Image_Generation__On_Incorporating_Intrinsic_Scene_Properties_into_Diff.pdf">Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AI Institute, Seoul National University</span></p>
<p>本文提出了Intrinsic Latent Diffusion Model (I-LDM)，通过联合生成图片和其对应的场景内在属性（深度、法线、分割图、线描），提升文本到图像生成模型的空间一致性。方法利用预训练估算器自动提取场景属性，通过自注意力跨域调度和VAE编码实现高效协同生成。实验显示I-LDM在多种基线模型和复杂场景下均显著改善了空间布局的自然性和一致性，同时保持了原始模型的图像质量和文本匹配能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Towards_Spatially_Consistent_Image_Generation_On_Incorporating_Intrinsic_Scene_Properties_into_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning"><a href="#68-SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning" class="headerlink" title="68. SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SynBrain__Enhancing_Visual-to-fMRI_Synthesis_via_Probabilistic_Representation_Learning.pdf">SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Artificial Intelligence Laboratory</span></p>
<p>该论文提出了SynBrain框架，通过引入概率建模与语义条件约束，采用BrainVAE和S2N Mapper关键模块，实现视觉刺激到fMRI脑响应的概率分布建模和一步语义到神经映射。实验结果表明，SynBrain在个体特异性编码和少样本自适应性能上显著优于现有方法，并生成高质量fMRI信号提升神经解码效果，同时揭示脑功能的一致性和生物神经变异性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SynBrain_Enhancing_Visual-to-fMRI_Synthesis_via_Probabilistic_Representation_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-DINOMotion-advanced-robust-tissue-motion-tracking-with-DINOv2-in-2D-Cine-MRI-guided-radiotherapy"><a href="#69-DINOMotion-advanced-robust-tissue-motion-tracking-with-DINOv2-in-2D-Cine-MRI-guided-radiotherapy" class="headerlink" title="69. DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/DINOMotion__advanced_robust_tissue_motion_tracking_with_DINOv2_in_2D-Cine_MRI-guided_radiotherapy.pdf">DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Concordia University</span></p>
<p>本文提出DINOMotion，一种基于DINOv2基础视觉模型和Low-Rank Adaptation (LoRA) 层的深度学习框架，用于2D-Cine MRI引导放疗中的组织运动追踪。该方法自动检测对应标志点以实现高效和可解释的图像配准，并能处理大幅度错位情况，在多个器官的Dice分数和Hausdorff距离评测中均优于现有方法，具有较强的实时性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/DINOMotion_advanced_robust_tissue_motion_tracking_with_DINOv2_in_2D-Cine_MRI-guided_radiotherapy.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-SC-Lane-Slope-aware-and-Consistent-Road-Height-Estimation-Framework-for-3D-Lane-Detection"><a href="#70-SC-Lane-Slope-aware-and-Consistent-Road-Height-Estimation-Framework-for-3D-Lane-Detection" class="headerlink" title="70. SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SC-Lane__Slope-aware_and_Consistent_Road_Height_Estimation_Framework_for_3D_Lane_Detection.pdf">SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Seoul National University</span></p>
<p>本文提出了SC-Lane，一种面向坡度且具有时序一致性的3D车道检测道路高度估计框架。方法核心包括坡度自适应特征模块，动态融合多坡度高度特征，并引入高度一致性模块，通过利用图像线索自适应加权及时序一致性损失，提升多帧高度估计的鲁棒性和准确性。实验采用MAE、RMSE和阈值精度等新标准，OpenLane基准上SC-Lane在高度估计和3D车道检测上均达到SOTA，F-score 64.3%。结论表明该方法提升了车道检测的准确性和高度估计的稳定性，可扩展至自动驾驶相关任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SC-Lane_Slope-aware_and_Consistent_Road_Height_Estimation_Framework_for_3D_Lane_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-Translation-of-Text-Embedding-via-Delta-Vector-to-Suppress-Strongly-Entangled-Content-in-Text-to-Image-Diffusion-Models"><a href="#71-Translation-of-Text-Embedding-via-Delta-Vector-to-Suppress-Strongly-Entangled-Content-in-Text-to-Image-Diffusion-Models" class="headerlink" title="71. Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Translation_of_Text_Embedding_via_Delta_Vector_to_Suppress_Strongly_Entangled_Content_in_Text-to-Ima.pdf">Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sungkyunkwan University</span></p>
<p>本文提出了通过在文本嵌入空间引入delta vector来直接抑制文本到图像扩散模型中强关联负内容的方法，并进一步提出了Selective Suppression with Delta Vector（SSDV），将delta vector集成到跨注意力机制，实现对特定区域负内容的精准抑制。实验结果表明，该方法在标准和个性化模型上均显著优于现有基线，在定量和定性评测中实现了更有效的强关联负内容抑制。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Translation_of_Text_Embedding_via_Delta_Vector_to_Suppress_Strongly_Entangled_Content_in_Text-to-Image_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-PQ-DAF-Pose-driven-Quality-controlled-Data-Augmentation-for-Data-scarce-Driver-Distraction-Detection"><a href="#72-PQ-DAF-Pose-driven-Quality-controlled-Data-Augmentation-for-Data-scarce-Driver-Distraction-Detection" class="headerlink" title="72. PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/PQ-DAF__Pose-driven_Quality-controlled_Data_Augmentation_for_Data-scarce_Driver_Distraction_Detectio.pdf">PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shandong University of Science and Technology</span></p>
<p>本文提出了PQ-DAF（Pose-driven Quality-controlled Data Augmentation Framework），结合姿态驱动的渐进式条件扩散模型（PCDMs）和视觉-语言模型（CogVLM）实现高质量驾驶员分心检测数据增强。方法通过提取驾驶员关键点姿态，利用PCDMs生成结构一致的伪样本，并用CogVLM对合成样本进行语义一致性筛选，大幅提升少样本下的模型泛化能力。实验结果显示，PQ-DAF在StateFarm和AUC-DDD数据集少样本设定下均显著提升检测准确率，验证了其在数据稀缺场景下的实用性和有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/PQ-DAF_Pose-driven_Quality-controlled_Data_Augmentation_for_Data-scarce_Driver_Distraction_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-Unlocking-Robust-Semantic-Segmentation-Performance-via-Label-only-Elastic-Deformations-against-Implicit-Label-Noise"><a href="#73-Unlocking-Robust-Semantic-Segmentation-Performance-via-Label-only-Elastic-Deformations-against-Implicit-Label-Noise" class="headerlink" title="73. Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Unlocking_Robust_Semantic_Segmentation_Performance_via_Label-only_Elastic_Deformations_against_Impli.pdf">Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">GIST</span></p>
<p>本文提出了一种名为NSegment+的新型数据增强框架，专为语义分割中的隐性标签噪声问题设计。方法核心在于仅对分割标签施加随机弹性形变（结合高斯平滑和尺度感知的小目标保护），而保持输入图像不变，从而模拟真实数据中的边界模糊和标注不一致，提升模型对标签不确定性的鲁棒性。大量实验表明，NSegment+在遥感和自然场景六大主流数据集上，平均mIoU提升显著，且与CutMix、Label Smoothing等现有增强&#x2F;正则化手段兼容，无需增加计算开销。结论认为，NSegment+为解决现实语义分割中的隐性标签噪声提供了轻量、高效且通用的解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Unlocking_Robust_Semantic_Segmentation_Performance_via_Label-only_Elastic_Deformations_against_Implicit_Label_Noise.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-AtomDiffuser-Time-Aware-Degradation-Modeling-for-Drift-and-Beam-Damage-in-STEM-Imaging"><a href="#74-AtomDiffuser-Time-Aware-Degradation-Modeling-for-Drift-and-Beam-Damage-in-STEM-Imaging" class="headerlink" title="74. AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/AtomDiffuser__Time-Aware_Degradation_Modeling_for_Drift_and_Beam_Damage_in_STEM_Imaging.pdf">AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Clemson University</span></p>
<p>该论文提出AtomDiffuser，一种结合时序卷积U-Net和物理启发降解建模的深度学习框架，能够从时序STEM图像中解耦并显式预测原子级漂移（通过仿射变换）和束流损伤（通过空间衰减图），利用合成降解序列训练并能泛化到真实冷冻STEM数据。实验表明，AtomDiffuser在损伤评估和漂移矫正任务上显著优于现有CNN方法，支持高分辨率降解推断和动态材料结构演化的可视化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AtomDiffuser_Time-Aware_Degradation_Modeling_for_Drift_and_Beam_Damage_in_STEM_Imaging.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-Improving-OCR-for-Historical-Texts-of-Multiple-Languages"><a href="#75-Improving-OCR-for-Historical-Texts-of-Multiple-Languages" class="headerlink" title="75. Improving OCR for Historical Texts of Multiple Languages"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Improving_OCR_for_Historical_Texts_of_Multiple_Languages.pdf">Improving OCR for Historical Texts of Multiple Languages</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Groningen</span></p>
<p>该论文提出了多语言历史文献的OCR与文档版面分析方法。针对死海古卷希伯来文，采用数据增强并比较了Kraken与TrOCR模型效果；在荷兰历史会议记录上，结合DeepLabV3+语义分割与BiLSTM，并通过伪标签提升训练效果；在现代英文手写识别任务中，采用CRNN与ResNet34编码并使用CTC损失。实验表明：数据增强与转移学习能显著提升历史文献OCR，伪标签自监督策略能有效提升文档结构分析的性能，复杂模型在小数据集上未必优于简单架构，未来需加强数据多样性与模型设计的平衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving_OCR_for_Historical_Texts_of_Multiple_Languages.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-Efficient-Image-Denoising-Using-Global-and-Local-Circulant-Representation"><a href="#76-Efficient-Image-Denoising-Using-Global-and-Local-Circulant-Representation" class="headerlink" title="76. Efficient Image Denoising Using Global and Local Circulant Representation"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Efficient_Image_Denoising_Using_Global_and_Local_Circulant_Representation.pdf">Efficient Image Denoising Using Global and Local Circulant Representation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China University of Technology</span></p>
<p>本文提出了一种高效的图像去噪算法Haar-tSVD，通过结合全球和局部循环结构，利用t-SVD和Haar变换在循环表示下有效捕获图像块的相关性，实现一步高并行可扩展的滤波，无需学习局部基。方法还引入CNN自适应噪声估计和特征值分析提升鲁棒性。实验表明，该方法在图像、视频、HSI、MRI等多任务中具有效果和效率并重的优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Efficient_Image_Denoising_Using_Global_and_Local_Circulant_Representation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-Improving-Learning-of-New-Diseases-through-Knowledge-Enhanced-Initialization-for-Federated-Adapter-Tuning"><a href="#77-Improving-Learning-of-New-Diseases-through-Knowledge-Enhanced-Initialization-for-Federated-Adapter-Tuning" class="headerlink" title="77. Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Improving_Learning_of_New_Diseases_through_Knowledge-Enhanced_Initialization_for_Federated_Adapter_T.pdf">Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR)</span></p>
<p>本文提出了一种名为FedKEI的联邦知识增强初始化框架，结合了全局聚类和双层聚合权重学习，实现了医疗领域中基础模型适配器调优过程中新任务的高效适应。FedKEI通过服务器端对任务特定模块进行聚类，并采用双层优化策略协同学习聚合权重，有效整合并转移跨任务与跨客户的知识，从而为适配器和头部生成更优的初始化。实验结果显示，FedKEI在皮肤、胸部X光和视网膜OCT等多模态医学影像数据集上，在新疾病适应任务中均超过现有最新方法，提升了新任务学习的准确性和效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving_Learning_of_New_Diseases_through_Knowledge-Enhanced_Initialization_for_Federated_Adapter_Tuning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-A-Sub-Pixel-Multimodal-Optical-Remote-Sensing-Images-Matching-Method"><a href="#78-A-Sub-Pixel-Multimodal-Optical-Remote-Sensing-Images-Matching-Method" class="headerlink" title="78. A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Sub-Pixel_Multimodal_Optical_Remote_Sensing_Images_Matching_Method.pdf">A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Central South University</span></p>
<p>本文提出了一种基于相位一致性加权最小绝对偏差（PCWLAD）的亚像素多模态光学遥感图像匹配方法。方法流程包括：首先利用结构相似性（SSIM）在未降噪的相位一致性（PC）图上进行粗匹配，然后通过互结构加权和加权最小绝对偏差（WLAD）准则，实现亚像素级别的精确匹配。实验结果显示，在三个类型的可见-红外遥感数据集上，PCWLAD在匹配正确率（CMR）和均方根误差（RMSE）方面均优于现有七种主流方法，平均RMSE约为0.4像素；但方法在大几何变形场景下效果有限。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Sub-Pixel_Multimodal_Optical_Remote_Sensing_Images_Matching_Method.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-Deep-Learning-for-Crack-Detection-A-Review-of-Learning-Paradigms-Generalizability-and-Datasets"><a href="#79-Deep-Learning-for-Crack-Detection-A-Review-of-Learning-Paradigms-Generalizability-and-Datasets" class="headerlink" title="79. Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Deep_Learning_for_Crack_Detection__A_Review_of_Learning_Paradigms,_Generalizability,_and_Datasets.pdf">Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Georgia Institute of Technology</span></p>
<p>本论文系统回顾了基于深度学习的裂缝检测领域，聚焦于监督、半监督、弱监督、无监督、少样本、领域自适应及基础模型等多种学习范式的技术演进和泛化能力，详述了主流方法（如CNN、Transformer、SAM等）、关键流程及代表性数据集，同时发布了高分辨率3D激光扫描裂缝数据集3DCrack。结论指出，未来需进一步突破数据高效利用、模型泛化、标准化评测与多模态融合等挑战，以实现更鲁棒、可扩展的裂缝自动检测系统。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Deep_Learning_for_Crack_Detection_A_Review_of_Learning_Paradigms_Generalizability_and_Datasets.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="80-Performance-of-GPT-5-in-Brain-Tumor-MRI-Reasoning"><a href="#80-Performance-of-GPT-5-in-Brain-Tumor-MRI-Reasoning" class="headerlink" title="80. Performance of GPT-5 in Brain Tumor MRI Reasoning"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Performance_of_GPT-5_in_Brain_Tumor_MRI_Reasoning.pdf">Performance of GPT-5 in Brain Tumor MRI Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Emory University School of Medicine</span></p>
<p>本论文系统评估了GPT-4o、GPT-5-nano、GPT-5-mini和GPT-5在三类脑肿瘤MRI多模态视觉问答(VQA)基准上的表现，方法包括将BraTS数据集的多序列MRI与结构化临床特征转化为标准化VQA问题，并采用zero-shot chain-of-thought提示进行模型推理与准确率对比。结果显示，GPT-5-mini取得最高平均准确率(44.19%)，但各模型性能相近，尚未达到临床实用水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Performance_of_GPT-5_in_Brain_Tumor_MRI_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-A-Survey-on-Diffusion-Language-Models"><a href="#81-A-Survey-on-Diffusion-Language-Models" class="headerlink" title="81. A Survey on Diffusion Language Models"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Survey_on_Diffusion_Language_Models.pdf">A Survey on Diffusion Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mohamed bin Zayed University of Artificial Intelligence</span></p>
<p>本文系统梳理了扩散语言模型（DLMs）的原理、发展、技术和挑战，涵盖连续、离散及混合扩散模型，并细致分析了其预训练、后训练（包括强化学习对齐）、推理优化、并行解码、缓存和多模态扩展等关键技术流程。结论指出，DLMs在推理速度、上下文建模和多模态统一方面展现出独特优势，与主流自回归模型性能相当，同时也面临并行一致性、基础设施和可扩展性等挑战，未来具备广阔研究与应用前景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Survey_on_Diffusion_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-Reinforced-Language-Models-for-Sequential-Decision-Making"><a href="#82-Reinforced-Language-Models-for-Sequential-Decision-Making" class="headerlink" title="82. Reinforced Language Models for Sequential Decision Making"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Reinforced_Language_Models_for_Sequential_Decision_Making.pdf">Reinforced Language Models for Sequential Decision Making</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Southampton</span></p>
<p>本文提出了Multi-Step Group-Relative Policy Optimization（MS-GRPO）算法，通过在Text-Mediated Stochastic Game（TMSG）和Language Agent Policy（LAP）框架下，将全局回报分配到每个决策步骤，并结合绝对优势加权（AAW）采样策略，以提升小型LLM的序列决策能力。实验表明，经过该方法微调的3B参数模型在Frozen Lake任务上决策能力提升，超越了72B参数的大模型50%；同时方法在效率和泛化能力上也表现出优势，但与专用DQN方法相比仍存在性能差距。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Reinforced_Language_Models_for_Sequential_Decision_Making.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-Memory-Augmented-Transformers-A-Systematic-Review-from-Neuroscience-Principles-to-Technical-Solutions"><a href="#83-Memory-Augmented-Transformers-A-Systematic-Review-from-Neuroscience-Principles-to-Technical-Solutions" class="headerlink" title="83. Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Memory-Augmented_Transformers__A_Systematic_Review_from_Neuroscience_Principles_to_Technical_Solutio.pdf">Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huawei Technologies</span></p>
<p>本文系统综述了基于神经科学原理（如多时间尺度动态记忆、选择性注意和巩固）与工程实现相结合的Memory-Augmented Transformers技术，提出了三维分类体系，分析了核心操作（读写、遗忘、容量管理）及其实现方式，展示了领域从静态缓存向自适应推理和终身学习系统的演变。结论指出，混合存储、自适应动态和智能遗忘成为主流，但在可扩展性、干扰协调和评测标准化等方面仍面临挑战，未来需借鉴生物记忆机制推动认知型AI发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Memory-Augmented_Transformers_A_Systematic_Review_from_Neuroscience_Principles_to_Technical_Solutions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-The-Knowledge-Reasoning-Dissociation-Fundamental-Limitations-of-LLMs-in-Clinical-Natural-Language-Inference"><a href="#84-The-Knowledge-Reasoning-Dissociation-Fundamental-Limitations-of-LLMs-in-Clinical-Natural-Language-Inference" class="headerlink" title="84. The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/The_Knowledge-Reasoning_Dissociation__Fundamental_Limitations_of_LLMs_in_Clinical_Natural_Language_I.pdf">The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Manchester</span></p>
<p>该论文提出了一个针对临床推理的自然语言推断（NLI）基准，包括四类推理任务（因果归因、组合性基础、认知验证和风险抽象），并设计了GKMRV探针用于区分知识检索与推理能力。实验发现，当前主流大语言模型在知识检索任务上表现极佳，但在结构化推理任务上普遍失败，结论认为仅靠模型规模扩展无法实现可靠的临床推理，需要更强的可组合内部表示。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/The_Knowledge-Reasoning_Dissociation_Fundamental_Limitations_of_LLMs_in_Clinical_Natural_Language_Inference.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-Modeling-Human-Responses-to-Multimodal-AI-Content"><a href="#85-Modeling-Human-Responses-to-Multimodal-AI-Content" class="headerlink" title="85. Modeling Human Responses to Multimodal AI Content"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Modeling_Human_Responses_to_Multimodal_AI_Content.pdf">Modeling Human Responses to Multimodal AI Content</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Salesforce Research Asia</span></p>
<p>该论文提出了T-Lens系统，通过人类中心的方法预测用户对多模态AI生成内容（AIGC）的反应，核心模块HR-MCP基于大规模人类标注数据，结合多模态一致性和情感一致性，能量化信任度、影响和开放性等指标，并可无缝集成至任意LLM。实验表明，T-Lens在模拟人类对多模态信息的信任与传播倾向上优于现有大模型及传统方法，为AI驱动的虚假信息治理提供了新工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Modeling_Human_Responses_to_Multimodal_AI_Content.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-GenOM-Ontology-Matching-with-Description-Generation-and-Large-Language-Model"><a href="#86-GenOM-Ontology-Matching-with-Description-Generation-and-Large-Language-Model" class="headerlink" title="86. GenOM: Ontology Matching with Description Generation and Large Language Model"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GenOM__Ontology_Matching_with_Description_Generation_and_Large_Language_Model.pdf">GenOM: Ontology Matching with Description Generation and Large Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Manchester</span></p>
<p>本文提出了GenOM，一种结合大语言模型（LLM）与文本定义生成的本体对齐框架。其方法流程包括：从本体中提取结构与词汇信息，用LLM生成包含领域知识的文本定义，利用嵌入模型进行候选检索，再通过LLM判别候选对的语义等价性，并与传统精确匹配工具融合提升精度。实验表明，GenOM在OAEI Bio-ML基准上表现优异，超越多种传统与现有LLM方案，且模块化设计和定义增强策略有效提升了泛化性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GenOM_Ontology_Matching_with_Description_Generation_and_Large_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-Learning-from-Natural-Language-Feedback-for-Personalized-Question-Answering"><a href="#87-Learning-from-Natural-Language-Feedback-for-Personalized-Question-Answering" class="headerlink" title="87. Learning from Natural Language Feedback for Personalized Question Answering"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Learning_from_Natural_Language_Feedback_for_Personalized_Question_Answering.pdf">Learning from Natural Language Feedback for Personalized Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Massachusetts Amherst</span></p>
<p>该论文提出VAC框架，用自然语言反馈（NLF）替代传统个性化问答中用于训练大语言模型的标量奖励信号。VAC通过交替优化反馈模型和策略模型，利用用户画像和问题叙述生成个性化反馈，有效提升模型的个性化响应能力。实验表明，VAC在LaMP-QA基准测试上显著优于现有方法，并被人工评价为更优，表明自然语言反馈为个性化问答优化提供了更有效的信号。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Learning_from_Natural_Language_Feedback_for_Personalized_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-FROGENT-An-End-to-End-Full-process-Drug-Design-Agent"><a href="#88-FROGENT-An-End-to-End-Full-process-Drug-Design-Agent" class="headerlink" title="88. FROGENT: An End-to-End Full-process Drug Design Agent"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/FROGENT__An_End-to-End_Full-process_Drug_Design_Agent.pdf">FROGENT: An End-to-End Full-process Drug Design Agent</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, Shenzhen University</span></p>
<p>FROGENT is an agentic framework leveraging Large Language Models (LLMs) and the Model Context Protocol (MCP) to unify and automate the entire drug discovery workflow, integrating dynamic biochemical databases, extensible tool libraries, and specialized AI models for tasks such as target identification, molecule generation, and retrosynthetic planning. Experimental results show FROGENT significantly outperforms state-of-the-art LLM agents and commercial models on eight drug discovery benchmarks, tripling hit-finding and doubling interaction profiling scores, thus streamlining and accelerating drug research.</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FROGENT_An_End-to-End_Full-process_Drug_Design_Agent.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-EgoCross-Benchmarking-Multimodal-Large-Language-Models-for-Cross-Domain-Egocentric-Video-Question-Answering"><a href="#89-EgoCross-Benchmarking-Multimodal-Large-Language-Models-for-Cross-Domain-Egocentric-Video-Question-Answering" class="headerlink" title="89. EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/EgoCross__Benchmarking_Multimodal_Large_Language_Models_for_Cross-Domain_Egocentric_Video_Question_A.pdf">EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">East China Normal University</span></p>
<p>本文提出了EgoCross基准，专为评估多模态大语言模型（MLLMs）在跨领域第一视角视频问答（EgocentricQA）中的泛化能力。通过涵盖外科手术、工业、极限运动和动物视角四大领域，共包含约1000对问答，支持识别、定位、预测与计数等多类任务，并采用CloseQA与OpenQA两种评测方式。实验发现，当前主流MLLMs在非日常领域表现显著下降，泛化能力有限。论文还探索了Prompt学习、微调和强化学习等提升方法，结果显示强化学习对提升跨域泛化效果最显著。结论表明，EgoCross为未来更具鲁棒性和适应性的多模态大模型研究提供了坚实基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/EgoCross_Benchmarking_Multimodal_Large_Language_Models_for_Cross-Domain_Egocentric_Video_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-Advancing-Autonomous-Incident-Response-Leveraging-LLMs-and-Cyber-Threat-Intelligence"><a href="#90-Advancing-Autonomous-Incident-Response-Leveraging-LLMs-and-Cyber-Threat-Intelligence" class="headerlink" title="90. Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Advancing_Autonomous_Incident_Response__Leveraging_LLMs_and_Cyber_Threat_Intelligence.pdf">Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">OODRIVE-Trusted Cloud Solutions</span></p>
<p>本文提出一种基于检索增强生成（RAG）的大语言模型自动化网络安全事件响应框架，通过结合NLP相似性检索和标准化CTI平台查询，实现对安全警报的上下文丰富与关联，并生成精确可操作的应对策略。实验结果表明，该方法在提高响应准确性、效率及减轻分析师工作负担方面表现优异，自动评估与专家验证均证实系统的鲁棒性和有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Advancing_Autonomous_Incident_Response_Leveraging_LLMs_and_Cyber_Threat_Intelligence.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-MSRS-Adaptive-Multi-Subspace-Representation-Steering-for-Attribute-Alignment-in-Large-Language-Models"><a href="#91-MSRS-Adaptive-Multi-Subspace-Representation-Steering-for-Attribute-Alignment-in-Large-Language-Models" class="headerlink" title="91. MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MSRS__Adaptive_Multi-Subspace_Representation_Steering_for_Attribute_Alignment_in_Large_Language_Mode.pdf">MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</span></p>
<p>本文提出了MSRS（Multi-Subspace Representation Steering）框架，通过为每个属性分配正交子空间并结合共享子空间，实现对大语言模型多属性的高效精准控制。方法利用SVD自适应分配子空间容量，并引入动态Token选择与子空间加权机制，有效减少属性间干扰。实验表明，MSRS在真值性、公平性、指令遵循和拒绝等多属性任务上，显著优于现有方法，且保持了模型的通用能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MSRS_Adaptive_Multi-Subspace_Representation_Steering_for_Attribute_Alignment_in_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-MCP2OSC-Parametric-Control-by-Natural-Language"><a href="#92-MCP2OSC-Parametric-Control-by-Natural-Language" class="headerlink" title="92. MCP2OSC: Parametric Control by Natural Language"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MCP2OSC__Parametric_Control_by_Natural_Language.pdf">MCP2OSC: Parametric Control by Natural Language</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Los Angeles, CA</span></p>
<p>本论文提出了MCP2OSC服务端和一套针对OpenSoundControl (OSC) 参数控制的自然语言提示设计准则，通过与Claude等大语言模型集成，实现了用自然语言生成、解释、调试和管理OSC消息，涵盖批量与流式控制、日志分析、可视化等多种实用场景。实验结果表明，该系统显著降低了复杂多媒体设备与创作流程中的OSC开发难度，提高了人机协作效率，展现了基于LLM的通用控制机制潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MCP2OSC_Parametric_Control_by_Natural_Language.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-Facilitating-the-Adoption-of-Causal-Inference-Methods-Through-LLM-Empowered-Co-Pilot"><a href="#93-Facilitating-the-Adoption-of-Causal-Inference-Methods-Through-LLM-Empowered-Co-Pilot" class="headerlink" title="93. Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Technical_Report__Facilitating_the_Adoption_of_Causal_Inference_Methods_Through_LLM-Empowered_Co-Pil.pdf">Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ataraxis.ai</span></p>
<p>本文提出了CATE-B系统，利用大语言模型（LLM）结合因果发现算法，帮助用户完成结构因果模型建设、不确定性最小调整集（MUAS）选择和回归方法推荐，实现从观测数据到处理效应估计的全流程自动化。实验表明，CATE-B在多数据集上提高了因果推断的准确性和健壮性，降低了领域专家门槛，促进了因果推断方法的普及与标准化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Facilitating_the_Adoption_of_Causal_Inference_Methods_Through_LLM-Empowered_Co-Pilot.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-When-Language-Overrules-Revealing-Text-Dominance-in-Multimodal-Large-Language-Models"><a href="#94-When-Language-Overrules-Revealing-Text-Dominance-in-Multimodal-Large-Language-Models" class="headerlink" title="94. When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/When_Language_Overrules__Revealing_Text_Dominance_in_Multimodal_Large_Language_Models.pdf">When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Computing Technology, Chinese Academy of Sciences</span></p>
<p>本文系统分析了多模态大语言模型（MLLMs）在多种模态（图像、视频、音频、时间序列和图结构数据）上的文本优势现象，提出了模态优势指数（MDI）和注意力效率指数（AEI）作为衡量指标，并通过实验发现文本模态在现有模型中普遍占主导地位。针对这一问题，作者提出了基于[CLS]注意力引导的非文本token压缩方法，有效缓解了注意力分配失衡并促进更公平的多模态融合，结论为压缩非文本token能显著改善多模态模型的模态均衡性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/When_Language_Overrules_Revealing_Text_Dominance_in_Multimodal_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-A-Unified-Multi-Agent-Framework-for-Universal-Multimodal-Understanding-and-Generation"><a href="#95-A-Unified-Multi-Agent-Framework-for-Universal-Multimodal-Understanding-and-Generation" class="headerlink" title="95. A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Unified_Multi-Agent_Framework_for_Universal_Multimodal_Understanding_and_Generation.pdf">A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Laboratory of General Artificial Intelligence, BIGAI</span></p>
<p>本文提出MAGUS框架，通过认知与推理（Cognition）和表达与生成（Deliberation）两阶段分离，采用多智能体协作机制，将多模态大语言模型（MLLM）与扩散模型有机结合，实现文本、图像、音频、视频等任意模态的理解和生成。核心技术包括Growth-Aware Search（GAS）算法，支持多轮智能体对话、结构化任务规划、动态优化生成质量，无需联合训练即可灵活扩展。实验结果表明，MAGUS在多模态理解与生成任务的主流基准上均优于现有强基线及闭源大模型（如GPT-4o），具备高可扩展性和强指令跟随能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Unified_Multi-Agent_Framework_for_Universal_Multimodal_Understanding_and_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-SEQ-GPT-LLM-assisted-Spatial-Query-via-Example"><a href="#96-SEQ-GPT-LLM-assisted-Spatial-Query-via-Example" class="headerlink" title="96. SEQ-GPT: LLM-assisted Spatial Query via Example"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SEQ-GPT__LLM-assisted_Spatial_Query_via_Example.pdf">SEQ-GPT: LLM-assisted Spatial Query via Example</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>该论文提出了SEQ-GPT系统，将大语言模型（LLM）与空间示例查询（SEQ）结合，实现了用户通过自然语言和实例对空间位置进行灵活检索。方法包括多模型协作的数据解析与对话调度、数据合成与微调，并支持多轮交互和反馈修正，显著提升了复杂空间查询的易用性和灵活性。实验展示了SEQ-GPT在真实场景下对多地点检索任务的有效性和用户体验提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SEQ-GPT_LLM-assisted_Spatial_Query_via_Example.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs"><a href="#97-HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs" class="headerlink" title="97. HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/HumanSense__From_Multimodal_Perception_to_Empathetic_Context-Aware_Responses_through_Reasoning_MLLMs.pdf">HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong University</span></p>
<p>该论文提出了HumanSense基准，系统评估多模态大语言模型（MLLMs）在人类中心场景下的感知、理解与互动能力，涵盖视觉、音频和文本。通过构建多层次任务体系、模态消融实验及多阶段模态递进式强化学习，发现多模态推理能显著提升模型在复杂人机交互场景下的表现，并通过提示工程无训练提升非推理模型效果，为未来AI互动体验指明方向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/HumanSense_From_Multimodal_Perception_to_Empathetic_Context-Aware_Responses_through_Reasoning_MLLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-Improving-Value-based-Process-Verifier-via-Low-Cost-Variance-Reduction"><a href="#98-Improving-Value-based-Process-Verifier-via-Low-Cost-Variance-Reduction" class="headerlink" title="98. Improving Value-based Process Verifier via Low-Cost Variance Reduction"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Improving_Value-based_Process_Verifier_via_Low-Cost_Variance_Reduction.pdf">Improving Value-based Process Verifier via Low-Cost Variance Reduction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology (Shenzhen)</span></p>
<p>本文提出了一种低成本方差降低方法ComMCS，通过对当前和后续步骤的Monte Carlo估计线性组合，优化了大语言模型（LLM）在数学推理任务中的value-based process verifier训练标签的方差，且无需增加额外的LLM推理成本。实验结果显示，ComMCS在MATH-500和GSM8K基准测试上相较于传统回归和分布建模方法取得了2-3个百分点的准确率提升，有效提升了模型的数学推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Improving_Value-based_Process_Verifier_via_Low-Cost_Variance_Reduction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-Reverse-Physician-AI-Relationship-Full-process-Clinical-Diagnosis-Driven-by-a-Large-Language-Model"><a href="#99-Reverse-Physician-AI-Relationship-Full-process-Clinical-Diagnosis-Driven-by-a-Large-Language-Model" class="headerlink" title="99. Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model"></a>99. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Reverse_Physician-AI_Relationship__Full-process_Clinical_Diagnosis_Driven_by_a_Large_Language_Model.pdf">Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Laboratory of AI Safety, Institute of Computing Technology, CAS</span></p>
<p>本文提出了DxDirector-7B，一种具备类人“慢思考”深度推理能力的大型语言模型，实现了从含糊主诉出发自动驱动全流程临床诊断，仅在必要时请求医生协助。通过持续医学预训练、全流程诊断指令微调和步级策略偏好优化，使模型能自主决策、分步推理并附权威文献支持。实验结果显示，DxDirector-7B在复杂、罕见和真实临床场景中诊断准确率显著优于参数量大数十倍的现有医疗&#x2F;通用LLM，并极大减轻医生工作量，部分科室可替代60%-75%的专科医生诊断工作，标志着AI主导诊断新时代的到来。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Reverse_Physician-AI_Relationship_Full-process_Clinical_Diagnosis_Driven_by_a_Large_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-FIRESPARQL-A-LLM-based-Framework-for-SPARQL-Query-Generation-over-Scholarly-Knowledge-Graphs"><a href="#100-FIRESPARQL-A-LLM-based-Framework-for-SPARQL-Query-Generation-over-Scholarly-Knowledge-Graphs" class="headerlink" title="100. FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs"></a>100. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/FIRESPARQL__A_LLM-based_Framework_for_SPARQL_Query_Generation_over_Scholarly_Knowledge_Graphs.pdf">FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Vrije Universiteit Amsterdam</span></p>
<p>本文提出FIRESPARQL框架，通过对大语言模型（LLM）进行领域特定微调（LoRA）、引入可选的检索增强生成（RAG）模块和轻量级SPARQL纠错层，以提升对学术知识图谱（SKG）中的自然语言问题转SPARQL查询的准确性和结构一致性。实验结果显示，微调后的LLaMA-3-8B-Instruct模型在SciQA基准上取得了最优表现，显著提升了查询准确率和执行效果，RAG模块反而未带来额外提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FIRESPARQL_A_LLM-based_Framework_for_SPARQL_Query_Generation_over_Scholarly_Knowledge_Graphs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="101-WE-MATH-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning"><a href="#101-WE-MATH-2-0-A-Versatile-MathBook-System-for-Incentivizing-Visual-Mathematical-Reasoning" class="headerlink" title="101. WE-MATH 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning"></a>101. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/We-Math_2.0__A_Versatile_MathBook_System_for_Incentivizing_Visual_Mathematical_Reasoning.pdf">WE-MATH 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">BUPT</span></p>
<p>WE-MATH 2.0提出了一个多模态大语言模型系统，融合了结构化数学知识体系、模型中心化数据难度建模以及两阶段强化学习训练，包括冷启动微调和动态难度递增RL优化，显著提升MLLM在视觉数学推理任务中的泛化与稳健性。实验证明，该方法在多个主流数学推理基准上取得了领先性能，尤其在多步推理和知识覆盖广度方面表现优异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/WE-MATH_2.0_A_Versatile_MathBook_System_for_Incentivizing_Visual_Mathematical_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="102-SC2Arena-and-StarEvolve-Benchmark-and-Self-Improvement-Framework-for-LLMs-in-Complex-Decision-Making-Tasks"><a href="#102-SC2Arena-and-StarEvolve-Benchmark-and-Self-Improvement-Framework-for-LLMs-in-Complex-Decision-Making-Tasks" class="headerlink" title="102. SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks"></a>102. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/SC2Arena_and_StarEvolve__Benchmark_and_Self-Improvement_Framework_for_LLMs_in_Complex_Decision-Makin.pdf">SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Automation, Chinese Academy of Sciences</span></p>
<p>该论文提出SC2Arena，作为评估大语言模型（LLM）在StarCraft II复杂决策任务中的基准，支持全程游戏、多种种族和完整低级动作空间，同时通过文本优化解决空间推理与信息过载问题。配套的StarEvolve框架采用分层规划与执行、迭代自我纠错机制，并结合高质量数据的监督微调，实现LLM智能体的持续自我提升和强对手适应能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/SC2Arena_and_StarEvolve_Benchmark_and_Self-Improvement_Framework_for_LLMs_in_Complex_Decision-Making_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="103-AnalogSeeker-An-Open-source-Foundation-Language-Model-for-Analog-Circuit-Design"><a href="#103-AnalogSeeker-An-Open-source-Foundation-Language-Model-for-Analog-Circuit-Design" class="headerlink" title="103. AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design"></a>103. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/AnalogSeeker__An_Open-source_Foundation_Language_Model_for_Analog_Circuit_Design.pdf">AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本论文提出AnalogSeeker，一种面向模拟电路设计的开源基础大语言模型。方法上，作者通过模拟电路领域知识框架系统收集高质量教材并清洗成文本语料，采用多智能体框架将未标注文本蒸馏为细粒度问答（QTSA）对，形成可微调数据集，并设计了邻域自约束监督微调算法（NSC-SFT），在Qwen2.5-32B-Instruct模型基础上进行训练。结论显示，AnalogSeeker在AMSBench-TQA基准上获得85.04%准确率，比原始模型提升15.67个百分点，且在运算放大器设计任务中表现有效，具备领域知识注入和设计辅助潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/AnalogSeeker_An_Open-source_Foundation_Language_Model_for_Analog_Circuit_Design.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="104-Layer-Wise-Perturbations-via-Sparse-Autoencoders-for-Adversarial-Text-Generation"><a href="#104-Layer-Wise-Perturbations-via-Sparse-Autoencoders-for-Adversarial-Text-Generation" class="headerlink" title="104. Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation"></a>104. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Layer-Wise_Perturbations_via_Sparse_Autoencoders_for_Adversarial_Text_Generation.pdf">Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">hydrox.ai</span></p>
<p>该论文提出Sparse Feature Perturbation Framework (SFPF)，利用稀疏自编码器（SAE）对大型语言模型的隐藏层激活进行重构和特征聚类，识别并扰动与攻击相关的关键特征，从而生成可绕过安全机制的对抗性文本。实验表明，SFPF生成的对抗性文本能显著提升攻击成功率并维持文本质量，但方法在不同模型结构和层次上的泛化性尚待验证。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Layer-Wise_Perturbations_via_Sparse_Autoencoders_for_Adversarial_Text_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="105-LeanRAG-Knowledge-Graph-Based-Generation-with-Semantic-Aggregation-and-Hierarchical-Retrieval"><a href="#105-LeanRAG-Knowledge-Graph-Based-Generation-with-Semantic-Aggregation-and-Hierarchical-Retrieval" class="headerlink" title="105. LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval"></a>105. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/LeanRAG__Knowledge-Graph-Based_Generation_with_Semantic_Aggregation_and_Hierarchical_Retrieval.pdf">LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Artificial Intelligence Laboratory</span></p>
<p>LeanRAG提出了一种结合知识图谱语义聚合与层次化检索的检索增强生成（RAG）新框架。方法上，LeanRAG首先通过高斯混合聚类和大模型生成，构建多层次、具备显式实体间关系的可导航语义网络，再利用自底向上的LCA结构化检索，有效定位相关细粒度实体并聚合至高层次语义，提升上下文相关性并大幅减少信息冗余。结论显示，LeanRAG在四个QA基准数据集上表现优于现有方法，检索冗余降低46%，且显式语义关系与原始文本并用对生成高质量答案至关重要。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/LeanRAG_Knowledge-Graph-Based_Generation_with_Semantic_Aggregation_and_Hierarchical_Retrieval.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="106-Contrast-Sensitivity-Function-of-Multimodal-Vision-Language-Models"><a href="#106-Contrast-Sensitivity-Function-of-Multimodal-Vision-Language-Models" class="headerlink" title="106. Contrast Sensitivity Function of Multimodal Vision-Language Models"></a>106. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Contrast_Sensitivity_Function_of_Multimodal_Vision-Language_Models.pdf">Contrast Sensitivity Function of Multimodal Vision-Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universidad de Valencia</span></p>
<p>本文提出了一种心理物理学启发的行为评估方法，通过直接向多模态视觉-语言模型（如LLaVA-1.5-7B、Blip2-7B等）提问来测量其对不同空间频率和对比度图像的敏感性，拟合模型的对比敏感函数（CSF），并与人类视觉CSF进行对比。结果显示，部分模型在CSF形状或绝对值上与人类相近，但均无法同时复现人类CSF的形状与稳定性，且对提示词变化敏感，提示内部一致性和鲁棒性有待提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Contrast_Sensitivity_Function_of_Multimodal_Vision-Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="107-What-to-Ask-Next-Probing-the-Imaginative-Reasoning-of-LLMs-with-TurtleSoup-Puzzles"><a href="#107-What-to-Ask-Next-Probing-the-Imaginative-Reasoning-of-LLMs-with-TurtleSoup-Puzzles" class="headerlink" title="107. What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles"></a>107. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/What_to_Ask_Next__Probing_the_Imaginative_Reasoning_of_LLMs_with_TurtleSoup_Puzzles.pdf">What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huazhong University of Science and Technology</span></p>
<p>本文提出了TurtleSoup-Bench，一个大规模、双语、交互式的想象性推理基准，结合TurtleSoup谜题、Mosaic-Agent智能体和多维自动评测协议，以系统评估大模型在信息稀缺环境下的假设构建、测试与修正能力。实验表明，当前主流大语言模型在动态推理与创造性假设生成方面仍与人类有明显差距，验证了该评测框架对模型推理过程诊断的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/What_to_Ask_Next_Probing_the_Imaginative_Reasoning_of_LLMs_with_TurtleSoup_Puzzles.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="108-ReviewRL-Towards-Automated-Scientific-Review-with-RL"><a href="#108-ReviewRL-Towards-Automated-Scientific-Review-with-RL" class="headerlink" title="108. ReviewRL: Towards Automated Scientific Review with RL"></a>108. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/ReviewRL__Towards_Automated_Scientific_Review_with_RL.pdf">ReviewRL: Towards Automated Scientific Review with RL</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本论文提出ReviewRL框架，通过ArXiv-MCP检索增强型上下文生成、监督微调和强化学习训练自动生成高质量、事实准确的学术论文评审。实验结果表明，ReviewRL在ICLR 2025论文评测中显著优于现有方法，有效提升了评审的质量和评分一致性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/ReviewRL_Towards_Automated_Scientific_Review_with_RL.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="109-Yet-another-algorithmic-bias-A-Discursive-Analysis-of-Large-Language-Models-Reinforcing-Dominant-Discourses-on-Gender-and-Race"><a href="#109-Yet-another-algorithmic-bias-A-Discursive-Analysis-of-Large-Language-Models-Reinforcing-Dominant-Discourses-on-Gender-and-Race" class="headerlink" title="109. Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race"></a>109. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Yet_another_algorithmic_bias__A_Discursive_Analysis_of_Large_Language_Models_Reinforcing_Dominant_Di.pdf">Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Instituto de Estudos da Linguagem (IEL), Universidade Estadual de Campinas (UNICAMP)</span></p>
<p>本论文提出以批判性话语分析为核心的定性分析框架，对主流LLMs（包括GPT-4、LLaMa、Sabiá等）生成的关于不同性别与种族女性的短篇故事进行深入剖析。研究发现，这些LLMs在文本输出中系统性地强化了西方主流关于黑人女性的抗争、祖先和群体责任等刻板印象，而对白人女性则更多强调自我发现和个体成长。即使在被提示修正偏见时，模型的修正也流于表面，难以消除深层的结构性歧视。这反映出LLMs在语义与社会层面的理解与创新能力有限，现有模型难以自发突破训练语料中固有的意识形态框架。论文强调，话语层面的定性研究对于发现和干预大模型输出中的隐性偏见至关重要，呼吁AI开发需引入跨学科和多元伦理视角。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Yet_another_algorithmic_bias_A_Discursive_Analysis_of_Large_Language_Models_Reinforcing_Dominant_Discourses_on_Gender_and_Race.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="110-Promoting-Efficient-Reasoning-with-Verifiable-Stepwise-Reward"><a href="#110-Promoting-Efficient-Reasoning-with-Verifiable-Stepwise-Reward" class="headerlink" title="110. Promoting Efficient Reasoning with Verifiable Stepwise Reward"></a>110. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Promoting_Efficient_Reasoning_with_Verifiable_Stepwise_Reward.pdf">Promoting Efficient Reasoning with Verifiable Stepwise Reward</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Meituan</span></p>
<p>该论文提出了一种可验证的逐步奖励机制（VSRM），通过规则分割和奖励传播，对大语言推理模型的中间推理步骤进行奖励或惩罚，鼓励有效步骤抑制无效冗余，显著减少输出长度。实验结果表明，VSRM在多个数学推理基准上有效缓解模型过度推理问题，同时保持甚至略微提升原有推理能力，实现高效准确的推理。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Promoting_Efficient_Reasoning_with_Verifiable_Stepwise_Reward.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="111-Why-Cannot-Large-Language-Models-Ever-Make-True-Correct-Reasoning"><a href="#111-Why-Cannot-Large-Language-Models-Ever-Make-True-Correct-Reasoning" class="headerlink" title="111. Why Cannot Large Language Models Ever Make True Correct Reasoning?"></a>111. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Why_Cannot_Large_Language_Models_Ever_Make_True_Correct_Reasoning_.pdf">Why Cannot Large Language Models Ever Make True Correct Reasoning?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Saitama University</span></p>
<p>本文从逻辑学和推理正确性的角度，系统分析了大语言模型（LLM）所谓“推理能力”的本质，指出LLM仅能模拟推理形式但无法实现真正的正确推理。作者通过定义真正正确推理的三大逻辑基础，阐述了LLM由于其基于概率与统计的生成原理，无法内嵌逻辑有效性评价标准和动态评估机制，因此永远无法获得100%正确的推理能力。结论认为，LLM只能给出表面上似乎合理的推理结果，追求其真正推理能力而忽视正确性标准是错误且无望的方向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Why_Cannot_Large_Language_Models_Ever_Make_True_Correct_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="112-Flexible-Personalized-Split-Federated-Learning-for-On-Device-Fine-Tuning-of-Foundation-Models"><a href="#112-Flexible-Personalized-Split-Federated-Learning-for-On-Device-Fine-Tuning-of-Foundation-Models" class="headerlink" title="112. Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models"></a>112. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Flexible_Personalized_Split_Federated_Learning_for_On-Device_Fine-Tuning_of_Foundation_Models.pdf">Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Duke Kunshan University</span></p>
<p>该论文提出FlexP-SFL框架，结合Split Federated Learning和个性化机制，实现基础模型在资源受限设备上的个性化高效微调。方法允许每个客户端按自身算力灵活分配本地训练层，服务器处理共享部分，并通过KL对齐策略平衡个性化与泛化。实验表明，该方法在多种语言模型上显著提升个性化准确率，减少通信与训练时间，支持异构设备高效协作。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Flexible_Personalized_Split_Federated_Learning_for_On-Device_Fine-Tuning_of_Foundation_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="113-MRFD-Multi-Region-Fusion-Decoding-with-Self-Consistency-for-Mitigating-Hallucinations-in-LVLMs"><a href="#113-MRFD-Multi-Region-Fusion-Decoding-with-Self-Consistency-for-Mitigating-Hallucinations-in-LVLMs" class="headerlink" title="113. MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs"></a>113. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/MRFD__Multi-Region_Fusion_Decoding_with_Self-Consistency_for_Mitigating_Hallucinations_in_LVLMs.pdf">MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Merced</span></p>
<p>该论文提出了一种训练无关的解码方法MRFD，通过交叉注意力机制选取图像多个显著区域，分别生成区域响应，并利用Jensen-Shannon Divergence评估区域间自洽性，获得可靠性权重后融合各区域预测。最终采用链式思维启发的区域提示，提升LVLMs对图像事实的捕捉与响应的真实度。实验表明MRFD能在多个LVLM和公开基准上显著降低幻觉现象，提升响应的事实性，无需模型参数更新。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/MRFD_Multi-Region_Fusion_Decoding_with_Self-Consistency_for_Mitigating_Hallucinations_in_LVLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="114-Generalizable-Federated-Learning-using-Client-Adaptive-Focal-Modulation"><a href="#114-Generalizable-Federated-Learning-using-Client-Adaptive-Focal-Modulation" class="headerlink" title="114. Generalizable Federated Learning using Client Adaptive Focal Modulation"></a>114. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Generalizable_Federated_Learning_using_Client_Adaptive_Focal_Modulation.pdf">Generalizable Federated Learning using Client Adaptive Focal Modulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">MBZUAI</span></p>
<p>该论文提出了AdaptFED，一种在联邦学习（FL）场景下利用中心化可学习生成器和任务感知客户端嵌入，实现焦点调制层个性化的Transformer框架。方法通过低秩超网络条件化和高效的参数聚合，提升了模型在多模态、非独立同分布（Non-IID）及源数据不可用（Source-Free Domain Adaptation）环境下的泛化能力。实验证明，AdaptFED在视觉、时间序列和多语言任务上均优于当前主流方法，理论上也给出了更紧的个性化泛化界。结论：AdaptFED提升了个性化联邦学习的效果与扩展性，适用于多领域和资源受限场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Generalizable_Federated_Learning_using_Client_Adaptive_Focal_Modulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="115-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms"><a href="#115-From-Black-Box-to-Transparency-Enhancing-Automated-Interpreting-Assessment-with-Explainable-AI-in-College-Classrooms" class="headerlink" title="115. From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms"></a>115. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/From_Black_Box_to_Transparency__Enhancing_Automated_Interpreting_Assessment_with_Explainable_AI_in_C.pdf">From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本论文提出了一种结合特征工程、变分自编码器（VAE）数据增强和可解释机器学习（如SHAP分析）的多维自动口译质量评价框架。方法在英中口译数据集上验证，重点提升对语言使用、流畅度和信息完整性的可解释预测能力，结果显示VAE增强显著提升模型表现，且各维度的关键特征均被精确识别，为教学反馈和自我调节学习提供了透明、可扩展的自动评分工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/From_Black_Box_to_Transparency_Enhancing_Automated_Interpreting_Assessment_with_Explainable_AI_in_College_Classrooms.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="116-Who-Benefits-from-AI-Explanations-Towards-Accessible-and-Interpretable-Systems"><a href="#116-Who-Benefits-from-AI-Explanations-Towards-Accessible-and-Interpretable-Systems" class="headerlink" title="116. Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems"></a>116. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Who_Benefits_from_AI_Explanations__Towards_Accessible_and_Interpretable_Systems.pdf">Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ontario Tech University</span></p>
<p>本文采用文献综述和四步法案例研究，系统评估现有可解释人工智能（XAI）技术在无障碍领域的缺口，尤其关注视觉障碍用户。研究设计并测试了面向盲人交通管理者的XAI原型，采用LIME和SHAP解释方法，结果发现简化、多模态解释更易于非视觉用户理解，强调需为不同能力用户设计多样化解释方式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Who_Benefits_from_AI_Explanations_Towards_Accessible_and_Interpretable_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="117-Insights-from-the-Algonauts-2025-Winners"><a href="#117-Insights-from-the-Algonauts-2025-Winners" class="headerlink" title="117. Insights from the Algonauts 2025 Winners"></a>117. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Insights_from_the_Algonauts_2025_Winners.pdf">Insights from the Algonauts 2025 Winners</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Medical AI Research Center (MedARC)</span></p>
<p>本论文总结了Algonauts 2025竞赛中的顶尖团队方法，主要采用预训练多模态特征提取器（视觉、音频和文本），结合transformer、RNN或线性卷积架构，通过模型集成预测自然电影刺激下的人类脑区fMRI反应。结果显示，架构复杂度影响有限，模型集成与多模态特征融合是提升脑编码准确性的关键，未来突破需跳出现有范式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Insights_from_the_Algonauts_2025_Winners.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="118-APFL-Analytic-Personalized-Federated-Learning-via-Dual-Stream-Least-Squares"><a href="#118-APFL-Analytic-Personalized-Federated-Learning-via-Dual-Stream-Least-Squares" class="headerlink" title="118. APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares"></a>118. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/APFL__Analytic_Personalized_Federated_Learning_via_Dual-Stream_Least_Squares.pdf">APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CSU, China</span></p>
<p>该论文提出了APFL方法，利用冻结的基础模型（如ViT）进行特征提取，并通过双流解析模型（主流用于全局泛化，细化流用于本地个性化）实现个性化联邦学习，采用解析最小二乘法避免梯度更新对非IID数据分布的敏感性。实验结果显示APFL在多个数据集和不同非IID程度下均优于现有方法，理论分析证明其具备异质性不变性、隐私保护及高效率，结论为APFL有效提升个性化模型性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/APFL_Analytic_Personalized_Federated_Learning_via_Dual-Stream_Least_Squares.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="119-FreeGAD-A-Training-Free-yet-Effective-Approach-for-Graph-Anomaly-Detection"><a href="#119-FreeGAD-A-Training-Free-yet-Effective-Approach-for-Graph-Anomaly-Detection" class="headerlink" title="119. FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection"></a>119. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/FreeGAD__A_Training-Free_yet_Effective_Approach_for_Graph_Anomaly_Detection.pdf">FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Guangxi University</span></p>
<p>本文提出FreeGAD，一种无需训练的图异常检测方法，采用亲和力门控残差编码器生成异常感知表示，并通过锚点节点筛选和引导统计偏差计算异常分数。实验结果表明，FreeGAD在多个真实和合成数据集上实现了优越的检测性能、极高效率和良好扩展性，显著优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FreeGAD_A_Training-Free_yet_Effective_Approach_for_Graph_Anomaly_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="120-Deep-Learning-in-Classical-and-Quantum-Physics"><a href="#120-Deep-Learning-in-Classical-and-Quantum-Physics" class="headerlink" title="120. Deep Learning in Classical and Quantum Physics"></a>120. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Deep_Learning_in_Classical_and_Quantum_Physics.pdf">Deep Learning in Classical and Quantum Physics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ICFO – Institut de Ciències Fotòniques</span></p>
<p>本论文系统梳理了深度学习（Deep Learning）和机器学习（Machine Learning）的基础理论、关键算法（如神经网络、自动微分、损失函数设计等），并重点介绍了其在经典物理和量子物理中的应用，包括变分量子本征求解器（VQE）、神经网络量子态（NQS）、量子态层析等。作者以物理视角阐释了深度学习在高维物理系统中的建模优势及其局限，指出深度学习可辅助物理研究发现模式，但尚不能替代理解物理机制。结论认为深度学习为物理学尤其是量子科学提供了强大工具，但其科学解释性与知识获取仍需进一步突破。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Deep_Learning_in_Classical_and_Quantum_Physics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="121-Self-Supervised-Temporal-Super-Resolution-of-Energy-Data-using-Generative-Adversarial-Transformers"><a href="#121-Self-Supervised-Temporal-Super-Resolution-of-Energy-Data-using-Generative-Adversarial-Transformers" class="headerlink" title="121. Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformers"></a>121. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Self-Supervised_Temporal_Super-Resolution_of_Energy_Data_using_Generative_Adversarial_Transformer.pdf">Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Karlsruhe Institute of Technology</span></p>
<p>该论文提出了一种无需高分辨率数据的自监督时间序列超分辨率方法，结合了Transformer的序列建模能力和GAN的对抗训练框架，并通过特征空间损失实现低分辨率到高分辨率数据的一致映射。实验结果表明，该方法在多种能源数据集上超越了传统插值与现有深度学习模型，RMSE降低9%、在MPC应用场景精度提升13%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Self-Supervised_Temporal_Super-Resolution_of_Energy_Data_using_Generative_Adversarial_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="122-FAKE-SPEECH-WILD-DETECTING-DEEPFAKE-SPEECH-ON-SOCIAL-MEDIA-PLATFORM"><a href="#122-FAKE-SPEECH-WILD-DETECTING-DEEPFAKE-SPEECH-ON-SOCIAL-MEDIA-PLATFORM" class="headerlink" title="122. FAKE SPEECH WILD: DETECTING DEEPFAKE SPEECH ON SOCIAL MEDIA PLATFORM"></a>122. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Fake_Speech_Wild__Detecting_Deepfake_Speech_on_Social_Media_Platform.pdf">FAKE SPEECH WILD: DETECTING DEEPFAKE SPEECH ON SOCIAL MEDIA PLATFORM</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Communication University of China</span></p>
<p>本文提出了Fake Speech Wild (FSW) 数据集，收集了来自四个社交媒体平台、共254小时的真实与深度伪造语音数据。采用自监督学习技术和多种数据增强方法（如MUSAN &amp; RIR噪声、Rawboost），联合训练多源数据集后，显著提升了深度伪造语音检测模型在跨领域场景下的鲁棒性，最终在全部测试集上达到3.54%的平均等错误率（EER）。结论：联合多域数据和数据增强显著提升了检测性能，FSW数据集为真实环境下伪造语音检测提供了重要基准。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/FAKE_SPEECH_WILD_DETECTING_DEEPFAKE_SPEECH_ON_SOCIAL_MEDIA_PLATFORM.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="123-Contrastive-ECOC-Learning-Output-Codes-for-Adversarial-Defense"><a href="#123-Contrastive-ECOC-Learning-Output-Codes-for-Adversarial-Defense" class="headerlink" title="123. Contrastive ECOC: Learning Output Codes for Adversarial Defense"></a>123. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Contrastive_ECOC__Learning_Output_Codes_for_Adversarial_Defense.pdf">Contrastive ECOC: Learning Output Codes for Adversarial Defense</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Central University</span></p>
<p>本文提出了三种基于对比学习的自动化编码本学习（ACL）方法，用于多分类任务中的自动生成Error Correcting Output Codes（ECOC）编码本，有效提升神经网络对抗攻击（如FGSM和PGD）的鲁棒性。实验表明，所提模型能够自动生成数据集特定的编码本，在多个开放数据集上对抗攻击下的表现优于传统手工或随机编码本。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Contrastive_ECOC_Learning_Output_Codes_for_Adversarial_Defense.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="124-GraphFedMIG-Tackling-Class-Imbalance-in-Federated-Graph-Learning-via-Mutual-Information-Guided-Generation"><a href="#124-GraphFedMIG-Tackling-Class-Imbalance-in-Federated-Graph-Learning-via-Mutual-Information-Guided-Generation" class="headerlink" title="124. GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation"></a>124. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/GraphFedMIG__Tackling_Class_Imbalance_in_Federated_Graph_Learning_via_Mutual_Information-Guided_Gene.pdf">GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chongqing University</span></p>
<p>该论文提出GraphFedMIG框架，通过将联邦图学习中的类别不平衡问题转化为生成式数据增强任务，采用分层生成对抗网络（GAN）和互信息引导的模型聚合机制。实验表明，GraphFedMIG在四个真实数据集上显著提升了少数类的识别能力并整体优于当前主流方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/GraphFedMIG_Tackling_Class_Imbalance_in_Federated_Graph_Learning_via_Mutual_Information-Guided_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="125-HiRef-Leveraging-Hierarchical-Ontology-and-Network-Refinement-for-Robust-Medication-Recommendation"><a href="#125-HiRef-Leveraging-Hierarchical-Ontology-and-Network-Refinement-for-Robust-Medication-Recommendation" class="headerlink" title="125. HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation"></a>125. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/HiRef__Leveraging_Hierarchical_Ontology_and_Network_Refinement_for_Robust_Medication_Recommendation.pdf">HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea University</span></p>
<p>该论文提出HiRef框架，将医疗本体的层次语义嵌入（使用超曲空间）与电子病历（EHR）中稀疏优化的共现图结构相结合，通过自适应融合机制提升药物推荐模型对罕见和未见医疗编码的泛化能力。实验表明，HiRef在MIMIC-III&#x2F;IV数据集下表现优异，尤其在未见编码场景下准确率高，且具备更好的鲁棒性和可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/HiRef_Leveraging_Hierarchical_Ontology_and_Network_Refinement_for_Robust_Medication_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="126-Mitigating-Exponential-Mixed-Frequency-Growth-through-Frequency-Selection-and-Dimensional-Separation-in-Quantum-Machine-Learning"><a href="#126-Mitigating-Exponential-Mixed-Frequency-Growth-through-Frequency-Selection-and-Dimensional-Separation-in-Quantum-Machine-Learning" class="headerlink" title="126. Mitigating Exponential Mixed Frequency Growth through Frequency Selection and Dimensional Separation in Quantum Machine Learning"></a>126. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Mitigating_Exponential_Mixed_Frequency_Growth_through_Frequency_Selection_and_Dimensional_Separation.pdf">Mitigating Exponential Mixed Frequency Growth through Frequency Selection and Dimensional Separation in Quantum Machine Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute for Informatics, LMU Munich</span></p>
<p>本文针对量子机器学习中多维傅里叶频谱导致参数双指数级增长、难以在当前硬件上拟合实际问题的瓶颈，提出了频率选择与维度分离两项关键技术，通过仅保留必要频率和混合频率组，极大减少所需模型参数。实验表明，这两种方法能在噪声模拟器及真实量子硬件上实现高精度回归拟合，突破了默认模型频谱密集导致的拟合失败，理论与实践上均有效缓解了参数爆炸问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Mitigating_Exponential_Mixed_Frequency_Growth_through_Frequency_Selection_and_Dimensional_Separation_in_Quantum_Machine_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="127-RealAC-A-Domain-Agnostic-Framework-for-Realistic-and-Actionable-Counterfactual-Explanations"><a href="#127-RealAC-A-Domain-Agnostic-Framework-for-Realistic-and-Actionable-Counterfactual-Explanations" class="headerlink" title="127. RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations"></a>127. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/RealAC__A_Domain-Agnostic_Framework_for_Realistic_and_Actionable_Counterfactual_Explanations.pdf">RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Arizona State University</span></p>
<p>本文提出了RealAC，一个面向领域无关、能够生成现实且可操作反事实解释的框架。RealAC通过最小化反事实与原始样本之间所有特征对的互信息差异，在无需领域知识的情况下自动保持复杂的特征依赖关系，并结合二进制掩码机制支持用户指定不可变特征，实现个性化可行性约束。实验表明，RealAC在因果边缘分数、依赖保持分数和IM1等指标上优于现有主流方法及大语言模型生成的反事实，兼顾了反事实的现实性与可操作性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/RealAC_A_Domain-Agnostic_Framework_for_Realistic_and_Actionable_Counterfactual_Explanations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="128-Semantic-Communication-with-Distribution-Learning-through-Sequential-Observations"><a href="#128-Semantic-Communication-with-Distribution-Learning-through-Sequential-Observations" class="headerlink" title="128. Semantic Communication with Distribution Learning through Sequential Observations"></a>128. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Semantic_Communication_with_Distribution_Learning_through_Sequential_Observations.pdf">Semantic Communication with Distribution Learning through Sequential Observations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Dalhousie University</span></p>
<p>本文提出了语义通信中的分布学习理论框架，研究在先验分布未知且编码方案与信道固定时接收端如何通过观察序列学习语义分布。方法上，作者提出以有效传输矩阵的满秩性为可学习性条件，推导出估计误差和性能损失的收敛界，并理论证明估计误差与样本数成O(1&#x2F;√T)衰减；结论表明，良好条件数的系统能实现高效学习和适应，压缩优化型编码则显著降低长期学习能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Semantic_Communication_with_Distribution_Learning_through_Sequential_Observations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="129-eMamba-Efficient-Acceleration-Framework-for-Mamba-Models-in-Edge-Computing"><a href="#129-eMamba-Efficient-Acceleration-Framework-for-Mamba-Models-in-Edge-Computing" class="headerlink" title="129. eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing"></a>129. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/eMamba__Efficient_Acceleration_Framework_for_Mamba_Models_in_Edge_Computing.pdf">eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Ulsan</span></p>
<p>本文提出了eMamba，一种面向边缘计算的Mamba模型端到端硬件加速框架。方法包括用硬件友好的归一化、分段线性近似及量化技术替代原有复杂操作，并结合近似感知的神经架构搜索优化可训练参数。实验表明，在Fashion-MNIST、CIFAR-10和MARS等视觉任务中，eMamba在保持精度的同时大幅减少参数量，并在FPGA和ASIC实现中获得更低延迟、更高吞吐率和显著的能耗降低，可广泛适用于边缘AI应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/eMamba_Efficient_Acceleration_Framework_for_Mamba_Models_in_Edge_Computing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="130-Layer-Wise-Analysis-of-Self-Supervised-Representations-for-Age-and-Gender-Classification-in-Children’s-Speech"><a href="#130-Layer-Wise-Analysis-of-Self-Supervised-Representations-for-Age-and-Gender-Classification-in-Children’s-Speech" class="headerlink" title="130. Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Children’s Speech"></a>130. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Layer-Wise_Analysis_of_Self-Supervised_Representations_for_Age_and_Gender_Classification_in_Children.pdf">Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Children’s Speech</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">NIT Sikkim</span></p>
<p>该论文提出了一种针对儿童语音年龄和性别分类的层级特征分析方法，利用四种Wav2Vec2自监督学习模型提取层级特征，并通过CNN分类器和主成分分析（PCA）进行降维和性能优化。实验结果显示，模型早期层能更有效地捕捉说话人特征，降维后不仅提升了分类准确率也提高了计算效率，揭示了特征在模型深度中的分布规律。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Layer-Wise_Analysis_of_Self-Supervised_Representations_for_Age_and_Gender_Classification_in_Children%E2%80%99s_Speech.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="131-A-Vision-Language-Pre-training-Model-Guided-Approach-for-Mitigating-Backdoor-Attacks-in-Federated-Learning"><a href="#131-A-Vision-Language-Pre-training-Model-Guided-Approach-for-Mitigating-Backdoor-Attacks-in-Federated-Learning" class="headerlink" title="131. A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning"></a>131. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/A_Vision-Language_Pre-training_Model-Guided_Approach_for_Mitigating_Backdoor_Attacks_in_Federated_Le.pdf">A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>该论文提出CLIP-Fed框架，利用视觉-语言预训练模型（如CLIP）的跨模态零样本学习能力，实现联邦学习中的前聚合恶意模型过滤和后聚合后门净化。通过多模态大模型和频域分析构建服务器数据集，并结合原型对比损失和知识蒸馏，纠正特征偏移、削弱后门关联。实验表明，CLIP-Fed在多种数据分布和攻击类型下显著降低ASR并提升主任务准确率，优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/A_Vision-Language_Pre-training_Model-Guided_Approach_for_Mitigating_Backdoor_Attacks_in_Federated_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="132-Pruning-and-Malicious-Injection-A-Retraining-Free-Backdoor-Attack-on-Transformer-Models"><a href="#132-Pruning-and-Malicious-Injection-A-Retraining-Free-Backdoor-Attack-on-Transformer-Models" class="headerlink" title="132. Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models"></a>132. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-15/Pruning_and_Malicious_Injection__A_Retraining-Free_Backdoor_Attack_on_Transformer_Models.pdf">Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Louisiana State University</span></p>
<p>本文提出了HPMI（Head-wise Pruning and Malicious Injection），一种无需额外重训练且不改变模型架构的变换器后门攻击方法。通过剪除多头注意力模块中最不重要的头，再注入预训练的恶意头，实现后门植入，理论与实验均表明在保持干净数据准确率的同时，能以超99.55%的成功率绕过四种主流防御机制。结论显示HPMI对现有防御手段有很强的隐蔽性和鲁棒性，且对模型性能影响极小。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-18/Pruning_and_Malicious_Injection_A_Retraining-Free_Backdoor_Attack_on_Transformer_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/18报纸</div>
      <div>http://ywfhhh.github.io/2025/08/2025-08-18_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>具身ADT</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月18日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/2025-08-19_article/" title="2025/08/19报纸">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">2025/08/19报纸</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-15_article/" title="2025/08/15报纸">
                        <span class="hidden-mobile">2025/08/15报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
