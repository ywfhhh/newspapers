

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Ywfhhh">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;19报纸">
<meta property="og:url" content="http://example.com/2025/08/2025-08-19_article/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Optimal_CO2_storage_management_considering_safety_constraints_in_multi-stakeholder_multi-site_CCS_projects_a_game_theoretic_perspective.jpg">
<meta property="article:published_time" content="2025-08-19T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-19T08:31:46.075Z">
<meta property="article:author" content="Ywfhhh">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Optimal_CO2_storage_management_considering_safety_constraints_in_multi-stakeholder_multi-site_CCS_projects_a_game_theoretic_perspective.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/19报纸 - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文库</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/19报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-19 00:00" pubdate>
          星期二, 八月 19日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          15k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/19报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/19报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-19<br>📄 <strong>发现论文数量</strong>：98  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Optimal-CO2-storage-management-considering-safety-constraints-in-multi-stakeholder-multi-site-CCS-projects-a-game-theoretic-perspective"><a href="#1-Optimal-CO2-storage-management-considering-safety-constraints-in-multi-stakeholder-multi-site-CCS-projects-a-game-theoretic-perspective" class="headerlink" title="1. Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Optimal_CO2_storage_management_considering_safety_constraints_in_multi-stakeholder_multi-site_CCS_pr.pdf">Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bureau of Economic Geology, University of Texas at Austin</span></p>
<p>本文提出了一种基于安全多智能体深度确定性策略梯度（MADDPG）和受限Markov博弈的框架，用于多利益相关方地质碳储存（GCS）项目的优化管理。方法结合Embed-to-Control（E2C）神经网络代理模型和分区奖励&#x2F;惩罚机制，实现了在安全约束下各方目标的最优兼顾。实验结果表明，该方法在协作与竞争场景下均能有效提升整体经济回报，确保注入安全，并优于传统多目标优化算法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Optimal_CO2_storage_management_considering_safety_constraints_in_multi-stakeholder_multi-site_CCS_projects_a_game_theoretic_perspective.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-SeamlessFlow-A-Trainer–Agent-Isolation-RL-Framework-Achieving-Bubble-Free-Pipelines-via-Tag-Scheduling"><a href="#2-SeamlessFlow-A-Trainer–Agent-Isolation-RL-Framework-Achieving-Bubble-Free-Pipelines-via-Tag-Scheduling" class="headerlink" title="2. SeamlessFlow: A Trainer–Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/SeamlessFlow__A_Trainer_Agent_Isolation_RL_Framework_Achieving_Bubble-Free_Pipelines_via_Tag_Schedul.pdf">SeamlessFlow: A Trainer–Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou</span></p>
<p>SeamlessFlow提出了一种服务器端强化学习框架，通过数据面层实现RL训练与多样化Agent的彻底隔离，并通过标签驱动的资源调度，将硬件抽象为能力标签资源，实现统一的空间-时间复用管道。该方法消除了分布式系统中的管道空闲，显著提升了大规模多Agent RL任务的训练效率和稳定性。实验显示其在工业级异构集群上具备极高的吞吐和可扩展性，显著优于现有主流RL框架。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SeamlessFlow_A_Trainer%E2%80%93Agent_Isolation_RL_Framework_Achieving_Bubble-Free_Pipelines_via_Tag_Scheduling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-Fusing-Rewards-and-Preferences-in-Reinforcement-Learning"><a href="#3-Fusing-Rewards-and-Preferences-in-Reinforcement-Learning" class="headerlink" title="3. Fusing Rewards and Preferences in Reinforcement Learning"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Fusing_Rewards_and_Preferences_in_Reinforcement_Learning.pdf">Fusing Rewards and Preferences in Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer and Communication Sciences, EPFL</span></p>
<p>本文提出了Dual-Feedback Actor (DFA)算法，将数值奖励和成对偏好反馈统一到单一的策略更新规则中，关键技术包括直接利用策略的对数概率建模偏好概率、无须奖励建模、支持on-&#x2F;off-policy学习，以及通过Q值合成偏好对。理论上，DFA的偏好损失最小化等价于恢复熵正则化Soft Actor-Critic (SAC)策略，实验表明在六个控制任务和GridWorld环境中，DFA的性能与SAC相当或优于SAC，对比奖励建模的RLHF基线表现更优且训练更平稳。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Fusing_Rewards_and_Preferences_in_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-HOID-R1-Reinforcement-Learning-for-Open-World-Human-Object-Interaction-Detection-Reasoning-with-Multimodal-Large-Language-Model"><a href="#4-HOID-R1-Reinforcement-Learning-for-Open-World-Human-Object-Interaction-Detection-Reasoning-with-Multimodal-Large-Language-Model" class="headerlink" title="4. HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/HOID-R1__Reinforcement_Learning_for_Open-World_Human-Object_Interaction_Detection_Reasoning_with_Mul.pdf">HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ShanghaiTech University</span></p>
<p>本文提出了HOID-R1，一种结合链式思维（CoT）监督微调和群组相对策略优化（GRPO）的强化学习框架，实现开放世界下的人体-物体交互（HOI）检测。该方法利用多模态大语言模型作为判官监督推理过程，通过多重奖励信号优化策略，并有效缓解推理幻觉，显著提升了模型在各类开放词汇场景下的泛化和检测准确性。在HICO-DET和SWIG-HOI数据集上的实验结果表明，HOID-R1在已见和未见场景均超越现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/HOID-R1_Reinforcement_Learning_for_Open-World_Human-Object_Interaction_Detection_Reasoning_with_Multimodal_Large_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="5-Investigating-Sensors-and-Methods-in-Grasp-State-Classification-in-Agricultural-Manipulation"><a href="#5-Investigating-Sensors-and-Methods-in-Grasp-State-Classification-in-Agricultural-Manipulation" class="headerlink" title="5. Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Investigating_Sensors_and_Methods_in_Grasp_State_Classification_in_Agricultural_Manipulation.pdf">Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois at Urbana-Champaign</span></p>
<p>本文提出了一种用于农业抓取状态分类的多传感器系统，将IMU、红外、张力、触觉和RGB摄像头集成到顺应性机械手中，并利用随机森林和LSTM模型对抓取状态（滑移、抓取失败、成功采摘等）进行分类。实验结果表明，随机森林模型在实验室训练并迁移到真实番茄植株上，能以100%准确率识别多种抓取状态，IMU与张力传感器的组合已足以实现高效分类，这为果实自动采摘提供了实时反馈和纠正动作的依据。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Investigating_Sensors_and_Methods_in_Grasp_State_Classification_in_Agricultural_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-Visual-Perception-Engine-Fast-and-Flexible-Multi-Head-Inference-for-Robotic-Vision-Tasks"><a href="#6-Visual-Perception-Engine-Fast-and-Flexible-Multi-Head-Inference-for-Robotic-Vision-Tasks" class="headerlink" title="6. Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Visual_Perception_Engine__Fast_and_Flexible_Multi-Head_Inference_for_Robotic_Vision_Tasks.pdf">Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Jet Propulsion Laboratory, California Institute of Technology</span></p>
<p>本文提出了VPEngine框架，通过共享视觉基础模型（如DINOv2）骨干，实现多任务（深度估计、目标检测、语义分割）模型头高效并行推理，采用CUDA MPS和GPU内存共享结构，显著提升推理速度和GPU利用率。实验表明，该方法在NVIDIA Jetson Orin AGX上可实现高达3.3倍速度提升，且具备可动态调整任务频率、内存可控、模块化扩展性，适合多种机器人视觉场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Visual_Perception_Engine_Fast_and_Flexible_Multi-Head_Inference_for_Robotic_Vision_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-Towards-Fully-Onboard-State-Estimation-and-Trajectory-Tracking-for-UAVs-with-Suspended-Payloads"><a href="#7-Towards-Fully-Onboard-State-Estimation-and-Trajectory-Tracking-for-UAVs-with-Suspended-Payloads" class="headerlink" title="7. Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Towards_Fully_Onboard_State_Estimation_and_Trajectory_Tracking_for_UAVs_with_Suspended_Payloads.pdf">Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Czech Technical University in Prague</span></p>
<p>该论文提出了一种面向无人机悬挂载荷的全自主、实用、低硬件需求的状态估计与轨迹跟踪框架。方法采用标准机载RTK GNSS与IMU，结合线性卡尔曼滤波器进行状态估计、模型预测轮廓控制（MPCC）规划平滑轨迹，并用增量式模型预测控制（MPC）实现鲁棒闭环控制。大量仿真和实地实验表明，该系统在仅用标准传感器的条件下，表现接近理想地面真值基准，跟踪误差增加小于6%，且对载荷参数变化具有很强鲁棒性，展示了面向实际无人机悬挂运输的可行性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Towards_Fully_Onboard_State_Estimation_and_Trajectory_Tracking_for_UAVs_with_Suspended_Payloads.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-MultiPark-Multimodal-Parking-Transformer-with-Next-Segment-Prediction"><a href="#8-MultiPark-Multimodal-Parking-Transformer-with-Next-Segment-Prediction" class="headerlink" title="8. MultiPark: Multimodal Parking Transformer with Next-Segment Prediction"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/MultiPark__Multimodal_Parking_Transformer_with_Next-Segment_Prediction.pdf">MultiPark: Multimodal Parking Transformer with Next-Segment Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了MultiPark，一种基于自回归Transformer的多模态自动泊车方法。其方法创新包括数据高效的next-segment预测范式、结合齿轮&#x2F;纵向&#x2F;横向因子的可学习泊车查询、并行多模态轨迹解码，以及两阶段训练策略（模仿损失+目标导向损失）以缓解因果混淆并提升泛化能力。实验表明MultiPark在真实数据集和实际车辆部署中均取得了优异的泊车成功率和安全性，显著优于同类方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MultiPark_Multimodal_Parking_Transformer_with_Next-Segment_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-Sim2Dust-Mastering-Dynamic-Waypoint-Tracking-on-Granular-Media"><a href="#9-Sim2Dust-Mastering-Dynamic-Waypoint-Tracking-on-Granular-Media" class="headerlink" title="9. Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Sim2Dust__Mastering_Dynamic_Waypoint_Tracking_on_Granular_Media.pdf">Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Luxembourg</span></p>
<p>本论文提出了一个完整的sim-to-real框架，用于在类月球颗粒介质上实现轮式机器人动态航点跟踪。方法上，利用Space Robotics Bench进行大规模并行仿真，结合程序化生成环境与物理参数域随机化，采用DreamerV3等强化学习算法训练策略，并通过零样本迁移在真实月壤实验场地实现精确稳定的导航。实验结果表明，程序化多样性训练显著提升了真实部署的泛化和稳定性，模型基DreamerV3在样本效率和精度上表现最佳，简单动作平滑可有效提升硬件部署的安全性和稳定性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Sim2Dust_Mastering_Dynamic_Waypoint_Tracking_on_Granular_Media.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-Relative-Position-Matters-Trajectory-Prediction-and-Planning-with-Polar-Representation"><a href="#10-Relative-Position-Matters-Trajectory-Prediction-and-Planning-with-Polar-Representation" class="headerlink" title="10. Relative Position Matters: Trajectory Prediction and Planning with Polar Representation"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Relative_Position_Matters__Trajectory_Prediction_and_Planning_with_Polar_Representation.pdf">Relative Position Matters: Trajectory Prediction and Planning with Polar Representation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本文提出Polaris框架，首次在轨迹预测与规划中采用极坐标系，通过Polar场景上下文编码和Polar关系细化模块，并引入Relative Embedding Transformer显式建模交通要素间的相对距离与方向。实验在Argoverse 2和nuPlan数据集上表明，该方法在预测准确性和推理效率上均达到SOTA水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Relative_Position_Matters_Trajectory_Prediction_and_Planning_with_Polar_Representation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-i2Nav-Robot-A-Large-Scale-Indoor-Outdoor-Robot-Dataset-for-Multi-Sensor-Fusion-Navigation-and-Mapping"><a href="#11-i2Nav-Robot-A-Large-Scale-Indoor-Outdoor-Robot-Dataset-for-Multi-Sensor-Fusion-Navigation-and-Mapping" class="headerlink" title="11. i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/i2Nav-Robot__A_Large-Scale_Indoor-Outdoor_Robot_Dataset_for_Multi-Sensor_Fusion_Navigation_and_Mappi.pdf">i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wuhan University</span></p>
<p>该论文提出了i2Nav-Robot，一个用于室内外环境多传感器融合导航与建图的大规模数据集。方法包括集成最新固态激光雷达、4D毫米波雷达、立体摄像头、里程计、GNSS和高精度IMU于全向轮式机器人，并采用硬件和离线校准实现微秒级时间同步，采集覆盖多样场景的十条长序列，提供厘米级高频地面真实数据。结论是该数据集已被十余种开源多传感器系统验证，数据质量优异，可广泛用于机器人导航、SLAM及融合方法研究。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/i2Nav-Robot_A_Large-Scale_Indoor-Outdoor_Robot_Dataset_for_Multi-Sensor_Fusion_Navigation_and_Mapping.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-OVSegDT-Segmenting-Transformer-for-Open-Vocabulary-Object-Goal-Navigation"><a href="#12-OVSegDT-Segmenting-Transformer-for-Open-Vocabulary-Object-Goal-Navigation" class="headerlink" title="12. OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/OVSegDT__Segmenting_Transformer_for_Open-Vocabulary_Object_Goal_Navigation.pdf">OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AIRI</span></p>
<p>本文提出OVSegDT，一种轻量级transformer策略，通过集成目标二值分割掩码编码器和辅助分割损失，实现对开放词汇物体目标导航任务的高效学习。方法还引入熵自适应损失调节（EALM），根据策略熵自动平衡模仿学习与强化学习信号，无需手动切换阶段。实验结果表明，该方法在未见类别上导航性能与见过类别一致，训练样本复杂度降低33%，碰撞次数减少一半，且优于现有RGB-only和部分大模型方法。结论：OVSegDT显著提升了机器人在新环境下对未知物体类别的导航能力，具备高泛化性和安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/OVSegDT_Segmenting_Transformer_for_Open-Vocabulary_Object_Goal_Navigation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-ReachVox-Clutter-free-Reachability-Visualization-for-Robot-Motion-Planning-in-Virtual-Reality"><a href="#13-ReachVox-Clutter-free-Reachability-Visualization-for-Robot-Motion-Planning-in-Virtual-Reality" class="headerlink" title="13. ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ReachVox__Clutter-free_Reachability_Visualization_for_Robot_Motion_Planning_in_Virtual_Reality.pdf">ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Coburg University of Applied Sciences and Arts</span></p>
<p>该论文提出了一种名为REACHVOX的新型可视化方法，通过在虚拟现实中以半透明有色体素的方式显示机器人在任务空间的可达区域，显著减少了视觉杂乱，帮助远程操作者高效规划机器人运动。用户实验表明，REACHVOX相比传统点可达性检查，在任务完成时间、操作次数、易用性和信任度等主客观指标上均有显著提升，用户更倾向于使用该方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ReachVox_Clutter-free_Reachability_Visualization_for_Robot_Motion_Planning_in_Virtual_Reality.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-Optimizing-ROS-2-Communication-for-Wireless-Robotic-Systems"><a href="#14-Optimizing-ROS-2-Communication-for-Wireless-Robotic-Systems" class="headerlink" title="14. Optimizing ROS 2 Communication for Wireless Robotic Systems"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Optimizing_ROS_2_Communication_for_Wireless_Robotic_Systems.pdf">Optimizing ROS 2 Communication for Wireless Robotic Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">DGIST</span></p>
<p>该论文针对无线环境下ROS 2 DDS通信栈在大数据载体传输时的瓶颈（如IP分片、重传时机、缓冲区突发）进行了系统建模和网络层深入分析，并提出了一种基于标准XML QoS配置的轻量级DDS优化框架，无需协议或应用修改即可提升无线通信的鲁棒性和实时性。实验证明该方法在各种无线场景下显著提高了数据接收率、降低了延迟和抖动，尤其在高丢包和链路中断条件下表现优异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Optimizing_ROS_2_Communication_for_Wireless_Robotic_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-CRAFT-GUI-Curriculum-Reinforced-Agent-For-GUI-Tasks"><a href="#15-CRAFT-GUI-Curriculum-Reinforced-Agent-For-GUI-Tasks" class="headerlink" title="15. CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CRAFT-GUI__Curriculum-Reinforced_Agent_For_GUI_Tasks.pdf">CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ant Group</span></p>
<p>该论文提出了CRAFT-GUI，一种结合分阶段课程学习和分组相对策略优化（GRPO）的强化学习框架，专门针对图形用户界面（GUI）任务。方法通过难度分层的任务训练和细粒度混合奖励机制提升了agent在移动设备上的操作与理解能力。实验结果表明，CRAFT-GUI在多个公开和内部基准上超过现有方法，任务成功率提升5.6%和10.3%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CRAFT-GUI_Curriculum-Reinforced_Agent_For_GUI_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-Swarm-in-Blocks-Simplifying-Drone-Swarm-Programming-with-Block-Based-Language"><a href="#16-Swarm-in-Blocks-Simplifying-Drone-Swarm-Programming-with-Block-Based-Language" class="headerlink" title="16. Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Swarm-in-Blocks__Simplifying_Drone_Swarm_Programming_with_Block-Based_Language.pdf">Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Sao Paulo</span></p>
<p>本文提出了Swarm-in-Blocks，一个基于模块化（Block-Based）编程语言的无人机集群高层接口，用户可通过拼接代码块轻松实现无人机编队的循环、条件与传感器操作，降低了ROS等底层知识门槛。该平台在Clover系统基础上，具备直观用户界面、实时监控、碰撞规避、队形与变换等关键技术，并已在仿真和实际应用中表现出良好可扩展性与易用性，适用于工业、农业、娱乐和教育等多场景。结论认为，该系统极大简化了无人机集群编程，提高了非专业用户和教育领域的可及性，未来将继续优化与拓展应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Swarm-in-Blocks_Simplifying_Drone_Swarm_Programming_with_Block-Based_Language.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-ImagiDrive-A-Unified-Imagination-and-Planning-Framework-for-Autonomous-Driving"><a href="#17-ImagiDrive-A-Unified-Imagination-and-Planning-Framework-for-Autonomous-Driving" class="headerlink" title="17. ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ImagiDrive__A_Unified_Imagination-and-Planning_Framework_for_Autonomous_Driving.pdf">ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>该论文提出ImagiDrive，一个将视觉语言模型（VLM）与驾驶世界模型（DWM）集成的自动驾驶端到端框架，通过想象与规划循环实现更安全高效的驾驶决策。方法包括VLM驱动的多模态驾驶代理、DWM驱动的场景想象器、早停和轨迹选择机制，框架在nuScenes和NAVSIM等数据集上显著优于现有方法。结论：ImagiDrive能提升复杂环境下的安全性和规划表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ImagiDrive_A_Unified_Imagination-and-Planning_Framework_for_Autonomous_Driving.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-Pedestrian-Dead-Reckoning-using-Invariant-Extended-Kalman-Filter"><a href="#18-Pedestrian-Dead-Reckoning-using-Invariant-Extended-Kalman-Filter" class="headerlink" title="18. Pedestrian Dead Reckoning using Invariant Extended Kalman Filter"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Pedestrian_Dead_Reckoning_using_Invariant_Extended_Kalman_Filter.pdf">Pedestrian Dead Reckoning using Invariant Extended Kalman Filter</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ShanghaiTech University</span></p>
<p>本文提出了一种面向双足机器人在无GPS环境下的低成本惯性行人航位推算方法，核心采用基于矩阵李群理论的InEKF（不变扩展卡尔曼滤波器），利用足部IMU的静止伪测量实现状态创新，有效提升估计精度。实验表明，在人体与双足机器人上，所提InEKF方法优于传统EKF，表现为轨迹误差更小、参数调优更容易，且对噪声协方差的敏感性更低。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Pedestrian_Dead_Reckoning_using_Invariant_Extended_Kalman_Filter.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-Tactile-Robotics-An-Outlook"><a href="#19-Tactile-Robotics-An-Outlook" class="headerlink" title="19. Tactile Robotics: An Outlook"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Tactile_Robotics__An_Outlook.pdf">Tactile Robotics: An Outlook</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">King’s College London</span></p>
<p>本论文系统性回顾了机器人触觉领域的关键技术，包括触觉材料、传感器、传感网络、仿真、数据采集与解释、多模态融合，以及主动触觉感知方法。文章总结认为，未来机器人触觉的发展需整合新型材料与高密度分布式传感、仿真工具、机器学习算法和多模态感知，推动机器人实现类人触觉灵巧，拓展其在制造、医疗、农业等多领域的应用。结论指出，尽管已取得显著进展，但触觉机器人的传感集成、数据处理、能量供给与大规模网络等仍具挑战，未来需跨学科协作持续创新以实现人机自然交互和环境敏感性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Tactile_Robotics_An_Outlook.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-Multi-Group-Equivariant-Augmentation-for-Reinforcement-Learning-in-Robot-Manipulation"><a href="#20-Multi-Group-Equivariant-Augmentation-for-Reinforcement-Learning-in-Robot-Manipulation" class="headerlink" title="20. Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Multi-Group_Equivariant_Augmentation_for_Reinforcement_Learning_in_Robot_Manipulation.pdf">Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong</span></p>
<p>该论文提出了一种基于多组非等距对称性的POMDP新建模方法，并设计了多组等变数据增强（MEA）策略，通过在空间和时间维度上对操作轨迹施加独立群变换，实现了更高多样性和有效性的数据扩充，结合离线强化学习和基于体素的视觉等变表征，显著提升了机器人操作任务的采样效率和泛化能力。实验结果表明，该方法在通用操作和外科抓取两大机器人领域均大幅提升了学习收敛速度与成功率，减少了97.5%以上的示范数据需求并降低训练时间超过58%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Multi-Group_Equivariant_Augmentation_for_Reinforcement_Learning_in_Robot_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-Visuomotor-Grasping-with-World-Models-for-Surgical-Robots"><a href="#21-Visuomotor-Grasping-with-World-Models-for-Surgical-Robots" class="headerlink" title="21. Visuomotor Grasping with World Models for Surgical Robots"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Visuomotor_Grasping_with_World_Models_for_Surgical_Robots.pdf">Visuomotor Grasping with World Models for Surgical Robots</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chinese University of Hong Kong</span></p>
<p>本文提出了GASv2框架，结合世界模型（DreamerV2）、专用视觉感知管线和混合控制策略，实现了在模拟环境中训练的视觉运动策略可直接迁移至实际外科机器人，仅依赖标准立体内窥镜视觉输入。实验表明，该方法在虚拟和实际外科场景中抓取成功率达65%，对未见物体、抓手及多类干扰具有强泛化和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Visuomotor_Grasping_with_World_Models_for_Surgical_Robots.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-Actor-Critic-for-Continuous-Action-Chunks-A-Reinforcement-Learning-Framework-for-Long-Horizon-Robotic-Manipulation-with-Sparse-Reward"><a href="#22-Actor-Critic-for-Continuous-Action-Chunks-A-Reinforcement-Learning-Framework-for-Long-Horizon-Robotic-Manipulation-with-Sparse-Reward" class="headerlink" title="22. Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Actor-Critic_for_Continuous_Action_Chunks__A_Reinforcement_Learning_Framework_for_Long-Horizon_Robot.pdf">Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>该论文提出了AC3，一种新颖的强化学习框架，针对长时序、稀疏奖励的机器人操作任务，直接学习生成高维连续动作序列。方法包含基于DDPG的actor-critic结构：actor采用不对称更新，仅从成功轨迹学习以保证策略稳定改进；critic利用intra-chunk n-step返回和自监督奖励模块提升学习稳定性和数据效率。实验证明，AC3在BiGym和RLBench 25个任务上，仅用少量专家演示和简单模型结构即优于现有方法，展现出在复杂操作任务中的高效稳定性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Actor-Critic_for_Continuous_Action_Chunks_A_Reinforcement_Learning_Framework_for_Long-Horizon_Robotic_Manipulation_with_Sparse_Reward.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-Geometry-Aware-Predictive-Safety-Filters-on-Humanoids-From-Poisson-Safety-Functions-to-CBF-Constrained-MPC"><a href="#23-Geometry-Aware-Predictive-Safety-Filters-on-Humanoids-From-Poisson-Safety-Functions-to-CBF-Constrained-MPC" class="headerlink" title="23. Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Geometry-Aware_Predictive_Safety_Filters_on_Humanoids__From_Poisson_Safety_Functions_to_CBF_Constrai.pdf">Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">California Institute of Technology</span></p>
<p>本文提出了一种基于非线性模型预测控制（MPC）与控制屏障函数（CBF）的几何感知预测安全滤波方法，针对动态和非结构化环境中的人形及四足机器人安全导航。方法通过扩展泊松安全函数，将其定义于时间和姿态的高维空间，并结合MPC实现实时安全约束轨迹生成，且利用Minkowski运算精确建模机器人几何。实验在多种安全关键场景下验证了该方法对机器人安全避障和复杂环境导航的有效性和实时性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Geometry-Aware_Predictive_Safety_Filters_on_Humanoids_From_Poisson_Safety_Functions_to_CBF_Constrained_MPC.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="24-LoRAtorio-An-intrinsic-approach-to-LoRA-Skill-Composition"><a href="#24-LoRAtorio-An-intrinsic-approach-to-LoRA-Skill-Composition" class="headerlink" title="24. LoRAtorio: An intrinsic approach to LoRA Skill Composition"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/LoRAtorio__An_intrinsic_approach_to_LoRA_Skill_Composition.pdf">LoRAtorio: An intrinsic approach to LoRA Skill Composition</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Cambridge Research Laboratory, Toshiba Europe</span></p>
<p>本文提出了一种无需训练的多LoRA技能组合方法LoRAtorio，通过在扩散模型的潜空间按空间patch划分，并计算LoRA与基础模型每个patch的余弦相似度，动态构建权重矩阵实现多LoRA输出的加权融合；同时在分类器自由引导机制中引入基础模型的无条件分数以缓解领域漂移。实验显示该方法在ComposLoRA基准和动态模块选择场景下均取得SOTA，提升了多概念生成的鲁棒性和图像质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LoRAtorio_An_intrinsic_approach_to_LoRA_Skill_Composition.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-DashCam-Video-A-complementary-low-cost-data-stream-for-on-demand-forest-infrastructure-system-monitoring"><a href="#25-DashCam-Video-A-complementary-low-cost-data-stream-for-on-demand-forest-infrastructure-system-monitoring" class="headerlink" title="25. DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/DashCam_Video__A_complementary_low-cost_data_stream_for_on-demand_forest-infrastructure_system_monit.pdf">DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Connecticut</span></p>
<p>本论文提出了一种基于车辆仪表盘摄像头（dashcam）视频的低成本、可扩展的城市道路绿化和基础设施实时监测框架。方法包含单目深度估计（Depth-anything V2模型）、XGBoost深度修正、GPS与图像三角测量定位、以及基于针孔相机几何的对象结构参数提取。实验结果显示，在内置摄像头和低速行驶条件下，定位误差平均为2.83米，结构参数（如高度）估计误差最低，并对树木等复杂目标仍有一定误差但整体表现优异。结论认为，该方法具备实时、低成本、自动化监测城市绿地与基础设施的潜力，适用于城市管理和智能交通等场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/DashCam_Video_A_complementary_low-cost_data_stream_for_on-demand_forest-infrastructure_system_monitoring.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-TrajSV-A-Trajectory-based-Model-for-Sports-Video-Representations-and-Applications"><a href="#26-TrajSV-A-Trajectory-based-Model-for-Sports-Video-Representations-and-Applications" class="headerlink" title="26. TrajSV: A Trajectory-based Model for Sports Video Representations and Applications"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/TrajSV__A_Trajectory-based_Model_for_Sports_Video_Representations_and_Applications.pdf">TrajSV: A Trajectory-based Model for Sports Video Representations and Applications</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huawei Technologies, Co., Ltd.</span></p>
<p>本文提出了TrajSV，一种基于轨迹的体育视频表征框架，包括数据预处理、轨迹增强Transformer模块的Clip Representation Network（CRNet）、以及聚合可视与轨迹特征的Video Representation Network（VRNet）。TrajSV采用三重对比损失在无监督方式下优化视频和片段表征，在体育视频检索、动作定位、视频字幕等多个任务上取得最优性能，显著提升了检索准确率和下游任务效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/TrajSV_A_Trajectory-based_Model_for_Sports_Video_Representations_and_Applications.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-Reinforcing-Video-Reasoning-Segmentation-to-Think-Before-It-Segments"><a href="#27-Reinforcing-Video-Reasoning-Segmentation-to-Think-Before-It-Segments" class="headerlink" title="27. Reinforcing Video Reasoning Segmentation to Think Before It Segments"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Reinforcing_Video_Reasoning_Segmentation_to_Think_Before_It_Segments.pdf">Reinforcing Video Reasoning Segmentation to Think Before It Segments</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Dalian University of Technology</span></p>
<p>该论文提出Veason-R1模型，通过两阶段训练实现视频推理分割：首先利用链式思考(CoT)数据进行有监督微调，赋予模型分层推理能力，然后采用基于GRPO的强化学习进一步优化时空定位和分割表现。结果表明，Veason-R1在多个基准上取得了领先性能，显著提升了分割准确性与鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Reinforcing_Video_Reasoning_Segmentation_to_Think_Before_It_Segments.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-Handwritten-Text-Recognition-of-Historical-Manuscripts-Using-Transformer-Based-Models"><a href="#28-Handwritten-Text-Recognition-of-Historical-Manuscripts-Using-Transformer-Based-Models" class="headerlink" title="28. Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Handwritten_Text_Recognition_of_Historical_Manuscripts_Using_Transformer-Based_Models.pdf">Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mississippi State University</span></p>
<p>本文采用TrOCR这一Transformer架构的手写文本识别模型，针对16世纪拉丁文历史手稿，提出并系统评估了多种针对性图像预处理与数据增强技术（包括四种新设计的历史手稿专用增强方法），并结合集成学习方法以提升识别精度。实验结果表明，Elastic Distortion增强和Top-5投票集成显著提升了字符错误率（CER），新方法在Gwalther数据集上CER降至1.60，相比先前最佳结果提升达50%，有效推动了历史手稿自动识别领域的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Handwritten_Text_Recognition_of_Historical_Manuscripts_Using_Transformer-Based_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-Data-Driven-Deepfake-Image-Detection-Method-The-2024-Global-Deepfake-Image-Detection-Challenge"><a href="#29-Data-Driven-Deepfake-Image-Detection-Method-The-2024-Global-Deepfake-Image-Detection-Challenge" class="headerlink" title="29. Data-Driven Deepfake Image Detection Method - The 2024 Global Deepfake Image Detection Challenge"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Data-Driven_Deepfake_Image_Detection_Method_--_The_2024_Global_Deepfake_Image_Detection_Challenge.pdf">Data-Driven Deepfake Image Detection Method - The 2024 Global Deepfake Image Detection Challenge</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AI Innovation Center, China Unicom</span></p>
<p>本文提出了一种基于Swin Transformer V2-B分类网络的深度伪造图像检测方法，通过多样的数据增强（如随机人脸区域Cutout、本地裁剪、卡通化等）和样本生成，提升模型泛化能力和鲁棒性。实验结果显示，该方法在多样化的深度伪造攻击下表现出较好的检测准确率和鲁棒性，最终在全球挑战赛中获得优异成绩。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Data-Driven_Deepfake_Image_Detection_Method_-_The_2024_Global_Deepfake_Image_Detection_Challenge.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-RMFAT-Recurrent-Multi-scale-Feature-Atmospheric-Turbulence-Mitigator"><a href="#30-RMFAT-Recurrent-Multi-scale-Feature-Atmospheric-Turbulence-Mitigator" class="headerlink" title="30. RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/RMFAT__Recurrent_Multi-scale_Feature_Atmospheric_Turbulence_Mitigator.pdf">RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Bristol</span></p>
<p>该论文提出了一种轻量级循环多尺度特征大气湍流缓解模型（RMFAT），通过将多帧视频恢复任务转化为双帧循环推理，结合多尺度特征提取、时序对齐和频率&#x2F;语义指导损失，有效提升了大气湍流视频的空间细节和时序一致性。实验表明，RMFAT在合成和真实数据集上均优于现有方法，恢复质量最佳且推理速度提升4倍以上，适合实时和资源受限场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/RMFAT_Recurrent_Multi-scale_Feature_Atmospheric_Turbulence_Mitigator.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration"><a href="#31-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration" class="headerlink" title="31. G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/G-CUT3R__Guided_3D_Reconstruction_with_Camera_and_Depth_Prior_Integration.pdf">G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Skoltech</span></p>
<p>该论文提出G-CUT3R，一种基于CUT3R的轻量级、模态无关的前馈式3D场景重建方法，通过整合深度、相机内参和位姿等先验信息，有效提升重建精度。方法采用多模态编码与ZeroConv融合技术，使模型可灵活整合多种数据源，实验表明相比当前主流方法在多项3D重建和相关任务上均实现性能提升，结论是G-CUT3R能生成更准确、细致的3D场景重建结果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/G-CUT3R_Guided_3D_Reconstruction_with_Camera_and_Depth_Prior_Integration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-Leveraging-the-RETFound-foundation-model-for-optic-disc-segmentation-in-retinal-images"><a href="#32-Leveraging-the-RETFound-foundation-model-for-optic-disc-segmentation-in-retinal-images" class="headerlink" title="32. Leveraging the RETFound foundation model for optic disc segmentation in retinal images"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Leveraging_the_RETFound_foundation_model_for_optic_disc_segmentation_in_retinal_images.pdf">Leveraging the RETFound foundation model for optic disc segmentation in retinal images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Dundee</span></p>
<p>本论文首次将RETFound视觉基础模型（基于ViT-large Transformer和自监督学习）适配并用于视网膜图像中的视盘分割任务，结合Segmenter的解码器，通过冻结基础模型权重，仅用少量带注释样本进行微调，实现了高效特征提取和分割。结果显示，在内部验证、领域泛化和领域适应实验中，模型在多个公开和私有数据集上取得约96% Dice分数，性能优于现有分割专用网络，且无需复杂数据增强，表现稳定且泛化能力强。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Leveraging_the_RETFound_foundation_model_for_optic_disc_segmentation_in_retinal_images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-Noise-Matters-Optimizing-Matching-Noise-for-Diffusion-Classifiers"><a href="#33-Noise-Matters-Optimizing-Matching-Noise-for-Diffusion-Classifiers" class="headerlink" title="33. Noise Matters: Optimizing Matching Noise for Diffusion Classifiers"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Noise_Matters__Optimizing_Matching_Noise_for_Diffusion_Classifiers.pdf">Noise Matters: Optimizing Matching Noise for Diffusion Classifiers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>该论文针对扩散分类器在噪声采样上的不稳定性首次进行系统分析，提出了“频率匹配”和“空间匹配”两大原则，并据此设计了噪声优化方法NoOp：通过优化数据集特定噪声和训练Meta-Network生成图像特定噪声偏置，替换随机噪声以实现稳定高效的分类。实验表明NoOp能提升Few-shot分类性能，兼容其他优化方法，具有较强稳定性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Noise_Matters_Optimizing_Matching_Noise_for_Diffusion_Classifiers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-AIM-Amending-Inherent-Interpretability-via-Self-Supervised-Masking"><a href="#34-AIM-Amending-Inherent-Interpretability-via-Self-Supervised-Masking" class="headerlink" title="34. AIM: Amending Inherent Interpretability via Self-Supervised Masking"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/AIM__Amending_Inherent_Interpretability_via_Self-Supervised_Masking.pdf">AIM: Amending Inherent Interpretability via Self-Supervised Masking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Max-Planck-Institute for Informatics</span></p>
<p>该论文提出了AIM方法，通过自监督特征掩码机制在卷积网络的多尺度特征融合过程中自动区分和保留与任务相关的“可靠”特征，同时抑制与标签无关的“伪”特征。AIM无需额外注释，仅依赖图像标签，通过可学习的二值掩码在网络前向阶段筛选特征，实现模型决策过程的内在可解释性。实验表明，AIM在ImageNet100、Waterbirds、CUB-200等多种数据集上提升了模型对真实特征的定位能力（EPG指标显著提升），并在分布外泛化和细粒度分类任务中提高了准确率。结论：AIM能够以极低的计算开销实现更高准确率和人类一致性的可解释性，适用于高鲁棒性和泛化性需求的视觉模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-Automated-Building-Heritage-Assessment-Using-Street-Level-Imagery"><a href="#35-Automated-Building-Heritage-Assessment-Using-Street-Level-Imagery" class="headerlink" title="35. Automated Building Heritage Assessment Using Street-Level Imagery"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Automated_Building_Heritage_Assessment_Using_Street-Level_Imagery.pdf">Automated Building Heritage Assessment Using Street-Level Imagery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Forschungszentrum Jülich GmbH</span></p>
<p>本论文提出了一种结合GPT大语言模型与机器学习，对街景建筑立面图像自动提取文化遗产价值特征的方法。通过GPT对建筑图像及辅助信息进行特征提取，再利用XGBoost等模型进行分类验证，最高宏F1分数达0.71。结论是该自动评估流程提升了建筑遗产价值识别的效率和准确性，可为能效改造和城市遗产保护提供支持。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Automated_Building_Heritage_Assessment_Using_Street-Level_Imagery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-CineTrans-Learning-to-Generate-Videos-with-Cinematic-Transitions-via-Masked-Diffusion-Models"><a href="#36-CineTrans-Learning-to-Generate-Videos-with-Cinematic-Transitions-via-Masked-Diffusion-Models" class="headerlink" title="36. CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CineTrans__Learning_to_Generate_Videos_with_Cinematic_Transitions_via_Masked_Diffusion_Models.pdf">CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>该论文提出了CineTrans框架，通过分析扩散模型中的注意力图，发现镜头内外的相关性差异，并基于此设计了块对角掩码机制，有效控制多镜头视频中的电影化转场。作者还构建了大规模多镜头视频文本数据集Cine250K，并提出专用评价指标。实验表明CineTrans在转场控制、时序一致性和整体质量均显著优于现有方法，推动了可控多镜头视频生成的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CineTrans_Learning_to_Generate_Videos_with_Cinematic_Transitions_via_Masked_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-SPG-Style-Prompting-Guidance-for-Style-Specific-Content-Creation"><a href="#37-SPG-Style-Prompting-Guidance-for-Style-Specific-Content-Creation" class="headerlink" title="37. SPG: Style-Prompting Guidance for Style-Specific Content Creation"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/SPG__Style-Prompting_Guidance_for_Style-Specific_Content_Creation.pdf">SPG: Style-Prompting Guidance for Style-Specific Content Creation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen University</span></p>
<p>本文提出了一种名为Style-Prompting Guidance (SPG)的训练自由采样策略，用于基于扩散模型的风格化图像生成。SPG通过构建风格噪声向量并结合无条件噪声的方向偏移，引导扩散过程朝向目标风格分布，并与Classifier-Free Guidance (CFG)集成，实现了语义一致性与风格一致性的平衡。实验结果表明，该方法在风格保真度和语义对齐方面优于现有方法，并可无缝集成现有扩散插件如ControlNet和IP-Adapter，具有高效、通用和实用性。结论是SPG能高效实现风格和内容双重一致的图像生成。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SPG_Style-Prompting_Guidance_for_Style-Specific_Content_Creation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-Inside-Knowledge-Graph-based-Path-Generation-with-Explainable-Data-Augmentation-and-Curriculum-Learning-for-Visual-Indoor-Navigation"><a href="#38-Inside-Knowledge-Graph-based-Path-Generation-with-Explainable-Data-Augmentation-and-Curriculum-Learning-for-Visual-Indoor-Navigation" class="headerlink" title="38. Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Inside_Knowledge__Graph-based_Path_Generation_with_Explainable_Data_Augmentation_and_Curriculum_Lear.pdf">Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Science and Technology POLITEHNICA Bucharest</span></p>
<p>本文提出了一种基于视觉输入的室内导航深度学习方法，通过创新的图结构路径生成、解释性数据增强（如Grad-CAM引导的遮挡）及课程学习，自动高效生成训练路径并提升模型鲁棒性。实验表明，该方法在无需额外传感器或场景地图的情况下，实现了高效、准确的室内导航，并能适应拥挤环境和分布外场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inside_Knowledge_Graph-based_Path_Generation_with_Explainable_Data_Augmentation_and_Curriculum_Learning_for_Visual_Indoor_Navigation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-LKFMixer-Exploring-Large-Kernel-Feature-For-Efficient-Image-Super-Resolution"><a href="#39-LKFMixer-Exploring-Large-Kernel-Feature-For-Efficient-Image-Super-Resolution" class="headerlink" title="39. LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/LKFMixer__Exploring_Large_Kernel_Feature_For_Efficient_Image_Super-Resolution.pdf">LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Yanshan University</span></p>
<p>本文提出了一种高效的轻量级图像超分辨率方法LKFMixer，该方法基于纯卷积神经网络（CNN），采用大卷积核（kernel size 31）配合坐标分解和部分通道卷积，模拟Transformer的非局部特征提取能力，并设计了空间特征调制模块和特征选择模块以适应局部与非局部特征融合。实验表明，LKFMixer在超分辨率性能和重建质量上优于当前主流轻量级方法，并显著提升推理速度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LKFMixer_Exploring_Large_Kernel_Feature_For_Efficient_Image_Super-Resolution.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-AnatoMaskGAN-GNN-Driven-Slice-Feature-Fusion-and-Noise-Augmentation-for-Medical-Semantic-Image-Synthesis"><a href="#40-AnatoMaskGAN-GNN-Driven-Slice-Feature-Fusion-and-Noise-Augmentation-for-Medical-Semantic-Image-Synthesis" class="headerlink" title="40. AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/AnatoMaskGAN__GNN-Driven_Slice_Feature_Fusion_and_Noise_Augmentation_for_Medical_Semantic_Image_Synt.pdf">AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southwest University</span></p>
<p>本文提出了AnatoMaskGAN，一种用于医学语义掩码图像合成的新型生成框架，结合了基于图神经网络（GNN）的切片特征融合模块，三维空间噪声注入机制，以及灰度-纹理联合分类器。实验结果显示，该方法在L2R-OASIS和L2R-Abdomen CT数据集上，PSNR和SSIM均显著超越现有主流方法，验证了其在结构一致性、纹理多样性和感知质量上的优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AnatoMaskGAN_GNN-Driven_Slice_Feature_Fusion_and_Noise_Augmentation_for_Medical_Semantic_Image_Synthesis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-Index-Aligned-Query-Distillation-for-Transformer-based-Incremental-Object-Detection"><a href="#41-Index-Aligned-Query-Distillation-for-Transformer-based-Incremental-Object-Detection" class="headerlink" title="41. Index-Aligned Query Distillation for Transformer-based Incremental Object Detection"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Index-Aligned_Query_Distillation_for_Transformer-based_Incremental_Object_Detection.pdf">Index-Aligned Query Distillation for Transformer-based Incremental Object Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本文提出了Index-Aligned Query Distillation (IAQD)方法，用于Transformer架构下的增量式目标检测。IAQD通过对关键查询索引进行一对一知识蒸馏，并结合Proxy Query Selection和标签重对齐策略，大幅缓解了旧类别的灾难性遗忘问题。实验结果表明，该方法在COCO和VOC等基准数据集的多种协议下均优于现有方法，有效提升了旧类别检测能力并兼顾新类别学习。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Index-Aligned_Query_Distillation_for_Transformer-based_Incremental_Object_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-Delving-into-Dynamic-Scene-Cue-Consistency-for-Robust-3D-Multi-Object-Tracking"><a href="#42-Delving-into-Dynamic-Scene-Cue-Consistency-for-Robust-3D-Multi-Object-Tracking" class="headerlink" title="42. Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Delving_into_Dynamic_Scene_Cue-Consistency_for_Robust_3D_Multi-Object_Tracking.pdf">Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出了DSC-Track，一种基于Transformer的3D多目标跟踪算法，通过统一的时空聚合模块和旋转不变的点对特征（PPF）编码，聚合每个轨迹的历史和空间上下文。核心创新在于引入线索一致性注意力机制，将轨迹与检测的邻域结构进行匹配，从而增强特征判别性并抑制干扰，实现鲁棒的数据关联。实验表明，DSC-Track在nuScenes和Waymo数据集上达到当前最优性能，有效减少了身份切换，证实了建模高阶关系一致性对复杂场景中多目标跟踪的价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Delving_into_Dynamic_Scene_Cue-Consistency_for_Robust_3D_Multi-Object_Tracking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-Logic-Unseen-Revealing-the-Logical-Blindspots-of-Vision-Language-Models"><a href="#43-Logic-Unseen-Revealing-the-Logical-Blindspots-of-Vision-Language-Models" class="headerlink" title="43. Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Logic_Unseen__Revealing_the_Logical_Blindspots_of_Vision-Language_Models.pdf">Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-sen University</span></p>
<p>本文提出了LogicBench基准，系统评估现有视觉-语言模型（VLMs）的逻辑推理能力，并发现现有VLMs在理解逻辑结构（如因果、条件、否定等）方面存在显著盲区。为此，作者提出了LogicCLIP训练框架，通过多LLM生成逻辑扰动负样本和逻辑敏感对比学习目标，大幅提升VLMs的逻辑理解能力，同时在通用任务上保持或提升性能，且对医学、视频等领域具备优良泛化性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Logic_Unseen_Revealing_the_Logical_Blindspots_of_Vision-Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models"><a href="#44-Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models" class="headerlink" title="44. Probing the Representational Power of Sparse Autoencoders in Vision Models"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Probing_the_Representational_Power_of_Sparse_Autoencoders_in_Vision_Models.pdf">Probing the Representational Power of Sparse Autoencoders in Vision Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Intel Labs</span></p>
<p>本论文系统性地研究了Sparse Autoencoders (SAEs) 在视觉模型中的表征能力，涵盖了视觉编码器、多模态大模型和扩散模型。方法上，作者将SAE应用于DINOv2等视觉基础模型、LLaVA多模态模型和Stable Diffusion扩散模型，通过特征提取、层级语义分析与生成可控性实验，提出了新颖的评估指标。实验显示，SAE提取的特征具有良好的语义解释性，可提升模型在分布外泛化和生成可控性，多模态实验还初步揭示了视觉与语言共享的表示。结论认为，SAE是提升视觉模型可解释性、泛化性和可控性的统一工具，具有广阔应用前景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Probing_the_Representational_Power_of_Sparse_Autoencoders_in_Vision_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation"><a href="#45-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation" class="headerlink" title="45. FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/FantasyTalking2__Timestep-Layer_Adaptive_Preference_Optimization_for_Audio-Driven_Portrait_Animation.pdf">FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AMAP, Alibaba Group</span></p>
<p>本文提出了一种多目标偏好优化框架FantasyTalking2（TLPO）用于音频驱动的肖像动画，创新地将人类偏好分解为运动自然性、唇同步和视觉质量等多个维度，通过多专家LoRA模块分别优化，并在扩散模型的时间步和网络层上自适应融合，实现细粒度无冲突的多目标协同优化。此外，作者构建了Talking-Critic多模态奖励模型和41万对大规模多维偏好数据集Talking-NSQ。实验表明，该方法在运动自然性、唇同步和视觉质量等关键指标上显著优于现有方法，实现了更契合人类偏好的高保真肖像动画生成。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/FantasyTalking2_Timestep-Layer_Adaptive_Preference_Optimization_for_Audio-Driven_Portrait_Animation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-Efficient-Image-to-Image-Schrodinger-Bridge-for-CT-Field-of-View-Extension"><a href="#46-Efficient-Image-to-Image-Schrodinger-Bridge-for-CT-Field-of-View-Extension" class="headerlink" title="46. Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Efficient_Image-to-Image_Schr%C3%B6dinger_Bridge_for_CT_Field_of_View_Extension.pdf">Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Cancer Center, Fudan University</span></p>
<p>该论文提出了一种基于Image-to-Image Schrödinger Bridge (I2SB)扩散模型的高效CT视野扩展方法。I2SB通过直接学习有限视野与扩展视野CT图像之间的随机映射，显著提升重建速度与图像结构一致性。在模拟和真实数据测试中，I2SB在重建质量及推理效率上均优于主流扩散和深度学习方法，单步推理达0.19秒&#x2F;切片，极适合临床实时应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Efficient_Image-to-Image_Schr%C3%B6dinger_Bridge_for_CT_Field_of_View_Extension.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-UAV-VL-R1-Generalizing-Vision-Language-Models-via-Supervised-Fine-Tuning-and-Multi-Stage-GRPO-for-UAV-Visual-Reasoning"><a href="#47-UAV-VL-R1-Generalizing-Vision-Language-Models-via-Supervised-Fine-Tuning-and-Multi-Stage-GRPO-for-UAV-Visual-Reasoning" class="headerlink" title="47. UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/UAV-VL-R1__Generalizing_Vision-Language_Models_via_Supervised_Fine-Tuning_and_Multi-Stage_GRPO_for_U.pdf">UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>本文提出UAV-VL-R1，一种专为无人机(UAV)高分辨率航拍图像结构化视觉推理任务设计的轻量级视觉语言模型。方法采用基于LoRA的监督微调(SFT)与多阶段群组相对策略优化(GRPO)强化学习结合，利用新建的HRVQA-VL数据集进行逐阶段训练，奖励函数兼顾结构化输出与答案准确性。实验表明，UAV-VL-R1在八类UAV视觉推理任务上的多任务泛化能力显著优于主流大模型（如Qwen2-VL-72B），且仅需3.9GB显存，适于资源受限的UAV平台实时部署。结论：融合SFT和GRPO的训练框架显著提升了模型在UAV航拍图像推理的泛化性、结构化输出能力和推理可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/UAV-VL-R1_Generalizing_Vision-Language_Models_via_Supervised_Fine-Tuning_and_Multi-Stage_GRPO_for_UAV_Visual_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-CHARM3R-Towards-Unseen-Camera-Height-Robust-Monocular-3D-Detector"><a href="#48-CHARM3R-Towards-Unseen-Camera-Height-Robust-Monocular-3D-Detector" class="headerlink" title="48. CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CHARM3R__Towards_Unseen_Camera_Height_Robust_Monocular_3D_Detector.pdf">CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Michigan State University</span></p>
<p>该论文系统性分析了当前单目3D检测模型在未见相机高度下的表现，发现深度估计是主要影响因素，并数学证明了回归深度与地面深度在相机高度变化下的误差趋势相反。为此，作者提出了CHARM3R方法，将回归深度与地面深度在模型内简单平均，有效抵消两者趋势，大幅提升模型在未见相机高度下的泛化能力，在CARLA数据集上取得了SOTA表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CHARM3R_Towards_Unseen_Camera_Height_Robust_Monocular_3D_Detector.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-HistoViT-Vision-Transformer-for-Accurate-and-Scalable-Histopathological-Cancer-Diagnosis"><a href="#49-HistoViT-Vision-Transformer-for-Accurate-and-Scalable-Histopathological-Cancer-Diagnosis" class="headerlink" title="49. HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/HistoViT__Vision_Transformer_for_Accurate_and_Scalable_Histopathological_Cancer_Diagnosis.pdf">HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Embry-Riddle Aeronautical University</span></p>
<p>本论文提出了一种基于Vision Transformer (ViT) 的深度学习框架，结合轻量级预处理流水线，对乳腺、前列腺、骨和宫颈等多种肿瘤的病理图像进行多分类任务。方法流程包括图像归一化、区域切片、PyTorch张量转换及ViT微调，模型在四个公开数据集上取得了超越当前深度学习方法的表现（所有任务准确率均超95%，AUC超99%），展示了优异的泛化能力和临床应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/HistoViT_Vision_Transformer_for_Accurate_and_Scalable_Histopathological_Cancer_Diagnosis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-VFM-Guided-Semi-Supervised-Detection-Transformer-for-Source-Free-Object-Detection-in-Remote-Sensing-Images"><a href="#50-VFM-Guided-Semi-Supervised-Detection-Transformer-for-Source-Free-Object-Detection-in-Remote-Sensing-Images" class="headerlink" title="50. VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/VFM-Guided_Semi-Supervised_Detection_Transformer_for_Source-Free_Object_Detection_in_Remote_Sensing_.pdf">VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>该论文提出了VG-DETR，一种融合视觉基础模型（VFM）指导的半监督检测Transformer，用于遥感图像的源自由目标检测。方法创新性地利用VFM的语义先验，通过VFM引导的伪标签挖掘和双层对齐（实例级和图像级），提升伪标签的可靠性和检测器的特征泛化能力，有效缓解因伪标签噪声导致的训练崩溃。实验结果表明，在多种遥感跨域场景下，VG-DETR以极低标注成本实现了显著优于现有方法的检测性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/VFM-Guided_Semi-Supervised_Detection_Transformer_for_Source-Free_Object_Detection_in_Remote_Sensing_Images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-LEARN-A-Story-Driven-Layout-to-Image-Generation-Framework-for-STEM-Instruction"><a href="#51-LEARN-A-Story-Driven-Layout-to-Image-Generation-Framework-for-STEM-Instruction" class="headerlink" title="51. LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/LEARN__A_Story-Driven_Layout-to-Image_Generation_Framework_for_STEM_Instruction.pdf">LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hiroshima University</span></p>
<p>LEARN提出了一种基于布局感知扩散模型的框架，用于生成与教学目标一致的STEM教育插图。其方法包括利用BookCover数据集进行布局语义学习、Caption2LayoutNet生成结构化布局、布局条件扩散生成图像，并通过CLIP对比损失实现语义一致性。实验和用户调研表明，该方法能有效提升插图的结构准确性和解释性，降低学生认知负担，促进课堂参与和概念理解。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LEARN_A_Story-Driven_Layout-to-Image_Generation_Framework_for_STEM_Instruction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-A-Cross-Modal-Rumor-Detection-Scheme-via-Contrastive-Learning-by-Exploring-Text–Image-internal-Correlations"><a href="#52-A-Cross-Modal-Rumor-Detection-Scheme-via-Contrastive-Learning-by-Exploring-Text–Image-internal-Correlations" class="headerlink" title="52. A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text–Image internal Correlations"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/A_Cross-Modal_Rumor_Detection_Scheme_via_Contrastive_Learning_by_Exploring_Text_and_Image_internal_C.pdf">A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text–Image internal Correlations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing University of Posts and Telecommunications</span></p>
<p>本论文提出了一种基于对比学习的跨模态谣言检测方法（MICC），结合多尺度卷积视觉特征提取（SCLIP编码器）、跨模态多尺度对齐模块和尺度感知融合网络，实现对文本与图像内部相关性的高效建模。实验结果表明，该方法在Weibo和PHEME两个真实谣言检测数据集上显著优于现有方法，提升了跨模态谣言检测的准确性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A_Cross-Modal_Rumor_Detection_Scheme_via_Contrastive_Learning_by_Exploring_Text%E2%80%93Image_internal_Correlations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-Denoise-then-Retrieve-Text-Conditioned-Video-Denoising-for-Video-Moment-Retrieval"><a href="#53-Denoise-then-Retrieve-Text-Conditioned-Video-Denoising-for-Video-Moment-Retrieval" class="headerlink" title="53. Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Denoise-then-Retrieve__Text-Conditioned_Video_Denoising_for_Video_Moment_Retrieval.pdf">Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southeast University</span></p>
<p>该论文提出了Denoise-then-Retrieve Network (DRNet)，通过文本条件化去噪模块（TCD）和文本重建反馈（TRF）模块，过滤掉与文本无关的视频片段，利用多模态交互和噪声掩码生成纯净的视频表示进行视频片段检索。实验结果表明，在Charades-STA和QVHighlights数据集上，方法在所有指标上超过现有主流方法，且去噪范式可无缝集成到其他视频检索模型中提升性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Denoise-then-Retrieve_Text-Conditioned_Video_Denoising_for_Video_Moment_Retrieval.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-Vision-Language-Models-display-a-strong-gender-bias"><a href="#54-Vision-Language-Models-display-a-strong-gender-bias" class="headerlink" title="54. Vision-Language Models display a strong gender bias"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Vision-Language_Models_display_a_strong_gender_bias.pdf">Vision-Language Models display a strong gender bias</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Togo AI Labs</span></p>
<p>本文提出了一种透明可复现的方法，利用CLIP风格的对比视觉-语言模型（VLM）和余弦相似度，量化模型在面部图像与职业&#x2F;活动短语之间的性别关联，并通过自助法估计不确定性及标签置换检验显著性。实验结果表明，不同架构的主流VLM在多种职业和活动类别上均显示出显著的性别偏见，Transformer类模型偏见略强于ResNet类，提示预训练数据和模型结构影响偏见方向和程度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Vision-Language_Models_display_a_strong_gender_bias.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-Generalized-Decoupled-Learning-for-Enhancing-Open-Vocabulary-Dense-Perception"><a href="#55-Generalized-Decoupled-Learning-for-Enhancing-Open-Vocabulary-Dense-Perception" class="headerlink" title="55. Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Generalized_Decoupled_Learning_for_Enhancing_Open-Vocabulary_Dense_Perception.pdf">Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology (Shenzhen)</span></p>
<p>该论文提出了DeCLIP，一种创新的无监督微调框架，通过将CLIP的自注意力模块解耦为“内容”和“上下文”特征，并分别采用自蒸馏、VFM语义引导与扩散模型边界补全进行优化，增强了CLIP在开放词汇密集感知任务中的局部判别能力和空间一致性。实验结果表明，DeCLIP在2D&#x2F;3D检测与分割、视频实例分割、6D姿态估计等多项开放词汇密集视觉任务中均取得了显著提升，证明其作为通用密集感知基础模型的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Generalized_Decoupled_Learning_for_Enhancing_Open-Vocabulary_Dense_Perception.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-A-CLIP-based-Uncertainty-Modal-Modeling-UMM-Framework-for-Pedestrian-Re-Identification-in-Autonomous-Driving"><a href="#56-A-CLIP-based-Uncertainty-Modal-Modeling-UMM-Framework-for-Pedestrian-Re-Identification-in-Autonomous-Driving" class="headerlink" title="56. A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/A_CLIP-based_Uncertainty_Modal_Modeling_(UMM)_Framework_for_Pedestrian_Re-Identification_in_Autonomo.pdf">A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Jiangsu Zhenjiang Technician College</span></p>
<p>本文提出了基于CLIP的轻量级不确定性模态建模（UMM）框架，用于自动驾驶中的行人再识别，融合多模态token映射器、合成模态增强和跨模态线索交互模块，实现RGB、红外、素描和文本描述的统一特征表示与高效融合。实验表明，该方法在模态缺失和跨模态检索任务中表现出更强的鲁棒性和泛化能力，且计算效率高，适用于资源有限的自动驾驶平台。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A_CLIP-based_Uncertainty_Modal_Modeling_%28UMM%29_Framework_for_Pedestrian_Re-Identification_in_Autonomous_Driving.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation"><a href="#57-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation" class="headerlink" title="57. StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/StyleMM__Stylized_3D_Morphable_Face_Model_via_Text-Driven_Aligned_Image_Translation.pdf">StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KAIST</span></p>
<p>该论文提出StyleMM，一种通过文本驱动的对齐图像翻译，实现风格化3D可变形人脸模型的方法。核心流程包括利用扩散模型生成风格化人脸、显式属性保持模块（EAM）确保表情与结构一致，并通过三阶段训练（几何预热、形状与纹理联合微调、纹理精细化）优化变形网络和纹理生成器。实验结果显示，StyleMM在保持人脸多样性、风格一致性及可动画性方面优于现有方法，能广泛应用于影视、动画和游戏。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/StyleMM_Stylized_3D_Morphable_Face_Model_via_Text-Driven_Aligned_Image_Translation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-E-CaTCH-Event-Centric-Cross-Modal-Attention-with-Temporal-Consistency-and-Class-Imbalance-Handling-for-Misinformation-Detection"><a href="#58-E-CaTCH-Event-Centric-Cross-Modal-Attention-with-Temporal-Consistency-and-Class-Imbalance-Handling-for-Misinformation-Detection" class="headerlink" title="58. E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/E-CaTCH__Event-Centric_Cross-Modal_Attention_with_Temporal_Consistency_and_Class-Imbalance_Handling_.pdf">E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">American University</span></p>
<p>该论文提出E-CaTCH框架，通过事件聚类、BERT与ResNet特征提取、层次化跨模态自注意力和软门控融合，结合重叠时间窗口及LSTM建模信息传播的动态趋势，并采用自适应类别加权和时序一致性正则，解决多模态社交媒体虚假信息检测中的模态不一致、时间变化和类别不平衡等核心挑战。实验显示E-CaTCH在Fakeddit、印度选举和COVID-19数据集上表现优于现有多模态检测模型，具备强泛化性与高计算效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/E-CaTCH_Event-Centric_Cross-Modal_Attention_with_Temporal_Consistency_and_Class-Imbalance_Handling_for_Misinformation_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-Versatile-Video-Tokenization-with-Generative-2D-Gaussian-Splatting"><a href="#59-Versatile-Video-Tokenization-with-Generative-2D-Gaussian-Splatting" class="headerlink" title="59. Versatile Video Tokenization with Generative 2D Gaussian Splatting"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Versatile_Video_Tokenization_with_Generative_2D_Gaussian_Splatting.pdf">Versatile Video Tokenization with Generative 2D Gaussian Splatting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Newcastle</span></p>
<p>本文提出Gaussian Video Transformer（GVT），通过生成式2D高斯泼洒（2DGS）策略进行视频分词，核心包含时空高斯嵌入（STGE）和高斯集合划分（GSP）机制。GVT能够自适应地为不同信息量的区域分配高斯分布，利用静态和动态高斯划分减少时域冗余，实现高效、紧凑的视频表示。实验表明，GVT在UCF101、Kinetics和DAVIS等数据集上，重建质量达到SOTA，并在动作识别和视频压缩任务上表现优异，证明其高效且可扩展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Versatile_Video_Tokenization_with_Generative_2D_Gaussian_Splatting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-Exploring-the-Tradeoff-Between-Diversity-and-Discrimination-for-Continuous-Category-Discovery"><a href="#60-Exploring-the-Tradeoff-Between-Diversity-and-Discrimination-for-Continuous-Category-Discovery" class="headerlink" title="60. Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Exploring_the_Tradeoff_Between_Diversity_and_Discrimination_for_Continuous_Category_Discovery.pdf">Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ocean University of China</span></p>
<p>本文提出了IDOD方法，针对连续类别发现（CCD）任务，包含独立多样性增强模块、联合新颖性发现模块和正交判别增量模块。通过独立训练骨干网络和投影头，结合正交原型与联合发现机制，方法在提升新颖类别发现与已知类别分类之间取得更优平衡，减少伪标签误差累积且显著降低存储开销，实验验证在多个细粒度视觉数据集上优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Exploring_the_Tradeoff_Between_Diversity_and_Discrimination_for_Continuous_Category_Discovery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-Semi-supervised-Image-Dehazing-via-Expectation-Maximization-and-Bidirectional-Brownian-Bridge-Diffusion-Models"><a href="#61-Semi-supervised-Image-Dehazing-via-Expectation-Maximization-and-Bidirectional-Brownian-Bridge-Diffusion-Models" class="headerlink" title="61. Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Semi-supervised_Image_Dehazing_via_Expectation-Maximization_and_Bidirectional_Brownian_Bridge_Diffus.pdf">Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">China University of Mining and Technology</span></p>
<p>本论文提出了一种结合期望最大化（EM）算法与双向布朗桥扩散模型（B3DM）的半监督图像去雾方法EM-B3DM。方法分为两阶段：首先利用EM算法将有雾与无雾图像对的联合分布解耦为条件分布，并用统一的布朗桥扩散模型建模结构与内容相关性；随后利用预训练模型和大量无配对数据进行半监督训练，并引入残差差分卷积块（RDC）以增强梯度特征表达。实验结果表明，该方法在合成与真实数据集上均优于或可比现有最优方法，显著降低对成对数据的依赖，提升了去雾效果和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Semi-supervised_Image_Dehazing_via_Expectation-Maximization_and_Bidirectional_Brownian_Bridge_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-Residual-based-Efficient-Bidirectional-Diffusion-Model-for-Image-Dehazing-and-Haze-Generation"><a href="#62-Residual-based-Efficient-Bidirectional-Diffusion-Model-for-Image-Dehazing-and-Haze-Generation" class="headerlink" title="62. Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Residual-based_Efficient_Bidirectional_Diffusion_Model_for_Image_Dehazing_and_Haze_Generation.pdf">Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">China University of Mining and Technology</span></p>
<p>本文提出了一种残差驱动的高效双向扩散模型（RBDM），可在单一模型内实现图像去雾与生成雾的条件分布建模。方法核心包括通过残差构建双马尔可夫链与基于图像patch的统一评分函数，支持任意尺寸的高效双向转换。实验结果表明，RBDM在多个合成及真实数据集上去雾和生成雾效果优于或至少等同于现有主流方法，并能自然保留图像细节和色彩。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Residual-based_Efficient_Bidirectional_Diffusion_Model_for_Image_Dehazing_and_Haze_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="63-Is-ChatGPT-5-Ready-for-Mammogram-VQA"><a href="#63-Is-ChatGPT-5-Ready-for-Mammogram-VQA" class="headerlink" title="63. Is ChatGPT-5 Ready for Mammogram VQA?"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Is_ChatGPT-5_Ready_for_Mammogram_VQA_.pdf">Is ChatGPT-5 Ready for Mammogram VQA?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Emory University School of Medicine</span></p>
<p>本文系统评估了ChatGPT-5及GPT-4o在四个公开乳腺癌筛查数据集上的视觉问答（VQA）任务，包括BI-RADS评估、异常检测及恶性分类，采用零样本链式推理多模态提示。结果显示，ChatGPT-5在各任务上优于小模型和前代GPT-4o，但其准确性和敏感性仍明显低于领域专家及医学专用AI模型，尚不能用于高风险临床场景。结论是：ChatGPT-5虽有显著进步，但需针对医学影像领域进行深度定向优化后才能满足临床需求。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Is_ChatGPT-5_Ready_for_Mammogram_VQA_.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding"><a href="#64-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding" class="headerlink" title="64. Controlling Multimodal LLMs via Reward-guided Decoding"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Controlling_Multimodal_LLMs_via_Reward-guided_Decoding.pdf">Controlling Multimodal LLMs via Reward-guided Decoding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mila - Quebec AI Institute</span></p>
<p>本文提出了一种针对多模态大语言模型（MLLM）的推理期自适应控制方法——多模态奖励引导解码（MRGD），通过构建视觉锚定相关的奖励模型（分别评估目标精度和召回率），并将其线性组合动态指导生成过程，实现对模型输出目标精度与召回、以及推理计算资源消耗的精细可控。实验结果表明，该方法在多个对象幻觉基准上显著优于现有方法，实现了更高的可控性和更低的幻觉率，同时兼容不同架构的MLLM，无需重新训练。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Controlling_Multimodal_LLMs_via_Reward-guided_Decoding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-CRYPTOSCOPE-UTILIZING-LARGE-LANGUAGE-MODELS-FOR-AUTOMATED-CRYPTOGRAPHIC-LOGIC-VULNERABILITY-DETECTION"><a href="#65-CRYPTOSCOPE-UTILIZING-LARGE-LANGUAGE-MODELS-FOR-AUTOMATED-CRYPTOGRAPHIC-LOGIC-VULNERABILITY-DETECTION" class="headerlink" title="65. CRYPTOSCOPE: UTILIZING LARGE LANGUAGE MODELS FOR AUTOMATED CRYPTOGRAPHIC LOGIC VULNERABILITY DETECTION"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CryptoScope__Utilizing_Large_Language_Models_for_Automated_Cryptographic_Logic_Vulnerability_Detecti.pdf">CRYPTOSCOPE: UTILIZING LARGE LANGUAGE MODELS FOR AUTOMATED CRYPTOGRAPHIC LOGIC VULNERABILITY DETECTION</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sichuan University</span></p>
<p>该论文提出了CRYPTOSCOPE框架，结合了链式思维（CoT）提示和检索增强生成（RAG），依托超过12000项的加密知识库，实现多语言、无需执行代码的自动化加密逻辑漏洞检测。实验表明，CRYPTOSCOPE在LLM-CLVA基准上显著提升了多种大模型的漏洞检测性能，并成功发现了9个真实开源项目中未公开的漏洞，验证了其实用性和通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CRYPTOSCOPE_UTILIZING_LARGE_LANGUAGE_MODELS_FOR_AUTOMATED_CRYPTOGRAPHIC_LOGIC_VULNERABILITY_DETECTION.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-Aware-First-Think-Less-Dynamic-Boundary-Self-Awareness-Drives-Extreme-Reasoning-Efficiency-in-Large-Language-Models"><a href="#66-Aware-First-Think-Less-Dynamic-Boundary-Self-Awareness-Drives-Extreme-Reasoning-Efficiency-in-Large-Language-Models" class="headerlink" title="66. Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Aware_First,_Think_Less__Dynamic_Boundary_Self-Awareness_Drives_Extreme_Reasoning_Efficiency_in_Larg.pdf">Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology</span></p>
<p>本文提出了动态推理边界自觉框架（DR. SAF），通过模型自我识别推理能力边界，动态调整推理深度以匹配每道题的复杂度。该方法包含边界自觉对齐、适应性长度管理和边界保持机制，显著提高了LLM推理效率，实现了平均响应长度减少49.27%、训练时间缩短5倍，并在极端压缩下精度提升超过16%。实验结果显示该框架在六项数学推理基准上均保持高准确率和极致token效率，适合资源受限场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Aware_First%2C_Think_Less_Dynamic_Boundary_Self-Awareness_Drives_Extreme_Reasoning_Efficiency_in_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-Causality-Matters-How-Temporal-Information-Emerges-in-Video-Language-Models"><a href="#67-Causality-Matters-How-Temporal-Information-Emerges-in-Video-Language-Models" class="headerlink" title="67. Causality Matters: How Temporal Information Emerges in Video Language Models"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Causality_Matters__How_Temporal_Information_Emerges_in_Video_Language_Models.pdf">Causality Matters: How Temporal Information Emerges in Video Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>该论文系统分析了Video Language Models（VideoLMs）中的时序理解机制，发现时序信息主要通过因果注意力机制自发涌现，而非传统认为的时序位置编码（PEs）。作者提出并实验证明，时序理解依赖于跨帧因果信息流动，并据此设计了两种高效推理策略，在主流基准上验证了其有效性。结论是，未来VideoLMs时序建模应聚焦于因果注意力机制与信息流路径优化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Causality_Matters_How_Temporal_Information_Emerges_in_Video_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-ADMIRE-BayesOpt-Accelerated-Data-MIxture-RE-weighting-for-Language-Models-with-Bayesian-Optimization"><a href="#68-ADMIRE-BayesOpt-Accelerated-Data-MIxture-RE-weighting-for-Language-Models-with-Bayesian-Optimization" class="headerlink" title="68. ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ADMIRE-BayesOpt__Accelerated_Data_MIxture_RE-weighting_for_Language_Models_with_Bayesian_Optimizatio.pdf">ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Thomson Reuters Foundational Research</span></p>
<p>本文提出了ADMIRE-BayesOpt方法，将大型语言模型训练中的数据混合权重选择问题建模为黑盒超参数优化问题，并采用贝叶斯优化（包括多保真优化）以高效搜索最优数据混合配置。通过在多种模型规模和数据集上的实验，ADMIRE-BayesOpt在收敛速度和最终性能上均显著优于现有的回归或代理模型方法，且极大降低了计算成本，提升了数据混合方案在不同模型规模间的可迁移性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ADMIRE-BayesOpt_Accelerated_Data_MIxture_RE-weighting_for_Language_Models_with_Bayesian_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-Inclusion-Arena-An-Open-Platform-for-Evaluating-Large-Foundation-Models-with-Real-World-Apps"><a href="#69-Inclusion-Arena-An-Open-Platform-for-Evaluating-Large-Foundation-Models-with-Real-World-Apps" class="headerlink" title="69. Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Inclusion_Arena__An_Open_Platform_for_Evaluating_Large_Foundation_Models_with_Real-World_Apps.pdf">Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Inclusion AI</span></p>
<p>该论文提出了Inclusion Arena平台，通过集成真实AI应用中的用户交互，收集并利用人类偏好数据，用于大模型（LLMs和MLLMs）评测与排名。方法上，平台引入了基于Bradley-Terry模型的最大似然估计排名、Placement Matches冷启动机制和Proximity Sampling智能采样算法，聚焦于相近能力模型间的对比以提升评测稳定性和抗攻击性。实证分析表明，该平台能够高效、稳健地反映模型在真实场景下的能力差异，同时显著减少数据操控风险，有助于推动大模型向用户体验和实用性优化发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inclusion_Arena_An_Open_Platform_for_Evaluating_Large_Foundation_Models_with_Real-World_Apps.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-MM-R1-Unleashing-the-Power-of-Unified-Multimodal-Large-Language-Models-for-Personalized-Image-Generation"><a href="#70-MM-R1-Unleashing-the-Power-of-Unified-Multimodal-Large-Language-Models-for-Personalized-Image-Generation" class="headerlink" title="70. MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/MM-R1__Unleashing_the_Power_of_Unified_Multimodal_Large_Language_Models_for_Personalized_Image_Gener.pdf">MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>本论文提出MM-R1框架，通过跨模态Chain-of-Thought (X-CoT)推理和Grouped Reward Proximal Policy Optimization (GRPO)强化学习，提升统一多模态大语言模型(MLLM)在个性化图像生成中的能力。方法包括：先对用户输入图片和文本进行理解和视觉概念分离，再结合用户提示进行图像生成，并设计多重奖励机制进行优化。实验结果表明，MM-R1在零样本个性化图像生成中显著提升了主题保真度和文本一致性，优于现有统一MLLM方法且无需针对每个主体单独微调，具备更强泛化和可控性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MM-R1_Unleashing_the_Power_of_Unified_Multimodal_Large_Language_Models_for_Personalized_Image_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-Trustworthy-AI-Psychotherapy-Multi-Agent-LLM-Workflow-for-Counseling-and-Explainable-Mental-Disorder-Diagnosis"><a href="#71-Trustworthy-AI-Psychotherapy-Multi-Agent-LLM-Workflow-for-Counseling-and-Explainable-Mental-Disorder-Diagnosis" class="headerlink" title="71. Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Trustworthy_AI_Psychotherapy__Multi-Agent_LLM_Workflow_for_Counseling_and_Explainable_Mental_Disorde.pdf">Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Vrije Universiteit Amsterdam</span></p>
<p>本文提出DSM5AgentFlow，一种多智能体大语言模型（LLM）工作流，可自动生成并进行DSM-5精神障碍问卷，通过三类LLM代理（治疗师、患者、诊断师）模拟真实对话，实现逐步、可溯源的诊断与解释。实验表明，该框架能生成高覆盖、连贯对话，诊断准确率领先，显著提升了精神健康AI系统的透明度和可信度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Trustworthy_AI_Psychotherapy_Multi-Agent_LLM_Workflow_for_Counseling_and_Explainable_Mental_Disorder_Diagnosis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-Rationalizing-Transformer-Predictions-via-End-To-End-Differentiable-Self-Training"><a href="#72-Rationalizing-Transformer-Predictions-via-End-To-End-Differentiable-Self-Training" class="headerlink" title="72. Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Rationalizing_Transformer_Predictions_via_End-To-End_Differentiable_Self-Training.pdf">Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bielefeld University</span></p>
<p>本文提出了一种端到端可微分的自训练范式，用于训练能够生成忠实解释（rationales）和做出分类决策的Transformer模型。该方法通过单一模型同时完成分类、输入token相关性评分，并采用自训练机制优化class-wise rationale掩码，显著提升了与人类注释的一致性与解释的忠实度。实验结果表明，该方法在两个NLP基准数据集上实现了与人类rationale高度一致且忠实性优异的解释，同时保持甚至略超标准分类器的分类性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Rationalizing_Transformer_Predictions_via_End-To-End_Differentiable_Self-Training.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-Inspire-or-Predict-Exploring-New-Paradigms-in-Assisting-Classical-Planners-with-Large-Language-Models"><a href="#73-Inspire-or-Predict-Exploring-New-Paradigms-in-Assisting-Classical-Planners-with-Large-Language-Models" class="headerlink" title="73. Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Inspire_or_Predict__Exploring_New_Paradigms_in_Assisting_Classical_Planners_with_Large_Language_Mode.pdf">Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Guizhou University</span></p>
<p>本文提出了一种结合问题分解与大语言模型（LLM）辅助的规划方法，将大型规划问题通过有向无环图分解为子任务，并探索两种LLM集成范式：LLM4Inspire（通用知识启发）和LLM4Predict（领域知识推理中间状态）。实验证明，LLM4Predict在缩减搜索空间和提升规划成功率上优于仅依赖通用知识的LLM4Inspire，验证了领域知识在复杂规划中的不可替代性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inspire_or_Predict_Exploring_New_Paradigms_in_Assisting_Classical_Planners_with_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-Reference-Points-in-LLM-Sentiment-Analysis-The-Role-of-Structured-Context"><a href="#74-Reference-Points-in-LLM-Sentiment-Analysis-The-Role-of-Structured-Context" class="headerlink" title="74. Reference Points in LLM Sentiment Analysis: The Role of Structured Context"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Reference_Points_in_LLM_Sentiment_Analysis__The_Role_of_Structured_Context.pdf">Reference Points in LLM Sentiment Analysis: The Role of Structured Context</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Meijo University</span></p>
<p>本论文提出在LLM情感分析中引入用户和商家历史评分等结构化参考点（如JSON格式）以补充文本上下文，实验使用轻量级Llama-3.2-3B模型。结果表明，结构化（尤其JSON格式）补充信息可显著提升模型在Yelp餐厅和夜生活场景下的准确率和回归性能，无需微调即可在资源受限设备上部署；进一步分析证实性能提升源于真实的上下文推理，而非标签代理效应。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Reference_Points_in_LLM_Sentiment_Analysis_The_Role_of_Structured_Context.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-AIM-Bench-Evaluating-Decision-making-Biases-of-Agentic-LLM-as-Inventory-Manager"><a href="#75-AIM-Bench-Evaluating-Decision-making-Biases-of-Agentic-LLM-as-Inventory-Manager" class="headerlink" title="75. AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/AIM-Bench__Evaluating_Decision-making_Biases_of_Agentic_LLM_as_Inventory_Manager.pdf">AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本论文提出AIM-Bench基准，用于系统评估大语言模型（LLM）在不确定库存管理环境下的决策能力和偏差，包括新建和改编的五种供应链环境，并量化人类常见的决策偏差如均值锚定和牛鞭效应。研究发现，不同LLM普遍表现出类似人类的决策偏差，但通过认知反思和信息共享等策略可有效缓解部分偏差，提示在实际部署LLM时需重视偏差识别与消除。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AIM-Bench_Evaluating_Decision-making_Biases_of_Agentic_LLM_as_Inventory_Manager.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-ON-POLICY-RL-MEETS-OFF-POLICY-EXPERTS-HARMONIZING-SUPERVISED-FINE-TUNING-AND-REINFORCEMENT-LEARNING-VIA-DYNAMIC-WEIGHTING"><a href="#76-ON-POLICY-RL-MEETS-OFF-POLICY-EXPERTS-HARMONIZING-SUPERVISED-FINE-TUNING-AND-REINFORCEMENT-LEARNING-VIA-DYNAMIC-WEIGHTING" class="headerlink" title="76. ON-POLICY RL MEETS OFF-POLICY EXPERTS: HARMONIZING SUPERVISED FINE-TUNING AND REINFORCEMENT LEARNING VIA DYNAMIC WEIGHTING"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/On-Policy_RL_Meets_Off-Policy_Experts__Harmonizing_Supervised_Fine-Tuning_and_Reinforcement_Learning.pdf">ON-POLICY RL MEETS OFF-POLICY EXPERTS: HARMONIZING SUPERVISED FINE-TUNING AND REINFORCEMENT LEARNING VIA DYNAMIC WEIGHTING</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Alibaba Group</span></p>
<p>本文提出CHORD框架，将监督微调（SFT）和强化学习（RL）以动态加权的方式统一于大语言模型（LLM）后训练，通过全局系数和逐token权重函数，灵活控制离策略专家数据对模型的影响。实验表明，CHORD在平衡专家知识吸收与模型自主探索能力方面优于传统SFT-then-RL范式，提升推理与泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ON-POLICY_RL_MEETS_OFF-POLICY_EXPERTS_HARMONIZING_SUPERVISED_FINE-TUNING_AND_REINFORCEMENT_LEARNING_VIA_DYNAMIC_WEIGHTING.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs"><a href="#77-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs" class="headerlink" title="77. When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/When_Punctuation_Matters__A_Large-Scale_Comparison_of_Prompt_Robustness_Methods_for_LLMs.pdf">When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AIRI</span></p>
<p>本论文提出了首个系统性评估大型语言模型（LLMs）在不同提示（prompt）格式下鲁棒性的方法，对5种主流鲁棒性提升技术（如Batch Calibration、Template Ensembles等）在Llama、Qwen、Gemma等8个模型和52项任务上进行了统一实验框架下的大规模对比。研究发现，Batch Calibration在无分布偏移时显著提升鲁棒性但对类别失衡敏感，LoRA微调虽提升准确率但对格式变化鲁棒性有限，而规模更大的前沿模型表现出更强鲁棒性，但仍存在部分任务下格式敏感问题，基于投票的Template Ensembles可进一步缓解这一问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/When_Punctuation_Matters_A_Large-Scale_Comparison_of_Prompt_Robustness_Methods_for_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-ETTRL-Balancing-Exploration-and-Exploitation-in-LLM-Test-Time-Reinforcement-Learning-via-Entropy-Mechanism"><a href="#78-ETTRL-Balancing-Exploration-and-Exploitation-in-LLM-Test-Time-Reinforcement-Learning-via-Entropy-Mechanism" class="headerlink" title="78. ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning via Entropy Mechanism"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ETTRL__Balancing_Exploration_and_Exploitation_in_LLM_Test-Time_Reinforcement_Learning_Via_Entropy_Me.pdf">ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning via Entropy Mechanism</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou Technology</span></p>
<p>本文提出了一种基于熵机制的测试时强化学习框架ETTRL，核心包括熵分叉树多数采样（ETMR）与基于熵的优势重塑（EAR）两大策略，有效提升LLM在无监督推理任务中的探索-利用平衡。实验结果表明，该方法在AIME 2024等数学推理基准上显著提升了Pass@1准确率，同时大幅降低了推理所需的token预算，优化了模型无监督自适应能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ETTRL_Balancing_Exploration_and_Exploitation_in_LLM_Test-Time_Reinforcement_Learning_via_Entropy_Mechanism.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-SGSimEval-A-Comprehensive-Multifaceted-and-Similarity-Enhanced-Benchmark-for-Automatic-Survey-Generation-Systems"><a href="#79-SGSimEval-A-Comprehensive-Multifaceted-and-Similarity-Enhanced-Benchmark-for-Automatic-Survey-Generation-Systems" class="headerlink" title="79. SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/SGSimEval__A_Comprehensive_Multifaceted_and_Similarity-Enhanced_Benchmark_for_Automatic_Survey_Gener.pdf">SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong Polytechnic University</span></p>
<p>本文提出了SGSimEval，一个用于自动化综述生成系统的多维综合评测基准，集成了结构、内容和参考文献三方面的评价，并将LLM打分与定量指标结合，创新性引入了语义相似度增强及人类偏好对齐的评测框架。实验结果表明，领域特定（如计算机科学）系统在结构和内容生成上能超越人类，但在参考文献质量方面仍有较大提升空间，SGSimEval的评测结果与人类判断高度一致。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SGSimEval_A_Comprehensive_Multifaceted_and_Similarity-Enhanced_Benchmark_for_Automatic_Survey_Generation_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-CSGO-Generalized-Optimization-for-Cold-Start-in-Wireless-Collaborative-Edge-LLM-Systems"><a href="#80-CSGO-Generalized-Optimization-for-Cold-Start-in-Wireless-Collaborative-Edge-LLM-Systems" class="headerlink" title="80. CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/CSGO__Generalized_Optimization_for_Cold_Start_in_Wireless_Collaborative_Edge_LLM_Systems.pdf">CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文针对在无线异构边缘设备上部署大语言模型（LLM）面临的推理冷启动延迟问题，提出了一种基于动态规划的分层分配算法，通过重叠模型加载与计算通信过程，动态优化模型分片和设备分配以最小化总体延迟。实验结果表明，所提方法显著降低了端到端冷启动延迟，相较于多种基线策略平均提升17.43%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CSGO_Generalized_Optimization_for_Cold_Start_in_Wireless_Collaborative_Edge_LLM_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-Hallucination-in-LLM-Based-Code-Generation-An-Automotive-Case-Study"><a href="#81-Hallucination-in-LLM-Based-Code-Generation-An-Automotive-Case-Study" class="headerlink" title="81. Hallucination in LLM-Based Code Generation: An Automotive Case Study"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Hallucination_in_LLM-Based_Code_Generation__An_Automotive_Case_Study.pdf">Hallucination in LLM-Based Code Generation: An Automotive Case Study</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University of Munich</span></p>
<p>本文针对汽车领域的代码生成任务，系统评估了多种主流代码生成型大语言模型（如GPT-4.1、Codex、GPT-4o）在不同提示复杂度下的幻觉现象，并采用迭代反馈修复策略提升代码可靠性。结果表明，在基线和信号增强提示下，幻觉现象频发且难以完全消除，仅通过模板增强提示与迭代反馈组合，部分模型可生成正确代码，凸显幻觉问题需针对性缓解方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Hallucination_in_LLM-Based_Code_Generation_An_Automotive_Case_Study.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-ORFuzz-Fuzzing-the-‘Other-Side’-of-LLM-Safety-–-Testing-Over-Refusal"><a href="#82-ORFuzz-Fuzzing-the-‘Other-Side’-of-LLM-Safety-–-Testing-Over-Refusal" class="headerlink" title="82. ORFuzz: Fuzzing the ‘Other Side’ of LLM Safety – Testing Over-Refusal"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/ORFuzz__Fuzzing_the__Other_Side__of_LLM_Safety_--_Testing_Over-Refusal.pdf">ORFuzz: Fuzzing the ‘Other Side’ of LLM Safety – Testing Over-Refusal</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了ORFUZZ，这是首个针对大语言模型（LLM）过度拒答（over-refusal）现象的进化式自动化检测框架。ORFUZZ集成了安全类别感知的种子选择、基于LLM推理的自适应变异器优化及人类对齐的评测模型OR-JUDGE，能够自动生成多样且高质量的测试用例，有效挖掘和分析LLM的过度拒答漏洞。实验证明，ORFUZZ在生成有效过度拒答用例上的触发率是现有方法的两倍以上，并构建了覆盖八大安全类别、在10个主流LLM均表现出高触发率（平均63.56%）的ORFUZZSET基准数据集。结论：ORFUZZ和ORFUZZSET为LLM安全测试和基准评测提供了高效、可靠、自动化的解决方案，显著提升了LLM的安全性与可用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ORFuzz_Fuzzing_the_%27Other_Side%27_of_LLM_Safety_%E2%80%93_Testing_Over-Refusal.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-MONACO-More-Natural-and-Complex-Questions-for-Reasoning-Across-Dozens-of-Documents"><a href="#83-MONACO-More-Natural-and-Complex-Questions-for-Reasoning-Across-Dozens-of-Documents" class="headerlink" title="83. MONACO: More Natural and Complex Questions for Reasoning Across Dozens of Documents"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/MoNaCo__More_Natural_and_Complex_Questions_for_Reasoning_Across_Dozens_of_Documents.pdf">MONACO: More Natural and Complex Questions for Reasoning Across Dozens of Documents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Pennsylvania</span></p>
<p>该论文提出了MONACO基准数据集，包含1315个由人工撰写的自然且复杂的问题，这些问题需要跨越数十至数百份文档进行推理和信息整合。通过分解注释流程和人工标注，MONACO为每个问题提供详细的中间推理链及证据，系统评测了前沿大语言模型（LLM）的表现，发现即使最新LLM在该基准上的F1分数仅为61.2%，存在召回率低和幻觉严重等问题，凸显了LLM在真实复杂信息检索和推理任务上的显著不足。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MONACO_More_Natural_and_Complex_Questions_for_Reasoning_Across_Dozens_of_Documents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-Dynamic-Quality-Latency-Aware-Routing-for-LLM-Inference-in-Wireless-Edge-Device-Networks"><a href="#84-Dynamic-Quality-Latency-Aware-Routing-for-LLM-Inference-in-Wireless-Edge-Device-Networks" class="headerlink" title="84. Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Dynamic_Quality-Latency_Aware_Routing_for_LLM_Inference_in_Wireless_Edge-Device_Networks.pdf">Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了一种动态质量-延迟感知路由框架，针对无线边缘设备协作环境下大语言模型（LLM）推理的质量与延迟权衡问题。通过结合轻量级BERT路由器进行语义难度预测与精细化延迟成本模型，并在多轮对话中显式建模KV-cache重计算开销，实现在保证推理质量的前提下平均响应延迟降低5-15%，大模型调用率减少10-20%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Dynamic_Quality-Latency_Aware_Routing_for_LLM_Inference_in_Wireless_Edge-Device_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-LETToT-Label-Free-Evaluation-of-Large-Language-Models-On-Tourism-Using-Expert-Tree-of-Thought"><a href="#85-LETToT-Label-Free-Evaluation-of-Large-Language-Models-On-Tourism-Using-Expert-Tree-of-Thought" class="headerlink" title="85. LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/LETToT__Label-Free_Evaluation_of_Large_Language_Models_On_Tourism_Using_Expert_Tree-of-Thought.pdf">LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Warwick</span></p>
<p>该论文提出LETToT框架，通过专家知识构建的树状推理结构（Tree-of-Thought, ToT）实现旅游领域大语言模型的无标签评估。方法包括对旅游问答任务进行域适配、分层ToT组件的迭代优化与AHP加权评分，并用优化的ToT指标评估五种开源LLM。结论显示，LETToT能显著提升模型在主题相关性、实用性等多个维度的表现，推理增强型小模型可缩小与大模型的性能差距。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LETToT_Label-Free_Evaluation_of_Large_Language_Models_On_Tourism_Using_Expert_Tree-of-Thought.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-Group-Fairness-Meets-the-Black-Box-Enabling-Fair-Algorithms-on-Closed-LLMs-via-Post-Processing"><a href="#86-Group-Fairness-Meets-the-Black-Box-Enabling-Fair-Algorithms-on-Closed-LLMs-via-Post-Processing" class="headerlink" title="86. Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Group_Fairness_Meets_the_Black_Box__Enabling_Fair_Algorithms_on_Closed_LLMs_via_Post-Processing.pdf">Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Urbana-Champaign</span></p>
<p>本文提出了一种在闭源大语言模型（LLM）上实现群体公平性的通用后处理框架：将LLM视为特征提取器，通过精心设计的提示（prompt）获取与公平性相关的概率预测（如token log probabilities），并利用这些特征应用传统公平算法，在不需访问模型权重或内部嵌入的情况下构建轻量级公平分类器。实验证明该方法在多个数据集和公平性标准下均能兼顾准确性与公平性，并在低样本场景下优于基于LLM嵌入或原始特征训练的公平模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Group_Fairness_Meets_the_Black_Box_Enabling_Fair_Algorithms_on_Closed_LLMs_via_Post-Processing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-Generating-Dialogues-from-Egocentric-Instructional-Videos-for-Task-Assistance-Dataset-Method-and-Benchmark"><a href="#87-Generating-Dialogues-from-Egocentric-Instructional-Videos-for-Task-Assistance-Dataset-Method-and-Benchmark" class="headerlink" title="87. Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Generating_Dialogues_from_Egocentric_Instructional_Videos_for_Task_Assistance__Dataset,_Method_and_B.pdf">Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Google</span></p>
<p>本论文提出了一种基于大语言模型的自动化方法，将单人叙述的教学视频转化为专家-新手的任务指导对话，流程包括步骤提取、对话生成与视频定位，并结合现有数据集构建了HowToDIV多模态对话数据集。实验结果表明，该方法能高效生成高质量、覆盖多任务领域的任务指导对话，为AI助手在实际情境下的训练和评估提供了强基线和新数据资源。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Generating_Dialogues_from_Egocentric_Instructional_Videos_for_Task_Assistance_Dataset%2C_Method_and_Benchmark.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-Role-Augmented-Intent-Driven-Generative-Search-Engine-Optimization"><a href="#88-Role-Augmented-Intent-Driven-Generative-Search-Engine-Optimization" class="headerlink" title="88. Role-Augmented Intent-Driven Generative Search Engine Optimization"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Role-Augmented_Intent-Driven_Generative_Search_Engine_Optimization.pdf">Role-Augmented Intent-Driven Generative Search Engine Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本文提出了Role-Augmented Intent-Driven Generative Search Engine Optimization (RAID G-SEO)，通过四阶段流程（内容摘要、意图推断及反思、步骤规划、内容重写）并结合4W多角色深度反思机制，对生成式搜索引擎中内容进行针对性优化。实验结果表明，基于意图建模的方法在提升内容可见性和适应多样化检索场景方面显著优于传统和现有基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Role-Augmented_Intent-Driven_Generative_Search_Engine_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-AlphaAgents-Large-Language-Model-based-Multi-Agents-for-Equity-Portfolio-Constructions"><a href="#89-AlphaAgents-Large-Language-Model-based-Multi-Agents-for-Equity-Portfolio-Constructions" class="headerlink" title="89. AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/AlphaAgents__Large_Language_Model_based_Multi-Agents_for_Equity_Portfolio_Constructions.pdf">AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">BlackRock, Inc.</span></p>
<p>该论文提出了一种基于大型语言模型（LLM）的多智能体系统AlphaAgents，用于股票分析与投资组合构建。方法上，设计了基金基本面、情绪和估值三类专用代理，通过角色提示和工具增强，协作分析股票并采用内部辩论机制达成投资共识，有效支持风险偏好建模和认知偏差缓解。结论显示，多智能体框架在股票筛选和投资决策中优于单智能体方案，能提升投资分析的透明度和稳健性，为未来智能投资系统奠定基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AlphaAgents_Large_Language_Model_based_Multi-Agents_for_Equity_Portfolio_Constructions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="90-Pretrained-Conformers-for-Audio-Fingerprinting-and-Retrieval"><a href="#90-Pretrained-Conformers-for-Audio-Fingerprinting-and-Retrieval" class="headerlink" title="90. Pretrained Conformers for Audio Fingerprinting and Retrieval"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Pretrained_Conformers_for_Audio_Fingerprinting_and_Retrieval.pdf">Pretrained Conformers for Audio Fingerprinting and Retrieval</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Infobip</span></p>
<p>本文提出了一种基于自监督对比学习预训练的Conformer编码器架构，用于音频指纹提取和检索，通过提取短音频片段的独特嵌入，实现对未见音频的高效检索。实验表明，该模型对时序错位、噪声、混响及极端时间拉伸等失真具有极强鲁棒性，在多个公开数据集上取得了与当前最优方法相当甚至更优的效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Pretrained_Conformers_for_Audio_Fingerprinting_and_Retrieval.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-A-Comprehensive-Perspective-on-Explainable-AI-across-the-Machine-Learning-Workflow"><a href="#91-A-Comprehensive-Perspective-on-Explainable-AI-across-the-Machine-Learning-Workflow" class="headerlink" title="91. A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/A_Comprehensive_Perspective_on_Explainable_AI_across_the_Machine_Learning_Workflow.pdf">A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">JADBio Gnosis DA S.A.</span></p>
<p>本文提出了Holistic Explainable AI (HXAI) 框架，将可解释性嵌入到机器学习工作流的所有阶段，并针对不同用户（领域专家、数据分析师、数据科学家）定制解释。通过构建六大核心组件和112项问题库，文章系统分析现有工具覆盖不足，总结出有效解释的特征，并提出利用AI Agent（基于LLM）自动聚合与个性化生成解释，促进AI系统的透明性、可用性和信任度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A_Comprehensive_Perspective_on_Explainable_AI_across_the_Machine_Learning_Workflow.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-Physics-Informed-Diffusion-Models-for-Unsupervised-Anomaly-Detection-in-Multivariate-Time-Series"><a href="#92-Physics-Informed-Diffusion-Models-for-Unsupervised-Anomaly-Detection-in-Multivariate-Time-Series" class="headerlink" title="92. Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Physics-Informed_Diffusion_Models_for_Unsupervised_Anomaly_Detection_in_Multivariate_Time_Series.pdf">Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fraunhofer IOSB-INA</span></p>
<p>该论文提出了一种面向多变量时序数据的无监督异常检测方法，通过在扩散模型训练过程中引入加权物理信息损失函数（基于静态权重调度），使模型更好地拟合数据分布并提升异常检测性能。实验证明，物理信息指导的训练方法在合成和真实数据集上提高了F1分数、数据多样性和对数似然值，优于现有的物理信息和纯数据驱动扩散模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Physics-Informed_Diffusion_Models_for_Unsupervised_Anomaly_Detection_in_Multivariate_Time_Series.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-A-Remedy-for-Over-Squashing-in-Graph-Learning-via-Forman-Ricci-Curvature-based-Graph-to-Hypergraph-Structural-Lifting"><a href="#93-A-Remedy-for-Over-Squashing-in-Graph-Learning-via-Forman-Ricci-Curvature-based-Graph-to-Hypergraph-Structural-Lifting" class="headerlink" title="93. A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/A_Remedy_for_Over-Squashing_in_Graph_Learning_via_Forman-Ricci_Curvature_based_Graph-to-Hypergraph_S.pdf">A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Perelyn GmbH</span></p>
<p>该论文提出利用Forman-Ricci曲率度量图结构，识别信息流瓶颈后进行结构提升，将图映射为超图，从而通过引入高阶拓扑关系缓解图神经网络中的过度压缩问题。实验表明该方法能提升多种图和超图神经网络模型在分类和分子属性预测任务上的表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A_Remedy_for_Over-Squashing_in_Graph_Learning_via_Forman-Ricci_Curvature_based_Graph-to-Hypergraph_Structural_Lifting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-NeMo-A-Neuron-Level-Modularizing-While-Training-Approach-for-Decomposing-DNN-Models"><a href="#94-NeMo-A-Neuron-Level-Modularizing-While-Training-Approach-for-Decomposing-DNN-Models" class="headerlink" title="94. NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/NeMo__A_Neuron-Level_Modularizing-While-Training_Approach_for_Decomposing_DNN_Models.pdf">NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Laboratory of Complex &amp; Critical Software Environment, Beihang University</span></p>
<p>本文提出了一种用于深度神经网络模型结构化分解的神经元级联训练模块化方法NeMo，通过在训练过程中结合神经元识别器和基于对比学习的复合损失函数，实现对Transformer和CNN等多种架构的模块化训练和分解。实验结果表明NeMo在保持模型准确率的同时大幅减少模型尺寸和计算开销，优于现有的主流模块化方法，有效支持按需模型复用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/NeMo_A_Neuron-Level_Modularizing-While-Training_Approach_for_Decomposing_DNN_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-Mitigating-Modality-Quantity-and-Quality-Imbalance-in-Multimodal-Online-Federated-Learning"><a href="#95-Mitigating-Modality-Quantity-and-Quality-Imbalance-in-Multimodal-Online-Federated-Learning" class="headerlink" title="95. Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Mitigating_Modality_Quantity_and_Quality_Imbalance_in_Multimodal_Online_Federated_Learning.pdf">Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peng Cheng Laboratory</span></p>
<p>本文系统分析了多模态在线联邦学习（MMO-FL）中，由于物联网设备不稳定导致的模态数量和质量失衡问题，并提出了理论后悔界限来量化其对学习性能的影响。为解决该问题，作者提出了基于原型学习的模态数量与质量重平衡（QQR）算法，包括在线全局原型构建、原型数量重平衡和原型质量重平衡模块，有效地在训练过程中并行补偿模态失衡。实验结果表明，QQR算法在UCI-HAR和MVSA-Single两个真实多模态数据集上均优于现有基线方法，在模态失衡场景下具备较强泛化能力和通信效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Mitigating_Modality_Quantity_and_Quality_Imbalance_in_Multimodal_Online_Federated_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-Towards-the-Next-generation-Bayesian-Network-Classifiers"><a href="#96-Towards-the-Next-generation-Bayesian-Network-Classifiers" class="headerlink" title="96. Towards the Next-generation Bayesian Network Classifiers"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Towards_the_Next-generation_Bayesian_Network_Classifiers.pdf">Towards the Next-generation Bayesian Network Classifiers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhengzhou University</span></p>
<p>本文提出了一种新范式，通过引入分布式表示学习来设计高阶贝叶斯网络分类器，突破了传统方法在参数爆炸和数据稀疏性上的限制。具体方法是在K-Dependence Bayesian classifier的基础上，构建三层神经网络（NeuralKDB），学习特征和标签值的分布式表示，并通过随机梯度下降高效训练模型，使其能够捕捉高阶特征依赖关系。实验结果显示，NeuralKDB在60个UCI数据集上显著优于传统贝叶斯网络分类器及其它主流分类方法，验证了分布式表示学习在提升高阶依赖建模和分类性能上的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Towards_the_Next-generation_Bayesian_Network_Classifiers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-Graph-Neural-Diffusion-via-Generalized-Opinion-Dynamics"><a href="#97-Graph-Neural-Diffusion-via-Generalized-Opinion-Dynamics" class="headerlink" title="97. Graph Neural Diffusion via Generalized Opinion Dynamics"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Graph_Neural_Diffusion_via_Generalized_Opinion_Dynamics.pdf">Graph Neural Diffusion via Generalized Opinion Dynamics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Australian National University</span></p>
<p>本文提出了GODNF（Generalized Opinion Dynamics Neural Framework），一种基于多种意见动力学理论的图神经网络扩散统一框架。该方法通过节点特性建模和动态邻域影响，结合特征保持、初始特征依赖、邻域影响和结构正则化，实现高效、可解释且理论收敛的消息传播机制。理论分析证明GODNF能适应多样化的扩散收敛模式，实验结果在节点分类和影响力估计任务上优于现有主流GNN方法。结论表明，GODNF不仅提升了表达能力，还兼具高效性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Graph_Neural_Diffusion_via_Generalized_Opinion_Dynamics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-Meta-learning-Structure-Preserving-Dynamics"><a href="#98-Meta-learning-Structure-Preserving-Dynamics" class="headerlink" title="98. Meta-learning Structure-Preserving Dynamics"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-18/Meta-learning_Structure-Preserving_Dynamics.pdf">Meta-learning Structure-Preserving Dynamics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Arizona State University</span></p>
<p>本文提出了一种基于调制（modulation）的元学习框架，用于学习结构保持的动力学系统，包括能量守恒与耗散系统。方法创新点在于引入低维潜在向量，通过自编码方式对模型参数进行条件化，并提出了两种新型调制技术（Rank-One与Multi-Rank），显著提升模型的表达能力和泛化性。实验结果表明，所提方法在多组参数化物理系统的少样本学习场景下实现了高精度预测，并保持了动力学系统的物理约束，优于现有优化型元学习和调制方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Meta-learning_Structure-Preserving_Dynamics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/19报纸</div>
      <div>http://example.com/2025/08/2025-08-19_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Ywfhhh</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月19日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/2025-08-21_article/" title="2025/08/21报纸">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">2025/08/21报纸</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-18_article/" title="2025/08/18报纸">
                        <span class="hidden-mobile">2025/08/18报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
