

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Ywfhhh">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="今日论文报纸">
<meta property="og:url" content="http://example.com/2025/08/2025-08-19_article/index.html">
<meta property="og:site_name" content="论文报纸集合">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Optimal%20CO2%20storage%20management%20considering%20safety%20constraints%20in%20multi-stakeholder%20multi-site%20CCS%20projects%3A%20a%20game%20theoretic%20perspective.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SeamlessFlow%3A%20A%20Trainer%E2%80%93Agent%20Isolation%20RL%20Framework%20Achieving%20Bubble-Free%20Pipelines%20via%20Tag%20Scheduling.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Fusing%20Rewards%20and%20Preferences%20in%20Reinforcement%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Investigating%20Sensors%20and%20Methods%20in%20Grasp%20State%20Classification%20in%20Agricultural%20Manipulation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Visual%20Perception%20Engine%3A%20Fast%20and%20Flexible%20Multi-Head%20Inference%20for%20Robotic%20Vision%20Tasks.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Towards%20Fully%20Onboard%20State%20Estimation%20and%20Trajectory%20Tracking%20for%20UAVs%20with%20Suspended%20Payloads.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MultiPark%3A%20Multimodal%20Parking%20Transformer%20with%20Next-Segment%20Prediction.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Sim2Dust%3A%20Mastering%20Dynamic%20Waypoint%20Tracking%20on%20Granular%20Media.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Relative%20Position%20Matters%3A%20Trajectory%20Prediction%20and%20Planning%20with%20Polar%20Representation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/i2Nav-Robot%3A%20A%20Large-Scale%20Indoor-Outdoor%20Robot%20Dataset%20for%20Multi-Sensor%20Fusion%20Navigation%20and%20Mapping.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/OVSegDT%3A%20Segmenting%20Transformer%20for%20Open-Vocabulary%20Object%20Goal%20Navigation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ReachVox%3A%20Clutter-free%20Reachability%20Visualization%20for%20Robot%20Motion%20Planning%20in%20Virtual%20Reality.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Optimizing%20ROS%202%20Communication%20for%20Wireless%20Robotic%20Systems.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Swarm-in-Blocks%3A%20Simplifying%20Drone%20Swarm%20Programming%20with%20Block-Based%20Language.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inside%20Knowledge%3A%20Graph-based%20Path%20Generation%20with%20Explainable%20Data%20Augmentation%20and%20Curriculum%20Learning%20for%20Visual%20Indoor%20Navigation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ImagiDrive%3A%20A%20Unified%20Imagination-and-Planning%20Framework%20for%20Autonomous%20Driving.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Pedestrian%20Dead%20Reckoning%20using%20Invariant%20Extended%20Kalman%20Filter.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Tactile%20Robotics%3A%20An%20Outlook.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Multi-Group%20Equivariant%20Augmentation%20for%20Reinforcement%20Learning%20in%20Robot%20Manipulation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Visuomotor%20Grasping%20with%20World%20Models%20for%20Surgical%20Robots.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A%20CLIP-based%20Uncertainty%20Modal%20Modeling%20%28UMM%29%20Framework%20for%20Pedestrian%20Re-Identification%20in%20Autonomous%20Driving.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Actor-Critic%20for%20Continuous%20Action%20Chunks%3A%20A%20Reinforcement%20Learning%20Framework%20for%20Long-Horizon%20Robotic%20Manipulation%20with%20Sparse%20Reward.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Geometry-Aware%20Predictive%20Safety%20Filters%20on%20Humanoids%3A%20From%20Poisson%20Safety%20Functions%20to%20CBF%20Constrained%20MPC.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LoRAtorio%3A%20An%20intrinsic%20approach%20to%20LoRA%20Skill%20Composition.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/DashCam%20Video%3A%20A%20complementary%20low-cost%20data%20stream%20for%20on-demand%20forest-infrastructure%20system%20monitoring.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/TrajSV%3A%20A%20Trajectory-based%20Model%20for%20Sports%20Video%20Representations%20and%20Applications.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Reinforcing%20Video%20Reasoning%20Segmentation%20to%20Think%20Before%20It%20Segments.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Handwritten%20Text%20Recognition%20of%20Historical%20Manuscripts%20Using%20Transformer-Based%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Data-Driven%20Deepfake%20Image%20Detection%20Method%20-%20The%202024%20Global%20Deepfake%20Image%20Detection%20Challenge.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/RMFAT%3A%20Recurrent%20Multi-scale%20Feature%20Atmospheric%20Turbulence%20Mitigator.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/G-CUT3R%3A%20Guided%203D%20Reconstruction%20with%20Camera%20and%20Depth%20Prior%20Integration.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Leveraging%20the%20RETFound%20foundation%20model%20for%20optic%20disc%20segmentation%20in%20retinal%20images.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Noise%20Matters%3A%20Optimizing%20Matching%20Noise%20for%20Diffusion%20Classifiers.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AIM%3A%20Amending%20Inherent%20Interpretability%20via%20Self-Supervised%20Masking.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CineTrans%3A%20Learning%20to%20Generate%20Videos%20with%20Cinematic%20Transitions%20via%20Masked%20Diffusion%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SPG%3A%20Style-Prompting%20Guidance%20for%20Style-Specific%20Content%20Creation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LKFMixer%3A%20Exploring%20Large%20Kernel%20Feature%20For%20Efficient%20Image%20Super-Resolution.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AnatoMaskGAN%3A%20GNN-Driven%20Slice%20Feature%20Fusion%20and%20Noise%20Augmentation%20for%20Medical%20Semantic%20Image%20Synthesis.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/HOID-R1%3A%20Reinforcement%20Learning%20for%20Open-World%20Human-Object%20Interaction%20Detection%20Reasoning%20with%20Multimodal%20Large%20Language%20Model.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Index-Aligned%20Query%20Distillation%20for%20Transformer-based%20Incremental%20Object%20Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Delving%20into%20Dynamic%20Scene%20Cue-Consistency%20for%20Robust%203D%20Multi-Object%20Tracking.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Logic%20Unseen%3A%20Revealing%20the%20Logical%20Blindspots%20of%20Vision-Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Probing%20the%20Representational%20Power%20of%20Sparse%20Autoencoders%20in%20Vision%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/FantasyTalking2%3A%20Timestep-Layer%20Adaptive%20Preference%20Optimization%20for%20Audio-Driven%20Portrait%20Animation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Efficient%20Image-to-Image%20Schr%C3%B6dinger%20Bridge%20for%20CT%20Field%20of%20View%20Extension.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/UAV-VL-R1%3A%20Generalizing%20Vision-Language%20Models%20via%20Supervised%20Fine-Tuning%20and%20Multi-Stage%20GRPO%20for%20UAV%20Visual%20Reasoning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CHARM3R%3A%20Towards%20Unseen%20Camera%20Height%20Robust%20Monocular%203D%20Detector.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/VFM-Guided%20Semi-Supervised%20Detection%20Transformer%20for%20Source-Free%20Object%20Detection%20in%20Remote%20Sensing%20Images.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A%20Cross-Modal%20Rumor%20Detection%20Scheme%20via%20Contrastive%20Learning%20by%20Exploring%20Text%E2%80%93Image%20internal%20Correlations.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Vision-Language%20Models%20display%20a%20strong%20gender%20bias.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Generalized%20Decoupled%20Learning%20for%20Enhancing%20Open-Vocabulary%20Dense%20Perception.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/StyleMM%3A%20Stylized%203D%20Morphable%20Face%20Model%20via%20Text-Driven%20Aligned%20Image%20Translation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/E-CaTCH%3A%20Event-Centric%20Cross-Modal%20Attention%20with%20Temporal%20Consistency%20and%20Class-Imbalance%20Handling%20for%20Misinformation%20Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Versatile%20Video%20Tokenization%20with%20Generative%202D%20Gaussian%20Splatting.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Exploring%20the%20Tradeoff%20Between%20Diversity%20and%20Discrimination%20for%20Continuous%20Category%20Discovery.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Semi-supervised%20Image%20Dehazing%20via%20Expectation-Maximization%20and%20Bidirectional%20Brownian%20Bridge%20Diffusion%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Residual-based%20Efficient%20Bidirectional%20Diffusion%20Model%20for%20Image%20Dehazing%20and%20Haze%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Is%20ChatGPT-5%20Ready%20for%20Mammogram%20VQA%3F.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Controlling%20Multimodal%20LLMs%20via%20Reward-guided%20Decoding.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CRYPTOSCOPE%3A%20Utilizing%20Large%20Language%20Models%20for%20Automated%20Cryptographic%20Logic%20Vulnerability%20Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Aware%20First%2C%20Think%20Less%3A%20Dynamic%20Boundary%20Self-Awareness%20Drives%20Extreme%20Reasoning%20Efficiency%20in%20Large%20Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Causality%20Matters%3A%20How%20Temporal%20Information%20Emerges%20in%20Video%20Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ADMIRE-BayesOpt%3A%20Accelerated%20Data%20Mixture%20Re-weighting%20for%20Language%20Models%20with%20Bayesian%20Optimization.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inclusion%20Arena%3A%20An%20Open%20Platform%20for%20Evaluating%20Large%20Foundation%20Models%20with%20Real-World%20Apps.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MM-R1%3A%20Unleashing%20the%20Power%20of%20Unified%20Multimodal%20Large%20Language%20Models%20for%20Personalized%20Image%20Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Trustworthy%20AI%20Psychotherapy%3A%20Multi-Agent%20LLM%20Workflow%20for%20Counseling%20and%20Explainable%20Mental%20Disorder%20Diagnosis.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inspire%20or%20Predict%3F%20Exploring%20New%20Paradigms%20in%20Assisting%20Classical%20Planners%20with%20Large%20Language%20Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Automated%20Building%20Heritage%20Assessment%20Using%20Street-Level%20Imagery.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Reference%20Points%20in%20LLM%20Sentiment%20Analysis%3A%20The%20Role%20of%20Structured%20Context.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AIM-Bench%3A%20Evaluating%20Decision-making%20Biases%20of%20Agentic%20LLM%20as%20Inventory%20Manager.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ON-POLICY%20RL%20MEETS%20OFF-POLICY%20EXPERTS%3A%20HARMONIZING%20SUPERVISED%20FINE-TUNING%20AND%20REINFORCEMENT%20LEARNING%20VIA%20DYNAMIC%20WEIGHTING.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/When%20Punctuation%20Matters%3A%20A%20Large-Scale%20Comparison%20of%20Prompt%20Robustness%20Methods%20for%20LLMs.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ETTRL%3A%20Balancing%20Exploration%20and%20Exploitation%20in%20LLM%20Test-Time%20Reinforcement%20Learning%20Via%20Entropy%20Mechanism.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SGSimEval%3A%20A%20Comprehensive%20Multifaceted%20and%20Similarity-Enhanced%20Benchmark%20for%20Automatic%20Survey%20Generation%20Systems.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CSGO%3A%20Generalized%20Optimization%20for%20Cold%20Start%20in%20Wireless%20Collaborative%20Edge%20LLM%20Systems.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ORFuzz%3A%20Fuzzing%20the%20%5C%22Other%20Side%5C%22%20of%20LLM%20Safety%20%E2%80%93%20Testing%20Over-Refusal.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MONACO%3A%20More%20Natural%20and%20Complex%20Questions%20for%20Reasoning%20Across%20Dozens%20of%20Documents.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Dynamic%20Quality-Latency%20Aware%20Routing%20for%20LLM%20Inference%20in%20Wireless%20Edge-Device%20Networks.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LETToT%3A%20Label-Free%20Evaluation%20of%20Large%20Language%20Models%20On%20Tourism%20Using%20Expert%20Tree-of-Thought.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Group%20Fairness%20Meets%20the%20Black%20Box%3A%20Enabling%20Fair%20Algorithms%20on%20Closed%20LLMs%20via%20Post-Processing.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Generating%20Dialogues%20from%20Egocentric%20Instructional%20Videos%20for%20Task%20Assistance%3A%20Dataset%2C%20Method%20and%20Benchmark.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Role-Augmented%20Intent-Driven%20Generative%20Search%20Engine%20Optimization.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AlphaAgents%3A%20Large%20Language%20Model%20based%20Multi-Agents%20for%20Equity%20Portfolio%20Constructions.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Pretrained%20Conformers%20for%20Audio%20Fingerprinting%20and%20Retrieval.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A%20Comprehensive%20Perspective%20on%20Explainable%20AI%20across%20the%20Machine%20Learning%20Workflow.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Physics-Informed%20Diffusion%20Models%20for%20Unsupervised%20Anomaly%20Detection%20in%20Multivariate%20Time%20Series.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A%20Remedy%20for%20Over-Squashing%20in%20Graph%20Learning%20via%20Forman-Ricci%20Curvature%20based%20Graph-to-Hypergraph%20Structural%20Lifting.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Mitigating%20Modality%20Quantity%20and%20Quality%20Imbalance%20in%20Multimodal%20Online%20Federated%20Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Graph%20Neural%20Diffusion%20via%20Generalized%20Opinion%20Dynamics.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Meta-learning%20Structure-Preserving%20Dynamics.jpg">
<meta property="article:published_time" content="2025-08-19T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-19T03:33:24.799Z">
<meta property="article:author" content="Ywfhhh">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Optimal%20CO2%20storage%20management%20considering%20safety%20constraints%20in%20multi-stakeholder%20multi-site%20CCS%20projects%3A%20a%20game%20theoretic%20perspective.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>今日论文报纸 - 论文报纸集合</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>【学术动态报告】论文报纸</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文集合</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="今日论文报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-19 00:00" pubdate>
          星期二, 八月 19日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          15k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">今日论文报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>今日论文报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-19<br>📄 <strong>发现论文数量</strong>：98  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Optimal-CO2-storage-management-considering-safety-constraints-in-multi-stakeholder-multi-site-CCS-projects-a-game-theoretic-perspective"><a href="#1-Optimal-CO2-storage-management-considering-safety-constraints-in-multi-stakeholder-multi-site-CCS-projects-a-game-theoretic-perspective" class="headerlink" title="1. Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Optimal_CO2_storage_management_considering_safety_constraints_in_multi-stakeholder_multi-site_CCS_pr.pdf">Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bureau of Economic Geology, Jackson School of Geosciences, University of Texas at Austin</span></p>
<p>本文提出了一种基于安全多智能体深度确定性策略梯度（MADDPG）和约束马尔可夫博弈（CMG）的地质碳储存（GCS）多方协同管理优化框架，将多方多场地CO2储存问题建模为带安全约束的多智能体强化学习任务，并结合Embed-to-Control（E2C）机器学习代理模型作为代理环境，实现高效模拟。实验结果表明，该方法能在安全约束下提升经济回报并有效协调多方利益，尤其在全合作场景下团队净现值（NPV）优于传统多目标优化（MOO），且区域约束模型可防止压力越界，保障储存安全。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Optimal%20CO2%20storage%20management%20considering%20safety%20constraints%20in%20multi-stakeholder%20multi-site%20CCS%20projects%3A%20a%20game%20theoretic%20perspective.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-SeamlessFlow-A-Trainer–Agent-Isolation-RL-Framework-Achieving-Bubble-Free-Pipelines-via-Tag-Scheduling"><a href="#2-SeamlessFlow-A-Trainer–Agent-Isolation-RL-Framework-Achieving-Bubble-Free-Pipelines-via-Tag-Scheduling" class="headerlink" title="2. SeamlessFlow: A Trainer–Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/SeamlessFlow__A_Trainer_Agent_Isolation_RL_Framework_Achieving_Bubble-Free_Pipelines_via_Tag_Schedul.pdf">SeamlessFlow: A Trainer–Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou</span></p>
<p>SeamlessFlow提出了一种针对工业级多智能体长期任务的服务器端强化学习框架，通过引入数据面实现trainer与agent的隔离，并采用标签驱动调度将硬件资源抽象为能力标签，实现统一的时空资源分配。实验显示，其能够消除传统管道中的空闲时间，提升训练吞吐量并增强系统稳定性和扩展性，在软件工程等复杂RL任务上表现优异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SeamlessFlow%3A%20A%20Trainer%E2%80%93Agent%20Isolation%20RL%20Framework%20Achieving%20Bubble-Free%20Pipelines%20via%20Tag%20Scheduling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-Fusing-Rewards-and-Preferences-in-Reinforcement-Learning"><a href="#3-Fusing-Rewards-and-Preferences-in-Reinforcement-Learning" class="headerlink" title="3. Fusing Rewards and Preferences in Reinforcement Learning"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Fusing_Rewards_and_Preferences_in_Reinforcement_Learning.pdf">Fusing Rewards and Preferences in Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">EPFL</span></p>
<p>本论文提出了一种名为Dual-Feedback Actor（DFA）的强化学习算法，将数值奖励和成对偏好反馈统一到单一的策略更新规则中。DFA直接利用策略的对数概率建模偏好概率，无需额外的奖励建模步骤，支持从人类标注偏好或Q值合成偏好中学习，并理论上证明其偏好损失最小化能恢复熵正则化SAC策略。实验结果表明，DFA在多个控制任务中与SAC表现相当或更优，并在仅有偏好数据时超过传统奖励建模RLHF方法，且训练过程更稳定。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Fusing%20Rewards%20and%20Preferences%20in%20Reinforcement%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="4-Investigating-Sensors-and-Methods-in-Grasp-State-Classification-in-Agricultural-Manipulation"><a href="#4-Investigating-Sensors-and-Methods-in-Grasp-State-Classification-in-Agricultural-Manipulation" class="headerlink" title="4. Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Investigating_Sensors_and_Methods_in_Grasp_State_Classification_in_Agricultural_Manipulation.pdf">Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois at Urbana-Champaign</span></p>
<p>该论文提出了一种基于多传感器（IMU、红外反射、张力、触觉及RGB摄像头）集成于柔性抓取器的农业抓取状态分类方法，并比较了随机森林与LSTM模型在抓取状态（滑移、抓取失败、成功采摘等）识别上的效果。实验表明，随机森林模型结合IMU与张力传感器在真实环境下实现了100%准确率，有效提升了农作物采摘的自动化效率与可靠性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Investigating%20Sensors%20and%20Methods%20in%20Grasp%20State%20Classification%20in%20Agricultural%20Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-Visual-Perception-Engine-Fast-and-Flexible-Multi-Head-Inference-for-Robotic-Vision-Tasks"><a href="#5-Visual-Perception-Engine-Fast-and-Flexible-Multi-Head-Inference-for-Robotic-Vision-Tasks" class="headerlink" title="5. Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Visual_Perception_Engine__Fast_and_Flexible_Multi-Head_Inference_for_Robotic_Vision_Tasks.pdf">Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Jet Propulsion Laboratory, California Institute of Technology</span></p>
<p>该论文提出了VPEngine，一个为机器人视觉多任务推理设计的模块化框架，采用共享视觉基础模型（如DINOv2）作为主干，通过高效的GPU内存管理与CUDA MPS实现多任务头并行推理。实验证明，VPEngine在NVIDIA Jetson Orin AGX等设备上可实现高达3.3倍的速度提升和恒定的内存占用，极大提升了机器人视觉感知系统的实时性和资源利用率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Visual%20Perception%20Engine%3A%20Fast%20and%20Flexible%20Multi-Head%20Inference%20for%20Robotic%20Vision%20Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-Towards-Fully-Onboard-State-Estimation-and-Trajectory-Tracking-for-UAVs-with-Suspended-Payloads"><a href="#6-Towards-Fully-Onboard-State-Estimation-and-Trajectory-Tracking-for-UAVs-with-Suspended-Payloads" class="headerlink" title="6. Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Towards_Fully_Onboard_State_Estimation_and_Trajectory_Tracking_for_UAVs_with_Suspended_Payloads.pdf">Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Czech Technical University in Prague</span></p>
<p>本文提出了一种无人机（UAV）在仅依赖标准机载传感器（RTK GNSS和IMU）条件下，实现悬挂载荷的状态估计与轨迹跟踪的完整控制框架。该方法集成了线性Kalman滤波器进行状态估计，递增型模型预测控制器（MPC）用于鲁棒控制，以及模型预测轮廓控制（MPCC）进行平滑轨迹规划。实验结果表明，该系统在仿真和真实户外环境中均能实现与理想地面真值测量控制相近的跟踪性能，跟踪误差差异仅为几个百分点，并对载荷参数变化具有较强鲁棒性。结论：该方法验证了只用常规UAV硬件即可实现高精度悬挂载荷控制，极大降低了实际部署门槛。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Towards%20Fully%20Onboard%20State%20Estimation%20and%20Trajectory%20Tracking%20for%20UAVs%20with%20Suspended%20Payloads.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-MultiPark-Multimodal-Parking-Transformer-with-Next-Segment-Prediction"><a href="#7-MultiPark-Multimodal-Parking-Transformer-with-Next-Segment-Prediction" class="headerlink" title="7. MultiPark: Multimodal Parking Transformer with Next-Segment Prediction"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MultiPark__Multimodal_Parking_Transformer_with_Next-Segment_Prediction.pdf">MultiPark: Multimodal Parking Transformer with Next-Segment Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了MultiPark，一种用于自动泊车的自回归Transformer架构，结合BEV（鸟瞰图）编码、分段预测和半锚点查询，实现多模态泊车路径规划。方法采用两阶段训练策略，通过目标导向和自车碰撞损失，缓解模仿学习中的因果混淆，并在真实数据和量产车上验证了其优越性。结论表明，MultiPark在多种场景下均实现了最优性能，具有很强的泛化和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MultiPark%3A%20Multimodal%20Parking%20Transformer%20with%20Next-Segment%20Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-Sim2Dust-Mastering-Dynamic-Waypoint-Tracking-on-Granular-Media"><a href="#8-Sim2Dust-Mastering-Dynamic-Waypoint-Tracking-on-Granular-Media" class="headerlink" title="8. Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Sim2Dust__Mastering_Dynamic_Waypoint_Tracking_on_Granular_Media.pdf">Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Luxembourg</span></p>
<p>本文提出了一个完整的sim-to-real框架，通过在高度并行和程序化生成的模拟环境中使用DreamerV3等强化学习算法训练轮式探测器，实现了在复杂颗粒介质上的动态航点跟踪。实验表明，结合域随机化和程序化多样性极大提升了策略的零样本迁移能力，且通过简单动作平滑有助于实际部署的稳定性，最终成功实现了在月壤类比设施中精确导航。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Sim2Dust%3A%20Mastering%20Dynamic%20Waypoint%20Tracking%20on%20Granular%20Media.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-Relative-Position-Matters-Trajectory-Prediction-and-Planning-with-Polar-Representation"><a href="#9-Relative-Position-Matters-Trajectory-Prediction-and-Planning-with-Polar-Representation" class="headerlink" title="9. Relative Position Matters: Trajectory Prediction and Planning with Polar Representation"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Relative_Position_Matters__Trajectory_Prediction_and_Planning_with_Polar_Representation.pdf">Relative Position Matters: Trajectory Prediction and Planning with Polar Representation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本论文提出Polaris框架，首次在自动驾驶轨迹预测与规划任务中全面采用极坐标表示，包含Polar场景上下文编码和Polar关系细化模块，并引入Relative Embedding Transformer显式建模交通元素之间的距离和方向关系。实验在Argoverse 2和nuPlan自动驾驶基准上表明，Polaris在轨迹预测和规划性能上达到最新最优表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Relative%20Position%20Matters%3A%20Trajectory%20Prediction%20and%20Planning%20with%20Polar%20Representation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-i2Nav-Robot-A-Large-Scale-Indoor-Outdoor-Robot-Dataset-for-Multi-Sensor-Fusion-Navigation-and-Mapping"><a href="#10-i2Nav-Robot-A-Large-Scale-Indoor-Outdoor-Robot-Dataset-for-Multi-Sensor-Fusion-Navigation-and-Mapping" class="headerlink" title="10. i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/i2Nav-Robot__A_Large-Scale_Indoor-Outdoor_Robot_Dataset_for_Multi-Sensor_Fusion_Navigation_and_Mappi.pdf">i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wuhan University</span></p>
<p>该论文提出了i2Nav-Robot，一个面向室内外环境、支持多传感器融合（含最新固态LiDAR、4D毫米波雷达、立体相机、GNSS&#x2F;IMU&#x2F;里程计）的大规模地面无人车（UGV）导航与建图数据集。方法包括高精度硬件同步、复杂场景采集与厘米级高频地面真值生成，并在十余种开源导航与SLAM系统上验证了数据集的高质量。结论显示该数据集极大促进了UGV在复杂环境下导航、定位和多传感器融合研究。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/i2Nav-Robot%3A%20A%20Large-Scale%20Indoor-Outdoor%20Robot%20Dataset%20for%20Multi-Sensor%20Fusion%20Navigation%20and%20Mapping.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-OVSegDT-Segmenting-Transformer-for-Open-Vocabulary-Object-Goal-Navigation"><a href="#11-OVSegDT-Segmenting-Transformer-for-Open-Vocabulary-Object-Goal-Navigation" class="headerlink" title="11. OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/OVSegDT__Segmenting_Transformer_for_Open-Vocabulary_Object_Goal_Navigation.pdf">OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AIRI</span></p>
<p>该论文提出了一种新型开集目标导航方法OVSegDT，通过在Transformer政策模型中引入目标对象分割二值掩码（目标mask）以及辅助语义分割损失，实现了对自由表述目标类别的强泛化能力。创新的EALM机制自适应混合模仿学习和强化学习信号，显著提升训练效率和安全性，实验在HM3D-OVON基准上达到SOTA，尤其在未见类别上表现优异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/OVSegDT%3A%20Segmenting%20Transformer%20for%20Open-Vocabulary%20Object%20Goal%20Navigation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-ReachVox-Clutter-free-Reachability-Visualization-for-Robot-Motion-Planning-in-Virtual-Reality"><a href="#12-ReachVox-Clutter-free-Reachability-Visualization-for-Robot-Motion-Planning-in-Virtual-Reality" class="headerlink" title="12. ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/ReachVox__Clutter-free_Reachability_Visualization_for_Robot_Motion_Planning_in_Virtual_Reality.pdf">ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Coburg University of Applied Sciences and Arts</span></p>
<p>该论文提出了一种名为REACHVOX的新型可视化方法，通过在虚拟现实环境中以半透明彩色体素（voxel）编码并显示机器人在任务空间中的可达性，显著减少视觉杂乱，帮助远程操作员更高效地规划和调整机器人与工件的空间关系。用户实验（n&#x3D;20）表明，REACHVOX在任务完成时间、操作次数、主观可用性和信任度等方面均优于无可视化基线，参与者更倾向于使用该方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ReachVox%3A%20Clutter-free%20Reachability%20Visualization%20for%20Robot%20Motion%20Planning%20in%20Virtual%20Reality.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-Optimizing-ROS-2-Communication-for-Wireless-Robotic-Systems"><a href="#13-Optimizing-ROS-2-Communication-for-Wireless-Robotic-Systems" class="headerlink" title="13. Optimizing ROS 2 Communication for Wireless Robotic Systems"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Optimizing_ROS_2_Communication_for_Wireless_Robotic_Systems.pdf">Optimizing ROS 2 Communication for Wireless Robotic Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">DGIST</span></p>
<p>该论文针对ROS 2在无线环境下大数据负载（如高分辨率图像和点云）传输的关键瓶颈，首次系统分析DDS通信栈的网络层表现，归纳出IP分片、重传时序和缓冲突发三大性能障碍。作者提出一种可通过XML QoS配置实现的轻量级DDS优化框架，无需协议或系统改动，通过调整RTPS消息大小、重传速率和历史缓存容量，显著提高无线大数据传输的可靠性和实时性。实验表明，该优化在高丢包和链路中断等复杂无线场景下均显著提升了消息接收率和延迟表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Optimizing%20ROS%202%20Communication%20for%20Wireless%20Robotic%20Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-CRAFT-GUI-Curriculum-Reinforced-Agent-For-GUI-Tasks"><a href="#14-CRAFT-GUI-Curriculum-Reinforced-Agent-For-GUI-Tasks" class="headerlink" title="14. CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CRAFT-GUI__Curriculum-Reinforced_Agent_For_GUI_Tasks.pdf">CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ant Group</span></p>
<p>本论文提出了一种针对GUI任务的课程化强化学习框架CRAFT-GUI，核心方法是基于Group Relative Policy Optimization (GRPO) 的分阶段学习策略，将任务按难度分层并通过细粒度混合奖励机制进行优化，实现从简单到复杂任务的逐步训练。实验证明CRAFT-GUI在公开和内部基准数据集上均明显优于现有方法，提升了5.6%~10.3%的任务成功率，验证了课程化强化学习在复杂GUI交互中的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-Swarm-in-Blocks-Simplifying-Drone-Swarm-Programming-with-Block-Based-Language"><a href="#15-Swarm-in-Blocks-Simplifying-Drone-Swarm-Programming-with-Block-Based-Language" class="headerlink" title="15. Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Swarm-in-Blocks__Simplifying_Drone_Swarm_Programming_with_Block-Based_Language.pdf">Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Sao Paulo</span></p>
<p>该论文提出了Swarm-in-Blocks，一个基于可视化积木编程的无人机集群控制高层接口，集成了Clover平台并通过模块化代码块实现无人机多机编队、碰撞规避、变换与实时监控。实验表明该系统显著降低了编程难度，提高了无人机集群管理的易用性和扩展性，尤其适用于教育和实际行业应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Swarm-in-Blocks%3A%20Simplifying%20Drone%20Swarm%20Programming%20with%20Block-Based%20Language.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-Inside-Knowledge-Graph-based-Path-Generation-with-Explainable-Data-Augmentation-and-Curriculum-Learning-for-Visual-Indoor-Navigation"><a href="#16-Inside-Knowledge-Graph-based-Path-Generation-with-Explainable-Data-Augmentation-and-Curriculum-Learning-for-Visual-Indoor-Navigation" class="headerlink" title="16. Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Inside_Knowledge__Graph-based_Path_Generation_with_Explainable_Data_Augmentation_and_Curriculum_Lear.pdf">Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Science and Technology POLITEHNICA Bucharest</span></p>
<p>本文提出了一种基于视觉输入的室内导航系统，仅通过移动设备摄像头实现目标方向预测，避免使用特殊传感器、地图或互联网。方法包括图结构路径生成、基于Grad-CAM的可解释数据增强和课程学习，有效提升模型鲁棒性和泛化能力。实验表明，该系统在复杂、拥挤环境下能够高效、准确地指导用户到达目标点，并在Android平台实现实时部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inside%20Knowledge%3A%20Graph-based%20Path%20Generation%20with%20Explainable%20Data%20Augmentation%20and%20Curriculum%20Learning%20for%20Visual%20Indoor%20Navigation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-ImagiDrive-A-Unified-Imagination-and-Planning-Framework-for-Autonomous-Driving"><a href="#17-ImagiDrive-A-Unified-Imagination-and-Planning-Framework-for-Autonomous-Driving" class="headerlink" title="17. ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/ImagiDrive__A_Unified_Imagination-and-Planning_Framework_for_Autonomous_Driving.pdf">ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>该论文提出ImagiDrive，一种端到端自动驾驶框架，将基于视觉-语言模型（VLM）的驾驶智能体与基于驾驶世界模型（DWM）的场景生成器结合，形成循环的想象-规划闭环。方法包括多模态输入、轨迹预测、场景想象、关键帧反馈与迭代优化，并引入早停和轨迹选择策略提升效率和安全性。实验表明ImagiDrive在NeuroNCAP、Turning-nuScenes和NAVSIM等数据集上均优于现有方法，尤其在复杂场景下展现更强鲁棒性和更低碰撞率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ImagiDrive%3A%20A%20Unified%20Imagination-and-Planning%20Framework%20for%20Autonomous%20Driving.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-Pedestrian-Dead-Reckoning-using-Invariant-Extended-Kalman-Filter"><a href="#18-Pedestrian-Dead-Reckoning-using-Invariant-Extended-Kalman-Filter" class="headerlink" title="18. Pedestrian Dead Reckoning using Invariant Extended Kalman Filter"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Pedestrian_Dead_Reckoning_using_Invariant_Extended_Kalman_Filter.pdf">Pedestrian Dead Reckoning using Invariant Extended Kalman Filter</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ShanghaiTech University</span></p>
<p>该论文提出了一种基于IMU的低成本惯性行人航位推算方法，利用矩阵Lie群理论推导并实现了Invariant Extended Kalman Filter（InEKF），结合站立足的伪测量进行创新校正，无需额外接触传感器。通过对比标准EKF和InEKF在运动捕捉、室内多楼层步行及双足机器人实验中的表现，结果显示InEKF在精度和参数调优上均优于EKF，且能在机器人由强化学习控制时仍保持稳定估计。结论表明该方法可显著提升机器人在GPS拒止环境下的定位能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Pedestrian%20Dead%20Reckoning%20using%20Invariant%20Extended%20Kalman%20Filter.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-Tactile-Robotics-An-Outlook"><a href="#19-Tactile-Robotics-An-Outlook" class="headerlink" title="19. Tactile Robotics: An Outlook"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Tactile_Robotics__An_Outlook.pdf">Tactile Robotics: An Outlook</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">King’s College London</span></p>
<p>本论文系统综述了触觉机器人领域的最新进展，包括触觉材料与传感器、传感器网络、仿真、数据采集与基准、数据解释、视觉与触觉多模态融合以及主动触觉感知。文章指出未来的发展方向在于提升传感器性能、网络通信、触觉数据处理和多模态集成，并强调机器学习在触觉数据解读中的重要作用。结论认为触觉机器人将在制造、医疗、农业等领域推动机器人实现类人灵巧交互，前景广阔。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Tactile%20Robotics%3A%20An%20Outlook.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-Multi-Group-Equivariant-Augmentation-for-Reinforcement-Learning-in-Robot-Manipulation"><a href="#20-Multi-Group-Equivariant-Augmentation-for-Reinforcement-Learning-in-Robot-Manipulation" class="headerlink" title="20. Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Multi-Group_Equivariant_Augmentation_for_Reinforcement_Learning_in_Robot_Manipulation.pdf">Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong</span></p>
<p>该论文提出了一种多组非等距对称性的数据增强方法（Multi-Group Equivariance Augmentation, MEA），在部分可观测马尔可夫决策过程（POMDP）框架下，利用多组独立空间和时间群变换提升机器人操作任务中视觉-运动学习的采样效率。方法包含新型数据增强流程与体素投影视觉表征，并结合主流离线强化学习算法，在通用与手术机器人操作任务中实现采样效率的显著提升。实验表明，该方法在仿真和真实机器人中均能提升任务成功率，减少示范数据和训练时间，无性能损失。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Multi-Group%20Equivariant%20Augmentation%20for%20Reinforcement%20Learning%20in%20Robot%20Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-Visuomotor-Grasping-with-World-Models-for-Surgical-Robots"><a href="#21-Visuomotor-Grasping-with-World-Models-for-Surgical-Robots" class="headerlink" title="21. Visuomotor Grasping with World Models for Surgical Robots"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Visuomotor_Grasping_with_World_Models_for_Surgical_Robots.pdf">Visuomotor Grasping with World Models for Surgical Robots</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chinese University of Hong Kong</span></p>
<p>本文提出了GASv2——一种基于世界模型的视觉运动学习框架，专为外科手术机器人自动抓取任务设计。方法上，GASv2结合了世界模型架构、专用视觉感知管道（包括深度估计、视频分割、体素正交投影和动态聚焦编码）以及混合控制架构（融合PID、Virtual Clutch和RL策略），通过仿真域随机化训练实现仿真到实物的迁移，仅需标准内窥镜双目相机即可在实体机器人（仿体及离体组织平台）部署，并无需真实抓取演示数据。实验结果表明，GASv2在仿体和离体平台均达到65%的抓取成功率，能泛化到未见目标和夹持器，并适应多类干扰，表现出优异的泛化性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Visuomotor%20Grasping%20with%20World%20Models%20for%20Surgical%20Robots.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-A-CLIP-based-Uncertainty-Modal-Modeling-UMM-Framework-for-Pedestrian-Re-Identification-in-Autonomous-Driving"><a href="#22-A-CLIP-based-Uncertainty-Modal-Modeling-UMM-Framework-for-Pedestrian-Re-Identification-in-Autonomous-Driving" class="headerlink" title="22. A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/A_CLIP-based_Uncertainty_Modal_Modeling_(UMM)_Framework_for_Pedestrian_Re-Identification_in_Autonomo.pdf">A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Jiangsu Zhenjiang Technician College</span></p>
<p>该论文提出了一种轻量级的不确定性模态建模（UMM）框架，结合多模态令牌映射器、合成模态增强策略和跨模态线索交互学习器，实现RGB、红外、素描和文本等多模态输入的统一特征表示并有效融合，显著提升在模态缺失情况下的鲁棒性。实验表明，该框架在自动驾驶场景下的人体重识别任务中具备强泛化能力和计算效率，超越现有方法，适合资源受限的车载平台部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A%20CLIP-based%20Uncertainty%20Modal%20Modeling%20%28UMM%29%20Framework%20for%20Pedestrian%20Re-Identification%20in%20Autonomous%20Driving.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-Actor-Critic-for-Continuous-Action-Chunks-A-Reinforcement-Learning-Framework-for-Long-Horizon-Robotic-Manipulation-with-Sparse-Reward"><a href="#23-Actor-Critic-for-Continuous-Action-Chunks-A-Reinforcement-Learning-Framework-for-Long-Horizon-Robotic-Manipulation-with-Sparse-Reward" class="headerlink" title="23. Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Actor-Critic_for_Continuous_Action_Chunks__A_Reinforcement_Learning_Framework_for_Long-Horizon_Robot.pdf">Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>该论文提出AC3（Actor-Critic for Continuous Chunks），一种用于长时序、稀疏奖励机器人操作任务的强化学习新框架。AC3基于DDPG风格的actor-critic结构，利用连续动作块预测和两大关键机制：1）仅从成功轨迹异步更新actor，提升策略稳定性；2）通过intra-chunk n步回报和自监督奖励进行critic稳定训练。实验证明，AC3在BiGym与RLBench 25项任务上，仅需少量示范和简单模型结构即可超越现有方法，实现高成功率和数据高效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Actor-Critic%20for%20Continuous%20Action%20Chunks%3A%20A%20Reinforcement%20Learning%20Framework%20for%20Long-Horizon%20Robotic%20Manipulation%20with%20Sparse%20Reward.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-Geometry-Aware-Predictive-Safety-Filters-on-Humanoids-From-Poisson-Safety-Functions-to-CBF-Constrained-MPC"><a href="#24-Geometry-Aware-Predictive-Safety-Filters-on-Humanoids-From-Poisson-Safety-Functions-to-CBF-Constrained-MPC" class="headerlink" title="24. Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Geometry-Aware_Predictive_Safety_Filters_on_Humanoids__From_Poisson_Safety_Functions_to_CBF_Constrai.pdf">Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">California Institute of Technology</span></p>
<p>该论文提出了一种基于非线性模型预测控制（MPC）与控制障碍函数（CBF）结合的预测安全滤波方法，利用Poisson安全函数从感知数据直接数值合成CBF约束，实现对机器人几何和环境动态变化的安全感知。通过将静态Dirichlet问题扩展为时间变化的动态边界问题，并采用Minkowski集合运算提升到高维配置空间，论文实现了实时安全轨迹生成，并在多种安全关键场景下的类人和四足机器人上进行了实验验证，结果显示该方法具备良好的通用性和实际安全性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Geometry-Aware%20Predictive%20Safety%20Filters%20on%20Humanoids%3A%20From%20Poisson%20Safety%20Functions%20to%20CBF%20Constrained%20MPC.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="25-LoRAtorio-An-intrinsic-approach-to-LoRA-Skill-Composition"><a href="#25-LoRAtorio-An-intrinsic-approach-to-LoRA-Skill-Composition" class="headerlink" title="25. LoRAtorio: An intrinsic approach to LoRA Skill Composition"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/LoRAtorio__An_intrinsic_approach_to_LoRA_Skill_Composition.pdf">LoRAtorio: An intrinsic approach to LoRA Skill Composition</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Cambridge Research Laboratory, Toshiba Europe</span></p>
<p>LoRAtorio提出了一种无训练、多LoRA适配器组合方法，通过在扩散模型的潜空间将特征划分为空间patch，计算每个patch与基础模型的余弦相似度，形成空间感知的加权矩阵，实现多LoRA输出的动态聚合。方法还通过改进classifier-free guidance机制，融合基础模型的无条件分数以缓解领域漂移，并扩展至动态模块选择场景，在推理时从大量LoRA自动选择相关模块。实验表明，LoRAtorio在ComposLoRA基准和多种潜扩散模型上达到SOTA，提升了多技能图像生成质量和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LoRAtorio%3A%20An%20intrinsic%20approach%20to%20LoRA%20Skill%20Composition.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-DashCam-Video-A-complementary-low-cost-data-stream-for-on-demand-forest-infrastructure-system-monitoring"><a href="#26-DashCam-Video-A-complementary-low-cost-data-stream-for-on-demand-forest-infrastructure-system-monitoring" class="headerlink" title="26. DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/DashCam_Video__A_complementary_low-cost_data_stream_for_on-demand_forest-infrastructure_system_monit.pdf">DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Connecticut</span></p>
<p>本文提出了一种基于车载行车记录仪（dashcam）视频的端到端监测框架，通过结合单目深度估计（Depth-anything V2）、XGBoost深度校正和基于相机几何的三角测量，实现了对城市道路旁植被与基础设施的实时结构化测量与地理定位。实验证明，该方法在低速、内置相机条件下，树木和杆状结构的定位均值误差分别为2.83米和0.88米，显著提升了低成本、高频率、对象级别的城市基础设施监测能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/DashCam%20Video%3A%20A%20complementary%20low-cost%20data%20stream%20for%20on-demand%20forest-infrastructure%20system%20monitoring.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-TrajSV-A-Trajectory-based-Model-for-Sports-Video-Representations-and-Applications"><a href="#27-TrajSV-A-Trajectory-based-Model-for-Sports-Video-Representations-and-Applications" class="headerlink" title="27. TrajSV: A Trajectory-based Model for Sports Video Representations and Applications"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/TrajSV__A_Trajectory-based_Model_for_Sports_Video_Representations_and_Applications.pdf">TrajSV: A Trajectory-based Model for Sports Video Representations and Applications</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huawei Technologies, Co., Ltd.</span></p>
<p>该论文提出了TrajSV框架，通过数据预处理、轨迹增强Transformer模块（CRNet）和编码器-解码器结构（VRNet），利用三重对比学习无监督方式从原始体育赛事视频中提取球员与球的运动轨迹，融合视觉与空间特征，学习视频及片段多粒度表示，支持视频检索、动作定位和视频描述等多任务。实验在多个体育项目和典型数据集上表明TrajSV在体育视频检索、动作定位和视频描述任务均大幅超越现有方法，检索性能提升近70%，在动作分类和描述上也有显著提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/TrajSV%3A%20A%20Trajectory-based%20Model%20for%20Sports%20Video%20Representations%20and%20Applications.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-Reinforcing-Video-Reasoning-Segmentation-to-Think-Before-It-Segments"><a href="#28-Reinforcing-Video-Reasoning-Segmentation-to-Think-Before-It-Segments" class="headerlink" title="28. Reinforcing Video Reasoning Segmentation to Think Before It Segments"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Reinforcing_Video_Reasoning_Segmentation_to_Think_Before_It_Segments.pdf">Reinforcing Video Reasoning Segmentation to Think Before It Segments</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Dalian University of Technology</span></p>
<p>该论文提出了Veason-R1框架，通过将视频推理分割（VRS）任务分解为关键帧选择和目标定位两个子任务，结合链式思维(CoT)监督微调和基于群体相对策略优化（GRPO）的强化学习，显式建模可解释推理轨迹。实验证明，Veason-R1在多个VRS基准上达到最新最优性能，显著提升了多模态理解和分割鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Reinforcing%20Video%20Reasoning%20Segmentation%20to%20Think%20Before%20It%20Segments.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-Handwritten-Text-Recognition-of-Historical-Manuscripts-Using-Transformer-Based-Models"><a href="#29-Handwritten-Text-Recognition-of-Historical-Manuscripts-Using-Transformer-Based-Models" class="headerlink" title="29. Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Handwritten_Text_Recognition_of_Historical_Manuscripts_Using_Transformer-Based_Models.pdf">Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mississippi State University</span></p>
<p>该论文提出在16世纪拉丁历史手稿上应用TrOCR（Transformer-based OCR）模型，通过针对性图像预处理、六种现有和四种新设计的数据增强技术提升识别性能，并利用集成学习整合多种增强模型。结果显示，Elastic Distortion增强单模型CER达1.86，Top-5投票集成CER为1.60，较现有TrOCR_BASE结果提升50%，显著改进了历史手稿的文本识别准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Handwritten%20Text%20Recognition%20of%20Historical%20Manuscripts%20Using%20Transformer-Based%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-Data-Driven-Deepfake-Image-Detection-Method-The-2024-Global-Deepfake-Image-Detection-Challenge"><a href="#30-Data-Driven-Deepfake-Image-Detection-Method-The-2024-Global-Deepfake-Image-Detection-Challenge" class="headerlink" title="30. Data-Driven Deepfake Image Detection Method - The 2024 Global Deepfake Image Detection Challenge"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Data-Driven_Deepfake_Image_Detection_Method_--_The_2024_Global_Deepfake_Image_Detection_Challenge.pdf">Data-Driven Deepfake Image Detection Method - The 2024 Global Deepfake Image Detection Challenge</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AI Innovation Center, China Unicom</span></p>
<p>本文提出了一种基于Swin Transformer V2-B分类网络的深度伪造图像检测方法，通过多样化的数据增强和样本生成（如随机人脸区域裁剪、卡通化、灰度叠加等），显著提升了模型的泛化能力和鲁棒性。实验结果显示该方法在多类型Deepfake攻击场景下表现优异，有效应对复杂的伪造检测任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Data-Driven%20Deepfake%20Image%20Detection%20Method%20-%20The%202024%20Global%20Deepfake%20Image%20Detection%20Challenge.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-RMFAT-Recurrent-Multi-scale-Feature-Atmospheric-Turbulence-Mitigator"><a href="#31-RMFAT-Recurrent-Multi-scale-Feature-Atmospheric-Turbulence-Mitigator" class="headerlink" title="31. RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/RMFAT__Recurrent_Multi-scale_Feature_Atmospheric_Turbulence_Mitigator.pdf">RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Bristol</span></p>
<p>该论文提出RMFAT，一种轻量级递归多尺度Transformer架构，通过将多帧恢复任务重新表述为两帧递归过程，结合多尺度特征提取、时序配准和基于光流的特征对齐以提升视频在大气湍流下的恢复质量和时序一致性。实验证明，RMFAT在清晰度恢复和推理速度上都优于现有方法，参数量小，适合实时应用场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/RMFAT%3A%20Recurrent%20Multi-scale%20Feature%20Atmospheric%20Turbulence%20Mitigator.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration"><a href="#32-G-CUT3R-Guided-3D-Reconstruction-with-Camera-and-Depth-Prior-Integration" class="headerlink" title="32. G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/G-CUT3R__Guided_3D_Reconstruction_with_Camera_and_Depth_Prior_Integration.pdf">G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Skoltech</span></p>
<p>G-CUT3R提出了一种轻量级、模态无关的3D场景重建方法，通过在CUT3R模型基础上集成辅助先验（如相机内参、位姿和深度图），采用多模态特征编码及ZeroConv融合机制，实现RGB图像与多种先验信息的高效结合。实验结果显示，G-CUT3R在多项3D重建、深度估计和相机位姿估计任务上显著提升了准确性和完整性，优于现有主流方法，验证了融合策略和架构设计的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/G-CUT3R%3A%20Guided%203D%20Reconstruction%20with%20Camera%20and%20Depth%20Prior%20Integration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-Leveraging-the-RETFound-foundation-model-for-optic-disc-segmentation-in-retinal-images"><a href="#33-Leveraging-the-RETFound-foundation-model-for-optic-disc-segmentation-in-retinal-images" class="headerlink" title="33. Leveraging the RETFound foundation model for optic disc segmentation in retinal images"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Leveraging_the_RETFound_foundation_model_for_optic_disc_segmentation_in_retinal_images.pdf">Leveraging the RETFound foundation model for optic disc segmentation in retinal images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Dundee</span></p>
<p>该论文首次将RETFound视觉基础模型应用于视盘分割任务，通过冻结预训练RETFound编码器并结合Segmenter解码器，实现了在多公开和私有数据集上的高效视盘分割。实验结果显示，该方法在内部验证、领域泛化和领域自适应任务上均优于当前主流分割网络，且在仅需极少任务专用标注样本下即可达到约96%的Dice分数，具有出色的泛化能力和临床应用前景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Leveraging%20the%20RETFound%20foundation%20model%20for%20optic%20disc%20segmentation%20in%20retinal%20images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-Noise-Matters-Optimizing-Matching-Noise-for-Diffusion-Classifiers"><a href="#34-Noise-Matters-Optimizing-Matching-Noise-for-Diffusion-Classifiers" class="headerlink" title="34. Noise Matters: Optimizing Matching Noise for Diffusion Classifiers"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Noise_Matters__Optimizing_Matching_Noise_for_Diffusion_Classifiers.pdf">Noise Matters: Optimizing Matching Noise for Diffusion Classifiers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>本论文针对扩散分类器（Diffusion Classifier, DC）在噪声采样不稳定导致分类结果波动大的问题，提出了一种噪声优化方法NoOp。该方法从频率匹配和空间匹配两个角度，分别通过优化数据集特定参数化噪声和训练Meta-Network输出图像特定噪声偏移，以学习能稳定提升分类性能的“好噪声”。实验表明NoOp能在多数据集上高效提升DC的稳定性和准确率，并与prompt优化等现有方法互补，兼具跨数据集泛化能力和对不同扩散模型的适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Noise%20Matters%3A%20Optimizing%20Matching%20Noise%20for%20Diffusion%20Classifiers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-AIM-Amending-Inherent-Interpretability-via-Self-Supervised-Masking"><a href="#35-AIM-Amending-Inherent-Interpretability-via-Self-Supervised-Masking" class="headerlink" title="35. AIM: Amending Inherent Interpretability via Self-Supervised Masking"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/AIM__Amending_Inherent_Interpretability_via_Self-Supervised_Masking.pdf">AIM: Amending Inherent Interpretability via Self-Supervised Masking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Max-Planck-Institute for Informatics</span></p>
<p>该论文提出了AIM（Amending Inherent Interpretability via Self-Supervised Masking）方法，通过自监督的特征掩码机制，在无需额外标注的情况下促使神经网络聚焦于真实、可解释的判别特征，抑制对虚假特征的依赖。AIM在多个卷积层引入顶向下的稀疏掩码学习流程，利用分类损失自监督优化掩码，显著提升了模型在多种视觉分类任务中的解释性（EPG分数）和泛化能力，且几乎无性能损失或提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AIM%3A%20Amending%20Inherent%20Interpretability%20via%20Self-Supervised%20Masking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-CineTrans-Learning-to-Generate-Videos-with-Cinematic-Transitions-via-Masked-Diffusion-Models"><a href="#36-CineTrans-Learning-to-Generate-Videos-with-Cinematic-Transitions-via-Masked-Diffusion-Models" class="headerlink" title="36. CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CineTrans__Learning_to_Generate_Videos_with_Cinematic_Transitions_via_Masked_Diffusion_Models.pdf">CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本文提出CineTrans框架，通过分析视频扩散模型中的注意力模式，发现镜头内外帧的相关性差异，进而设计出基于掩码机制的控制方法，实现对多镜头视频中转场的精细化控制。同时，作者构建了Cine250K多镜头视频文本数据集，并在此基础上微调模型，使其生成符合电影剪辑风格的多镜头视频。实验表明，CineTrans在转场控制、时序一致性及整体视频质量方面优于现有方法。结论显示，掩码机制可稳定实现电影级转场，推动可控视频合成研究。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CineTrans%3A%20Learning%20to%20Generate%20Videos%20with%20Cinematic%20Transitions%20via%20Masked%20Diffusion%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-SPG-Style-Prompting-Guidance-for-Style-Specific-Content-Creation"><a href="#37-SPG-Style-Prompting-Guidance-for-Style-Specific-Content-Creation" class="headerlink" title="37. SPG: Style-Prompting Guidance for Style-Specific Content Creation"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/SPG__Style-Prompting_Guidance_for_Style-Specific_Content_Creation.pdf">SPG: Style-Prompting Guidance for Style-Specific Content Creation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen University</span></p>
<p>该论文提出了Style-Prompting Guidance（SPG），这是一种训练无关、采样为主的方法，通过在扩散模型采样过程中引入风格噪声向量并结合Classifier-Free Guidance（CFG），实现了既能保持文本语义又能高保真风格对齐的图像生成。实验证明SPG在风格一致性与语义对齐方面优于同类方法，且可无缝集成ControlNet、IP-Adapter等插件，具有高效、通用和强鲁棒性等优点。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SPG%3A%20Style-Prompting%20Guidance%20for%20Style-Specific%20Content%20Creation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-LKFMixer-Exploring-Large-Kernel-Feature-For-Efficient-Image-Super-Resolution"><a href="#38-LKFMixer-Exploring-Large-Kernel-Feature-For-Efficient-Image-Super-Resolution" class="headerlink" title="38. LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/LKFMixer__Exploring_Large_Kernel_Feature_For_Efficient_Image_Super-Resolution.pdf">LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Yanshan University</span></p>
<p>该论文提出了一种纯卷积神经网络模型LKFMixer，通过大卷积核（31×31）结合坐标分解和部分通道卷积，实现了高效的非局部特征提取，提升了超分辨率性能且保持低模型复杂度。实验结果显示，LKFMixer在多个公开数据集上超越了当前轻量级SOTA方法，推理速度远快于Transformer模型且重建质量更佳。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LKFMixer%3A%20Exploring%20Large%20Kernel%20Feature%20For%20Efficient%20Image%20Super-Resolution.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-AnatoMaskGAN-GNN-Driven-Slice-Feature-Fusion-and-Noise-Augmentation-for-Medical-Semantic-Image-Synthesis"><a href="#39-AnatoMaskGAN-GNN-Driven-Slice-Feature-Fusion-and-Noise-Augmentation-for-Medical-Semantic-Image-Synthesis" class="headerlink" title="39. AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/AnatoMaskGAN__GNN-Driven_Slice_Feature_Fusion_and_Noise_Augmentation_for_Medical_Semantic_Image_Synt.pdf">AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southwest University</span></p>
<p>该论文提出AnatoMaskGAN框架，通过GNN驱动的切片特征融合模块、三维空间噪声注入和灰度纹理分类器，提升医学语义图像合成的空间一致性和结构细节。实验结果显示，在L2R-OASIS和L2R-Abdomen CT数据集上，AnatoMaskGAN显著提升了PSNR和SSIM，优于现有主流方法，验证了各核心模块对重建精度和感知质量的独立贡献。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AnatoMaskGAN%3A%20GNN-Driven%20Slice%20Feature%20Fusion%20and%20Noise%20Augmentation%20for%20Medical%20Semantic%20Image%20Synthesis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-HOID-R1-Reinforcement-Learning-for-Open-World-Human-Object-Interaction-Detection-Reasoning-with-Multimodal-Large-Language-Model"><a href="#40-HOID-R1-Reinforcement-Learning-for-Open-World-Human-Object-Interaction-Detection-Reasoning-with-Multimodal-Large-Language-Model" class="headerlink" title="40. HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/HOID-R1__Reinforcement_Learning_for_Open-World_Human-Object_Interaction_Detection_Reasoning_with_Mul.pdf">HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ShanghaiTech University</span></p>
<p>本文提出HOID-R1方法，将链式思维（Chain-of-Thought, CoT）引导的监督微调与群体相对策略优化（Group Relative Policy Optimization, GRPO）结合于强化学习框架内，融合多模态大语言模型（MLLM）作为判别器，提升人-物体交互（HOI）检测的开放世界泛化与推理能力。实验表明，该方法在HICO-DET和SWIG-HOI基准上显著超过现有方法，尤其在未见类别和开放词汇描述下表现优异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/HOID-R1%3A%20Reinforcement%20Learning%20for%20Open-World%20Human-Object%20Interaction%20Detection%20Reasoning%20with%20Multimodal%20Large%20Language%20Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-Index-Aligned-Query-Distillation-for-Transformer-based-Incremental-Object-Detection"><a href="#41-Index-Aligned-Query-Distillation-for-Transformer-based-Incremental-Object-Detection" class="headerlink" title="41. Index-Aligned Query Distillation for Transformer-based Incremental Object Detection"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Index-Aligned_Query_Distillation_for_Transformer-based_Incremental_Object_Detection.pdf">Index-Aligned Query Distillation for Transformer-based Incremental Object Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本文提出了Index-Aligned Query Distillation (IAQD)方法，针对基于Transformer的增量式目标检测中的灾难性遗忘问题，通过对齐不同阶段相同索引的查询，保持旧类别的语义和空间编码能力。方法还引入Proxy Query Selection和标签重对齐机制，进一步提升对新旧类别的平衡检测能力。实验结果表明，IAQD在COCO和PASCAL VOC等基准上大幅优于现有方法，有效缓解知识遗忘，取得新的state-of-the-art。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Index-Aligned%20Query%20Distillation%20for%20Transformer-based%20Incremental%20Object%20Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-Delving-into-Dynamic-Scene-Cue-Consistency-for-Robust-3D-Multi-Object-Tracking"><a href="#42-Delving-into-Dynamic-Scene-Cue-Consistency-for-Robust-3D-Multi-Object-Tracking" class="headerlink" title="42. Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Delving_into_Dynamic_Scene_Cue-Consistency_for_Robust_3D_Multi-Object_Tracking.pdf">Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出了DSC-Track，一种基于Transformer的3D多目标跟踪方法。其关键流程包括利用Point Pair Features（PPF）进行几何编码，统一时空聚合模块学习判别性轨迹特征，Cue-Consistency Transformer模块显式对齐历史轨迹与当前检测的特征以增强鲁棒性，并通过动态更新机制保持显著时空信息。实验结果表明，DSC-Track在nuScenes和Waymo等主流自动驾驶数据集上达到新的SOTA性能，有效提升跟踪准确率并显著减少ID切换，验证了空间一致性建模在复杂场景中的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Delving%20into%20Dynamic%20Scene%20Cue-Consistency%20for%20Robust%203D%20Multi-Object%20Tracking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-Logic-Unseen-Revealing-the-Logical-Blindspots-of-Vision-Language-Models"><a href="#43-Logic-Unseen-Revealing-the-Logical-Blindspots-of-Vision-Language-Models" class="headerlink" title="43. Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Logic_Unseen__Revealing_the_Logical_Blindspots_of_Vision-Language_Models.pdf">Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-sen University</span></p>
<p>本文提出了LogicBench，一个涵盖九大逻辑类别和四类场景（图像、视频、异常检测和医学诊断）的综合基准，用于系统性评估视觉-语言模型（VLMs）的逻辑理解能力。作者还提出了基于逻辑感知数据生成和多重对比优化目标的LogicCLIP训练框架，显著提升了VLMs对复杂逻辑结构的敏感性和推理能力。实验结果表明，LogicCLIP在逻辑任务上远超现有模型，并在通用视觉-语言基准上保持甚至提升了性能，模型具备良好的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Logic%20Unseen%3A%20Revealing%20the%20Logical%20Blindspots%20of%20Vision-Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models"><a href="#44-Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models" class="headerlink" title="44. Probing the Representational Power of Sparse Autoencoders in Vision Models"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Probing_the_Representational_Power_of_Sparse_Autoencoders_in_Vision_Models.pdf">Probing the Representational Power of Sparse Autoencoders in Vision Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Intel Labs</span></p>
<p>本文系统评估了Sparse Autoencoders（SAEs）在视觉模型中的表征能力，方法涵盖对DINOv2等视觉编码器、扩散模型（Stable Diffusion）、以及多模态大模型（LLaVA）的SAE训练和分析。结果表明SAE特征具有语义意义，显著提升了视觉模型的可解释性、跨域泛化能力，并能实现扩散模型的语义可控生成，同时揭示了视觉与语言模态间的共享表征。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Probing%20the%20Representational%20Power%20of%20Sparse%20Autoencoders%20in%20Vision%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation"><a href="#45-FantasyTalking2-Timestep-Layer-Adaptive-Preference-Optimization-for-Audio-Driven-Portrait-Animation" class="headerlink" title="45. FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/FantasyTalking2__Timestep-Layer_Adaptive_Preference_Optimization_for_Audio-Driven_Portrait_Animation.pdf">FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AMAP, Alibaba Group</span></p>
<p>该论文提出了FantasyTalking2方法，通过引入多模态奖励模型Talking-Critic和大规模多维人类偏好数据集Talking-NSQ，实现了对音频驱动人像动画的多维度人类偏好对齐。关键技术包括多专家LoRA模块分别优化运动自然性、唇形同步和视觉质量，并通过时序-层级自适应融合机制动态调整各专家权重。实验表明，FantasyTalking2在运动自然性、唇形同步和视觉质量等指标上显著优于现有方法，全面提升生成动画的真实感和人类偏好对齐效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/FantasyTalking2%3A%20Timestep-Layer%20Adaptive%20Preference%20Optimization%20for%20Audio-Driven%20Portrait%20Animation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-Efficient-Image-to-Image-Schrodinger-Bridge-for-CT-Field-of-View-Extension"><a href="#46-Efficient-Image-to-Image-Schrodinger-Bridge-for-CT-Field-of-View-Extension" class="headerlink" title="46. Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Efficient_Image-to-Image_Schr%C3%B6dinger_Bridge_for_CT_Field_of_View_Extension.pdf">Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Cancer Center, Fudan University</span></p>
<p>该论文提出了一种基于Image-to-Image Schrödinger Bridge (I2SB)扩散模型的CT视野扩展方法，通过直接学习有限视野与扩展视野CT图像之间的映射，提升了重建的结构一致性和效率。实验结果显示，I2SB在图像质量和推理速度上均优于主流扩散模型和深度学习方法，单步推理耗时仅0.19秒，适用于临床实时应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Efficient%20Image-to-Image%20Schr%C3%B6dinger%20Bridge%20for%20CT%20Field%20of%20View%20Extension.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-UAV-VL-R1-Generalizing-Vision-Language-Models-via-Supervised-Fine-Tuning-and-Multi-Stage-GRPO-for-UAV-Visual-Reasoning"><a href="#47-UAV-VL-R1-Generalizing-Vision-Language-Models-via-Supervised-Fine-Tuning-and-Multi-Stage-GRPO-for-UAV-Visual-Reasoning" class="headerlink" title="47. UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/UAV-VL-R1__Generalizing_Vision-Language_Models_via_Supervised_Fine-Tuning_and_Multi-Stage_GRPO_for_U.pdf">UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>该论文提出UAV-VL-R1，一种专为无人机(UAV)高分辨率航拍视觉推理任务设计的轻量级视觉语言模型，通过Supervised Fine-Tuning (SFT)与多阶段Group Relative Policy Optimization (GRPO)强化学习联合训练，结合结构化奖励（格式合规与答案正确性）激励模型生成可解释的推理链。实验证明，该模型在HRVQA-VL数据集上显著提升多任务泛化能力，推理准确率超越主流大模型且具备实时低资源部署优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/UAV-VL-R1%3A%20Generalizing%20Vision-Language%20Models%20via%20Supervised%20Fine-Tuning%20and%20Multi-Stage%20GRPO%20for%20UAV%20Visual%20Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-CHARM3R-Towards-Unseen-Camera-Height-Robust-Monocular-3D-Detector"><a href="#48-CHARM3R-Towards-Unseen-Camera-Height-Robust-Monocular-3D-Detector" class="headerlink" title="48. CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CHARM3R__Towards_Unseen_Camera_Height_Robust_Monocular_3D_Detector.pdf">CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Michigan State University</span></p>
<p>本文提出了CHARM3R，一种单目3D目标检测器，旨在提升对未见摄像头高度的鲁棒性。方法上，CHARM3R分别计算回归深度和基于地面参数的深度，并在模型内部对二者进行平均，从而抵消摄像头高度变化导致的负向和正向深度误差趋势。实验表明，CHARM3R在CARLA数据集上的未见摄像头高度泛化性能提升超过45%，达到了当前最优水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CHARM3R%3A%20Towards%20Unseen%20Camera%20Height%20Robust%20Monocular%203D%20Detector.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-HistoViT-Vision-Transformer-for-Accurate-and-Scalable-Histopathological-Cancer-Diagnosis"><a href="#49-HistoViT-Vision-Transformer-for-Accurate-and-Scalable-Histopathological-Cancer-Diagnosis" class="headerlink" title="49. HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/HistoViT__Vision_Transformer_for_Accurate_and_Scalable_Histopathological_Cancer_Diagnosis.pdf">HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Embry-Riddle Aeronautical University</span></p>
<p>该论文提出了一种基于Vision Transformer（ViT）的深度学习框架HistoViT，用于多类肿瘤组织病理图像分类。方法包括：设计轻量级预处理流程，将切片病理图像标准化适配ViT输入，并在四个公开癌症数据集（乳腺、前列腺、骨、宫颈）上进行端到端微调和评估。结果显示，HistoViT在所有癌症类型上均取得95%以上准确率和99%以上AUC，显著优于现有CNN和深度学习方法，展示了Transformer架构在数字病理学中的强大泛化能力和临床应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-VFM-Guided-Semi-Supervised-Detection-Transformer-for-Source-Free-Object-Detection-in-Remote-Sensing-Images"><a href="#50-VFM-Guided-Semi-Supervised-Detection-Transformer-for-Source-Free-Object-Detection-in-Remote-Sensing-Images" class="headerlink" title="50. VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/VFM-Guided_Semi-Supervised_Detection_Transformer_for_Source-Free_Object_Detection_in_Remote_Sensing_.pdf">VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>该论文提出了一种结合视觉基础模型（VFM）和检测Transformer（VG-DETR）的半监督源无关目标检测方法，针对遥感图像领域训练崩溃和伪标签噪声问题。方法包括VFM引导的伪标签挖掘（VPM）、实例与图像级特征对齐（DVA），在仅用少量目标域标注数据下，可大幅提升遥感跨域检测鲁棒性和准确率。实验表明VG-DETR在多种遥感数据集跨域检测任务中均优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/VFM-Guided%20Semi-Supervised%20Detection%20Transformer%20for%20Source-Free%20Object%20Detection%20in%20Remote%20Sensing%20Images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-LEARN-A-Story-Driven-Layout-to-Image-Generation-Framework-for-STEM-Instruction"><a href="#51-LEARN-A-Story-Driven-Layout-to-Image-Generation-Framework-for-STEM-Instruction" class="headerlink" title="51. LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/LEARN__A_Story-Driven_Layout-to-Image_Generation_Framework_for_STEM_Instruction.pdf">LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hiroshima University</span></p>
<p>该论文提出了LEARN，一个基于布局感知扩散模型的生成框架，专为STEM教育生成教学插图。其方法包括三大模块：叙事编码与布局生成（Caption2LayoutNet）、布局到图像的扩散生成，以及以BookCover数据集为基础的知识驱动可迭代可视化，结合CLIP对比学习、结构损失和Prompt调节，确保生成图像在空间结构和语义上与教学需求高度一致。评测结果显示，LEARN在结构准确度、语义对齐和用户主观体验上均优于现有方法，有效提升了教学插图的解释性和学习效率，降低了学生的认知负担，促进了概念层级推进。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-A-Cross-Modal-Rumor-Detection-Scheme-via-Contrastive-Learning-by-Exploring-Text–Image-internal-Correlations"><a href="#52-A-Cross-Modal-Rumor-Detection-Scheme-via-Contrastive-Learning-by-Exploring-Text–Image-internal-Correlations" class="headerlink" title="52. A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text–Image internal Correlations"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/A_Cross-Modal_Rumor_Detection_Scheme_via_Contrastive_Learning_by_Exploring_Text_and_Image_internal_C.pdf">A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text–Image internal Correlations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing University of Posts and Telecommunications</span></p>
<p>本文提出了一种基于对比学习的跨模态谣言检测方法MICC，核心包括SCLIP多尺度图像-文本编码器、跨模态多尺度对齐模块及尺度感知融合网络。通过多尺度卷积和Transformer提取图像与文本语义，结合互信息最大化和信息瓶颈原则，筛选与文本强相关的图像区域并进行加权融合。实验表明该方法在Weibo和PHEME数据集上显著优于现有主流多模态谣言检测方法，表现出更高的准确率和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A%20Cross-Modal%20Rumor%20Detection%20Scheme%20via%20Contrastive%20Learning%20by%20Exploring%20Text%E2%80%93Image%20internal%20Correlations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-Denoise-then-Retrieve-Text-Conditioned-Video-Denoising-for-Video-Moment-Retrieval"><a href="#53-Denoise-then-Retrieve-Text-Conditioned-Video-Denoising-for-Video-Moment-Retrieval" class="headerlink" title="53. Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Denoise-then-Retrieve__Text-Conditioned_Video_Denoising_for_Video_Moment_Retrieval.pdf">Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southeast University</span></p>
<p>本文提出了一种 Denoise-then-Retrieve 网络（DRNet），通过文本引导的视频去噪模块（TCD）和文本重构反馈模块（TRF），动态过滤与查询无关的视频片段，生成纯净的多模态表达用于视频时刻检索。大量实验表明，该方法在 Charades-STA 和 QVHighlights 基准上显著优于现有方法，且去噪范式可无缝提升其他 VMR 模型性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-Vision-Language-Models-display-a-strong-gender-bias"><a href="#54-Vision-Language-Models-display-a-strong-gender-bias" class="headerlink" title="54. Vision-Language Models display a strong gender bias"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Vision-Language_Models_display_a_strong_gender_bias.pdf">Vision-Language Models display a strong gender bias</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Togo AI Labs</span></p>
<p>本文提出了利用CLIP风格的对比视觉-语言模型，通过计算面部图像与描述职业和活动的中性短语之间的余弦相似度，系统评估模型在嵌入空间中的性别偏见，并采用bootstrap置信区间和标签置换方法验证显著性。实验结果显示所有主流架构在不同工作类别和具体短语上均表现出显著且一致的性别偏见，且Transformer架构偏见略高于ResNet架构。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Vision-Language%20Models%20display%20a%20strong%20gender%20bias.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-Generalized-Decoupled-Learning-for-Enhancing-Open-Vocabulary-Dense-Perception"><a href="#55-Generalized-Decoupled-Learning-for-Enhancing-Open-Vocabulary-Dense-Perception" class="headerlink" title="55. Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Generalized_Decoupled_Learning_for_Enhancing_Open-Vocabulary_Dense_Perception.pdf">Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology (Shenzhen)</span></p>
<p>该论文提出DeCLIP，一种针对像素级开放词汇密集感知任务的无监督微调框架。核心技术在于将CLIP的自注意力模块解耦为“内容”和“上下文”特征，分别对齐图像裁剪CLS表征和VFM&#x2F;Diffusion模型的语义关系，实现本地判别性和空间一致性的双提升。实验表明，DeCLIP在2D&#x2F;3D检测与分割、视频分割、6D姿态估计等任务上达到了SOTA性能，证实其作为开放词汇密集感知基础模型的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Generalized%20Decoupled%20Learning%20for%20Enhancing%20Open-Vocabulary%20Dense%20Perception.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation"><a href="#56-StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation" class="headerlink" title="56. StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/StyleMM__Stylized_3D_Morphable_Face_Model_via_Text-Driven_Aligned_Image_Translation.pdf">StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KAIST</span></p>
<p>本论文提出了StyleMM框架，通过文本引导的扩散模型生成风格化人脸图像，并采用三阶段训练策略细化3D网格变形网络与纹理生成器，实现三维人脸模型的风格化。引入显式属性保持模块（EAM）和一致性位移损失（CDL），有效保证面部身份、结构对应和表达多样性。实验结果显示，StyleMM在风格化能力与身份多样性上均优于现有方法，适用于动画、游戏等领域。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/StyleMM%3A%20Stylized%203D%20Morphable%20Face%20Model%20via%20Text-Driven%20Aligned%20Image%20Translation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-E-CaTCH-Event-Centric-Cross-Modal-Attention-with-Temporal-Consistency-and-Class-Imbalance-Handling-for-Misinformation-Detection"><a href="#57-E-CaTCH-Event-Centric-Cross-Modal-Attention-with-Temporal-Consistency-and-Class-Imbalance-Handling-for-Misinformation-Detection" class="headerlink" title="57. E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/E-CaTCH__Event-Centric_Cross-Modal_Attention_with_Temporal_Consistency_and_Class-Imbalance_Handling_.pdf">E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">American University</span></p>
<p>本文提出了E-CaTCH框架，通过事件级聚类将社交媒体多模态（文本与图像）内容分组，并利用BERT与ResNet提取特征，采用自注意力与双向跨模态注意力融合，再结合软门控机制自适应整合多模态信息。模型在事件内部按时间窗口建模叙事演化，利用带有语义漂移与动量信号的LSTM捕捉时序动态，分类损失集成自适应类别加权、时间一致性正则和难例挖掘，从而显著提升了多模态虚假信息检测准确性和泛化能力。实验表明，E-CaTCH在Fakeddit、IND和COVID-19等数据集上均优于现有方法，具备良好可解释性和跨领域适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/E-CaTCH%3A%20Event-Centric%20Cross-Modal%20Attention%20with%20Temporal%20Consistency%20and%20Class-Imbalance%20Handling%20for%20Misinformation%20Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-Versatile-Video-Tokenization-with-Generative-2D-Gaussian-Splatting"><a href="#58-Versatile-Video-Tokenization-with-Generative-2D-Gaussian-Splatting" class="headerlink" title="58. Versatile Video Tokenization with Generative 2D Gaussian Splatting"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Versatile_Video_Tokenization_with_Generative_2D_Gaussian_Splatting.pdf">Versatile Video Tokenization with Generative 2D Gaussian Splatting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Newcastle</span></p>
<p>本文提出了一种新的视频离散化方法Gaussian Video Transformer（GVT），利用生成式2D Gaussian Splatting（2DGS）策略，通过Spatio-Temporal Gaussian Embedding (STGE)模块实现对视频的自适应空间建模，并通过Gaussian Set Partitioning (GSP)策略区分静态和动态内容以提升时序压缩能力。实验表明，GVT在视频重建、动作识别和压缩任务上均取得了领先或有竞争力的表现，有效提升了视频表示的紧凑性和语义保真度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Versatile%20Video%20Tokenization%20with%20Generative%202D%20Gaussian%20Splatting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-Exploring-the-Tradeoff-Between-Diversity-and-Discrimination-for-Continuous-Category-Discovery"><a href="#59-Exploring-the-Tradeoff-Between-Diversity-and-Discrimination-for-Continuous-Category-Discovery" class="headerlink" title="59. Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Exploring_the_Tradeoff_Between_Diversity_and_Discrimination_for_Continuous_Category_Discovery.pdf">Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ocean University of China</span></p>
<p>本文提出了一种面向连续类别发现（CCD）任务的新方法IDOD，包括独立多样性增强、联合新颖性发现和正交判别三个模块。通过独立训练骨干网络和投影器、联合多阶段新类别发现以及正交原型分配，有效平衡了特征多样性与判别性的矛盾，减少累积误差并降低存储消耗。大量实验表明，IDOD在多个细粒度视觉数据集上优于当前主流方法，在新类别发现和防遗忘方面表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Exploring%20the%20Tradeoff%20Between%20Diversity%20and%20Discrimination%20for%20Continuous%20Category%20Discovery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-Semi-supervised-Image-Dehazing-via-Expectation-Maximization-and-Bidirectional-Brownian-Bridge-Diffusion-Models"><a href="#60-Semi-supervised-Image-Dehazing-via-Expectation-Maximization-and-Bidirectional-Brownian-Bridge-Diffusion-Models" class="headerlink" title="60. Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Semi-supervised_Image_Dehazing_via_Expectation-Maximization_and_Bidirectional_Brownian_Bridge_Diffus.pdf">Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">China University of Mining and Technology</span></p>
<p>该论文提出了EM-B3DM，一种结合EM算法与双向布朗桥扩散模型的半监督图像去雾方法，包括两阶段训练：首先利用有限配对数据通过EM算法解耦联合分布并用布朗桥扩散模型建模，随后在大规模未配对数据上进行半监督训练以提升泛化能力。通过引入残差差分卷积块增强细节表达，实验结果显示EM-B3DM在合成与真实数据集上均优于现有方法，在有限配对数据下实现更优质的去雾效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Semi-supervised%20Image%20Dehazing%20via%20Expectation-Maximization%20and%20Bidirectional%20Brownian%20Bridge%20Diffusion%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-Residual-based-Efficient-Bidirectional-Diffusion-Model-for-Image-Dehazing-and-Haze-Generation"><a href="#61-Residual-based-Efficient-Bidirectional-Diffusion-Model-for-Image-Dehazing-and-Haze-Generation" class="headerlink" title="61. Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Residual-based_Efficient_Bidirectional_Diffusion_Model_for_Image_Dehazing_and_Haze_Generation.pdf">Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">China University of Mining and Technology</span></p>
<p>本文提出了一种残差驱动的高效双向扩散模型（RBDM），通过构建双Markov链和残差迁移，实现了图像去雾与雾生成的双向转换，采用基于图像块的统一得分函数提升小数据集表现并降低计算成本。实验结果表明，RBDM在合成和真实数据集上对比主流方法表现更优，能高效去除雾霾并生成自然真实的雾效图像。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Residual-based%20Efficient%20Bidirectional%20Diffusion%20Model%20for%20Image%20Dehazing%20and%20Haze%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="62-Is-ChatGPT-5-Ready-for-Mammogram-VQA"><a href="#62-Is-ChatGPT-5-Ready-for-Mammogram-VQA" class="headerlink" title="62. Is ChatGPT-5 Ready for Mammogram VQA?"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Is_ChatGPT-5_Ready_for_Mammogram_VQA_.pdf">Is ChatGPT-5 Ready for Mammogram VQA?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Emory University</span></p>
<p>本文系统评估了ChatGPT-5及GPT-4o在四个公共乳腺X光片数据集(EMBED、InBreast、CMMD、CBIS-DDSM)上的视觉问答任务表现，包括BI-RADS分级、异常检测及恶性肿瘤分类，采用零样本多模态推理和标准化链式思维提示流程。结果显示，GPT-5在所有任务中超越前代GPT和轻量版本，但仍显著落后于人类专家及领域专用细调模型，尤其在敏感性和特异性上存在较大差距。结论认为，通用大模型虽有进步，但临床应用仍需领域适配和优化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Is%20ChatGPT-5%20Ready%20for%20Mammogram%20VQA%3F.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding"><a href="#63-Controlling-Multimodal-LLMs-via-Reward-guided-Decoding" class="headerlink" title="63. Controlling Multimodal LLMs via Reward-guided Decoding"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Controlling_Multimodal_LLMs_via_Reward-guided_Decoding.pdf">Controlling Multimodal LLMs via Reward-guided Decoding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mila - Quebec AI Institute</span></p>
<p>该论文提出了多模态大语言模型（MLLMs）推理阶段的奖励引导解码方法MRGD，通过构建用于视觉锚定的多模态奖励模型（分别针对目标精准和召回）引导输出生成，用户可动态调节两种奖励函数的权重，实现精确度和召回率以及计算资源消耗的可控平衡。实验表明，MRGD方法在标准对象幻觉基准上显著提升了MLLM推理的可控性，并优于现有幻觉缓解方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Controlling%20Multimodal%20LLMs%20via%20Reward-guided%20Decoding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-CRYPTOSCOPE-Utilizing-Large-Language-Models-for-Automated-Cryptographic-Logic-Vulnerability-Detection"><a href="#64-CRYPTOSCOPE-Utilizing-Large-Language-Models-for-Automated-Cryptographic-Logic-Vulnerability-Detection" class="headerlink" title="64. CRYPTOSCOPE: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CryptoScope__Utilizing_Large_Language_Models_for_Automated_Cryptographic_Logic_Vulnerability_Detecti.pdf">CRYPTOSCOPE: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sichuan University</span></p>
<p>该论文提出了CRYPTOSCOPE框架，利用大语言模型结合Chain-of-Thought（CoT）推理和检索增强生成（RAG），通过对多源加密知识库的检索自动检测代码中的加密逻辑漏洞。实验证明，该方法在LLM-CLVA基准上显著提升了各类LLM的漏洞检测能力，并在20个开源项目中发现了9个未公开的安全漏洞，验证了其实用性与通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CRYPTOSCOPE%3A%20Utilizing%20Large%20Language%20Models%20for%20Automated%20Cryptographic%20Logic%20Vulnerability%20Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-Aware-First-Think-Less-Dynamic-Boundary-Self-Awareness-Drives-Extreme-Reasoning-Efficiency-in-Large-Language-Models"><a href="#65-Aware-First-Think-Less-Dynamic-Boundary-Self-Awareness-Drives-Extreme-Reasoning-Efficiency-in-Large-Language-Models" class="headerlink" title="65. Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Aware_First,_Think_Less__Dynamic_Boundary_Self-Awareness_Drives_Extreme_Reasoning_Efficiency_in_Larg.pdf">Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harbin Institute of Technology</span></p>
<p>本文提出了动态推理边界自觉框架（DR. SAF），通过边界自觉对齐、适应性长度管理和边界保持机制，使大语言模型能够根据自身能力和任务难度动态调整推理深度。实验表明，该方法可在保持高精度的前提下将推理响应token数量减少49.27%，提升token效率6.59倍，训练时间缩短5倍，极端压缩场景下还能提升准确率16%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Aware%20First%2C%20Think%20Less%3A%20Dynamic%20Boundary%20Self-Awareness%20Drives%20Extreme%20Reasoning%20Efficiency%20in%20Large%20Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-Causality-Matters-How-Temporal-Information-Emerges-in-Video-Language-Models"><a href="#66-Causality-Matters-How-Temporal-Information-Emerges-in-Video-Language-Models" class="headerlink" title="66. Causality Matters: How Temporal Information Emerges in Video Language Models"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Causality_Matters__How_Temporal_Information_Emerges_in_Video_Language_Models.pdf">Causality Matters: How Temporal Information Emerges in Video Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>本文系统分析了当前视频语言模型（VideoLMs）中的时序理解机制，发现时序信息的获得主要依赖于因果注意力机制中顺序敏感的结构，而非传统认为的显式位置编码（PEs）。通过一系列消融实验，作者揭示了时序推理是通过跨帧注意力逐步合成、在最后一帧聚合并注入查询token的因果通路中涌现的，并据此提出了分阶段稀疏注意力和时序提前终止策略，实现了推理效率提升且无性能损失。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Causality%20Matters%3A%20How%20Temporal%20Information%20Emerges%20in%20Video%20Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-ADMIRE-BayesOpt-Accelerated-Data-Mixture-Re-weighting-for-Language-Models-with-Bayesian-Optimization"><a href="#67-ADMIRE-BayesOpt-Accelerated-Data-Mixture-Re-weighting-for-Language-Models-with-Bayesian-Optimization" class="headerlink" title="67. ADMIRE-BayesOpt: Accelerated Data Mixture Re-weighting for Language Models with Bayesian Optimization"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/ADMIRE-BayesOpt__Accelerated_Data_MIxture_RE-weighting_for_Language_Models_with_Bayesian_Optimizatio.pdf">ADMIRE-BayesOpt: Accelerated Data Mixture Re-weighting for Language Models with Bayesian Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Thomson Reuters Foundational Research</span></p>
<p>本文提出ADMIRE-BayesOpt方法，将大语言模型训练中的数据混合权重选择视为黑盒超参数优化问题，并采用贝叶斯优化（包括多保真贝叶斯优化）系统性探索不同模型规模和数据混合的最优方案。实验表明，该方法在多个主流数据集和模型规模下，显著加速了最优数据混合的发现过程（最高加速超500%），并在准确性和效率上均优于现有方法，同时公开了大规模训练与评测数据集，降低了研究门槛。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ADMIRE-BayesOpt%3A%20Accelerated%20Data%20Mixture%20Re-weighting%20for%20Language%20Models%20with%20Bayesian%20Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-Inclusion-Arena-An-Open-Platform-for-Evaluating-Large-Foundation-Models-with-Real-World-Apps"><a href="#68-Inclusion-Arena-An-Open-Platform-for-Evaluating-Large-Foundation-Models-with-Real-World-Apps" class="headerlink" title="68. Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Inclusion_Arena__An_Open_Platform_for_Evaluating_Large_Foundation_Models_with_Real-World_Apps.pdf">Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Inclusion AI</span></p>
<p>该论文提出了Inclusion Arena，一个通过真实应用场景收集人类反馈、动态评估并排名大模型（LLMs和MLLMs）的开放平台。其核心方法为：在AI应用中嵌入模型对比机制，采用Bradley-Terry模型进行排序，并创新性地引入Placement Matches（冷启动机制）和Proximity Sampling（智能近邻采样），实现高效、稳定且抗恶意干扰的模型排名。实验和模拟表明，平台能比传统众包数据获得更可信的模型排序，推动大模型面向实际应用的优化和发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inclusion%20Arena%3A%20An%20Open%20Platform%20for%20Evaluating%20Large%20Foundation%20Models%20with%20Real-World%20Apps.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-MM-R1-Unleashing-the-Power-of-Unified-Multimodal-Large-Language-Models-for-Personalized-Image-Generation"><a href="#69-MM-R1-Unleashing-the-Power-of-Unified-Multimodal-Large-Language-Models-for-Personalized-Image-Generation" class="headerlink" title="69. MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MM-R1__Unleashing_the_Power_of_Unified_Multimodal_Large_Language_Models_for_Personalized_Image_Gener.pdf">MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>该论文提出了MM-R1框架，通过跨模态Chain-of-Thought（X-CoT）推理与强化学习（GRPO），使统一多模态大语言模型具备个性化图像生成能力。方法包括先理解用户图像及上下文，抽取主体特征，再结合用户提示生成图像，并通过多重奖励优化生成结果。实验表明，该方法无需针对特定主体微调，在零样本条件下实现高主体一致性和文本对齐，性能优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MM-R1%3A%20Unleashing%20the%20Power%20of%20Unified%20Multimodal%20Large%20Language%20Models%20for%20Personalized%20Image%20Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-Trustworthy-AI-Psychotherapy-Multi-Agent-LLM-Workflow-for-Counseling-and-Explainable-Mental-Disorder-Diagnosis"><a href="#70-Trustworthy-AI-Psychotherapy-Multi-Agent-LLM-Workflow-for-Counseling-and-Explainable-Mental-Disorder-Diagnosis" class="headerlink" title="70. Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Trustworthy_AI_Psychotherapy__Multi-Agent_LLM_Workflow_for_Counseling_and_Explainable_Mental_Disorde.pdf">Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Vrije Universiteit Amsterdam</span></p>
<p>本文提出DSM5AgentFlow，一种多智能体大语言模型（LLM）工作流，自动生成并管理DSM-5一级诊断问卷，通过三类LLM代理（治疗师、患者、诊断师）模拟真实心理咨询对话。结论显示该系统可生成连贯对话并提供基于DSM-5标准的透明、可解释的诊断，增强AI心理健康工具的可信度与可审计性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Trustworthy%20AI%20Psychotherapy%3A%20Multi-Agent%20LLM%20Workflow%20for%20Counseling%20and%20Explainable%20Mental%20Disorder%20Diagnosis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-Rationalizing-Transformer-Predictions-via-End-To-End-Differentiable-Self-Training"><a href="#71-Rationalizing-Transformer-Predictions-via-End-To-End-Differentiable-Self-Training" class="headerlink" title="71. Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Rationalizing_Transformer_Predictions_via_End-To-End_Differentiable_Self-Training.pdf">Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bielefeld University</span></p>
<p>本文提出了一种端到端可微分的自训练方法，用于训练能够同时进行分类和生成类相关解释（rationales）的Transformer模型。该方法通过单模型结构，避免了传统多玩家博弈的不稳定性和采样问题，实现高效、稳定的训练，并在无需显式人工监督的情况下，显著提升与人类标注的合理性对齐度和解释的忠实性，在两个NLP基准任务中达到SOTA表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-Inspire-or-Predict-Exploring-New-Paradigms-in-Assisting-Classical-Planners-with-Large-Language-Models"><a href="#72-Inspire-or-Predict-Exploring-New-Paradigms-in-Assisting-Classical-Planners-with-Large-Language-Models" class="headerlink" title="72. Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Inspire_or_Predict__Exploring_New_Paradigms_in_Assisting_Classical_Planners_with_Large_Language_Mode.pdf">Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Guizhou University</span></p>
<p>本文提出了一种结合大规模语言模型（LLMs）与问题分解的规划器，首先将大规模规划问题分解为多个子任务，并提出两种LLM辅助范式：LLM4Inspire（基于通用知识的启发式引导）和LLM4Predict（注入领域知识预测中间状态）。结果显示，LLM4Predict方法在大多数领域显著提升了搜索空间划分和规划成功率，验证了领域知识不可替代的重要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Inspire%20or%20Predict%3F%20Exploring%20New%20Paradigms%20in%20Assisting%20Classical%20Planners%20with%20Large%20Language%20Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-Automated-Building-Heritage-Assessment-Using-Street-Level-Imagery"><a href="#73-Automated-Building-Heritage-Assessment-Using-Street-Level-Imagery" class="headerlink" title="73. Automated Building Heritage Assessment Using Street-Level Imagery"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Automated_Building_Heritage_Assessment_Using_Street-Level_Imagery.pdf">Automated Building Heritage Assessment Using Street-Level Imagery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Forschungszentrum Jülich GmbH</span></p>
<p>该论文提出了一种结合GPT大语言模型与街景图像分析的方法，用于自动评估建筑的文化遗产价值。具体流程包括收集瑞典斯德哥尔摩多户住宅和非住宅建筑的街景图像，利用GPT提取可见特征，再用机器学习模型（如XGBoost）对遗产价值进行分类，最终验证结果与专家评估一致性。结论表明，集成GPT提取的特征后，模型的宏F1分数提升5%，在无建筑年份信息时性能提升25%，显示该方法能为大规模能源改造与遗产保护提供高效辅助，但仍需专家参与以确保决策全面可靠。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Automated%20Building%20Heritage%20Assessment%20Using%20Street-Level%20Imagery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-Reference-Points-in-LLM-Sentiment-Analysis-The-Role-of-Structured-Context"><a href="#74-Reference-Points-in-LLM-Sentiment-Analysis-The-Role-of-Structured-Context" class="headerlink" title="74. Reference Points in LLM Sentiment Analysis: The Role of Structured Context"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Reference_Points_in_LLM_Sentiment_Analysis__The_Role_of_Structured_Context.pdf">Reference Points in LLM Sentiment Analysis: The Role of Structured Context</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Meijo University</span></p>
<p>本文提出在LLM情感分析任务中引入用户均值、商户均值等结构化上下文信息（如JSON格式）作为参考点，系统比较了自然语言和结构化提示对小型LLM（Llama-3.2-3B-Instruct）的影响，并在Yelp餐饮和夜生活数据集上验证。结果显示，JSON结构化信息显著提升情感分类Macro-F1分数（提升1.6%-4%）并降低RMSE（降幅9.1%-16%），且性能提升来源于真实的上下文推理能力而非标签代理，该方法无需微调即可部署于边缘设备。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Reference%20Points%20in%20LLM%20Sentiment%20Analysis%3A%20The%20Role%20of%20Structured%20Context.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-AIM-Bench-Evaluating-Decision-making-Biases-of-Agentic-LLM-as-Inventory-Manager"><a href="#75-AIM-Bench-Evaluating-Decision-making-Biases-of-Agentic-LLM-as-Inventory-Manager" class="headerlink" title="75. AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/AIM-Bench__Evaluating_Decision-making_Biases_of_Agentic_LLM_as_Inventory_Manager.pdf">AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本论文提出AIM-Bench基准，系统性评估LLM代理在不确定供应链环境中的库存决策与人类常见决策偏差（如锚定、需求追逐、牛鞭效应等），并在多种现实库存管理任务和环境下对主流开源与闭源LLM性能进行定量和过程分析。结果显示，LLM代理普遍存在人类类似的决策偏差，且适当的认知反思和信息共享策略可有效缓解锚定和牛鞭效应，强调部署LLM于库存管理需关注偏差与模型选择。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AIM-Bench%3A%20Evaluating%20Decision-making%20Biases%20of%20Agentic%20LLM%20as%20Inventory%20Manager.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-ON-POLICY-RL-MEETS-OFF-POLICY-EXPERTS-HARMONIZING-SUPERVISED-FINE-TUNING-AND-REINFORCEMENT-LEARNING-VIA-DYNAMIC-WEIGHTING"><a href="#76-ON-POLICY-RL-MEETS-OFF-POLICY-EXPERTS-HARMONIZING-SUPERVISED-FINE-TUNING-AND-REINFORCEMENT-LEARNING-VIA-DYNAMIC-WEIGHTING" class="headerlink" title="76. ON-POLICY RL MEETS OFF-POLICY EXPERTS: HARMONIZING SUPERVISED FINE-TUNING AND REINFORCEMENT LEARNING VIA DYNAMIC WEIGHTING"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/On-Policy_RL_Meets_Off-Policy_Experts__Harmonizing_Supervised_Fine-Tuning_and_Reinforcement_Learning.pdf">ON-POLICY RL MEETS OFF-POLICY EXPERTS: HARMONIZING SUPERVISED FINE-TUNING AND REINFORCEMENT LEARNING VIA DYNAMIC WEIGHTING</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Alibaba Group</span></p>
<p>本文提出CHORD框架，通过动态加权机制，将监督微调（SFT）作为辅助目标整合到大语言模型的在策略强化学习（RL）过程中，包含全局系数调节和逐token权重函数精细控制专家离策略数据的影响。实验显示，CHORD能在保持探索能力的同时，稳定有效地融合专家知识，显著优于传统SFT-then-RL范式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ON-POLICY%20RL%20MEETS%20OFF-POLICY%20EXPERTS%3A%20HARMONIZING%20SUPERVISED%20FINE-TUNING%20AND%20REINFORCEMENT%20LEARNING%20VIA%20DYNAMIC%20WEIGHTING.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs"><a href="#77-When-Punctuation-Matters-A-Large-Scale-Comparison-of-Prompt-Robustness-Methods-for-LLMs" class="headerlink" title="77. When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/When_Punctuation_Matters__A_Large-Scale_Comparison_of_Prompt_Robustness_Methods_for_LLMs.pdf">When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AIRI</span></p>
<p>本文系统性评估了五种提升大语言模型（LLM）在不同提示格式下鲁棒性的方法，包括Batch Calibration、Template Ensembles、Sensitivity-Aware Decoding和LoRA等，在Llama、Qwen、Gemma等8种模型的52项任务上进行统一对比测试。结果表明，Batch Calibration在无分布漂移时表现最佳，但对类别不均衡极为敏感，规模较大的前沿模型更鲁棒，但仍存在格式敏感性，采用Template Ensembles多数投票能有效缓解这一问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/When%20Punctuation%20Matters%3A%20A%20Large-Scale%20Comparison%20of%20Prompt%20Robustness%20Methods%20for%20LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-ETTRL-Balancing-Exploration-and-Exploitation-in-LLM-Test-Time-Reinforcement-Learning-Via-Entropy-Mechanism"><a href="#78-ETTRL-Balancing-Exploration-and-Exploitation-in-LLM-Test-Time-Reinforcement-Learning-Via-Entropy-Mechanism" class="headerlink" title="78. ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/ETTRL__Balancing_Exploration_and_Exploitation_in_LLM_Test-Time_Reinforcement_Learning_Via_Entropy_Me.pdf">ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou Technology</span></p>
<p>本文提出了一种面向大语言模型测试时强化学习（TTRL）的熵机制方法ETTRL，包括熵分叉树多数投票采样（ETMR）与基于熵的优势重塑（EAR），分别在采样和奖励估计阶段平衡探索与利用。实验表明，该方法在数学推理等无监督任务上显著提升推理准确率，同时将推理计算成本降低至60%，有效缓解了TTRL的高推理开销和早期过度自信等问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ETTRL%3A%20Balancing%20Exploration%20and%20Exploitation%20in%20LLM%20Test-Time%20Reinforcement%20Learning%20Via%20Entropy%20Mechanism.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-SGSimEval-A-Comprehensive-Multifaceted-and-Similarity-Enhanced-Benchmark-for-Automatic-Survey-Generation-Systems"><a href="#79-SGSimEval-A-Comprehensive-Multifaceted-and-Similarity-Enhanced-Benchmark-for-Automatic-Survey-Generation-Systems" class="headerlink" title="79. SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/SGSimEval__A_Comprehensive_Multifaceted_and_Similarity-Enhanced_Benchmark_for_Automatic_Survey_Gener.pdf">SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong Polytechnic University</span></p>
<p>SGSimEval提出了一套综合性、多维度的自动综述生成（ASG）系统评测基准，涵盖了结构、内容和参考文献等方面，通过融合LLM打分与语义相似度等量化指标，构建了兼顾人类偏好和内容质量的评价体系。实验表明，CS领域专用系统在所有维度上优于通用系统，尤其在结构生成上超过人类，但在参考文献生成上仍有较大提升空间。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/SGSimEval%3A%20A%20Comprehensive%20Multifaceted%20and%20Similarity-Enhanced%20Benchmark%20for%20Automatic%20Survey%20Generation%20Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-CSGO-Generalized-Optimization-for-Cold-Start-in-Wireless-Collaborative-Edge-LLM-Systems"><a href="#80-CSGO-Generalized-Optimization-for-Cold-Start-in-Wireless-Collaborative-Edge-LLM-Systems" class="headerlink" title="80. CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/CSGO__Generalized_Optimization_for_Cold_Start_in_Wireless_Collaborative_Edge_LLM_Systems.pdf">CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了一种面向无线协同边缘大语言模型（LLM）系统的冷启动延迟优化框架。该方法通过动态编程算法，基于设备和模型参数动态调整模型分层和分配，在推理流程中实现模型加载与计算、通信的高效重叠，大幅减少了冷启动带来的总延迟。实验结果表明，该算法相比常规策略可显著降低用户感知的推理延迟，并有效提升流水线效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/CSGO%3A%20Generalized%20Optimization%20for%20Cold%20Start%20in%20Wireless%20Collaborative%20Edge%20LLM%20Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-Hallucination-in-LLM-Based-Code-Generation-An-Automotive-Case-Study"><a href="#81-Hallucination-in-LLM-Based-Code-Generation-An-Automotive-Case-Study" class="headerlink" title="81. Hallucination in LLM-Based Code Generation: An Automotive Case Study"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Hallucination_in_LLM-Based_Code_Generation__An_Automotive_Case_Study.pdf">Hallucination in LLM-Based Code Generation: An Automotive Case Study</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University of Munich</span></p>
<p>本文针对大语言模型（LLM）在汽车领域代码生成中的幻觉问题进行了系统研究，通过设计不同复杂度的提示（baseline、信号增强、模板增强）并采用迭代修正策略，评估了多种主流代码生成LLM的表现。结果显示，只有在上下文最丰富的模板增强提示下，部分主流模型（如GPT-4o和GPT-4.1）才能生成正确代码，其他情况下易出现语法错误、无效引用和API知识冲突等幻觉，提示结构和模型能力对幻觉缓解至关重要。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-ORFuzz-Fuzzing-the-Other-Side-of-LLM-Safety-–-Testing-Over-Refusal"><a href="#82-ORFuzz-Fuzzing-the-Other-Side-of-LLM-Safety-–-Testing-Over-Refusal" class="headerlink" title="82. ORFuzz: Fuzzing the &quot;Other Side&quot; of LLM Safety – Testing Over-Refusal"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/ORFuzz__Fuzzing_the__Other_Side__of_LLM_Safety_--_Testing_Over-Refusal.pdf">ORFuzz: Fuzzing the &quot;Other Side&quot; of LLM Safety – Testing Over-Refusal</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了ORFUZZ——首个系统检测大语言模型（LLM）过度拒答（over-refusal）行为的进化式模糊测试框架。ORFUZZ融合了基于安全类别的种子选择、由推理LLM驱动的自适应变异器优化，以及与人类感知一致的判别模型OR-JUDGE，实现了多样且高效的过度拒答测试用例自动生成。实验表明，ORFUZZ在检测有效过度拒答实例方面检测率（6.98%）远超现有方法，并据此构建了高效的基准数据集ORFUZZSET（1855条样本，10个主流LLM平均触发率63.56%），为LLM安全性评测提供了重要工具和社区资源。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/ORFuzz%3A%20Fuzzing%20the%20%5C%22Other%20Side%5C%22%20of%20LLM%20Safety%20%E2%80%93%20Testing%20Over-Refusal.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-MONACO-More-Natural-and-Complex-Questions-for-Reasoning-Across-Dozens-of-Documents"><a href="#83-MONACO-More-Natural-and-Complex-Questions-for-Reasoning-Across-Dozens-of-Documents" class="headerlink" title="83. MONACO: More Natural and Complex Questions for Reasoning Across Dozens of Documents"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/MoNaCo__More_Natural_and_Complex_Questions_for_Reasoning_Across_Dozens_of_Documents.pdf">MONACO: More Natural and Complex Questions for Reasoning Across Dozens of Documents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Pennsylvania</span></p>
<p>该论文提出了MONACO基准数据集，包含1315个复杂且自然的问题，要求跨越数十乃至上百份文档进行推理与信息整合。作者设计了基于多步分解的人类标注流水线，收集问题、推理链与中间答案，并在多种主流LLM上评估表现，实验表明当前最前沿大模型在该数据集上的F1最高仅为61.2%，表现受限于召回率低和幻觉问题。结论认为MONACO能够有效暴露和衡量LLM在复杂、信息密集型任务上的推理瓶颈，是未来提升模型能力的重要测试基准。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/MONACO%3A%20More%20Natural%20and%20Complex%20Questions%20for%20Reasoning%20Across%20Dozens%20of%20Documents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-Dynamic-Quality-Latency-Aware-Routing-for-LLM-Inference-in-Wireless-Edge-Device-Networks"><a href="#84-Dynamic-Quality-Latency-Aware-Routing-for-LLM-Inference-in-Wireless-Edge-Device-Networks" class="headerlink" title="84. Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Dynamic_Quality-Latency_Aware_Routing_for_LLM_Inference_in_Wireless_Edge-Device_Networks.pdf">Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了一个动态质量-延迟感知的路由框架，用于无线边缘设备网络下协作式大语言模型（LLM）推理。方法上，框架结合了基于BERT的语义难度预测与精细化的延迟成本模型，支持单轮及多轮对话，并首次定量建模了模型切换时KV-cache管理带来的开销。实验结果表明，该方法能在不降低推理质量的前提下，将平均响应延迟降低5-15%，边缘大模型调用率降低10-20%。结论是该框架为无线边缘设备上的大模型协作推理提供了高效实用的解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Dynamic%20Quality-Latency%20Aware%20Routing%20for%20LLM%20Inference%20in%20Wireless%20Edge-Device%20Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-LETToT-Label-Free-Evaluation-of-Large-Language-Models-On-Tourism-Using-Expert-Tree-of-Thought"><a href="#85-LETToT-Label-Free-Evaluation-of-Large-Language-Models-On-Tourism-Using-Expert-Tree-of-Thought" class="headerlink" title="85. LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/LETToT__Label-Free_Evaluation_of_Large_Language_Models_On_Tourism_Using_Expert_Tree-of-Thought.pdf">LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Warwick</span></p>
<p>本论文提出了LETToT框架，通过专家树状思维（Expert Tree-of-Thought, ToT）构建无监督、无标签的旅游领域大语言模型（LLM）评测体系。方法包括对旅游问答进行三阶段分类和11主题归纳，设计多维内容质量指标，结合AHP加权和高效评分公式，实现了领域专家知识驱动的评测流程。实验表明，LETToT显著提升了模型响应质量（提升4.99-14.15%），并发现小参数推理模型可缩小与大模型的性能差距。结论：LETToT为旅游等专业领域LLM评测提供了可扩展、可复制的无标签解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/LETToT%3A%20Label-Free%20Evaluation%20of%20Large%20Language%20Models%20On%20Tourism%20Using%20Expert%20Tree-of-Thought.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-Group-Fairness-Meets-the-Black-Box-Enabling-Fair-Algorithms-on-Closed-LLMs-via-Post-Processing"><a href="#86-Group-Fairness-Meets-the-Black-Box-Enabling-Fair-Algorithms-on-Closed-LLMs-via-Post-Processing" class="headerlink" title="86. Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Group_Fairness_Meets_the_Black_Box__Enabling_Fair_Algorithms_on_Closed_LLMs_via_Post-Processing.pdf">Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Urbana-Champaign</span></p>
<p>该论文提出了一种针对闭源大语言模型（LLM）的公平分类框架：通过精心设计的提示从模型输出概率中提取标签和敏感属性相关特征，并在此基础上应用公平算法（如Reductions、MinDiff、LinearPost）训练轻量级公平分类器。实验证明，该方法在低数据资源环境下表现优异，精度和公平性的权衡优于直接用LLM嵌入或原始特征训练的分类器，且数据效率高。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Group%20Fairness%20Meets%20the%20Black%20Box%3A%20Enabling%20Fair%20Algorithms%20on%20Closed%20LLMs%20via%20Post-Processing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-Generating-Dialogues-from-Egocentric-Instructional-Videos-for-Task-Assistance-Dataset-Method-and-Benchmark"><a href="#87-Generating-Dialogues-from-Egocentric-Instructional-Videos-for-Task-Assistance-Dataset-Method-and-Benchmark" class="headerlink" title="87. Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Generating_Dialogues_from_Egocentric_Instructional_Videos_for_Task_Assistance__Dataset,_Method_and_B.pdf">Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Google</span></p>
<p>该论文提出一种利用大语言模型，将单人叙述的教学视频自动转换为用户-专家双人对话，流程包括指令提取、对话生成和视频片段定位。方法无需人工标注，生成HowToDIV大规模多模态任务辅助对话数据集，并在Gemma-3模型上建立基线，验证了方法的有效性和扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Generating%20Dialogues%20from%20Egocentric%20Instructional%20Videos%20for%20Task%20Assistance%3A%20Dataset%2C%20Method%20and%20Benchmark.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-Role-Augmented-Intent-Driven-Generative-Search-Engine-Optimization"><a href="#88-Role-Augmented-Intent-Driven-Generative-Search-Engine-Optimization" class="headerlink" title="88. Role-Augmented Intent-Driven Generative Search Engine Optimization"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Role-Augmented_Intent-Driven_Generative_Search_Engine_Optimization.pdf">Role-Augmented Intent-Driven Generative Search Engine Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本文提出了Role-Augmented Intent-Driven Generative Search Engine Optimization (RAID G-SEO)，一种针对生成式搜索引擎优化的结构化方法，通过四阶段流程（内容摘要、意图推断与细化、步骤规划、内容重写）及多角色深度反思机制，建模用户隐含搜索意图并引导内容优化。实验结果表明，该方法在主观与客观内容可见性提升上均优于传统与基线方法，意图建模显著提升内容在生成式搜索引擎中的曝光度，但仍需在意图泛化与精度间平衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Role-Augmented%20Intent-Driven%20Generative%20Search%20Engine%20Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-AlphaAgents-Large-Language-Model-based-Multi-Agents-for-Equity-Portfolio-Constructions"><a href="#89-AlphaAgents-Large-Language-Model-based-Multi-Agents-for-Equity-Portfolio-Constructions" class="headerlink" title="89. AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/AlphaAgents__Large_Language_Model_based_Multi-Agents_for_Equity_Portfolio_Constructions.pdf">AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">BlackRock, Inc.</span></p>
<p>该论文提出了一种基于大型语言模型（LLM）的多智能体系统AlphaAgents，用于股票分析和组合构建。系统包含基本面、情绪和估值三类特化智能体，通过角色提示与专用工具协作分析，采用多智能体辩论机制达成投资决策共识，并通过回测实验证明多智能体协作能提升投资决策质量、缓解人类认知偏差、提升风险调整后收益。结论认为多智能体LLM系统在股票选取和投资决策中优于单一智能体，具备较高可解释性和良好扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/AlphaAgents%3A%20Large%20Language%20Model%20based%20Multi-Agents%20for%20Equity%20Portfolio%20Constructions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="90-Pretrained-Conformers-for-Audio-Fingerprinting-and-Retrieval"><a href="#90-Pretrained-Conformers-for-Audio-Fingerprinting-and-Retrieval" class="headerlink" title="90. Pretrained Conformers for Audio Fingerprinting and Retrieval"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Pretrained_Conformers_for_Audio_Fingerprinting_and_Retrieval.pdf">Pretrained Conformers for Audio Fingerprinting and Retrieval</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Infobip</span></p>
<p>该论文提出了基于自监督对比学习的Conformer编码器，用于生成小段音频的独特嵌入，实现鲁棒的音频指纹及检索。实验结果表明，模型在3秒音频片段下对时间错位、噪声、混响等失真具有极强鲁棒性，达到音频检索任务的最新性能，且模型参数规模灵活，适用于不同应用场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Pretrained%20Conformers%20for%20Audio%20Fingerprinting%20and%20Retrieval.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-A-Comprehensive-Perspective-on-Explainable-AI-across-the-Machine-Learning-Workflow"><a href="#91-A-Comprehensive-Perspective-on-Explainable-AI-across-the-Machine-Learning-Workflow" class="headerlink" title="91. A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/A_Comprehensive_Perspective_on_Explainable_AI_across_the_Machine_Learning_Workflow.pdf">A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">JADBio Gnosis DA S.A.</span></p>
<p>该论文提出了Holistic Explainable Artificial Intelligence (HXAI) 框架，将可解释性嵌入到机器学习工作流的每个阶段，并通过六大组成部分（数据、分析设置、学习过程、模型输出、模型质量、交流渠道）形成统一分类体系，结合用户需求和多学科理论。结论显示，HXAI能够提升AI系统的透明度、信任度和用户适应性，填补现有XAI工具链的覆盖空白，并推动负责任AI的部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A%20Comprehensive%20Perspective%20on%20Explainable%20AI%20across%20the%20Machine%20Learning%20Workflow.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-Physics-Informed-Diffusion-Models-for-Unsupervised-Anomaly-Detection-in-Multivariate-Time-Series"><a href="#92-Physics-Informed-Diffusion-Models-for-Unsupervised-Anomaly-Detection-in-Multivariate-Time-Series" class="headerlink" title="92. Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Physics-Informed_Diffusion_Models_for_Unsupervised_Anomaly_Detection_in_Multivariate_Time_Series.pdf">Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fraunhofer IOSB-INA</span></p>
<p>本文提出了一种基于物理约束扩散模型的无监督多变量时间序列异常检测方法，通过在扩散模型训练中引入加权物理信息损失（PINN），结合静态权重调度以优化物理损失，提升模型对物理相关数据的分布拟合能力。实验表明，该方法在多组合成和真实数据集上提升了异常检测F1分数、数据多样性和对数似然，优于数据驱动和现有物理约束方法，物理信息的引入对异常检测有显著增益。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Physics-Informed%20Diffusion%20Models%20for%20Unsupervised%20Anomaly%20Detection%20in%20Multivariate%20Time%20Series.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-A-Remedy-for-Over-Squashing-in-Graph-Learning-via-Forman-Ricci-Curvature-based-Graph-to-Hypergraph-Structural-Lifting"><a href="#93-A-Remedy-for-Over-Squashing-in-Graph-Learning-via-Forman-Ricci-Curvature-based-Graph-to-Hypergraph-Structural-Lifting" class="headerlink" title="93. A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/A_Remedy_for_Over-Squashing_in_Graph_Learning_via_Forman-Ricci_Curvature_based_Graph-to-Hypergraph_S.pdf">A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Perelyn GmbH</span></p>
<p>本论文提出基于Forman-Ricci曲率的图到超图结构提升方法，通过评估图中边的曲率，识别网络骨架并构建超边，从而缓解GNN中的信息over-squashing问题。实验证明，该方法在多个图分类和分子属性预测任务中提升了主流GNN与超图神经网络的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/A%20Remedy%20for%20Over-Squashing%20in%20Graph%20Learning%20via%20Forman-Ricci%20Curvature%20based%20Graph-to-Hypergraph%20Structural%20Lifting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-NeMo-A-Neuron-Level-Modularizing-While-Training-Approach-for-Decomposing-DNN-Models"><a href="#94-NeMo-A-Neuron-Level-Modularizing-While-Training-Approach-for-Decomposing-DNN-Models" class="headerlink" title="94. NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/NeMo__A_Neuron-Level_Modularizing-While-Training_Approach_for_Decomposing_DNN_Models.pdf">NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Laboratory of Complex &amp; Critical Software Environment, Beihang University</span></p>
<p>NeMo提出了一种支持Transformer和CNN模型的神经元级“边训练边模块化”新方法，通过引入对比学习优化模块内聚与耦合损失，实现模块化训练及结构化分解。实验表明，NeMo有效提升模块分类准确率，显著减小模型规模，在多任务高效复用场景下表现优异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-Mitigating-Modality-Quantity-and-Quality-Imbalance-in-Multimodal-Online-Federated-Learning"><a href="#95-Mitigating-Modality-Quantity-and-Quality-Imbalance-in-Multimodal-Online-Federated-Learning" class="headerlink" title="95. Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Mitigating_Modality_Quantity_and_Quality_Imbalance_in_Multimodal_Online_Federated_Learning.pdf">Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peng Cheng Laboratory</span></p>
<p>本文针对多模态在线联邦学习（MMO-FL）中由于IoT设备不稳定导致的数据模态数量与质量失衡问题，提出了一套理论分析框架，量化失衡对学习性能的影响，并据此设计了基于原型学习的模态数量与质量再平衡（QQR）算法。大量实验证明QQR算法在不同模态失衡场景下均优于现有基线方法，提升模型泛化能力并有效缓解模态失衡带来的性能损失。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Mitigating%20Modality%20Quantity%20and%20Quality%20Imbalance%20in%20Multimodal%20Online%20Federated%20Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-Towards-the-Next-generation-Bayesian-Network-Classifiers"><a href="#96-Towards-the-Next-generation-Bayesian-Network-Classifiers" class="headerlink" title="96. Towards the Next-generation Bayesian Network Classifiers"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Towards_the_Next-generation_Bayesian_Network_Classifiers.pdf">Towards the Next-generation Bayesian Network Classifiers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhengzhou University</span></p>
<p>本文提出了一种新范式，通过引入分布式表示学习（distributional representation learning）设计高阶贝叶斯网络分类器，解决传统方法中的参数爆炸和数据稀疏问题。具体方法是将K-依赖贝叶斯分类器（KDB）扩展成神经网络版本NeuralKDB，利用三层神经网络学习特征值和标签的分布式表示，并用随机梯度下降进行优化。实验表明，该方法能有效建模高阶特征依赖关系，在60个UCI数据集上显著优于传统贝叶斯分类器和其它主流分类方法。结论是分布式表示学习可显著提升贝叶斯网络分类器的性能和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-Graph-Neural-Diffusion-via-Generalized-Opinion-Dynamics"><a href="#97-Graph-Neural-Diffusion-via-Generalized-Opinion-Dynamics" class="headerlink" title="97. Graph Neural Diffusion via Generalized Opinion Dynamics"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Graph_Neural_Diffusion_via_Generalized_Opinion_Dynamics.pdf">Graph Neural Diffusion via Generalized Opinion Dynamics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Australian National University</span></p>
<p>该论文提出了GODNF，一个基于多个意见动力学模型的图神经扩散统一框架，通过节点特性建模和动态邻居影响实现异质信息扩散和时序动态捕捉；方法融合当前特征保留、初始特征依赖、邻居影响和结构正则，以理论保证收敛的可解释可扩展消息传播机制。实验证明GODNF在节点分类和影响估计任务上优于主流GNN模型，具备更强表达力和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Graph%20Neural%20Diffusion%20via%20Generalized%20Opinion%20Dynamics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-Meta-learning-Structure-Preserving-Dynamics"><a href="#98-Meta-learning-Structure-Preserving-Dynamics" class="headerlink" title="98. Meta-learning Structure-Preserving Dynamics"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-19/Meta-learning_Structure-Preserving_Dynamics.pdf">Meta-learning Structure-Preserving Dynamics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Arizona State University</span></p>
<p>本文提出了一种基于调制的元学习框架，用于学习物理系统的结构保持动力学建模。方法通过将结构保持模型直接调制到系统参数的紧凑潜在表示上，提出了两种新颖的调制策略（低秩和多秩调制），避免了对系统参数先验知识和显式优化的依赖。实验表明，该方法在少样本学习下能准确预测能量守恒和耗散系统的动力学，并且在保持物理约束的同时具有良好的泛化能力，显著优于基线元学习与调制方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-19/Meta-learning%20Structure-Preserving%20Dynamics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>今日论文报纸</div>
      <div>http://example.com/2025/08/2025-08-19_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Ywfhhh</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月19日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-18_article/" title="今日论文报纸">
                        <span class="hidden-mobile">今日论文报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
