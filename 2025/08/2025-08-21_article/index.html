

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="具身ADT">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;21报纸">
<meta property="og:url" content="http://ywfhhh.github.io/2025/08/2025-08-21_article/index.html">
<meta property="og:site_name" content="最新前沿论文分享">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Learning_from_Preferences_and_Mixed_Demonstrations_in_General_Settings.jpg">
<meta property="article:published_time" content="2025-08-21T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-22T04:28:37.390Z">
<meta property="article:author" content="具身ADT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Learning_from_Preferences_and_Mixed_Demonstrations_in_General_Settings.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/21报纸 - 最新前沿论文分享</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ywfhhh.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文库</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/21报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-21 00:00" pubdate>
          星期四, 八月 21日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          17k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/21报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/21报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-21<br>📄 <strong>发现论文数量</strong>：111  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Learning-from-Preferences-and-Mixed-Demonstrations-in-General-Settings"><a href="#1-Learning-from-Preferences-and-Mixed-Demonstrations-in-General-Settings" class="headerlink" title="1. Learning from Preferences and Mixed Demonstrations in General Settings"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Learning_from_Preferences_and_Mixed_Demonstrations_in_General_Settings.pdf">Learning from Preferences and Mixed Demonstrations in General Settings</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cambridge</span></p>
<p>本文提出了RRPO（Reward-Rational Partial Orderings）理论框架，将人类偏好和混合示范数据统一编码为部分序关系，并据此设计了LEOPARD算法，实现从偏好反馈、正&#x2F;负示范及其排序中联合学习奖励函数。实验表明，LEOPARD在多种RL环境下显著超越现有基线方法，且混合多种反馈类型优于单一反馈，验证了方法的通用性及高效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Learning_from_Preferences_and_Mixed_Demonstrations_in_General_Settings.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-COMPUTERRL-Scaling-End-to-End-Online-Reinforcement-Learning-for-Computer-Use-Agents"><a href="#2-COMPUTERRL-Scaling-End-to-End-Online-Reinforcement-Learning-for-Computer-Use-Agents" class="headerlink" title="2. COMPUTERRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ComputerRL__Scaling_End-to-End_Online_Reinforcement_Learning_for_Computer_Use_Agents.pdf">COMPUTERRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出了COMPUTERRL框架，实现了API-GUI交互范式，自动化API构建和分布式虚拟桌面环境，大幅提升桌面智能体在复杂数字工作空间的操作能力。通过分布式RL基础设施和Entropulse交替训练策略，有效缓解训练中的熵塌缩，实现高效大规模RL训练，并在OSWorld基准测试上取得了48.1%的新SOTA准确率，显著提升了桌面自动化智能体的泛化和执行效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/COMPUTERRL_Scaling_End-to-End_Online_Reinforcement_Learning_for_Computer_Use_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-Convergent-Reinforcement-Learning-Algorithms-for-Stochastic-Shortest-Path-Problem"><a href="#3-Convergent-Reinforcement-Learning-Algorithms-for-Stochastic-Shortest-Path-Problem" class="headerlink" title="3. Convergent Reinforcement Learning Algorithms for Stochastic Shortest Path Problem"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Convergent_Reinforcement_Learning_Algorithms_for_Stochastic_Shortest_Path_Problem.pdf">Convergent Reinforcement Learning Algorithms for Stochastic Shortest Path Problem</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Indian Institute of Science</span></p>
<p>本文针对随机最短路径（SSP）问题，提出了两种表格式（tabular）两时间尺度算法（Actor-Critic与Critic-Actor）和一种函数逼近（Function Approximation）版本的Actor-Critic算法。所有算法均在理论上证明了渐近几乎必然收敛，并在实验中表现优于Q-Learning和SARSA，尤其在函数逼近场景下表现出更可靠的收敛性，填补了现有算法在SSP问题上的收敛可靠性空白。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Convergent_Reinforcement_Learning_Algorithms_for_Stochastic_Shortest_Path_Problem.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-Categorical-Policies-Multimodal-Policy-Learning-and-Exploration-in-Continuous-Control"><a href="#4-Categorical-Policies-Multimodal-Policy-Learning-and-Exploration-in-Continuous-Control" class="headerlink" title="4. Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Categorical_Policies__Multimodal_Policy_Learning_and_Exploration_in_Continuous_Control.pdf">Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Texas at Arlington</span></p>
<p>本文提出了一种新的连续控制多模态策略学习方法——Categorical Policies，通过中间的多个类别分布变量对行为模式进行建模，并采用STE和Gumbel-Softmax两种可微分采样方案，使策略能够表达多模态行为且保持梯度优化。实验在DeepMind Control Suite多个任务上表明，该方法能更快收敛并优于传统高斯单模态策略，尤其在探索和任务适应性上有显著提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Categorical_Policies_Multimodal_Policy_Learning_and_Exploration_in_Continuous_Control.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-Reinforcement-Learning-based-Adaptive-Path-Selection-for-Programmable-Networks"><a href="#5-Reinforcement-Learning-based-Adaptive-Path-Selection-for-Programmable-Networks" class="headerlink" title="5. Reinforcement Learning-based Adaptive Path Selection for Programmable Networks"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Reinforcement_Learning-based_Adaptive_Path_Selection_for_Programmable_Networks.pdf">Reinforcement Learning-based Adaptive Path Selection for Programmable Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Informatics, University of Amsterdam</span></p>
<p>本文提出了一种分布式、可嵌入数据面的强化学习（IN-RL）方法，用于可编程网络中的自适应路径选择。方法结合了轻量级随机学习自动机（SLA）算法和实时带内网络遥测（INT）收集的拥塞指标，在P4可编程交换机上实现了本地、数据驱动的转发决策，能根据队列长度和出队延迟动态调整流量路径。实验结果表明，SLA机制可稳定收敛于高效路径，并对网络状态变化做出快速响应，且额外开销极低。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Reinforcement_Learning-based_Adaptive_Path_Selection_for_Programmable_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-Revisiting-Diffusion-Q-Learning-From-Iterative-Denoising-to-One-Step-Action-Generation"><a href="#6-Revisiting-Diffusion-Q-Learning-From-Iterative-Denoising-to-One-Step-Action-Generation" class="headerlink" title="6. Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Revisiting_Diffusion_Q-Learning__From_Iterative_Denoising_to_One-Step_Action_Generation.pdf">Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea Advanced Institute of Science and Technology</span></p>
<p>该论文提出了One-Step Flow Q-Learning (OFQL)，通过在Flow Matching框架下学习平均速度场，实现了训练和推理阶段高效的一步动作生成，无需多步去噪、辅助模型或多阶段训练。实验结果显示，OFQL在D4RL基准上显著超越当前扩散模型与Q学习方法，同时大幅提升训练与推理速度，简化了离线强化学习流程。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Revisiting_Diffusion_Q-Learning_From_Iterative_Denoising_to_One-Step_Action_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-MACTAS-Self-Attention-Based-Module-for-Inter-Agent-Communication-in-Multi-Agent-Reinforcement-Learning"><a href="#7-MACTAS-Self-Attention-Based-Module-for-Inter-Agent-Communication-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="7. MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MACTAS__Self-Attention-Based_Module_for_Inter-Agent_Communication_in_Multi-Agent_Reinforcement_Learn.pdf">MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Warsaw</span></p>
<p>本文提出了一种基于Transformer自注意力机制的多智能体通信模块MACTAS，可无缝集成到任意动作-价值函数分解方法中，实现全可微的奖励驱动通信。实验在SMAC基准上验证了MACTAS在多个难度场景下取得了SOTA或媲美SOTA的表现，并证明了其参数量与智能体数量无关且易于部署。结论是MACTAS显著提升了多智能体强化学习中的通信与协作效率，尤其在大规模场景下表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MACTAS_Self-Attention-Based_Module_for_Inter-Agent_Communication_in_Multi-Agent_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-Breaking-the-SFT-Plateau-Multimodal-Structured-Reinforcement-Learning-for-Chart-to-Code-Generation"><a href="#8-Breaking-the-SFT-Plateau-Multimodal-Structured-Reinforcement-Learning-for-Chart-to-Code-Generation" class="headerlink" title="8. Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Breaking_the_SFT_Plateau__Multimodal_Structured_Reinforcement_Learning_for_Chart-to-Code_Generation.pdf">Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Meituan</span></p>
<p>本文提出了多模态结构化强化学习（MSRL）方法，针对图表转代码任务，结合文本与视觉多粒度奖励机制，通过两阶段训练流程对多模态大模型进行优化。实验表明，MSRL显著突破了监督微调（SFT）性能瓶颈，在ChartMimic与ReachQA基准上分别提升高层指标6.2%和9.9%，超越所有开源模型并接近闭源大模型水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Breaking_the_SFT_Plateau_Multimodal_Structured_Reinforcement_Learning_for_Chart-to-Code_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-MuFlex-A-Scalable-Physics-based-Platform-for-Multi-Building-Flexibility-Analysis-and-Coordination"><a href="#9-MuFlex-A-Scalable-Physics-based-Platform-for-Multi-Building-Flexibility-Analysis-and-Coordination" class="headerlink" title="9. MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MuFlex__A_Scalable,_Physics-based_Platform_for_Multi-Building_Flexibility_Analysis_and_Coordination.pdf">MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University College London</span></p>
<p>本文提出了MuFlex，一个基于物理模型、可扩展的多建筑柔性协调开源测试平台，通过同步多座EnergyPlus建筑模型与最新OpenAI Gym标准接口，支持多种控制策略（如强化学习和MPC）的训练与基准测试。案例研究采用Soft Actor–Critic算法实现多建筑负荷灵活性协调，结果显示平台可有效保持建筑能效与舒适度，同时将总峰值负荷控制在指定阈值以下。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MuFlex_A_Scalable_Physics-based_Platform_for_Multi-Building_Flexibility_Analysis_and_Coordination.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-CrafterDojo-A-Suite-of-Foundation-Models-for-Building-Open-Ended-Embodied-Agents-in-Crafter"><a href="#10-CrafterDojo-A-Suite-of-Foundation-Models-for-Building-Open-Ended-Embodied-Agents-in-Crafter" class="headerlink" title="10. CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/CrafterDojo__A_Suite_of_Foundation_Models_for_Building_Open-Ended_Embodied_Agents_in_Crafter.pdf">CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KAIST</span></p>
<p>该论文提出CrafterDojo，一套为Crafter环境打造的基础模型和工具，包括C-VPT（行为先验）、C-CLIP（视觉-语言对齐）、C-Steve-1（指令跟随）以及自动化数据生成工具CrafterPlay和CrafterCaption。实验表明，这些模型和数据集能高效支持基础模型训练，模型在行为生成、视觉-语言理解和指令执行等任务上表现优异，并通过分层规划实现多样且长时序任务，证明CrafterDojo可作为轻量级通用智能体研究平台。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/CrafterDojo_A_Suite_of_Foundation_Models_for_Building_Open-Ended_Embodied_Agents_in_Crafter.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="11-Multimodal-Data-Storage-and-Retrieval-for-Embodied-AI-A-Survey"><a href="#11-Multimodal-Data-Storage-and-Retrieval-for-Embodied-AI-A-Survey" class="headerlink" title="11. Multimodal Data Storage and Retrieval for Embodied AI: A Survey"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Multimodal_Data_Storage_and_Retrieval_for_Embodied_AI__A_Survey.pdf">Multimodal Data Storage and Retrieval for Embodied AI: A Survey</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China Normal University</span></p>
<p>本文系统性综述了面向Embodied AI（具身智能）多模态数据存储与检索的五大架构（图数据库、多模型数据库、数据湖、向量数据库、时序数据库）及五大检索范式（融合策略、表示对齐、图结构、生成模型、高效优化）。论文指出，当前技术在物理语境建模、跨模态集成、动态适应和开放世界泛化等方面存在核心瓶颈，并提出未来需发展物理感知数据模型、协同优化存储与检索、标准化基准测试等方向。结论认为，具身智能的数据管理需面向物理世界特点创新，当前技术迁移仍不足以满足实际需求。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Multimodal_Data_Storage_and_Retrieval_for_Embodied_AI_A_Survey.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-Driving-Style-Recognition-Like-an-Expert-Using-Semantic-Privileged-Information-from-Large-Language-Models"><a href="#12-Driving-Style-Recognition-Like-an-Expert-Using-Semantic-Privileged-Information-from-Large-Language-Models" class="headerlink" title="12. Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Driving_Style_Recognition_Like_an_Expert_Using_Semantic_Privileged_Information_from_Large_Language_M.pdf">Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>该论文提出了LUSPI框架，将大语言模型（LLM）生成的驾驶行为语义信息作为特权信息，通过DriBehavGPT模块自动生成专家级语义描述，再经文本嵌入和降维后作为SVM+训练输入。方法在真实驾驶数据中显著提升了驾驶风格识别准确性，F1分数在跟车和变道场景分别提升7.6%和7.9%；部署阶段仅用传感器数据实现高效推断。结论显示语义行为表征能有效改善模型性能并提升解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Driving_Style_Recognition_Like_an_Expert_Using_Semantic_Privileged_Information_from_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-Online-3D-Gaussian-Splatting-Modeling-with-Novel-View-Selection"><a href="#13-Online-3D-Gaussian-Splatting-Modeling-with-Novel-View-Selection" class="headerlink" title="13. Online 3D Gaussian Splatting Modeling with Novel View Selection"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Online_3D_Gaussian_Splatting_Modeling_with_Novel_View_Selection.pdf">Online 3D Gaussian Splatting Modeling with Novel View Selection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Dongguk University</span></p>
<p>本文提出了一种在线3D高斯Splatting建模方法，通过自适应视角选择显著提升模型的完整性。关键技术包括利用高斯形状和位置梯度的不确定性估计来选取信息增益最大的非关键帧，并结合多视图立体（MVS）网络与全局Bundle Adjustment优化，显著提升了室内外场景的渲染质量和模型鲁棒性。实验结果表明，该方法在复杂户外环境下性能优于现有主流SLAM方法，首次实现了3DGS框架下的非关键帧选择，有效提升了场景建模的完整性和精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Online_3D_Gaussian_Splatting_Modeling_with_Novel_View_Selection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-Toward-Deployable-Multi-Robot-Collaboration-via-a-Symbolically-Guided-Decision-Transformer"><a href="#14-Toward-Deployable-Multi-Robot-Collaboration-via-a-Symbolically-Guided-Decision-Transformer" class="headerlink" title="14. Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Toward_Deployable_Multi-Robot_Collaboration_via_a_Symbolically-Guided_Decision_Transformer.pdf">Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Purdue University</span></p>
<p>本文提出了Symbolically-Guided Decision Transformer（SGDT）框架，将神经符号规划器与因果Transformer结合，实现了多机器人协作的高效决策。方法包括使用PDDL与开源大语言模型生成高层次符号子目标，再由目标条件决策Transformer完成低层次动作决策。实验表明SGDT在仿真多机器人任务（如三明治制作和杂货打包）中具备优异的任务成功率、强大的零样本与少样本泛化能力。该方法提升了多机器人协作的可部署性、解释性和泛化性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Toward_Deployable_Multi-Robot_Collaboration_via_a_Symbolically-Guided_Decision_Transformer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-Trajectory-Tracking-and-Stabilization-of-Quadrotors-Using-Deep-Koopman-Model-Predictive-Control"><a href="#15-Trajectory-Tracking-and-Stabilization-of-Quadrotors-Using-Deep-Koopman-Model-Predictive-Control" class="headerlink" title="15. Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Trajectory_Tracking_and_Stabilization_of_Quadrotors_Using_Deep_Koopman_Model_Predictive_Control.pdf">Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Egypt-Japan University of Science and Technology</span></p>
<p>本论文提出了一种面向四旋翼系统的数据驱动控制框架，结合深度Koopman算子与模型预测控制（DK-MPC）。方法通过神经网络学习高维线性化表征，实现复杂非线性飞行动力学的建模，并将其嵌入MPC以高效优化控制输入。实验结果表明，DK-MPC在轨迹跟踪和点稳定任务中表现优于传统非线性MPC，兼具高精度与低计算成本，展示了其在实时嵌入式飞控系统的应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Trajectory_Tracking_and_Stabilization_of_Quadrotors_Using_Deep_Koopman_Model_Predictive_Control.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-Blast-Hole-Seeking-and-Dipping-–-The-Navigation-and-Perception-Framework-in-a-Mine-Site-Inspection-Robot"><a href="#16-Blast-Hole-Seeking-and-Dipping-–-The-Navigation-and-Perception-Framework-in-a-Mine-Site-Inspection-Robot" class="headerlink" title="16. Blast Hole Seeking and Dipping – The Navigation and Perception Framework in a Mine Site Inspection Robot"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Blast_Hole_Seeking_and_Dipping_--_The_Navigation_and_Perception_Framework_in_a_Mine_Site_Inspection_.pdf">Blast Hole Seeking and Dipping – The Navigation and Perception Framework in a Mine Site Inspection Robot</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Australian Centre for Robotics, The University of Sydney</span></p>
<p>该论文提出了一套用于露天矿坑爆破孔自动检测和导航的机器人感知与控制框架。方法包括基于多传感器（GPS、IMU、双LiDAR）融合、点云处理、虚拟深度图像生成、两阶段（粗到精）孔检测流程、FRST特征提取、RANSAC和Taubin圆拟合、非极大值抑制，以及自适应相机参数调整，实现了高效精准的爆破孔导航与传感器对准。实验表明，该系统无需数据驱动学习方法即可在真实矿区和仿真环境中实现可靠、精确的自动爆破孔定位与检测，显著提升了矿区检测效率与安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Blast_Hole_Seeking_and_Dipping_%E2%80%93_The_Navigation_and_Perception_Framework_in_a_Mine_Site_Inspection_Robot.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-AutoMPC-A-Code-Generator-for-MPC-based-Automated-Driving"><a href="#17-AutoMPC-A-Code-Generator-for-MPC-based-Automated-Driving" class="headerlink" title="17. AutoMPC: A Code Generator for MPC-based Automated Driving"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/AutoMPC__A_Code_Generator_for_MPC-based_Automated_Driving.pdf">AutoMPC: A Code Generator for MPC-based Automated Driving</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Luebeck</span></p>
<p>本文提出了AutoMPC软件包，通过集成非线性主动集（NAS）算法与代码生成器，为自动驾驶车辆轨迹跟踪提供高效、可定制的MPC控制解决方案。AutoMPC可自动生成高效、无依赖的C代码，支持多种车辆模型和数值积分方法，便于在Matlab&#x2F;Simulink®和ROS等嵌入式平台上部署，具备高鲁棒性、实时性和自愈能力。论文通过多种仿真实验证明，AutoMPC能在保证解可行性的同时，满足大多数自动驾驶场景下对复杂动态、约束和切换工况的需求。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/AutoMPC_A_Code_Generator_for_MPC-based_Automated_Driving.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-MimicFunc-Imitating-Tool-Manipulation-from-a-Single-Human-Video-via-Functional-Correspondence"><a href="#18-MimicFunc-Imitating-Tool-Manipulation-from-a-Single-Human-Video-via-Functional-Correspondence" class="headerlink" title="18. MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MimicFunc__Imitating_Tool_Manipulation_from_a_Single_Human_Video_via_Functional_Correspondence.pdf">MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southern University of Science and Technology</span></p>
<p>MimicFunc是一种机器人模仿工具操作的方法，通过分析单个人类RGB-D视频，分三步提取功能关键点、建立功能帧对应关系并生成基于功能帧的动作轨迹。实验证明，MimicFunc能让机器人从单一人类视频泛化到多种新工具及任务，并生成高质量数据高效训练视觉运动策略，无需繁重遥操作数据采集。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MimicFunc_Imitating_Tool_Manipulation_from_a_Single_Human_Video_via_Functional_Correspondence.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-A-Three-Level-Whole-Body-Disturbance-Rejection-Control-Framework-for-Dynamic-Motions-in-Legged-Robots"><a href="#19-A-Three-Level-Whole-Body-Disturbance-Rejection-Control-Framework-for-Dynamic-Motions-in-Legged-Robots" class="headerlink" title="19. A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/A_Three-Level_Whole-Body_Disturbance_Rejection_Control_Framework_for_Dynamic_Motions_in_Legged_Robot.pdf">A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huazhong University of Science and Technology</span></p>
<p>本文提出了一种三层全身扰动抑制控制框架（T-WB-DRC），通过集成移动视界扩展状态观测器（MH-ESO）与稳健模型预测控制（MPC），能有效估计并补偿腿式机器人在动态运动中遇到的模型不确定性、外部扰动与故障。该方法在Gazebo仿真和实物四足机器人实验中显示出卓越的鲁棒性和稳定性，显著提升了负载运输、扰动抑制和故障容错能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/A_Three-Level_Whole-Body_Disturbance_Rejection_Control_Framework_for_Dynamic_Motions_in_Legged_Robots.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-Unified-Hierarchical-MPC-in-Task-Executing-for-Modular-Manipulators-across-Diverse-Morphologies"><a href="#20-Unified-Hierarchical-MPC-in-Task-Executing-for-Modular-Manipulators-across-Diverse-Morphologies" class="headerlink" title="20. Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Unified_Hierarchical_MPC_in_Task_Executing_for_Modular_Manipulators_across_Diverse_Morphologies.pdf">Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Istituto Italiano Di Tecnologia (IIT)</span></p>
<p>该论文提出了一种统一的分层模型预测控制（H-MPC）方法，可自动适应多种模块化机械臂形态，实现无需参数调优的任务执行。方法将控制过程分为高层MPC和低层MPC，高层负责预测未来状态并提供轨迹，低层基于高层信息更新模型以提升控制精度，同时隐式引入二阶动力学信息。实验验证显示，该方法在多种机械臂形态下具有更高的轨迹跟踪精度和鲁棒性，尤其在处理奇异点和实际抓取任务中表现优异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Unified_Hierarchical_MPC_in_Task_Executing_for_Modular_Manipulators_across_Diverse_Morphologies.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-Modeling-and-Control-of-AWOISV-A-Filtered-Tube-Based-MPC-Approach-for-Simultaneous-Tracking-of-Lateral-Position-and-Heading-Angle"><a href="#21-Modeling-and-Control-of-AWOISV-A-Filtered-Tube-Based-MPC-Approach-for-Simultaneous-Tracking-of-Lateral-Position-and-Heading-Angle" class="headerlink" title="21. Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Modeling_and_Control_of_AWOISV__A_Filtered_Tube-Based_MPC_Approach_for_Simultaneous_Tracking_of_Late.pdf">Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>本文提出了一种针对全轮全向独立转向车辆（AWOISV）的运动模式理论定义及切换标准，并建立了可应用于任意轴数AWOISV的广义v-β-r动态模型。基于此模型，设计了一种滤波管式线性时变MPC（FT-LTVMPC）控制策略，实现了车辆横向位置与任意航向角的同时高精度跟踪，具备对模型不确定性和参数扰动的强鲁棒性。仿真与硬件在环实验表明，该方法在多种速度和工况下均显著优于传统MPC，提升了跟踪精度和控制平滑性，满足实时控制需求。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Modeling_and_Control_of_AWOISV_A_Filtered_Tube-Based_MPC_Approach_for_Simultaneous_Tracking_of_Lateral_Position_and_Heading_Angle.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-Switch4EAI-Leveraging-Console-Game-Platform-for-Benchmarking-Robotic-Athletics"><a href="#22-Switch4EAI-Leveraging-Console-Game-Platform-for-Benchmarking-Robotic-Athletics" class="headerlink" title="22. Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Switch4EAI__Leveraging_Console_Game_Platform_for_Benchmarking_Robotic_Athletics.pdf">Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Georgia Institute of Technology</span></p>
<p>本文提出Switch4EAI管道，利用任天堂Switch的Just Dance游戏，通过视频流捕获、ROMP三维动作重建、GMR动作重定向等模块，将人类舞蹈动作映射到机器人并由机器人实际执行，使用内置评分系统进行定量评估。结果显示该方法低成本、易部署，可实现机器人与人类在运动能力上的直接比较，验证了商业游戏平台用于机器人运动基准测试的可行性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Switch4EAI_Leveraging_Console_Game_Platform_for_Benchmarking_Robotic_Athletics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-ROVER-Robust-Loop-Closure-Verification-with-Trajectory-Prior-in-Repetitive-Environments"><a href="#23-ROVER-Robust-Loop-Closure-Verification-with-Trajectory-Prior-in-Repetitive-Environments" class="headerlink" title="23. ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ROVER__Robust_Loop_Closure_Verification_with_Trajectory_Prior_in_Repetitive_Environments.pdf">ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CKS Robotics Institute, Hong Kong University of Science and Technology</span></p>
<p>本文提出了一种基于轨迹先验约束的循环闭合验证方法ROVER，用于解决重复环境下SLAM中外观特征失效导致的误检问题。方法流程为对每个循环候选，通过位姿图优化估计新轨迹，并与历史轨迹对齐，采用评分机制判断该循环约束是否被接受。实验结果表明，ROVER在多个公开和自采数据集上超越传统几何验证方法，能有效提升机器人定位鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ROVER_Robust_Loop_Closure_Verification_with_Trajectory_Prior_in_Repetitive_Environments.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-CAST-Counterfactual-Labels-Improve-Instruction-Following-in-Vision-Language-Action-Models"><a href="#24-CAST-Counterfactual-Labels-Improve-Instruction-Following-in-Vision-Language-Action-Models" class="headerlink" title="24. CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/CAST__Counterfactual_Labels_Improve_Instruction_Following_in_Vision-Language-Action_Models.pdf">CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California Berkeley</span></p>
<p>该论文提出CAST方法，通过利用视觉语言模型（VLM）为现有机器人轨迹数据生成反事实语言和动作标签，扩充数据集以提升视觉-语言-动作（VLA）模型的指令跟随能力。该方法无需额外人工采集数据，仅通过合成多样化的标签，强制VLA模型更好地理解并响应复杂自然语言指令。实验证明，采用CAST增强后的模型在真实环境中导航任务的指令跟随成功率提升了27%，并在多个基准任务上优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/CAST_Counterfactual_Labels_Improve_Instruction_Following_in_Vision-Language-Action_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="25-Beyond-Simple-Edits-Composed-Video-Retrieval-with-Dense-Modifications"><a href="#25-Beyond-Simple-Edits-Composed-Video-Retrieval-with-Dense-Modifications" class="headerlink" title="25. Beyond Simple Edits: Composed Video Retrieval with Dense Modifications"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Beyond_Simple_Edits__Composed_Video_Retrieval_with_Dense_Modifications.pdf">Beyond Simple Edits: Composed Video Retrieval with Dense Modifications</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mohamed bin Zayed University of AI</span></p>
<p>该论文提出Dense-WebVid-CoVR大规模细粒度视频检索数据集，包含1.6百万组视频及丰富的修改文本，显著提升了视频内容细粒度变化的描述能力。作者还提出了统一融合视觉和文本的跨注意力视频检索模型，能够精准对齐复杂的多模态查询与目标视频，在主流基准上取得SOTA性能，Recall@1达71.3%，显著优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Beyond_Simple_Edits_Composed_Video_Retrieval_with_Dense_Modifications.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-InfiniteTalk-Audio-driven-Video-Generation-for-Sparse-Frame-Video-Dubbing"><a href="#26-InfiniteTalk-Audio-driven-Video-Generation-for-Sparse-Frame-Video-Dubbing" class="headerlink" title="26. InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/InfiniteTalk__Audio-driven_Video_Generation_for_Sparse-Frame_Video_Dubbing.pdf">InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, University of Chinese Academy of Sciences</span></p>
<p>该论文提出了一种新的视频配音范式——稀疏帧视频配音(sparse-frame video dubbing)，通过保留关键帧来实现身份、情感和镜头运动的保持，同时利用音频驱动生成全身动态与情感同步的视频。核心方法InfiniteTalk采用流式音频驱动生成器，结合时序上下文帧实现顺滑片段衔接，并提出细粒度参考帧采样策略以平衡控制强度与运动对齐。实验表明，该方法在唇部、面部及全身运动同步方面达到SOTA效果，显著提升了长视频配音的自然度和沉浸感。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/InfiniteTalk_Audio-driven_Video_Generation_for_Sparse-Frame_Video_Dubbing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-GeoSAM2-Unleashing-the-Power-of-SAM2-for-3D-Part-Segmentation"><a href="#27-GeoSAM2-Unleashing-the-Power-of-SAM2-for-3D-Part-Segmentation" class="headerlink" title="27. GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/GeoSAM2__Unleashing_the_Power_of_SAM2_for_3D_Part_Segmentation.pdf">GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">VAST</span></p>
<p>该论文提出了GeoSAM2，一种基于2D交互提示的3D部件分割新框架，将3D分割任务转化为多视角2D掩码预测问题。方法利用LoRA和残差几何特征融合对SAM2进行高效适配，实现了对无纹理3D物体的精细分割，并通过多视图一致性和轻量后处理获得高精度的3D标签。实验表明，GeoSAM2在PartObjaverse-Tiny和PartNetE等公开基准上显著超过现有方法，具备较强的泛化和用户可控性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/GeoSAM2_Unleashing_the_Power_of_SAM2_for_3D_Part_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-Backdooring-Self-Supervised-Contrastive-Learning-by-Noisy-Alignment"><a href="#28-Backdooring-Self-Supervised-Contrastive-Learning-by-Noisy-Alignment" class="headerlink" title="28. Backdooring Self-Supervised Contrastive Learning by Noisy Alignment"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Backdooring_Self-Supervised_Contrastive_Learning_by_Noisy_Alignment.pdf">Backdooring Self-Supervised Contrastive Learning by Noisy Alignment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southeast University</span></p>
<p>本论文提出了一种面向自监督对比学习的数据投毒后门攻击方法Noisy Alignment（NA），通过显式压缩中毒样本中的噪声成分，并将触发器与目标类别特征对齐，提升了后门攻击在图像对比学习中的有效性。作者将数据投毒过程建模为图像布局优化问题，理论推导最优参数，并通过大量实验表明该方法在不同数据集和模型下均取得了SOTA攻击成功率，且对常见后门防御表现出较强鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Backdooring_Self-Supervised_Contrastive_Learning_by_Noisy_Alignment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-ASDFormer-A-Transformer-with-Mixtures-of-Pooling-Classifier-Experts-for-Robust-Autism-Diagnosis-and-Biomarker-Discovery"><a href="#29-ASDFormer-A-Transformer-with-Mixtures-of-Pooling-Classifier-Experts-for-Robust-Autism-Diagnosis-and-Biomarker-Discovery" class="headerlink" title="29. ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ASDFormer__A_Transformer_with_Mixtures_of_Pooling-Classifier_Experts_for_Robust_Autism_Diagnosis_and.pdf">ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Isfahan University of Technology</span></p>
<p>本文提出了ASDFormer，一种结合Transformer与专家混合（MoE）解码器的新方法，用于自闭症谱系障碍（ASD）基于脑功能连接的fMRI数据分类和生物标志物发现。通过专家专化与注意力机制，ASDFormer不仅提升了分类准确率，还增强了解释性，能够识别与ASD相关的关键脑区和连接异常，实验在ABIDE数据集上达到最新性能并揭示了可靠的ASD生物标志模式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ASDFormer_A_Transformer_with_Mixtures_of_Pooling-Classifier_Experts_for_Robust_Autism_Diagnosis_and_Biomarker_Discovery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-ViT-FIQA-Assessing-Face-Image-Quality-using-Vision-Transformers"><a href="#30-ViT-FIQA-Assessing-Face-Image-Quality-using-Vision-Transformers" class="headerlink" title="30. ViT-FIQA: Assessing Face Image Quality using Vision Transformers"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ViT-FIQA__Assessing_Face_Image_Quality_using_Vision_Transformers.pdf">ViT-FIQA: Assessing Face Image Quality using Vision Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fraunhofer Institute for Computer Graphics Research IGD</span></p>
<p>该论文提出了一种基于视觉Transformer的新型人脸图像质量评估方法ViT-FIQA，通过在ViT骨干网络中引入可学习的质量token，实现对人脸图像可用于识别任务的质量分数回归。该方法将质量token与标准patch token拼接，并利用全局自注意力机制集成图像上下文信息，输出的质量token用于回归人脸质量分数，同时patch tokens用于识别任务。实验结果表明，ViT-FIQA在多个主流数据集和不同人脸识别模型下表现出色，特别是在姿态和质量挑战较大的数据集上，验证了Transformer架构建模人脸图像效用的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ViT-FIQA_Assessing_Face_Image_Quality_using_Vision_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-Real-Time-Population-Based-Reconstruction-of-3D-Bone-Models-via-Very-Low-Dose-Protocols"><a href="#31-Real-Time-Population-Based-Reconstruction-of-3D-Bone-Models-via-Very-Low-Dose-Protocols" class="headerlink" title="31. Real-Time, Population-Based Reconstruction of 3D Bone Models via Very-Low-Dose Protocols"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Real-Time,_Population-Based_Reconstruction_of_3D_Bone_Models_via_Very-Low-Dose_Protocols.pdf">Real-Time, Population-Based Reconstruction of 3D Bone Models via Very-Low-Dose Protocols</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology</span></p>
<p>本文提出了一种半监督重建与知识蒸馏（SSR-KD）方法，通过深度神经网络结合隐式场、半监督学习和知识蒸馏，实现了从双平面X线快速重建高精度三维骨骼模型，仅需30秒，平均误差低于1.0mm，显著减少辐射暴露且无需CT和人工标注。临床评估表明，该自动重建的骨骼模型在外科手术规划和导板设计中与CT人工标注模型效果相当，大幅提升了骨模型在骨科应用的实用性和实时性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Real-Time_Population-Based_Reconstruction_of_3D_Bone_Models_via_Very-Low-Dose_Protocols.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-RED-AI-Id-Pattern-First-Results-of-Stone-Deterioration-Patterns-with-Multi-Agent-Systems"><a href="#32-RED-AI-Id-Pattern-First-Results-of-Stone-Deterioration-Patterns-with-Multi-Agent-Systems" class="headerlink" title="32. RED.AI Id-Pattern: First Results of Stone Deterioration Patterns with Multi-Agent Systems"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/RED.AI_Id-Pattern__First_Results_of_Stone_Deterioration_Patterns_with_Multi-Agent_Systems.pdf">RED.AI Id-Pattern: First Results of Stone Deterioration Patterns with Multi-Agent Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">STAP Reabilitação Estrutural, SA</span></p>
<p>本文提出了一种基于多智能体系统的石材劣化图像诊断方法，采用五类领域专家AI代理（岩石学家、病理学家、环境专家、修复师和诊断协调员），通过多模态LLM驱动结合RAG知识库分阶段实现单独分析、多学科讨论及共识整合。实验结果表明，Id-Pattern系统在准确率、召回率和F1得分上均较基础大模型有显著提升，尤其召回率接近翻倍，极大提升了石材劣化自动检测的全面性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/RED.AI_Id-Pattern_First_Results_of_Stone_Deterioration_Patterns_with_Multi-Agent_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-Extracting-Structured-Requirements-from-Unstructured-Building-Technical-Specifications-for-Building-Information-Modeling"><a href="#33-Extracting-Structured-Requirements-from-Unstructured-Building-Technical-Specifications-for-Building-Information-Modeling" class="headerlink" title="33. Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Extracting_Structured_Requirements_from_Unstructured_Building_Technical_Specifications_for_Building_.pdf">Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Arts et Métiers Institute of Technology</span></p>
<p>该论文提出将自然语言处理技术（如命名实体识别NER和关系抽取RE）与建筑信息建模（BIM）结合，通过CamemBERT等预训练变换器模型和转移学习方法自动从法语建筑技术规范（BTS）文档中提取结构化需求。实验结果显示，CamemBERT和Fr core news lg在NER任务的F1分数超过90%，随机森林模型在RE任务中F1分数超过80%，显著提升了BIM自动化验证系统的信息抽取效率和准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Extracting_Structured_Requirements_from_Unstructured_Building_Technical_Specifications_for_Building_Information_Modeling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-UNICON-UNIfied-CONtinual-Learning-for-Medical-Foundational-Models"><a href="#34-UNICON-UNIfied-CONtinual-Learning-for-Medical-Foundational-Models" class="headerlink" title="34. UNICON: UNIfied CONtinual Learning for Medical Foundational Models"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/UNICON__UNIfied_CONtinual_Learning_for_Medical_Foundational_Models.pdf">UNICON: UNIfied CONtinual Learning for Medical Foundational Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mohamed bin Zayed University of Artificial Intelligence</span></p>
<p>该论文提出UNICON框架，实现了医学影像基础模型在多任务、多模态（如CT、PET和文本报告）和不同解剖区域上的统一持续学习。通过LoRA与适配器模块高效扩展模型能力，实验证明模型可在无需大规模重训练的情况下动态适应新任务和模态，提升如预后预测与分割等任务表现，同时避免灾难性遗忘。结论认为UNICON可提升医学影像基础模型的通用性和适应性，为医疗AI系统的泛化奠定基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/UNICON_UNIfied_CONtinual_Learning_for_Medical_Foundational_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-Self-Supervised-Sparse-Sensor-Fusion-for-Long-Range-Perception"><a href="#35-Self-Supervised-Sparse-Sensor-Fusion-for-Long-Range-Perception" class="headerlink" title="35. Self-Supervised Sparse Sensor Fusion for Long Range Perception"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Self-Supervised_Sparse_Sensor_Fusion_for_Long_Range_Perception.pdf">Self-Supervised Sparse Sensor Fusion for Long Range Perception</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Torc Robotics</span></p>
<p>本文提出了一种名为LRS4Fusion的高效自监督稀疏体素多模态融合方法，实现了长距离（达250米）高速公路自动驾驶环境下的3D场景理解。通过稀疏体素编码、多模态（相机+LiDAR）特征自监督融合与时空上下文建模，显著提升了目标检测和LiDAR预测精度，分别在目标检测上提升26.6% mAP、在LiDAR预测上降低30.5% Chamfer Distance，突破了以往感知距离和计算瓶颈。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Self-Supervised_Sparse_Sensor_Fusion_for_Long_Range_Perception.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-ROVR-Open-Dataset-A-Large-Scale-Depth-Dataset-for-Autonomous-Driving"><a href="#36-ROVR-Open-Dataset-A-Large-Scale-Depth-Dataset-for-Autonomous-Driving" class="headerlink" title="36. ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ROVR-Open-Dataset__A_Large-Scale_Depth_Dataset_for_Autonomous_Driving.pdf">ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science, Wuhan University</span></p>
<p>该论文提出了一种大规模、场景多样、帧连续的自动驾驶深度估计公开数据集ROVR-Open-Dataset，采用轻量化多传感器同步采集与稀疏但统计充分的标注策略，覆盖多种道路、天气与光照条件。实验证明，现有主流深度估计算法在该数据集上表现大幅下降，凸显其挑战性和泛化需求，推动了复杂真实场景下深度估计方法的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ROVR-Open-Dataset_A_Large-Scale_Depth_Dataset_for_Autonomous_Driving.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-MMIS-Net-for-Retinal-Fluid-Segmentation-and-Detection"><a href="#37-MMIS-Net-for-Retinal-Fluid-Segmentation-and-Detection" class="headerlink" title="37. MMIS-Net for Retinal Fluid Segmentation and Detection"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MMIS-Net_for_Retinal_Fluid_Segmentation_and_Detection.pdf">MMIS-Net for Retinal Fluid Segmentation and Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Brunel University London</span></p>
<p>该论文提出了MMIS-Net（多模态医学图像分割网络），结合了卷积神经网络与创新的Similarity Fusion Block，通过监督和像素级相似性选择实现特征融合，并采用独特的one-hot标签空间以解决多数据集标签不一致与重叠问题。MMIS-Net在10个数据集、19个器官、2种模态上联合训练，并在RETOUCH挑战隐测集上实现了DS均值0.83、AVD 0.035和AUC 1的分割与检测表现，显著优于现有SOTA和大模型，验证了其通用性和高效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MMIS-Net_for_Retinal_Fluid_Segmentation_and_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-SCRNet-Spatial-Channel-Regulation-Network-for-Medical-Ultrasound-Image-Segmentation"><a href="#38-SCRNet-Spatial-Channel-Regulation-Network-for-Medical-Ultrasound-Image-Segmentation" class="headerlink" title="38. SCRNet: Spatial-Channel Regulation Network for Medical Ultrasound Image Segmentation"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/SCRNet__Spatial-Channel_Regulation_Network_for_Medical_Ultrasound_Image_Segmentation.pdf">SCRNet: Spatial-Channel Regulation Network for Medical Ultrasound Image Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本文提出了一种用于医学超声图像分割的新型网络SCRNet，通过设计特征聚合模块（FAM）结合卷积和交叉注意力机制，实现长距离依赖与局部上下文信息的融合；并提出空间-通道调控模块（SCRM），用于区分和强化重要特征，抑制冗余信息。将SCRM集成到UNet编码器后，SCRNet在BUSI、BUSIS和TN3K等医学分割数据集上取得了最先进的性能，显著优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/SCRNet_Spatial-Channel_Regulation_Network_for_Medical_Ultrasound_Image_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-UniECS-Unified-Multimodal-E-Commerce-Search-Framework-with-Gated-Cross-modal-Fusion"><a href="#39-UniECS-Unified-Multimodal-E-Commerce-Search-Framework-with-Gated-Cross-modal-Fusion" class="headerlink" title="39. UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/UniECS__Unified_Multimodal_E-Commerce_Search_Framework_with_Gated_Cross-modal_Fusion.pdf">UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou Technology</span></p>
<p>该论文提出了UniECS，一种统一的多模态电商搜索框架，核心技术为自适应门控多模态编码器和跨模态对齐、局部一致性及模态内对比损失的综合训练策略，支持任意图文输入组合检索。实验表明，UniECS在多个电商检索基准上超越现有方法，尤其在跨模态检索任务中提升显著，并在真实电商平台上线后显著提升点击率和营收，具备实际应用价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/UniECS_Unified_Multimodal_E-Commerce_Search_Framework_with_Gated_Cross-modal_Fusion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-Latent-Interpolation-Learning-Using-Diffusion-Models-for-Cardiac-Volume-Reconstruction"><a href="#40-Latent-Interpolation-Learning-Using-Diffusion-Models-for-Cardiac-Volume-Reconstruction" class="headerlink" title="40. Latent Interpolation Learning Using Diffusion Models for Cardiac Volume Reconstruction"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Latent_Interpolation_Learning_Using_Diffusion_Models_for_Cardiac_Volume_Reconstruction.pdf">Latent Interpolation Learning Using Diffusion Models for Cardiac Volume Reconstruction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University Munich</span></p>
<p>本文提出了一种名为Cardiac Latent Interpolation Diffusion (CaLID)的新框架，利用数据驱动的扩散模型在潜空间实现心脏MRI稀疏切片的高效三维重建。该方法通过端到端联合训练条件网络和U-Net去噪器，实现了复杂空间关系和时空动态的自适应插值，并在无需辅助语义信息的前提下，显著提升了重建精度和速度。实验表明，CaLID在重建质量、解剖准确性和计算效率上均优于现有方法，适用于临床心血管成像。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Latent_Interpolation_Learning_Using_Diffusion_Models_for_Cardiac_Volume_Reconstruction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-Sketch3DVE-Sketch-based-3D-Aware-Scene-Video-Editing"><a href="#41-Sketch3DVE-Sketch-based-3D-Aware-Scene-Video-Editing" class="headerlink" title="41. Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Sketch3DVE__Sketch-based_3D-Aware_Scene_Video_Editing.pdf">Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Computing Technology, Chinese Academy of Sciences</span></p>
<p>Sketch3DVE提出了一个基于草图的3D感知视频编辑方法，利用MagicQuill进行首帧编辑，并通过点云和深度图对3D结构进行分析和对齐，实现编辑效果在多视角视频中的传播。该方法结合3D点云编辑、3D感知掩码传播和视频扩散模型，能实现插入、移除、替换及外观修改等多样化视频结构编辑，在大幅视角变化下保留未编辑区域并生成高质量结果。实验表明，Sketch3DVE在视频质量、时序一致性和未编辑区域保留方面均优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Sketch3DVE_Sketch-based_3D-Aware_Scene_Video_Editing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-A-Fully-Transformer-Based-Multimodal-Framework-for-Explainable-Cancer-Image-Segmentation-Using-Radiology-Reports"><a href="#42-A-Fully-Transformer-Based-Multimodal-Framework-for-Explainable-Cancer-Image-Segmentation-Using-Radiology-Reports" class="headerlink" title="42. A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/A_Fully_Transformer_Based_Multimodal_Framework_for_Explainable_Cancer_Image_Segmentation_Using_Radio.pdf">A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Brunel University of London</span></p>
<p>本论文提出Med-CTX框架，将ViT与Swin Transformer组成双分支视觉编码器，并融合BioClinicalBERT编码的BI-RADS结构化和放射学自由文本，通过不确定性感知的跨模态注意力实现乳腺癌超声图像的分割与解释。Med-CTX在BUS-BRA数据集上取得Dice 99%和IoU 95%，显著优于主流基线，且通过生成分割掩码、不确定性热图和诊断解释，实现可解释性和可信度的提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/A_Fully_Transformer_Based_Multimodal_Framework_for_Explainable_Cancer_Image_Segmentation_Using_Radiology_Reports.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-Optimizing-Region-of-Interest-Selection-for-Effective-Embedding-in-Video-Steganography-Based-on-Genetic-Algorithms"><a href="#43-Optimizing-Region-of-Interest-Selection-for-Effective-Embedding-in-Video-Steganography-Based-on-Genetic-Algorithms" class="headerlink" title="43. Optimizing Region of Interest Selection for Effective Embedding in Video Steganography Based on Genetic Algorithms"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Optimizing_Region_of_Interest_Selection_for_Effective_Embedding_in_Video_Steganography_Based_on_Gene.pdf">Optimizing Region of Interest Selection for Effective Embedding in Video Steganography Based on Genetic Algorithms</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Duhok</span></p>
<p>本文提出了一种结合遗传算法（GA）与高级加密标准（AES）的新型视频隐写方法，通过GA在视频帧的Y分量中优化选取嵌入区域，实现机密数据的高效隐写与高安全性。实验表明，该方法在保证视频质量（PSNR 64-75 dB）的同时，提升了数据嵌入容量和编码解码效率，适用于实时和高安全需求的场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Optimizing_Region_of_Interest_Selection_for_Effective_Embedding_in_Video_Steganography_Based_on_Genetic_Algorithms.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-DiffIER-Optimizing-Diffusion-Models-with-Iterative-Error-Reduction"><a href="#44-DiffIER-Optimizing-Diffusion-Models-with-Iterative-Error-Reduction" class="headerlink" title="44. DiffIER: Optimizing Diffusion Models with Iterative Error Reduction"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/DiffIER__Optimizing_Diffusion_Models_with_Iterative_Error_Reduction.pdf">DiffIER: Optimizing Diffusion Models with Iterative Error Reduction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出DiffIER，一种无需训练、基于优化的方法，通过迭代误差减小机制提升扩散模型的生成质量。DiffIER在推理阶段通过梯度优化收敛每一步的误差，显著缩小训练推理差距，有效提升文本生成图像、图像超分辨率和文本生成语音等任务的表现。实验显示该方法在多任务上优于主流基线，具备良好泛化性和实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/DiffIER_Optimizing_Diffusion_Models_with_Iterative_Error_Reduction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-Unleashing-Semantic-and-Geometric-Priors-for-3D-Scene-Completion"><a href="#45-Unleashing-Semantic-and-Geometric-Priors-for-3D-Scene-Completion" class="headerlink" title="45. Unleashing Semantic and Geometric Priors for 3D Scene Completion"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Unleashing_Semantic_and_Geometric_Priors_for_3D_Scene_Completion.pdf">Unleashing Semantic and Geometric Priors for 3D Scene Completion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Soochow University</span></p>
<p>本文提出了FoundationSSC框架，通过在源头和路径层面对语义与几何特征进行双解耦，结合FoundationStereo视觉基础模型生成的高质量语义与几何先验，并设计了轴感知融合（AAF）模块实现3D特征自适应融合。实验结果表明，该方法在SemanticKITTI和SSCBench-KITTI-360数据集上显著提升了3D场景完成的语义与几何性能，达到最新最优水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Unleashing_Semantic_and_Geometric_Priors_for_3D_Scene_Completion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-GazeProphet-Software-Only-Gaze-Prediction-for-VR-Foveated-Rendering"><a href="#46-GazeProphet-Software-Only-Gaze-Prediction-for-VR-Foveated-Rendering" class="headerlink" title="46. GazeProphet: Software-Only Gaze Prediction for VR Foveated Rendering"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/GazeProphet__Software-Only_Gaze_Prediction_for_VR_Foveated_Rendering.pdf">GazeProphet: Software-Only Gaze Prediction for VR Foveated Rendering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">PES University</span></p>
<p>本文提出GazeProphet，一种无需硬件眼动追踪的纯软件VR凝视点预测方法，结合Spherical Vision Transformer进行360°场景特征提取，LSTM编码凝视序列时序特征，再经多模态融合网络输出高置信凝视点。实验证明在VR数据集上，该方法预测精度显著优于传统显著性模型，角度误差降低24%，一致性和置信度均表现优异，表明可在无专用硬件下实现高效的注视点驱动渲染。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/GazeProphet_Software-Only_Gaze_Prediction_for_VR_Foveated_Rendering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-Enhancing-Targeted-Adversarial-Attacks-on-Large-Vision-Language-Models-through-Intermediate-Projector-Guidance"><a href="#47-Enhancing-Targeted-Adversarial-Attacks-on-Large-Vision-Language-Models-through-Intermediate-Projector-Guidance" class="headerlink" title="47. Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Enhancing_Targeted_Adversarial_Attacks_on_Large_Vision-Language_Models_through_Intermediate_Projecto.pdf">Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong Polytechnic University</span></p>
<p>本文提出了一种针对大型视觉-语言模型（VLMs）的新型定向对抗攻击方法IPGA（Intermediate Projector Guided Attack），首次利用视觉-语言对齐管道中的投影器模块（特别是Q-Former的中间训练阶段），通过操作细粒度的视觉特征实现更精细的对抗扰动控制，并提出了残差查询对齐（RQA）机制以保护无关内容。实验结果表明，IPGA及其扩展（IPGA-R）在图像字幕和视觉问答等任务中，在黑盒环境下优于现有对抗攻击方法，并能有效迁移至包括Google Gemini和OpenAI GPT等商用VLM，揭示了VLMs的关键安全漏洞。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Enhancing_Targeted_Adversarial_Attacks_on_Large_Vision-Language_Models_through_Intermediate_Projector_Guidance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-subCellSAM-Zero-Shot-Sub-Cellular-Segmentation-for-Hit-Validation-in-Drug-Discovery"><a href="#48-subCellSAM-Zero-Shot-Sub-Cellular-Segmentation-for-Hit-Validation-in-Drug-Discovery" class="headerlink" title="48. subCellSAM: Zero-Shot (Sub-)Cellular Segmentation for Hit Validation in Drug Discovery"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/subCellSAM__Zero-Shot_(Sub-)Cellular_Segmentation_for_Hit_Validation_in_Drug_Discovery.pdf">subCellSAM: Zero-Shot (Sub-)Cellular Segmentation for Hit Validation in Drug Discovery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Bern</span></p>
<p>该论文提出了subCellSAM，一种利用预训练分割基础模型的零样本细胞及亚细胞分割方法，通过递归自提示和嵌入形态学及拓扑先验实现高通用性，核心流程包括核分割、细胞分割以及亚细胞结构分割。实验证明，subCellSAM在细胞分割基准数据集和药物发现相关验证任务中，无需数据集特定调参即可取得与专业方法相当或更优的分割和下游分析结果，有效减少手动配置需求。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/subCellSAM_Zero-Shot_%28Sub-%29Cellular_Segmentation_for_Hit_Validation_in_Drug_Discovery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-RCGNet-RGB-based-Category-Level-6D-Object-Pose-Estimation-with-Geometric-Guidance"><a href="#49-RCGNet-RGB-based-Category-Level-6D-Object-Pose-Estimation-with-Geometric-Guidance" class="headerlink" title="49. RCGNet: RGB-based Category-Level 6D Object Pose Estimation with Geometric Guidance"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/RCGNet__RGB-based_Category-Level_6D_Object_Pose_Estimation_with_Geometric_Guidance.pdf">RCGNet: RGB-based Category-Level 6D Object Pose Estimation with Geometric Guidance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>本文提出了一种仅基于RGB图像的类别级6D物体姿态估计方法RCGNet，采用DINOv2视觉Transformer作为骨干网络，直接从RGB图像预测物体几何特征，并通过几何特征指导算法提高几何特征预测的准确性。方法融合先验点云与图像特征，无需深度数据，利用RANSAC-PnP实现姿态解算，并在CAMERA25与REAL275等数据集上显著优于同类RGB方法，实现了高效且准确的类别级物体姿态估计。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/RCGNet_RGB-based_Category-Level_6D_Object_Pose_Estimation_with_Geometric_Guidance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-Bridging-Clear-and-Adverse-Driving-Conditions-Domain-Adaptation-with-Simulation-Diffusion-and-GANs"><a href="#50-Bridging-Clear-and-Adverse-Driving-Conditions-Domain-Adaptation-with-Simulation-Diffusion-and-GANs" class="headerlink" title="50. Bridging Clear and Adverse Driving Conditions: Domain Adaptation with Simulation, Diffusion, and GANs"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Bridging_Clear_and_Adverse_Driving_Conditions.pdf">Bridging Clear and Adverse Driving Conditions: Domain Adaptation with Simulation, Diffusion, and GANs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bosch Center for Artificial Intelligence</span></p>
<p>本论文针对自动驾驶在恶劣环境下感知能力下降的问题，提出了一种无需目标域数据的多阶段域适应数据生成管线，结合仿真（CARLA）、扩散模型（ALDM）和GAN（DAUNIT），将标注的晴天图像转化为雾、雨、雪及夜间等多种恶劣天气高保真合成图像。通过自适应图像融合提升真实感，利用仿真+真实数据混合训练增强泛化能力，在ACDC数据集上语义分割提升1.85%，夜间场景提升4.62%，验证了方法对自动驾驶感知鲁棒性的增益。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Bridging_Clear_and_Adverse_Driving_Conditions_Domain_Adaptation_with_Simulation_Diffusion_and_GANs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-Temporal-Conditional-Referring-Video-Object-Segmentation-with-Noise-Free-Text-to-Video-Diffusion-Model"><a href="#51-Temporal-Conditional-Referring-Video-Object-Segmentation-with-Noise-Free-Text-to-Video-Diffusion-Model" class="headerlink" title="51. Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Temporal-Conditional_Referring_Video_Object_Segmentation_with_Noise-Free_Text-to-Video_Diffusion_Mod.pdf">Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Soochow University</span></p>
<p>该论文提出了一种新型时序条件指代视频目标分割模型，利用噪声消除的文本到视频扩散模型进行特征提取，并引入Hybrid CondDot（HCD）分割头实现并行融合Segmentation方法，显著提升边界分割能力和时序一致性。实验结果显示，去除传统噪声预测模块和创新的时序上下文掩码优化模块（TCMR），使模型在四大RVOS基准上均取得了最优性能，提升了分割精度与稳定性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Temporal-Conditional_Referring_Video_Object_Segmentation_with_Noise-Free_Text-to-Video_Diffusion_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-Color-Spike-Data-Generation-via-Bio-inspired-Neuron-like-Encoding-with-an-Artificial-Photoreceptor-Layer"><a href="#52-Color-Spike-Data-Generation-via-Bio-inspired-Neuron-like-Encoding-with-an-Artificial-Photoreceptor-Layer" class="headerlink" title="52. Color Spike Data Generation via Bio-inspired Neuron-like Encoding with an Artificial Photoreceptor Layer"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Color_Spike_Data_Generation_via_Bio-inspired_Neuron-like_Encoding_with_an_Artificial_Photoreceptor_L.pdf">Color Spike Data Generation via Bio-inspired Neuron-like Encoding with an Artificial Photoreceptor Layer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fu Jen Catholic University</span></p>
<p>该论文提出了一种基于生物神经元原理的神经元式编码方法，并结合人工光感受器层，将静态彩色图像转化为包含色彩与亮度信息的脉冲数据，用于提升脉冲神经网络（SNNs）在视觉任务中的表现。实验结果表明，这种信息更丰富的脉冲编码方法不仅提高了SNN的分类准确率，同时保持了神经形态计算的低功耗优势，验证了其在信息密度和能效上的可行性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Color_Spike_Data_Generation_via_Bio-inspired_Neuron-like_Encoding_with_an_Artificial_Photoreceptor_Layer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-FLAIR-Frequency-and-Locality-Aware-Implicit-Neural-Representations"><a href="#53-FLAIR-Frequency-and-Locality-Aware-Implicit-Neural-Representations" class="headerlink" title="53. FLAIR: Frequency- and Locality-Aware Implicit Neural Representations"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/FLAIR__Frequency-_and_Locality-Aware_Implicit_Neural_Representations.pdf">FLAIR: Frequency- and Locality-Aware Implicit Neural Representations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chung-Ang University</span></p>
<p>该论文提出了FLAIR框架，引入RC-GAUSS激活函数和Wavelet-Energy-Guided Encoding（WEGE）以提升隐式神经表示（INR）对频率选择性和空间定位能力。通过RC-GAUSS实现精确的频域选择与时域定位，WEGE则利用小波变换计算区域自适应频率信息，二者共同有效缓解传统INR的频谱偏置问题。在2D图像重建、超分辨率、去噪和3D重建等任务上，FLAIR均显著优于现有方法，能够更好地捕捉高频细节。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/FLAIR_Frequency-_and_Locality-Aware_Implicit_Neural_Representations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-Calibrating-Biased-Distribution-in-VFM-derived-Latent-Space-via-Cross-Domain-Geometric-Consistency"><a href="#54-Calibrating-Biased-Distribution-in-VFM-derived-Latent-Space-via-Cross-Domain-Geometric-Consistency" class="headerlink" title="54. Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Calibrating_Biased_Distribution_in_VFM-derived_Latent_Space_via_Cross-Domain_Geometric_Consistency.pdf">Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Gaoling School of Artificial Intelligence, Renmin University of China</span></p>
<p>本论文提出了一种基于视觉基础模型（如CLIP、DINOv2）嵌入空间几何一致性的分布校准新范式，核心方法利用跨域几何一致性，提取全局分布的几何信息，在联邦学习中通过安全聚合本地协方差矩阵实现全局分布重构，在长尾识别中通过匹配外部数据集的类别几何形态恢复尾类分布。实验表明，该几何知识指导的分布校准框架显著提升了联邦学习和长尾识别等数据受限场景下的模型表现，具有良好的适应性和通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Calibrating_Biased_Distribution_in_VFM-derived_Latent_Space_via_Cross-Domain_Geometric_Consistency.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-2D-Gaussians-Meet-Visual-Tokenizer"><a href="#55-2D-Gaussians-Meet-Visual-Tokenizer" class="headerlink" title="55. 2D Gaussians Meet Visual Tokenizer"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/2D_Gaussians_Meet_Visual_Tokenizer.pdf">2D Gaussians Meet Visual Tokenizer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huazhong University of Science and Technology</span></p>
<p>本文提出了一种新的视觉分词器架构Visual Gaussian Quantization (VGQ)，通过将2D高斯分布显式引入视觉分词流程，增强结构建模能力。VGQ采用双分支结构，结合VQ分支的外观信息和2DGS分支的几何结构信息，实验在ImageNet等数据集上实现了重建性能新SOTA（rFID 0.556），尤其在复杂结构和文本区域表现优越。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/2D_Gaussians_Meet_Visual_Tokenizer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-AdaptiveAE-An-Adaptive-Exposure-Strategy-for-HDR-Capturing-in-Dynamic-Scenes"><a href="#56-AdaptiveAE-An-Adaptive-Exposure-Strategy-for-HDR-Capturing-in-Dynamic-Scenes" class="headerlink" title="56. AdaptiveAE: An Adaptive Exposure Strategy for HDR Capturing in Dynamic Scenes"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/AdaptiveAE__An_Adaptive_Exposure_Strategy_for_HDR_Capturing_in_Dynamic_Scenes.pdf">AdaptiveAE: An Adaptive Exposure Strategy for HDR Capturing in Dynamic Scenes</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai AI Laboratory</span></p>
<p>AdaptiveAE提出了一种基于深度强化学习的自适应曝光控制方法，通过三阶段顺序优化策略自动预测每帧的ISO和快门速度，结合Blur-aware数据合成管线对动态场景中的HDR成像进行曝光融合。实验结果显示，AdaptiveAE在动态场景中能有效平衡噪声与运动模糊，HDR重建质量优于现有方法，兼具高鲁棒性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/AdaptiveAE_An_Adaptive_Exposure_Strategy_for_HDR_Capturing_in_Dynamic_Scenes.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-CORENet-Cross-Modal-4D-Radar-Denoising-Network-with-LiDAR-Supervision-for-Autonomous-Driving"><a href="#57-CORENet-Cross-Modal-4D-Radar-Denoising-Network-with-LiDAR-Supervision-for-Autonomous-Driving" class="headerlink" title="57. CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/CORENet__Cross-Modal_4D_Radar_Denoising_Network_with_LiDAR_Supervision_for_Autonomous_Driving.pdf">CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Computing Technology, Chinese Academy of Sciences</span></p>
<p>本文提出了CORENet，一种结合LiDAR监督的4D雷达点云去噪网络，核心包括分层多尺度去噪模块（HMSD-Net）和跨模态监督机制。方法在训练阶段利用LiDAR点云生成监督掩码，对4D雷达点云去噪，推理阶段仅需雷达数据，极大提升了噪声点云的物体检测精度。实验在Dual-Radar数据集上，集成CORENet的检测框架平均精度提升7%-18%，显著优于传统和现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/CORENet_Cross-Modal_4D_Radar_Denoising_Network_with_LiDAR_Supervision_for_Autonomous_Driving.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-Distribution-Aware-Hadamard-Quantization-for-Hardware-Efficient-Implicit-Neural-Representations"><a href="#58-Distribution-Aware-Hadamard-Quantization-for-Hardware-Efficient-Implicit-Neural-Representations" class="headerlink" title="58. Distribution-Aware Hadamard Quantization for Hardware-Efficient Implicit Neural Representations"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Distribution-Aware_Hadamard_Quantization_for_Hardware-Efficient_Implicit_Neural_Representations.pdf">Distribution-Aware Hadamard Quantization for Hardware-Efficient Implicit Neural Representations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Hong Kong</span></p>
<p>本文提出DHQ，一种面向隐式神经表示（INR）中权重和激活分布的Hadamard变换量化方法，通过将不同层的数据分布标准化为统一的钟形分布，便于使用标准量化器实现高效硬件部署。结论表明，DHQ在图像重构任务中能大幅提升硬件效率（延迟降低32.7%、能耗降低40.1%、资源利用降低98.3%），同时保持接近全精度的重建性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Distribution-Aware_Hadamard_Quantization_for_Hardware-Efficient_Implicit_Neural_Representations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-Vision-Transformers-for-Kidney-Stone-Image-Classification-A-Comparative-Study-with-CNNs"><a href="#59-Vision-Transformers-for-Kidney-Stone-Image-Classification-A-Comparative-Study-with-CNNs" class="headerlink" title="59. Vision Transformers for Kidney Stone Image Classification: A Comparative Study with CNNs"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Vision_Transformers_for_Kidney_Stone_Image_Classification__A_Comparative_Study_with_CNNs.pdf">Vision Transformers for Kidney Stone Image Classification: A Comparative Study with CNNs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CINVESTAV, Guadalajara, Mexico</span></p>
<p>本论文提出了使用预训练的Vision Transformer (ViT)模型对肾结石内镜图像进行分类的方法，并与传统的卷积神经网络（CNN）架构进行了系统对比。通过在两个肾结石图像数据集（CCD相机和内窥镜采集）上训练和测试，ViT在准确率、F1分数及特征分辨率等方面均显著优于ResNet50等CNN模型，尤其在复杂和易混淆的图像子集表现突出。结论显示ViT模型具有更强的特征表达能力和分类鲁棒性，为临床自动化肾结石识别提供了更优的技术方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Vision_Transformers_for_Kidney_Stone_Image_Classification_A_Comparative_Study_with_CNNs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-Bridging-the-Gap-Doubles-Badminton-Analysis-with-Singles-Trained-Models"><a href="#60-Bridging-the-Gap-Doubles-Badminton-Analysis-with-Singles-Trained-Models" class="headerlink" title="60. Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Bridging_the_Gap__Doubles_Badminton_Analysis_with_Singles-Trained_Models.pdf">Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Soongsil University</span></p>
<p>本论文提出了一种将基于单打训练的姿态识别模型迁移到双打羽毛球分析的方法，结合ViT-Pose关键点提取、ST-GCN对时空骨架特征的对比学习嵌入，以及自定义多目标跟踪算法以解决ID切换问题，最终通过Transformer分类器进行击球识别。实验结果显示，该方法在无需额外标注双打数据的情况下实现了双打场景下的有效击球识别，具备较强的跨场景迁移能力，为双打分析和数据集建设奠定了基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Bridging_the_Gap_Doubles_Badminton_Analysis_with_Singles-Trained_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-MINR-Efficient-Implicit-Neural-Representations-for-Multi-Image-Encoding"><a href="#61-MINR-Efficient-Implicit-Neural-Representations-for-Multi-Image-Encoding" class="headerlink" title="61. MINR: Efficient Implicit Neural Representations for Multi-Image Encoding"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MINR__Efficient_Implicit_Neural_Representations_for_Multi-Image_Encoding.pdf">MINR: Efficient Implicit Neural Representations for Multi-Image Encoding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Hong Kong</span></p>
<p>该论文提出了MINR框架，通过分析多图像隐式神经表示（INR）网络的层权重分布，相似的中间层被多个图像共享，仅输入、输出层和额外投影层保持图像特异性，从而高效编码多张图片。实验表明，MINR在保持重建性能（PSNR高于34dB）的同时，参数量较传统独立INR方法最多可减少60%，有效提升存储与计算效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MINR_Efficient_Implicit_Neural_Representations_for_Multi-Image_Encoding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="62-Unintended-Misalignment-from-Agentic-Fine-Tuning-Risks-and-Mitigation"><a href="#62-Unintended-Misalignment-from-Agentic-Fine-Tuning-Risks-and-Mitigation" class="headerlink" title="62. Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Unintended_Misalignment_from_Agentic_Fine-Tuning__Risks_and_Mitigation.pdf">Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KAIST</span></p>
<p>本文发现将大型语言模型（LLM）在代理任务上微调会导致模型安全性下降，增加执行有害任务的风险。为此，作者提出了Prefix Injection Guard (PING)方法，通过自动生成自然语言前缀引导模型拒绝有害请求，同时保持在正常任务上的性能。实验结果表明，PING能显著提升微调后LLM代理的安全性，在网页导航和代码生成领域均有效，且不影响其任务能力，机制分析也揭示了前缀在行为修正中的关键作用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Unintended_Misalignment_from_Agentic_Fine-Tuning_Risks_and_Mitigation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-Ask-Good-Questions-for-Large-Language-Models"><a href="#63-Ask-Good-Questions-for-Large-Language-Models" class="headerlink" title="63. Ask Good Questions for Large Language Models"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Ask_Good_Questions_for_Large_Language_Models.pdf">Ask Good Questions for Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">China University of Petroleum-Beijing</span></p>
<p>该论文提出了Ask-Good-Question (AGQ) 框架，结合了Concept-Enhanced Item Response Theory (CEIRT)模型与大语言模型，动态评估用户多维知识状态，并生成针对性引导问题，提升信息检索效率。实验结果显示，AGQ框架生成的问题在准确率、文本相似度及知识增益方面显著优于Zero-shot和CoT等基线方法，接近人类专家水平，具备跨模型通用性和适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Ask_Good_Questions_for_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-Chunks-as-Arms-Multi-Armed-Bandit-Guided-Sampling-for-Long-Context-LLM-Preference-Optimization"><a href="#64-Chunks-as-Arms-Multi-Armed-Bandit-Guided-Sampling-for-Long-Context-LLM-Preference-Optimization" class="headerlink" title="64. Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Chunks_as_Arms__Multi-Armed_Bandit-Guided_Sampling_for_Long-Context_LLM_Preference_Optimization.pdf">Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Department of Computer Science and Technology, Northeastern University</span></p>
<p>本文提出LongMab-PO框架，将长文本上下文划分为多个chunk，并采用多臂老虎机（MAB）策略，以UCB算法动态采样最有信息的chunk组合，引导LLM生成高质量多样化的响应，并构建偏好数据对进行DPO优化。实验表明，LongMab-PO显著提升了长上下文任务中LLM的理解与推理能力，在多个基准上优于现有SFT和DPO方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Chunks_as_Arms_Multi-Armed_Bandit-Guided_Sampling_for_Long-Context_LLM_Preference_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-ChronoLLM-Customizing-Language-Models-for-Physics-Based-Simulation-Code-Generation"><a href="#65-ChronoLLM-Customizing-Language-Models-for-Physics-Based-Simulation-Code-Generation" class="headerlink" title="65. ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ChronoLLM__Customizing_Language_Models_for_Physics-Based_Simulation_Code_Generation.pdf">ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Wisconsin-Madison</span></p>
<p>该论文提出ChronoLLM框架，通过持续预训练和监督微调等方法，对大语言模型（LLM）进行定制，使其能够自动生成用于PyChrono物理仿真工具的Python脚本。实验结果表明，经过微调后的ChronoLLM在生成仿真代码的准确性和实用性方面明显优于预训练模型和仅用上下文学习的方法，可有效提升领域专家使用仿真工具的效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ChronoLLM_Customizing_Language_Models_for_Physics-Based_Simulation_Code_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-MME-SCI-A-Comprehensive-and-Challenging-Science-Benchmark-for-Multimodal-Large-Language-Models"><a href="#66-MME-SCI-A-Comprehensive-and-Challenging-Science-Benchmark-for-Multimodal-Large-Language-Models" class="headerlink" title="66. MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MME-SCI__A_Comprehensive_and_Challenging_Science_Benchmark_for_Multimodal_Large_Language_Models.pdf">MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了MME-SCI，一个面向多模态大语言模型（MLLMs）的科学推理基准，涵盖数学、物理、化学、生物四大学科、五种语言和三种输入模态（文本、图像、图文混合），并对知识点进行细粒度标注。通过对20个主流MLLMs的实验证明，MME-SCI能有效区分模型性能，揭示模型在多语言一致性、模态适应性、推理能力和知识掌握上的不足，推动科学领域多模态模型的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MME-SCI_A_Comprehensive_and_Challenging_Science_Benchmark_for_Multimodal_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-RotBench-Evaluating-Multimodal-Large-Language-Models-on-Identifying-Image-Rotation"><a href="#67-RotBench-Evaluating-Multimodal-Large-Language-Models-on-Identifying-Image-Rotation" class="headerlink" title="67. RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/RotBench__Evaluating_Multimodal_Large_Language_Models_on_Identifying_Image_Rotation.pdf">RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">UNC Chapel Hill</span></p>
<p>该论文提出ROTBENCH基准，系统评测多模态大语言模型（MLLMs）在识别图像0°、90°、180°和270°旋转任务上的空间推理能力。实验发现，主流MLLMs能准确识别正向或倒置（0°&#x2F;180°）图像，但对90°与270°区分力极弱，辅以辅助信息或链式思考提示提升有限；仅部分推理型模型利用旋转网格和投票机制有适度提升。结论指出，MLLMs在空间定向推理方面存在明显短板，需在训练中增强旋转感知。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/RotBench_Evaluating_Multimodal_Large_Language_Models_on_Identifying_Image_Rotation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-Learning-to-Use-AI-for-Learning-How-Can-We-Effectively-Teach-and-Measure-Prompting-Literacy-for-K–12-Students"><a href="#68-Learning-to-Use-AI-for-Learning-How-Can-We-Effectively-Teach-and-Measure-Prompting-Literacy-for-K–12-Students" class="headerlink" title="68. Learning to Use AI for Learning: How Can We Effectively Teach and Measure Prompting Literacy for K–12 Students?"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Learning_to_Use_AI_for_Learning__How_Can_We_Effectively_Teach_and_Measure_Prompting_Literacy_for_K-1.pdf">Learning to Use AI for Learning: How Can We Effectively Teach and Measure Prompting Literacy for K–12 Students?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carnegie Mellon University</span></p>
<p>本文提出并实现了一套基于大语言模型（LLM）的K-12学生提示素养教学与评测模块，包含情境化练习与AI自动评分反馈，目标是培养学生负责任和高效使用AI聊天机器人。两轮真实课堂实验表明，AI自动评分系统可以高质量评估学生写作的提示，模块能提升学生在提示写作中的背景信息嵌入能力和使用AI学习的信心，且经过迭代后，True&#x2F;False与开放式问题能更有效地测量提示素养，显示出方法的可扩展性和应用前景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Learning_to_Use_AI_for_Learning_How_Can_We_Effectively_Teach_and_Measure_Prompting_Literacy_for_K%E2%80%9312_Students.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-Prompt-Orchestration-Markup-Language"><a href="#69-Prompt-Orchestration-Markup-Language" class="headerlink" title="69. Prompt Orchestration Markup Language"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Prompt_Orchestration_Markup_Language.pdf">Prompt Orchestration Markup Language</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Microsoft Research</span></p>
<p>该论文提出了POML（Prompt Orchestration Markup Language），一种面向大语言模型（LLM）应用开发的结构化标记语言。POML采用HTML类组件化结构，具备专用标签用于文档、表格、图片等多模态数据集成，并引入CSS风格的样式系统实现内容与格式分离，显著降低格式敏感性。作者同时开发了IDE工具套件（包括VSCode插件、SDKs），支持实时预览、诊断、版本管理和多语言集成。通过两个案例（iOS智能体应用PomLink和表格问答TableQA）及用户研究，验证POML在复杂LLM应用和数据密集型场景中的开发效率、可维护性和性能提升。结论：POML显著提升了LLM提示工程的结构化、可复用性和开发协作，推动智能体应用的快速实现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Prompt_Orchestration_Markup_Language.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-The-Collaboration-Paradox-Why-Generative-AI-Requires-Both-Strategic-Intelligence-and-Operational-Stability-in-Supply-Chain-Management"><a href="#70-The-Collaboration-Paradox-Why-Generative-AI-Requires-Both-Strategic-Intelligence-and-Operational-Stability-in-Supply-Chain-Management" class="headerlink" title="70. The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/The_Collaboration_Paradox__Why_Generative_AI_Requires_Both_Strategic_Intelligence_and_Operational_St.pdf">The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Indian Institute of Technology (IIT) Kharagpur</span></p>
<p>本文提出了“协作悖论”，通过多阶段仿真实验分析了基于大语言模型（LLM）的生成式AI代理在多级供应链管理中的行为，发现单靠智能协作AI（如VMI）可能导致系统灾难性失稳（如囤货效应）。最终研究证明，只有将高层AI策略生成与低层协同执行（信息共享与主动补货）结合，才能实现供应链的韧性与稳定，相关方法可自动生成并评估多策略方案，为未来AI驱动的业务分析系统设计提供了蓝图。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/The_Collaboration_Paradox_Why_Generative_AI_Requires_Both_Strategic_Intelligence_and_Operational_Stability_in_Supply_Chain_Management.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-InPars-Supercharging-Synthetic-Data-Generation-for-Information-Retrieval-Systems"><a href="#71-InPars-Supercharging-Synthetic-Data-Generation-for-Information-Retrieval-Systems" class="headerlink" title="71. InPars+: Supercharging Synthetic Data Generation for Information Retrieval Systems"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/InPars+__Supercharging_Synthetic_Data_Generation_for_Information_Retrieval_Systems.pdf">InPars+: Supercharging Synthetic Data Generation for Information Retrieval Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Amsterdam</span></p>
<p>本文在复现InPars Toolkit合成查询生成流程的基础上，提出两项关键扩展：一是通过对生成器大语言模型采用对比偏好优化（CPO）方法微调，提升生成查询的质量；二是利用DSPy框架用动态链式思维（CoT）优化prompt，替代静态模板，从而改进合成数据生成流程。实验结果表明，这两项改进可减少过滤需求并提升下游信息检索性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/InPars%2B_Supercharging_Synthetic_Data_Generation_for_Information_Retrieval_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-Structured-Agentic-Workflows-for-Financial-Time-Series-Modeling-with-LLMs-and-Reflective-Feedback"><a href="#72-Structured-Agentic-Workflows-for-Financial-Time-Series-Modeling-with-LLMs-and-Reflective-Feedback" class="headerlink" title="72. Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Structured_Agentic_Workflows_for_Financial_Time-Series_Modeling_with_LLMs_and_Reflective_Feedback.pdf">Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Singapore</span></p>
<p>该论文提出TS-Agent，一种基于大语言模型的模块化智能体框架，通过结构化知识库、案例推理和反馈驱动的决策，自动化金融时序建模流程。TS-Agent在模型选择、代码优化和超参数微调等多阶段中结合专家知识和实验反馈，显著提升了预测精度、稳健性和可审计性，并在多个金融预测与生成任务上优于现有AutoML及智能体基线。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Structured_Agentic_Workflows_for_Financial_Time-Series_Modeling_with_LLMs_and_Reflective_Feedback.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-Prompt-Based-One-Shot-Exact-Length-Controlled-Generation-with-LLMs"><a href="#73-Prompt-Based-One-Shot-Exact-Length-Controlled-Generation-with-LLMs" class="headerlink" title="73. Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Prompt-Based_One-Shot_Exact_Length-Controlled_Generation_with_LLMs.pdf">Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Taiwan University</span></p>
<p>本文提出了一种基于倒计时标记和显式计数规则的Prompt工程方法（CAPEL），无需微调或多次采样即可实现大语言模型（LLM）在英中两种语言下的精确长度控制。实验表明，该方法将多种主流模型在多任务场景下的长度合规率从30%提升至95%以上，同时保持生成内容质量，证实仅靠Prompt就能实现严格的长度控制。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Prompt-Based_One-Shot_Exact_Length-Controlled_Generation_with_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-BetaWeb-Towards-a-Blockchain-enabled-Trustworthy-Agentic-Web"><a href="#74-BetaWeb-Towards-a-Blockchain-enabled-Trustworthy-Agentic-Web" class="headerlink" title="74. BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/BetaWeb__Towards_a_Blockchain-enabled_Trustworthy_Agentic_Web.pdf">BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Innovation Institute</span></p>
<p>本文提出了BetaWeb架构，将区块链技术与大语言模型驱动的多智能体系统（LaMAS）深度融合，解决Agentic Web在开放协作、隐私保护、数据管理和价值度量等方面的核心挑战。具体方法包括通过区块链实现身份验证、不可篡改的过程记录与智能合约驱动的激励分配，并提出了五阶段进化路线，为智能体自治与去中心化治理奠定基础。结论显示，区块链赋能的BetaWeb能有效提升LaMAS系统的信任度、自治性和可扩展性，为智能经济和智能社会的构建提供坚实基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/BetaWeb_Towards_a_Blockchain-enabled_Trustworthy_Agentic_Web.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-Expertise-aware-Multi-LLM-Recruitment-and-Collaboration-for-Medical-Decision-Making"><a href="#75-Expertise-aware-Multi-LLM-Recruitment-and-Collaboration-for-Medical-Decision-Making" class="headerlink" title="75. Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Expertise-aware_Multi-LLM_Recruitment_and_Collaboration_for_Medical_Decision-Making.pdf">Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hangzhou Dianzi University</span></p>
<p>本文提出了一种名为EMRC（Expertise-aware Multi-LLM Recruitment and Collaboration）的新型框架，通过两阶段流程提升医学决策系统的准确性与可靠性。第一阶段基于公开医学数据集，构建LLM专业能力表，实现针对具体医学查询的动态最优LLM专家代理招募；第二阶段则融合各代理自评置信度，并通过置信度加权和对抗验证机制，集成多代理诊断结果，提升决策一致性和鲁棒性。实验表明，该方法在多个医学决策数据集上均超越了单一及多LLM现有方法，有效提升了诊断准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Expertise-aware_Multi-LLM_Recruitment_and_Collaboration_for_Medical_Decision-Making.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-Prediction-is-not-Explanation-Revisiting-the-Explanatory-Capacity-of-Mapping-Embeddings"><a href="#76-Prediction-is-not-Explanation-Revisiting-the-Explanatory-Capacity-of-Mapping-Embeddings" class="headerlink" title="76. Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Prediction_is_not_Explanation__Revisiting_the_Explanatory_Capacity_of_Mapping_Embeddings.pdf">Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universität Hamburg</span></p>
<p>本论文系统分析了通过映射词嵌入到人工注释的语义特征（feature norms）来解释大型语言模型（LLM）知识的方法，采用了Partial Least Squares Regression和Feed Forward Neural Networks两种映射技术，并设计多组消融实验和上界分析。结果表明，特征预测准确性主要反映了数据的几何结构和方法论上界而非真实语义知识，映射方法解释的是向量空间的几何相似性而非真实的语义属性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Prediction_is_not_Explanation_Revisiting_the_Explanatory_Capacity_of_Mapping_Embeddings.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-Generics-and-Default-Reasoning-in-Large-Language-Models"><a href="#77-Generics-and-Default-Reasoning-in-Large-Language-Models" class="headerlink" title="77. Generics and Default Reasoning in Large Language Models"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Generics_and_Default_Reasoning_in_Large_Language_Models.pdf">Generics and Default Reasoning in Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Oxford</span></p>
<p>本文系统评估了28种大型语言模型（LLM）在20种默认（defeasible）推理模式下处理通用泛化（generics）的能力，设计了多样化的推理基准并对模型在零样本、少样本和链式思考提示下表现进行了分析。研究发现，前沿LLM在默认推理任务上准确率高，但模型之间和提示方式之间表现差异大，链式思考提示反而导致性能下降，且大多数模型难以区分可撤销推理与演绎推理以及泛化与全称命题，凸显当前LLM在非单调逻辑推理上的局限。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Generics_and_Default_Reasoning_in_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-HumanPCR-Probing-MLLM-Capabilities-in-Diverse-Human-Centric-Scenes"><a href="#78-HumanPCR-Probing-MLLM-Capabilities-in-Diverse-Human-Centric-Scenes" class="headerlink" title="78. HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/HumanPCR__Probing_MLLM_Capabilities_in_Diverse_Human-Centric_Scenes.pdf">HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</span></p>
<p>该论文提出了HumanPCR，一套针对多模态大语言模型（MLLMs）在多样化人类场景下视觉理解能力的评测基准，包括感知、理解、推理三个层级，涵盖6000多个由人工验证的多选题和442个开放式视频推理题，并配有人类标注的推理链（CoT）。实验结果表明，现有MLLMs在人类中心视觉理解尤其是细粒度感知、时序理解和复杂推理方面存在明显挑战，模型普遍依赖于问题线索而难以主动提取关键视觉证据，即使采用更大视觉上下文和测试时推理策略，提升也有限。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/HumanPCR_Probing_MLLM_Capabilities_in_Diverse_Human-Centric_Scenes.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-Neuro-Symbolic-Artificial-Intelligence-Towards-Improving-the-Reasoning-Abilities-of-Large-Language-Models"><a href="#79-Neuro-Symbolic-Artificial-Intelligence-Towards-Improving-the-Reasoning-Abilities-of-Large-Language-Models" class="headerlink" title="79. Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Neuro-Symbolic_Artificial_Intelligence__Towards_Improving_the_Reasoning_Abilities_of_Large_Language_.pdf">Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本文系统综述了神经符号方法在提升大语言模型（LLM）推理能力方面的最新进展。通过从Symbolic→LLM（数据生成与增强）、LLM→Symbolic（引入符号模块&#x2F;工具提升推理步骤精度）、LLM+Symbolic（端到端融合及符号反馈）三大视角，梳理了关键技术路径及典型工作。结论认为，神经符号方法在补足数据稀缺、提升推理严谨性及优化推理过程表达等方面具有独特优势，是推动LLM向强推理智能（AGI）演进的重要方向，但多模态推理、混合架构及理论基础等仍面临挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Neuro-Symbolic_Artificial_Intelligence_Towards_Improving_the_Reasoning_Abilities_of_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-Input-Time-Scaling"><a href="#80-Input-Time-Scaling" class="headerlink" title="80. Input Time Scaling"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Input_Time_Scaling.pdf">Input Time Scaling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Chinese Academy of Sciences</span></p>
<p>该论文提出了一种新的规模扩展范式——Input Time Scaling，通过在训练和测试阶段利用LLM元认知自动生成多样化persona修饰输入查询，极大提升了模型的数学推理能力。实验证明，无需高质量数据和复杂训练流程，仅用1k低过滤样本，经多策略训练-测试协同设计，在AIME24和AIME25数学推理任务上达到甚至超越现有SOTA水平，强调数据多样性和训练-测试协同比传统高质量、规模扩展更有效。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Input_Time_Scaling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-Who-Gets-the-Mic-Investigating-Gender-Bias-in-the-Speaker-Assignment-of-a-Speech-LLM"><a href="#81-Who-Gets-the-Mic-Investigating-Gender-Bias-in-the-Speaker-Assignment-of-a-Speech-LLM" class="headerlink" title="81. Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Who_Gets_the_Mic__Investigating_Gender_Bias_in_the_Speaker_Assignment_of_a_Speech-LLM.pdf">Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Uppsala University</span></p>
<p>该论文提出了一种利用发言人分配作为分析工具的方法，研究Speech-LLM（以Bark为例）在文本到语音转换中的性别偏见问题。通过构建“职业”和“性别色彩词”两个数据集，并结合基线实验，系统评估模型在不同文本输入下的默认发言人性别分配，结果显示Bark没有显著的系统性性别偏见，但在部分词语和职业上表现出性别倾向，并能从文本中推断性别信息。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Who_Gets_the_Mic_Investigating_Gender_Bias_in_the_Speaker_Assignment_of_a_Speech-LLM.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-A-Comparative-Study-of-Decoding-Strategies-in-Medical-Text-Generation"><a href="#82-A-Comparative-Study-of-Decoding-Strategies-in-Medical-Text-Generation" class="headerlink" title="82. A Comparative Study of Decoding Strategies in Medical Text Generation"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/A_Comparative_Study_of_Decoding_Strategies_in_Medical_Text_Generation.pdf">A Comparative Study of Decoding Strategies in Medical Text Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Science and Technology Politehnica Bucharest</span></p>
<p>本文系统对比了11种解码策略（包括beam search、greedy、top-k、top-p、min-p、DBS等）在医疗文本生成的五大任务（翻译、摘要、问答、对话、图像描述）中的表现，涵盖通用和医学专用大模型。结果显示，确定性搜索（如beam search）通常优于随机采样法，输出质量更高但推理速度较慢，而医学领域大模型对解码策略更敏感，且整体性能并不总优于通用模型。结论指出，解码策略选择对医疗应用效果影响巨大，需针对任务精细调优。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/A_Comparative_Study_of_Decoding_Strategies_in_Medical_Text_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-Agentic-DraCor-and-the-Art-of-Docstring-Engineering-Evaluating-MCP-empowered-LLM-Usage-of-the-DraCor-API"><a href="#83-Agentic-DraCor-and-the-Art-of-Docstring-Engineering-Evaluating-MCP-empowered-LLM-Usage-of-the-DraCor-API" class="headerlink" title="83. Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Agentic_DraCor_and_the_Art_of_Docstring_Engineering__Evaluating_MCP-empowered_LLM_Usage_of_the_DraCo.pdf">Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Potsdam</span></p>
<p>本文实现并评估了Model Context Protocol（MCP）服务器在DraCor平台上的应用，使大型语言模型（LLM）能够自主调用DraCor API工具进行文学数据检索与分析。通过定性和定量实验，研究了LLM的工具选择、调用效率和可靠性，发现“docstring工程”对优化LLM工具交互至关重要。实验结果表明，MCP架构下的LLM具备数字人文领域研究的潜力，但在数据处理能力和工具使用知识积累方面仍有限，基础设施需进一步完善。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Agentic_DraCor_and_the_Art_of_Docstring_Engineering_Evaluating_MCP-empowered_LLM_Usage_of_the_DraCor_API.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-DEPTH-BREADTH-SYNERGY-IN-RLVR-UNLOCKING-LLM-REASONING-GAINS-WITH-ADAPTIVE-EXPLORATION"><a href="#84-DEPTH-BREADTH-SYNERGY-IN-RLVR-UNLOCKING-LLM-REASONING-GAINS-WITH-ADAPTIVE-EXPLORATION" class="headerlink" title="84. DEPTH-BREADTH SYNERGY IN RLVR: UNLOCKING LLM REASONING GAINS WITH ADAPTIVE EXPLORATION"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Depth-Breadth_Synergy_in_RLVR__Unlocking_LLM_Reasoning_Gains_with_Adaptive_Exploration.pdf">DEPTH-BREADTH SYNERGY IN RLVR: UNLOCKING LLM REASONING GAINS WITH ADAPTIVE EXPLORATION</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>本文针对大语言模型（LLM）中的可验证奖励强化学习（RLVR）训练，系统分析了“深度”（难题探索）和“广度”（批量实例数）两个关键维度。提出了难度自适应采样算法DARS，通过多阶段采样提升对高难度问题的关注，并结合大批量训练（DARS-B），实现了Pass@1和Pass@K指标的双提升，验证了深度与广度在RLVR中的互补作用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/DEPTH-BREADTH_SYNERGY_IN_RLVR_UNLOCKING_LLM_REASONING_GAINS_WITH_ADAPTIVE_EXPLORATION.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-CausalPlan-Empowering-Efficient-LLM-Multi-Agent-Collaboration-Through-Causality-Driven-Planning"><a href="#85-CausalPlan-Empowering-Efficient-LLM-Multi-Agent-Collaboration-Through-Causality-Driven-Planning" class="headerlink" title="85. CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/CausalPlan__Empowering_Efficient_LLM_Multi-Agent_Collaboration_Through_Causality-Driven_Planning.pdf">CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Deakin University</span></p>
<p>本论文提出CausalPlan框架，通过结构化因果推理模型（Structural Causal Action, SCA）学习多智能体任务中行为和状态的因果图，进而在大语言模型（LLM）行动规划中引入因果分数重加权或因果备选动作，实现有效约束与提升协作。实验证明，CausalPlan能显著减少因果无效动作，提升AI-AI及人-AI多智能体协作表现，超越强RL基线，尤其对开源LLM提升明显。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/CausalPlan_Empowering_Efficient_LLM_Multi-Agent_Collaboration_Through_Causality-Driven_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions"><a href="#86-ViExam-Are-Vision-Language-Models-Better-than-Humans-on-Vietnamese-Multimodal-Exam-Questions" class="headerlink" title="86. ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ViExam__Are_Vision_Language_Models_Better_than_Humans_on_Vietnamese_Multimodal_Exam_Questions_.pdf">ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KAIST</span></p>
<p>本论文提出了第一个越南语多模态考试基准ViExam，包括2548道融合文本与图像的多领域教育题目，并系统评测了14个主流视觉语言模型（VLMs）在越南语多模态题上的表现。结果显示，SOTA VLMs平均准确率仅57.74%，明显低于人类平均水平（66.54%），仅有o3模型超越人类均值但仍远低于人类最佳，表明当前VLM在低资源语言的多模态推理上仍存在显著挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ViExam_Are_Vision_Language_Models_Better_than_Humans_on_Vietnamese_Multimodal_Exam_Questions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-Text2Weight-Bridging-Natural-Language-and-Neural-Network-Weight-Spaces"><a href="#87-Text2Weight-Bridging-Natural-Language-and-Neural-Network-Weight-Spaces" class="headerlink" title="87. Text2Weight: Bridging Natural Language and Neural Network Weight Spaces"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Text2Weight__Bridging_Natural_Language_and_Neural_Network_Weight_Spaces.pdf">Text2Weight: Bridging Natural Language and Neural Network Weight Spaces</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>该论文提出了一种名为T2W（Text2Weight）的扩散Transformer框架，通过将自然语言任务描述编码为文本特征，结合分块层次化神经网络权重和对称性约束、对抗训练，实现从文本直接生成高质量、任务相关且具有泛化能力的神经网络权重。实验表明，T2W在未见任务上表现优异，支持文本驱动的权重初始化、增强和模型融合，显著提升神经网络权重生成的实用性和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Text2Weight_Bridging_Natural_Language_and_Neural_Network_Weight_Spaces.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-PersonaVlog-Personalized-Multimodal-Vlog-Generation-with-Multi-Agent-Collaboration-and-Iterative-Self-Correction"><a href="#88-PersonaVlog-Personalized-Multimodal-Vlog-Generation-with-Multi-Agent-Collaboration-and-Iterative-Self-Correction" class="headerlink" title="88. PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/PersonaVlog__Personalized_Multimodal_Vlog_Generation_with_Multi-Agent_Collaboration_and_Iterative_Se.pdf">PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>该论文提出PersonaVlog框架，通过多模态大语言模型驱动的多智能体协作，自动生成个性化Vlog内容，包括视频、背景音乐和角色内心独白。方法包含多智能体内容生成、反馈回滚机制和主题化自动评测基准，显著提升了内容多样性、一致性及自动化评估能力。实验结果显示PersonaVlog在故事连贯性、角色一致性等多项指标上超过现有方法，推动了多模态个性化内容自动生成的研究进展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/PersonaVlog_Personalized_Multimodal_Vlog_Generation_with_Multi-Agent_Collaboration_and_Iterative_Self-Correction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-Toward-Better-EHR-Reasoning-in-LLMs-Reinforcement-Learning-with-Expert-Attention-Guidance"><a href="#89-Toward-Better-EHR-Reasoning-in-LLMs-Reinforcement-Learning-with-Expert-Attention-Guidance" class="headerlink" title="89. Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Toward_Better_EHR_Reasoning_in_LLMs__Reinforcement_Learning_with_Expert_Attention_Guidance.pdf">Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science, Peking University</span></p>
<p>本文提出EAG-RL框架，通过两阶段训练增强大语言模型对电子健康记录（EHR）的推理能力。第一阶段利用专家EHR模型引导的蒙特卡洛树搜索生成高质量推理轨迹进行策略初始化，第二阶段采用强化学习结合专家注意力对齐和熵自适应剪切优化推理策略。实验表明，EAG-RL在多个EHR预测任务上平均提升14.62%，并显著增强模型对输入扰动和跨域泛化的鲁棒性，显示了其在实际临床部署的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Toward_Better_EHR_Reasoning_in_LLMs_Reinforcement_Learning_with_Expert_Attention_Guidance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-Saudi-Dialect-ALLaM-LoRA-Fine-Tuning-for-Dialectal-Arabic-Generation"><a href="#90-Saudi-Dialect-ALLaM-LoRA-Fine-Tuning-for-Dialectal-Arabic-Generation" class="headerlink" title="90. Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Saudi-Dialect-ALLaM__LoRA_Fine-Tuning_for_Dialectal_Arabic_Generation.pdf">Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Umm Al-Qura University</span></p>
<p>本论文提出了一种基于LoRA的参数高效微调方法，使用私有的平衡合成指令-响应数据集对ALLaM-7B-Instruct-preview大模型进行沙特阿拉伯方言（Hijazi和Najdi）生成优化。核心方法包括Dialect-Token（显式方言标签）与No-Token（隐式推断）对比，实验结合外部方言分类器和多项文本质量指标，结果显示Dialect-Token模型显著提升方言正确率至84.2%，减少MSA泄漏至6.2%，并在人类和自动评测中超越多种主流阿拉伯LLM基线，验证了针对性数据与显式条件标签在阿拉伯方言大模型适配中的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Saudi-Dialect-ALLaM_LoRA_Fine-Tuning_for_Dialectal_Arabic_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-NovoMolGen-Rethinking-Molecular-Language-Model-Pretraining"><a href="#91-NovoMolGen-Rethinking-Molecular-Language-Model-Pretraining" class="headerlink" title="91. NovoMolGen: Rethinking Molecular Language Model Pretraining"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/NovoMolGen__Rethinking_Molecular_Language_Model_Pretraining.pdf">NovoMolGen: Rethinking Molecular Language Model Pretraining</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chandar Research Lab</span></p>
<p>本文提出NovoMolGen，一套基于Transformer的分子大语言模型（Mol-LLMs），在15亿分子大数据集上预训练，系统研究分子表征、分词策略、模型规模与数据集大小对分子生成性能的影响，并采用强化学习微调实现目标导向的分子设计。实验表明，预训练指标与下游性能相关性弱，模型性能在早期训练即趋于饱和，NovoMolGen在无约束分子生成和目标优化任务中均显著优于现有Mol-LLMs和专用生成模型，为高效分子建模提供了新基准。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/NovoMolGen_Rethinking_Molecular_Language_Model_Pretraining.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-ProMed-Shapley-Information-Gain-Guided-Reinforcement-Learning-for-Proactive-Medical-LLMs"><a href="#92-ProMed-Shapley-Information-Gain-Guided-Reinforcement-Learning-for-Proactive-Medical-LLMs" class="headerlink" title="92. ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ProMed__Shapley_Information_Gain_Guided_Reinforcement_Learning_for_Proactive_Medical_LLMs.pdf">ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出ProMed框架，通过引入Shapley Information Gain（SIG）奖励，结合合作博弈理论动态量化每个问题在临床诊断中的信息价值，采用两阶段强化学习流程：SIG引导的蒙特卡洛树搜索（MCTS）用于高质量数据初始化，SIG增强的策略优化实现奖励分配机制，针对性提升LLM主动信息获取能力。实验结果显示，ProMed在MedQA与CMB-Exam数据集上显著优于现有方法，准确率提升6.29%，较被动范式提升54.45%，并具备良好的跨域泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ProMed_Shapley_Information_Gain_Guided_Reinforcement_Learning_for_Proactive_Medical_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-LLM-Enhanced-Linear-Autoencoders-for-Recommendation"><a href="#93-LLM-Enhanced-Linear-Autoencoders-for-Recommendation" class="headerlink" title="93. LLM-Enhanced Linear Autoencoders for Recommendation"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/LLM-Enhanced_Linear_Autoencoders_for_Recommendation.pdf">LLM-Enhanced Linear Autoencoders for Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sungkyunkwan University</span></p>
<p>本文提出了L3AE模型，将大语言模型（LLM）生成的语义嵌入与协同过滤信号通过两阶段优化策略融合：首先用LLM生成的语义表征构建物品间相关矩阵，其次通过知识蒸馏式正则化，联合学习用户-物品交互权重矩阵，两阶段均采用闭式解以保证全局最优和计算效率。实验结果显示，L3AE在三大推荐数据集上显著超过现有LLM增强模型，尤其在长尾物品推荐上表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/LLM-Enhanced_Linear_Autoencoders_for_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-Revisiting-MLLM-Token-Technology-through-the-Lens-of-Classical-Visual-Coding"><a href="#94-Revisiting-MLLM-Token-Technology-through-the-Lens-of-Classical-Visual-Coding" class="headerlink" title="94. Revisiting MLLM Token Technology through the Lens of Classical Visual Coding"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Revisiting_MLLM_Token_Technology_through_the_Lens_of_Classical_Visual_Coding.pdf">Revisiting MLLM Token Technology through the Lens of Classical Visual Coding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Eastern Institute of Technology, Ningbo, China</span></p>
<p>本文系统性地将多模态大语言模型（MLLM）中的视觉Token技术（包括Token化、Token压缩、Token推理）与经典视觉编码理论进行模块化对比，提出统一的率-失真优化框架，并梳理两者在信息表达、冗余去除、上下文建模等流程上的异同。结论认为视觉编码原理可提升MLLM Token效率和鲁棒性，而MLLM Token范式也能反哺下一代语义视觉编码器，两领域应协同创新。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Revisiting_MLLM_Token_Technology_through_the_Lens_of_Classical_Visual_Coding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-ALIGN-Word-Association-Learning-for-Cross-Cultural-Generalization-in-Large-Language-Models"><a href="#95-ALIGN-Word-Association-Learning-for-Cross-Cultural-Generalization-in-Large-Language-Models" class="headerlink" title="95. ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ALIGN__Word_Association_Learning_for_Cross-Cultural_Generalization_in_Large_Language_Models.pdf">ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Melbourne</span></p>
<p>该论文提出基于母语者自由词语联想数据的参数高效微调方法，通过监督微调（SFT）和PPO强化学习分别在Llama-3.1-8B与Qwen-2.5-7B模型上，显著提升模型词语联想的准确性及心理属性贴近人类水平。结论显示，词语联想微调不仅提升词汇级表现，还显著促进模型在世界价值观调查中的文化价值对齐，尤其在目标文化知识稀缺时更为有效。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ALIGN_Word_Association_Learning_for_Cross-Cultural_Generalization_in_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-MAVIS-Multi-Objective-Alignment-via-Value-Guided-Inference-Time-Search"><a href="#96-MAVIS-Multi-Objective-Alignment-via-Value-Guided-Inference-Time-Search" class="headerlink" title="96. MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/MAVIS__Multi-Objective_Alignment_via_Value-Guided_Inference-Time_Search.pdf">MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Texas A&amp;M University</span></p>
<p>MAVIS是一种轻量级推理时多目标对齐方法，通过训练每个目标的小型价值模型（value models），并在推理时根据用户指定的权重组合这些模型，动态调整LLM的输出分布，无需微调基础模型权重。实验表明，MAVIS在多目标权衡和推理效率上优于微调和模型集成等主流方法，可达到接近于针对用户偏好微调模型的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/MAVIS_Multi-Objective_Alignment_via_Value-Guided_Inference-Time_Search.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="97-How-Usable-is-Automated-Feature-Engineering-for-Tabular-Data"><a href="#97-How-Usable-is-Automated-Feature-Engineering-for-Tabular-Data" class="headerlink" title="97. How Usable is Automated Feature Engineering for Tabular Data?"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/How_Usable_is_Automated_Feature_Engineering_for_Tabular_Data_.pdf">How Usable is Automated Feature Engineering for Tabular Data?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Freiburg</span></p>
<p>本论文系统性调研了53种自动特征工程（AutoFE）方法，从可用性角度出发，基于代码、文档、社区活跃度及资源约束支持等九项指标，实测评估了其在实际应用中的可用性。研究发现，绝大多数AutoFE方法难以直接使用，缺乏文档和社区支持，且无一支持用户设置时间和内存限制，导致目前尚无真正适合工业应用的AutoFE方法。结论是，社区应将更多关注点从方法创新转向可用性改进，以推动AutoFE在实际中的广泛应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/How_Usable_is_Automated_Feature_Engineering_for_Tabular_Data.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-Automated-Energy-Aware-Time-Series-Model-Deployment-on-Embedded-FPGAs-for-Resilient-Combined-Sewer-Overflow-Management"><a href="#98-Automated-Energy-Aware-Time-Series-Model-Deployment-on-Embedded-FPGAs-for-Resilient-Combined-Sewer-Overflow-Management" class="headerlink" title="98. Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Automated_Energy-Aware_Time-Series_Model_Deployment_on_Embedded_FPGAs_for_Resilient_Combined_Sewer_O.pdf">Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Duisburg-Essen</span></p>
<p>该论文提出一种端到端、自动化的时序预测框架，将轻量级Transformer和LSTM模型通过整数量化后部署在嵌入式FPGA上，实现了污水溢流监测的本地、低能耗推理。实验表明8位量化Transformer在准确率和能耗之间取得平衡，LSTM则更适用于极低能耗场景，两者可根据实际部署需求灵活选择，显著提升了污水系统预警的韧性和可靠性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Automated_Energy-Aware_Time-Series_Model_Deployment_on_Embedded_FPGAs_for_Resilient_Combined_Sewer_Overflow_Management.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-DegDiT-Controllable-Audio-Generation-with-Dynamic-Event-Graph-Guided-Diffusion-Transformer"><a href="#99-DegDiT-Controllable-Audio-Generation-with-Dynamic-Event-Graph-Guided-Diffusion-Transformer" class="headerlink" title="99. DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer"></a>99. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/DegDiT__Controllable_Audio_Generation_with_Dynamic_Event_Graph_Guided_Diffusion_Transformer.pdf">DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>该论文提出了DegDiT，一种结合动态事件图和扩散Transformer的可控文本到音频生成框架。方法通过将音频事件建模为包含语义、时间和事件间关系的动态事件图，利用图Transformer生成上下文嵌入并指导扩散模型合成音频，辅以高质量数据筛选和多维奖励共识优化。实验结果显示，DegDiT在多个音频生成指标上达到SOTA，具备更高的灵活性和时序精度，显著提升了音频生成的可控性和质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/DegDiT_Controllable_Audio_Generation_with_Dynamic_Event_Graph_Guided_Diffusion_Transformer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-Communication-Efficient-Federated-Learning-with-Adaptive-Number-of-Participants"><a href="#100-Communication-Efficient-Federated-Learning-with-Adaptive-Number-of-Participants" class="headerlink" title="100. Communication-Efficient Federated Learning with Adaptive Number of Participants"></a>100. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Communication-Efficient_Federated_Learning_with_Adaptive_Number_of_Participants.pdf">Communication-Efficient Federated Learning with Adaptive Number of Participants</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ivannikov Institute for System Programming, Moscow, Russia</span></p>
<p>本文提出了一种名为ISP（Intelligent Selection of Participants）的自适应机制，通过动态确定每轮联邦学习中参与客户端的数量，以在保证模型精度的前提下最大限度地提升通信效率。该方法将客户端数量的选择建模为优化问题，并通过蒙特卡洛估计和动量等技术，结合多种采样策略，在多个视觉和时序任务（如CIFAR-10、Tiny-ImageNet、ECG心电图分类）及梯度压缩设置下进行实验，结果表明在维持或提升模型精度的同时，通信轮数最高可减少30%。结论是，动态调整参与客户端数量不仅可显著降低通信开销，而且对大规模实际部署场景具有重要意义。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Communication-Efficient_Federated_Learning_with_Adaptive_Number_of_Participants.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="101-PENGUIN-Enhancing-Transformer-with-Periodic-Nested-Group-Attention-for-Long-term-Time-Series-Forecasting"><a href="#101-PENGUIN-Enhancing-Transformer-with-Periodic-Nested-Group-Attention-for-Long-term-Time-Series-Forecasting" class="headerlink" title="101. PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting"></a>101. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/PENGUIN__Enhancing_Transformer_with_Periodic-Nested_Group_Attention_for_Long-term_Time_Series_Foreca.pdf">PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>该论文提出了PENGUIN，一种增强Transformer的周期嵌套分组注意力机制，通过显式建模多周期性和相对注意力偏置，有效捕获时间序列中的周期结构。实验结果表明，PENGUIN在九个基准数据集上显著优于现有MLP和Transformer模型，实现了更高的长期预测精度和更优的计算效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/PENGUIN_Enhancing_Transformer_with_Periodic-Nested_Group_Attention_for_Long-term_Time_Series_Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="102-End-to-End-Audio-Visual-Learning-for-Cochlear-Implant-Sound-Coding-in-Noisy-Environments"><a href="#102-End-to-End-Audio-Visual-Learning-for-Cochlear-Implant-Sound-Coding-in-Noisy-Environments" class="headerlink" title="102. End-to-End Audio-Visual Learning for Cochlear Implant Sound Coding in Noisy Environments"></a>102. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/End-to-End_Audio-Visual_Learning_for_Cochlear_Implant_Sound_Coding_in_Noisy_Environments.pdf">End-to-End Audio-Visual Learning for Cochlear Implant Sound Coding in Noisy Environments</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Taiwan University</span></p>
<p>该论文提出了一种端到端人工耳蜗系统AVSE-ECS，将音频-视觉语音增强（AVSE）与深度学习声码策略ElectrodeNet-CS（ECS）结合，通过联合训练优化音频与电极刺激信号。在嘈杂环境下实验表明，该方法显著提升了人工耳蜗的语音可懂度，验证了多模态深度学习集成的可行性和有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/End-to-End_Audio-Visual_Learning_for_Cochlear_Implant_Sound_Coding_in_Noisy_Environments.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="103-Trans-XFed-An-Explainable-Federated-Learning-for-Supply-Chain-Credit-Assessment"><a href="#103-Trans-XFed-An-Explainable-Federated-Learning-for-Supply-Chain-Credit-Assessment" class="headerlink" title="103. Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment"></a>103. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Trans-XFed__An_Explainable_Federated_Learning_for_Supply_Chain_Credit_Assessment.pdf">Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Utrecht University</span></p>
<p>本文提出了Trans-XFed架构，将联邦学习与可解释AI方法结合应用于供应链信用评估。方法上，采用基于性能的客户端选择策略（PBCS）以解决类别不平衡与非独立同分布（Non-IID）问题，整合FedProx架构、同态加密与Transformer编码器，并结合集成梯度实现可解释性；实验结果表明，该方法在保证隐私与可解释性的同时，实现了更高效和准确的信用评估，优于现有基线模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Trans-XFed_An_Explainable_Federated_Learning_for_Supply_Chain_Credit_Assessment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="104-ITL-LIME-Instance-Based-Transfer-Learning-for-Enhancing-Local-Explanations-in-Low-Resource-Data-Settings"><a href="#104-ITL-LIME-Instance-Based-Transfer-Learning-for-Enhancing-Local-Explanations-in-Low-Resource-Data-Settings" class="headerlink" title="104. ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings"></a>104. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/ITL-LIME__Instance-Based_Transfer_Learning_for_Enhancing_Local_Explanations_in_Low-Resource_Data_Set.pdf">ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Murdoch University</span></p>
<p>本文提出ITL-LIME框架，通过引入基于实例的迁移学习和对比学习编码器，改进LIME在数据资源有限场景下的局部解释能力。方法首先对源域数据进行K-medoids聚类，检索与目标实例最相似的源实例，并与目标领域的邻域实例结合，利用对比学习编码器为实例赋权，最后训练局部可解释模型。实验结果表明，ITL-LIME在稳定性、忠实度和鲁棒性上均优于现有LIME变体，尤其适用于医疗等关键小样本领域。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/ITL-LIME_Instance-Based_Transfer_Learning_for_Enhancing_Local_Explanations_in_Low-Resource_Data_Settings.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="105-DyMixOp-Guiding-Neural-Operator-Design-for-PDEs-from-a-Complex-Dynamics-Perspective-with-Local-Global-Mixing"><a href="#105-DyMixOp-Guiding-Neural-Operator-Design-for-PDEs-from-a-Complex-Dynamics-Perspective-with-Local-Global-Mixing" class="headerlink" title="105. DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing"></a>105. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/DyMixOp__Guiding_Neural_Operator_Design_for_PDEs_from_a_Complex_Dynamics_Perspective_with_Local-Glob.pdf">DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Aeronautics and Astronautics, Shanghai Jiao Tong University</span></p>
<p>该论文提出了DyMixOp，一种理论驱动的神经算子框架，用于高效求解由非线性偏微分方程（PDEs）描述的复杂动力系统。方法基于惯性流形理论，将无限维PDE动态降维至有限维潜空间，并设计了结合局部-全局混合（Local-Global-Mixing, LGM）变换的多层架构，能有效捕捉线性与非线性动力学并增强物理可解释性。实验显示，DyMixOp在多类PDE基准（包括对流、扩散及混合型方程）上性能优异，误差显著降低，且保持高效可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/DyMixOp_Guiding_Neural_Operator_Design_for_PDEs_from_a_Complex_Dynamics_Perspective_with_Local-Global-Mixing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="106-EVENTTSF-Event-Aware-Non-Stationary-Time-Series-Forecasting"><a href="#106-EVENTTSF-Event-Aware-Non-Stationary-Time-Series-Forecasting" class="headerlink" title="106. EVENTTSF: Event-Aware Non-Stationary Time Series Forecasting"></a>106. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/EventTSF__Event-Aware_Non-Stationary_Time_Series_Forecasting.pdf">EVENTTSF: Event-Aware Non-Stationary Time Series Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Griffith University</span></p>
<p>该论文提出了EVENTTSF，一种新颖的自动回归扩散框架，利用历史时间序列和文本事件进行非平稳时序预测。方法包括事件控制的流匹配采样机制与多模态U型扩散Transformer，有效融合并对齐时间与文本模态，解决事件同步、事件诱导不确定性及跨模态表征错配等挑战。实验结果显示，EVENTTSF在八个数据集和十二项基线方法中表现优越，预测精度提升10.7%，训练效率提升1.13倍，尤其在事件丰富的数据集上有显著优势。结论表明该方法为事件感知的非平稳时序分析开辟了新方向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/EVENTTSF_Event-Aware_Non-Stationary_Time_Series_Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="107-AlphaX-An-AI-Based-Value-Investing-Strategy-for-the-Brazilian-Stock-Market"><a href="#107-AlphaX-An-AI-Based-Value-Investing-Strategy-for-the-Brazilian-Stock-Market" class="headerlink" title="107. AlphaX: An AI-Based Value Investing Strategy for the Brazilian Stock Market"></a>107. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/AlphaX__An_AI-Based_Value_Investing_Strategy_for_the_Brazilian_Stock_Market.pdf">AlphaX: An AI-Based Value Investing Strategy for the Brazilian Stock Market</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Aeronautics Institute of Technology - ITA</span></p>
<p>本文提出AlphaX，一种融合财务基本面和市场数据、采用集成回归算法（如随机森林）的AI自动化价值投资策略，专为巴西股市设计。通过严格的回测和偏差控制，AlphaX在18个季度内在总收益、年化回报和风险调整收益方面均显著优于Ibovespa指数、Selic利率及常见技术分析策略（RSI、MFI等），展现了AI驱动的价值投资在实际金融市场中的有效性和稳健性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/AlphaX_An_AI-Based_Value_Investing_Strategy_for_the_Brazilian_Stock_Market.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="108-Virtuous-Machines-Towards-Artificial-General-Science"><a href="#108-Virtuous-Machines-Towards-Artificial-General-Science" class="headerlink" title="108. Virtuous Machines: Towards Artificial General Science"></a>108. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Virtuous_Machines__Towards_Artificial_General_Science.pdf">Virtuous Machines: Towards Artificial General Science</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Explore Science</span></p>
<p>本论文提出了一种端到端的、领域无关的多智能体AI系统，能够自主完成科学研究全流程，包括假设生成、实验设计、数据采集、统计分析、可视化、论文撰写和‘自我’评审。系统在认知科学领域验证，通过完全自动化流程设计并完成了三项视觉工作记忆、心理旋转和意象清晰度的心理学实验，展现了AI系统独立开展实证研究的能力。结论表明，AI系统可在理论推理和方法严谨性上达到人类专家水平，但在理论细节和解释上尚有不足，标志着AI向自主科学发现迈出了重要一步。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Virtuous_Machines_Towards_Artificial_General_Science.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="109-Enhancing-Robustness-of-Implicit-Neural-Representations-Against-Weight-Perturbations"><a href="#109-Enhancing-Robustness-of-Implicit-Neural-Representations-Against-Weight-Perturbations" class="headerlink" title="109. Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations"></a>109. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/Enhancing_Robustness_of_Implicit_Neural_Representations_Against_Weight_Perturbations.pdf">Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Hong Kong</span></p>
<p>该论文首次系统性研究了隐式神经表示（INRs）对权重扰动的脆弱性，提出通过最小化有扰动和无扰动权重下损失函数差异的方法，推导出一种基于梯度的鲁棒损失函数以提升模型对噪声的抵抗力。大量实验结果表明，该方法在多模态信号重构任务中能在多种噪声条件下将峰值信噪比（PSNR）提升至比原始模型高出最多7.5 dB，有效增强了INRs的鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/Enhancing_Robustness_of_Implicit_Neural_Representations_Against_Weight_Perturbations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="110-SVDformer-Direction-Aware-Spectral-Graph-Embedding-Learning-via-SVD-and-Transformer"><a href="#110-SVDformer-Direction-Aware-Spectral-Graph-Embedding-Learning-via-SVD-and-Transformer" class="headerlink" title="110. SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer"></a>110. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/SVDformer__Direction-Aware_Spectral_Graph_Embedding_Learning_via_SVD_and_Transformer.pdf">SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Sydney</span></p>
<p>SVDformer提出了一种结合奇异值分解（SVD）和Transformer多头自注意力机制的有向图表征学习框架，通过自适应地增强重要谱分量并抑制高频噪声，实现了可学习的低&#x2F;高通谱图滤波，并利用奇异向量实现方向感知特征传播。实验结果表明，SVDformer在六个有向图基准数据集上节点分类准确率优于现有主流方法，尤其在异质和大规模稀疏图上表现突出，实现了更优的可扩展性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/SVDformer_Direction-Aware_Spectral_Graph_Embedding_Learning_via_SVD_and_Transformer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="111-STPFormer-A-State-of-the-Art-Pattern-Aware-Spatio-Temporal-Transformer-for-Traffic-Forecasting"><a href="#111-STPFormer-A-State-of-the-Art-Pattern-Aware-Spatio-Temporal-Transformer-for-Traffic-Forecasting" class="headerlink" title="111. STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting"></a>111. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-20/STPFormer__A_State-of-the-Art_Pattern-Aware_Spatio-Temporal_Transformer_for_Traffic_Forecasting.pdf">STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Somewhere</span></p>
<p>本文提出了STPFormer，一种面向交通预测的统一可解释的时空Transformer模型，集成了时间位置聚合器（TPA）、空间序列聚合器（SSA）、时空图匹配模块（STGM）与注意力融合模块，实现了多尺度时空特征的高效建模。方法在五个真实交通数据集上进行评测，结果显示STPFormer优于当前主流方法，具有更强的泛化能力和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-21/STPFormer_A_State-of-the-Art_Pattern-Aware_Spatio-Temporal_Transformer_for_Traffic_Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/21报纸</div>
      <div>http://ywfhhh.github.io/2025/08/2025-08-21_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>具身ADT</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月21日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/2025-08-22_article/" title="2025/08/22报纸">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">2025/08/22报纸</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-20_article/" title="2025/08/20报纸">
                        <span class="hidden-mobile">2025/08/20报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<!-- hexo injector body_end start --><script src="/data/aaron_workspace/pic_server/newspaper/hexo_articles/scripts/custom.js"></script><!-- hexo injector body_end end --></body>
</html>
