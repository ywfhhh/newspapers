

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Ywfhhh">
  <meta name="keywords" content="">
  
    <meta name="description" content="æ¯æ—¥æœ€æ–°è®ºæ–‡åˆ†äº«">
<meta property="og:type" content="article">
<meta property="og:title" content="ä»Šæ—¥è®ºæ–‡æŠ¥çº¸">
<meta property="og:url" content="http://example.com/2025/08/2025-08-11_article/index.html">
<meta property="og:site_name" content="è®ºæ–‡æŠ¥çº¸é›†åˆ">
<meta property="og:description" content="æ¯æ—¥æœ€æ–°è®ºæ–‡åˆ†äº«">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Test-Time_Reinforcement_Learning_for_GUI_Grounding_via_Region_Consistency.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Discovering_Interpretable_Programmatic_Policies_via_Multimodal_LLM-assisted_Evolutionary_Search.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/ECHO_Decoupling_Inference_and_Training_for_Large-Scale_RL_Alignment_on_Heterogeneous_Swarms.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Posterior-GRPO_Rewarding_Reasoning_Processes_in_Code_Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Domain-driven_Metrics_for_Reinforcement_Learning_A_Case_Study_on_Epidemic_Control_using_Agent-based_Simulation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Hallucination-Free_Music_A_Reinforcement_Learning_Preference_Optimization_Framework_for_Reliable_Song_Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Anti-Jamming_Sensing_with_Distributed_Reconfigurable_Intelligent_Metasurface_Antennas.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Genie_Envisioner_A_Unified_World_Foundation_Platform_for_Robotic_Manipulation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/TRAJEVO_Trajectory_Prediction_Heuristics_Design_via_LLM-driven_Evolution.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/OMNIEAR_Benchmarking_Agent_Reasoning_in_Embodied_Tasks.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/CleanUpBench_Embodied_Sweeping_and_Grasping_Benchmark.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Generalizable_Safety_in_Crowd_Navigation_via_Conformal_Uncertainty_Handling.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Multi-view_Landmark_Representation_Approach_with_Application_to_GNSS-Visual-Inertial_Odometry.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Information-Theoretic_Graph_Fusion_with_Vision-Language-Action_Model_for_Policy_Reasoning_and_Dual_Robotic_Control.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/DistillDrive_End-to-End_Multi-Mode_Autonomous_Driving_Distillation_by_Isomorphic_Hetero-Source_Planning_Model.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Embodied_Agentic_AI_Review_and_Classification_of_LLM-_and_VLM-Driven_Robot_Autonomy_and_Interaction.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Driver_Assistant_Persuading_Drivers_to_Adjust_Secondary_Tasks_Using_Large_Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EndoMatcher_Generalizable_Endoscopic_Image_Matcher_via_Multi-Domain_Pre-training_for_Robot-Assisted_Surgery.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/GhostShell_Streaming_LLM_Function_Calls_for_Concurrent_Embodied_Programming.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Advanced_Hybrid_Transformer%E2%80%93LSTM_Technique_with_Attention_and_TS-Mixer_for_Drilling_Rate_of_Penetration_Prediction.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Learning_to_See_and_Act_Task-Aware_View_Planning_for_Robotic_Manipulation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/PhysPatch_A_Physically_Realizable_and_Transferable_Adversarial_Patch_Attack_for_Multimodal_Large_Language_Models-based_Autonomous_Driving_Systems.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Examining_the_legibility_of_humanoid_robot_arm_movements_in_a_pointing_task.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Study_of_the_Framework_and_Real-World_Applications_of_Language_Embedding_for_3D_Scene_Understanding.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Will_You_Be_Aware_Eye_Tracking%E2%80%93Based_Modeling_of_Situational_Awareness_in_Augmented_Reality.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/From_Canada_to_Japan_How_10%2C000_km_Affect_User_Perception_in_Robot_Teleoperation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Analyzing_the_Impact_of_Multimodal_Perception_on_Sample_Complexity_and_Optimization_Landscapes_in_Imitation_Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Hierarchical_Deep_Deterministic_Policy_Gradient_for_Autonomous_Maze_Navigation_of_Mobile_Robots.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/GAP_Gaussianize_Any_Point_Clouds_with_Text_Guidance.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Uni-CoT_Towards_Unified_Chain-of-Thought_Reasoning_Across_Text_and_Vision.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/DART_Dual_Adaptive_Refinement_Transfer_for_Open-Vocabulary_Multi-Label_Recognition.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/%CF%87_Symmetry_Understanding_of_3D_Shapes_via_Chirality_Disentanglement.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SMOL-MapSeg_Show_Me_One_Label.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/How_and_Why_Taming_Flow_Matching_for_Unsupervised_Anomaly_Detection_and_Localization.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Explaining_Similarity_in_Vision%E2%80%93Language_Encoders_with_Weighted_Banzhaf_Interactions.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/UNCAGE_Contrastive_Attention_Guidance_for_Masked_Generative_Transformers_in_Text-to-Image_Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MagicHOI_Leveraging_3D_Priors_for_Accurate_Hand-object_Reconstruction_from_Short_Monocular_Video_Clips.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/F2PASeg_Feature_Fusion_for_Pituitary_Anatomy_Segmentation_in_Endoscopic_Surgery.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Smoothing_Slot_Attention_Iterations_and_Recurrences.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/CT-GRAPH_Hierarchical_Graph_Attention_Network_for_Anatomy-Guided_CT_Report_Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/PriorRG_Prior-Guided_Contrastive_Pre-training_and_Coarse-to-Fine_Decoding_for_Chest_X-ray_Report_Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Textual_Inversion_for_Efficient_Adaptation_of_Open-Vocabulary_Object_Detectors_Without_Forgetting.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/mKG-RAG_Multimodal_Knowledge_Graph-Enhanced_RAG_for_Visual_Question_Answering.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SGDFuse_SAM-Guided_Diffusion_for_High-Fidelity_Infrared_and_Visible_Image_Fusion.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/CF3_Compact_and_Fast_3D_Feature_Fields.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Study_of_Gender_Classification_Techniques_Based_on_Iris_Images_A_Deep_Survey_and_Analysis.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Coarse-to-Fine_Joint_Registration_of_MR_and_Ultrasound_Images_via_Imaging_Style_Transfer.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/ReasoningTrack_Chain-of-Thought_Reasoning_for_Long-term_Vision-Language_Tracking.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SPA%2B%2B_Generalized_Graph_Spectral_Alignment_for_Versatile_Domain_Adaptation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Wavelet-Guided_Dual-Frequency_Encoding_for_Remote_Sensing_Change_Detection.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/RegionMed-CLIP_A_Region-Aware_Multimodal_Contrastive_Learning_Pre-trained_Model_for_Medical_Image_Understanding.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/ArbiViewGen_Controllable_Arbitrary_Viewpoint_Camera_Data_Generation_for_Autonomous_Driving_via_Stable_Diffusion_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Segmenting_the_Complex_and_Irregular_in_Two-Phase_Flows_A_Real-World_Empirical_Study_with_SAM2.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Beyond_Pixels_Medical_Image_Quality_Assessment_with_Implicit_Neural_Representations.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/X-MoGen_Unified_Motion_Generation_across_Humans_and_Animals.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FedGIN_Federated_Learning_with_Dynamic_Global_Intensity_Non-linear_Augmentation_for_Organ_Segmentation_using_Multi-modal_Images.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/PoseGen_In-Context_LoRA_Finetuning_for_Pose-Controllable_Long_Human_Video_Generation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/AdaFusion_Prompt-Guided_Inference_with_Adaptive_Fusion_of_Pathology_Foundation_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/AU-IQA_A_Benchmark_Dataset_for_Perceptual_Quality_Assessment_of_AI-Enhanced_User-Generated_Content.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Multimodal_Causal-Driven_Representation_Learning_for_Generalizable_Medical_Image_Segmentation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Propagating_Sparse_Depth_via_Depth_Foundation_Model_for_Out-of-Distribution_Depth_Completion.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Rotation_Equivariant_Arbitrary-scale_Image_Super-Resolution.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Decoupling_Continual_Semantic_Segmentation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Modeling_Rapid_Contextual_Learning_in_the_Visual_Cortex_with_Fast-Weight_Deep_Autoencoder_Networks.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/UGOD_Uncertainty-Guided_Differentiable_Opacity_and_Soft_Dropout_for_Enhanced_Sparse-View_3DGS.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Open-world_Point_Cloud_Semantic_Segmentation_A_Human-in-the-loop_Framework.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Laplacian_Analysis_Meets_Dynamics_Modelling_Gaussian_Splatting_for_4D_Reconstruction.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/H-NET%2B%2B_Hierarchical_Dynamic_Chunking_for_Tokenizer-Free_Language_Modelling_in_Morphologically-Rich_Languages.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Simulating_Human-Like_Learning_Dynamics_with_LLM-Empowered_Agents.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Shuffle-R1_Efficient_RL_framework_for_Multimodal_Large_Language_Models_via_Data-centric_Dynamic_Shuffle.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/LLaVA-RE_Binary_Image-Text_Relevancy_Evaluation_with_Multimodal_Large_Language_Model.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Iterative_Learning_of_Computable_Phenotypes_for_Treatment_Resistant_Hypertension_using_Large_Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MV-Debate_Multi-view_Agent_Debate_with_Dynamic_Reflection_Gating_for_Multimodal_Harmful_Content_Detection_in_Social_Media.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/On_the_Generalization_of_SFT_A_Reinforcement_Learning_Perspective_with_Reward_Rectification.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/How_Do_LLMs_Persuade_Linear_Probes_Can_Uncover_Persuasion_Dynamics_in_Multi-Turn_Conversations.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/The_Missing_Reward_Active_Inference_in_the_Era_of_Experience.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/COOPER_Co-Optimizing_Policy_and_Reward_Models_in_Reinforcement_Learning_for_Large_Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Follow-Your-Instruction_A_Comprehensive_MLLM_Agent_for_World_Data_Synthesis.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Conformal_Sets_in_Multiple-Choice_Question_Answering_under_Black-Box_Settings_with_Provable_Coverage_Guarantees.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/AI_vs._Human_Moderators_A_Comparative_Evaluation_of_Multimodal_LLMs_in_Content_Moderation_for_Brand_Safety.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Leveraging_AI_to_Accelerate_Clinical_Data_Cleaning_A_Comparative_Study_of_AI-Assisted_vs._Traditional_Methods.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/The_World_According_to_LLMs_How_Geographic_Origin_Influences_LLMs%E2%80%99_Entity_Deduction_Capabilities.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Streamlining_Admission_with_LOR_Insights_AI-Based_Leadership_Assessment_in_Online_Master%27s_Program.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Auto-Eval_Judge_Towards_a_General_Agentic_Framework_for_Task_Completion_Evaluation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/InfiAlign_A_Scalable_and_Sample-Efficient_Framework_for_Aligning_LLMs_to_Enhance_Reasoning_Capabilities.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Can_Large_Language_Models_Generate_Effective_Datasets_for_Emotion_Recognition_in_Conversations_.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Group_Causal_Policy_Optimization_for_Post-Training_Large_Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/LLM-based_Multi-Agent_Copilot_for_Quantum_Sensor.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/StructVRM_Aligning_Multimodal_Reasoning_with_Structured_and_Verifiable_Reward_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/The_Term_%E2%80%98Agent%E2%80%99_Has_Been_Diluted_Beyond_Utility_and_Requires_Redefinition.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/LAG_Logic-Augmented_Generation_from_a_Cartesian_Perspective.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MELLA_Bridging_Linguistic_Capability_and_Cultural_Groundedness_for_Low-Resource_Language_MLLMs.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/GRAIL_Learning_to_Interact_with_Large_Knowledge_Graphs_for_Retrieval_Augmented_Reasoning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MoMA_A_Mixture-of-Multimodal-Agents_Architecture_for_Enhancing_Clinical_Prediction_Modelling.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MyCulture_Exploring_Malaysia%E2%80%99s_Diverse_Culture_under_Low-Resource_Language_Constraints.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Large_Language_Models_Transform_Organic_Synthesis_From_Reaction_Prediction_to_Automation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/NomicLaw_Emergent_Trust_and_Strategic_Argumentation_in_LLMs_During_Collaborative_Law-Making.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Novel_Architecture_for_Symbolic_Reasoning_with_Decision_Trees_and_LLM_Agents.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/B4DL_A_Benchmark_for_4D_LiDAR_LLM_in_Spatio-Temporal_Understanding.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Resource-Limited_Joint_Multimodal_Sentiment_Reasoning_and_Classification_via_Chain-of-Thought_Enhancement_and_Distillation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/VFlowOpt_A_Token_Pruning_Framework_for_LMMs_with_Visual_Information_Flow-Guided_Optimization.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FAITH_A_Framework_for_Assessing_Intrinsic_Tabular_Hallucinations_in_Finance.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Incident_Response_Planning_Using_a_Lightweight_Large_Language_Model_with_Reduced_Hallucination.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/RLHF_Fine-Tuning_of_LLMs_for_Alignment_with_Implicit_User_Feedback_in_Conversational_Recommenders.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Understanding_and_Mitigating_Errors_of_LLM-Generated_RTL_Code.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MoBE_Mixture-of-Basis-Experts_for_Compressing_MoE-based_LLMs.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Pruning_Large_Language_Models_by_Identifying_and_Preserving_Functional_Networks.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Cross-LoRA_A_Data-Free_LoRA_Transfer_Framework_across_Heterogeneous_LLMs.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SPEX_A_Vision-Language_Model_for_Land_Cover_Extraction_on_Spectral_Remote_Sensing_Images.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EvoGraph_Hybrid_Directed_Graph_Evolution_toward_Software_3.0.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/QA-Dragon_Query-Aware_Dynamic_RAG_System_for_Knowledge-Intensive_Visual_Question_Answering.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Attention_Basin_Why_Contextual_Position_Matters_in_Large_Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Beyond_Automation_Socratic_AI%2C_Epistemic_Agency%2C_and_the_Implications_of_the_Emergence_of_Orchestrated_Multi-Agent_Learning_Architectures.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Align%2C_Don%27t_Divide_Revisiting_the_LoRA_Architecture_in_Multi-Task_Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Aligning_LLMs_on_a_Budget_Inference-Time_Alignment_with_Heuristic_Reward_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Speech_LLMs_in_Low-Resource_Scenarios_Data_Volume_Requirements_and_the_Impact_of_Pretraining_on_High-Resource_Languages.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Assessing_Medical_Ethics_from_Knowledge_to_Practice.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Exploring_Superior_Function_Calls_via_Reinforcement_Learning.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EasySize_Elastic_Analog_Circuit_Sizing_via_LLM-Guided_Heuristic_Search.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/JPS_Jailbreak_Multimodal_Large_Language_Models_with_Collaborative_Visual_Perturbation_and_Textual_Steering.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MedMKEB_A_Comprehensive_Knowledge_Editing_Benchmark_for_Medical_Multimodal_Large_Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Finding_Needles_in_Images_Can_Multimodal_LLMs_Locate_Fine_Details_.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Evaluation_of_Large_Language_Model_Architectures_in_Abstract_Meaning_Representation_Parsing.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Skin-SOAP_A_Weakly_Supervised_Framework_for_Generating_Structured_SOAP_Notes.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SPaRFT_Self-Paced_Reinforcement_Fine-Tuning_for_Large_Language_Models.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Can_Large_Language_Models_Integrate_Spatial_Data_Empirical_Insights_into_Reasoning_Strengths_and_Computational_Weaknesses.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/R-Zero_Self-Evolving_Reasoning_LLM_from_Zero_Data.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Multi-Stage_Large_Language_Model_Framework_for_Extracting_Suicide-Related_Social_Determinants_of_Health.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Situated_Epistemic_Infrastructures_A_Diagnostic_Framework_for_Post-Coherence_Knowledge.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Metric_for_MLLM_Alignment_in_Large-scale_Recommendation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Learning_Geometric-Aware_Quadrature_Rules_for_Functional_Minimization.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MolSnap_Snap-Fast_Molecular_Generation_with_Latent_Variational_Mean_Flow.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Multi-Modal_Multi-Behavior_Sequential_Recommendation_with_Conditional_Diffusion-Based_Feature_Denoising.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EnergyPatchTST_Multi-scale_Time_Series_Transformers_with_Uncertainty_Estimation_for_Energy_Forecasting.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Harmonic_fractal_transformation_for_modeling_complex_neuronal_effects_from_bursting_and_noise_shaping_to_waveform_sensitivity_and_noise-induced_subthreshold_spiking.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Estimating_Musical_Surprisal_from_Audio_in_Autoregressive_Diffusion_Model_Noise_Spaces.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FlowState_Sampling_Rate_Invariant_Time_Series_Forecasting.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FDC-Net_Rethinking_the_association_between_EEG_artifact_removal_and_multi-dimensional_affective_computing.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Balancing_Accuracy_and_Novelty_with_Sub-Item_Popularity.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Hybrid_quantum_tensor_networks_for_aeroelastic_applications.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Salt-Rock_Creep_Deformation_Forecasting_Using_Deep_Neural_Networks_and_Analytical_Models_for_Subsurface_Energy_Storage_Applications.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/High-Dimensional_Differentially_Private_Quantile_Regression_Distributed_Estimation_and_Statistical_Inference.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Graph-based_Event_Log_Repair.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Align-for-Fusion_Harmonizing_Triple_Preferences_via_Dual-oriented_Diffusion_for_Cross-domain_Sequential_Recommendation.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Q-DPTS_Quantum_Differentially_Private_Time_Series_Forecasting_via_Variational_Quantum_Circuits.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Fairness_in_Dysarthric_Speech_Synthesis_Understanding_Intrinsic_Bias_in_Dysarthric_Speech_Cloning_using_F5-TTS.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Disentangling_Bias_by_Modeling_Intra-_and_Inter-modal_Causal_Attention_for_Multimodal_Sentiment_Analysis.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Compressed_Decentralized_Momentum_Stochastic_Gradient_Methods_for_Nonconvex_Optimization.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MENDR_Manifold_Explainable_Neural_Data_Representations.jpg">
<meta property="article:published_time" content="2025-08-11T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-19T06:44:09.082Z">
<meta property="article:author" content="Ywfhhh">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Test-Time_Reinforcement_Learning_for_GUI_Grounding_via_Region_Consistency.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>ä»Šæ—¥è®ºæ–‡æŠ¥çº¸ - è®ºæ–‡æŠ¥çº¸é›†åˆ</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- ä¸»é¢˜ä¾èµ–çš„å›¾æ ‡åº“ï¼Œä¸è¦è‡ªè¡Œä¿®æ”¹ -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ã€å­¦æœ¯åŠ¨æ€æŠ¥å‘Šã€‘è®ºæ–‡æŠ¥çº¸</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>è®ºæ–‡é›†åˆ</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="ä»Šæ—¥è®ºæ–‡æŠ¥çº¸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-11 00:00" pubdate>
          æ˜ŸæœŸä¸€, å…«æœˆ 11æ—¥ 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          22k å­—
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> æ¬¡
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">ä»Šæ—¥è®ºæ–‡æŠ¥çº¸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>ä»Šæ—¥è®ºæ–‡æŠ¥çº¸</h1></center>

<p>ğŸ“… <strong>æ—¥æœŸ</strong>ï¼š2025-08-11<br>ğŸ“„ <strong>å‘ç°è®ºæ–‡æ•°é‡</strong>ï¼š149  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency"><a href="#1-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency" class="headerlink" title="1. Test-Time Reinforcement Learning for GUI Grounding via Region Consistency"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Test-Time_Reinforcement_Learning_for_GUI_Grounding_via_Region_Consistency.pdf">Test-Time Reinforcement Learning for GUI Grounding via Region Consistency</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zhejiang University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†GUI-RCï¼ˆRegion Consistencyï¼‰ä¸GUI-RCPOï¼ˆRegion Consistency Policy Optimizationï¼‰ä¸¤ç§æµ‹è¯•æ—¶ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºæå‡GUI groundingä»»åŠ¡ï¼ˆè‡ªç„¶è¯­è¨€åˆ°ç•Œé¢åæ ‡æ˜ å°„ï¼‰çš„å‡†ç¡®ç‡ã€‚GUI-RCé€šè¿‡å¤šæ¬¡é‡‡æ ·æ¨¡å‹è¾“å‡ºå¹¶ç©ºé—´æŠ•ç¥¨å¾—åˆ°ä¸€è‡´æ€§åŒºåŸŸï¼Œæ— éœ€é¢å¤–æ ‡æ³¨å³å¯æå‡æ¨¡å‹å®šä½èƒ½åŠ›ï¼›GUI-RCPOåˆ™å°†åŒºåŸŸä¸€è‡´æ€§è½¬åŒ–ä¸ºå¥–åŠ±ä¿¡å·ï¼Œåœ¨æµ‹è¯•æ—¶è¿›è¡Œè‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä½¿æ¨¡å‹åœ¨æ— æ ‡æ³¨æ•°æ®ä¸Šè‡ªæˆ‘ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜æ–¹æ³•åœ¨å¤šä¸ªä¸»æµæ¨¡å‹å’ŒåŸºå‡†ä¸Šå¹³å‡æå‡2-5%çš„å‡†ç¡®ç‡ï¼Œå¹¶å¯è‡ªä¸¾å¼è¿­ä»£æå‡ï¼Œå±•ç°äº†æµ‹è¯•æ—¶ä¼˜åŒ–çš„æ–°æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Test-Time_Reinforcement_Learning_for_GUI_Grounding_via_Region_Consistency.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-Discovering-Interpretable-Programmatic-Policies-via-Multimodal-LLM-assisted-Evolutionary-Search"><a href="#2-Discovering-Interpretable-Programmatic-Policies-via-Multimodal-LLM-assisted-Evolutionary-Search" class="headerlink" title="2. Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Discovering_Interpretable_Programmatic_Policies_via_Multimodal_LLM-assisted_Evolutionary_Search.pdf">Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">City University of Hong Kong</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯è§£é‡Šç¨‹åºåŒ–ç­–ç•¥å‘ç°æ–¹æ³•â€”â€”å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©è¿›åŒ–æœç´¢ï¼ˆMLESï¼‰ï¼Œè¯¥æ–¹æ³•å°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸è¿›åŒ–ä¼˜åŒ–ç»“åˆï¼Œç›´æ¥ç”Ÿæˆå¸¦æœ‰è‡ªç„¶è¯­è¨€è§£é‡Šçš„ç¨‹åºåŒ–ç­–ç•¥ã€‚MLESåœ¨è¿›åŒ–è¿‡ç¨‹ä¸­èåˆäº†è§†è§‰åé¦ˆé©±åŠ¨çš„è¡Œä¸ºåˆ†æï¼Œæœ‰æ•ˆæå‡äº†ç­–ç•¥å‘ç°æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚å®éªŒè¯æ˜ï¼ŒMLESåœ¨Lunar Landerå’ŒCar Racingä¸¤ä¸ªå…¸å‹æ§åˆ¶ä»»åŠ¡ä¸Šï¼Œç­–ç•¥æ€§èƒ½ä¸PPOç›¸å½“ï¼Œå¹¶æ˜¾è‘—æå‡äº†ç­–ç•¥çš„é€æ˜åº¦å’ŒçŸ¥è¯†å¯å¤ç”¨æ€§ï¼Œä¸ºä¸‹ä¸€ä»£å¯è§£é‡Šç­–ç•¥å‘ç°æä¾›äº†æ–°èŒƒå¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Discovering_Interpretable_Programmatic_Policies_via_Multimodal_LLM-assisted_Evolutionary_Search.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-ECHO-Decoupling-Inference-and-Training-for-Large-Scale-RL-Alignment-on-Heterogeneous-Swarms"><a href="#3-ECHO-Decoupling-Inference-and-Training-for-Large-Scale-RL-Alignment-on-Heterogeneous-Swarms" class="headerlink" title="3. ECHO: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Echo__Decoupling_Inference_and_Training_for_Large-Scale_RL_Alignment_on_Heterogeneous_Swarms.pdf">ECHO: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Gradient</span></p>
<p>æœ¬æ–‡æå‡ºäº†ECHOç³»ç»Ÿï¼Œé€šè¿‡å°†RLä¸­çš„æ¨ç†ï¼ˆtrajectory samplingï¼‰å’Œè®­ç»ƒï¼ˆpolicy optimizationï¼‰åœ¨å¼‚æ„é›†ç¾¤ä¸Šå½»åº•è§£è€¦ï¼Œè®¾è®¡äº†é¡ºåºæ‹‰å–å’Œå¼‚æ­¥æ¨é€ä¸¤ç§è½»é‡çº§åŒæ­¥åè®®ï¼Œå®ç°äº†åœ¨è¾¹ç¼˜è®¾å¤‡å’Œæ•°æ®ä¸­å¿ƒç¡¬ä»¶ä¹‹é—´çµæ´»åä½œã€‚å®éªŒè¡¨æ˜ï¼ŒECHOåœ¨Qwenç³»åˆ—å¤§æ¨¡å‹ä¸Šçš„RLä»»åŠ¡ä¸­ï¼Œæ—¢èƒ½æå‡ç¡¬ä»¶åˆ©ç”¨ç‡ï¼Œä¹Ÿèƒ½ä¸ä¼ ç»Ÿå…¨åŒä½ç³»ç»Ÿåœ¨æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½æŒå¹³ç”šè‡³è¶…è¶Šï¼ŒéªŒè¯äº†å»ä¸­å¿ƒåŒ–RLè®­ç»ƒçš„å¯è¡Œæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/ECHO_Decoupling_Inference_and_Training_for_Large-Scale_RL_Alignment_on_Heterogeneous_Swarms.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation"><a href="#4-Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation" class="headerlink" title="4. Posterior-GRPO: Rewarding Reasoning Processes in Code Generation"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Posterior-GRPO__Rewarding_Reasoning_Processes_in_Code_Generation.pdf">Posterior-GRPO: Rewarding Reasoning Processes in Code Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The State Key Laboratory of Blockchain and Data Security</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•Posterior-GRPO (P-GRPO)ç”¨äºå¤§è¯­è¨€æ¨¡å‹ä»£ç ç”Ÿæˆï¼Œåˆ›æ–°æ€§åœ°å°†åŸºäºä¼˜åŒ–-é™çº§ï¼ˆOD-basedï¼‰æ–¹æ³•è®­ç»ƒçš„æ€ç»´å¥–åŠ±æ¨¡å‹ä¸ä¼ ç»Ÿç»“æœå¥–åŠ±ç»“åˆï¼Œé€šè¿‡åœ¨ä»…æœ‰æ­£ç¡®ç»“æœæ—¶èµ‹äºˆæ€ç»´å¥–åŠ±ï¼Œæœ‰æ•ˆé˜²æ­¢å¥–åŠ±æŠ•æœºã€‚å®éªŒè¯æ˜ï¼ŒP-GRPOåœ¨å¤šä¸ªä»£ç ä¸æ•°å­¦æ¨ç†åŸºå‡†ä¸Šæå‡Pass@1å¹³å‡4.5%ï¼Œè¡¨ç°ä¸GPT-4-Turboç›¸å½“ï¼Œä¸”æ³›åŒ–èƒ½åŠ›å¼ºã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Posterior-GRPO_Rewarding_Reasoning_Processes_in_Code_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-Domain-driven-Metrics-for-Reinforcement-Learning-A-Case-Study-on-Epidemic-Control-using-Agent-based-Simulation"><a href="#5-Domain-driven-Metrics-for-Reinforcement-Learning-A-Case-Study-on-Epidemic-Control-using-Agent-based-Simulation" class="headerlink" title="5. Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Domain-driven_Metrics_for_Reinforcement_Learning__A_Case_Study_on_Epidemic_Control_using_Agent-based.pdf">Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Thoughtworks Technologies</span></p>
<p>æœ¬æ–‡æå‡ºäº†ç»“åˆé¢†åŸŸçŸ¥è¯†å’Œä¼ ç»Ÿå¥–åŠ±çš„â€˜Domain-driven RL metricsâ€™è¯„ä»·æŒ‡æ ‡ä½“ç³»ï¼Œç”¨äºå…¬æ­£ã€ç¨³å¥åœ°æ¯”è¾ƒç”¨äºç†æ€§ä½“å»ºæ¨¡ä¸ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚é€šè¿‡åœ¨ç–«æƒ…æ§åˆ¶çš„ä»£ç†å»ºæ¨¡æ¡ˆä¾‹ä¸­ï¼Œåˆ©ç”¨å¤šç§RLç®—æ³•å’Œæ–°çš„å¤åˆè¯„ä»·æŒ‡æ ‡ï¼ˆå¦‚çŠ¶æ€ç©ºé—´è¦†ç›–ã€æœ€ä½³åºåˆ—ç™¾åˆ†æ¯”ç­‰ï¼‰ï¼Œæœ‰æ•ˆæå‡äº†ç®—æ³•é€‰æ‹©çš„å¯é æ€§å’Œè§£é‡Šæ€§ï¼Œç»“è®ºæ˜¾ç¤ºæ–°æŒ‡æ ‡ä½“ç³»æ¯”å•ä¸€å‡å€¼å¥–åŠ±æ›´ç¨³å¥ã€æ›´å…·ä¿¡ä»»åº¦ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Domain-driven_Metrics_for_Reinforcement_Learning_A_Case_Study_on_Epidemic_Control_using_Agent-based_Simulation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-Towards-Hallucination-Free-Music-A-Reinforcement-Learning-Preference-Optimization-Framework-for-Reliable-Song-Generation"><a href="#6-Towards-Hallucination-Free-Music-A-Reinforcement-Learning-Preference-Optimization-Framework-for-Reliable-Song-Generation" class="headerlink" title="6. Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Hallucination-Free_Music__A_Reinforcement_Learning_Preference_Optimization_Framework_for_Rel.pdf">Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Wuhan University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„åå¥½ä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºå‡å°‘æ­Œè¯åˆ°æ­Œæ›²ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¹»è§‰ç°è±¡ï¼ˆå³ç”Ÿæˆå†…å®¹ä¸æ­Œè¯ä¸ç¬¦ï¼‰ã€‚æ–¹æ³•åŒ…æ‹¬æ„å»ºåŸºäºéŸ³ç´ é”™è¯¯ç‡ï¼ˆPERï¼‰çš„å¹»è§‰åå¥½æ•°æ®é›†ï¼Œå¹¶å®ç°ä¸‰ç§RLåå¥½ä¼˜åŒ–ç­–ç•¥ï¼šDPOã€PPOå’ŒGRPOï¼Œé€šè¿‡å¥–åŠ±å»ºæ¨¡å’Œå¯¹æ¯”å­¦ä¹ ï¼Œæœ‰æ•ˆæŠ‘åˆ¶äº†å¹»è§‰çš„å‘ç”Ÿã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æ˜¾è‘—é™ä½å¹»è§‰æ¯”ä¾‹ï¼ˆå¦‚DPO+RSæ–¹æ³•å¹»è§‰æ ·æœ¬ä¸‹é™16.5%ï¼Œé«˜è´¨é‡æ ·æœ¬æå‡38.47%ï¼‰ï¼Œä¸”ä¸æŸå¤±éŸ³ä¹æ€§å’ŒéŸ³è´¨ï¼Œå…·å¤‡è‰¯å¥½è¿ç§»æ€§ï¼Œå¯æ‰©å±•åˆ°é£æ ¼æ§åˆ¶ç­‰å…¶ä»–ç”Ÿæˆä»»åŠ¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Hallucination-Free_Music_A_Reinforcement_Learning_Preference_Optimization_Framework_for_Reliable_Song_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-Anti-Jamming-Sensing-with-Distributed-Reconfigurable-Intelligent-Metasurface-Antennas"><a href="#7-Anti-Jamming-Sensing-with-Distributed-Reconfigurable-Intelligent-Metasurface-Antennas" class="headerlink" title="7. Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Anti-Jamming_Sensing_with_Distributed_Reconfigurable_Intelligent_Metasurface_Antennas.pdf">Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Xiâ€™an Jiaotong University</span></p>
<p>æœ¬æ–‡æå‡ºåˆ©ç”¨åˆ†å¸ƒå¼å¯é‡æ„æ™ºèƒ½è¶…è¡¨é¢å¤©çº¿ï¼ˆRIMSAï¼‰è¿›è¡Œå°„é¢‘æ„ŸçŸ¥ï¼Œé€šè¿‡ç­–ç•¥ç½‘ç»œï¼ˆæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼‰ä¼˜åŒ–æ³¢æŸå½¢æˆæ¨¡å¼åŠç¥ç»ç½‘ç»œæ˜ å°„æ¥æ”¶ä¿¡å·ï¼Œå®ç°ç«¯åˆ°ç«¯æ„ŸçŸ¥ã€‚ä¸ºæŠµæŠ—å¹²æ‰°ï¼Œè®¾è®¡äº†èåˆæ¥æ”¶ä¿¡å·SINRçš„å¤šå› å­æŸå¤±å‡½æ•°ï¼Œæ˜¾è‘—æå‡äº†æŠ—å¹²æ‰°ä¸‹çš„æ„ŸçŸ¥ç²¾åº¦ï¼Œä»¿çœŸè¡¨æ˜åˆ†å¸ƒå¼RIMSAç³»ç»Ÿå¯¹ç¯å¢ƒåŠæ”»å‡»æœ‰æ›´å¼ºé€‚åº”æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Anti-Jamming_Sensing_with_Distributed_Reconfigurable_Intelligent_Metasurface_Antennas.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation"><a href="#8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation" class="headerlink" title="8. Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Genie_Envisioner__A_Unified_World_Foundation_Platform_for_Robotic_Manipulation.pdf">Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">AgiBot Genie Team</span></p>
<p>è¯¥è®ºæ–‡æå‡ºGenie Envisioner (GE)ï¼Œä¸€ä¸ªé›†è§†é¢‘ç”Ÿæˆã€ç­–ç•¥å­¦ä¹ ã€è¯„ä¼°å’Œä»¿çœŸäºä¸€ä½“çš„æœºå™¨äººæ“ä½œä¸–ç•ŒåŸºç¡€å¹³å°ã€‚GEåŒ…æ‹¬ä¸‰å¤§æ¨¡å—ï¼šGE-Baseä¸ºå¤§è§„æ¨¡æŒ‡ä»¤æ¡ä»¶è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæ•æ‰çœŸå®æœºå™¨äººäº¤äº’çš„ç©ºé—´ã€æ—¶é—´å’Œè¯­ä¹‰åŠ¨æ€ï¼›GE-Acté€šè¿‡è½»é‡åŒ–è§£ç å™¨å°†è§†è§‰è¡¨å¾æ˜ å°„ä¸ºå¯æ‰§è¡ŒåŠ¨ä½œè½¨è¿¹ï¼Œå®ç°å¤šå¹³å°æ³›åŒ–å’Œé«˜æ•ˆç­–ç•¥æ¨æ–­ï¼›GE-Simä¸ºåŠ¨ä½œæ¡ä»¶ç¥ç»æ¨¡æ‹Ÿå™¨ï¼Œæ”¯æŒé«˜ä¿çœŸé—­ç¯ç­–ç•¥è¯„ä¼°ä¸å¹¶è¡Œä»¿çœŸã€‚å¹³å°è¿˜é…å¤‡EWMBenchæ ‡å‡†åŒ–è¯„æµ‹å·¥å…·ï¼Œç»¼åˆè¡¡é‡è§†è§‰çœŸå®åº¦ã€ç‰©ç†ä¸€è‡´æ€§å’ŒæŒ‡ä»¤-åŠ¨ä½œå¯¹é½ã€‚å®éªŒè¯æ˜GEåœ¨å¤šç§æœºå™¨äººå¹³å°å’Œå¤æ‚ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·å¤‡æå¼ºçš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Genie_Envisioner_A_Unified_World_Foundation_Platform_for_Robotic_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-TRAJEVO-Trajectory-Prediction-Heuristics-Design-via-LLM-driven-Evolution"><a href="#9-TRAJEVO-Trajectory-Prediction-Heuristics-Design-via-LLM-driven-Evolution" class="headerlink" title="9. TRAJEVO: Trajectory Prediction Heuristics Design via LLM-driven Evolution"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/TrajEvo__Trajectory_Prediction_Heuristics_Design_via_LLM-driven_Evolution.pdf">TRAJEVO: Trajectory Prediction Heuristics Design via LLM-driven Evolution</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">KAIST, Korea Advanced Institute of Science and Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºTRAJEVOæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è¿›åŒ–ç®—æ³•è‡ªåŠ¨ç”Ÿæˆå’Œä¼˜åŒ–è½¨è¿¹é¢„æµ‹å¯å‘å¼æ–¹æ³•ï¼Œåˆ©ç”¨Cross-Generation Elite Samplingå’ŒStatistics Feedback Loopå¢å¼ºå¯å‘å¼å¤šæ ·æ€§ä¸æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTRAJEVOåœ¨å¤šæ•°æ®é›†ä¸Šè¶…è¶Šä¼ ç»Ÿå¯å‘å¼æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æœªè§åˆ†å¸ƒåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ä¼˜äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒæ—¶å…·å¤‡é«˜é€Ÿåº¦å’Œå¯è§£é‡Šæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/TRAJEVO_Trajectory_Prediction_Heuristics_Design_via_LLM-driven_Evolution.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-OMNIEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks"><a href="#10-OMNIEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks" class="headerlink" title="10. OMNIEAR: Benchmarking Agent Reasoning in Embodied Tasks"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/OmniEAR__Benchmarking_Agent_Reasoning_in_Embodied_Tasks.pdf">OMNIEAR: Benchmarking Agent Reasoning in Embodied Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zhejiang University</span></p>
<p>OmniEARæå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å…·èº«ä»»åŠ¡ä¸­æ¨ç†èƒ½åŠ›çš„ç»¼åˆæ¡†æ¶ï¼ŒåŒ…æ‹¬EAR-Simç¯å¢ƒå»ºæ¨¡ã€è‡ªåŠ¨åŒ–åœºæ™¯ç”Ÿæˆå’ŒEAR-Benchç³»ç»Ÿæ€§è¯„æµ‹ï¼Œè¦†ç›–ç‰©ç†å±æ€§æ¨ç†ã€å·¥å…·ä½¿ç”¨ä¸å¤šæ™ºèƒ½ä½“åä½œã€‚ç»“è®ºæ˜¾ç¤ºï¼šç°æœ‰è¯­è¨€æ¨¡å‹åœ¨éœ€è¦ä»ç‰©ç†çº¦æŸæ¨ç†æ—¶æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å·¥å…·æ¨ç†å’Œéšå¼åä½œä»»åŠ¡ä¸­ï¼Œè¡¨æ˜å…·èº«æ¨ç†å¯¹AIæ¶æ„æå‡ºäº†æ–°çš„æŒ‘æˆ˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/OMNIEAR_Benchmarking_Agent_Reasoning_in_Embodied_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-CleanUpBench-Embodied-Sweeping-and-Grasping-Benchmark"><a href="#11-CleanUpBench-Embodied-Sweeping-and-Grasping-Benchmark" class="headerlink" title="11. CleanUpBench: Embodied Sweeping and Grasping Benchmark"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/CleanUpBench__Embodied_Sweeping_and_Grasping_Benchmark.pdf">CleanUpBench: Embodied Sweeping and Grasping Benchmark</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Sichuan University</span></p>
<p>æœ¬æ–‡æå‡ºäº†CleanUpBenchï¼Œä¸€ä¸ªé¢å‘ç°å®å®¶å±…ç¯å¢ƒä¸­æœåŠ¡æœºå™¨äººåŒæ¨¡æ€ï¼ˆæ‰«åœ°ä¸æŠ“å–ï¼‰æ¸…æ´ä»»åŠ¡çš„é«˜ä¿çœŸä»¿çœŸåŸºå‡†ã€‚å¹³å°åŸºäºNVIDIA Isaac Simï¼Œæ”¯æŒå¤šåœºæ™¯ã€å¤šæœºå™¨äººåä½œï¼Œå¹¶æä¾›ç³»ç»ŸåŒ–è¯„æµ‹æŒ‡æ ‡ï¼ˆç©ºé—´è¦†ç›–ã€ä»»åŠ¡å®Œæˆç‡ã€è¿åŠ¨è´¨é‡ã€è®¡ç®—æ•ˆç‡ç­‰ï¼‰ï¼Œå¹¶å†…ç½®å¤šç§å¯å‘å¼ã€è§„åˆ’ä¸å­¦ä¹ åŸºçº¿æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œå…·å¤‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ èƒ½åŠ›çš„åŒæ¨¡æ€ç®—æ³•ï¼ˆå¦‚PRIMAL2ï¼‰åœ¨å¤æ‚æ¸…æ´ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä»…å…·å¤‡å•ä¸€æ¨¡å¼çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒéªŒè¯äº†CleanUpBenchä½œä¸ºè¯„æµ‹å’Œæ¨åŠ¨ç°å®æœåŠ¡æœºå™¨äººæ™ºèƒ½å‘å±•çš„ä»·å€¼å’Œæ‰©å±•æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/CleanUpBench_Embodied_Sweeping_and_Grasping_Benchmark.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-Towards-Generalizable-Safety-in-Crowd-Navigation-via-Conformal-Uncertainty-Handling"><a href="#12-Towards-Generalizable-Safety-in-Crowd-Navigation-via-Conformal-Uncertainty-Handling" class="headerlink" title="12. Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Generalizable_Safety_in_Crowd_Navigation_via_Conformal_Uncertainty_Handling.pdf">Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of California, Riverside</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆè‡ªé€‚åº”ä¿å½¢ä¸ç¡®å®šæ€§é‡åŒ–(ACI)ä¸çº¦æŸå¼ºåŒ–å­¦ä¹ (CRL)çš„ç§»åŠ¨æœºå™¨äººç¾¤ä½“å¯¼èˆªæ–¹æ³•ã€‚æ–¹æ³•æµç¨‹ä¸ºï¼šé¦–å…ˆé€šè¿‡è§„åˆ™æˆ–å­¦ä¹ å¼è½¨è¿¹é¢„æµ‹æ¨¡å‹ä¼°ç®—äººç±»æœªæ¥è½¨è¿¹ï¼Œå¹¶ç”¨ACIåœ¨çº¿é‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§ï¼Œå°†ä¸ç¡®å®šæ€§ä¼°è®¡ä½œä¸ºè§‚å¯Ÿè¾“å…¥ï¼Œç»“åˆæ³¨æ„åŠ›æœºåˆ¶ç¼–ç äººä¸æœºå™¨äººåŠäºº-äººäº¤äº’ï¼Œæœ€åç”¨CRLçº¦æŸæœºå™¨äººå†³ç­–ä»¥å‡å°‘å¯¹äººç±»æœªæ¥è½¨è¿¹çš„ä¸å®‰å…¨ä¾µå…¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åˆ†å¸ƒå†…åŠä¸‰ç§åˆ†å¸ƒå¤–åœºæ™¯ï¼ˆé€Ÿåº¦å˜åŒ–ã€è¡Œä¸ºç­–ç•¥å˜åŒ–ã€ç¾¤ä½“åŠ¨æ€ï¼‰ä¸­ï¼Œè¯¥æ–¹æ³•å®‰å…¨æ€§å’ŒæˆåŠŸç‡å‡æ˜¾è‘—ä¼˜äºç°æœ‰SOTAåŸºçº¿ï¼Œä¸”èƒ½å¤Ÿç›´æ¥è¿ç§»åˆ°å®é™…æœºå™¨äººå¹³å°å®ç°å®‰å…¨ç¨³å¥å¯¼èˆªã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Generalizable_Safety_in_Crowd_Navigation_via_Conformal_Uncertainty_Handling.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-A-Multi-view-Landmark-Representation-Approach-with-Application-to-GNSS-Visual-Inertial-Odometry"><a href="#13-A-Multi-view-Landmark-Representation-Approach-with-Application-to-GNSS-Visual-Inertial-Odometry" class="headerlink" title="13. A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Multi-view_Landmark_Representation_Approach_with_Application_to_GNSS-Visual-Inertial_Odometry.pdf">A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Jiao Tong University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šè§†è§’çš„ä»…ä½å§¿ç‰¹å¾ç‚¹è¡¨ç¤ºæ–¹æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºGNSS-è§†è§‰-æƒ¯æ€§é‡Œç¨‹è®¡ï¼ˆGVIOï¼‰ç³»ç»Ÿï¼Œæ ¸å¿ƒæŠ€æœ¯åŒ…æ‹¬å¤šè§†è§’å‡ ä½•ç‰¹å¾å»ºæ¨¡ã€æ»¤æ³¢æ›´æ–°ä¸ç‰¹å¾ç®¡ç†ç­–ç•¥ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒä¸­æå‡äº†å®šä½æ•ˆç‡å’Œç²¾åº¦ï¼Œä¿æŒæ»¤æ³¢ä¸€è‡´æ€§ä¸”è®¡ç®—è´Ÿæ‹…æ›´ä½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Multi-view_Landmark_Representation_Approach_with_Application_to_GNSS-Visual-Inertial_Odometry.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-Information-Theoretic-Graph-Fusion-with-Vision-Language-Action-Model-for-Policy-Reasoning-and-Dual-Robotic-Control"><a href="#14-Information-Theoretic-Graph-Fusion-with-Vision-Language-Action-Model-for-Policy-Reasoning-and-Dual-Robotic-Control" class="headerlink" title="14. Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Information-Theoretic_Graph_Fusion_with_Vision-Language-Action_Model_for_Policy_Reasoning_and_Dual_R.pdf">Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Hangzhou Dianzi University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¿¡æ¯è®ºå›¾èåˆè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆGF-VLAï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä»äººç±»æ¼”ç¤ºè§†é¢‘ä¸­æå–åŸºäºShannonä¿¡æ¯çš„æ‰‹-ç‰©ä½“å’Œç‰©ä½“-ç‰©ä½“äº¤äº’ï¼Œæ„å»ºæ—¶åºåœºæ™¯å›¾ï¼Œå¹¶ä¸è¯­è¨€æ¡ä»¶Transformerç»“åˆï¼Œç”Ÿæˆå¯è§£é‡Šçš„è¡Œä¸ºæ ‘å’Œç¬›å¡å°”åŠ¨ä½œæŒ‡ä»¤ã€‚é€šè¿‡é“¾å¼æ¨ç†ï¼ˆCoTï¼‰ã€è‡ªéªŒè¯å’Œè·¨æ‰‹é€‰æ‹©ç­–ç•¥å®ç°åŒè‡‚æœºå™¨äººä»»åŠ¡æ¨ç†ä¸é«˜æ•ˆåä½œï¼Œå®éªŒåœ¨å¤šç§è£…é…å’Œæ³›åŒ–ä»»åŠ¡ä¸­å–å¾—äº†é«˜äº95%å›¾å‡†ç¡®ç‡å’Œ90%ä»»åŠ¡æˆåŠŸç‡ï¼Œå±•ç°äº†ä¼˜å¼‚çš„æ³›åŒ–ä¸ç¨³å¥æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Information-Theoretic_Graph_Fusion_with_Vision-Language-Action_Model_for_Policy_Reasoning_and_Dual_Robotic_Control.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-DistillDrive-End-to-End-Multi-Mode-Autonomous-Driving-Distillation-by-Isomorphic-Hetero-Source-Planning-Model"><a href="#15-DistillDrive-End-to-End-Multi-Mode-Autonomous-Driving-Distillation-by-Isomorphic-Hetero-Source-Planning-Model" class="headerlink" title="15. DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/DistillDrive__End-to-End_Multi-Mode_Autonomous_Driving_Distillation_by_Isomorphic_Hetero-Source_Plan.pdf">DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">East China University of Science and Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºDistillDriveï¼Œä¸€ç§ç»“åˆçŸ¥è¯†è’¸é¦ã€å¼ºåŒ–å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹çš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€è‡ªåŠ¨é©¾é©¶æ¡†æ¶ã€‚æ–¹æ³•ä»¥ç»“æ„åŒ–åœºæ™¯è§„åˆ’æ¨¡å‹ä¸ºæ•™å¸ˆï¼Œé€šè¿‡å¤šæ¨¡å®ä¾‹ç›‘ç£æå‡å­¦ç”Ÿæ¨¡å‹çš„è¿åŠ¨ç‰¹å¾å­¦ä¹ ï¼Œå¹¶å¼•å…¥å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çŠ¶æ€-å†³ç­–æ˜ å°„ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹å®ç°è¿åŠ¨åˆ†å¸ƒç‰¹å¾çš„éšç©ºé—´äº¤äº’ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDistillDriveåœ¨nuSceneså’ŒNAVSIMæ•°æ®é›†ä¸Šè¾ƒåŸºçº¿æ¨¡å‹ç¢°æ’ç‡ä¸‹é™50%ï¼Œé—­ç¯æ€§èƒ½æå‡3åˆ†ï¼Œæœ‰æ•ˆå¢å¼ºäº†è‡ªåŠ¨é©¾é©¶è®¡åˆ’çš„å¤šæ ·æ€§å’Œå®‰å…¨æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/DistillDrive_End-to-End_Multi-Mode_Autonomous_Driving_Distillation_by_Isomorphic_Hetero-Source_Planning_Model.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-Towards-Embodied-Agentic-AI-Review-and-Classification-of-LLM-and-VLM-Driven-Robot-Autonomy-and-Interaction"><a href="#16-Towards-Embodied-Agentic-AI-Review-and-Classification-of-LLM-and-VLM-Driven-Robot-Autonomy-and-Interaction" class="headerlink" title="16. Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Embodied_Agentic_AI__Review_and_Classification_of_LLM-_and_VLM-Driven_Robot_Autonomy_and_Int.pdf">Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Turku</span></p>
<p>è¯¥è®ºæ–‡ç»¼è¿°äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æœºå™¨äººè‡ªä¸»æ€§å’Œäººæœºäº¤äº’ä¸­çš„é›†æˆæ–¹æ³•ï¼Œæå‡ºäº†å››ç±»é›†æˆèŒƒå¼ï¼ˆåè®®ã€æ¥å£ã€ç¼–æ’å’ŒåµŒå…¥ï¼‰åŠå¤šç§æ™ºèƒ½ä½“è§’è‰²çš„åˆ†ç±»ä½“ç³»ã€‚ç»“è®ºè®¤ä¸ºï¼ŒAgentic AIç³»ç»Ÿä½œä¸ºç”¨æˆ·ä¸æœºå™¨äººé—´çš„æ™ºèƒ½ä¸­ä»‹ï¼Œæå‡äº†æœºå™¨äººçµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¹¶å±•æœ›äº†æœªæ¥åœ¨å®é™…éƒ¨ç½²ã€è®°å¿†ã€å®‰å…¨æ€§å’Œé«˜é˜¶æ™ºèƒ½ç­‰æ–¹é¢çš„ç ”ç©¶æŒ‘æˆ˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Embodied_Agentic_AI_Review_and_Classification_of_LLM-_and_VLM-Driven_Robot_Autonomy_and_Interaction.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-Driver-Assistant-Persuading-Drivers-to-Adjust-Secondary-Tasks-Using-Large-Language-Models"><a href="#17-Driver-Assistant-Persuading-Drivers-to-Adjust-Secondary-Tasks-Using-Large-Language-Models" class="headerlink" title="17. Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Driver_Assistant__Persuading_Drivers_to_Adjust_Secondary_Tasks_Using_Large_Language_Models.pdf">Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zhejiang University</span></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é©¾é©¶è¾…åŠ©å·¥å…·ï¼Œé€šè¿‡é“è·¯é£é™©è¯„ä¼°ä¸çœ¼åŠ¨è¿½è¸ªåŠæ—¶åˆ¤æ–­å¹¶ç”Ÿæˆäººæ€§åŒ–åŠå¯¼å†…å®¹ï¼Œå¼•å¯¼é©¾é©¶å‘˜åœ¨L3è‡ªåŠ¨é©¾é©¶ä¸­åˆç†åˆ†é…æ³¨æ„åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆé™ä½é©¾é©¶å‘˜åˆ†å¿ƒä»»åŠ¡æ•°é‡å’Œè®¤çŸ¥è´Ÿè·ï¼Œæå‡é©¾é©¶å®‰å…¨æ€§åŠä½“éªŒã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Driver_Assistant_Persuading_Drivers_to_Adjust_Secondary_Tasks_Using_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-EndoMatcher-Generalizable-Endoscopic-Image-Matcher-via-Multi-Domain-Pre-training-for-Robot-Assisted-Surgery"><a href="#18-EndoMatcher-Generalizable-Endoscopic-Image-Matcher-via-Multi-Domain-Pre-training-for-Robot-Assisted-Surgery" class="headerlink" title="18. EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EndoMatcher__Generalizable_Endoscopic_Image_Matcher_via_Multi-Domain_Pre-training_for_Robot-Assisted.pdf">EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute of Automation, Chinese Academy of Sciences</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†EndoMatcherï¼Œä¸€ç§åŸºäºåŒåˆ†æ”¯Vision Transformerçš„é€šç”¨å†…é•œå›¾åƒåŒ¹é…ç½‘ç»œï¼Œé€šè¿‡å¤šåŸŸå¤§è§„æ¨¡æ•°æ®é›†Endo-Mix6çš„é¢„è®­ç»ƒå’Œæ¸è¿›å¼å¤šç›®æ ‡ä¼˜åŒ–å®ç°å¯¹å¼±çº¹ç†ã€è§†è§’å¤§å˜åŒ–ç­‰æŒ‘æˆ˜åœºæ™¯ä¸‹çš„é²æ£’å¯†é›†åŒ¹é…ã€‚ç»“æœæ˜¾ç¤ºï¼ŒEndoMatcheråœ¨å¤šä¸ªé›¶æ ·æœ¬å†…é•œæ•°æ®é›†ä¸Šå¤§å¹…æå‡åŒ¹é…æ•°é‡å’Œç²¾åº¦ï¼Œå®ç°äº†è·¨å™¨å®˜å’Œæˆåƒæ¡ä»¶çš„é«˜æ•ˆæ³›åŒ–ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EndoMatcher_Generalizable_Endoscopic_Image_Matcher_via_Multi-Domain_Pre-training_for_Robot-Assisted_Surgery.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-GhostShell-Streaming-LLM-Function-Calls-for-Concurrent-Embodied-Programming"><a href="#19-GhostShell-Streaming-LLM-Function-Calls-for-Concurrent-Embodied-Programming" class="headerlink" title="19. GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/GhostShell__Streaming_LLM_Function_Calls_for_Concurrent_Embodied_Programming.pdf">GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Leapwatt Robotics</span></p>
<p>æœ¬æ–‡æå‡ºGhostShellï¼Œä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°æœºå™¨äººå¤šé€šé“å¹¶å‘è¡Œä¸ºç¼–ç¨‹çš„ç³»ç»Ÿï¼Œæ ¸å¿ƒåŒ…æ‹¬æµå¼XMLå‡½æ•°ä»¤ç‰Œè§£æã€åŠ¨æ€æ¥å£æ˜ å°„å’Œå¤šé€šé“åŒæ­¥&#x2F;å¼‚æ­¥è°ƒåº¦ï¼Œæ”¯æŒæœºå™¨äººå„éƒ¨ä»¶çš„å¹¶è¡Œä¸ä¸²è¡ŒæŒ‡ä»¤æ‰§è¡Œã€‚å®éªŒåœ¨COCOæœºå™¨äººä¸Šï¼Œé€šè¿‡34é¡¹ä»»åŠ¡éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œåœ¨Claude-4-Sonnetæ¨¡å‹ä¸Šè¾¾åˆ°0.85çš„è¡Œä¸ºæ­£ç¡®ç‡ï¼Œå“åº”é€Ÿåº¦æ¯”ä¼ ç»Ÿå‡½æ•°è°ƒç”¨å¿«66å€ï¼Œé€‚ç”¨äºé•¿æ—¶å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå±•ç°å‡ºé«˜åº¦é€šç”¨æ€§å’Œå®æ—¶æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/GhostShell_Streaming_LLM_Function_Calls_for_Concurrent_Embodied_Programming.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-Advanced-Hybrid-Transformerâ€“LSTM-Technique-with-Attention-and-TS-Mixer-for-Drilling-Rate-of-Penetration-Prediction"><a href="#20-Advanced-Hybrid-Transformerâ€“LSTM-Technique-with-Attention-and-TS-Mixer-for-Drilling-Rate-of-Penetration-Prediction" class="headerlink" title="20. Advanced Hybrid Transformerâ€“LSTM Technique with Attention and TS-Mixer for Drilling Rate of Penetration Prediction"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Advanced_Hybrid_Transformer_LSTM_Technique_with_Attention_and_TS_Mixer_for_Drilling_Rate_of_Penetrat.pdf">Advanced Hybrid Transformerâ€“LSTM Technique with Attention and TS-Mixer for Drilling Rate of Penetration Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Engineering and Applied Sciences (UEAS), Swat, Pakistan</span></p>
<p>è¯¥è®ºæ–‡æå‡ºä¸€ç§ç»“åˆLSTMã€Transformerç¼–ç å™¨ã€TS-Mixeræ¨¡å—å’Œæ³¨æ„åŠ›æœºåˆ¶çš„æ··åˆæ·±åº¦å­¦ä¹ æ¶æ„ï¼Œç”¨äºé’»äº•ç©¿é€ç‡ï¼ˆROPï¼‰é¢„æµ‹ã€‚æ–¹æ³•åŒ…æ‹¬æ•°æ®æ¸…æ´—ã€ç‰¹å¾æ ‡å‡†åŒ–ã€æ¶æ„è®¾è®¡ï¼ˆLSTMæ•æ‰æ—¶åºä¾èµ–ï¼ŒTransformerå»ºæ¨¡å…¨å±€å…³ç³»ï¼ŒTS-Mixerèåˆé™æ€ç‰¹å¾ï¼Œæ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€åˆ†é…ç‰¹å¾æƒé‡ï¼‰ï¼Œå¹¶åœ¨çœŸå®é’»äº•æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¨¡å‹åœ¨å¤šé¡¹å›å½’æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒRÂ²è¾¾0.9991ï¼ŒMAPEä½è‡³1.1572%ï¼Œå…·å¤‡å¼ºæ³›åŒ–èƒ½åŠ›å’Œå®æ—¶é¢„æµ‹ä»·å€¼ã€‚ç»“è®ºæ˜¯è¯¥æ··åˆæ¨¡å‹èƒ½ç¨³å®šå¹¶ç²¾ç¡®é¢„æµ‹ROPï¼Œæ˜¾è‘—æå‡é’»äº•ä¼˜åŒ–ç³»ç»Ÿçš„æ™ºèƒ½åŒ–ä¸ç»æµæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Advanced_Hybrid_Transformer%E2%80%93LSTM_Technique_with_Attention_and_TS-Mixer_for_Drilling_Rate_of_Penetration_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-Learning-to-See-and-Act-Task-Aware-View-Planning-for-Robotic-Manipulation"><a href="#21-Learning-to-See-and-Act-Task-Aware-View-Planning-for-Robotic-Manipulation" class="headerlink" title="21. Learning to See and Act: Task-Aware View Planning for Robotic Manipulation"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Learning_to_See_and_Act__Task-Aware_View_Planning_for_Robotic_Manipulation.pdf">Learning to See and Act: Task-Aware View Planning for Robotic Manipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Computer Science and Engineering, Sun Yat-sen University</span></p>
<p>æœ¬æ–‡æå‡ºäº†Task-Aware View Planning (TAVP) æ¡†æ¶ï¼Œå°†ä¸»åŠ¨å¤šè§†è§’æ¢ç´¢ä¸ä»»åŠ¡æ„ŸçŸ¥ç‰¹å¾æå–ç›¸ç»“åˆï¼Œæå‡æœºå™¨äººå¤šä»»åŠ¡æ“ä½œçš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¤šè§†è§’æ¢ç´¢ç­–ç•¥å’Œä»»åŠ¡æ„ŸçŸ¥ä¸“å®¶æ··åˆï¼ˆTaskMoEï¼‰è§†è§‰ç¼–ç å™¨ï¼ŒTAVPæ˜¾è‘—æå‡äº†åœ¨RLBenchç­‰å¤šä»»åŠ¡æ“ä½œç¯å¢ƒä¸­çš„å‡†ç¡®ç‡å’Œé²æ£’æ€§ï¼Œèƒ½æœ‰æ•ˆå…‹æœé®æŒ¡é—®é¢˜å¹¶æå‡å¯¹æ–°ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Learning_to_See_and_Act_Task-Aware_View_Planning_for_Robotic_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-PhysPatch-A-Physically-Realizable-and-Transferable-Adversarial-Patch-Attack-for-Multimodal-Large-Language-Models-based-Autonomous-Driving-Systems"><a href="#22-PhysPatch-A-Physically-Realizable-and-Transferable-Adversarial-Patch-Attack-for-Multimodal-Large-Language-Models-based-Autonomous-Driving-Systems" class="headerlink" title="22. PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/PhysPatch__A_Physically_Realizable_and_Transferable_Adversarial_Patch_Attack_for_Multimodal_Large_La.pdf">PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Software Engineering, Xiâ€™an Jiaotong University</span></p>
<p>æœ¬æ–‡æå‡ºPhysPatchï¼Œä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)é©±åŠ¨çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„ç‰©ç†å¯å®ç°ä¸”é«˜è¿ç§»æ€§å¯¹æŠ—è¡¥ä¸æ”»å‡»æ¡†æ¶ã€‚æ–¹æ³•ä¸Šï¼ŒPhysPatché€šè¿‡è¯­ä¹‰æ„ŸçŸ¥çš„æ©ç åˆå§‹åŒ–ã€SVDåˆ†è§£çš„å±€éƒ¨ç‰¹å¾å¯¹é½æŸå¤±ä»¥åŠè¡¥ä¸å¼•å¯¼çš„è£å‰ª-ç¼©æ”¾ç­–ç•¥ï¼Œå®ç°äº†è¡¥ä¸ä½ç½®ã€å½¢çŠ¶å’Œå†…å®¹çš„è”åˆä¼˜åŒ–ï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”åŠ¿åœºæ©ç ç»†åŒ–æå‡ç‰©ç†å¯éƒ¨ç½²æ€§ã€‚å®éªŒè¯æ˜ï¼ŒPhysPatchåœ¨å¤šç§å¼€æºã€å•†ç”¨å’Œæ¨ç†å‹MLLMsä¸Šå‡å¤§å¹…ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆè¯¯å¯¼è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæ„ŸçŸ¥ä¸è§„åˆ’è¾“å‡ºï¼Œå¹¶å…¼å…·é«˜ç‰©ç†å¯éƒ¨ç½²æ€§ä¸å¯¹æŠ—æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/PhysPatch_A_Physically_Realizable_and_Transferable_Adversarial_Patch_Attack_for_Multimodal_Large_Language_Models-based_Autonomous_Driving_Systems.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-Examining-the-legibility-of-humanoid-robot-arm-movements-in-a-pointing-task"><a href="#23-Examining-the-legibility-of-humanoid-robot-arm-movements-in-a-pointing-task" class="headerlink" title="23. Examining the legibility of humanoid robot arm movements in a pointing task"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Examining_the_legibility_of_humanoid_robot_arm_movements_in_a_pointing_task.pdf">Examining the legibility of humanoid robot arm movements in a pointing task</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Comenius University Bratislava</span></p>
<p>è¯¥è®ºæ–‡è®¾è®¡äº†åŸºäºNICOä»¿äººæœºå™¨äººï¼Œç»“åˆå‡è§†ä¸æŒ‡å‘æ‰‹åŠ¿çš„å®éªŒï¼Œè€ƒå¯Ÿäººåœ¨è§‚å¯Ÿæˆªæ–­æœºå™¨äººæ‰‹è‡‚è¿åŠ¨æ—¶å¯¹ç›®æ ‡é¢„æµ‹çš„å‡†ç¡®æ€§å’Œååº”é€Ÿåº¦ã€‚ç»“æœæ”¯æŒå¤šæ¨¡æ€ä¼˜åŠ¿å’Œçœ¼åŠ¨ä¼˜å…ˆå‡è®¾ï¼Œè¡¨æ˜å‡è§†å’ŒæŒ‡å‘çš„ç»„åˆèƒ½æ˜¾è‘—æé«˜æ„å›¾è¯†åˆ«çš„å‡†ç¡®æ€§ï¼Œå‡è§†ä¿¡å·å¯åŠ é€Ÿäººç±»å†³ç­–ï¼Œä¸ºäººæœºäº¤äº’ä¸­è¿åŠ¨è®¾è®¡æä¾›ç†è®ºä¾æ®ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Examining_the_legibility_of_humanoid_robot_arm_movements_in_a_pointing_task.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-A-Study-of-the-Framework-and-Real-World-Applications-of-Language-Embedding-for-3D-Scene-Understanding"><a href="#24-A-Study-of-the-Framework-and-Real-World-Applications-of-Language-Embedding-for-3D-Scene-Understanding" class="headerlink" title="24. A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Study_of_the_Framework_and_Real-World_Applications_of_Language_Embedding_for_3D_Scene_Understandin.pdf">A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Victoria</span></p>
<p>æœ¬è®ºæ–‡ç³»ç»Ÿç»¼è¿°äº†å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åŠè¯­è¨€åµŒå…¥ä¸3Dé«˜æ–¯æŠ•å½±ï¼ˆGaussian Splatting, 3DGSï¼‰ç»“åˆä»¥å®ç°ä¸‰ç»´åœºæ™¯ç†è§£çš„æ–¹æ³•å’Œåº”ç”¨ã€‚å…¶æ–¹æ³•æ¢³ç†äº†3DGSçš„ç†è®ºåŸºç¡€ã€è¯­è¨€ç‰¹å¾ä¸è§†è§‰æ¨¡å‹çš„é›†æˆç­–ç•¥ï¼Œä»¥åŠåœ¨æœºå™¨äººã€è™šæ‹Ÿç°å®ã€è‡ªåŠ¨é©¾é©¶ç­‰çœŸå®ç¯å¢ƒä¸­çš„åº”ç”¨å®ä¾‹ã€‚ç»“è®ºæŒ‡å‡ºï¼Œå°½ç®¡è¿™äº›æŠ€æœ¯æ¨åŠ¨äº†3Dåœºæ™¯çš„è¯­ä¹‰ç†è§£å’Œæœºå™¨äººæ™ºèƒ½äº¤äº’ï¼Œä½†ä»é¢ä¸´è®¡ç®—æ¶ˆè€—é«˜ã€è¯­ä¹‰æ³›åŒ–èƒ½åŠ›æœ‰é™åŠå®æ—¶äº¤äº’å·¥å…·ä¸è¶³ç­‰æŒ‘æˆ˜ï¼Œæœªæ¥éœ€åœ¨é«˜æ•ˆå¯æ‰©å±•å’Œè·¨åœºæ™¯æ³›åŒ–ç­‰æ–¹é¢æŒç»­çªç ´ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Study_of_the_Framework_and_Real-World_Applications_of_Language_Embedding_for_3D_Scene_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-Will-You-Be-Aware-Eye-Trackingâ€“Based-Modeling-of-Situational-Awareness-in-Augmented-Reality"><a href="#25-Will-You-Be-Aware-Eye-Trackingâ€“Based-Modeling-of-Situational-Awareness-in-Augmented-Reality" class="headerlink" title="25. Will You Be Aware? Eye Trackingâ€“Based Modeling of Situational Awareness in Augmented Reality"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Will_You_Be_Aware__Eye_Tracking-Based_Modeling_of_Situational_Awareness_in_Augmented_Reality.pdf">Will You Be Aware? Eye Trackingâ€“Based Modeling of Situational Awareness in Augmented Reality</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Duke University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºçœ¼åŠ¨è¿½è¸ªçš„æƒ…å¢ƒæ„è¯†ï¼ˆSAï¼‰å»ºæ¨¡æ–¹æ³•ï¼Œé’ˆå¯¹ARè¾…åŠ©å¿ƒè‚ºå¤è‹ï¼ˆCPRï¼‰ä»»åŠ¡ï¼Œå¼€å‘äº†å®æ—¶æŒ‡å¯¼çš„ARåº”ç”¨å¹¶é€šè¿‡æ¨¡æ‹Ÿçªå‘äº‹ä»¶æ”¶é›†ç”¨æˆ·çœ¼åŠ¨æ•°æ®ã€‚æå‡ºäº†FixGraphPoolå›¾ç¥ç»ç½‘ç»œï¼Œå°†æ³¨è§†ç‚¹å’Œæ‰«è§†äº‹ä»¶ç»“æ„åŒ–ä¸ºæ—¶ç©ºå›¾ï¼Œå®ç°äº†å¯¹ç”¨æˆ·SAæ°´å¹³çš„é¢„æµ‹ï¼Œå‡†ç¡®ç‡è¾¾83%ï¼Œä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œå…ˆè¿›æ—¶åºæ¨¡å‹ã€‚ç»“è®ºæ˜¾ç¤ºçœ¼åŠ¨è¿½è¸ªå¯æœ‰æ•ˆå»ºæ¨¡ARç¯å¢ƒä¸‹çš„SAï¼Œå¹¶ä¸ºå®‰å…¨å…³é”®åœºæ™¯çš„ARç³»ç»Ÿè®¾è®¡æä¾›äº†æŒ‡å¯¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Will_You_Be_Aware_Eye_Tracking%E2%80%93Based_Modeling_of_Situational_Awareness_in_Augmented_Reality.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-From-Canada-to-Japan-How-10-000-km-Affect-User-Perception-in-Robot-Teleoperation"><a href="#26-From-Canada-to-Japan-How-10-000-km-Affect-User-Perception-in-Robot-Teleoperation" class="headerlink" title="26. From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/From_Canada_to_Japan__How_10,000_km_Affect_User_Perception_in_Robot_Teleoperation.pdf">From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tokyo University of Science</span></p>
<p>æœ¬è®ºæ–‡ç ”ç©¶äº†é•¿è·ç¦»æœºå™¨äººè¿œç¨‹æ“ä½œï¼ˆRToï¼‰å¯¹ç”¨æˆ·æ„ŸçŸ¥çš„å½±å“ï¼Œç‰¹åˆ«å…³æ³¨å…¶åœ¨è€å¹´æŠ¤ç†é¢†åŸŸçš„åº”ç”¨ã€‚ä½œè€…è®¾è®¡äº†åŸºäºROSå’ŒUnityçš„è¿œç¨‹æ“æ§ç³»ç»ŸåŠç”¨æˆ·è¯„ä¼°åè®®ï¼Œé€šè¿‡é—®å·å’Œäº¤äº’å®éªŒæ¯”è¾ƒæœ¬åœ°ä¸è·¨å›½ï¼ˆåŠ æ‹¿å¤§è‡³æ—¥æœ¬ï¼‰æœºå™¨äººæ“ä½œä½“éªŒã€‚ç»“æœæ˜¾ç¤ºï¼Œæ— è®ºæ˜¯æ“ä½œè´Ÿæ‹…è¿˜æ˜¯å¯ç”¨æ€§ï¼Œç”¨æˆ·å¯¹è¿œç¨‹å’Œæœ¬åœ°æœºå™¨äººæ— æ˜¾è‘—å·®å¼‚ï¼Œè¡¨æ˜è¿œç¨‹æœºå™¨äººå¯ä½œä¸ºæœ¬åœ°æ§åˆ¶çš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/From_Canada_to_Japan_How_10%2C000_km_Affect_User_Perception_in_Robot_Teleoperation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-Analyzing-the-Impact-of-Multimodal-Perception-on-Sample-Complexity-and-Optimization-Landscapes-in-Imitation-Learning"><a href="#27-Analyzing-the-Impact-of-Multimodal-Perception-on-Sample-Complexity-and-Optimization-Landscapes-in-Imitation-Learning" class="headerlink" title="27. Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Analyzing_the_Impact_of_Multimodal_Perception_on_Sample_Complexity_and_Optimization_Landscapes_in_Im.pdf">Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of California, Berkeley</span></p>
<p>æœ¬æ–‡åŸºäºç»Ÿè®¡å­¦ä¹ ç†è®ºç³»ç»Ÿåˆ†æäº†å¤šæ¨¡æ€æ„ŸçŸ¥ï¼ˆRGB-Dã€è¯­è¨€ã€ä½å§¿ï¼‰åœ¨æœºå™¨äººæ¨¡ä»¿å­¦ä¹ ä¸­çš„ä½œç”¨ï¼Œé‡‡ç”¨Rademacherå¤æ‚åº¦ã€PACå­¦ä¹ ç­‰ç†è®ºå·¥å…·ï¼Œæ­ç¤ºå¤šæ¨¡æ€é›†æˆå¯æ˜¾è‘—é™ä½æ ·æœ¬å¤æ‚åº¦å¹¶æ”¹å–„ä¼˜åŒ–æ™¯è§‚ã€‚ç»“è®ºæŒ‡å‡ºï¼Œåˆç†èåˆå¤šæ¨¡æ€ä¿¡æ¯èƒ½åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å®ç°æ›´å¼ºæ³›åŒ–èƒ½åŠ›ã€æ›´é«˜æ•°æ®æ•ˆç‡å’Œæ›´ç¨³å®šè®­ç»ƒè¡¨ç°ï¼Œç†è®ºä¸ä»¿çœŸå®éªŒå‡å¾—åˆ°äº†éªŒè¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Analyzing_the_Impact_of_Multimodal_Perception_on_Sample_Complexity_and_Optimization_Landscapes_in_Imitation_Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-Hierarchical-Deep-Deterministic-Policy-Gradient-for-Autonomous-Maze-Navigation-of-Mobile-Robots"><a href="#28-Hierarchical-Deep-Deterministic-Policy-Gradient-for-Autonomous-Maze-Navigation-of-Mobile-Robots" class="headerlink" title="28. Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Hierarchical_Deep_Deterministic_Policy_Gradient_for_Autonomous_Maze_Navigation_of_Mobile_Robots.pdf">Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Universiti Sains Malaysia</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åˆ†å±‚æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆHDDPGï¼‰ç®—æ³•ç”¨äºç§»åŠ¨æœºå™¨äººè‡ªä¸»è¿·å®«å¯¼èˆªã€‚è¯¥æ–¹æ³•é€šè¿‡é«˜å±‚ç­–ç•¥ç”Ÿæˆä¸­é—´å­ç›®æ ‡å¹¶ä½å±‚ç­–ç•¥å®ç°å…·ä½“åŠ¨ä½œï¼Œç»“åˆè‡ªé€‚åº”å‚æ•°å™ªå£°ã€ç¦»ç­–ç•¥ä¿®æ­£ã€æ¢¯åº¦è£å‰ªå’Œå¥–åŠ±å‡½æ•°é‡å¡‘ï¼Œæ˜¾è‘—æå‡äº†æ¢ç´¢æ•ˆç‡ã€è·¯å¾„è§„åˆ’å’Œè®­ç»ƒç¨³å®šæ€§ã€‚åœ¨ROSå’ŒGazeboä»¿çœŸå¹³å°ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHDDPGåœ¨å¤šä¸ªè¿·å®«å¯¼èˆªä»»åŠ¡ä¸­å°†æˆåŠŸç‡æå‡è‡³å°‘56.59%ï¼Œå¹³å‡å¥–åŠ±æå‡519.03ï¼Œæ˜æ˜¾ä¼˜äºæ ‡å‡†DDPGåŠå…¶å˜ä½“ï¼ŒéªŒè¯äº†åˆ†å±‚ç»“æ„å’Œå„é¡¹ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Hierarchical_Deep_Deterministic_Policy_Gradient_for_Autonomous_Maze_Navigation_of_Mobile_Robots.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="29-GAP-Gaussianize-Any-Point-Clouds-with-Text-Guidance"><a href="#29-GAP-Gaussianize-Any-Point-Clouds-with-Text-Guidance" class="headerlink" title="29. GAP: Gaussianize Any Point Clouds with Text Guidance"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/GAP__Gaussianize_Any_Point_Clouds_with_Text_Guidance.pdf">GAP: Gaussianize Any Point Clouds with Text Guidance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tsinghua University</span></p>
<p>æœ¬æ–‡æå‡ºGAPæ–¹æ³•ï¼Œå°†æ— é¢œè‰²çš„3Dç‚¹äº‘åœ¨æ–‡æœ¬å¼•å¯¼ä¸‹è½¬åŒ–ä¸ºé«˜ä¿çœŸ3Dé«˜æ–¯è¡¨ç¤ºã€‚å…¶æ ¸å¿ƒåŒ…æ‹¬å¤šè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼Œç»“åˆæ·±åº¦æ„ŸçŸ¥çš„å›¾åƒæ‰©æ•£æ¨¡å‹å’Œè¡¨é¢é”šå®šæœºåˆ¶ä¿è¯å‡ ä½•ç²¾åº¦ï¼Œå¹¶å¼•å…¥åŸºäºæ‰©æ•£çš„é«˜æ–¯è¡¥å…¨ç­–ç•¥ä¿®å¤éš¾ä»¥è§‚æµ‹åŒºåŸŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGAPåœ¨å¤šç§æ•°æ®é›†å’Œåœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½é«˜æ•ˆç”Ÿæˆç»“æ„ç²¾ç¡®ã€å¤–è§‚å¤šæ ·çš„3Dé«˜æ–¯ä½“ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/GAP_Gaussianize_Any_Point_Clouds_with_Text_Guidance.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-Uni-CoT-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision"><a href="#30-Uni-CoT-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision" class="headerlink" title="30. Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Uni-cot__Towards_Unified_Chain-of-Thought_Reasoning_Across_Text_and_Vision.pdf">Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Academy of AI for Science</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†Uni-CoTï¼Œä¸€ç§ç»Ÿä¸€çš„é“¾å¼æ€ç»´(CoT)æ¡†æ¶ï¼Œé‡‡ç”¨å®è§‚ä»»åŠ¡è§„åˆ’ä¸å¾®è§‚å­ä»»åŠ¡æ‰§è¡Œçš„åŒå±‚å±‚æ¬¡åŒ–æ¨ç†ç»“æ„ï¼Œå°†å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡åˆ†è§£ä¸ºé«˜å±‚è§„åˆ’å’Œé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)å»ºæ¨¡çš„å­ä»»åŠ¡ï¼Œé€šè¿‡ç»“æ„åŒ–è®­ç»ƒèŒƒå¼å®ç°é«˜æ•ˆç›‘ç£å’Œåå¥½å¾®è°ƒã€‚å®éªŒè¡¨æ˜ï¼ŒUni-CoTåœ¨å¤šé¡¹æ¨ç†é©±åŠ¨çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨WISEã€RISEå’ŒKRISç­‰åŸºå‡†ä¸Šå–å¾—äº†é¢†å…ˆæ€§èƒ½å’Œè‰¯å¥½è§£é‡Šæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Uni-CoT_Towards_Unified_Chain-of-Thought_Reasoning_Across_Text_and_Vision.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-DART-Dual-Adaptive-Refinement-Transfer-for-Open-Vocabulary-Multi-Label-Recognition"><a href="#31-DART-Dual-Adaptive-Refinement-Transfer-for-Open-Vocabulary-Multi-Label-Recognition" class="headerlink" title="31. DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/DART__Dual_Adaptive_Refinement_Transfer_for_Open-Vocabulary_Multi-Label_Recognition.pdf">DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Sun Yat-sen University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºDARTæ¡†æ¶ï¼Œé€šè¿‡åœ¨å†»ç»“çš„è§†è§‰-è¯­è¨€é¢„è®­ç»ƒ(VLP)æ¨¡å‹åŸºç¡€ä¸Šï¼Œé›†æˆè‡ªé€‚åº”ç»†åŒ–æ¨¡å—(ARM)ä¸å¼±ç›‘ç£Patché€‰æ‹©æŸå¤±(WPS)ï¼Œå®ç°å¯¹å±€éƒ¨è§†è§‰ç‰¹å¾çš„ç²¾ç»†å®šä½ï¼ŒåŒæ—¶å¼•å…¥åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)æŒ–æ˜çš„ç±»å…³ç³»å›¾(CRG)æŒ‡å¯¼è‡ªé€‚åº”è½¬ç§»æ¨¡å—(ATM)ï¼Œä»¥å›¾æ³¨æ„åŠ›ç½‘ç»œå®Œæˆè·¨ç±»åˆ«ä¿¡æ¯ä¼ é€’å’Œå¤šæ¨¡æ€èåˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDARTåœ¨NUS-WIDEã€MS-COCOå’ŒOpen Imagesç­‰å¤šé¡¹å¼€æ”¾è¯è¡¨å¤šæ ‡ç­¾è¯†åˆ«åŸºå‡†ä¸Šå–å¾—äº†æ–°çš„SOTAæ€§èƒ½ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/DART_Dual_Adaptive_Refinement_Transfer_for_Open-Vocabulary_Multi-Label_Recognition.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-Ï‡-Symmetry-Understanding-of-3D-Shapes-via-Chirality-Disentanglement"><a href="#32-Ï‡-Symmetry-Understanding-of-3D-Shapes-via-Chirality-Disentanglement" class="headerlink" title="32. Ï‡: Symmetry Understanding of 3D Shapes via Chirality Disentanglement"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Symmetry_Understanding_of_3D_Shapes_via_Chirality_Disentanglement.pdf">Ï‡: Symmetry Understanding of 3D Shapes via Chirality Disentanglement</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Bonn</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºDiff3Fæ¡†æ¶çš„æ— ç›‘ç£ç®¡é“ï¼Œé€šè¿‡ä»2DåŸºç¡€è§†è§‰æ¨¡å‹ï¼ˆDINO-V2å’ŒStableDiffusionï¼‰æå–ç‰¹å¾ï¼Œå®ç°3Då½¢çŠ¶é¡¶ç‚¹æ‰‹æ€§ï¼ˆchiralityï¼‰ä¿¡æ¯çš„è§£è€¦ä¸å»ºæ¨¡ã€‚æ–°ç‰¹å¾èƒ½æœ‰æ•ˆåŒºåˆ†3Då½¢çŠ¶çš„å·¦å³éƒ¨åˆ†ï¼Œè§£å†³äº†ä¼ ç»Ÿç‰¹å¾åœ¨å½¢çŠ¶åŒ¹é…å’Œåˆ†å‰²æ—¶çš„å·¦å³æ­§ä¹‰ï¼Œåœ¨å¤šæ•°æ®é›†å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”å¯¹å±€éƒ¨æˆ–å„å‘å¼‚æ€§å½¢çŠ¶å…·æœ‰è‰¯å¥½é²æ£’æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/%CF%87_Symmetry_Understanding_of_3D_Shapes_via_Chirality_Disentanglement.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-SMOL-MapSeg-Show-Me-One-Label"><a href="#33-SMOL-MapSeg-Show-Me-One-Label" class="headerlink" title="33. SMOL-MapSeg: Show Me One Label"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SMOL-MapSeg__Show_Me_One_Label.pdf">SMOL-MapSeg: Show Me One Label</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Leibniz University Hannover</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æŒ‰éœ€å£°æ˜å¼ï¼ˆONDï¼‰çŸ¥è¯†é©±åŠ¨çš„æç¤ºæ–¹æ³•ï¼Œç”¨äºæŒ‡å¯¼åŸºç¡€åˆ†å‰²æ¨¡å‹ï¼ˆä»¥SAMä¸ºåŸºç¡€ï¼‰åœ¨å†å²åœ°å›¾ä¸Šè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚æ–¹æ³•é€šè¿‡æºå›¾åƒ-æ ‡ç­¾å¯¹ä½œä¸ºæ˜¾å¼æç¤ºï¼Œç»“åˆæ–°è®¾è®¡çš„Prompt Encoderï¼Œå¯¹å†å²åœ°å›¾è¿›è¡Œå•æ ·æœ¬ç»†ç²’åº¦åˆ†å‰²ï¼Œä¸”æ”¯æŒæ–°ç±»åˆ«çš„å°‘æ ·æœ¬è‡ªé€‚åº”ã€‚å®éªŒè¡¨æ˜ï¼ŒSMOL-MapSegåœ¨å¹³å‡IoUä¸Šä¼˜äºUNetåŸºçº¿æ¨¡å‹ï¼Œå¯¹æ–°ç±»åˆ«æœ‰è¾ƒå¥½æ³›åŒ–èƒ½åŠ›ï¼Œä½†å¯¹å±€éƒ¨ç‰¹å¾ä¸æ˜æ˜¾çš„ç±»åˆ«å¦‚ç™½è‰²æ°´åŸŸåˆ†å‰²æœ‰å±€é™ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SMOL-MapSeg_Show_Me_One_Label.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-How-and-Why-Taming-Flow-Matching-for-Unsupervised-Anomaly-Detection-and-Localization"><a href="#34-How-and-Why-Taming-Flow-Matching-for-Unsupervised-Anomaly-Detection-and-Localization" class="headerlink" title="34. How and Why: Taming Flow Matching for Unsupervised Anomaly Detection and Localization"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/How_and_Why__Taming_Flow_Matching_for_Unsupervised_Anomaly_Detection_and_Localization.pdf">How and Why: Taming Flow Matching for Unsupervised Anomaly Detection and Localization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Electronic Science and Technology of China</span></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºæµåŒ¹é…ï¼ˆFlow Matching, FMï¼‰åŠå…¶å˜ä½“â€œWorst Transport Flow Matchingâ€ï¼ˆWT-Flowï¼‰çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ä¸å®šä½æ–°èŒƒå¼ï¼Œé€šè¿‡ç†è®ºå’Œå®è¯åˆ†ææ­ç¤ºäº†ä¼ ç»Ÿæµæ–¹æ³•åœ¨é«˜ç»´ç©ºé—´ä¸­çš„ä¸å¯é€†æ€§å’Œè¡¨è¾¾èƒ½åŠ›é™åˆ¶ï¼Œå¹¶æ„å»ºäº†éæ¦‚ç‡æ€§è·¯å¾„æ§åˆ¶æœºåˆ¶ã€‚WT-Flowæ–¹æ³•é€šè¿‡å¯¹æ ·æœ¬å½’ä¸€åŒ–å’Œæ½œåœ¨äº•æ„å»ºï¼Œå®ç°äº†å¯¹å¼‚å¸¸æ ·æœ¬çš„æœ‰æ•ˆåˆ†ç¦»ï¼Œåœ¨MVTecå·¥ä¸šè§†è§‰å¼‚å¸¸æ£€æµ‹æ•°æ®é›†ä¸Šè¾¾åˆ°äº†å•å°ºåº¦ä¸‹çš„SOTAæ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†æ£€æµ‹æ•ˆç‡å’Œç²¾åº¦ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/How_and_Why_Taming_Flow_Matching_for_Unsupervised_Anomaly_Detection_and_Localization.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-Explaining-Similarity-in-Visionâ€“Language-Encoders-with-Weighted-Banzhaf-Interactions"><a href="#35-Explaining-Similarity-in-Visionâ€“Language-Encoders-with-Weighted-Banzhaf-Interactions" class="headerlink" title="35. Explaining Similarity in Visionâ€“Language Encoders with Weighted Banzhaf Interactions"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Explaining_Similarity_in_Vision-Language_Encoders_with_Weighted_Banzhaf_Interactions.pdf">Explaining Similarity in Visionâ€“Language Encoders with Weighted Banzhaf Interactions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Warsaw</span></p>
<p>æœ¬æ–‡æå‡ºäº†FIXLIPï¼Œä¸€ç§åŸºäºåŠ æƒBanzhafäº¤äº’æŒ‡æ•°çš„è§†è§‰-è¯­è¨€ç¼–ç å™¨è§£é‡Šæ–¹æ³•ï¼Œé€šè¿‡æ¸¸æˆç†è®ºå»ºæ¨¡å’Œé«˜æ•ˆçš„è·¨æ¨¡æ€é‡‡æ ·ç­–ç•¥ï¼Œèƒ½å¤Ÿåˆ†è§£å›¾åƒ-æ–‡æœ¬å¯¹åœ¨ç¼–ç å™¨ä¸­çš„ç›¸ä¼¼æ€§å¾—åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFIXLIPåœ¨MS COCOå’ŒImageNet-1kç­‰åŸºå‡†ä¸Šæ¯”ä¸€é˜¶å½’å› æ–¹æ³•æ›´å‡†ç¡®ã€å¯æ‰©å±•ï¼Œå¹¶æ”¯æŒæ¨¡å‹é—´å¯¹æ¯”åˆ†æã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Explaining_Similarity_in_Vision%E2%80%93Language_Encoders_with_Weighted_Banzhaf_Interactions.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation"><a href="#36-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation" class="headerlink" title="36. UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/UNCAGE__Contrastive_Attention_Guidance_for_Masked_Generative_Transformers_in_Text-to-Image_Generatio.pdf">UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Seoul National University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºUNCAGEï¼ˆUnmasking with Contrastive Attention Guidanceï¼‰ï¼Œä¸ºMasked Generative Transformersï¼ˆMGTsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„å¤šç‰©ä½“å’Œå±æ€§ç»‘å®šé—®é¢˜æä¾›é¦–ä¸ªè®­ç»ƒå…æ–¹æ³•ã€‚UNCAGEåˆ©ç”¨æ³¨æ„åŠ›å›¾ï¼Œé€šè¿‡æ­£è´Ÿå¯¹æ¯”æœºåˆ¶æŒ‡å¯¼tokenè§£é®é¡ºåºï¼Œä¼˜å…ˆè§£é”èƒ½æ¸…æ™°ä»£è¡¨å•ä¸€å¯¹è±¡çš„tokenï¼Œæå‡åˆæˆä¸€è‡´æ€§ä¸”æ¨ç†å¼€é”€æå°ã€‚å®éªŒè¡¨æ˜UNCAGEåœ¨å¤šåŸºå‡†å’Œå¤šæŒ‡æ ‡ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†æ–‡æœ¬-å›¾åƒå¯¹é½å’Œäººç±»åå¥½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/UNCAGE_Contrastive_Attention_Guidance_for_Masked_Generative_Transformers_in_Text-to-Image_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-MagicHOI-Leveraging-3D-Priors-for-Accurate-Hand-object-Reconstruction-from-Short-Monocular-Video-Clips"><a href="#37-MagicHOI-Leveraging-3D-Priors-for-Accurate-Hand-object-Reconstruction-from-Short-Monocular-Video-Clips" class="headerlink" title="37. MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MagicHOI__Leveraging_3D_Priors_for_Accurate_Hand-object_Reconstruction_from_Short_Monocular_Video_Cl.pdf">MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>MagicHOIæå‡ºäº†ä¸€ç§ç»“åˆç»“æ„å…‰æ¢å¤ã€NVSæ‰©æ•£æ¨¡å‹å’Œå¯è§æ€§åŠ æƒç­–ç•¥çš„æ–¹æ³•ï¼Œä»çŸ­æ—¶å•ç›®è§†é¢‘ä¸­å®ç°å‡†ç¡®çš„æ‰‹ä¸ç‰©ä½“ä¸‰ç»´é‡å»ºï¼ŒåŒ…æ‹¬å¤„ç†é®æŒ¡åŒºåŸŸã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰é™è§†è§’å’Œé®æŒ¡ä¸¥é‡çš„åœºæ™¯ä¸‹ï¼Œé‡å»ºè´¨é‡å’Œæ‰‹ç‰©ä½“ç©ºé—´å…³ç³»å‡ä¼˜äºå½“å‰ä¸»æµæ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MagicHOI_Leveraging_3D_Priors_for_Accurate_Hand-object_Reconstruction_from_Short_Monocular_Video_Clips.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-F2PASeg-Feature-Fusion-for-Pituitary-Anatomy-Segmentation-in-Endoscopic-Surgery"><a href="#38-F2PASeg-Feature-Fusion-for-Pituitary-Anatomy-Segmentation-in-Endoscopic-Surgery" class="headerlink" title="38. F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/F2PASeg__Feature_Fusion_for_Pituitary_Anatomy_Segmentation_in_Endoscopic_Surgery.pdf">F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science &amp; Innovation, Chinese Academy of Sciences</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†F2PASegæ–¹æ³•ï¼Œé€šè¿‡åœ¨è§†é¢‘çº§è¯­ä¹‰åˆ†å‰²ä¸­å¼•å…¥ç‰¹å¾èåˆæ¨¡å—å’ŒLoRAåˆ†æ”¯ï¼Œç»“åˆé«˜åˆ†è¾¨ç‡å›¾åƒç‰¹å¾ä¸æ·±å±‚è¯­ä¹‰ä¿¡æ¯ï¼Œå®ç°äº†å¯¹å¤æ‚æœ¯ä¸­åœºæ™¯ä¸‹å‚ä½“è§£å‰–ç»“æ„çš„é«˜æ•ˆåˆ†å‰²ã€‚å®éªŒè¡¨æ˜ï¼ŒF2PASegåœ¨æ–°æ„å»ºçš„PASæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„ç²¾åº¦å’Œå®æ—¶æ€§ï¼Œæœ‰æ•ˆæå‡äº†æ‰‹æœ¯å…³é”®ç»“æ„çš„åˆ†å‰²å‡†ç¡®ç‡å¹¶æ»¡è¶³æœ¯ä¸­åº”ç”¨éœ€æ±‚ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/F2PASeg_Feature_Fusion_for_Pituitary_Anatomy_Segmentation_in_Endoscopic_Surgery.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-Smoothing-Slot-Attention-Iterations-and-Recurrences"><a href="#39-Smoothing-Slot-Attention-Iterations-and-Recurrences" class="headerlink" title="39. Smoothing Slot Attention Iterations and Recurrences"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Smoothing_Slot_Attention_Iterations_and_Recurrences.pdf">Smoothing Slot Attention Iterations and Recurrences</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Aalto University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºSmoothSAæ–¹æ³•ï¼Œé€šè¿‡é¢„çƒ­å†·å¯åŠ¨æŸ¥è¯¢å’ŒåŒºåˆ†ä¸åŒå¸§çš„èšåˆå˜æ¢ï¼Œè§£å†³Slot Attentionåœ¨å›¾åƒå’Œè§†é¢‘å¯¹è±¡èšåˆä¸­çš„å†·å¯åŠ¨å’Œå˜æ¢åŒè´¨æ€§é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒSmoothSAåœ¨å¯¹è±¡å‘ç°ã€è¯†åˆ«å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†æ–°çš„SOTAè¡¨ç°ï¼Œæ˜¾è‘—æå‡äº†å¯¹è±¡è¡¨ç¤ºè´¨é‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Smoothing_Slot_Attention_Iterations_and_Recurrences.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-CT-GRAPH-Hierarchical-Graph-Attention-Network-for-Anatomy-Guided-CT-Report-Generation"><a href="#40-CT-GRAPH-Hierarchical-Graph-Attention-Network-for-Anatomy-Guided-CT-Report-Generation" class="headerlink" title="40. CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT Report Generation"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/CT-GRAPH__Hierarchical_Graph_Attention_Network_for_Anatomy-Guided_CT_Report_Generation.pdf">CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT Report Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute for AI in Medicine (IKIM), University Hospital Essen</span></p>
<p>è®ºæ–‡æå‡ºäº†CT-GRAPHï¼Œä¸€ç§ç»“åˆé¢„è®­ç»ƒ3DåŒ»ç–—ç‰¹å¾ç¼–ç å™¨å’Œè§£å‰–ç»“æ„åˆ†å±‚å›¾æ³¨æ„åŠ›ç½‘ç»œçš„æ–¹æ³•ï¼Œé€šè¿‡è§£å‰–æ©ç æå–ç»†ç²’åº¦å™¨å®˜åŠå…¨å±€ç‰¹å¾ï¼Œå¹¶æ„å»ºå¤šå±‚æ¬¡è§£å‰–å­¦å›¾ï¼Œåˆ©ç”¨å›¾æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç‰¹å¾èšåˆï¼Œæœ€ç»ˆå°†ç»“æ„åŒ–ç‰¹å¾è¾“å…¥å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯¦ç»†åŒ»å­¦æŠ¥å‘Šã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CTæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†ä¸´åºŠå®ä½“F1åˆ†æ•°ï¼ˆæå‡7.9%ï¼‰ï¼Œåœ¨ç»†ç²’åº¦å’Œå…¨å±€è¯Šæ–­ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/CT-GRAPH_Hierarchical_Graph_Attention_Network_for_Anatomy-Guided_CT_Report_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-PriorRG-Prior-Guided-Contrastive-Pre-training-and-Coarse-to-Fine-Decoding-for-Chest-X-ray-Report-Generation"><a href="#41-PriorRG-Prior-Guided-Contrastive-Pre-training-and-Coarse-to-Fine-Decoding-for-Chest-X-ray-Report-Generation" class="headerlink" title="41. PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/PriorRG__Prior-Guided_Contrastive_Pre-training_and_Coarse-to-Fine_Decoding_for_Chest_X-ray_Report_Ge.pdf">PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Xidian University</span></p>
<p>PriorRGæå‡ºäº†ä¸€ä¸ªé’ˆå¯¹èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç»“åˆäº†æ‚£è€…ç‰¹å®šçš„ä¸´åºŠå…ˆéªŒä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸´åºŠèƒŒæ™¯å’Œè¿‘æœŸå½±åƒã€‚å…¶æ–¹æ³•åŒ…æ‹¬å…ˆéªŒå¼•å¯¼çš„å¯¹æ¯”é¢„è®­ç»ƒï¼Œæå‡å›¾åƒä¸æ–‡æœ¬çš„è¯­ä¹‰å¯¹é½èƒ½åŠ›ï¼Œä»¥åŠå…ˆéªŒæ„ŸçŸ¥çš„ç²—åˆ°ç»†è§£ç ç­–ç•¥ï¼Œé€æ­¥èåˆä¸´åºŠè¯­å¢ƒã€ç–¾ç—…è¿›å±•å’Œåˆ†å±‚è§†è§‰ç‰¹å¾ï¼Œæ˜¾è‘—æå‡ç”ŸæˆæŠ¥å‘Šçš„ä¸´åºŠå‡†ç¡®æ€§ä¸æµç•…åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPriorRGåœ¨MIMIC-CXRåŠMIMIC-ABNç­‰ä¸»æµåŒ»å­¦æ•°æ®é›†ä¸Šï¼ŒæŠ¥å‘Šç”Ÿæˆå’ŒåŒ»å­¦å›¾åƒ-æ–‡æœ¬æ£€ç´¢ä»»åŠ¡å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œéƒ¨ç½²æ•ˆç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/PriorRG_Prior-Guided_Contrastive_Pre-training_and_Coarse-to-Fine_Decoding_for_Chest_X-ray_Report_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-Textual-Inversion-for-Efficient-Adaptation-of-Open-Vocabulary-Object-Detectors-Without-Forgetting"><a href="#42-Textual-Inversion-for-Efficient-Adaptation-of-Open-Vocabulary-Object-Detectors-Without-Forgetting" class="headerlink" title="42. Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Textual_Inversion_for_Efficient_Adaptation_of_Open-Vocabulary_Object_Detectors_Without_Forgetting.pdf">Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">TNO, Intelligent Imaging, the Hague, the Netherlands</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†æ–‡æœ¬å€’ç½®ï¼ˆTextual Inversion, TIï¼‰æ–¹æ³•åº”ç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹çš„æ–°æ–¹æ³•ã€‚é€šè¿‡ä»…ä¼˜åŒ–æ–°è¯åµŒå…¥å‘é‡ã€å†»ç»“åŸæœ‰æ¨¡å‹æƒé‡ï¼Œå®ç°ä»¥æå°‘æ ·æœ¬ï¼ˆå¦‚3å¼ å›¾ç‰‡ï¼‰é«˜æ•ˆæ‰©å±•æ£€æµ‹è¯æ±‡ï¼ŒåŒæ—¶æ˜¾è‘—å‡ç¼“é—å¿˜é—®é¢˜ï¼Œä¿ç•™åŸæ¨¡å‹é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜TIåœ¨few-shotå’Œfine-grainedæ£€æµ‹ä»»åŠ¡ä¸­æ€§èƒ½æ¥è¿‘ç”šè‡³è¶…è¶Šå¸¸è§„æç¤ºå¾®è°ƒï¼ˆPrompt Tuningï¼‰æ–¹æ³•ï¼Œä¸”å¯å°†å·²å­¦ç±»åˆ«è¿ç§»åˆ°æ–°é¢†åŸŸï¼Œæ˜¾è‘—å‡å°‘å¯¹å…¨æ¨¡å‹å¾®è°ƒå’ŒåŸå§‹æ•°æ®çš„ä¾èµ–ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Textual_Inversion_for_Efficient_Adaptation_of_Open-Vocabulary_Object_Detectors_Without_Forgetting.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-mKG-RAG-Multimodal-Knowledge-Graph-Enhanced-RAG-for-Visual-Question-Answering"><a href="#43-mKG-RAG-Multimodal-Knowledge-Graph-Enhanced-RAG-for-Visual-Question-Answering" class="headerlink" title="43. mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/mKG-RAG__Multimodal_Knowledge_Graph-Enhanced_RAG_for_Visual_Question_Answering.pdf">mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Hong Kong Polytechnic University</span></p>
<p>æœ¬æ–‡æå‡ºmKG-RAGï¼Œä¸€ç§ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æ–¹æ³•åŒ…æ‹¬åˆ©ç”¨MLLMè¿›è¡Œå¤šæ¨¡æ€æ–‡æ¡£çš„å®ä½“å’Œå…³ç³»æŠ½å–ï¼Œæ„å»ºç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼Œå¹¶é€šè¿‡åŒé˜¶æ®µæ£€ç´¢ï¼ˆå‘é‡æ£€ç´¢å’Œå›¾æ£€ç´¢ï¼‰ç²¾å‡†ç­›é€‰ç›¸å…³çŸ¥è¯†ï¼Œæ˜¾è‘—æå‡VQAå‡†ç¡®ç‡ï¼Œå®éªŒç»“æœä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/mKG-RAG_Multimodal_Knowledge_Graph-Enhanced_RAG_for_Visual_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion"><a href="#44-SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion" class="headerlink" title="44. SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SGDFuse__SAM-Guided_Diffusion_for_High-Fidelity_Infrared_and_Visible_Image_Fusion.pdf">SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shandong Technology and Business University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSGDFuseçš„çº¢å¤–ä¸å¯è§å…‰å›¾åƒèåˆæ–¹æ³•ï¼Œé€šè¿‡Segment Anything Model (SAM)ç”Ÿæˆé«˜è´¨é‡è¯­ä¹‰æ©ç ï¼Œå¼•å¯¼æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå®ç°é«˜ä¿çœŸã€è¯­ä¹‰ä¸€è‡´çš„å›¾åƒèåˆã€‚æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼šç¬¬ä¸€é˜¶æ®µå¤šæ¨¡æ€ç‰¹å¾èåˆï¼Œç¬¬äºŒé˜¶æ®µç»“åˆè¯­ä¹‰æ©ç å’Œæ‰©æ•£ç”Ÿæˆè¿›è¡Œç»“æ„ç»†åŒ–å’Œè¯­ä¹‰å¢å¼ºï¼Œæ˜¾è‘—æå‡èåˆå›¾åƒåœ¨ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºSGDFuseåœ¨ä¸»æµæ•°æ®é›†å’Œå¤šé¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç»“è®ºè¡¨æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆç»Ÿä¸€è¯­ä¹‰ä¸€è‡´æ€§ä¸è§†è§‰è´¨é‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SGDFuse_SAM-Guided_Diffusion_for_High-Fidelity_Infrared_and_Visible_Image_Fusion.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-CF3-Compact-and-Fast-3D-Feature-Fields"><a href="#45-CF3-Compact-and-Fast-3D-Feature-Fields" class="headerlink" title="45. CF3: Compact and Fast 3D Feature Fields"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/CF3__Compact_and_Fast_3D_Feature_Fields.pdf">CF3: Compact and Fast 3D Feature Fields</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Seoul National University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºCF3ï¼Œä¸€ç§ä»å¤šè§†è§’2Dè§†è§‰åŸºç¡€æ¨¡å‹ç‰¹å¾é€šè¿‡åŠ æƒèåˆæå‡åˆ°3Dé«˜æ–¯ç‰¹å¾åœºçš„æ–¹æ³•ï¼Œå¹¶åœ¨æ¯ä¸ªé«˜æ–¯ç‚¹ä¸Šè®­ç»ƒè‡ªé€‚åº”è‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œç‰¹å¾å‹ç¼©ï¼Œå†é€šè¿‡è‡ªé€‚åº”ç¨€ç–åŒ–ç­–ç•¥åˆå¹¶å’Œå‰ªæå†—ä½™é«˜æ–¯ç‚¹ï¼Œå¤§å¹…é™ä½å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒCF3åœ¨å¤šç§3Dåœºæ™¯åˆ†å‰²ä¸å®šä½ä»»åŠ¡ä¸­ï¼Œèƒ½ä»¥ä»…5%é«˜æ–¯æ•°é‡è¾¾åˆ°ä¸ä¸»æµæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œä¸”æ¸²æŸ“ä¸æŸ¥è¯¢é€Ÿåº¦æ˜¾è‘—æå‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/CF3_Compact_and_Fast_3D_Feature_Fields.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-A-Study-of-Gender-Classification-Techniques-Based-on-Iris-Images-A-Deep-Survey-and-Analysis"><a href="#46-A-Study-of-Gender-Classification-Techniques-Based-on-Iris-Images-A-Deep-Survey-and-Analysis" class="headerlink" title="46. A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Study_of_Gender_Classification_Techniques_Based_on_Iris_Images__A_Deep_Survey_and_Analysis.pdf">A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Technical College of Informatics Akre, Duhok Polytechnic University</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿç»¼è¿°äº†åŸºäºè™¹è†œå›¾åƒçš„æ€§åˆ«åˆ†ç±»æŠ€æœ¯ï¼Œæ¶µç›–äº†ä¼ ç»Ÿæ–¹æ³•ä¸æ·±åº¦å­¦ä¹ ï¼ˆå¦‚CNNï¼‰åœ¨å›¾åƒé‡‡é›†ã€åˆ†å‰²ã€ç‰¹å¾æå–å’Œåˆ†ç±»ç­‰æµç¨‹ä¸­çš„åº”ç”¨ï¼Œå¹¶å¯¹ä¸»æµç®—æ³•ï¼ˆGaboræ»¤æ³¢ã€DWTã€LBPã€PCAç­‰ï¼‰åŠå…¶åœ¨å®é™…æ•°æ®é›†ä¸Šçš„è¡¨ç°è¿›è¡Œæ¯”è¾ƒåˆ†æã€‚ç»“è®ºè®¤ä¸ºï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨ç‰¹å¾è‡ªåŠ¨æå–å’Œåˆ†ç±»å‡†ç¡®ç‡æ–¹é¢è¾ƒä¼ ç»Ÿæ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œä½†ä»é¢ä¸´å›¾åƒè´¨é‡ã€ç‰¹å¾é€‰æ‹©ç­‰æŒ‘æˆ˜ï¼Œæœªæ¥å¯é€šè¿‡ä¼˜åŒ–ç‰¹å¾é€‰æ‹©ä¸åˆ†ç±»å™¨èåˆè¿›ä¸€æ­¥æé«˜å®ç”¨æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Study_of_Gender_Classification_Techniques_Based_on_Iris_Images_A_Deep_Survey_and_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-Coarse-to-Fine-Joint-Registration-of-MR-and-Ultrasound-Images-via-Imaging-Style-Transfer"><a href="#47-Coarse-to-Fine-Joint-Registration-of-MR-and-Ultrasound-Images-via-Imaging-Style-Transfer" class="headerlink" title="47. Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Coarse-to-Fine_Joint_Registration_of_MR_and_Ultrasound_Images_via_Imaging_Style_Transfer.pdf">Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Electronic Science and Technology of China</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆ3D CycleGANæ— ç›‘ç£é£æ ¼è¿ç§»ã€åˆ†å±‚å—åŒ¹é…ä»¿å°„å˜æ¢ä¸SynthMorphå±€éƒ¨å½¢å˜ä¼°è®¡çš„ç²—åˆ°ç»†å¤šé˜¶æ®µMRä¸è¶…å£°å›¾åƒé…å‡†æ–¹æ³•ã€‚æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†MRä¸USå›¾åƒçš„ä¸€è‡´æ€§ï¼Œä½†åœ¨éƒ¨åˆ†æƒ…å†µä¸‹ä¼šå¯¼è‡´ç›®æ ‡é…å‡†è¯¯å·®ï¼ˆTREï¼‰å¢åŠ ï¼Œå› æ­¤æœªæ¥éœ€ç ”ç©¶æ›´ä¸€è‡´çš„ä¸Šä¸‹æ–‡æè¿°æŒ‡æ ‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Coarse-to-Fine_Joint_Registration_of_MR_and_Ultrasound_Images_via_Imaging_Style_Transfer.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-ReasoningTrack-Chain-of-Thought-Reasoning-for-Long-term-Vision-Language-Tracking"><a href="#48-ReasoningTrack-Chain-of-Thought-Reasoning-for-Long-term-Vision-Language-Tracking" class="headerlink" title="48. ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/ReasoningTrack__Chain-of-Thought_Reasoning_for_Long-term_Vision-Language_Tracking.pdf">ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Anhui University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ReasoningTrackï¼Œä¸€ç§åŸºäºé“¾å¼æ¨ç†ï¼ˆChain-of-Thought, CoTï¼‰çš„è§†è§‰-è¯­è¨€é•¿æœŸè·Ÿè¸ªæ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹Qwen2.5-VLï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆGRPOï¼‰ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†ä¸è¯­è¨€ç”Ÿæˆèƒ½åŠ›ã€‚æ–¹æ³•ç»“åˆåŠ¨æ€è¯­è¨€æè¿°å’Œè§†è§‰ç‰¹å¾ï¼Œé€šè¿‡æ¨ç†é“¾ä¸æ–­æ›´æ–°ç›®æ ‡æè¿°ï¼Œå¹¶åœ¨ç»Ÿä¸€çš„è·Ÿè¸ªéª¨å¹²ç½‘ç»œä¸­å®ç°é«˜é²æ£’æ€§è·Ÿè¸ªã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒReasoningTrackæ˜¾è‘—æå‡äº†è·Ÿè¸ªç²¾åº¦ï¼Œå¹¶æå‡ºäº†åŒ…å«200ä¸ªè§†é¢‘åºåˆ—çš„TNLLTé•¿è§†é¢‘è§†è§‰è¯­è¨€è·Ÿè¸ªæ•°æ®é›†ï¼Œä¸ºé¢†åŸŸåç»­ç ”ç©¶å¥ å®šåŸºç¡€ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/ReasoningTrack_Chain-of-Thought_Reasoning_for_Long-term_Vision-Language_Tracking.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-SPA-Generalized-Graph-Spectral-Alignment-for-Versatile-Domain-Adaptation"><a href="#49-SPA-Generalized-Graph-Spectral-Alignment-for-Versatile-Domain-Adaptation" class="headerlink" title="49. SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SPA++__Generalized_Graph_Spectral_Alignment_for_Versatile_Domain_Adaptation.pdf">SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zhejiang University</span></p>
<p>SPA++æå‡ºäº†ä¸€ç§åŸºäºå›¾è°±å¯¹é½å’Œé‚»å±…æ„ŸçŸ¥ä¼ æ’­æœºåˆ¶çš„é€šç”¨åŸŸè‡ªé€‚åº”æ–¹æ³•ï¼Œé€šè¿‡åœ¨ç‰¹å¾ç©ºé—´è¿›è¡Œå›¾è°±å¯¹é½ã€åˆ©ç”¨é‚»å±…ä¿¡æ¯æå‡ç›®æ ‡åŸŸåˆ¤åˆ«èƒ½åŠ›ï¼Œå¹¶ç»“åˆæ•°æ®å¢å¼ºå’Œä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œé€‚é…å¤šç§å¤æ‚åŸŸè‡ªé€‚åº”åœºæ™¯ï¼ˆå¦‚UDAã€SSDAã€MSDAã€MTDAã€é•¿å°¾&#x2F;å­ç¾¤åˆ†å¸ƒï¼‰ã€‚å®éªŒè¯æ˜SPA++åœ¨å¤šç§è§†è§‰åŸŸè‡ªé€‚åº”åŸºå‡†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ›´å¼ºçš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SPA%2B%2B_Generalized_Graph_Spectral_Alignment_for_Versatile_Domain_Adaptation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-Wavelet-Guided-Dual-Frequency-Encoding-for-Remote-Sensing-Change-Detection"><a href="#50-Wavelet-Guided-Dual-Frequency-Encoding-for-Remote-Sensing-Change-Detection" class="headerlink" title="50. Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Wavelet-Guided_Dual-Frequency_Encoding_for_Remote_Sensing_Change_Detection.pdf">Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shandong Technology and Business University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†Wavelet-Guided Dual-Frequency Encoding (WGDF)æ–¹æ³•ï¼Œé€šè¿‡åœ¨è¾“å…¥é˜¶æ®µåˆ©ç”¨ç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰å°†é¥æ„Ÿå½±åƒåˆ†è§£ä¸ºé«˜ä½é¢‘åˆ†æ”¯ï¼Œåˆ†åˆ«æå–å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€è¯­ä¹‰ï¼šé«˜é¢‘åˆ†æ”¯åˆ©ç”¨DFFEå’ŒFDIDæ¨¡å—å¢å¼ºè¾¹ç¼˜å˜åŒ–å»ºæ¨¡ï¼Œä½é¢‘åˆ†æ”¯ç»“åˆTransformerå’ŒPCDMæ•æ‰å…¨å±€è¯­ä¹‰å˜åŒ–ï¼Œæœ€ç»ˆèåˆè·å¾—å˜åŒ–æ£€æµ‹ç»“æœã€‚å®éªŒåœ¨å¤šä¸ªé¥æ„Ÿæ•°æ®é›†ä¸ŠéªŒè¯äº†WGDFåœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¾¹ç¼˜æ¶ˆæ­§å’Œæ£€æµ‹å‡†ç¡®æ€§ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰è¾ƒé«˜çš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Wavelet-Guided_Dual-Frequency_Encoding_for_Remote_Sensing_Change_Detection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding"><a href="#51-RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding" class="headerlink" title="51. RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/RegionMed-CLIP__A_Region-Aware_Multimodal_Contrastive_Learning_Pre-trained_Model_for_Medical_Image_U.pdf">RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Anhui Polytechnic University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºRegionMed-CLIPï¼Œä¸€ç§ç»“åˆå…¨å±€ä¸å±€éƒ¨ç‰¹å¾çš„åŒºåŸŸæ„ŸçŸ¥å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œæ ¸å¿ƒåŒ…æ‹¬ROIï¼ˆæ„Ÿå…´è¶£åŒºåŸŸï¼‰å¤„ç†å™¨å’Œåˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæœ‰æ•ˆèåˆåŒ»å­¦å½±åƒçš„ç»†ç²’åº¦ç—…ç¶ä¿¡æ¯ä¸è¯­ä¹‰æ–‡æœ¬ã€‚é€šè¿‡è‡ªå»ºé«˜è´¨é‡åŒºåŸŸæ ‡æ³¨çš„MedRegion-500kæ•°æ®é›†ï¼Œå®éªŒåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å›¾æ–‡æ£€ç´¢å’ŒåŒ»å­¦VQAä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰SOTAæ¨¡å‹ï¼Œè¯æ˜åŒºåŸŸæ„ŸçŸ¥å¯¹æ¯”é¢„è®­ç»ƒå¯¹äºåŒ»å­¦å›¾åƒç†è§£çš„å…³é”®ä½œç”¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/RegionMed-CLIP_A_Region-Aware_Multimodal_Contrastive_Learning_Pre-trained_Model_for_Medical_Image_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-ArbiViewGen-Controllable-Arbitrary-Viewpoint-Camera-Data-Generation-for-Autonomous-Driving-via-Stable-Diffusion-Models"><a href="#52-ArbiViewGen-Controllable-Arbitrary-Viewpoint-Camera-Data-Generation-for-Autonomous-Driving-via-Stable-Diffusion-Models" class="headerlink" title="52. ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/ArbiViewGen__Controllable_Arbitrary_Viewpoint_Camera_Data_Generation_for_Autonomous_Driving_via_Stab.pdf">ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tsinghua University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ArbiViewGenï¼Œä¸€ç§åŸºäºStable Diffusionæ¨¡å‹çš„å¯æ§ä»»æ„è§†è§’ç›¸æœºå›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œä¸“ä¸ºè‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸‹å¤šè½¦è¾†å¤šæ‘„åƒå¤´ç³»ç»Ÿè®¾è®¡ã€‚æ–¹æ³•æ ¸å¿ƒåŒ…æ‹¬ç‰¹å¾æ„ŸçŸ¥è‡ªé€‚åº”è§†å›¾æ‹¼æ¥ï¼ˆFAVSï¼‰ä¸è·¨è§†å›¾ä¸€è‡´æ€§è‡ªç›‘ç£å­¦ä¹ ï¼ˆCVC-SSLï¼‰ï¼šFAVSé€šè¿‡åˆ†å±‚å‡ ä½•ä¸ç‰¹å¾åŒ¹é…ç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾ï¼ŒCVC-SSLåˆ©ç”¨æ‰©æ•£æ¨¡å‹å®ç°é—­ç¯è‡ªç›‘ç£è®­ç»ƒï¼Œæ— éœ€çœŸå®å¤–æ’è§†è§’ç›‘ç£ï¼Œä»…ä¾èµ–å¤šæ‘„åƒå¤´å›¾åƒå’Œå§¿æ€ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨nuScenesè‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šå®ç°äº†é«˜äºç°æœ‰æ–¹æ³•çš„ä»»æ„è§†è§’åˆæˆè´¨é‡ï¼Œæ˜¾è‘—æå‡äº†è·¨å¹³å°æ•°æ®å¤ç”¨èƒ½åŠ›ã€‚ç»“è®ºæŒ‡å‡ºï¼ŒArbiViewGenæœ‰æ•ˆè§£å†³äº†ç¼ºä¹å¤–æ’è§†è§’ç›‘ç£çš„éš¾é¢˜ï¼Œæå‡äº†è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿçš„é€‚åº”æ€§å’Œé²æ£’æ€§ï¼Œä½†åœ¨é«˜åº¦åŠ¨æ€ç¯å¢ƒä¸‹ç»†èŠ‚è¿˜éœ€æå‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/ArbiViewGen_Controllable_Arbitrary_Viewpoint_Camera_Data_Generation_for_Autonomous_Driving_via_Stable_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-Segmenting-the-Complex-and-Irregular-in-Two-Phase-Flows-A-Real-World-Empirical-Study-with-SAM2"><a href="#53-Segmenting-the-Complex-and-Irregular-in-Two-Phase-Flows-A-Real-World-Empirical-Study-with-SAM2" class="headerlink" title="53. Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Segmenting_the_Complex_and_Irregular_in_Two-Phase_Flows__A_Real-World_Empirical_Study_with_SAM2.pdf">Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Delft University of Technology</span></p>
<p>æœ¬è®ºæ–‡å°†å¤æ‚æ°”æ³¡åˆ†å‰²ä»»åŠ¡è§†ä¸ºè¿ç§»å­¦ä¹ é—®é¢˜ï¼Œé¦–æ¬¡å¯¹Segment Anything Model (SAM v2.1)è¿›è¡Œç»†è‡´å¾®è°ƒï¼Œå¹¶é€šè¿‡æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå®ç°åœ¨çœŸå®å¤šç›¸æµåœºæ™¯ä¸‹å¯¹é«˜åº¦ä¸è§„åˆ™ã€éå‡¸å½¢çŠ¶æ°”æ³¡çš„é«˜ç²¾åº¦åˆ†å‰²ã€‚å®éªŒè¡¨æ˜ï¼Œä»…ç”¨100å¼ æ ‡æ³¨å›¾åƒå³å¯è·å¾—F1å’ŒDiceåˆ†æ•°é«˜è¾¾0.95ï¼Œä¼˜äºä»¥å¾€éœ€è¦å¤§è§„æ¨¡æ•°æ®è®­ç»ƒçš„æ–¹æ³•ï¼Œç‰¹åˆ«åœ¨ä¸­ã€å¤§æ°”æ³¡åˆ†å‰²ä¸Šè¡¨ç°çªå‡ºï¼Œæ˜¾è‘—é™ä½äº†äººå·¥æ ‡æ³¨æˆæœ¬ã€‚ç»“è®ºæŒ‡å‡ºï¼Œè¯¥æ–¹æ³•æ•°æ®æ•ˆç‡é«˜ï¼Œæ³›åŒ–æ€§å¼ºï¼Œå¹¶å…¬å¼€äº†æ•°æ®é›†å’Œå¾®è°ƒæµç¨‹ï¼ŒåŠ©åŠ›å¤šç›¸æµé¢†åŸŸçš„åç»­ç ”ç©¶ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Segmenting_the_Complex_and_Irregular_in_Two-Phase_Flows_A_Real-World_Empirical_Study_with_SAM2.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-Beyond-Pixels-Medical-Image-Quality-Assessment-with-Implicit-Neural-Representations"><a href="#54-Beyond-Pixels-Medical-Image-Quality-Assessment-with-Implicit-Neural-Representations" class="headerlink" title="54. Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Beyond_Pixels__Medical_Image_Quality_Assessment_with_Implicit_Neural_Representations.pdf">Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Istanbul Technical University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºä½¿ç”¨åŸºäºSIRENçš„éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰å¯¹å¿ƒè„MRIå›¾åƒè´¨é‡è¿›è¡Œè¯„ä¼°ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆå°†2Dåˆ‡ç‰‡æ‹Ÿåˆä¸ºINRå‚æ•°ï¼Œç„¶ååˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå¦‚å¤šå±‚æ„ŸçŸ¥æœºã€å›¾ç¥ç»ç½‘ç»œã€å…³ç³»æ³¨æ„åŠ›Transformerç­‰ï¼‰ç›´æ¥åœ¨INRå‚æ•°ç©ºé—´è¿›è¡Œåˆ†ç±»ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ACDCæ•°æ®é›†ä¸Šå®ç°äº†ä¼˜äºä¼ ç»Ÿåƒç´ çº§æ–¹æ³•çš„å‡†ç¡®ç‡ï¼Œå¹¶å¤§å¹…å‡å°‘äº†å‚æ•°é‡ï¼Œå°¤å…¶Transformerï¼ˆProbe&#x3D;32ï¼‰æ¨¡å‹åœ¨Good vs. Poorä»»åŠ¡ä¸Šè¾¾åˆ°94.4%å‡†ç¡®ç‡ï¼Œå±•ç°äº†INRä½œä¸ºåŒ»å­¦å›¾åƒè´¨é‡æè¿°ç¬¦çš„æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Beyond_Pixels_Medical_Image_Quality_Assessment_with_Implicit_Neural_Representations.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-X-MoGen-Unified-Motion-Generation-across-Humans-and-Animals"><a href="#55-X-MoGen-Unified-Motion-Generation-across-Humans-and-Animals" class="headerlink" title="55. X-MoGen: Unified Motion Generation across Humans and Animals"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/X-MoGen__Unified_Motion_Generation_across_Humans_and_Animals.pdf">X-MoGen: Unified Motion Generation across Humans and Animals</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zhejiang University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºX-MoGenï¼Œé¦–æ¬¡å®ç°è·¨äººç±»ä¸åŠ¨ç‰©ç»Ÿä¸€çš„æ–‡æœ¬é©±åŠ¨åŠ¨ä½œç”Ÿæˆæ¡†æ¶ã€‚æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡æ¡ä»¶å›¾å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆCGAEï¼‰å­¦ä¹ T-poseéª¨æ¶å…ˆéªŒåŠè‡ªç¼–ç å™¨ï¼ˆAEï¼‰ç¼–ç åŠ¨ä½œè‡³å…±äº«æ½œåœ¨ç©ºé—´ï¼Œå¹¶å¼•å…¥å½¢æ€æŸå¤±æ­£åˆ™åŒ–ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨æ©ç å˜æ¢å™¨ï¼ˆMasked Transformerï¼‰ç»“åˆæ‰©æ•£æ¨¡å‹ï¼Œæ ¹æ®æ–‡æœ¬æè¿°ç”ŸæˆåŠ¨ä½œåµŒå…¥ï¼Œè®­ç»ƒæ—¶å½¢æ€ä¸€è‡´æ€§æ¨¡å—ï¼ˆMCMï¼‰ä¿è¯ç»“æ„åˆç†æ€§ã€‚ä½œè€…è¿˜æ„å»ºäº†åŒ…å«115ç§ç‰©ç§ã€119kåºåˆ—çš„UniMo4Dç»Ÿä¸€æ•°æ®é›†ï¼Œæ”¯æŒè·¨ç‰©ç§è”åˆå»ºæ¨¡ã€‚å®éªŒæ˜¾ç¤ºX-MoGenåœ¨å·²è§åŠæœªè§ç‰©ç§ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”ŸæˆåŠ¨ä½œçœŸå®ä¸”å…·è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/X-MoGen_Unified_Motion_Generation_across_Humans_and_Animals.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-FedGIN-Federated-Learning-with-Dynamic-Global-Intensity-Non-linear-Augmentation-for-Organ-Segmentation-using-Multi-modal-Images"><a href="#56-FedGIN-Federated-Learning-with-Dynamic-Global-Intensity-Non-linear-Augmentation-for-Organ-Segmentation-using-Multi-modal-Images" class="headerlink" title="56. FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FedGIN__Federated_Learning_with_Dynamic_Global_Intensity_Non-linear_Augmentation_for_Organ_Segmentat.pdf">FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Norwegian University of Science and Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºFedGINï¼Œä¸€ç§ç»“åˆäº†å…¨å±€å¼ºåº¦éçº¿æ€§ï¼ˆGINï¼‰å¢å¼ºçš„è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€åŒ»å­¦å›¾åƒï¼ˆCTå’ŒMRIï¼‰åœ¨éšç§ä¿æŠ¤ä¸‹çš„å™¨å®˜åˆ†å‰²ã€‚FedGINåœ¨æœ¬åœ°è®­ç»ƒé˜¶æ®µé€šè¿‡GINæ¨¡å—åŠ¨æ€å¯¹CTå’ŒMRIå›¾åƒè¿›è¡Œå¼ºåº¦åŸŸå¢å¼ºï¼Œä¿ƒä½¿æ¨¡å‹å­¦ä¹ æ¨¡æ€æ— å…³çš„ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ¨¡å‹å‚æ•°èšåˆå®ç°è·¨æœºæ„çš„ååŒå»ºæ¨¡ï¼Œæ˜¾è‘—æå‡äº†åœ¨å¤æ‚å’Œä½å¯¹æ¯”åº¦å™¨å®˜ä¸Šçš„åˆ†å‰²è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedGINåœ¨å¤šæ¨¡æ€æ•°æ®ä¸‹æ¥è¿‘ä¸­å¿ƒåŒ–è®­ç»ƒæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨èƒ°è…ºå’Œèƒ†å›Šç­‰éš¾åˆ†å‰²å™¨å®˜ä¸Šä¼˜äºå•æ¨¡æ€å’Œæ— GINå¢å¼ºçš„è”é‚¦å­¦ä¹ åŸºçº¿ï¼ŒéªŒè¯äº†å…¶åœ¨éšç§ä¿æŠ¤ã€å¤šæœºæ„ã€å¤šæ¨¡æ€å®é™…åº”ç”¨åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FedGIN_Federated_Learning_with_Dynamic_Global_Intensity_Non-linear_Augmentation_for_Organ_Segmentation_using_Multi-modal_Images.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-PoseGen-In-Context-LoRA-Finetuning-for-Pose-Controllable-Long-Human-Video-Generation"><a href="#57-PoseGen-In-Context-LoRA-Finetuning-for-Pose-Controllable-Long-Human-Video-Generation" class="headerlink" title="57. PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/PoseGen__In-Context_LoRA_Finetuning_for_Pose-Controllable_Long_Human_Video_Generation.pdf">PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Xiaoice</span></p>
<p>è¯¥è®ºæ–‡æå‡ºPoseGenï¼Œä¸€ç§é«˜æ•ˆä¸”åˆ›æ–°çš„äººä½“è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œå®ç°äº†ä»å•å¼ å‚è€ƒå›¾åƒå’ŒåŠ¨ä½œå§¿æ€åºåˆ—ç”Ÿæˆä»»æ„é•¿åº¦ã€é«˜ä¿çœŸã€å§¿æ€å¯æ§çš„äººä½“è§†é¢‘ã€‚æ–¹æ³•æ ¸å¿ƒåŒ…æ‹¬åŒé‡æ¡ä»¶æœºåˆ¶ï¼šç”¨LoRAå¾®è°ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå‚è€ƒä¿¡æ¯åœ¨tokençº§æ³¨å…¥ã€å§¿æ€ä¿¡æ¯åœ¨channelçº§æ³¨å…¥ï¼Œå¹¶é‡‡ç”¨åˆ†æ®µç”Ÿæˆä¸KVç¼“å­˜å…±äº«æŠ€æœ¯ï¼Œå®ç°è§†é¢‘ç‰‡æ®µæ— ç¼æ‹¼æ¥ï¼Œä¿è¯èº«ä»½ä¸€è‡´æ€§å’Œæ—¶åºè¿è´¯æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPoseGenåœ¨èº«ä»½ä¿æŒã€åŠ¨ä½œæ§åˆ¶å’Œé•¿è§†é¢‘ä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”ä»…éœ€æå°‘è®­ç»ƒæ•°æ®ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/PoseGen_In-Context_LoRA_Finetuning_for_Pose-Controllable_Long_Human_Video_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-AdaFusion-Prompt-Guided-Inference-with-Adaptive-Fusion-of-Pathology-Foundation-Models"><a href="#58-AdaFusion-Prompt-Guided-Inference-with-Adaptive-Fusion-of-Pathology-Foundation-Models" class="headerlink" title="58. AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/AdaFusion__Prompt-Guided_Inference_with_Adaptive_Fusion_of_Pathology_Foundation_Models.pdf">AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">South China University of Technology</span></p>
<p>æœ¬æ–‡æå‡ºAdaFusionï¼Œä¸€ç§é¢å‘ç—…ç†å›¾åƒåˆ†æçš„å¤šæ¨¡å‹ç‰¹å¾è‡ªé€‚åº”èåˆæ¨ç†æ¡†æ¶ã€‚å…¶æ–¹æ³•æ˜¯åœ¨æ— éœ€å¾®è°ƒåŸºç¡€æ¨¡å‹çš„å‰æä¸‹ï¼Œé‡‡ç”¨è½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶å¯¹å¤šç§é¢„è®­ç»ƒç—…ç†åŸºç¡€æ¨¡å‹ï¼ˆPFMsï¼‰çš„ä½ç»´ç‰¹å¾è¿›è¡Œå‹ç¼©ã€å¯¹é½å’ŒåŸºäºç»„ç»‡è¡¨å‹ä¸Šä¸‹æ–‡çš„åŠ¨æ€åŠ æƒèåˆã€‚å¯¹æ²»ç–—å“åº”é¢„æµ‹ã€è‚¿ç˜¤åˆ†çº§å’Œç©ºé—´åŸºå› è¡¨è¾¾æ¨æ–­ç­‰å¤šé¡¹çœŸå®ä¸–ç•Œä»»åŠ¡çš„å®éªŒè¡¨æ˜ï¼ŒAdaFusionåœ¨åˆ†ç±»å’Œå›å½’ä»»åŠ¡ä¸Šå‡ä¼˜äºå•ä¸€PFMå’Œä¸»æµèåˆåŸºçº¿ï¼Œå¹¶èƒ½å¯è§£é‡Šæ€§åœ°é‡åŒ–å„æ¨¡å‹å¯¹ä¸åŒç»„ç»‡è¡¨å‹çš„è´¡çŒ®ï¼Œæå‡é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/AdaFusion_Prompt-Guided_Inference_with_Adaptive_Fusion_of_Pathology_Foundation_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-AU-IQA-A-Benchmark-Dataset-for-Perceptual-Quality-Assessment-of-AI-Enhanced-User-Generated-Content"><a href="#59-AU-IQA-A-Benchmark-Dataset-for-Perceptual-Quality-Assessment-of-AI-Enhanced-User-Generated-Content" class="headerlink" title="59. AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/AU-IQA__A_Benchmark_Dataset_for_Perceptual_Quality_Assessment_of_AI-Enhanced_User-Generated_Content.pdf">AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Jiao Tong University</span></p>
<p>æœ¬æ–‡æå‡ºäº†AU-IQAï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘AIå¢å¼ºç”¨æˆ·ç”Ÿæˆå†…å®¹ï¼ˆAI-UGCï¼‰æ„ŸçŸ¥è´¨é‡è¯„ä¼°çš„åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–è¶…åˆ†è¾¨ç‡ã€ä½å…‰å¢å¼ºå’Œå»å™ªä¸‰ç±»AIå¢å¼ºæ–¹å¼ï¼Œå…±4800å¼ å›¾åƒï¼Œå¹¶é‡‡ç”¨ä¸»è§‚å‡å€¼è¯„åˆ†ï¼ˆMOSï¼‰è¿›è¡Œæ ‡æ³¨ã€‚ä½œè€…ç³»ç»Ÿè¯„æµ‹äº†ä¼ ç»ŸIQAæ¨¡å‹å’Œå¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå‘ç°ä»¥UGCä¸ºç›®æ ‡çš„æ¨¡å‹æ•ˆæœæœ€ä½³ï¼Œç°æœ‰æ¨¡å‹åœ¨ä¸åŒAI-UGCç±»å‹é—´è¡¨ç°å·®å¼‚å¤§ä¸”ä¸ç¨³å®šï¼Œå¼ºè°ƒäº†å¼€å‘ä¸“ç”¨AI-UGCè´¨é‡è¯„ä¼°æ¨¡å‹çš„å¿…è¦æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/AU-IQA_A_Benchmark_Dataset_for_Perceptual_Quality_Assessment_of_AI-Enhanced_User-Generated_Content.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-Multimodal-Causal-Driven-Representation-Learning-for-Generalizable-Medical-Image-Segmentation"><a href="#60-Multimodal-Causal-Driven-Representation-Learning-for-Generalizable-Medical-Image-Segmentation" class="headerlink" title="60. Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Multimodal_Causal-Driven_Representation_Learning_for_Generalizable_Medical_Image_Segmentation.pdf">Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Hong Kong Institute of Science &amp; Innovation</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å› æœé©±åŠ¨è¡¨å¾å­¦ä¹ ï¼ˆMCDRLï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå› æœæ¨æ–­å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ï¼Œåˆ†ä¸¤æ­¥å®ç°åŒ»å­¦å›¾åƒåˆ†å‰²çš„é¢†åŸŸæ³›åŒ–ï¼šé¦–å…ˆåˆ©ç”¨CLIPè·¨æ¨¡æ€èƒ½åŠ›è¯†åˆ«ç—…ç¶åŒºåŸŸå¹¶é€šè¿‡æ–‡æœ¬æç¤ºæ„å»ºæ··æ‚å› ç´ å­—å…¸ï¼Œç„¶åè®­ç»ƒå› æœå¹²é¢„ç½‘ç»œï¼Œæ¶ˆé™¤é¢†åŸŸç‰¹å¼‚å˜å¼‚å½±å“ï¼Œä¿ç•™å…³é”®è§£å‰–ç»“æ„ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMCDRLåœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·å¤‡æ›´å¼ºé¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Multimodal_Causal-Driven_Representation_Learning_for_Generalizable_Medical_Image_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-Propagating-Sparse-Depth-via-Depth-Foundation-Model-for-Out-of-Distribution-Depth-Completion"><a href="#61-Propagating-Sparse-Depth-via-Depth-Foundation-Model-for-Out-of-Distribution-Depth-Completion" class="headerlink" title="61. Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Propagating_Sparse_Depth_via_Depth_Foundation_Model_for_Out-of-Distribution_Depth_Completion.pdf">Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Dalian University of Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†PSDæ·±åº¦è¡¥å…¨æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ·±åº¦åŸºç¡€æ¨¡å‹ä»RGBå›¾åƒä¸­æå–ç»“æ„å’Œè¯­ä¹‰çº¿ç´¢ï¼Œå¼•å¯¼ç¨€ç–æ·±åº¦å‘ç¼ºå¤±åŒºåŸŸä¼ æ’­ï¼ŒåŒ…å«æ— å‚æ•°çš„3D&#x2F;2DåŒç©ºé—´ä¼ æ’­æ¨¡å—åŠå¯å­¦ä¹ çš„æ®‹å·®æ ¡æ­£æ¨¡å—ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨16ä¸ªOODåœºæ™¯ä¸‹è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå…·å¤‡æå¼ºæ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Propagating_Sparse_Depth_via_Depth_Foundation_Model_for_Out-of-Distribution_Depth_Completion.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-Rotation-Equivariant-Arbitrary-scale-Image-Super-Resolution"><a href="#62-Rotation-Equivariant-Arbitrary-scale-Image-Super-Resolution" class="headerlink" title="62. Rotation Equivariant Arbitrary-scale Image Super-Resolution"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Rotation_Equivariant_Arbitrary-scale_Image_Super-Resolution.pdf">Rotation Equivariant Arbitrary-scale Image Super-Resolution</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Xiâ€™an Jiaotong University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯æ—‹è½¬ç­‰å˜çš„ä»»æ„å°ºåº¦å›¾åƒè¶…åˆ†è¾¨ç‡(ASISR)æ–¹æ³•ï¼Œé€šè¿‡é‡æ–°è®¾è®¡éšå¼ç¥ç»è¡¨ç¤º(INR)å’Œç¼–ç å™¨æ¨¡å—ï¼ŒåµŒå…¥æ—‹è½¬ç­‰å˜æ€§ï¼Œç†è®ºåˆ†æäº†ç­‰å˜è¯¯å·®å¹¶å®ç°äº†å¯¹ç°æœ‰ASISRæ–¹æ³•çš„å¯æ’æ‹”å¢å¼ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ›´å¥½åœ°ä¿æŒå›¾åƒå‡ ä½•ç»“æ„åŠæ—‹è½¬å¯¹ç§°æ€§ï¼Œæœ‰æ•ˆæå‡è¶…åˆ†è¾¨ç‡æ¢å¤è´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Rotation_Equivariant_Arbitrary-scale_Image_Super-Resolution.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-Decoupling-Continual-Semantic-Segmentation"><a href="#63-Decoupling-Continual-Semantic-Segmentation" class="headerlink" title="63. Decoupling Continual Semantic Segmentation"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Decoupling_Continual_Semantic_Segmentation.pdf">Decoupling Continual Semantic Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Sun Yat-sen University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†DecoupleCSSï¼Œä¸€ä¸ªç”¨äºæŒç»­è¯­ä¹‰åˆ†å‰²ï¼ˆCSSï¼‰çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå°†ç±»åˆ«æ„ŸçŸ¥æ£€æµ‹ä¸ç±»åˆ«æ— å…³åˆ†å‰²è§£è€¦ã€‚æ–¹æ³•ä¸­ï¼Œç¬¬ä¸€é˜¶æ®µåˆ©ç”¨é¢„è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹å¹¶ç»“åˆLoRAé€‚é…å™¨è¿›è¡Œç±»åˆ«æ£€æµ‹å’Œå®šä½æç¤ºç”Ÿæˆï¼Œç¬¬äºŒé˜¶æ®µç”¨Segment Anything Modelï¼ˆSAMï¼‰å®ç°ç²¾ç»†åˆ†å‰²ï¼Œæ˜¾è‘—ç¼“è§£é—å¿˜é—®é¢˜å¹¶æå‡æ–°æ—§çŸ¥è¯†å¹³è¡¡ã€‚å®éªŒè¡¨æ˜ï¼ŒDecoupleCSSåœ¨å¤šä¸ªå¸¸ç”¨CSSä»»åŠ¡ä¸Šå–å¾—äº†å½“å‰æœ€ä¼˜è¡¨ç°ï¼Œå…·å¤‡è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†æ¨ç†æ—¶å¤šä»»åŠ¡åˆ‡æ¢å¸¦æ¥ä¸€å®šå»¶è¿Ÿã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Decoupling_Continual_Semantic_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-Modeling-Rapid-Contextual-Learning-in-the-Visual-Cortex-with-Fast-Weight-Deep-Autoencoder-Networks"><a href="#64-Modeling-Rapid-Contextual-Learning-in-the-Visual-Cortex-with-Fast-Weight-Deep-Autoencoder-Networks" class="headerlink" title="64. Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Modeling_Rapid_Contextual_Learning_in_the_Visual_Cortex_with_Fast-Weight_Deep_Autoencoder_Networks.pdf">Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Carnegie Mellon University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºVision Transformerï¼ˆViTï¼‰è‡ªç¼–ç å™¨çš„æ¶æ„ï¼Œåˆ©ç”¨ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æ¨¡å—æ¨¡æ‹Ÿå¤§è„‘è§†è§‰çš®å±‚ä¸­çš„å¿«é€Ÿæƒé‡æœºåˆ¶ï¼Œå®ç°ç†Ÿæ‚‰ä¸Šä¸‹æ–‡çš„å¿«é€Ÿå­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼Œç†Ÿæ‚‰è®­ç»ƒä¿ƒä½¿æ—©æœŸå±‚å¯¹å…¨å±€ä¸Šä¸‹æ–‡æ•æ„Ÿï¼Œå‹ç¼©ä»»åŠ¡æ— å…³çš„å˜å¼‚ï¼ŒåŒæ—¶LoRAè¿›ä¸€æ­¥å¢å¼ºäº†è¿™ä¸€æ•ˆæœï¼Œæœ‰åŠ©äºå®ç°æ›´é²æ£’çš„è§†è§‰è¡¨ç¤ºå’Œæ›´æ¸…æ™°çš„å›¾åƒåˆ†å‰²ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Modeling_Rapid_Contextual_Learning_in_the_Visual_Cortex_with_Fast-Weight_Deep_Autoencoder_Networks.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-UGOD-Uncertainty-Guided-Differentiable-Opacity-and-Soft-Dropout-for-Enhanced-Sparse-View-3DGS"><a href="#65-UGOD-Uncertainty-Guided-Differentiable-Opacity-and-Soft-Dropout-for-Enhanced-Sparse-View-3DGS" class="headerlink" title="65. UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/UGOD__Uncertainty-Guided_Differentiable_Opacity_and_Soft_Dropout_for_Enhanced_Sparse-View_3DGS.pdf">UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Manchester Metropolitan University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é›†æˆç¥ç»ç½‘ç»œçš„ä¸ç¡®å®šæ€§å­¦ä¹ æ¨¡å—ï¼Œé€šè¿‡ç»“åˆå¤šå±‚HashGridç¼–ç ï¼Œå¯¹3D Gaussian Splattingï¼ˆ3DGSï¼‰ä¸­çš„æ¯ä¸ªé«˜æ–¯è¿›è¡Œè§†ç‚¹ç›¸å…³çš„ä¸ç¡®å®šæ€§å»ºæ¨¡ã€‚æ–¹æ³•åˆ©ç”¨å­¦ä¹ åˆ°çš„ä¸ç¡®å®šæ€§åˆ†åˆ«æŒ‡å¯¼é«˜æ–¯çš„ä¸é€æ˜åº¦è°ƒèŠ‚åŠå¯å¾®åˆ†è½¯ä¸¢å¼ƒæœºåˆ¶ï¼Œæœ‰æ•ˆå‡å°‘ç¨€ç–è§†è§’åœºæ™¯ä¸‹çš„è¿‡æ‹Ÿåˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ª3Dé‡å»ºæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½ä»¥æ›´å°‘çš„é«˜æ–¯å®ç°æ›´é«˜çš„æ¸²æŸ“è´¨é‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/UGOD_Uncertainty-Guided_Differentiable_Opacity_and_Soft_Dropout_for_Enhanced_Sparse-View_3DGS.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-Open-world-Point-Cloud-Semantic-Segmentation-A-Human-in-the-loop-Framework"><a href="#66-Open-world-Point-Cloud-Semantic-Segmentation-A-Human-in-the-loop-Framework" class="headerlink" title="66. Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Open-world_Point_Cloud_Semantic_Segmentation__A_Human-in-the-loop_Framework.pdf">Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Nanjing University of Science and Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†HOW-Segæ¡†æ¶ï¼Œå®ç°äº†ç‚¹äº‘çš„å¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ï¼Œé€šè¿‡åœ¨æŸ¥è¯¢æ ·æœ¬ä¸Šä»¥ç¨€ç–äººå·¥æ ‡æ³¨ä¸ºæŒ‡å¯¼ï¼Œç›´æ¥æ„å»ºç±»åˆ«åŸå‹å¹¶ç»“åˆäº¤äº’å¼åŸå‹æ¶ˆæ­§å’ŒåŸºäºCRFçš„åŸå‹æ ‡ç­¾åˆ†é…æœºåˆ¶ï¼Œé¿å…äº†è·¨æ ·æœ¬åˆ†å¸ƒåç§»å’ŒåŸå‹åå·®ã€‚å®éªŒè¡¨æ˜ï¼ŒHOW-Segæ— éœ€é¢å¤–æ”¯æŒæ ·æœ¬æˆ–ç¦»çº¿å¢é‡å­¦ä¹ ï¼Œä»…é æå°‘é‡äººå·¥å¹²é¢„å³å¯åœ¨S3DISå’ŒScanNetv2ç­‰æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°é«˜è´¨é‡çš„åŸºç±»ä¸æ–°ç±»ç‚¹äº‘åˆ†å‰²ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Open-world_Point_Cloud_Semantic_Segmentation_A_Human-in-the-loop_Framework.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-Laplacian-Analysis-Meets-Dynamics-Modelling-Gaussian-Splatting-for-4D-Reconstruction"><a href="#67-Laplacian-Analysis-Meets-Dynamics-Modelling-Gaussian-Splatting-for-4D-Reconstruction" class="headerlink" title="67. Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Laplacian_Analysis_Meets_Dynamics_Modelling__Gaussian_Splatting_for_4D_Reconstruction.pdf">Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>æœ¬è®ºæ–‡æå‡ºä¸€ç§åŠ¨æ€3Dé«˜æ–¯æŠ•å½±(3DGS)çš„æ··åˆæ˜¾å¼-éšå¼ç¼–ç æ¡†æ¶ï¼Œèåˆå¤šå°ºåº¦å“ˆå¸Œç¼–ç ä¸æ‹‰æ™®æ‹‰æ–¯é¢‘åŸŸåˆ†æï¼Œæœ‰æ•ˆè§£è€¦å¹¶æ§åˆ¶å¤æ‚åŠ¨æ€åœºæ™¯ä¸­ä¸åŒé¢‘ç‡çš„è¿åŠ¨åˆ†é‡ã€‚æ–¹æ³•è¿˜åŒ…æ‹¬å¢å¼ºçš„é«˜æ–¯åŠ¨æ€å±æ€§å’ŒåŸºäºKDTreeçš„è‡ªé€‚åº”é«˜æ–¯åˆ’åˆ†ç­–ç•¥ï¼Œå®ç°å¯¹é«˜åŠ¨æ€åŒºåŸŸçš„ç²¾ç»†å»ºæ¨¡å’Œé«˜æ•ˆä¼˜åŒ–ã€‚å®éªŒç»“æœåœ¨ä¸‰ä¸ªä¸»æµåŠ¨æ€åœºæ™¯æ•°æ®é›†ä¸ŠéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨é‡å»ºç²¾åº¦ä¸è¿åŠ¨ä¸€è‡´æ€§ä¸Šçš„é¢†å…ˆæ€§èƒ½ï¼Œä¼˜äºç°æœ‰åŠ¨æ€é‡å»ºæ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Laplacian_Analysis_Meets_Dynamics_Modelling_Gaussian_Splatting_for_4D_Reconstruction.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="68-H-NET-Hierarchical-Dynamic-Chunking-for-Tokenizer-Free-Language-Modelling-in-Morphologically-Rich-Languages"><a href="#68-H-NET-Hierarchical-Dynamic-Chunking-for-Tokenizer-Free-Language-Modelling-in-Morphologically-Rich-Languages" class="headerlink" title="68. H-NET++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/H-Net++__Hierarchical_Dynamic_Chunking_for_Tokenizer-Free_Language_Modelling_in_Morphologically-Rich.pdf">H-NET++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Independent (no institution listed)</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†H-NET++ï¼Œä¸€ç§é’ˆå¯¹å½¢æ€å¤æ‚è¯­è¨€ï¼ˆå¦‚æ³¢æ–¯è¯­ï¼‰çš„åˆ†å±‚åŠ¨æ€åˆ†å—è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œæ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬å±‚æ¬¡åŒ–GRUè·¯ç”±å™¨ã€è½»é‡Transformerä¸Šä¸‹æ–‡æ··åˆå™¨ã€æ–‡æ¡£çº§å˜åˆ†è¶…å…ˆéªŒã€ç‰¹æ®Šå¤„ç†é›¶å®½éè¿æ¥å­—ç¬¦ï¼ˆZWNJï¼‰ä»¥åŠé˜¶æ®µå¼è¯¾ç¨‹è®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºH-NET++åœ¨è¯­æ–™å‹ç¼©ã€ä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®ç‡ã€å¯¹æ­£å­—æ³•å™ªå£°çš„é²æ£’æ€§å’Œå½¢æ€è¾¹ç•Œè¯†åˆ«æ–¹é¢å‡è¶…è¶Šç°æœ‰æ¨¡å‹ï¼Œå¹¶èƒ½è‡ªåŠ¨å­¦ä¹ ç¬¦åˆè¯­è¨€å­¦è§„å¾‹çš„åˆ†å—ï¼Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/H-NET%2B%2B_Hierarchical_Dynamic_Chunking_for_Tokenizer-Free_Language_Modelling_in_Morphologically-Rich_Languages.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-Simulating-Human-Like-Learning-Dynamics-with-LLM-Empowered-Agents"><a href="#69-Simulating-Human-Like-Learning-Dynamics-with-LLM-Empowered-Agents" class="headerlink" title="69. Simulating Human-Like Learning Dynamics with LLM-Empowered Agents"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Simulating_Human-Like_Learning_Dynamics_with_LLM-Empowered_Agents.pdf">Simulating Human-Like Learning Dynamics with LLM-Empowered Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Science and Technology of China</span></p>
<p>è¯¥è®ºæ–‡æå‡ºLearnerAgentæ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨¡æ‹Ÿå…·æœ‰ä¸åŒå¿ƒç†ç”»åƒï¼ˆæ·±åº¦ã€è¡¨å±‚ã€æ‡’æƒ°ã€æ— ç”»åƒï¼‰çš„å­¦ç”Ÿåœ¨ä¸ºæœŸä¸€å¹´çš„çœŸå®æ•™å­¦ç¯å¢ƒä¸­çš„å­¦ä¹ è¡Œä¸ºã€‚é€šè¿‡å‘¨æœŸæ€§çŸ¥è¯†è·å–ã€æµ‹è¯•ä¸äº’åŠ¨ï¼Œæ­ç¤ºåªæœ‰æ·±åº¦å‹å­¦ä¹ è€…å…·å¤‡é•¿æœŸè®¤çŸ¥æˆé•¿ï¼ŒLLMé»˜è®¤è¡Œä¸ºä¸ºâ€œå‹¤å¥‹ä½†è„†å¼±çš„è¡¨å±‚å­¦ä¹ è€…â€ï¼Œå­˜åœ¨å¯¹è¡¨å±‚æ¨¡å¼çš„ä¾èµ–ï¼Œç¼ºä¹æ·±å±‚æ³›åŒ–èƒ½åŠ›ã€‚ç»“è®ºï¼šLearnerAgentèƒ½é«˜ä¿çœŸå¤ç°äººç±»å­¦ä¹ è¡Œä¸ºï¼Œæ­ç¤ºLLMçŸ­æ¿å¹¶ä¸ºæ•™è‚²ä¸AIè®¤çŸ¥æä¾›æ–°æ´å¯Ÿã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Simulating_Human-Like_Learning_Dynamics_with_LLM-Empowered_Agents.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle"><a href="#70-Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle" class="headerlink" title="70. Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Shuffle-R1__Efficient_RL_framework_for_Multimodal_Large_Language_Models_via_Data-centric_Dynamic_Shu.pdf">Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Huazhong University of Science and Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†Shuffle-R1æ¡†æ¶ï¼Œé€šè¿‡Pairwise Trajectory Samplingï¼ˆç»“æ„åŒ–é«˜å¯¹æ¯”è½¨è¿¹é‡‡æ ·ï¼‰å’ŒAdvantage-based Batch Shuffleï¼ˆä¼˜åŠ¿æ„ŸçŸ¥æ‰¹æ¬¡é‡æ„ï¼‰ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒæµç¨‹ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼˜åŠ¿å¡Œç¼©å’Œå›åˆé™é»˜é—®é¢˜ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒShuffle-R1åœ¨å¤šé¡¹ä¸»æµæ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰RLæ–¹æ³•ï¼Œä¸”å…·å¤‡è¾ƒä½è®¡ç®—å¼€é”€ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Shuffle-R1_Efficient_RL_framework_for_Multimodal_Large_Language_Models_via_Data-centric_Dynamic_Shuffle.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-LLaVA-RE-Binary-Image-Text-Relevancy-Evaluation-with-Multimodal-Large-Language-Model"><a href="#71-LLaVA-RE-Binary-Image-Text-Relevancy-Evaluation-with-Multimodal-Large-Language-Model" class="headerlink" title="71. LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/LLaVA-RE__Binary_Image-Text_Relevancy_Evaluation_with_Multimodal_Large_Language_Model.pdf">LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Stony Brook University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºLLaVA-REæ¡†æ¶ï¼ŒåŸºäºLLaVA 1.5å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥è¯¦ç»†ä»»åŠ¡æŒ‡ä»¤å’Œå¤šæ¨¡æ€in-context learningï¼Œå®ç°å¤æ‚å›¾æ–‡å¯¹äºŒå…ƒç›¸å…³æ€§è¯„ä¼°ï¼Œå¹¶æ„å»ºäº†å¤šä»»åŠ¡çš„äºŒå…ƒç›¸å…³æ€§æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨è®­ç»ƒåŠæœªè§ä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºä¸»æµæ¨¡å‹ï¼Œå…·å¤‡è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/LLaVA-RE_Binary_Image-Text_Relevancy_Evaluation_with_Multimodal_Large_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-Iterative-Learning-of-Computable-Phenotypes-for-Treatment-Resistant-Hypertension-using-Large-Language-Models"><a href="#72-Iterative-Learning-of-Computable-Phenotypes-for-Treatment-Resistant-Hypertension-using-Large-Language-Models" class="headerlink" title="72. Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Iterative_Learning_of_Computable_Phenotypes_for_Treatment_Resistant_Hypertension_using_Large_Languag.pdf">Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Federal University of ABC</span></p>
<p>è¯¥è®ºæ–‡æå‡ºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆé«˜è¡€å‹åŠå…¶å¤æ‚å˜å‹çš„å¯è®¡ç®—è¡¨å‹ï¼Œé€šè¿‡â€œsynthesize, execute, debug, instructï¼ˆSEDIï¼‰â€è¿­ä»£ç­–ç•¥å¯¹æ¨¡å‹è¿›è¡Œæ•°æ®é©±åŠ¨åé¦ˆä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEDIç­–ç•¥ä¸‹çš„LLMç”Ÿæˆæ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢æ¥è¿‘ç”šè‡³ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œä½†æ‰€éœ€ä¸“å®¶æ ‡æ³¨æ ·æœ¬æ›´å°‘ï¼Œä¸”æ¨¡å‹æ›´ç®€æ´æ˜“äºä¸´åºŠåº”ç”¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Iterative_Learning_of_Computable_Phenotypes_for_Treatment_Resistant_Hypertension_using_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-MV-Debate-Multi-view-Agent-Debate-with-Dynamic-Reflection-Gating-for-Multimodal-Harmful-Content-Detection-in-Social-Media"><a href="#73-MV-Debate-Multi-view-Agent-Debate-with-Dynamic-Reflection-Gating-for-Multimodal-Harmful-Content-Detection-in-Social-Media" class="headerlink" title="73. MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MV-Debate__Multi-view_Agent_Debate_with_Dynamic_Reflection_Gating_for_Multimodal_Harmful_Content_Det.pdf">MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shandong University</span></p>
<p>æœ¬æ–‡æå‡ºMV-Debateå¤šè§†è§’å¤šæ™ºèƒ½ä½“è¾©è®ºæ¡†æ¶ï¼Œé€šè¿‡ç»„è£…è¡¨å±‚åˆ†æã€æ·±åº¦æ¨ç†ã€æ¨¡æ€å¯¹æ¯”å’Œç¤¾ä¼šè¯­å¢ƒå››ç±»å¼‚æ„æ™ºèƒ½ä½“ï¼Œç»“åˆåŠ¨æ€åæ€é—¨æ§æœºåˆ¶ï¼Œå®ç°å¤šæ¨¡æ€æœ‰å®³å†…å®¹ï¼ˆå¦‚è®½åˆºã€ä»‡æ¨è¨€è®ºã€è™šå‡ä¿¡æ¯ï¼‰çš„ç»Ÿä¸€æ£€æµ‹ã€‚å®éªŒæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¤šé¡¹åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå•æ¨¡å‹å’Œç°æœ‰å¤šæ™ºèƒ½ä½“æ–¹æ³•ï¼Œæå‡äº†å‡†ç¡®æ€§ã€æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MV-Debate_Multi-view_Agent_Debate_with_Dynamic_Reflection_Gating_for_Multimodal_Harmful_Content_Detection_in_Social_Media.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification"><a href="#74-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification" class="headerlink" title="74. On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/On_the_Generalization_of_SFT__A_Reinforcement_Learning_Perspective_with_Reward_Rectification.pdf">On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Southeast University</span></p>
<p>æœ¬æ–‡ä»ç†è®ºä¸Šæ­ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¸¸ç”¨çš„Supervised Fine-Tuningï¼ˆSFTï¼‰æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›å—é™ï¼ŒåŸå› åœ¨äºå…¶æ¢¯åº¦æœ¬è´¨ä¸Šå…·æœ‰éšå¼çš„å¥–åŠ±ç»“æ„ï¼Œå¯¼è‡´ä¼˜åŒ–ä¸ç¨³å®šå’Œè¿‡æ‹Ÿåˆã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§ç®€å•çš„åŠ¨æ€é‡åŠ æƒæ–¹æ³•Dynamic Fine-Tuningï¼ˆDFTï¼‰ï¼Œé€šè¿‡å¯¹æ¯ä¸ªtokençš„ç›®æ ‡å‡½æ•°åŠ¨æ€ç¼©æ”¾ï¼Œå¤§å¹…æå‡äº†SFTçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†å’Œç¦»çº¿RLåœºæ™¯ä¸‹è¶…è¶Šäº†æ ‡å‡†SFTåŠå¤šç§RLæ–¹æ³•ã€‚ç»“è®ºï¼šDFTæ˜¾è‘—æå‡äº†SFTçš„æ³›åŒ–å’Œè®­ç»ƒç¨³å®šæ€§ï¼Œä¸ºLLMå¾®è°ƒæä¾›äº†é«˜æ•ˆå®ç”¨çš„æ–°èŒƒå¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/On_the_Generalization_of_SFT_A_Reinforcement_Learning_Perspective_with_Reward_Rectification.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-How-Do-LLMs-Persuade-Linear-Probes-Can-Uncover-Persuasion-Dynamics-in-Multi-Turn-Conversations"><a href="#75-How-Do-LLMs-Persuade-Linear-Probes-Can-Uncover-Persuasion-Dynamics-in-Multi-Turn-Conversations" class="headerlink" title="75. How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/How_Do_LLMs_Persuade__Linear_Probes_Can_Uncover_Persuasion_Dynamics_in_Multi-Turn_Conversations.pdf">How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Mila</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨çº¿æ€§æ¢é’ˆï¼ˆlinear probesï¼‰åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè½®å¯¹è¯ä¸­è¯´æœèƒ½åŠ›çš„æ–¹æ³•ã€‚ä½œè€…è®¾è®¡äº†å¯é«˜æ•ˆæ£€æµ‹è¯´æœç»“æœã€è¢«è¯´æœè€…ä¸ªæ€§å’Œè¯´æœç­–ç•¥çš„æ¢é’ˆï¼Œå¹¶åœ¨çœŸå®åŠåˆæˆæ•°æ®é›†ä¸ŠéªŒè¯å…¶èƒ½æ•æ‰å¯¹è¯ä¸­çš„å…³é”®è¯´æœåŠ¨æ€ï¼Œå‘ç°æ¢é’ˆä¸ä»…æ•ˆç‡è¿œè¶…æç¤ºï¼ˆpromptingï¼‰ï¼Œå¹¶åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸Šæ•ˆæœæ›´ä¼˜ã€‚ç»“è®ºæ˜¾ç¤ºï¼Œçº¿æ€§æ¢é’ˆèƒ½å¤Ÿé«˜æ•ˆæ­ç¤ºLLMè¯´æœè¡Œä¸ºç»†èŠ‚ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡å¯¹è¯åˆ†æä¸å¤æ‚è¡Œä¸ºç ”ç©¶ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/How_Do_LLMs_Persuade_Linear_Probes_Can_Uncover_Persuasion_Dynamics_in_Multi-Turn_Conversations.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-The-Missing-Reward-Active-Inference-in-the-Era-of-Experience"><a href="#76-The-Missing-Reward-Active-Inference-in-the-Era-of-Experience" class="headerlink" title="76. The Missing Reward: Active Inference in the Era of Experience"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/The_Missing_Reward__Active_Inference_in_the_Era_of_Experience.pdf">The Missing Reward: Active Inference in the Era of Experience</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">IBM T.J. Watson Research Center</span></p>
<p>æœ¬æ–‡æå‡ºå°†Active Inferenceï¼ˆAIFï¼‰ä¸å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆï¼Œä½œä¸ºå®ç°AIè‡ªä¸»å­¦ä¹ ä¸ç»éªŒé©±åŠ¨æ™ºèƒ½çš„ç†è®ºåŸºç¡€ã€‚æ–¹æ³•ä¸Šï¼Œä»¥AIFçš„è‡ªç”±èƒ½æœ€å°åŒ–ä¸ºå†…åœ¨ç›®æ ‡ï¼Œåˆ©ç”¨LLMä½œä¸ºç”Ÿæˆå¼ä¸–ç•Œæ¨¡å‹ï¼Œç»Ÿä¸€æ¢ç´¢ä¸åˆ©ç”¨ï¼Œå¹¶æ¶ˆé™¤å¯¹äººå·¥å¥–åŠ±å·¥ç¨‹çš„ä¾èµ–ã€‚ç»“è®ºè®¤ä¸ºè¯¥æ¶æ„èƒ½æå‡AIè‡ªä¸»æ€§ä¸èƒ½æºæ•ˆç‡ï¼Œæ¨åŠ¨ç»éªŒé©±åŠ¨çš„å¯æŒç»­AIå‘å±•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/The_Missing_Reward_Active_Inference_in_the_Era_of_Experience.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-COOPER-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models"><a href="#77-COOPER-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models" class="headerlink" title="77. COOPER: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Cooper__Co-Optimizing_Policy_and_Reward_Models_in_Reinforcement_Learning_for_Large_Language_Models.pdf">COOPER: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zhejiang University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºCooperæ¡†æ¶ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­åŒæ­¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„ç­–ç•¥æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ã€‚æ–¹æ³•ä¸Šï¼Œé€šè¿‡å‚è€ƒå¼å¥–åŠ±æ¨¡å‹ï¼ˆVerifyRMï¼‰ã€æ··åˆæ³¨é‡Šç­–ç•¥å’Œæ­£è´Ÿå¯¹æ¯”å­¦ä¹ åŠ¨æ€æ›´æ–°å¥–åŠ±æ¨¡å‹ï¼Œç»“åˆè§„åˆ™å‹é«˜ç²¾åº¦å¥–åŠ±ç­›é€‰æ­£æ ·æœ¬å’ŒåŠ©æ‰‹LLMç”Ÿæˆè´Ÿæ ·æœ¬ï¼Œæå¤§æå‡äº†å¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§ã€å‡å°‘reward hackingã€‚å®éªŒè¯æ˜ï¼ŒCooperåœ¨å¤šä¸ªæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šä¼˜äºä»…ç”¨è§„åˆ™æˆ–é™æ€å¥–åŠ±æ¨¡å‹ï¼ŒåŠ¨æ€æ›´æ–°å¥–åŠ±æ¨¡å‹èƒ½æœ‰æ•ˆæŠ‘åˆ¶reward hackingå¹¶æå‡RLæ•ˆæœã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/COOPER_Co-Optimizing_Policy_and_Reward_Models_in_Reinforcement_Learning_for_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-Follow-Your-Instruction-A-Comprehensive-MLLM-Agent-for-World-Data-Synthesis"><a href="#78-Follow-Your-Instruction-A-Comprehensive-MLLM-Agent-for-World-Data-Synthesis" class="headerlink" title="78. Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Follow-Your-Instruction__A_Comprehensive_MLLM_Agent_for_World_Data_Synthesis.pdf">Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">HKUST(GZ)</span></p>
<p>æœ¬æ–‡æå‡ºäº†Follow-Your-Instructionæ¡†æ¶ï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ•°æ®åˆæˆä»£ç†ï¼Œæ”¯æŒ2Dã€3Då’Œ4Dæ•°æ®çš„è‡ªåŠ¨é«˜è´¨é‡åˆæˆã€‚è¯¥æ–¹æ³•é€šè¿‡MLLM-Collectoræ”¶é›†å¤šæ¨¡æ€èµ„äº§ï¼ŒMLLM-Generatorç”Ÿæˆ3Då¸ƒå±€ï¼ŒMLLM-Optimizerå¤šè§†å›¾ä¼˜åŒ–åœºæ™¯ï¼ŒMLLM-Plannerç»“åˆVLMå®ç°æ—¶åºè¿è´¯çš„è§†é¢‘ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åˆæˆçš„æ•°æ®èƒ½æ˜¾è‘—æå‡å¤šé¡¹AIGCä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œå…·æœ‰è‰¯å¥½å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Follow-Your-Instruction_A_Comprehensive_MLLM_Agent_for_World_Data_Synthesis.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-Conformal-Sets-in-Multiple-Choice-Question-Answering-under-Black-Box-Settings-with-Provable-Coverage-Guarantees"><a href="#79-Conformal-Sets-in-Multiple-Choice-Question-Answering-under-Black-Box-Settings-with-Provable-Coverage-Guarantees" class="headerlink" title="79. Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Conformal_Sets_in_Multiple-Choice_Question_Answering_under_Black-Box_Settings_with_Provable_Coverage.pdf">Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Jinan</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé¢‘ç‡çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œç»“åˆå¤šæ¬¡ç‹¬ç«‹é‡‡æ ·å’Œæ¨¡æ€è¾“å‡ºä½œä¸ºå‚è€ƒï¼Œé€šè¿‡ç»éªŒé¢‘ç‡è®¡ç®—é¢„æµ‹ç†µï¼Œå¹¶åˆ©ç”¨Conformal Predictionç†è®ºæ„å»ºå…·æœ‰å¯è¯æ˜è¦†ç›–ç‡ä¿è¯çš„é¢„æµ‹é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§LLMå’ŒMCQAæ•°æ®é›†ä¸Šè¾ƒä¼ ç»Ÿlogitæ¦‚ç‡æ–¹æ³•æ›´æœ‰æ•ˆåœ°åŒºåˆ†æ­£ç¡®ä¸é”™è¯¯é¢„æµ‹ï¼Œå¹¶èƒ½åœ¨ä¸åŒé£é™©çº§åˆ«ä¸‹æœ‰æ•ˆæ§åˆ¶è¯¯è¦†ç›–ç‡ï¼Œæå‡LLMåœ¨é»‘ç›’åœºæ™¯ä¸‹çš„ä¸ç¡®å®šæ€§é‡åŒ–å¯é æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Conformal_Sets_in_Multiple-Choice_Question_Answering_under_Black-Box_Settings_with_Provable_Coverage_Guarantees.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-AI-vs-Human-Moderators-A-Comparative-Evaluation-of-Multimodal-LLMs-in-Content-Moderation-for-Brand-Safety"><a href="#80-AI-vs-Human-Moderators-A-Comparative-Evaluation-of-Multimodal-LLMs-in-Content-Moderation-for-Brand-Safety" class="headerlink" title="80. AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/AI_vs._Human_Moderators__A_Comparative_Evaluation_of_Multimodal_LLMs_in_Content_Moderation_for_Brand.pdf">AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zefr Inc</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€å’Œå¤šè¯­è¨€è§†é¢‘æ•°æ®é›†ï¼Œç”¨äºå“ç‰Œå®‰å…¨å†…å®¹å®¡æ ¸ï¼Œå¹¶ç³»ç»Ÿè¯„ä¼°äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼Œå¦‚Geminiã€GPTã€Llamaï¼‰åœ¨è§†é¢‘å†…å®¹å®¡æ ¸ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ–¹æ³•åŒ…æ‹¬å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†ã€é›¶æ ·æœ¬åˆ†ç±»å’Œç²¾ç»†åŒ–æç¤ºè®¾è®¡ï¼Œç»“æœæ˜¾ç¤ºMLLMså¯æ˜¾è‘—é™ä½äººå·¥å®¡æ ¸æˆæœ¬ï¼Œä¸”å¤šæ¨¡æ€æ¨¡å‹ä¼˜äºæ–‡æœ¬æ¨¡å‹ï¼Œä½†å‡†ç¡®ç‡ä»ä¸åŠä¸“ä¸šäººå·¥å®¡æ ¸ï¼Œå°¤å…¶åœ¨å¤æ‚å’Œç»†å¾®åˆ†ç±»ä¸Šè¡¨ç°ä¸è¶³ã€‚ç»“è®ºæŒ‡å‡ºMLLMså…·å¤‡æå‡å†…å®¹å®¡æ ¸æ•ˆç‡çš„æ½œåŠ›ï¼Œæœªæ¥å¯é€šè¿‡å¾®è°ƒå’Œå¤šè¯­è¨€ä¼˜åŒ–è¿›ä¸€æ­¥æå‡æ•ˆæœã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/AI_vs._Human_Moderators_A_Comparative_Evaluation_of_Multimodal_LLMs_in_Content_Moderation_for_Brand_Safety.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-Leveraging-AI-to-Accelerate-Clinical-Data-Cleaning-A-Comparative-Study-of-AI-Assisted-vs-Traditional-Methods"><a href="#81-Leveraging-AI-to-Accelerate-Clinical-Data-Cleaning-A-Comparative-Study-of-AI-Assisted-vs-Traditional-Methods" class="headerlink" title="81. Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Leveraging_AI_to_Accelerate_Clinical_Data_Cleaning__A_Comparative_Study_of_AI-Assisted_vs._Tradition.pdf">Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Octozi</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†Octoziå¹³å°ï¼Œé€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Llama 4å¾®è°ƒï¼‰å’Œä¸´åºŠé¢†åŸŸå¯å‘å¼ç®—æ³•ï¼Œå®ç°äº†ä¸´åºŠæ•°æ®æ¸…æ´—æµç¨‹çš„è‡ªåŠ¨åŒ–å’Œæ™ºèƒ½åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAIè¾…åŠ©ä¸ä»…æå‡äº†æ•°æ®æ¸…æ´—é€Ÿåº¦6å€ï¼Œè¿˜å°†é”™è¯¯ç‡ä»54.67%é™è‡³8.48%ï¼Œæ˜¾è‘—å‡è½»äº†äººå·¥è´Ÿæ‹…å¹¶ä¿è¯åˆè§„æ€§å’Œå‡†ç¡®æ€§ï¼ŒéªŒè¯äº†AIåœ¨è¯ç‰©ä¸´åºŠè¯•éªŒæ•°æ®ç®¡ç†ä¸­çš„å˜é©æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Leveraging_AI_to_Accelerate_Clinical_Data_Cleaning_A_Comparative_Study_of_AI-Assisted_vs._Traditional_Methods.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMsâ€™-Entity-Deduction-Capabilities"><a href="#82-The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMsâ€™-Entity-Deduction-Capabilities" class="headerlink" title="82. The World According to LLMs: How Geographic Origin Influences LLMsâ€™ Entity Deduction Capabilities"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/The_World_According_to_LLMs__How_Geographic_Origin_Influences_LLMs'_Entity_Deduction_Capabilities.pdf">The World According to LLMs: How Geographic Origin Influences LLMsâ€™ Entity Deduction Capabilities</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">BITS, Pilani</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºâ€œ20é—®æ¸¸æˆâ€çš„æ–°å‹è¯„æµ‹æ¡†æ¶ï¼Œé€šè¿‡è®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªå‘åœ°æå‡ºé—®é¢˜å¹¶æ¨ç†ï¼Œç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†æ¨¡å‹åœ¨åœ°ç†å®ä½“æ¨ç†ä»»åŠ¡ä¸­çš„éšæ€§åœ°ç†åè§ã€‚ç ”ç©¶å‘ç°ï¼Œä¸»æµLLMsåœ¨æ¨ç†æ¥è‡ªå…¨çƒåŒ—æ–¹å’Œè¥¿æ–¹åœ°åŒºçš„çŸ¥åäººç‰©å’Œäº‹ç‰©æ—¶è¡¨ç°æ›´ä¼˜ï¼Œè€Œå®ä½“çš„æµè¡Œåº¦å’Œè®­ç»ƒè¯­æ–™é¢‘æ¬¡å¯¹è¿™ç§å·®å¼‚çš„è§£é‡ŠåŠ›æœ‰é™ï¼Œæ¸¸æˆè¯­è¨€å½±å“ä¹Ÿæå°ï¼Œå‡¸æ˜¾äº†æ¨¡å‹æ¨ç†è·¯å¾„ä¸­çš„åœ°ç†å’Œæ–‡åŒ–ä¸å¹³ç­‰ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/The_World_According_to_LLMs_How_Geographic_Origin_Influences_LLMs%E2%80%99_Entity_Deduction_Capabilities.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-Streamlining-Admission-with-LOR-Insights-AI-Based-Leadership-Assessment-in-Online-Masterâ€™s-Program"><a href="#83-Streamlining-Admission-with-LOR-Insights-AI-Based-Leadership-Assessment-in-Online-Masterâ€™s-Program" class="headerlink" title="83. Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Masterâ€™s Program"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Streamlining_Admission_with_LOR_Insights__AI-Based_Leadership_Assessment_in_Online_Master's_Program.pdf">Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Masterâ€™s Program</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Georgia Institute of Technology</span></p>
<p>æœ¬è®ºæ–‡æå‡ºLORIç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆRoBERTaå¼±ç›‘ç£æ¨¡å‹å’ŒLLAMA2ç­‰å¤§è¯­è¨€æ¨¡å‹ï¼Œå¯¹ç¡•å£«ç”³è¯·è€…æ¨èä¿¡ä¸­çš„é¢†å¯¼åŠ›ç‰¹è´¨ï¼ˆå¦‚å›¢é˜Ÿåä½œã€æ²Ÿé€šã€åˆ›æ–°ï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–æ£€æµ‹ä¸å¾®æ ‡ç­¾åˆ†ç±»ã€‚å®éªŒè¯æ˜RoBERTaæ¨¡å‹åœ¨é¢†å¯¼åŠ›å¥å­è¯†åˆ«ä¸Šå–å¾—äº†91.6%çš„F1åˆ†æ•°ï¼Œå¹¶é€šè¿‡LLMå®ç°æ›´æ·±å±‚æ¬¡çš„çŸ­è¯­æå–ä¸éªŒè¯ï¼Œæœ‰æ•ˆæå‡äº†æ‹›ç”Ÿæµç¨‹æ•ˆç‡å’Œå…¬å¹³æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Streamlining_Admission_with_LOR_Insights_AI-Based_Leadership_Assessment_in_Online_Master%27s_Program.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-Auto-Eval-Judge-Towards-a-General-Agentic-Framework-for-Task-Completion-Evaluation"><a href="#84-Auto-Eval-Judge-Towards-a-General-Agentic-Framework-for-Task-Completion-Evaluation" class="headerlink" title="84. Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Auto-Eval_Judge__Towards_a_General_Agentic_Framework_for_Task_Completion_Evaluation.pdf">Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Massachusetts Amherst</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨ã€æ¨¡å—åŒ–çš„Agentä»»åŠ¡å®Œæˆè¯„ä»·æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åˆ†è§£ä»»åŠ¡ä¸ºå­ä»»åŠ¡ï¼Œé€šè¿‡LLMå’Œå¤šAgentæ¨¡å—éªŒè¯æ¯ä¸€æ­¥æ¨ç†å’Œæœ€ç»ˆç»“æœã€‚å®éªŒåœ¨GAIAå’ŒBigCodeBenchæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œä¸GPT-4o LLM-as-a-JudgeåŸºçº¿ç›¸æ¯”ï¼Œè¯¥Judge Agentä¸äººç±»è¯„ä¼°çš„å¯¹é½åº¦æå‡4.76%å’Œ10.52%ã€‚ç»“è®ºæ˜¾ç¤ºè¯¥æ¡†æ¶èƒ½æ›´å…¨é¢ã€å¯æ‰©å±•åœ°è¯„ä»·å¤æ‚Agentç³»ç»Ÿï¼Œå‡å°‘äººå·¥ä¾èµ–ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Auto-Eval_Judge_Towards_a_General_Agentic_Framework_for_Task_Completion_Evaluation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities"><a href="#85-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities" class="headerlink" title="85. InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/InfiAlign__A_Scalable_and_Sample-Efficient_Framework_for_Aligning_LLMs_to_Enhance_Reasoning_Capabili.pdf">InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">InfiX.ai</span></p>
<p>æœ¬æ–‡æå‡ºäº†InfiAlignæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤šç»´åº¦è‡ªåŠ¨åŒ–æ•°æ®ç­›é€‰ã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œå®ç°å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„é«˜æ•ˆå¯¹é½ã€‚æ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬å¤šç»´è´¨é‡æŒ‡æ ‡é©±åŠ¨çš„æ•°æ®é‡‡æ ·ï¼ˆæ¶µç›–å¤šæ ·æ€§ã€éš¾åº¦ã€è´¨é‡ç­‰ï¼‰ï¼Œå¹¶é‡‡ç”¨æ¨¡å—åŒ–å’Œå¯æ‰©å±•è®¾è®¡ï¼Œå®ç°å°æ•°æ®é‡ä¸‹çš„é«˜æ€§èƒ½æ¨ç†èƒ½åŠ›ã€‚ç»“è®ºæ˜¾ç¤ºï¼ŒInfiAlignåœ¨ä»…ç”¨12%æ•°æ®é‡ä¸‹è¾¾åˆ°ä¸ä¸»æµå¼ºåŸºçº¿ç›¸å½“æ€§èƒ½ï¼ŒDPOè¿›ä¸€æ­¥æå‡æ•°å­¦æ¨ç†è¡¨ç°ï¼Œå®ç°äº†é«˜æ•°æ®åˆ©ç”¨ç‡å’Œæ˜“æ‰©å±•æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/InfiAlign_A_Scalable_and_Sample-Efficient_Framework_for_Aligning_LLMs_to_Enhance_Reasoning_Capabilities.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-Can-Large-Language-Models-Generate-Effective-Datasets-for-Emotion-Recognition-in-Conversations"><a href="#86-Can-Large-Language-Models-Generate-Effective-Datasets-for-Emotion-Recognition-in-Conversations" class="headerlink" title="86. Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Can_Large_Language_Models_Generate_Effective_Datasets_for_Emotion_Recognition_in_Conversations_.pdf">Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Hamburg</span></p>
<p>æœ¬æ–‡æå‡ºåˆ©ç”¨å°å‹ã€é€šç”¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆVicuna 1.5-13Bï¼‰ç»“åˆç»“æ„åŒ–æç¤ºå·¥ç¨‹ï¼Œè‡ªåŠ¨åˆæˆå¤šæ ·åŒ–çš„å¯¹è¯æƒ…ç»ªè¯†åˆ«ï¼ˆERCï¼‰æ•°æ®é›†ï¼ŒåŒ…æ‹¬è‡ªç„¶åˆ†å¸ƒå’Œå‡è¡¡åˆ†å¸ƒä¸¤ç§ç±»å‹ï¼Œé’ˆå¯¹ä¸»æµERCåŸºå‡†ï¼ˆMELDã€EmoryNLPã€IEMOCAPï¼‰åˆ†åˆ«ç”Ÿæˆç›¸åº”çš„æ•°æ®é›†ï¼Œå¹¶é€šè¿‡å¤šç§ç°æœ‰ERCåˆ†ç±»æ¨¡å‹éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœå’Œç»Ÿè®¡æ£€éªŒæ˜¾ç¤ºï¼ŒåŸºäºLLMç”Ÿæˆçš„æ•°æ®é›†èƒ½æå‡æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹çš„é²æ£’æ€§å’Œæ€§èƒ½ï¼Œå¹¶å¯¹æ ‡ç­¾åˆ†å¸ƒå¤±è¡¡ç­‰é—®é¢˜å…·æœ‰ç§¯æå½±å“ï¼Œç»“è®ºä¸ºè¯¥æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆã€å¯å¤ç°åœ°ç”Ÿæˆé«˜è´¨é‡ERCæ•°æ®é›†ï¼Œä¿ƒè¿›ä¸‹æ¸¸æ¨¡å‹æ€§èƒ½æå‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Can_Large_Language_Models_Generate_Effective_Datasets_for_Emotion_Recognition_in_Conversations_.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-Group-Causal-Policy-Optimization-for-Post-Training-Large-Language-Models"><a href="#87-Group-Causal-Policy-Optimization-for-Post-Training-Large-Language-Models" class="headerlink" title="87. Group Causal Policy Optimization for Post-Training Large Language Models"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Group_Causal_Policy_Optimization_for_Post-Training_Large_Language_Models.pdf">Group Causal Policy Optimization for Post-Training Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute of Software, Chinese Academy of Sciences</span></p>
<p>æœ¬è®ºæ–‡æå‡ºGroup Causal Policy Optimizationï¼ˆGCPOï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç»“æ„åŒ–å› æœæ¨¡å‹ï¼ˆSCMï¼‰ï¼Œå‘ç°LLMå€™é€‰å“åº”é—´å› æœ€ç»ˆé›†æˆè¾“å‡ºè€Œäº§ç”Ÿå› æœå…³è”ï¼Œè¿›è€Œå°†å€™é€‰è¾“å‡ºæŠ•å½±åˆ°å› æœå­ç©ºé—´ï¼Œå¹¶ç»“åˆKLæ­£åˆ™é¡¹ï¼Œå¼•å¯¼ç­–ç•¥ä¼˜åŒ–åœ¨ç»“æ„ä¸Šæ›´ä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGCPOåœ¨å¤šé¡¹æ•°å­¦ä¸ä»£ç æ¨ç†åŸºå‡†ä¸Šå‡ä¼˜äºGRPOç­‰ç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å› æœç»“æ„å»ºæ¨¡å¯¹LLMåè®­ç»ƒæ•ˆæœçš„æå‡å’Œé²æ£’æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Group_Causal_Policy_Optimization_for_Post-Training_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-LLM-based-Multi-Agent-Copilot-for-Quantum-Sensor"><a href="#88-LLM-based-Multi-Agent-Copilot-for-Quantum-Sensor" class="headerlink" title="88. LLM-based Multi-Agent Copilot for Quantum Sensor"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/LLM-based_Multi-Agent_Copilot_for_Quantum_Sensor.pdf">LLM-based Multi-Agent Copilot for Quantum Sensor</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">National University of Defense Technology</span></p>
<p>æœ¬æ–‡æå‡ºQCopilotï¼Œä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé›†æˆå¤–éƒ¨çŸ¥è¯†è®¿é—®ã€ä¸»åŠ¨å­¦ä¹ å’Œä¸ç¡®å®šæ€§é‡åŒ–ï¼Œç”¨äºé‡å­ä¼ æ„Ÿå™¨çš„è®¾è®¡ä¸è¯Šæ–­ã€‚QCopiloté€šè¿‡ä¸“ç”¨æ™ºèƒ½ä½“å®ç°å®éªŒå‚æ•°è‡ªé€‚åº”ä¼˜åŒ–ã€è‡ªåŠ¨å»ºæ¨¡åˆ†æå’Œç‹¬ç«‹æ•…éšœè¯Šæ–­ï¼Œå®éªŒåœ¨å†·åŸå­åŸå‹ç³»ç»Ÿä¸­å®ç°äº†æ— äººå·¥å¹²é¢„ä¸‹çš„é«˜æ•ˆåˆ¶å†·å’Œå¼‚å¸¸å‚æ•°è‡ªä¸»è¯†åˆ«ï¼Œå¤§å¹…æå‡äº†å®éªŒæ•ˆç‡å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/LLM-based_Multi-Agent_Copilot_for_Quantum_Sensor.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-StructVRM-Aligning-Multimodal-Reasoning-with-Structured-and-Verifiable-Reward-Models"><a href="#89-StructVRM-Aligning-Multimodal-Reasoning-with-Structured-and-Verifiable-Reward-Models" class="headerlink" title="89. StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/StructVRM__Aligning_Multimodal_Reasoning_with_Structured_and_Verifiable_Reward_Models.pdf">StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">ByteDance Seed China</span></p>
<p>StructVRMæå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„ç»†ç²’åº¦å¥–åŠ±æœºåˆ¶ï¼Œé€šè¿‡è®­ç»ƒå¯éªŒè¯çš„å¥–åŠ±æ¨¡å‹ï¼ˆverifierï¼‰ï¼Œåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å¯¹å¤šå­é—®é¢˜è¿›è¡Œç»“æ„åŒ–ã€å¯éªŒè¯çš„è¯„åˆ†ï¼Œå®ç°äº†éƒ¨åˆ†æ­£ç¡®æ€§åé¦ˆï¼Œå¹¶ç»“åˆPPOå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒStructVRMåœ¨å…­ä¸ªå…¬å¼€å¤šæ¨¡æ€åŸºå‡†å’Œé«˜éš¾åº¦STEM-Benchä¸Šå–å¾—SOTAè¡¨ç°ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚ç°å®æ¨ç†ä»»åŠ¡ä¸­çš„å¤šæ¨¡æ€æ¨¡å‹èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/StructVRM_Aligning_Multimodal_Reasoning_with_Structured_and_Verifiable_Reward_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-The-Term-â€˜Agentâ€™-Has-Been-Diluted-Beyond-Utility-and-Requires-Redefinition"><a href="#90-The-Term-â€˜Agentâ€™-Has-Been-Diluted-Beyond-Utility-and-Requires-Redefinition" class="headerlink" title="90. The Term â€˜Agentâ€™ Has Been Diluted Beyond Utility and Requires Redefinition"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/The_Term_'Agent'_Has_Been_Diluted_Beyond_Utility_and_Requires_Redefinition.pdf">The Term â€˜Agentâ€™ Has Been Diluted Beyond Utility and Requires Redefinition</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Duke University</span></p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹â€œagentâ€ä¸€è¯åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸè¢«å¹¿æ³›æ»¥ç”¨å’Œå®šä¹‰æ¨¡ç³Šçš„é‡æ–°å®šä¹‰æ¡†æ¶ã€‚é€šè¿‡å†å²å’Œç°æœ‰ç”¨æ³•åˆ†æï¼Œä½œè€…æå‡ºäº†äº”ç»´åº¦ï¼ˆç¯å¢ƒäº¤äº’ã€ç›®æ ‡å¯¼å‘ã€æ—¶é—´ä¸€è‡´æ€§ã€å­¦ä¹ é€‚åº”æ€§å’Œè‡ªä¸»æ€§ï¼‰è¯„ä»·ä½“ç³»ï¼Œå¹¶æ˜ç¡®äº†æˆä¸ºâ€˜agentâ€™çš„æœ€ä½è¦æ±‚ã€‚ç»“è®ºæŒ‡å‡ºï¼Œè¯¥æ¡†æ¶æœ‰åŠ©äºæå‡ç ”ç©¶æ–¹æ³•çš„ä¸¥è°¨æ€§ã€æ²Ÿé€šçš„æ¸…æ™°åº¦å’Œæ”¿ç­–åˆ¶å®šçš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/The_Term_%E2%80%98Agent%E2%80%99_Has_Been_Diluted_Beyond_Utility_and_Requires_Redefinition.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-LAG-Logic-Augmented-Generation-from-a-Cartesian-Perspective"><a href="#91-LAG-Logic-Augmented-Generation-from-a-Cartesian-Perspective" class="headerlink" title="91. LAG: Logic-Augmented Generation from a Cartesian Perspective"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/LAG__Logic-Augmented_Generation_from_a_Cartesian_Perspective.pdf">LAG: Logic-Augmented Generation from a Cartesian Perspective</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Hong Kong Polytechnic University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†LAGï¼ˆLogic-Augmented Generationï¼‰ï¼Œä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„åˆ›æ–°æ¨ç†ç®¡é“ã€‚é€šè¿‡è‡ªé€‚åº”é—®é¢˜åˆ†è§£ã€é€»è¾‘é“¾æ¨ç†ã€é€»è¾‘ç»ˆæ­¢æœºåˆ¶ï¼ŒLAGèƒ½å°†å¤æ‚é—®é¢˜æ‹†è§£ä¸ºåŸå­å­é—®é¢˜ï¼ŒæŒ‰é€»è¾‘ä¾èµ–é¡ºåºé€æ­¥è§£å†³ï¼Œå¹¶åœ¨å„ç¯èŠ‚æœ‰æ•ˆé˜²æ­¢é”™è¯¯ä¼ é€’ã€‚å®éªŒè¡¨æ˜LAGåœ¨å¤šè·³é—®ç­”å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡å‡†ç¡®ç‡å’Œåˆç†æ€§ï¼Œä¼˜äºä¸»æµRAGæ–¹æ³•ï¼Œå®ç°æ›´æ¥è¿‘äººç±»è®¤çŸ¥çš„æ¨ç†è¿‡ç¨‹ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/LAG_Logic-Augmented_Generation_from_a_Cartesian_Perspective.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs"><a href="#92-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs" class="headerlink" title="92. MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MELLA__Bridging_Linguistic_Capability_and_Cultural_Groundedness_for_Low-Resource_Language_MLLMs.pdf">MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Artificial Intelligence Laboratory</span></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹ä½èµ„æºè¯­è¨€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„åŒç›®æ ‡ä¸åŒæºæ•°æ®æ¡†æ¶ï¼Œå¼ºè°ƒè¯­è¨€èƒ½åŠ›å’Œæ–‡åŒ–æ‰æ ¹æ€§ã€‚é€šè¿‡æ„å»ºMELLAå¤§è§„æ¨¡å¤šè¯­ç§å¤šæ¨¡æ€æ•°æ®é›†ï¼Œåˆ†åˆ«é‡‡é›†æœ¬åœ°ç½‘é¡µåŸç”Ÿalt-textï¼ˆæ–‡åŒ–çŸ¥è¯†ï¼‰å’ŒMLLMç”Ÿæˆæè¿°ï¼ˆè¯­è¨€èƒ½åŠ›ï¼‰ï¼Œå¹¶è”åˆè®­ç»ƒï¼Œæ˜¾è‘—æå‡æ¨¡å‹åœ¨å…«ç§ä½èµ„æºè¯­è¨€ä¸Šçš„æè¿°å‡†ç¡®åº¦å’Œæ–‡åŒ–è®¤çŸ¥èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡MELLAå¾®è°ƒåï¼Œå¤šæ¨¡æ€æ¨¡å‹åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ›´ä¸°å¯Œçš„â€œåšæè¿°â€ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MELLA_Bridging_Linguistic_Capability_and_Cultural_Groundedness_for_Low-Resource_Language_MLLMs.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-GRAIL-Learning-to-Interact-with-Large-Knowledge-Graphs-for-Retrieval-Augmented-Reasoning"><a href="#93-GRAIL-Learning-to-Interact-with-Large-Knowledge-Graphs-for-Retrieval-Augmented-Reasoning" class="headerlink" title="93. GRAIL: Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/GRAIL_Learning_to_Interact_with_Large_Knowledge_Graphs_for_Retrieval_Augmented_Reasoning.pdf">GRAIL: Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute for AI Industry Research (AIR), Tsinghua University</span></p>
<p>GRAILæå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å¤§å‹çŸ¥è¯†å›¾è°±çš„äº¤äº’å¼æ£€ç´¢å¢å¼ºæ¨ç†æ¡†æ¶ï¼Œé€šè¿‡LLMå¼•å¯¼çš„æ•°æ®åˆæˆã€ä¸¤é˜¶æ®µï¼ˆç›‘ç£å¾®è°ƒ+å¼ºåŒ–å­¦ä¹ ï¼‰è®­ç»ƒå’ŒåŠ¨æ€äº¤äº’æ£€ç´¢æœºåˆ¶ï¼Œå®ç°å¤šæ­¥é«˜æ•ˆã€ç»“æ„æ„ŸçŸ¥çš„ä¿¡æ¯æ£€ç´¢ä¸æ¨ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGRAILåœ¨å¤šä¸ªçŸ¥è¯†å›¾è°±é—®ç­”åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œå¹³å‡å‡†ç¡®ç‡æå‡21.01%ã€F1æå‡22.43%ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘å†—ä½™æ£€ç´¢ä¿¡æ¯ï¼Œå®ç°æ£€ç´¢æ·±åº¦ä¸ç²¾åº¦çš„é«˜æ•ˆå¹³è¡¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/GRAIL_Learning_to_Interact_with_Large_Knowledge_Graphs_for_Retrieval_Augmented_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-MoMA-A-Mixture-of-Multimodal-Agents-Architecture-for-Enhancing-Clinical-Prediction-Modelling"><a href="#94-MoMA-A-Mixture-of-Multimodal-Agents-Architecture-for-Enhancing-Clinical-Prediction-Modelling" class="headerlink" title="94. MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MoMA__A_Mixture-of-Multimodal-Agents_Architecture_for_Enhancing_Clinical_Prediction_Modelling.pdf">MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Wisconsin-Madison</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†MoMAæ¶æ„ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“åä½œï¼Œåˆ©ç”¨å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†å¤šæ¨¡æ€ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ•°æ®ä¸­çš„éæ–‡æœ¬æ¨¡æ€ï¼ˆå¦‚åŒ»å­¦å½±åƒã€å®éªŒå®¤ç»“æœï¼‰è½¬åŒ–ä¸ºç»“æ„åŒ–æ–‡æœ¬æ‘˜è¦ï¼Œå†ç”±èšåˆæ™ºèƒ½ä½“æ•´åˆæ‰€æœ‰æ–‡æœ¬åäº¤ç»™é¢„æµ‹æ™ºèƒ½ä½“ç”¨äºä¸´åºŠé¢„æµ‹ã€‚è¯„æµ‹ç»“æœè¡¨æ˜ï¼ŒMoMAåœ¨å¤šä¸ªå¤šæ¨¡æ€ä¸´åºŠä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰ä¸»æµæ–¹æ³•ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„é¢„æµ‹å‡†ç¡®æ€§å’Œçµæ´»æ€§ï¼Œä¸”å¯çµæ´»é€‚é…æ–°æ¨¡æ€ï¼Œé™ä½å¯¹å¤§è§„æ¨¡é…å¯¹æ•°æ®çš„ä¾èµ–ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MoMA_A_Mixture-of-Multimodal-Agents_Architecture_for_Enhancing_Clinical_Prediction_Modelling.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-MyCulture-Exploring-Malaysiaâ€™s-Diverse-Culture-under-Low-Resource-Language-Constraints"><a href="#95-MyCulture-Exploring-Malaysiaâ€™s-Diverse-Culture-under-Low-Resource-Language-Constraints" class="headerlink" title="95. MyCulture: Exploring Malaysiaâ€™s Diverse Culture under Low-Resource Language Constraints"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MyCulture__Exploring_Malaysia's_Diverse_Culture_under_Low-Resource_Language_Constraints.pdf">MyCulture: Exploring Malaysiaâ€™s Diverse Culture under Low-Resource Language Constraints</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Universiti Malaya</span></p>
<p>æœ¬æ–‡æå‡ºMyCultureåŸºå‡†ï¼Œæ˜¯é¦–ä¸ªé¢å‘é©¬æ¥è¥¿äºšã€ä»¥é©¬æ¥è¯­å‘ˆç°çš„å¤šå…ƒæ–‡åŒ–LLMè¯„æµ‹åŸºå‡†ï¼Œè¦†ç›–è‰ºæœ¯ã€æœé¥°ã€ä¹ ä¿—ã€å¨±ä¹ã€é£Ÿç‰©å’Œå®—æ•™å…­å¤§é¢†åŸŸï¼Œå¹¶é‡‡ç”¨åˆ›æ–°çš„æ— æ ‡å‡†é€‰é¡¹çš„å¼€æ”¾å¼å¤šé€‰é¢˜æ ¼å¼ï¼ˆåŒ…æ‹¬å¤šç­”æ¡ˆã€æ’åºã€åŒ¹é…ä¸‰ç§ç±»å‹ï¼‰ä»¥å‡å°‘çŒœæµ‹å’Œæ ¼å¼åå€šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¼ ç»Ÿå°é—­å¼å¤šé€‰é¢˜é«˜ä¼°äº†æ¨¡å‹çš„æ–‡åŒ–ç†è§£èƒ½åŠ›ï¼Œé‡‡ç”¨å¼€æ”¾å¼é¢˜å‹åä¸»æµLLMçš„æˆç»©å¹³å‡ä¸‹é™è‡³å°‘17%ï¼Œå‡¸æ˜¾æ›´ä¸¥æ ¼ã€æ–‡åŒ–åµŒå…¥å‹è¯„æµ‹çš„é‡è¦æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MyCulture_Exploring_Malaysia%E2%80%99s_Diverse_Culture_under_Low-Resource_Language_Constraints.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-Large-Language-Models-Transform-Organic-Synthesis-From-Reaction-Prediction-to-Automation"><a href="#96-Large-Language-Models-Transform-Organic-Synthesis-From-Reaction-Prediction-to-Automation" class="headerlink" title="96. Large Language Models Transform Organic Synthesis: From Reaction Prediction to Automation"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Large_Language_Models_Transform_Organic_Synthesis_From_Reaction_Prediction_to_Automation.pdf">Large Language Models Transform Organic Synthesis: From Reaction Prediction to Automation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Government Boys Higher Secondary School, Bukera Sharif, Tando Allahyar, Affiliated with BISE Hyderabad, Sindh, Pakistan</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿç»¼è¿°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœ‰æœºåˆæˆä¸­çš„åº”ç”¨è¿›å±•ï¼ŒåŒ…æ‹¬åŒ–å­¦ååº”é¢„æµ‹ã€é€†åˆæˆè§„åˆ’ã€æ¡ä»¶ä¼˜åŒ–åŠä¸æœºå™¨äººå¹³å°çš„è‡ªåŠ¨åŒ–é›†æˆã€‚é€šè¿‡ç»“åˆLLMsä¸å›¾ç¥ç»ç½‘ç»œã€å¼ºåŒ–å­¦ä¹ å’Œå¤šæ¨¡æ€æ•°æ®ï¼Œæ˜¾è‘—æå‡äº†ååº”é¢„æµ‹å‡†ç¡®ç‡å’Œå®éªŒè‡ªåŠ¨åŒ–æ•ˆç‡ï¼ŒåŒæ—¶è®¨è®ºäº†æ•°æ®åå·®ã€æ¨¡å‹è§£é‡Šæ€§å’Œå®‰å…¨é£é™©ç­‰æŒ‘æˆ˜ã€‚ç»“è®ºè®¤ä¸ºLLMsæ­£åœ¨æ¨åŠ¨åŒ–å­¦åˆæˆå‘é«˜æ•ˆã€è‡ªåŠ¨åŒ–å’Œå¯æŒç»­æ–¹å‘å˜é©ï¼Œä½†ä»éœ€è§£å†³å¯é æ€§ä¸å¯è§£é‡Šæ€§ç­‰å…³é”®é—®é¢˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Large_Language_Models_Transform_Organic_Synthesis_From_Reaction_Prediction_to_Automation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-NomicLaw-Emergent-Trust-and-Strategic-Argumentation-in-LLMs-During-Collaborative-Law-Making"><a href="#97-NomicLaw-Emergent-Trust-and-Strategic-Argumentation-in-LLMs-During-Collaborative-Law-Making" class="headerlink" title="97. NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/NomicLaw__Emergent_Trust_and_Strategic_Argumentation_in_LLMs_During_Collaborative_Law-Making.pdf">NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Jyvaskyla</span></p>
<p>è¯¥è®ºæ–‡æå‡ºNomicLawæ¡†æ¶ï¼Œè®©å¤šä¸ªå¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ³•å¾‹æ²»ç†åœºæ™¯ä¸‹ä»¥â€œææ¡ˆ-è®ºè¯-æŠ•ç¥¨â€å¾ªç¯åä½œç«‹æ³•ï¼Œé‡åŒ–åˆ†æä¿¡ä»»ã€äº’æƒ å’Œè”ç›Ÿå½¢æˆç­‰ç¤¾ä¼šåŠ¨æ€ï¼Œå¹¶é€šè¿‡ä¸»é¢˜åˆ†ææ­ç¤ºæ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“åä½œä¸­å‘ˆç°çš„æ³•å¾‹æ¨ç†å’Œè¯´æœèƒ½åŠ›ã€‚å®éªŒæ˜¾ç¤ºï¼Œæ¨¡å‹å¼‚è´¨æ€§èƒ½å‡å°‘è‡ªæŠ•ã€å¢åŠ è”ç›Ÿåˆ‡æ¢å¹¶ä¸°å¯Œæ³•å¾‹è®ºè¯ç±»å‹ï¼Œå¼ºè°ƒå¤šæ ·åŒ–æ¨¡å‹ç»„åˆå¯æå‡AIè¾…åŠ©ç«‹æ³•çš„åˆç†æ€§å’Œå¤šå…ƒæ€§ï¼Œä½†å½“å‰LLMæ¨ç†å¤šä¸ºç»Ÿè®¡æ¨¡å¼è€ŒéçœŸæ­£ç†è§£ï¼Œéœ€äººç±»ç›‘ç®¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/NomicLaw_Emergent_Trust_and_Strategic_Argumentation_in_LLMs_During_Collaborative_Law-Making.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-A-Novel-Architecture-for-Symbolic-Reasoning-with-Decision-Trees-and-LLM-Agents"><a href="#98-A-Novel-Architecture-for-Symbolic-Reasoning-with-Decision-Trees-and-LLM-Agents" class="headerlink" title="98. A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Novel_Architecture_for_Symbolic_Reasoning_with_Decision_Trees_and_LLM_Agents.pdf">A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Harvard University</span></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆå†³ç­–æ ‘ç¬¦å·æ¨ç†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæ¨ç†èƒ½åŠ›çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¶æ„ã€‚æ–¹æ³•åŒ…æ‹¬å°†å†³ç­–æ ‘ä½œä¸ºå¯è°ƒç”¨çš„ç¬¦å·æ¨ç†æ¨¡å—åµŒå…¥åˆ°ç”±LLMé©±åŠ¨çš„åè°ƒæ¨ç†æµç¨‹ä¸­ï¼Œé€šè¿‡ä¸­å¤®ç¼–æ’å™¨ç»´æŠ¤ä¿¡å¿µä¸€è‡´æ€§å¹¶åŠ¨æ€è°ƒç”¨å·¥å…·ï¼Œå®ç°ç»“æ„åŒ–å’Œéç»“æ„åŒ–çŸ¥è¯†çš„è·¨æ¨¡æ€æ¨ç†ã€‚å®éªŒè¡¨æ˜è¯¥æ¶æ„åœ¨ProofWriterã€GSM8kå’ŒARCç­‰åŸºå‡†ä»»åŠ¡ä¸Šæ¨ç†ä¸€è‡´æ€§å’Œå‡†ç¡®ç‡å‡è¶…è¿‡ç°æœ‰æ–¹æ³•ï¼Œä¸”ç³»ç»Ÿå…·å¤‡é«˜åº¦å¯è§£é‡Šæ€§å’Œå¯æ‰©å±•æ€§ï¼Œé€‚ç”¨äºåŒ»ç–—å†³ç­–æ”¯æŒå’Œç§‘å­¦å‘ç°ç­‰é¢†åŸŸã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Novel_Architecture_for_Symbolic_Reasoning_with_Decision_Trees_and_LLM_Agents.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-B4DL-A-Benchmark-for-4D-LiDAR-LLM-in-Spatio-Temporal-Understanding"><a href="#99-B4DL-A-Benchmark-for-4D-LiDAR-LLM-in-Spatio-Temporal-Understanding" class="headerlink" title="99. B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding"></a>99. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/B4DL__A_Benchmark_for_4D_LiDAR_LLM_in_Spatio-Temporal_Understanding.pdf">B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Korea Advanced Institute of Science and Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºB4DLåŸºå‡†ä¸æ•°æ®é›†ï¼Œä¸“ä¸ºè®­ç»ƒå’Œè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨4D LiDARæ—¶ç©ºç†è§£ä»»åŠ¡ä¸­è€Œè®¾è®¡ã€‚æ–¹æ³•åŒ…æ‹¬åˆ›æ–°çš„æ•°æ®ç”Ÿæˆç®¡çº¿ï¼ˆèåˆå¤šè§†è§’å›¾åƒä¸äººå·¥æ³¨é‡Šï¼Œè‡ªåŠ¨ç”ŸæˆLiDARæ—¶ç©ºåœºæ™¯çš„é—®ç­”å¯¹ï¼‰ï¼Œä»¥åŠæå‡ºB4DLæ¨¡å‹ï¼Œé€šè¿‡LiDARç¼–ç å™¨ã€å¯¹é½å±‚å’Œå…ƒä¿¡æ¯æ¨¡å—å®ç°4D LiDARä¸è¯­è¨€çš„é«˜æ•ˆå¯¹é½ï¼Œé‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥æå‡æ¨¡å‹ç©ºé—´ä¸æ—¶åºæ¨ç†èƒ½åŠ›ã€‚å®éªŒè¯æ˜B4DLæ¨¡å‹åœ¨ç©ºé—´åŠæ—¶åºç†è§£ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰3Dæˆ–è§†é¢‘å¤šæ¨¡æ€LLMï¼Œåœ¨å¤šé¡¹ä»»åŠ¡å’Œè·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢è¡¨ç°çªå‡ºã€‚ç»“è®ºï¼šB4DLä¸º4D LiDARå¤šæ¨¡æ€ç†è§£æä¾›äº†é¦–ä¸ªé«˜è´¨é‡åŸºå‡†å’Œæ–¹æ³•ä½“ç³»ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†çœŸå®ä¸–ç•Œæ—¶ç©ºåŠ¨æ€ç†è§£é¢†åŸŸçš„å‘å±•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/B4DL_A_Benchmark_for_4D_LiDAR_LLM_in_Spatio-Temporal_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-Resource-Limited-Joint-Multimodal-Sentiment-Reasoning-and-Classification-via-Chain-of-Thought-Enhancement-and-Distillation"><a href="#100-Resource-Limited-Joint-Multimodal-Sentiment-Reasoning-and-Classification-via-Chain-of-Thought-Enhancement-and-Distillation" class="headerlink" title="100. Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation"></a>100. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Resource-Limited_Joint_Multimodal_Sentiment_Reasoning_and_Classification_via_Chain-of-Thought_Enhanc.pdf">Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Northeastern University</span></p>
<p>æœ¬æ–‡æå‡ºMulCoT-RDæ¡†æ¶ï¼Œé’ˆå¯¹èµ„æºå—é™åœºæ™¯ä¸‹çš„å¤šæ¨¡æ€æƒ…æ„Ÿæ¨ç†ä¸åˆ†ç±»ï¼ˆJMSRCï¼‰ä»»åŠ¡ï¼Œé‡‡ç”¨â€œæ•™å¸ˆ-åŠ©ç†-å­¦ç”Ÿâ€ä¸‰é˜¶æ®µæ¨ç†è’¸é¦èŒƒå¼ï¼Œé€šè¿‡å¤šä»»åŠ¡å­¦ä¹ åŠç»“æ„åŒ–CoTï¼ˆChain-of-Thoughtï¼‰å¢å¼ºå’Œè½¯ç¡¬æ ‡ç­¾è”åˆä¼˜åŒ–ï¼Œä½¿è½»é‡çº§å¤šæ¨¡æ€å¤§æ¨¡å‹èƒ½è‡ªä¸»ç”Ÿæˆé«˜è´¨é‡æƒ…æ„Ÿæ¨ç†é“¾å¹¶å®ç°é«˜æ•ˆæƒ…æ„Ÿåˆ†ç±»ã€‚å®éªŒè¡¨æ˜ï¼ŒMulCoT-RDåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå®ç°äº†è¾ƒä¼˜çš„æƒ…æ„Ÿåˆ†ç±»æ€§èƒ½å’Œæ¨ç†è§£é‡Šæ€§ï¼Œä¸”æ¨¡å‹ä»…3Bå‚æ•°ï¼Œå…·å¤‡è‰¯å¥½çš„éƒ¨ç½²é€‚åº”æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Resource-Limited_Joint_Multimodal_Sentiment_Reasoning_and_Classification_via_Chain-of-Thought_Enhancement_and_Distillation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="101-VFlowOpt-A-Token-Pruning-Framework-for-LMMs-with-Visual-Information-Flow-Guided-Optimization"><a href="#101-VFlowOpt-A-Token-Pruning-Framework-for-LMMs-with-Visual-Information-Flow-Guided-Optimization" class="headerlink" title="101. VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization"></a>101. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/VFlowOpt__A_Token_Pruning_Framework_for_LMMs_with_Visual_Information_Flow-Guided_Optimization.pdf">VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai AI Laboratory</span></p>
<p>æœ¬æ–‡æå‡ºVFlowOptï¼Œä¸€ç§é’ˆå¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å¯å¾®è§†è§‰ä¿¡æ¯æµå¼•å¯¼çš„è§†è§‰tokenè£å‰ªæ¡†æ¶ã€‚æ–¹æ³•é€šè¿‡ç»“åˆæ³¨æ„åŠ›æ ¡å‡†å’Œä¿¡æ¯ç†µï¼Œåˆ†é˜¶æ®µè¯„ä¼°è§†è§‰tokené‡è¦æ€§ï¼Œé‡‡ç”¨å›æ”¶æœºåˆ¶å‡å°‘ä¿¡æ¯æŸå¤±ï¼Œå¹¶åˆ©ç”¨è§†è§‰ä¿¡æ¯æµä¼˜åŒ–å„é˜¶æ®µè£å‰ªè¶…å‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒVFlowOptå¯åœ¨ä¿ç•™10%è§†è§‰tokençš„æƒ…å†µä¸‹ä»ç»´æŒ90%æ¨¡å‹æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨å’Œæ¨ç†æ—¶å»¶ï¼Œé€‚ç”¨äºå®é™…éƒ¨ç½²ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/VFlowOpt_A_Token_Pruning_Framework_for_LMMs_with_Visual_Information_Flow-Guided_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="102-FAITH-A-Framework-for-Assessing-Intrinsic-Tabular-Hallucinations-in-Finance"><a href="#102-FAITH-A-Framework-for-Assessing-Intrinsic-Tabular-Hallucinations-in-Finance" class="headerlink" title="102. FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in Finance"></a>102. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FAITH__A_Framework_for_Assessing_Intrinsic_Tabular_Hallucinations_in_finance.pdf">FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in Finance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Asian Institute of Digital Finance, National University of Singapore</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ç”¨äºé‡‘èé¢†åŸŸè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å†…åœ¨å¹»è§‰çš„æ–°æ¡†æ¶ï¼ŒåŒ…æ‹¬è‡ªåŠ¨åŒ–æ•°æ®é›†æ„å»ºã€åŸºäºçœŸå®å¹´æŠ¥çš„é‡‘èå¹»è§‰åŸºå‡†æ•°æ®é›†ï¼Œä»¥åŠå››ç±»é‡‘èæ¨ç†å¤æ‚åº¦çš„åˆ†ç±»æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸»æµLLMåœ¨å¤æ‚é‡‘èè¡¨æ ¼æ¨ç†ä»»åŠ¡ä¸­ä¾ç„¶å­˜åœ¨è¾ƒé«˜å¹»è§‰ç‡ï¼Œå‡†ç¡®æ€§éšæ¨ç†å¤æ‚åº¦ä¸‹é™ï¼Œè¡¨æ˜å½“å‰æ¨¡å‹åœ¨é«˜ç²¾åº¦é‡‘èåº”ç”¨éƒ¨ç½²ä¸Šä»æœ‰æ˜¾è‘—æŒ‘æˆ˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FAITH_A_Framework_for_Assessing_Intrinsic_Tabular_Hallucinations_in_Finance.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="103-Incident-Response-Planning-Using-a-Lightweight-Large-Language-Model-with-Reduced-Hallucination"><a href="#103-Incident-Response-Planning-Using-a-Lightweight-Large-Language-Model-with-Reduced-Hallucination" class="headerlink" title="103. Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination"></a>103. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Incident_Response_Planning_Using_a_Lightweight_Large_Language_Model_with_Reduced_Hallucination.pdf">Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Melbourne</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é›†æˆè½»é‡çº§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºç½‘ç»œå®‰å…¨äº‹ä»¶å“åº”çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æŒ‡ä»¤å¾®è°ƒã€ä¿¡æ¯æ£€ç´¢å’Œå†³ç­–è§„åˆ’ä¸‰æ­¥ã€‚è¯¥æ–¹æ³•é€šè¿‡ç†è®ºè¯æ˜æœ‰æ•ˆå‡å°‘å¹»è§‰æ¦‚ç‡ï¼Œå¹¶åœ¨å¤šæ•°æ®é›†ä¸Šå®éªŒæ˜¾ç¤ºæ¯”å‰æ²¿LLMæ¢å¤æ—¶é—´ç¼©çŸ­æœ€é«˜22%ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºä¸”èµ„æºæ¶ˆè€—ä½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Incident_Response_Planning_Using_a_Lightweight_Large_Language_Model_with_Reduced_Hallucination.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="104-RLHF-Fine-Tuning-of-LLMs-for-Alignment-with-Implicit-User-Feedback-in-Conversational-Recommenders"><a href="#104-RLHF-Fine-Tuning-of-LLMs-for-Alignment-with-Implicit-User-Feedback-in-Conversational-Recommenders" class="headerlink" title="104. RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders"></a>104. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/RLHF_Fine-Tuning_of_LLMs_for_Alignment_with_Implicit_User_Feedback_in_Conversational_Recommenders.pdf">RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Northeastern University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å°†RLHFï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰ç”¨äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šè½®å¯¹è¯æ¨èç³»ç»Ÿä¸­çš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡éšå¼ç”¨æˆ·åé¦ˆï¼ˆå¦‚åœç•™æ—¶é—´ã€æƒ…æ„Ÿå˜åŒ–ã€è¯­ä¹‰ä¸€è‡´æ€§ï¼‰æ„å»ºå¥–åŠ±æ¨¡å‹ï¼Œå¹¶ç”¨PPOä¼˜åŒ–LLMç”Ÿæˆçš„æ¨èç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨REDIALå’ŒOpenDialKGç­‰æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ¨èå‡†ç¡®ç‡ã€å¯¹è¯æµç•…æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ï¼ŒéªŒè¯äº†éšå¼ä¿¡å·æŒ‡å¯¼ä¸‹çš„è‡ªé€‚åº”æ¨èèƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/RLHF_Fine-Tuning_of_LLMs_for_Alignment_with_Implicit_User_Feedback_in_Conversational_Recommenders.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="105-Understanding-and-Mitigating-Errors-of-LLM-Generated-RTL-Code"><a href="#105-Understanding-and-Mitigating-Errors-of-LLM-Generated-RTL-Code" class="headerlink" title="105. Understanding and Mitigating Errors of LLM-Generated RTL Code"></a>105. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Understanding_and_Mitigating_Errors_of_LLM-Generated_RTL_Code.pdf">Understanding and Mitigating Errors of LLM-Generated RTL Code</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute of Computing Technology, Chinese Academy of Sciences</span></p>
<p>æœ¬è®ºæ–‡ç³»ç»Ÿåˆ†æLLMç”Ÿæˆçš„RTLä»£ç é”™è¯¯ç±»å‹ï¼Œå‘ç°ä¸»è¦é—®é¢˜ä¸ºç¼ºä¹RTLç¼–ç¨‹çŸ¥è¯†ã€ç†è§£ç”µè·¯æ¦‚å¿µä¸è¶³ã€æè¿°æ­§ä¹‰å’Œå¤šæ¨¡æ€è¾“å…¥è¯¯è§£ã€‚ä½œè€…æå‡ºRAGçŸ¥è¯†è¡¥å……ã€è§„åˆ™æè¿°ä¿®æ­£ã€å¤šæ¨¡æ€è½¬æ¢åŠä»¿çœŸè°ƒè¯•ç­‰çº é”™æœºåˆ¶ï¼Œå¹¶é›†æˆè‡³ä»£ç ç”Ÿæˆæµç¨‹ï¼Œåœ¨VerilogEvalåŸºå‡†ä¸Šå‡†ç¡®ç‡è¾¾91%ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Understanding_and_Mitigating_Errors_of_LLM-Generated_RTL_Code.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="106-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs"><a href="#106-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs" class="headerlink" title="106. MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs"></a>106. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MoBE__Mixture-of-Basis-Experts_for_Compressing_MoE-based_LLMs.pdf">MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Inclusion AI</span></p>
<p>è¯¥è®ºæ–‡æå‡ºMixture-of-Basis-Experts (MoBE) æ–¹æ³•ï¼Œé€šè¿‡å°†ä¸“å®¶æ¨¡å—ä¸­çš„æƒé‡çŸ©é˜µåˆ†è§£ä¸ºä¸“å®¶ç‰¹æœ‰çš„å˜æ¢çŸ©é˜µå’Œå…±äº«çš„åŸºçŸ©é˜µçº¿æ€§ç»„åˆï¼Œå®ç°å¤§è§„æ¨¡MoEè¯­è¨€æ¨¡å‹çš„é«˜æ•ˆå‚æ•°å‹ç¼©ã€‚å®éªŒè¡¨æ˜ï¼ŒMoBEåœ¨ä¸»æµMoEæ¨¡å‹ä¸Šèƒ½å‡å°‘24%-30%çš„å‚æ•°ï¼Œä»…å¸¦æ¥1%-2%çš„å‡†ç¡®ç‡ä¸‹é™ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰å‹ç¼©æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MoBE_Mixture-of-Basis-Experts_for_Compressing_MoE-based_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="107-Pruning-Large-Language-Models-by-Identifying-and-Preserving-Functional-Networks"><a href="#107-Pruning-Large-Language-Models-by-Identifying-and-Preserving-Functional-Networks" class="headerlink" title="107. Pruning Large Language Models by Identifying and Preserving Functional Networks"></a>107. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Pruning_Large_Language_Models_by_Identifying_and_Preserving_Functional_Networks.pdf">Pruning Large Language Models by Identifying and Preserving Functional Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Automation, Northwestern Polytechnical University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å—ç¥ç»ç§‘å­¦å¯å‘çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“æ„åŒ–å‰ªææ–°æ–¹æ³•ï¼Œé€šè¿‡ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰åˆ†è§£æ¯å±‚ç¥ç»å…ƒä¿¡å·ï¼Œè¯†åˆ«å¹¶ä¿ç•™LLMä¸­çš„åŠŸèƒ½ç½‘ç»œï¼Œå®ç°é«˜æ•ˆå‰ªæã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å’Œå‰ªæç‡ä¸‹å‡ä¼˜äºç°æœ‰ä¸»æµå‰ªææ–¹æ³•ï¼Œæœ‰æ•ˆé™ä½è®¡ç®—å’Œå†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Pruning_Large_Language_Models_by_Identifying_and_Preserving_Functional_Networks.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="108-Cross-LoRA-A-Data-Free-LoRA-Transfer-Framework-across-Heterogeneous-LLMs"><a href="#108-Cross-LoRA-A-Data-Free-LoRA-Transfer-Framework-across-Heterogeneous-LLMs" class="headerlink" title="108. Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs"></a>108. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Cross-LoRA__A_Data-Free_LoRA_Transfer_Framework_across_Heterogeneous_LLMs.pdf">Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Baidu Inc</span></p>
<p>æœ¬æ–‡æå‡ºCross-LoRAï¼Œä¸€ç§æ— éœ€æ•°æ®å’Œè®­ç»ƒå³å¯å°†LoRAé€‚é…å™¨åœ¨ä¸åŒå¤§è¯­è¨€æ¨¡å‹é—´è¿ç§»çš„æ¡†æ¶ã€‚æ–¹æ³•åŒ…æ‹¬é€šè¿‡æˆªæ–­å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å’ŒFrobeniusæœ€ä¼˜çº¿æ€§å˜æ¢å¯¹æºã€ç›®æ ‡æ¨¡å‹è¿›è¡Œå­ç©ºé—´å¯¹é½ï¼ˆLoRA-Alignï¼‰ï¼Œç„¶åå°†æºLoRAæƒé‡æ›´æ–°æŠ•å½±åˆ°ç›®æ ‡æ¨¡å‹å‚æ•°ç©ºé—´ï¼ˆLoRA-Shiftï¼‰ï¼Œå®ç°æ¶æ„æ— å…³çš„è¿ç§»ã€‚å®éªŒæ˜¾ç¤ºCross-LoRAåœ¨å¤šä¸ªNLPæ¨ç†åŸºå‡†ä¸Šå®ç°äº†å¯¹åŸºçº¿æ¨¡å‹çš„ç¨³å®šæå‡ï¼Œæ€§èƒ½æ¥è¿‘äºç›´æ¥è®­ç»ƒçš„LoRAé€‚é…å™¨ï¼Œä¸”åœ¨å•å¡20åˆ†é’Ÿå†…å®Œæˆè¿ç§»ï¼Œé€‚åˆä½èµ„æºå’Œå®é™…åº”ç”¨åœºæ™¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Cross-LoRA_A_Data-Free_LoRA_Transfer_Framework_across_Heterogeneous_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="109-SPEX-A-Vision-Language-Model-for-Land-Cover-Extraction-on-Spectral-Remote-Sensing-Images"><a href="#109-SPEX-A-Vision-Language-Model-for-Land-Cover-Extraction-on-Spectral-Remote-Sensing-Images" class="headerlink" title="109. SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images"></a>109. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SPEX__A_Vision-Language_Model_for_Land_Cover_Extraction_on_Spectral_Remote_Sensing_Images.pdf">SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Xinjiang University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†SPEXæ¨¡å‹ï¼Œé€šè¿‡èåˆå¤šå°ºåº¦è§†è§‰ç‰¹å¾èšåˆã€Tokenä¸Šä¸‹æ–‡å‹ç¼©å’Œå¤šå…‰è°±è§†è§‰é¢„è®­ç»ƒï¼Œå®ç°äº†åŸºäºæŒ‡ä»¤çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç”¨äºé¥æ„Ÿå¤šå…‰è°±å½±åƒçš„åœ°è¡¨è¦†ç›–ç‰©æå–ã€‚æ„å»ºäº†SPIEæ•°æ®é›†ï¼Œå°†åœ°ç‰©å…‰è°±å…ˆéªŒç¼–ç åˆ°å¯è¢«LLMè¯†åˆ«çš„æ–‡æœ¬å±æ€§ï¼Œæ˜¾è‘—æå‡äº†å¯¹æ¤è¢«ã€æ°´ä½“ã€å»ºç­‘ç­‰ç±»åˆ«çš„åƒç´ çº§è¯†åˆ«ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPEXåœ¨äº”ä¸ªå…¬å¼€å¤šå…‰è°±æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”ç”Ÿæˆç»“æœæ›´å…·è§£é‡Šæ€§ä¸äº¤äº’æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SPEX_A_Vision-Language_Model_for_Land_Cover_Extraction_on_Spectral_Remote_Sensing_Images.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="110-EvoGraph-Hybrid-Directed-Graph-Evolution-toward-Software-3-0"><a href="#110-EvoGraph-Hybrid-Directed-Graph-Evolution-toward-Software-3-0" class="headerlink" title="110. EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0"></a>110. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EvoGraph__Hybrid_Directed_Graph_Evolution_toward_Software_3.0.pdf">EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">AutoHand AI</span></p>
<p>EvoGraphæå‡ºäº†ä¸€ä¸ªåˆ©ç”¨ä¸“é—¨å°è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰é©±åŠ¨çš„æ··åˆå®šå‘å›¾è¿›åŒ–æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨æ¼”åŒ–ä¼ä¸šè½¯ä»¶ç³»ç»Ÿçš„ä»£ç ã€æ–‡æ¡£ã€æ„å»ºæµç¨‹å’Œå·¥å•ç­‰å…¨éƒ¨è½¯ä»¶åˆ¶å“ã€‚å…¶æ–¹æ³•åŒ…æ‹¬åŸºäºæœ‰ç±»å‹å®šå‘å›¾çš„ç»Ÿä¸€è¡¨å¾ã€å¤šç§SLMå¼•å¯¼çš„å˜å¼‚ç®—å­ã€å¸¦å®‰å…¨çº¦æŸçš„å¤šç›®æ ‡è¿›åŒ–é€‰æ‹©ï¼Œå¹¶åœ¨å¤šè¯­è¨€ç°ä»£åŒ–ï¼ˆCOBOLã€.NETã€Lispç­‰ï¼‰ä»»åŠ¡ä¸­å®ç°äº†82-96%è¯­ä¹‰ç­‰ä»·å’Œ90%ç®—åŠ›èŠ‚çœã€‚ç»“è®ºæ˜¾ç¤ºï¼ŒEvoGraphæœ‰æ•ˆæ”¯æŒä¼ä¸šçº§é—ç•™ç³»ç»Ÿç°ä»£åŒ–ï¼ŒSLMä¼˜äºå¤§æ¨¡å‹å¹¶ç»æµå®ç”¨ï¼Œæ¨è¿›äº†è‡ªä¸»æ¼”åŒ–çš„è½¯ä»¶3.0è·¯å¾„ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EvoGraph_Hybrid_Directed_Graph_Evolution_toward_Software_3.0.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="111-QA-Dragon-Query-Aware-Dynamic-RAG-System-for-Knowledge-Intensive-Visual-Question-Answering"><a href="#111-QA-Dragon-Query-Aware-Dynamic-RAG-System-for-Knowledge-Intensive-Visual-Question-Answering" class="headerlink" title="111. QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering"></a>111. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/QA-Dragon__Query-Aware_Dynamic_RAG_System_for_Knowledge-Intensive_Visual_Question_Answering.pdf">QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Hong Kong Polytechnic University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†QA-Dragonç³»ç»Ÿï¼Œé€šè¿‡å¼•å…¥é¢†åŸŸè·¯ç”±å™¨ã€æœç´¢è·¯ç”±å™¨ã€å·¥å…·è·¯ç”±å™¨ä»¥åŠå¤šé˜¶æ®µé‡æ’åºæœºåˆ¶ï¼Œå®ç°äº†åŠ¨æ€å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä»¥æå‡çŸ¥è¯†å¯†é›†å‹è§†è§‰é—®ç­”ï¼ˆVQAï¼‰çš„å‡†ç¡®æ€§ã€‚å…¶æ–¹æ³•ç»“åˆäº†å›¾åƒå’Œæ–‡æœ¬çš„æ··åˆæ£€ç´¢ã€é¢†åŸŸæ„ŸçŸ¥æ¨ç†å’Œå¤šè½®å¤šè·³æ¨ç†ï¼Œæœ‰æ•ˆæé«˜äº†å¤æ‚VQAä»»åŠ¡ä¸­çš„ç­”æ¡ˆå‡†ç¡®ç‡å’ŒçŸ¥è¯†é‡å åº¦ï¼Œåœ¨KDD Cup 2025 Meta CRAG-MM Challengeä¸­å…¨é¢è¶…è¶Šç°æœ‰å¼ºåŸºçº¿æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/QA-Dragon_Query-Aware_Dynamic_RAG_System_for_Knowledge-Intensive_Visual_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="112-Attention-Basin-Why-Contextual-Position-Matters-in-Large-Language-Models"><a href="#112-Attention-Basin-Why-Contextual-Position-Matters-in-Large-Language-Models" class="headerlink" title="112. Attention Basin: Why Contextual Position Matters in Large Language Models"></a>112. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Attention_Basin__Why_Contextual_Position_Matters_in_Large_Language_Models.pdf">Attention Basin: Why Contextual Position Matters in Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Sun Yat-sen University</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿæ­ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ç»“æ„åŒ–è¾“å…¥æ—¶å­˜åœ¨â€œattention basinâ€ç°è±¡ï¼Œå³æ¨¡å‹ä¼šå¯¹è¾“å…¥åºåˆ—å¼€å¤´å’Œç»“å°¾çš„ä¿¡æ¯åˆ†é…æ›´é«˜å…³æ³¨ï¼Œä¸­é—´éƒ¨åˆ†åˆ™è¢«å¿½è§†ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€æ¨¡å‹æ— å…³çš„Attention-Driven Rerankingï¼ˆAttnRankï¼‰æ–¹æ³•ï¼Œé€šè¿‡å°‘é‡æ ¡å‡†é›†ä¼°ç®—æ¨¡å‹å›ºæœ‰çš„æ³¨æ„åŠ›ä½ç½®åå¥½ï¼Œå¹¶æ®æ­¤é‡æ’å…³é”®å†…å®¹ï¼Œä½¿å…¶ä½äºæ¨¡å‹å…³æ³¨åº¦é«˜çš„ä½ç½®ã€‚å®éªŒè¡¨æ˜ï¼ŒAttnRankåœ¨å¤šè·³é—®ç­”å’Œfew-shotå­¦ä¹ ç­‰ä»»åŠ¡ä¸­ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ä¾¿èƒ½æ˜¾è‘—æå‡10ç§ä¸»æµLLMçš„è¡¨ç°ã€‚ç»“è®ºï¼šAttnRanké«˜æ•ˆç¼“è§£äº†ä½ç½®åå·®ï¼Œæé«˜äº†ä¿¡æ¯åˆ©ç”¨ç‡ï¼Œä¸”ä¸ç°æœ‰æ¨ç†åŠ é€Ÿæ¡†æ¶å…¼å®¹ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Attention_Basin_Why_Contextual_Position_Matters_in_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="113-Beyond-Automation-Socratic-AI-Epistemic-Agency-and-the-Implications-of-the-Emergence-of-Orchestrated-Multi-Agent-Learning-Architectures"><a href="#113-Beyond-Automation-Socratic-AI-Epistemic-Agency-and-the-Implications-of-the-Emergence-of-Orchestrated-Multi-Agent-Learning-Architectures" class="headerlink" title="113. Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures"></a>113. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Beyond_Automation__Socratic_AI,_Epistemic_Agency,_and_the_Implications_of_the_Emergence_of_Orchestra.pdf">Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Kassel</span></p>
<p>è¯¥è®ºæ–‡æå‡ºå¹¶å®è¯éªŒè¯äº†ä¸€ç§åŸºäºSocraticæ–¹æ³•çš„å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹AIå¯¼å¸ˆï¼Œé€šè¿‡ç»“æ„åŒ–æé—®ä¿ƒè¿›å­¦ç”Ÿæ‰¹åˆ¤æ€§ã€ç‹¬ç«‹æ€§å’Œåæ€æ€§æ€ç»´ï¼Œå¹¶ä¸é€šç”¨AIèŠå¤©æœºå™¨äººè¿›è¡Œäº†å¯¹æ¯”å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼ŒSocratic AIæ˜¾è‘—æå‡äº†å­¦ç”Ÿçš„å…ƒè®¤çŸ¥å‚ä¸å’Œé«˜é˜¶æ€ç»´ï¼Œæ¨åŠ¨äº†AIæ•™è‚²ä»å•ä¸€å·¥å…·å‘å¤šæ™ºèƒ½ä½“åä½œç”Ÿæ€ç³»ç»Ÿè½¬å˜ï¼Œå¹¶å¯¹é«˜ç­‰æ•™è‚²çš„æ•™å¸ˆè§’è‰²ã€è¯¾ç¨‹è®¾è®¡ã€åŸºç¡€è®¾æ–½åŠè¯„ä¼°ä½“ç³»æå‡ºäº†ç³»ç»Ÿæ€§å»ºè®®ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Beyond_Automation_Socratic_AI%2C_Epistemic_Agency%2C_and_the_Implications_of_the_Emergence_of_Orchestrated_Multi-Agent_Learning_Architectures.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="114-Align-Donâ€™t-Divide-Revisiting-the-LoRA-Architecture-in-Multi-Task-Learning"><a href="#114-Align-Donâ€™t-Divide-Revisiting-the-LoRA-Architecture-in-Multi-Task-Learning" class="headerlink" title="114. Align, Donâ€™t Divide: Revisiting the LoRA Architecture in Multi-Task Learning"></a>114. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Align,_Don't_Divide__Revisiting_the_LoRA_Architecture_in_Multi-Task_Learning.pdf">Align, Donâ€™t Divide: Revisiting the LoRA Architecture in Multi-Task Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Jilin University</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•LoRAåœ¨å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰ä¸­çš„ä¸»æµå¤šå¤´&#x2F;å¤šé€‚é…å™¨ç»“æ„ï¼Œå‘ç°é«˜ç»“æ„å¤æ‚åº¦å’Œä»»åŠ¡ç‰¹å¼‚æ€§åˆ†ç¦»å¹¶æœªå¸¦æ¥æ›´ä¼˜æ³›åŒ–æ€§èƒ½ï¼Œåè€Œç®€å•å•å¤´é«˜ç§©LoRAè¡¨ç°æ›´ä¼˜ã€‚åŸºäºæ­¤ï¼Œä½œè€…æå‡ºAlign-LoRAï¼Œé€šè¿‡åœ¨è®­ç»ƒç›®æ ‡ä¸­å¼•å…¥KLæˆ–MMDå¯¹é½æŸå¤±ï¼Œæ˜¾å¼å¢å¼ºä»»åŠ¡é—´å…±äº«è¡¨å¾ï¼Œæ— éœ€å¢åŠ æ¨ç†å¼€é”€ä¸”è·å¾—äº†æ˜¾è‘—ä¼˜äºå¤æ‚ç»“æ„çš„å¤šä»»åŠ¡æ³›åŒ–æ•ˆæœã€‚ç»“è®ºï¼šå¯¹é½è¡¨å¾ã€å¼ºåŒ–ä»»åŠ¡å…±äº«çŸ¥è¯†æ¯”ç»“æ„åˆ†ç¦»æ›´å…³é”®ï¼ŒAlign-LoRAä¸ºå¤šä»»åŠ¡PEFTæä¾›äº†æ›´é«˜æ•ˆæ–°èŒƒå¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Align%2C_Don%27t_Divide_Revisiting_the_LoRA_Architecture_in_Multi-Task_Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="115-Aligning-LLMs-on-a-Budget-Inference-Time-Alignment-with-Heuristic-Reward-Models"><a href="#115-Aligning-LLMs-on-a-Budget-Inference-Time-Alignment-with-Heuristic-Reward-Models" class="headerlink" title="115. Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models"></a>115. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Aligning_LLMs_on_a_Budget__Inference-Time_Alignment_with_Heuristic_Reward_Models.pdf">Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Massachusetts Amherst</span></p>
<p>è¯¥è®ºæ–‡æå‡ºHIAï¼ˆHeuristic-Guided Inference-time Alignmentï¼‰æ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§å¯å‘å¼å¥–åŠ±æ¨¡å‹å’Œä¸¤é˜¶æ®µç­›é€‰æœºåˆ¶ï¼Œåœ¨ä¸éœ€å¾®è°ƒå’Œä»…éœ€é»‘ç›’è®¿é—®çš„å‰æä¸‹ï¼Œå®ç°å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ—¶çš„å¤šç›®æ ‡ã€ä¸ªæ€§åŒ–å¯¹é½ã€‚å®éªŒè¯æ˜ï¼Œåœ¨HELPSTEERå’ŒCOMPREDç­‰çœŸå®æ•°æ®é›†ä¸Šï¼ŒHIAåœ¨ç›¸åŒæ¨ç†é¢„ç®—ä¸‹ä¼˜äºBoNé‡‡æ ·ã€æŸæœç´¢å’Œè´ªå¿ƒæœç´¢ï¼Œå°¤å…¶åœ¨æ¨ç†é¢„ç®—ä½è‡³1-2æ¬¡æŸ¥è¯¢æ—¶æå‡æ˜¾è‘—ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Aligning_LLMs_on_a_Budget_Inference-Time_Alignment_with_Heuristic_Reward_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="116-Speech-LLMs-in-Low-Resource-Scenarios-Data-Volume-Requirements-and-the-Impact-of-Pretraining-on-High-Resource-Languages"><a href="#116-Speech-LLMs-in-Low-Resource-Scenarios-Data-Volume-Requirements-and-the-Impact-of-Pretraining-on-High-Resource-Languages" class="headerlink" title="116. Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages"></a>116. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Speech_LLMs_in_Low-Resource_Scenarios__Data_Volume_Requirements_and_the_Impact_of_Pretraining_on_Hig.pdf">Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Trento</span></p>
<p>è¯¥è®ºæ–‡åŸºäºSLAM-ASRæ¡†æ¶ï¼Œç ”ç©¶äº†åœ¨ä½èµ„æºè¯­éŸ³è¯†åˆ«åœºæ™¯ä¸­ï¼Œè®­ç»ƒæ•°æ®é‡åŠé«˜èµ„æºè¯­è¨€é¢„è®­ç»ƒå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ä½œè€…é€šè¿‡å°†Whisper-large-v3-turboè¯­éŸ³ç¼–ç å™¨ã€è½»é‡çº¿æ€§æŠ•å½±å™¨å’Œå¤šè¯­ç§LLMï¼ˆEuroLLM&#x2F;Salamandraï¼‰ç»“åˆï¼Œå®éªŒè¯æ˜ï¼šéœ€100-200å°æ—¶è®­ç»ƒæ•°æ®æ‰èƒ½è¾¾åˆ°Whisper-onlyæ¨¡å‹æ•ˆæœï¼Œä¸”æŠ•å½±å™¨åœ¨é«˜èµ„æºè¯­è¨€ä¸Šé¢„è®­ç»ƒå¹¶è¿ç§»åˆ°ä½èµ„æºè¯­è¨€ï¼Œå°¤å…¶æ˜¯åœ¨ä»…æœ‰10-15å°æ—¶å¾®è°ƒæ•°æ®æ—¶ï¼Œèƒ½æ˜¾è‘—æå‡è¯†åˆ«å‡†ç¡®ç‡ï¼Œå¤šè¯­ç§æŠ•å½±å™¨è¿›ä¸€æ­¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚ç»“è®ºæŒ‡å‡ºï¼Œé¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥ä¸ºä½èµ„æºè¯­ç§è¯­éŸ³è¯†åˆ«æä¾›äº†ä¼˜åŒ–æ–¹å‘ï¼Œä½†å¯¹æ•°æ®é‡å’Œè·¨é¢†åŸŸæ³›åŒ–ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Speech_LLMs_in_Low-Resource_Scenarios_Data_Volume_Requirements_and_the_Impact_of_Pretraining_on_High-Resource_Languages.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="117-Towards-Assessing-Medical-Ethics-from-Knowledge-to-Practice"><a href="#117-Towards-Assessing-Medical-Ethics-from-Knowledge-to-Practice" class="headerlink" title="117. Towards Assessing Medical Ethics from Knowledge to Practice"></a>117. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Assessing_Medical_Ethics_from_Knowledge_to_Practice.pdf">Towards Assessing Medical Ethics from Knowledge to Practice</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Chinese University of Hong Kong, Shenzhen</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†PrinciplismQAåŸºå‡†ï¼Œç”¨äºç³»ç»Ÿæ€§è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—ä¼¦ç†é¢†åŸŸçš„çŸ¥è¯†æŒæ¡ä¸å®é™…åº”ç”¨èƒ½åŠ›ï¼Œç»“åˆå¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾å¼æ¡ˆä¾‹åˆ†æï¼Œé‡‡ç”¨ä¸“å®¶æ ¡éªŒä¸LLMè‡ªåŠ¨è¯„åˆ†æµç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹å‡å­˜åœ¨â€œçŸ¥è¯†-å®è·µé¸¿æ²Ÿâ€ï¼Œå°¤å…¶åœ¨å®é™…æƒ…å¢ƒåŠ¨æ€åº”ç”¨ä¼¦ç†åŸåˆ™ï¼ˆå¦‚ä»æ…ˆæ€§ï¼‰æ–¹é¢è¡¨ç°è¾ƒå¼±ï¼ŒåŒ»ç–—é¢†åŸŸå¾®è°ƒè™½æå‡å®é™…ä¼¦ç†è¡¨ç°ä½†ç•¥æœ‰çŸ¥è¯†é—å¿˜ï¼Œæœªæ¥éœ€å¼ºåŒ–æ¨¡å‹ä¼¦ç†å¯¹é½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Assessing_Medical_Ethics_from_Knowledge_to_Practice.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="118-Exploring-Superior-Function-Calls-via-Reinforcement-Learning"><a href="#118-Exploring-Superior-Function-Calls-via-Reinforcement-Learning" class="headerlink" title="118. Exploring Superior Function Calls via Reinforcement Learning"></a>118. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Exploring_Superior_Function_Calls_via_Reinforcement_Learning.pdf">Exploring Superior Function Calls via Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">AWorld Team, Inclusion AI</span></p>
<p>æœ¬æ–‡æå‡ºäº†FunRLï¼Œä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹å‡½æ•°è°ƒç”¨ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚æ–¹æ³•åˆ›æ–°æ€§åœ°å°†Chain-of-Thoughtï¼ˆCoTï¼‰ç†µçº³å…¥GRPOä¼˜åŠ¿ä¼°è®¡ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæ•°æ®å‡†å¤‡ï¼ˆLLMå’ŒASTè¯„ä¼°ï¼‰ä¸ç²¾ç¡®çš„äºŒå…ƒå¥–åŠ±å‡½æ•°ï¼Œä¿ƒè¿›æ¨¡å‹åœ¨æ¨ç†å’Œå‚æ•°éªŒè¯ä¸Šæ¢ç´¢å¤šæ ·åŒ–æ€è·¯ã€‚å®éªŒè¡¨æ˜ï¼ŒFunRLåœ¨BFCLv2åŸºå‡†ä¸Šå®ç°äº†å¼€æºæ¨¡å‹æœ€ä¼˜çš„86.02%æ€»ä½“å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºæ ‡å‡†GRPOï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç é¢„è®­ç»ƒæ¨¡å‹ä¸Šæå‡æ•ˆæœæ˜æ˜¾ã€‚ç»“è®ºï¼šFunRLæ˜¾è‘—æå‡äº†LLMå·¥å…·è°ƒç”¨çš„å¤æ‚æ¨ç†èƒ½åŠ›å’Œå®é™…åº”ç”¨å¯é æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Exploring_Superior_Function_Calls_via_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="119-EasySize-Elastic-Analog-Circuit-Sizing-via-LLM-Guided-Heuristic-Search"><a href="#119-EasySize-Elastic-Analog-Circuit-Sizing-via-LLM-Guided-Heuristic-Search" class="headerlink" title="119. EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search"></a>119. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EasySize__Elastic_Analog_Circuit_Sizing_via_LLM-Guided_Heuristic_Search.pdf">EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Jiao Tong University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºEasySizeï¼Œä¸€ç§åŸºäºå¾®è°ƒQwen3-8Bæ¨¡å‹çš„è½»é‡åŒ–æ¨¡æ‹Ÿç”µè·¯å°ºå¯¸è°ƒæ•´æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹ç”ŸæˆåŠ¨æ€æŸå¤±å‡½æ•°å’Œå¯å‘å¼æœç´¢ï¼ˆDEä¸PSOï¼‰ï¼Œå®ç°è·¨å·¥è‰ºèŠ‚ç‚¹å’Œç”µè·¯æ‹“æ‰‘çš„é€šç”¨é«˜æ•ˆä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜EasySizeåœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æ€§èƒ½ä¼˜äºAutoCktï¼Œå¹¶å¤§å¹…å‡å°‘ä»¿çœŸèµ„æºæ¶ˆè€—ï¼Œæ˜¾è‘—æå‡æ¨¡æ‹Ÿç”µè·¯è®¾è®¡è‡ªåŠ¨åŒ–çš„æ•ˆç‡ä¸é€‚åº”æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EasySize_Elastic_Analog_Circuit_Sizing_via_LLM-Guided_Heuristic_Search.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="120-JPS-Jailbreak-Multimodal-Large-Language-Models-with-Collaborative-Visual-Perturbation-and-Textual-Steering"><a href="#120-JPS-Jailbreak-Multimodal-Large-Language-Models-with-Collaborative-Visual-Perturbation-and-Textual-Steering" class="headerlink" title="120. JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering"></a>120. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/JPS__Jailbreak_Multimodal_Large_Language_Models_with_Collaborative_Visual_Perturbation_and_Textual_S.pdf">JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tsinghua University</span></p>
<p>æœ¬æ–‡æå‡ºJPSæ–¹æ³•ï¼Œé€šè¿‡ååŒä¼˜åŒ–è§†è§‰æ‰°åŠ¨ä¸å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç”Ÿæˆçš„æ–‡æœ¬å¼•å¯¼ï¼Œå®ç°å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„é«˜è´¨é‡è¶Šç‹±æ”»å‡»ã€‚JPSå°†å®‰å…¨ç»•è¿‡ä»»åŠ¡åˆ†é…ç»™ç›®æ ‡å¼•å¯¼çš„å›¾åƒå¯¹æŠ—æ‰°åŠ¨ï¼Œå°†é«˜æ•ˆæ¶æ„æ„å›¾å®ç°äº¤ç”±å¤šæ™ºèƒ½ä½“ä¼˜åŒ–çš„å¼•å¯¼æç¤ºï¼Œä¸¤è€…è¿­ä»£ååŒæå‡æ”»å‡»æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJPSåœ¨å¤šç§æ¨¡å‹å’ŒåŸºå‡†ä¸Šï¼Œè¶Šç‹±æˆåŠŸç‡ï¼ˆASRï¼‰å’Œæ¶æ„æ„å›¾å®ç°ç‡ï¼ˆMIFRï¼‰å‡è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œæœ‰æ•ˆæå‡äº†å®é™…æ”»å‡»æ•ˆç”¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/JPS_Jailbreak_Multimodal_Large_Language_Models_with_Collaborative_Visual_Perturbation_and_Textual_Steering.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="121-MedMKEB-A-Comprehensive-Knowledge-Editing-Benchmark-for-Medical-Multimodal-Large-Language-Models"><a href="#121-MedMKEB-A-Comprehensive-Knowledge-Editing-Benchmark-for-Medical-Multimodal-Large-Language-Models" class="headerlink" title="121. MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models"></a>121. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MedMKEB__A_Comprehensive_Knowledge_Editing_Benchmark_for_Medical_Multimodal_Large_Language_Models.pdf">MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Peking University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºMedMKEBï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºåŒ»å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è®¾è®¡çš„çŸ¥è¯†ç¼–è¾‘ç»¼åˆåŸºå‡†ï¼Œæ¶µç›–è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„ç³»ç»Ÿæ€§ç¼–è¾‘è¯„ä¼°ï¼ŒåŒ…å«å¯é æ€§ã€å±€éƒ¨æ€§ã€æ³›åŒ–æ€§ã€å¯è¿ç§»æ€§å’Œé²æ£’æ€§äº”å¤§æŒ‡æ ‡ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰çŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨åŒ»å­¦å¤šæ¨¡æ€åœºæ™¯ä¸‹è¡¨ç°æœ‰é™ï¼Œéœ€è¦é’ˆå¯¹åŒ»å­¦é¢†åŸŸå¼€å‘æ›´ç²¾ç»†åŒ–çš„ç¼–è¾‘ç®—æ³•ï¼ŒMedMKEBå°†æ¨åŠ¨åŒ»å­¦çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯çš„è¿›æ­¥ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MedMKEB_A_Comprehensive_Knowledge_Editing_Benchmark_for_Medical_Multimodal_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="122-Finding-Needles-in-Images-Can-Multimodal-LLMs-Locate-Fine-Details"><a href="#122-Finding-Needles-in-Images-Can-Multimodal-LLMs-Locate-Fine-Details" class="headerlink" title="122. Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?"></a>122. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Finding_Needles_in_Images__Can_Multimodal_LLMs_Locate_Fine_Details_.pdf">Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Fujitsu Research India</span></p>
<p>æœ¬è®ºæ–‡æå‡ºNiM-Benchmarkï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚æ–‡æ¡£ä¸­å®šä½å’Œæ¨ç†ç»†ç²’åº¦ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶æå‡ºSpot-ITæ–¹æ³•ï¼šç»“åˆè¯­ä¹‰ç›¸ä¼¼æ€§åˆ†å—å’Œè‡ªé€‚åº”é«˜æ–¯æ³¨æ„åŠ›ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨å…³é”®åŒºåŸŸã€‚å®éªŒè¡¨æ˜ï¼ŒSpot-ITåœ¨å¤šä¸ªDocVQAæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†å¤æ‚å¸ƒå±€ä¸­æ–‡æ¡£ç»†èŠ‚æå–çš„å‡†ç¡®ç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Finding_Needles_in_Images_Can_Multimodal_LLMs_Locate_Fine_Details_.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="123-Evaluation-of-Large-Language-Model-Architectures-in-Abstract-Meaning-Representation-Parsing"><a href="#123-Evaluation-of-Large-Language-Model-Architectures-in-Abstract-Meaning-Representation-Parsing" class="headerlink" title="123. Evaluation of Large Language Model Architectures in Abstract Meaning Representation Parsing"></a>123. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Evaluation_of_LLMs_in_AMR_Parsing.pdf">Evaluation of Large Language Model Architectures in Abstract Meaning Representation Parsing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University College London</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†å››ç§å¼€æºè§£ç å™¨å¼å¤§è¯­è¨€æ¨¡å‹ï¼ˆPhi-3.5ã€Gemma-2ã€LLaMA-3.2ã€DeepSeek-R1-LLaMA-Distilledï¼‰åœ¨AMRè¯­ä¹‰è§£æä»»åŠ¡ä¸­çš„å¾®è°ƒè¡¨ç°ï¼Œé‡‡ç”¨LoRAé«˜æ•ˆå‚æ•°å¾®è°ƒå’ŒSMATCHå›¾å¯¹é½æŒ‡æ ‡ï¼Œåœ¨LDC2020T02 Gold AMR3.0ç­‰æ•°æ®é›†ä¸Šè¿›è¡Œå¯¹æ¯”åˆ†æã€‚ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡ç®€å•å¾®è°ƒåï¼ŒLLMæ¨¡å‹ï¼ˆå°¤å…¶æ˜¯LLaMA-3.2ï¼‰åœ¨è¯­ä¹‰è§£ææ€§èƒ½ä¸Šæ¥è¿‘ä¸“ç”¨SOTAè§£æå™¨ï¼ŒPhi-3.5åˆ™åœ¨ç»“æ„æœ‰æ•ˆæ€§ä¸Šè¡¨ç°æœ€ä¼˜ï¼Œè¯æ˜LLMæ— éœ€å¤æ‚æ¶æ„å³å¯é«˜æ•ˆå®ŒæˆAMRè§£æä»»åŠ¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Evaluation_of_Large_Language_Model_Architectures_in_Abstract_Meaning_Representation_Parsing.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="124-Skin-SOAP-A-Weakly-Supervised-Framework-for-Generating-Structured-SOAP-Notes"><a href="#124-Skin-SOAP-A-Weakly-Supervised-Framework-for-Generating-Structured-SOAP-Notes" class="headerlink" title="124. Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes"></a>124. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Skin-SOAP__A_Weakly_Supervised_Framework_for_Generating_Structured_SOAP_Notes.pdf">Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Maryland, Baltimore County</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†Skin-SOAPï¼Œä¸€ç§å¼±ç›‘ç£å¤šæ¨¡æ€æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆçš®æŸå›¾åƒå’Œç¨€ç–ä¸´åºŠæ–‡æœ¬ï¼Œåˆ©ç”¨ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸´åºŠæè¿°ã€æ£€ç´¢å¢å¼ºçŸ¥è¯†æ•´åˆï¼Œå¹¶å¯¹Vision-LLaMAæ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œå®ç°ç»“æ„åŒ–SOAPç—…å†è‡ªåŠ¨ç”Ÿæˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSkin-SOAPåœ¨ä¸´åºŠç›¸å…³æ€§å’Œç»“æ„ä¸€è‡´æ€§ä¸Šä¸æœ€å…ˆè¿›çš„LLMæ¨¡å‹ï¼ˆå¦‚GPT-4oç­‰ï¼‰æ€§èƒ½ç›¸å½“ï¼Œæœ‰åŠ©äºç¼“è§£åŒ»ç”Ÿæ–‡æ¡£è´Ÿæ‹…å¹¶æå‡çš®è‚¤ç§‘ä¸´åºŠæµç¨‹æ•ˆç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Skin-SOAP_A_Weakly_Supervised_Framework_for_Generating_Structured_SOAP_Notes.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="125-SPaRFT-Self-Paced-Reinforcement-Fine-Tuning-for-Large-Language-Models"><a href="#125-SPaRFT-Self-Paced-Reinforcement-Fine-Tuning-for-Large-Language-Models" class="headerlink" title="125. SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models"></a>125. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SPaRFT__Self-Paced_Reinforcement_Fine-Tuning_for_Large_Language_Models.pdf">SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Deakin University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºSPaRFTï¼Œä¸€ç§è‡ªé€‚åº”çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¡†æ¶ï¼Œé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒã€‚æ–¹æ³•é¦–å…ˆä½¿ç”¨è¯­ä¹‰å’Œéš¾åº¦ç»“åˆçš„èšç±»è¿›è¡Œæ•°æ®ç²¾ç®€ï¼Œå†ä»¥å¤šè‡‚èµŒåšæœºåŠ¨æ€åˆ†é…è®­ç»ƒæ ·æœ¬ï¼Œå®ç°æ€§èƒ½é©±åŠ¨çš„è¯¾ç¨‹å­¦ä¹ ã€‚å®éªŒæ˜¾ç¤ºï¼ŒSPaRFTåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ä»¥æå°‘æ ·æœ¬è¾¾åˆ°æˆ–è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡å°æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶èµ„æºæ¶ˆè€—æä½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SPaRFT_Self-Paced_Reinforcement_Fine-Tuning_for_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="126-Can-Large-Language-Models-Integrate-Spatial-Data-Empirical-Insights-into-Reasoning-Strengths-and-Computational-Weaknesses"><a href="#126-Can-Large-Language-Models-Integrate-Spatial-Data-Empirical-Insights-into-Reasoning-Strengths-and-Computational-Weaknesses" class="headerlink" title="126. Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses"></a>126. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Can_Large_Language_Models_Integrate_Spatial_Data__Empirical_Insights_into_Reasoning_Strengths_and_Co.pdf">Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Washington</span></p>
<p>æœ¬è®ºæ–‡ç³»ç»Ÿè¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŸå¸‚ç©ºé—´æ•°æ®æ•´åˆä¸­çš„èƒ½åŠ›ï¼Œæå‡ºé€šè¿‡å¼•å…¥å‡ ä½•ç‰¹å¾ï¼ˆå¦‚æœ€å°è§’åº¦ã€æœ€å°è·ç¦»ã€æœ€å¤§é‡å é¢ç§¯ï¼‰æ˜¾è‘—æå‡LLMåœ¨ç©ºé—´æ•°æ®åŒ¹é…ã€èåˆç­‰ä»»åŠ¡çš„è¡¨ç°ã€‚å®éªŒå‘ç°ï¼ŒLLMéš¾ä»¥ä»…å‡­è‡ªç„¶è¯­è¨€æŒ‡ä»¤å®Œæˆç©ºé—´æ¨ç†ï¼Œä½†è‹¥ç»™å®šç›¸å…³ç‰¹å¾ï¼Œåˆ™å¯æ¨æ–­å‡ºåˆç†é˜ˆå€¼ï¼Œå‡†ç¡®ç‡æœ€é«˜å¯è¾¾99.5%ï¼Œæå‡ºçš„â€œreview-and-refineâ€ä¸¤æ­¥è‡ªæˆ‘ä¿®æ­£æ–¹æ³•è¿›ä¸€æ­¥æå‡äº†ç»“æœçš„å¯é æ€§ã€‚ç»“è®ºè®¤ä¸ºï¼ŒLLMèƒ½æœ‰æ•ˆè¾…åŠ©ç©ºé—´æ•°æ®é›†æˆï¼Œä½†å°šä¸é€‚åˆå¤æ‚è®¡ç®—å‡ ä½•ä»»åŠ¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Can_Large_Language_Models_Integrate_Spatial_Data_Empirical_Insights_into_Reasoning_Strengths_and_Computational_Weaknesses.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="127-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data"><a href="#127-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data" class="headerlink" title="127. R-Zero: Self-Evolving Reasoning LLM from Zero Data"></a>127. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/R-Zero__Self-Evolving_Reasoning_LLM_from_Zero_Data.pdf">R-Zero: Self-Evolving Reasoning LLM from Zero Data</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tencent AI Seattle Lab</span></p>
<p>æœ¬æ–‡æå‡ºäº†R-Zeroæ¡†æ¶ï¼Œå®ç°äº†æ— éœ€ä»»ä½•å¤–éƒ¨æ•°æ®ã€å®Œå…¨è‡ªä¸»è¿›åŒ–çš„æ¨ç†å‹å¤§è¯­è¨€æ¨¡å‹ã€‚æ–¹æ³•é‡‡ç”¨åŒæ¨¡å‹ååŒè¿›åŒ–æœºåˆ¶ï¼ˆChallengerå’ŒSolverï¼‰ï¼Œé€šè¿‡Group Relative Policy Optimizationï¼ˆGRPOï¼‰å®ç°æ¨¡å‹è‡ªæˆ‘ç”Ÿæˆã€è¿‡æ»¤ä¸è§£ç­”éš¾é¢˜çš„å¾ªç¯ï¼Œå®Œå…¨æ‘†è„±äººå·¥æ ‡æ³¨ä¸ç°æœ‰ä»»åŠ¡ä¾èµ–ã€‚å®éªŒè¯æ˜ï¼ŒR-Zeroèƒ½æ˜¾è‘—æå‡ä¸åŒåŸºç¡€LLMåœ¨æ•°å­¦åŠé€šç”¨æ¨ç†é¢†åŸŸçš„èƒ½åŠ›ï¼Œå¹¶å¯ä½œä¸ºæœ‰ç›‘ç£å¾®è°ƒçš„æœ‰æ•ˆå¢å¼ºæ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/R-Zero_Self-Evolving_Reasoning_LLM_from_Zero_Data.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="128-A-Multi-Stage-Large-Language-Model-Framework-for-Extracting-Suicide-Related-Social-Determinants-of-Health"><a href="#128-A-Multi-Stage-Large-Language-Model-Framework-for-Extracting-Suicide-Related-Social-Determinants-of-Health" class="headerlink" title="128. A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health"></a>128. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Multi-Stage_Large_Language_Model_Framework_for_Extracting_Suicide-Related_Social_Determinants_of_H.pdf">A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Cockrell School of Engineering, The University of Texas at Austin</span></p>
<p>æœ¬è®ºæ–‡æå‡ºä¸€ç§å¤šé˜¶æ®µå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡æ£€ç´¢ã€ç›¸å…³æ€§éªŒè¯å’ŒSDoHå› å­æŠ½å–ä¸‰ä¸ªæ­¥éª¤ï¼Œå®ç°å¯¹è‡ªæ€ç›¸å…³ç¤¾ä¼šå¥åº·å†³å®šå› ç´ ï¼ˆSDoHï¼‰ä»éç»“æ„åŒ–æ–‡æœ¬ä¸­çš„é«˜æ•ˆæå–ã€‚æ–¹æ³•åœ¨é›¶æ ·æœ¬è®¾å®šä¸‹ï¼Œå¯¹æ¯”BioBERTã€GPT-3.5-turboå’ŒDeepSeek-R1ç­‰åŸºçº¿ï¼Œåœ¨æå–å°‘è§å’Œå¸¸è§SDoHå› å­ä¸Šå–å¾—æ›´é«˜çš„å‡†ç¡®ç‡å’Œå¬å›ç‡ï¼Œå¹¶é€šè¿‡æä¾›ä¸­é—´è§£é‡Šæé«˜äº†æ¨¡å‹å¯è§£é‡Šæ€§å’Œäººå·¥æ ‡æ³¨æ•ˆç‡ã€‚ç»“è®ºï¼šæœ¬æ–¹æ³•æå‡äº†ä»æ–‡æœ¬ä¸­æå–è‡ªæ€ç›¸å…³SDoHå› å­çš„å‡†ç¡®æ€§ä¸é€æ˜åº¦ï¼Œæœ‰åŠ©äºæ—©æœŸé£é™©è¯†åˆ«å’Œå¹²é¢„ç­–ç•¥åˆ¶å®šã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Multi-Stage_Large_Language_Model_Framework_for_Extracting_Suicide-Related_Social_Determinants_of_Health.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="129-Situated-Epistemic-Infrastructures-A-Diagnostic-Framework-for-Post-Coherence-Knowledge"><a href="#129-Situated-Epistemic-Infrastructures-A-Diagnostic-Framework-for-Post-Coherence-Knowledge" class="headerlink" title="129. Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge"></a>129. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Situated_Epistemic_Infrastructures__A_Diagnostic_Framework_for_Post-Coherence_Knowledge.pdf">Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Toronto</span></p>
<p>æœ¬æ–‡æå‡ºäº†â€œSituated Epistemic Infrastructures (SEI)â€æ¡†æ¶ï¼Œç”¨ä»¥è¯Šæ–­å’Œåˆ†æåœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç­‰AIæŠ€æœ¯æ¨åŠ¨ä¸‹ï¼ŒçŸ¥è¯†ç”Ÿäº§ã€æµé€šå’Œæƒå¨è®¤å®šè¿‡ç¨‹ä¸­çš„åŸºç¡€ç»“æ„å˜é©ã€‚è¯¥æ–¹æ³•å¼ºè°ƒé€šè¿‡åŸºç¡€è®¾æ–½ã€æƒåŠ›ç­¾åã€ç¬¦å·å‹ç¼©å’Œæ•…éšœåŠ¨æ€å››ä¸ªç»´åº¦ï¼Œåˆ†æçŸ¥è¯†æƒå¨æ˜¯å¦‚ä½•åœ¨ç®—æ³•ã€æœºæ„ã€äººç±»åä½œç­‰æ··åˆç³»ç»Ÿä¸­ä¸æ–­è¢«é‡å¡‘ã€‚ç»“è®ºæŒ‡å‡ºï¼ŒSEIèƒ½å¤Ÿå¸®åŠ©å­¦è€…å’Œæœºæ„è¯†åˆ«å’Œé€‚åº”ç”±AIé©±åŠ¨çš„çŸ¥è¯†ä½“ç³»ä¸ç¨³å®šæ€§ï¼Œä¿ƒè¿›æ›´å…·åæ€æ€§å’Œé€‚åº”æ€§çš„çŸ¥è¯†ç»„ç»‡æ¨¡å¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Situated_Epistemic_Infrastructures_A_Diagnostic_Framework_for_Post-Coherence_Knowledge.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="130-A-Metric-for-MLLM-Alignment-in-Large-scale-Recommendation"><a href="#130-A-Metric-for-MLLM-Alignment-in-Large-scale-Recommendation" class="headerlink" title="130. A Metric for MLLM Alignment in Large-scale Recommendation"></a>130. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Metric_for_MLLM_Alignment_in_Large-scale_Recommendation.pdf">A Metric for MLLM Alignment in Large-scale Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Xiaohongshu Inc.</span></p>
<p>æœ¬æ–‡æå‡ºäº†Leakage Impact Score (LIS)ï¼Œä¸€ç§ç”¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤§è§„æ¨¡æ¨èç³»ç»Ÿä¸­å¯¹é½è¯„ä»·çš„æ–°æŒ‡æ ‡ã€‚LISé€šè¿‡é¢„å…ˆé‡åŒ–åå¥½æ•°æ®çš„ä¸Šç•Œï¼Œè¾…åŠ©MLLMå¯¹é½æµç¨‹ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€æ¨èéƒ¨ç½²æ•ˆç‡ã€‚å®éªŒåœ¨å°çº¢ä¹¦çœŸå®åœºæ™¯ä¸‹ï¼ŒåŒ…æ‹¬å†…å®¹æ¨èå’Œå¹¿å‘Šæ¨èï¼ŒA&#x2F;Bæµ‹è¯•æ˜¾ç¤ºç”¨æˆ·æ—¶é•¿ã€å¹¿å‘Šä»·å€¼ç­‰æŒ‡æ ‡å‡æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†æ–¹æ³•çš„å®é™…ä»·å€¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Metric_for_MLLM_Alignment_in_Large-scale_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="131-Learning-Geometric-Aware-Quadrature-Rules-for-Functional-Minimization"><a href="#131-Learning-Geometric-Aware-Quadrature-Rules-for-Functional-Minimization" class="headerlink" title="131. Learning Geometric-Aware Quadrature Rules for Functional Minimization"></a>131. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Learning_Geometric-Aware_Quadrature_Rules_for_Functional_Minimization.pdf">Learning Geometric-Aware Quadrature Rules for Functional Minimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of the Aegean</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†QuadrANNï¼Œä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œ(GNN)çš„æ–°å‹æ¶æ„ï¼Œç”¨äºä»éå‡åŒ€ç‚¹äº‘å‡ ä½•ä¸­ç›´æ¥å­¦ä¹ æœ€ä¼˜ç§¯åˆ†æƒé‡ï¼Œå®ç°å¯¹å˜åˆ†åŸç†ä¸‹æ³›å‡½æœ€å°åŒ–é—®é¢˜çš„é«˜æ•ˆæ•°å€¼ç§¯åˆ†ã€‚é€šè¿‡å¤šå±‚æ¶ˆæ¯ä¼ é€’ç»“åˆå±€éƒ¨å¯†åº¦å’Œå…¨å±€åŸŸå½¢çŠ¶ç‰¹å¾ï¼ŒQuadrANNæ˜¾è‘—é™ä½äº†ç§¯åˆ†ä¼°è®¡çš„æ–¹å·®ï¼Œå¹¶åœ¨å¤šç§ç§¯åˆ†å’ŒPDEæ±‚è§£ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»ŸQMCæ–¹æ³•ï¼Œæå‡äº†æ·±åº¦å­¦ä¹ å˜åˆ†æ±‚è§£å™¨çš„ç¨³å®šæ€§å’Œç²¾åº¦ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Learning_Geometric-Aware_Quadrature_Rules_for_Functional_Minimization.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="132-MolSnap-Snap-Fast-Molecular-Generation-with-Latent-Variational-Mean-Flow"><a href="#132-MolSnap-Snap-Fast-Molecular-Generation-with-Latent-Variational-Mean-Flow" class="headerlink" title="132. MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow"></a>132. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MolSnap__Snap-Fast_Molecular_Generation_with_Latent_Variational_Mean_Flow.pdf">MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Kentucky</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†MolSnapæ¡†æ¶ï¼Œé€šè¿‡åˆ›æ–°çš„å› æœæ„ŸçŸ¥Transformerï¼ˆCATï¼‰å’Œå˜åˆ†å‡å€¼æµï¼ˆVMFï¼‰æ–¹æ³•ï¼Œå®ç°äº†åŸºäºæ–‡æœ¬æè¿°çš„é«˜è´¨é‡ã€å¤šæ ·åŒ–åˆ†å­ç”Ÿæˆã€‚CATæ˜¾å¼å»ºæ¨¡åˆ†å­å›¾ä¸æ–‡æœ¬é—´çš„å› æœä¾èµ–ï¼ŒVMFå°†æ½œåœ¨ç©ºé—´å»ºæ¨¡ä¸ºé«˜æ–¯æ··åˆåˆ†å¸ƒï¼Œæ”¯æŒé«˜æ•ˆä¸€æ­¥æ¨æ–­ã€‚å®éªŒè¡¨æ˜æ–¹æ³•åœ¨æ–°é¢–æ€§ã€å¤šæ ·æ€§å’Œæœ‰æ•ˆæ€§ä¸Šå‡ä¼˜äºSOTAæ¨¡å‹ï¼Œä¸”æ¨ç†é€Ÿåº¦æå‡æ˜¾è‘—ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MolSnap_Snap-Fast_Molecular_Generation_with_Latent_Variational_Mean_Flow.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="133-Multi-Modal-Multi-Behavior-Sequential-Recommendation-with-Conditional-Diffusion-Based-Feature-Denoising"><a href="#133-Multi-Modal-Multi-Behavior-Sequential-Recommendation-with-Conditional-Diffusion-Based-Feature-Denoising" class="headerlink" title="133. Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising"></a>133. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Multi-Modal_Multi-Behavior_Sequential_Recommendation_with_Conditional_Diffusion-Based_Feature_Denois.pdf">Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Takway.AI</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å¤šè¡Œä¸ºåºåˆ—æ¨èæ¨¡å‹M3BSRï¼Œé€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹åˆ†åˆ«å¯¹å¤šæ¨¡æ€ç‰¹å¾å’Œè¡Œä¸ºç‰¹å¾è¿›è¡Œå»å™ªï¼Œå¹¶å¼•å…¥å¤šä¸“å®¶å…´è¶£æå–å±‚ä»¥å»ºæ¨¡ç”¨æˆ·åœ¨ä¸åŒæ¨¡æ€å’Œè¡Œä¸ºä¸‹çš„å…±æ€§å’Œç‰¹æ€§å…´è¶£ã€‚å®éªŒè¯æ˜ï¼ŒM3BSRåœ¨å…¬å¼€æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ï¼Œæå‡äº†åå¥½å»ºæ¨¡å’Œæ¨èå‡†ç¡®æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Multi-Modal_Multi-Behavior_Sequential_Recommendation_with_Conditional_Diffusion-Based_Feature_Denoising.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="134-EnergyPatchTST-Multi-scale-Time-Series-Transformers-with-Uncertainty-Estimation-for-Energy-Forecasting"><a href="#134-EnergyPatchTST-Multi-scale-Time-Series-Transformers-with-Uncertainty-Estimation-for-Energy-Forecasting" class="headerlink" title="134. EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting"></a>134. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EnergyPatchTST__Multi-scale_Time_Series_Transformers_with_Uncertainty_Estimation_for_Energy_Forecast.pdf">EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºEnergyPatchTSTæ¨¡å‹ï¼Œç”¨äºèƒ½æºé¢†åŸŸçš„æ—¶é—´åºåˆ—é¢„æµ‹ã€‚æ–¹æ³•åŒ…æ‹¬å¤šå°ºåº¦ç‰¹å¾æå–ã€è’™ç‰¹å¡ç½—Dropoutä¸ç¡®å®šæ€§ä¼°è®¡ã€æœªæ¥å˜é‡é›†æˆä»¥åŠé¢„è®­ç»ƒ-å¾®è°ƒè¿ç§»å­¦ä¹ ï¼Œæœ‰æ•ˆæå‡èƒ½æºé¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šä¸ªèƒ½æºæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•è¾ƒä¸»æµæ–¹æ³•è¯¯å·®é™ä½7-12%ï¼Œå¹¶æä¾›äº†å¯é çš„ä¸ç¡®å®šæ€§åŒºé—´é¢„æµ‹ï¼Œæ˜¾è‘—ä¿ƒè¿›äº†èƒ½æºé¢†åŸŸçš„æ—¶é—´åºåˆ—åˆ†ææ€§èƒ½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EnergyPatchTST_Multi-scale_Time_Series_Transformers_with_Uncertainty_Estimation_for_Energy_Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="135-Harmonic-fractal-transformation-for-modeling-complex-neuronal-effects-from-bursting-and-noise-shaping-to-waveform-sensitivity-and-noise-induced-subthreshold-spiking"><a href="#135-Harmonic-fractal-transformation-for-modeling-complex-neuronal-effects-from-bursting-and-noise-shaping-to-waveform-sensitivity-and-noise-induced-subthreshold-spiking" class="headerlink" title="135. Harmonic fractal transformation for modeling complex neuronal effects: from bursting and noise shaping to waveform sensitivity and noise-induced subthreshold spiking"></a>135. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Harmonic_fractal_transformation_for_modeling_complex_neuronal_effects__from_bursting_and_noise_shapi.pdf">Harmonic fractal transformation for modeling complex neuronal effects: from bursting and noise shaping to waveform sensitivity and noise-induced subthreshold spiking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">90 Navigation street, Birmingham, UK</span></p>
<p>æœ¬æ–‡æå‡ºäº†è°æ³¢åˆ†å½¢å˜æ¢ï¼ˆHFTï¼‰ä½œä¸ºä¸€ç§å…¨æ–°çš„é¢‘ç‡æ˜ å°„æ–¹æ³•ï¼Œèƒ½å¤Ÿä»¥ç®€å•å½¢å¼å¤åˆ»ç¥ç»å…ƒå¤æ‚æ•ˆåº”ï¼ŒåŒ…æ‹¬ä¿¡å·æ¿€å‘ã€å™ªå£°æ•´å½¢ã€æ³¢å½¢æ•æ„Ÿæ€§å’Œäºšé˜ˆå€¼å™ªå£°è¯±å¯¼è„‰å†²ã€‚HFTé€šè¿‡åˆ†å½¢é‡ç»„è¾“å…¥é¢‘è°±ï¼Œæ¿€å‘æ–°çš„è°æ³¢åˆ†é‡ï¼Œå®ç°å¯¹å™ªå£°çš„é²æ£’æ€§å’Œä¿¡å·æ”¾å¤§ï¼Œç»“è®ºè¡¨æ˜è¯¥æ–¹æ³•èƒ½çµæ´»å»ºæ¨¡å¤šç§è„‰å†²å½¢æ€ä¸ç¥ç»å…ƒç‰¹æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Harmonic_fractal_transformation_for_modeling_complex_neuronal_effects_from_bursting_and_noise_shaping_to_waveform_sensitivity_and_noise-induced_subthreshold_spiking.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="136-Estimating-Musical-Surprisal-from-Audio-in-Autoregressive-Diffusion-Model-Noise-Spaces"><a href="#136-Estimating-Musical-Surprisal-from-Audio-in-Autoregressive-Diffusion-Model-Noise-Spaces" class="headerlink" title="136. Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces"></a>136. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Estimating_Musical_Surprisal_from_Audio_in_Autoregressive_Diffusion_Model_Noise_Spaces.pdf">Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Johannes Kepler University Linz</span></p>
<p>è¯¥è®ºæ–‡æå‡ºåˆ©ç”¨è‡ªå›å½’æ‰©æ•£æ¨¡å‹ï¼ˆEDMå’ŒRFFï¼‰åœ¨è¿ç»­éŸ³é¢‘åµŒå…¥ç©ºé—´ä¼°è®¡éŸ³ä¹æƒŠå¥‡æ€§ï¼ˆsurprisalï¼‰ï¼Œé€šè¿‡ä¿¡æ¯å«é‡ï¼ˆICï¼‰å’Œè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰è¡¡é‡æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨å•éŸ³é«˜æƒŠå¥‡æ€§å’Œå¤šè½¨éŸ³é¢‘åˆ†æ®µæ£€æµ‹ä¸¤ä»»åŠ¡ä¸Šä¸GIVTæ¨¡å‹å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰©æ•£æ¨¡å‹åœ¨æè¿°éŸ³ä¹æ•°æ®å’Œæ•æ‰éŸ³é«˜æƒŠå¥‡æ€§æ–¹é¢ä¼˜äºGIVTæ¨¡å‹ï¼Œä¸”åœ¨é€‚å½“å™ªå£°æ°´å¹³ä¸‹å¯¹éŸ³è‰²å…·æœ‰æ›´é«˜ä¸å˜æ€§ï¼Œæå‡åˆ†æ®µæ£€æµ‹ç²¾åº¦ä¸å¬å›ç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Estimating_Musical_Surprisal_from_Audio_in_Autoregressive_Diffusion_Model_Noise_Spaces.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="137-FlowState-Sampling-Rate-Invariant-Time-Series-Forecasting"><a href="#137-FlowState-Sampling-Rate-Invariant-Time-Series-Forecasting" class="headerlink" title="137. FlowState: Sampling Rate Invariant Time Series Forecasting"></a>137. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FlowState__Sampling_Rate_Invariant_Time_Series_Forecasting.pdf">FlowState: Sampling Rate Invariant Time Series Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">IBM Research Europe â€“ Zurich, Switzerland</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†FlowStateï¼Œä¸€ç§ç»“åˆS5çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ç¼–ç å™¨å’ŒåŠŸèƒ½åŸºè§£ç å™¨ï¼ˆFBDï¼‰çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼Œå®ç°äº†å¯¹ä¸åŒé‡‡æ ·ç‡çš„è‡ªé€‚åº”ä¸è¿ç»­æ—¶é—´å»ºæ¨¡ã€‚é€šè¿‡å¹¶è¡Œé¢„æµ‹è®­ç»ƒæ–¹æ¡ˆå’Œå› æœå½’ä¸€åŒ–ï¼ŒFlowStateåœ¨GIFT-ZSå’ŒChronos-ZSåŸºå‡†ä¸Šè¾¾åˆ°æ–°çš„SOTAï¼Œä¼˜äºä½“ç§¯æ›´å¤§çš„æ¨¡å‹ï¼Œå…·å¤‡å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FlowState_Sampling_Rate_Invariant_Time_Series_Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="138-FDC-Net-Rethinking-the-association-between-EEG-artifact-removal-and-multi-dimensional-affective-computing"><a href="#138-FDC-Net-Rethinking-the-association-between-EEG-artifact-removal-and-multi-dimensional-affective-computing" class="headerlink" title="138. FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing"></a>138. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FDC-Net__Rethinking_the_association_between_EEG_artifact_removal_and_multi-dimensional_affective_com.pdf">FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Beijing University of Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„EEGæƒ…æ„Ÿè¯†åˆ«ä¸å»ä¼ªå½±ååŒæ¡†æ¶FDC-Netï¼Œé‡‡ç”¨åŒå‘æ¢¯åº¦åé¦ˆä¼˜åŒ–å’Œé¢‘å¸¦å¯å­¦ä¹ ç¼–ç çš„Transformerï¼ˆEEGSPTransformerï¼‰ï¼Œå®ç°äº†å»å™ªä¸æƒ…æ„Ÿè¯†åˆ«çš„æ·±åº¦è€¦åˆä¸åŠ¨æ€åä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¼ºå™ªå£°å¹²æ‰°ä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨DEAPå’ŒDREAMERæ•°æ®é›†ä¸Šçš„å»å™ªå’Œæƒ…æ„Ÿè¯†åˆ«æ€§èƒ½å‡ä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FDC-Net_Rethinking_the_association_between_EEG_artifact_removal_and_multi-dimensional_affective_computing.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="139-Balancing-Accuracy-and-Novelty-with-Sub-Item-Popularity"><a href="#139-Balancing-Accuracy-and-Novelty-with-Sub-Item-Popularity" class="headerlink" title="139. Balancing Accuracy and Novelty with Sub-Item Popularity"></a>139. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Balancing_Accuracy_and_Novelty_with_Sub-Item_Popularity.pdf">Balancing Accuracy and Novelty with Sub-Item Popularity</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Politecnico di Bari</span></p>
<p>æœ¬æ–‡æå‡ºåœ¨Transformer-basedæ¨èç³»ç»ŸRecJPQä¸­å¼•å…¥ä¸ªæ€§åŒ–æµè¡Œåº¦ä¿¡å·ï¼ŒåŒ…æ‹¬åŸºäºitem-IDä¸sub-IDçš„ä¸ªæ€§åŒ–æµè¡Œåº¦åˆ†æ•°ï¼ˆPPSå’ŒsPPSï¼‰ï¼Œé€šè¿‡å¯¹sub-IDæµè¡Œåº¦å»ºæ¨¡ï¼Œå®ç°æ›´ç»†ç²’åº¦çš„ç”¨æˆ·é‡å¤è¡Œä¸ºæ•æ‰ã€‚å®éªŒè¯æ˜ï¼Œsub-IDç²’åº¦çš„æµè¡Œåº¦å»ºæ¨¡åœ¨æå‡æ¨èæ–°é¢–æ€§çš„åŒæ—¶ä¸æŸå¤±å‡†ç¡®æ€§ï¼Œå¹¶å¯çµæ´»è°ƒæ§å‡†ç¡®æ€§ä¸æ–°é¢–æ€§çš„æƒè¡¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Balancing_Accuracy_and_Novelty_with_Sub-Item_Popularity.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="140-Hybrid-quantum-tensor-networks-for-aeroelastic-applications"><a href="#140-Hybrid-quantum-tensor-networks-for-aeroelastic-applications" class="headerlink" title="140. Hybrid quantum tensor networks for aeroelastic applications"></a>140. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Hybrid_quantum_tensor_networks_for_aeroelastic_applications.pdf">Hybrid quantum tensor networks for aeroelastic applications</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute for AI Safety and Security, German Aerospace Center (DLR)</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘æ°”åŠ¨å¼¹æ€§é—®é¢˜çš„æ··åˆé‡å­å¼ é‡ç½‘ç»œç®—æ³•ï¼Œé›†æˆäº†å¼ é‡ç½‘ç»œï¼ˆTNï¼‰é™ç»´ã€æ•°æ®ç¼–ç å’Œå¼ é‡ç½‘ç»œå¯å‘çš„å¯è®­ç»ƒå˜åˆ†é‡å­ç”µè·¯ï¼ˆVQCï¼‰ï¼Œå®ç°ç«¯åˆ°ç«¯è®­ç»ƒã€‚æ–¹æ³•åœ¨æ°”åŠ¨å¼¹æ€§æ—¶åºæ•°æ®çš„äºŒåˆ†ç±»ä»»åŠ¡ä¸­å–å¾—äº†æé«˜å‡†ç¡®ç‡ï¼ˆF1&gt;0.99ï¼‰ï¼Œå¹¶åœ¨å¤š&#x2F;å•å˜é‡å›å½’ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œä½†æ¨¡å‹çš„è¶…å‚æ•°é€‰æ‹©ä¾ç„¶æ˜¯ä¸»è¦æŒ‘æˆ˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Hybrid_quantum_tensor_networks_for_aeroelastic_applications.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="141-Salt-Rock-Creep-Deformation-Forecasting-Using-Deep-Neural-Networks-and-Analytical-Models-for-Subsurface-Energy-Storage-Applications"><a href="#141-Salt-Rock-Creep-Deformation-Forecasting-Using-Deep-Neural-Networks-and-Analytical-Models-for-Subsurface-Energy-Storage-Applications" class="headerlink" title="141. Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications"></a>141. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Salt-Rock_Creep_Deformation_Forecasting_Using_Deep_Neural_Networks_and_Analytical_Models_for_Subsurf.pdf">Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">IIT (ISM) Dhanbad</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†èåˆæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆN-BEATSã€TCNã€RNNã€Transformerï¼‰ä¸åˆ†ææ¨¡å‹ï¼ˆå¦‚Power Lawã€Burgerç­‰ï¼‰çš„æ–¹æ³•ï¼Œå¯¹ç›å²©åœ¨ä¸åŒå‹åŠ›ä¸‹çš„è •å˜å˜å½¢è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå°¤å…¶N-BEATSå’ŒTCNï¼‰åœ¨é¢„æµ‹ç²¾åº¦ä¸Šä¼˜äºä¼ ç»Ÿç»Ÿè®¡å’Œåˆ†ææ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡15-20%ï¼Œä¸ºåœ°ä¸‹ç›ç©´èƒ½æºå‚¨å­˜çš„å®‰å…¨ç®¡ç†æä¾›äº†æ›´å¯é çš„å»ºæ¨¡åŸºç¡€ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Salt-Rock_Creep_Deformation_Forecasting_Using_Deep_Neural_Networks_and_Analytical_Models_for_Subsurface_Energy_Storage_Applications.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="142-High-Dimensional-Differentially-Private-Quantile-Regression-Distributed-Estimation-and-Statistical-Inference"><a href="#142-High-Dimensional-Differentially-Private-Quantile-Regression-Distributed-Estimation-and-Statistical-Inference" class="headerlink" title="142. High-Dimensional Differentially Private Quantile Regression: Distributed Estimation and Statistical Inference"></a>142. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/High-Dimensional_Differentially_Private_Quantile_Regression__Distributed_Estimation_and_Statistical_.pdf">High-Dimensional Differentially Private Quantile Regression: Distributed Estimation and Statistical Inference</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Statistics and Data Science, Shanghai University of Finance and Economics</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é€‚ç”¨äºé«˜ç»´åˆ†å¸ƒå¼æ•°æ®çš„å·®åˆ†éšç§åˆ†ä½æ•°å›å½’æ–¹æ³•ï¼Œæ ¸å¿ƒåŒ…æ‹¬åˆ©ç”¨Newtonå‹å˜æ¢å°†éå…‰æ»‘åˆ†ä½æ•°å›å½’é—®é¢˜è½¬åŒ–ä¸ºæ™®é€šæœ€å°äºŒä¹˜é—®é¢˜ï¼Œå¹¶ç»“åˆNoisy Hard Thresholdingç®—æ³•å®ç°è¿­ä»£å¼åˆ†å¸ƒå¼å‚æ•°ä¼°è®¡ï¼Œä¿è¯ç¨€ç–æ€§å’Œå·®åˆ†éšç§ï¼›åœ¨æ¨æ–­é˜¶æ®µï¼Œæå‡ºäº†å·®åˆ†éšç§å»åä¼°è®¡å’Œåˆ†å¸ƒå¼ç§æœ‰bootstrapæ–¹æ³•ç”¨äºå¤šé‡å‡è®¾æ£€éªŒå’Œç½®ä¿¡åŒºé—´æ„å»ºã€‚ç†è®ºåˆ†æç»™å‡ºäº†ä¼°è®¡è¯¯å·®ç•Œå’Œæ¨æ–­ä¸€è‡´æ€§ï¼Œå®éªŒè¯æ˜ç®—æ³•åœ¨å¼‚è´¨å’Œé‡å°¾åˆ†å¸ƒä¸‹å…·æœ‰ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä¸”æ­ç¤ºäº†éšç§ä¿æŠ¤ä¸ç»Ÿè®¡ç²¾åº¦ä¹‹é—´çš„åŸºæœ¬æƒè¡¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/High-Dimensional_Differentially_Private_Quantile_Regression_Distributed_Estimation_and_Statistical_Inference.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="143-Graph-based-Event-Log-Repair"><a href="#143-Graph-based-Event-Log-Repair" class="headerlink" title="143. Graph-based Event Log Repair"></a>143. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Graph-based_Event_Log_Repair.pdf">Graph-based Event Log Repair</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Trento</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†SANAGRAPHï¼Œä¸€ç§åˆ©ç”¨å¼‚æ„å›¾ç¥ç»ç½‘ç»œï¼ˆHGNNï¼‰ç¼–ç äº‹ä»¶æ—¥å¿—ç—•è¿¹å¹¶ä¿®å¤å…¶ä¸­ç¼ºå¤±æ•°æ®çš„æ–¹æ³•ã€‚æ–¹æ³•å°†æ—¥å¿—ä¸­çš„æ¯ä¸ªäº‹ä»¶å’Œå±æ€§æ˜ å°„ä¸ºå›¾èŠ‚ç‚¹ï¼Œå¹¶é€šè¿‡SAGEConvå·ç§¯æ“ä½œå®ç°ä¿¡æ¯ä¼ æ’­ï¼Œä½¿ç”¨å¤šç§æŸå¤±å‡½æ•°åŒæ—¶ä¼˜åŒ–åˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚å®éªŒå¯¹æ¯”äº†ç°æœ‰çš„è‡ªç¼–ç å™¨æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºSANAGRAPHåœ¨é‡å»ºæ´»åŠ¨å’Œæ—¶é—´æˆ³æ–¹é¢å‡†ç¡®ç‡æ›´é«˜ï¼Œå¯¹æ‰€æœ‰å±æ€§çš„ä¿®å¤ä¹Ÿè¡¨ç°è‰¯å¥½ï¼Œæ˜¾è‘—æå‡äº†æµç¨‹æŒ–æ˜æ•°æ®è´¨é‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Graph-based_Event_Log_Repair.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="144-Align-for-Fusion-Harmonizing-Triple-Preferences-via-Dual-oriented-Diffusion-for-Cross-domain-Sequential-Recommendation"><a href="#144-Align-for-Fusion-Harmonizing-Triple-Preferences-via-Dual-oriented-Diffusion-for-Cross-domain-Sequential-Recommendation" class="headerlink" title="144. Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation"></a>144. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Align-for-Fusion__Harmonizing_Triple_Preferences_via_Dual-oriented_Diffusion_for_Cross-domain_Sequen.pdf">Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">National University of Defense Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘è·¨åŸŸåºåˆ—æ¨èçš„æ–°æ¡†æ¶HorizonRecï¼Œç»“åˆæ··åˆæ¡ä»¶åˆ†å¸ƒæ£€ç´¢ï¼ˆMDRï¼‰å’ŒåŒå‘æ‰©æ•£åå¥½å»ºæ¨¡ï¼ˆDPDï¼‰ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨æºåŸŸã€ç›®æ ‡åŸŸå’Œæ··åˆåŸŸé—´è¿›è¡Œç»†ç²’åº¦çš„ç”¨æˆ·å…´è¶£èåˆã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†è·¨åŸŸæ¨èçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼ŒéªŒè¯äº†MDRå’ŒDPDæ¨¡å—å¯¹äºå¤šåŸŸåå¥½å¯¹é½åŠé²æ£’å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Align-for-Fusion_Harmonizing_Triple_Preferences_via_Dual-oriented_Diffusion_for_Cross-domain_Sequential_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="145-Q-DPTS-Quantum-Differentially-Private-Time-Series-Forecasting-via-Variational-Quantum-Circuits"><a href="#145-Q-DPTS-Quantum-Differentially-Private-Time-Series-Forecasting-via-Variational-Quantum-Circuits" class="headerlink" title="145. Q-DPTS: Quantum Differentially Private Time Series Forecasting via Variational Quantum Circuits"></a>145. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Q-DPTS__Quantum_Differentially_Private_Time_Series_Forecasting_via_Variational_Quantum_Circuits.pdf">Q-DPTS: Quantum Differentially Private Time Series Forecasting via Variational Quantum Circuits</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Independent Researcher</span></p>
<p>æœ¬æ–‡æå‡ºQ-DPTSæ¡†æ¶ï¼Œå°†å˜åˆ†é‡å­ç”µè·¯ï¼ˆVQCï¼‰ä¸å¾®åˆ†éšç§ï¼ˆDifferential Privacy, DPï¼‰æœºåˆ¶ç»“åˆï¼Œç”¨äºæ—¶åºæ•°æ®é¢„æµ‹ã€‚é‡‡ç”¨DP-SGDç®—æ³•ä¸­çš„æ¯æ ·æœ¬æ¢¯åº¦è£å‰ªä¸é«˜æ–¯å™ªå£°æ³¨å…¥ï¼Œå¹¶ç”¨RÃ©nyiå¾®åˆ†éšç§ä¼šè®¡ï¼Œä¸ºé‡å­-ç»å…¸æ··åˆæ¨¡å‹æä¾›ä¸¥æ ¼çš„éšç§ä¿éšœã€‚å®éªŒåœ¨ETTæ•°æ®é›†ä¸Šï¼Œç³»ç»Ÿå¯¹æ¯”äº†LSTMã€QASAã€QRWKVã€QLSTMç­‰æ¨¡å‹åœ¨ä¸åŒéšç§é¢„ç®—ä¸‹çš„é¢„æµ‹è¯¯å·®ï¼Œç»“æœè¡¨æ˜ï¼ŒQ-DPTSé‡å­æ¨¡å‹åœ¨ç‰ºç‰²æœ€å°ç²¾åº¦çš„å‰æä¸‹å®ç°ä¼˜è‰¯çš„éšç§-æ•ˆç”¨æƒè¡¡ï¼Œå°¤å…¶QASAå’ŒQRWKVåœ¨é«˜å™ªå£°ç¯å¢ƒä¸‹ä»ä¼˜äºç»å…¸æ¨¡å‹ã€‚ç»“è®ºï¼šé‡å­å·®åˆ†éšç§é¢„æµ‹æ¨¡å‹ä¸ä»…å¯è¡Œä¸”æœ‰æ•ˆï¼Œèƒ½åœ¨æ•æ„Ÿé¢†åŸŸå®ç°å®‰å…¨é«˜æ•ˆçš„æ—¶åºå»ºæ¨¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Q-DPTS_Quantum_Differentially_Private_Time_Series_Forecasting_via_Variational_Quantum_Circuits.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="146-Fairness-in-Dysarthric-Speech-Synthesis-Understanding-Intrinsic-Bias-in-Dysarthric-Speech-Cloning-using-F5-TTS"><a href="#146-Fairness-in-Dysarthric-Speech-Synthesis-Understanding-Intrinsic-Bias-in-Dysarthric-Speech-Cloning-using-F5-TTS" class="headerlink" title="146. Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS"></a>146. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Fairness_in_Dysarthric_Speech_Synthesis__Understanding_Intrinsic_Bias_in_Dysarthric_Speech_Cloning_u.pdf">Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">International Institute of Information Technology-Hyderabad</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œåˆ©ç”¨F5-TTSé›¶æ ·æœ¬è¯­éŸ³å…‹éš†æŠ€æœ¯å’ŒTORGOæ•°æ®åº“ï¼Œç³»ç»Ÿè¯„ä¼°äº†åˆæˆdysarthricï¼ˆæ„éŸ³éšœç¢ï¼‰è¯­éŸ³åœ¨å¯æ‡‚åº¦ã€è¯´è¯äººç›¸ä¼¼æ€§å’ŒéŸµå¾‹ä¿ç•™æ–¹é¢çš„æ•ˆæœï¼Œå¹¶é€šè¿‡Parity Differenceå’ŒDisparate Impactç­‰å…¬å¹³æ€§æŒ‡æ ‡åˆ†æä¸åŒç—…æƒ…ä¸¥é‡ç¨‹åº¦ä¸‹çš„åå·®ã€‚ç ”ç©¶å‘ç°ï¼ŒF5-TTSå¯¹è¯­éŸ³å¯æ‡‚åº¦åç½®è¾ƒå¤§ï¼Œå°¤å…¶å¯¹é«˜ä¸¥é‡åº¦æ‚£è€…ï¼Œå¯¼è‡´æ•°æ®å¢å¼ºæ—¶å¯¹ç›¸å…³è¾…åŠ©è¯­éŸ³è¯†åˆ«ä»»åŠ¡æ•ˆæœä¸ä½³ï¼Œå¼ºè°ƒäº†åœ¨è¯­éŸ³æ•°æ®å¢å¼ºä¸­éœ€è€ƒè™‘å…¬å¹³æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Fairness_in_Dysarthric_Speech_Synthesis_Understanding_Intrinsic_Bias_in_Dysarthric_Speech_Cloning_using_F5-TTS.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="147-Disentangling-Bias-by-Modeling-Intra-and-Inter-modal-Causal-Attention-for-Multimodal-Sentiment-Analysis"><a href="#147-Disentangling-Bias-by-Modeling-Intra-and-Inter-modal-Causal-Attention-for-Multimodal-Sentiment-Analysis" class="headerlink" title="147. Disentangling Bias by Modeling Intra- and Inter-modal Causal Attention for Multimodal Sentiment Analysis"></a>147. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Disentangling_Bias_by_Modeling_Intra-_and_Inter-modal_Causal_Attention_for_Multimodal_Sentiment_Anal.pdf">Disentangling Bias by Modeling Intra- and Inter-modal Causal Attention for Multimodal Sentiment Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Computer Science, South China Normal University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šå…³ç³»å¤šæ¨¡æ€å› æœå¹²é¢„æ¨¡å‹ï¼ˆMMCIï¼‰ï¼Œåˆ©ç”¨å› æœç†è®ºä¸­çš„èƒŒé—¨è°ƒæ•´æœºåˆ¶ï¼Œé€šè¿‡æ„å»ºå¤šå…³ç³»å›¾å’Œå›¾æ³¨æ„åŠ›ç½‘ç»œæ˜¾å¼å»ºæ¨¡æ–‡æœ¬ã€éŸ³é¢‘ã€è§†è§‰æ¨¡æ€é—´çš„å†…å¤–éƒ¨ä¾èµ–ï¼Œåˆ†åˆ«ä¼°ç®—å’Œè§£è€¦å› æœç‰¹å¾ä¸æ·å¾„ç‰¹å¾ï¼Œå¹¶åŠ¨æ€ç»“åˆä»¥æŠµå¾¡æ•°æ®åˆ†å¸ƒåç§»ã€‚å®éªŒè¡¨æ˜ï¼ŒMMCIåœ¨å¤šé¡¹åŸºå‡†å’ŒOODæµ‹è¯•æ•°æ®ä¸Šæœ‰æ•ˆå»åã€æå‡äº†å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Disentangling_Bias_by_Modeling_Intra-_and_Inter-modal_Causal_Attention_for_Multimodal_Sentiment_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="148-Compressed-Decentralized-Momentum-Stochastic-Gradient-Methods-for-Nonconvex-Optimization"><a href="#148-Compressed-Decentralized-Momentum-Stochastic-Gradient-Methods-for-Nonconvex-Optimization" class="headerlink" title="148. Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization"></a>148. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Compressed_Decentralized_Momentum_Stochastic_Gradient_Methods_for_Nonconvex_Optimization.pdf">Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Rensselaer Polytechnic Institute</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸¤ç§ç”¨äºå¤šæ™ºèƒ½ä½“éå‡¸éšæœºä¼˜åŒ–çš„å‹ç¼©å»ä¸­å¿ƒåŒ–åŠ¨é‡éšæœºæ¢¯åº¦ç®—æ³•ï¼ˆDAMSCoå’ŒDaSHCoï¼‰ï¼Œåˆ†åˆ«é’ˆå¯¹æ¢¯åº¦æœ‰ç•Œå’Œæ•°æ®å¼‚è´¨ä¸¤ç§åœºæ™¯ï¼Œç»“åˆäº†åŠ¨é‡&#x2F;è‡ªé€‚åº”æŠ€æœ¯ä¸é€šä¿¡å‹ç¼©ä»¥åŠ é€Ÿæ”¶æ•›å¹¶é™ä½é€šä¿¡æˆæœ¬ã€‚ç†è®ºä¸Šï¼Œä¸¤ç§æ–¹æ³•å‡è¯æ˜èƒ½è¾¾åˆ°æœ€ä¼˜æ”¶æ•›é€Ÿç‡ï¼Œä¸”åœ¨ä¸€å®šè¯¯å·®å®¹å¿åŒºé—´å†…å®ç°çº¿æ€§åŠ é€Ÿå’Œç®—æ³•å‚æ•°çš„æ‹“æ‰‘æ— å…³æ€§ï¼Œå®éªŒä¸Šåœ¨è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œå’ŒTransformeræ—¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Compressed_Decentralized_Momentum_Stochastic_Gradient_Methods_for_Nonconvex_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="149-MENDR-Manifold-Explainable-Neural-Data-Representations"><a href="#149-MENDR-Manifold-Explainable-Neural-Data-Representations" class="headerlink" title="149. MENDR: Manifold Explainable Neural Data Representations"></a>149. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MENDR__Manifold_Explainable_Neural_Data_Representations.pdf">MENDR: Manifold Explainable Neural Data Representations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Georgia Institute of Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºMENDRï¼Œä¸€ç§ç”¨äºè„‘ç”µï¼ˆEEGï¼‰ä¿¡å·çš„åŸºç¡€æ¨¡å‹ï¼Œåˆ›æ–°æ€§åœ°ç»“åˆç¦»æ•£å°æ³¢åŒ…å˜æ¢ã€å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç©ºé—´å½’ä¸€åŒ–ã€è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ å’ŒRiemannianæµå½¢Transformerç»“æ„ï¼Œå®ç°å¯¹EEGä¿¡å·çš„å¯è§£é‡Šå’Œé«˜æ•ˆè¡¨ç¤ºã€‚å®éªŒæ˜¾ç¤ºï¼ŒMENDRåœ¨å¤šä¸ªEEGä¸‹æ¸¸ä»»åŠ¡ä¸Šå‚æ•°é‡æ›´ä½ä½†è¡¨ç°æ¥è¿‘ä¸»æµæ¨¡å‹ï¼ŒåŒæ—¶æå‡äº†è¡¨ç¤ºå¯è§£é‡Šæ€§å’Œé‡å»ºèƒ½åŠ›ï¼Œå±•ç°å‡ºé«˜æ•ˆã€å¯è§£é‡Šã€ä¸´åºŠé€‚ç”¨çš„æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MENDR_Manifold_Explainable_Neural_Data_Representations.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>ä»Šæ—¥è®ºæ–‡æŠ¥çº¸</div>
      <div>http://example.com/2025/08/2025-08-11_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>ä½œè€…</div>
          <div>Ywfhhh</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>å‘å¸ƒäº</div>
          <div>2025å¹´8æœˆ11æ—¥</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>è®¸å¯åè®®</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - ç½²å">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/2025-08-12_article/" title="2025/08/12æŠ¥çº¸">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">2025/08/12æŠ¥çº¸</span>
                        <span class="visible-mobile">ä¸Šä¸€ç¯‡</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/01/2024-01-01_article/" title="ä»Šæ—¥è®ºæ–‡æŠ¥çº¸">
                        <span class="hidden-mobile">ä»Šæ—¥è®ºæ–‡æŠ¥çº¸</span>
                        <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"æ¬¢è¿äº¤æµè®¨è®º...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>ç›®å½•</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        æ€»è®¿é—®é‡ 
        <span id="busuanzi_value_site_pv"></span>
         æ¬¡
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        æ€»è®¿å®¢æ•° 
        <span id="busuanzi_value_site_uv"></span>
         äºº
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- ä¸»é¢˜çš„å¯åŠ¨é¡¹ï¼Œå°†å®ƒä¿æŒåœ¨æœ€åº•éƒ¨ -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div>
  </noscript>
</body>
</html>
