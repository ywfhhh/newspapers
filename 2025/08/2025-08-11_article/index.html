

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="具身ADT">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;11报纸">
<meta property="og:url" content="http://ywfhhh.github.io/2025/08/2025-08-11_article/index.html">
<meta property="og:site_name" content="最新前沿论文分享">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Test-Time_Reinforcement_Learning_for_GUI_Grounding_via_Region_Consistency.jpg">
<meta property="article:published_time" content="2025-08-11T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-19T08:49:44.755Z">
<meta property="article:author" content="具身ADT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Test-Time_Reinforcement_Learning_for_GUI_Grounding_via_Region_Consistency.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/11报纸 - 最新前沿论文分享</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ywfhhh.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文库</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/11报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-11 00:00" pubdate>
          星期一, 八月 11日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          22k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/11报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/11报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-11<br>📄 <strong>发现论文数量</strong>：149  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency"><a href="#1-Test-Time-Reinforcement-Learning-for-GUI-Grounding-via-Region-Consistency" class="headerlink" title="1. Test-Time Reinforcement Learning for GUI Grounding via Region Consistency"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Test-Time_Reinforcement_Learning_for_GUI_Grounding_via_Region_Consistency.pdf">Test-Time Reinforcement Learning for GUI Grounding via Region Consistency</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出了GUI-RC（Region Consistency）与GUI-RCPO（Region Consistency Policy Optimization）两种测试时优化方法，用于提升GUI grounding任务（自然语言到界面坐标映射）的准确率。GUI-RC通过多次采样模型输出并空间投票得到一致性区域，无需额外标注即可提升模型定位能力；GUI-RCPO则将区域一致性转化为奖励信号，在测试时进行自监督强化学习训练，使模型在无标注数据上自我优化。实验表明方法在多个主流模型和基准上平均提升2-5%的准确率，并可自举式迭代提升，展现了测试时优化的新潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Test-Time_Reinforcement_Learning_for_GUI_Grounding_via_Region_Consistency.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-Discovering-Interpretable-Programmatic-Policies-via-Multimodal-LLM-assisted-Evolutionary-Search"><a href="#2-Discovering-Interpretable-Programmatic-Policies-via-Multimodal-LLM-assisted-Evolutionary-Search" class="headerlink" title="2. Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Discovering_Interpretable_Programmatic_Policies_via_Multimodal_LLM-assisted_Evolutionary_Search.pdf">Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">City University of Hong Kong</span></p>
<p>本文提出了一种新的可解释程序化策略发现方法——多模态大语言模型辅助进化搜索（MLES），该方法将多模态大语言模型（MLLMs）与进化优化结合，直接生成带有自然语言解释的程序化策略。MLES在进化过程中融合了视觉反馈驱动的行为分析，有效提升了策略发现效率和可解释性。实验证明，MLES在Lunar Lander和Car Racing两个典型控制任务上，策略性能与PPO相当，并显著提升了策略的透明度和知识可复用性，为下一代可解释策略发现提供了新范式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Discovering_Interpretable_Programmatic_Policies_via_Multimodal_LLM-assisted_Evolutionary_Search.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-ECHO-Decoupling-Inference-and-Training-for-Large-Scale-RL-Alignment-on-Heterogeneous-Swarms"><a href="#3-ECHO-Decoupling-Inference-and-Training-for-Large-Scale-RL-Alignment-on-Heterogeneous-Swarms" class="headerlink" title="3. ECHO: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Echo__Decoupling_Inference_and_Training_for_Large-Scale_RL_Alignment_on_Heterogeneous_Swarms.pdf">ECHO: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Gradient</span></p>
<p>本文提出了ECHO系统，通过将RL中的推理（trajectory sampling）和训练（policy optimization）在异构集群上彻底解耦，设计了顺序拉取和异步推送两种轻量级同步协议，实现了在边缘设备和数据中心硬件之间灵活协作。实验表明，ECHO在Qwen系列大模型上的RL任务中，既能提升硬件利用率，也能与传统全同位系统在收敛速度和最终性能持平甚至超越，验证了去中心化RL训练的可行性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/ECHO_Decoupling_Inference_and_Training_for_Large-Scale_RL_Alignment_on_Heterogeneous_Swarms.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation"><a href="#4-Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation" class="headerlink" title="4. Posterior-GRPO: Rewarding Reasoning Processes in Code Generation"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Posterior-GRPO__Rewarding_Reasoning_Processes_in_Code_Generation.pdf">Posterior-GRPO: Rewarding Reasoning Processes in Code Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The State Key Laboratory of Blockchain and Data Security</span></p>
<p>本论文提出了一种新的强化学习方法Posterior-GRPO (P-GRPO)用于大语言模型代码生成，创新性地将基于优化-降级（OD-based）方法训练的思维奖励模型与传统结果奖励结合，通过在仅有正确结果时赋予思维奖励，有效防止奖励投机。实验证明，P-GRPO在多个代码与数学推理基准上提升Pass@1平均4.5%，表现与GPT-4-Turbo相当，且泛化能力强。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Posterior-GRPO_Rewarding_Reasoning_Processes_in_Code_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-Domain-driven-Metrics-for-Reinforcement-Learning-A-Case-Study-on-Epidemic-Control-using-Agent-based-Simulation"><a href="#5-Domain-driven-Metrics-for-Reinforcement-Learning-A-Case-Study-on-Epidemic-Control-using-Agent-based-Simulation" class="headerlink" title="5. Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Domain-driven_Metrics_for_Reinforcement_Learning__A_Case_Study_on_Epidemic_Control_using_Agent-based.pdf">Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Thoughtworks Technologies</span></p>
<p>本文提出了结合领域知识和传统奖励的‘Domain-driven RL metrics’评价指标体系，用于公正、稳健地比较用于理性体建模与优化的强化学习算法。通过在疫情控制的代理建模案例中，利用多种RL算法和新的复合评价指标（如状态空间覆盖、最佳序列百分比等），有效提升了算法选择的可靠性和解释性，结论显示新指标体系比单一均值奖励更稳健、更具信任度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Domain-driven_Metrics_for_Reinforcement_Learning_A_Case_Study_on_Epidemic_Control_using_Agent-based_Simulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-Towards-Hallucination-Free-Music-A-Reinforcement-Learning-Preference-Optimization-Framework-for-Reliable-Song-Generation"><a href="#6-Towards-Hallucination-Free-Music-A-Reinforcement-Learning-Preference-Optimization-Framework-for-Reliable-Song-Generation" class="headerlink" title="6. Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Hallucination-Free_Music__A_Reinforcement_Learning_Preference_Optimization_Framework_for_Rel.pdf">Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wuhan University</span></p>
<p>本论文提出了一个基于强化学习的偏好优化框架，用于减少歌词到歌曲生成过程中的幻觉现象（即生成内容与歌词不符）。方法包括构建基于音素错误率（PER）的幻觉偏好数据集，并实现三种RL偏好优化策略：DPO、PPO和GRPO，通过奖励建模和对比学习，有效抑制了幻觉的发生。实验表明，该框架能显著降低幻觉比例（如DPO+RS方法幻觉样本下降16.5%，高质量样本提升38.47%），且不损失音乐性和音质，具备良好迁移性，可扩展到风格控制等其他生成任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Hallucination-Free_Music_A_Reinforcement_Learning_Preference_Optimization_Framework_for_Reliable_Song_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-Anti-Jamming-Sensing-with-Distributed-Reconfigurable-Intelligent-Metasurface-Antennas"><a href="#7-Anti-Jamming-Sensing-with-Distributed-Reconfigurable-Intelligent-Metasurface-Antennas" class="headerlink" title="7. Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Anti-Jamming_Sensing_with_Distributed_Reconfigurable_Intelligent_Metasurface_Antennas.pdf">Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong University</span></p>
<p>本文提出利用分布式可重构智能超表面天线（RIMSA）进行射频感知，通过策略网络（深度强化学习）优化波束形成模式及神经网络映射接收信号，实现端到端感知。为抵抗干扰，设计了融合接收信号SINR的多因子损失函数，显著提升了抗干扰下的感知精度，仿真表明分布式RIMSA系统对环境及攻击有更强适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Anti-Jamming_Sensing_with_Distributed_Reconfigurable_Intelligent_Metasurface_Antennas.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation"><a href="#8-Genie-Envisioner-A-Unified-World-Foundation-Platform-for-Robotic-Manipulation" class="headerlink" title="8. Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Genie_Envisioner__A_Unified_World_Foundation_Platform_for_Robotic_Manipulation.pdf">Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AgiBot Genie Team</span></p>
<p>该论文提出Genie Envisioner (GE)，一个集视频生成、策略学习、评估和仿真于一体的机器人操作世界基础平台。GE包括三大模块：GE-Base为大规模指令条件视频扩散模型，捕捉真实机器人交互的空间、时间和语义动态；GE-Act通过轻量化解码器将视觉表征映射为可执行动作轨迹，实现多平台泛化和高效策略推断；GE-Sim为动作条件神经模拟器，支持高保真闭环策略评估与并行仿真。平台还配备EWMBench标准化评测工具，综合衡量视觉真实度、物理一致性和指令-动作对齐。实验证明GE在多种机器人平台和复杂任务上均优于现有方法，具备极强的通用性和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Genie_Envisioner_A_Unified_World_Foundation_Platform_for_Robotic_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-TRAJEVO-Trajectory-Prediction-Heuristics-Design-via-LLM-driven-Evolution"><a href="#9-TRAJEVO-Trajectory-Prediction-Heuristics-Design-via-LLM-driven-Evolution" class="headerlink" title="9. TRAJEVO: Trajectory Prediction Heuristics Design via LLM-driven Evolution"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/TrajEvo__Trajectory_Prediction_Heuristics_Design_via_LLM-driven_Evolution.pdf">TRAJEVO: Trajectory Prediction Heuristics Design via LLM-driven Evolution</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">KAIST, Korea Advanced Institute of Science and Technology</span></p>
<p>该论文提出TRAJEVO框架，通过结合大语言模型（LLM）与进化算法自动生成和优化轨迹预测启发式方法，利用Cross-Generation Elite Sampling和Statistics Feedback Loop增强启发式多样性与性能。实验结果显示，TRAJEVO在多数据集上超越传统启发式方法，并且在未见分布场景下的泛化能力优于深度学习模型，同时具备高速度和可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/TRAJEVO_Trajectory_Prediction_Heuristics_Design_via_LLM-driven_Evolution.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-OMNIEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks"><a href="#10-OMNIEAR-Benchmarking-Agent-Reasoning-in-Embodied-Tasks" class="headerlink" title="10. OMNIEAR: Benchmarking Agent Reasoning in Embodied Tasks"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/OmniEAR__Benchmarking_Agent_Reasoning_in_Embodied_Tasks.pdf">OMNIEAR: Benchmarking Agent Reasoning in Embodied Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>OmniEAR提出了一个用于评估大语言模型在具身任务中推理能力的综合框架，包括EAR-Sim环境建模、自动化场景生成和EAR-Bench系统性评测，覆盖物理属性推理、工具使用与多智能体协作。结论显示：现有语言模型在需要从物理约束推理时性能严重下降，特别是在工具推理和隐式协作任务中，表明具身推理对AI架构提出了新的挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/OMNIEAR_Benchmarking_Agent_Reasoning_in_Embodied_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-CleanUpBench-Embodied-Sweeping-and-Grasping-Benchmark"><a href="#11-CleanUpBench-Embodied-Sweeping-and-Grasping-Benchmark" class="headerlink" title="11. CleanUpBench: Embodied Sweeping and Grasping Benchmark"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/CleanUpBench__Embodied_Sweeping_and_Grasping_Benchmark.pdf">CleanUpBench: Embodied Sweeping and Grasping Benchmark</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sichuan University</span></p>
<p>本文提出了CleanUpBench，一个面向现实家居环境中服务机器人双模态（扫地与抓取）清洁任务的高保真仿真基准。平台基于NVIDIA Isaac Sim，支持多场景、多机器人协作，并提供系统化评测指标（空间覆盖、任务完成率、运动质量、计算效率等），并内置多种启发式、规划与学习基线方法。实验表明，具备多智能体强化学习能力的双模态算法（如PRIMAL2）在复杂清洁任务中显著优于仅具备单一模式的传统方法，验证了CleanUpBench作为评测和推动现实服务机器人智能发展的价值和扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/CleanUpBench_Embodied_Sweeping_and_Grasping_Benchmark.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-Towards-Generalizable-Safety-in-Crowd-Navigation-via-Conformal-Uncertainty-Handling"><a href="#12-Towards-Generalizable-Safety-in-Crowd-Navigation-via-Conformal-Uncertainty-Handling" class="headerlink" title="12. Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Generalizable_Safety_in_Crowd_Navigation_via_Conformal_Uncertainty_Handling.pdf">Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Riverside</span></p>
<p>本文提出了一种结合自适应保形不确定性量化(ACI)与约束强化学习(CRL)的移动机器人群体导航方法。方法流程为：首先通过规则或学习式轨迹预测模型估算人类未来轨迹，并用ACI在线量化预测不确定性，将不确定性估计作为观察输入，结合注意力机制编码人与机器人及人-人交互，最后用CRL约束机器人决策以减少对人类未来轨迹的不安全侵入。实验结果显示，在分布内及三种分布外场景（速度变化、行为策略变化、群体动态）中，该方法安全性和成功率均显著优于现有SOTA基线，且能够直接迁移到实际机器人平台实现安全稳健导航。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Generalizable_Safety_in_Crowd_Navigation_via_Conformal_Uncertainty_Handling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-A-Multi-view-Landmark-Representation-Approach-with-Application-to-GNSS-Visual-Inertial-Odometry"><a href="#13-A-Multi-view-Landmark-Representation-Approach-with-Application-to-GNSS-Visual-Inertial-Odometry" class="headerlink" title="13. A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Multi-view_Landmark_Representation_Approach_with_Application_to_GNSS-Visual-Inertial_Odometry.pdf">A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了一种多视角的仅位姿特征点表示方法，并将其应用于GNSS-视觉-惯性里程计（GVIO）系统，核心技术包括多视角几何特征建模、滤波更新与特征管理策略。实验表明该方法在仿真和真实环境中提升了定位效率和精度，保持滤波一致性且计算负担更低。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Multi-view_Landmark_Representation_Approach_with_Application_to_GNSS-Visual-Inertial_Odometry.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-Information-Theoretic-Graph-Fusion-with-Vision-Language-Action-Model-for-Policy-Reasoning-and-Dual-Robotic-Control"><a href="#14-Information-Theoretic-Graph-Fusion-with-Vision-Language-Action-Model-for-Policy-Reasoning-and-Dual-Robotic-Control" class="headerlink" title="14. Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Information-Theoretic_Graph_Fusion_with_Vision-Language-Action_Model_for_Policy_Reasoning_and_Dual_R.pdf">Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hangzhou Dianzi University</span></p>
<p>本文提出了一种信息论图融合视觉-语言-动作（GF-VLA）框架，通过从人类演示视频中提取基于Shannon信息的手-物体和物体-物体交互，构建时序场景图，并与语言条件Transformer结合，生成可解释的行为树和笛卡尔动作指令。通过链式推理（CoT）、自验证和跨手选择策略实现双臂机器人任务推理与高效协作，实验在多种装配和泛化任务中取得了高于95%图准确率和90%任务成功率，展现了优异的泛化与稳健性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Information-Theoretic_Graph_Fusion_with_Vision-Language-Action_Model_for_Policy_Reasoning_and_Dual_Robotic_Control.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-DistillDrive-End-to-End-Multi-Mode-Autonomous-Driving-Distillation-by-Isomorphic-Hetero-Source-Planning-Model"><a href="#15-DistillDrive-End-to-End-Multi-Mode-Autonomous-Driving-Distillation-by-Isomorphic-Hetero-Source-Planning-Model" class="headerlink" title="15. DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/DistillDrive__End-to-End_Multi-Mode_Autonomous_Driving_Distillation_by_Isomorphic_Hetero-Source_Plan.pdf">DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">East China University of Science and Technology</span></p>
<p>该论文提出DistillDrive，一种结合知识蒸馏、强化学习和生成模型的端到端多模态自动驾驶框架。方法以结构化场景规划模型为教师，通过多模实例监督提升学生模型的运动特征学习，并引入强化学习优化状态-决策映射，利用生成模型实现运动分布特征的隐空间交互。实验结果显示，DistillDrive在nuScenes和NAVSIM数据集上较基线模型碰撞率下降50%，闭环性能提升3分，有效增强了自动驾驶计划的多样性和安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/DistillDrive_End-to-End_Multi-Mode_Autonomous_Driving_Distillation_by_Isomorphic_Hetero-Source_Planning_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-Towards-Embodied-Agentic-AI-Review-and-Classification-of-LLM-and-VLM-Driven-Robot-Autonomy-and-Interaction"><a href="#16-Towards-Embodied-Agentic-AI-Review-and-Classification-of-LLM-and-VLM-Driven-Robot-Autonomy-and-Interaction" class="headerlink" title="16. Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Embodied_Agentic_AI__Review_and_Classification_of_LLM-_and_VLM-Driven_Robot_Autonomy_and_Int.pdf">Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Turku</span></p>
<p>该论文综述了大语言模型（LLM）和视觉语言模型（VLM）在机器人自主性和人机交互中的集成方法，提出了四类集成范式（协议、接口、编排和嵌入）及多种智能体角色的分类体系。结论认为，Agentic AI系统作为用户与机器人间的智能中介，提升了机器人灵活性和可扩展性，并展望了未来在实际部署、记忆、安全性和高阶智能等方面的研究挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Embodied_Agentic_AI_Review_and_Classification_of_LLM-_and_VLM-Driven_Robot_Autonomy_and_Interaction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-Driver-Assistant-Persuading-Drivers-to-Adjust-Secondary-Tasks-Using-Large-Language-Models"><a href="#17-Driver-Assistant-Persuading-Drivers-to-Adjust-Secondary-Tasks-Using-Large-Language-Models" class="headerlink" title="17. Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Driver_Assistant__Persuading_Drivers_to_Adjust_Secondary_Tasks_Using_Large_Language_Models.pdf">Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出一种基于大语言模型（LLM）的驾驶辅助工具，通过道路风险评估与眼动追踪及时判断并生成人性化劝导内容，引导驾驶员在L3自动驾驶中合理分配注意力。实验结果显示，该系统能够有效降低驾驶员分心任务数量和认知负荷，提升驾驶安全性及体验。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Driver_Assistant_Persuading_Drivers_to_Adjust_Secondary_Tasks_Using_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-EndoMatcher-Generalizable-Endoscopic-Image-Matcher-via-Multi-Domain-Pre-training-for-Robot-Assisted-Surgery"><a href="#18-EndoMatcher-Generalizable-Endoscopic-Image-Matcher-via-Multi-Domain-Pre-training-for-Robot-Assisted-Surgery" class="headerlink" title="18. EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EndoMatcher__Generalizable_Endoscopic_Image_Matcher_via_Multi-Domain_Pre-training_for_Robot-Assisted.pdf">EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Automation, Chinese Academy of Sciences</span></p>
<p>该论文提出了EndoMatcher，一种基于双分支Vision Transformer的通用内镜图像匹配网络，通过多域大规模数据集Endo-Mix6的预训练和渐进式多目标优化实现对弱纹理、视角大变化等挑战场景下的鲁棒密集匹配。结果显示，EndoMatcher在多个零样本内镜数据集上大幅提升匹配数量和精度，实现了跨器官和成像条件的高效泛化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EndoMatcher_Generalizable_Endoscopic_Image_Matcher_via_Multi-Domain_Pre-training_for_Robot-Assisted_Surgery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-GhostShell-Streaming-LLM-Function-Calls-for-Concurrent-Embodied-Programming"><a href="#19-GhostShell-Streaming-LLM-Function-Calls-for-Concurrent-Embodied-Programming" class="headerlink" title="19. GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/GhostShell__Streaming_LLM_Function_Calls_for_Concurrent_Embodied_Programming.pdf">GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Leapwatt Robotics</span></p>
<p>本文提出GhostShell，一种利用大语言模型（LLM）实现机器人多通道并发行为编程的系统，核心包括流式XML函数令牌解析、动态接口映射和多通道同步&#x2F;异步调度，支持机器人各部件的并行与串行指令执行。实验在COCO机器人上，通过34项任务验证方法有效性，在Claude-4-Sonnet模型上达到0.85的行为正确率，响应速度比传统函数调用快66倍，适用于长时多模态任务，展现出高度通用性和实时性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/GhostShell_Streaming_LLM_Function_Calls_for_Concurrent_Embodied_Programming.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-Advanced-Hybrid-Transformer–LSTM-Technique-with-Attention-and-TS-Mixer-for-Drilling-Rate-of-Penetration-Prediction"><a href="#20-Advanced-Hybrid-Transformer–LSTM-Technique-with-Attention-and-TS-Mixer-for-Drilling-Rate-of-Penetration-Prediction" class="headerlink" title="20. Advanced Hybrid Transformer–LSTM Technique with Attention and TS-Mixer for Drilling Rate of Penetration Prediction"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Advanced_Hybrid_Transformer_LSTM_Technique_with_Attention_and_TS_Mixer_for_Drilling_Rate_of_Penetrat.pdf">Advanced Hybrid Transformer–LSTM Technique with Attention and TS-Mixer for Drilling Rate of Penetration Prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Engineering and Applied Sciences (UEAS), Swat, Pakistan</span></p>
<p>该论文提出一种结合LSTM、Transformer编码器、TS-Mixer模块和注意力机制的混合深度学习架构，用于钻井穿透率（ROP）预测。方法包括数据清洗、特征标准化、架构设计（LSTM捕捉时序依赖，Transformer建模全局关系，TS-Mixer融合静态特征，注意力机制动态分配特征权重），并在真实钻井数据集上进行评估。结果显示，所提模型在多项回归指标上显著优于传统方法，R²达0.9991，MAPE低至1.1572%，具备强泛化能力和实时预测价值。结论是该混合模型能稳定并精确预测ROP，显著提升钻井优化系统的智能化与经济性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Advanced_Hybrid_Transformer%E2%80%93LSTM_Technique_with_Attention_and_TS-Mixer_for_Drilling_Rate_of_Penetration_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-Learning-to-See-and-Act-Task-Aware-View-Planning-for-Robotic-Manipulation"><a href="#21-Learning-to-See-and-Act-Task-Aware-View-Planning-for-Robotic-Manipulation" class="headerlink" title="21. Learning to See and Act: Task-Aware View Planning for Robotic Manipulation"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Learning_to_See_and_Act__Task-Aware_View_Planning_for_Robotic_Manipulation.pdf">Learning to See and Act: Task-Aware View Planning for Robotic Manipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science and Engineering, Sun Yat-sen University</span></p>
<p>本文提出了Task-Aware View Planning (TAVP) 框架，将主动多视角探索与任务感知特征提取相结合，提升机器人多任务操作的鲁棒性与泛化能力。通过多视角探索策略和任务感知专家混合（TaskMoE）视觉编码器，TAVP显著提升了在RLBench等多任务操作环境中的准确率和鲁棒性，能有效克服遮挡问题并提升对新任务的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Learning_to_See_and_Act_Task-Aware_View_Planning_for_Robotic_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-PhysPatch-A-Physically-Realizable-and-Transferable-Adversarial-Patch-Attack-for-Multimodal-Large-Language-Models-based-Autonomous-Driving-Systems"><a href="#22-PhysPatch-A-Physically-Realizable-and-Transferable-Adversarial-Patch-Attack-for-Multimodal-Large-Language-Models-based-Autonomous-Driving-Systems" class="headerlink" title="22. PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/PhysPatch__A_Physically_Realizable_and_Transferable_Adversarial_Patch_Attack_for_Multimodal_Large_La.pdf">PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Software Engineering, Xi’an Jiaotong University</span></p>
<p>本文提出PhysPatch，一种针对多模态大语言模型(MLLMs)驱动的自动驾驶系统的物理可实现且高迁移性对抗补丁攻击框架。方法上，PhysPatch通过语义感知的掩码初始化、SVD分解的局部特征对齐损失以及补丁引导的裁剪-缩放策略，实现了补丁位置、形状和内容的联合优化，并采用自适应势场掩码细化提升物理可部署性。实验证明，PhysPatch在多种开源、商用和推理型MLLMs上均大幅优于现有方法，能有效误导自动驾驶系统感知与规划输出，并兼具高物理可部署性与对抗性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/PhysPatch_A_Physically_Realizable_and_Transferable_Adversarial_Patch_Attack_for_Multimodal_Large_Language_Models-based_Autonomous_Driving_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-Examining-the-legibility-of-humanoid-robot-arm-movements-in-a-pointing-task"><a href="#23-Examining-the-legibility-of-humanoid-robot-arm-movements-in-a-pointing-task" class="headerlink" title="23. Examining the legibility of humanoid robot arm movements in a pointing task"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Examining_the_legibility_of_humanoid_robot_arm_movements_in_a_pointing_task.pdf">Examining the legibility of humanoid robot arm movements in a pointing task</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Comenius University Bratislava</span></p>
<p>该论文设计了基于NICO仿人机器人，结合凝视与指向手势的实验，考察人在观察截断机器人手臂运动时对目标预测的准确性和反应速度。结果支持多模态优势和眼动优先假设，表明凝视和指向的组合能显著提高意图识别的准确性，凝视信号可加速人类决策，为人机交互中运动设计提供理论依据。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Examining_the_legibility_of_humanoid_robot_arm_movements_in_a_pointing_task.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-A-Study-of-the-Framework-and-Real-World-Applications-of-Language-Embedding-for-3D-Scene-Understanding"><a href="#24-A-Study-of-the-Framework-and-Real-World-Applications-of-Language-Embedding-for-3D-Scene-Understanding" class="headerlink" title="24. A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Study_of_the_Framework_and_Real-World_Applications_of_Language_Embedding_for_3D_Scene_Understandin.pdf">A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Victoria</span></p>
<p>本论文系统综述了将大语言模型（LLMs）及语言嵌入与3D高斯投影（Gaussian Splatting, 3DGS）结合以实现三维场景理解的方法和应用。其方法梳理了3DGS的理论基础、语言特征与视觉模型的集成策略，以及在机器人、虚拟现实、自动驾驶等真实环境中的应用实例。结论指出，尽管这些技术推动了3D场景的语义理解和机器人智能交互，但仍面临计算消耗高、语义泛化能力有限及实时交互工具不足等挑战，未来需在高效可扩展和跨场景泛化等方面持续突破。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Study_of_the_Framework_and_Real-World_Applications_of_Language_Embedding_for_3D_Scene_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-Will-You-Be-Aware-Eye-Tracking–Based-Modeling-of-Situational-Awareness-in-Augmented-Reality"><a href="#25-Will-You-Be-Aware-Eye-Tracking–Based-Modeling-of-Situational-Awareness-in-Augmented-Reality" class="headerlink" title="25. Will You Be Aware? Eye Tracking–Based Modeling of Situational Awareness in Augmented Reality"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Will_You_Be_Aware__Eye_Tracking-Based_Modeling_of_Situational_Awareness_in_Augmented_Reality.pdf">Will You Be Aware? Eye Tracking–Based Modeling of Situational Awareness in Augmented Reality</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Duke University</span></p>
<p>该论文提出了一种基于眼动追踪的情境意识（SA）建模方法，针对AR辅助心肺复苏（CPR）任务，开发了实时指导的AR应用并通过模拟突发事件收集用户眼动数据。提出了FixGraphPool图神经网络，将注视点和扫视事件结构化为时空图，实现了对用户SA水平的预测，准确率达83%，优于传统机器学习和先进时序模型。结论显示眼动追踪可有效建模AR环境下的SA，并为安全关键场景的AR系统设计提供了指导。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Will_You_Be_Aware_Eye_Tracking%E2%80%93Based_Modeling_of_Situational_Awareness_in_Augmented_Reality.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-From-Canada-to-Japan-How-10-000-km-Affect-User-Perception-in-Robot-Teleoperation"><a href="#26-From-Canada-to-Japan-How-10-000-km-Affect-User-Perception-in-Robot-Teleoperation" class="headerlink" title="26. From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/From_Canada_to_Japan__How_10,000_km_Affect_User_Perception_in_Robot_Teleoperation.pdf">From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tokyo University of Science</span></p>
<p>本论文研究了长距离机器人远程操作（RTo）对用户感知的影响，特别关注其在老年护理领域的应用。作者设计了基于ROS和Unity的远程操控系统及用户评估协议，通过问卷和交互实验比较本地与跨国（加拿大至日本）机器人操作体验。结果显示，无论是操作负担还是可用性，用户对远程和本地机器人无显著差异，表明远程机器人可作为本地控制的可行替代方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/From_Canada_to_Japan_How_10%2C000_km_Affect_User_Perception_in_Robot_Teleoperation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-Analyzing-the-Impact-of-Multimodal-Perception-on-Sample-Complexity-and-Optimization-Landscapes-in-Imitation-Learning"><a href="#27-Analyzing-the-Impact-of-Multimodal-Perception-on-Sample-Complexity-and-Optimization-Landscapes-in-Imitation-Learning" class="headerlink" title="27. Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Analyzing_the_Impact_of_Multimodal_Perception_on_Sample_Complexity_and_Optimization_Landscapes_in_Im.pdf">Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Berkeley</span></p>
<p>本文基于统计学习理论系统分析了多模态感知（RGB-D、语言、位姿）在机器人模仿学习中的作用，采用Rademacher复杂度、PAC学习等理论工具，揭示多模态集成可显著降低样本复杂度并改善优化景观。结论指出，合理融合多模态信息能在机器人操作任务中实现更强泛化能力、更高数据效率和更稳定训练表现，理论与仿真实验均得到了验证。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Analyzing_the_Impact_of_Multimodal_Perception_on_Sample_Complexity_and_Optimization_Landscapes_in_Imitation_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-Hierarchical-Deep-Deterministic-Policy-Gradient-for-Autonomous-Maze-Navigation-of-Mobile-Robots"><a href="#28-Hierarchical-Deep-Deterministic-Policy-Gradient-for-Autonomous-Maze-Navigation-of-Mobile-Robots" class="headerlink" title="28. Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Hierarchical_Deep_Deterministic_Policy_Gradient_for_Autonomous_Maze_Navigation_of_Mobile_Robots.pdf">Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universiti Sains Malaysia</span></p>
<p>本文提出了一种高效的分层深度确定性策略梯度（HDDPG）算法用于移动机器人自主迷宫导航。该方法通过高层策略生成中间子目标并低层策略实现具体动作，结合自适应参数噪声、离策略修正、梯度裁剪和奖励函数重塑，显著提升了探索效率、路径规划和训练稳定性。在ROS和Gazebo仿真平台上的实验表明，HDDPG在多个迷宫导航任务中将成功率提升至少56.59%，平均奖励提升519.03，明显优于标准DDPG及其变体，验证了分层结构和各项优化的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Hierarchical_Deep_Deterministic_Policy_Gradient_for_Autonomous_Maze_Navigation_of_Mobile_Robots.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="29-GAP-Gaussianize-Any-Point-Clouds-with-Text-Guidance"><a href="#29-GAP-Gaussianize-Any-Point-Clouds-with-Text-Guidance" class="headerlink" title="29. GAP: Gaussianize Any Point Clouds with Text Guidance"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/GAP__Gaussianize_Any_Point_Clouds_with_Text_Guidance.pdf">GAP: Gaussianize Any Point Clouds with Text Guidance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出GAP方法，将无颜色的3D点云在文本引导下转化为高保真3D高斯表示。其核心包括多视角优化框架，结合深度感知的图像扩散模型和表面锚定机制保证几何精度，并引入基于扩散的高斯补全策略修复难以观测区域。实验结果显示，GAP在多种数据集和场景下均优于现有方法，能高效生成结构精确、外观多样的3D高斯体。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/GAP_Gaussianize_Any_Point_Clouds_with_Text_Guidance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-Uni-CoT-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision"><a href="#30-Uni-CoT-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision" class="headerlink" title="30. Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Uni-cot__Towards_Unified_Chain-of-Thought_Reasoning_Across_Text_and_Vision.pdf">Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Academy of AI for Science</span></p>
<p>该论文提出了Uni-CoT，一种统一的链式思维(CoT)框架，采用宏观任务规划与微观子任务执行的双层层次化推理结构，将多模态推理任务分解为高层规划和马尔可夫决策过程(MDP)建模的子任务，通过结构化训练范式实现高效监督和偏好微调。实验表明，Uni-CoT在多项推理驱动的图像生成和编辑任务中表现出色，在WISE、RISE和KRIS等基准上取得了领先性能和良好解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Uni-CoT_Towards_Unified_Chain-of-Thought_Reasoning_Across_Text_and_Vision.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-DART-Dual-Adaptive-Refinement-Transfer-for-Open-Vocabulary-Multi-Label-Recognition"><a href="#31-DART-Dual-Adaptive-Refinement-Transfer-for-Open-Vocabulary-Multi-Label-Recognition" class="headerlink" title="31. DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/DART__Dual_Adaptive_Refinement_Transfer_for_Open-Vocabulary_Multi-Label_Recognition.pdf">DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-sen University</span></p>
<p>本论文提出DART框架，通过在冻结的视觉-语言预训练(VLP)模型基础上，集成自适应细化模块(ARM)与弱监督Patch选择损失(WPS)，实现对局部视觉特征的精细定位，同时引入基于大语言模型(LLM)挖掘的类关系图(CRG)指导自适应转移模块(ATM)，以图注意力网络完成跨类别信息传递和多模态融合。实验结果显示，DART在NUS-WIDE、MS-COCO和Open Images等多项开放词表多标签识别基准上取得了新的SOTA性能，验证了方法的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/DART_Dual_Adaptive_Refinement_Transfer_for_Open-Vocabulary_Multi-Label_Recognition.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-χ-Symmetry-Understanding-of-3D-Shapes-via-Chirality-Disentanglement"><a href="#32-χ-Symmetry-Understanding-of-3D-Shapes-via-Chirality-Disentanglement" class="headerlink" title="32. χ: Symmetry Understanding of 3D Shapes via Chirality Disentanglement"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Symmetry_Understanding_of_3D_Shapes_via_Chirality_Disentanglement.pdf">χ: Symmetry Understanding of 3D Shapes via Chirality Disentanglement</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Bonn</span></p>
<p>本文提出了一种基于Diff3F框架的无监督管道，通过从2D基础视觉模型（DINO-V2和StableDiffusion）提取特征，实现3D形状顶点手性（chirality）信息的解耦与建模。新特征能有效区分3D形状的左右部分，解决了传统特征在形状匹配和分割时的左右歧义，在多数据集和任务中表现出色，且对局部或各向异性形状具有良好鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/%CF%87_Symmetry_Understanding_of_3D_Shapes_via_Chirality_Disentanglement.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-SMOL-MapSeg-Show-Me-One-Label"><a href="#33-SMOL-MapSeg-Show-Me-One-Label" class="headerlink" title="33. SMOL-MapSeg: Show Me One Label"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SMOL-MapSeg__Show_Me_One_Label.pdf">SMOL-MapSeg: Show Me One Label</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Leibniz University Hannover</span></p>
<p>本文提出了一种新颖的按需声明式（OND）知识驱动的提示方法，用于指导基础分割模型（以SAM为基础）在历史地图上进行语义分割。方法通过源图像-标签对作为显式提示，结合新设计的Prompt Encoder，对历史地图进行单样本细粒度分割，且支持新类别的少样本自适应。实验表明，SMOL-MapSeg在平均IoU上优于UNet基线模型，对新类别有较好泛化能力，但对局部特征不明显的类别如白色水域分割有局限。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SMOL-MapSeg_Show_Me_One_Label.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-How-and-Why-Taming-Flow-Matching-for-Unsupervised-Anomaly-Detection-and-Localization"><a href="#34-How-and-Why-Taming-Flow-Matching-for-Unsupervised-Anomaly-Detection-and-Localization" class="headerlink" title="34. How and Why: Taming Flow Matching for Unsupervised Anomaly Detection and Localization"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/How_and_Why__Taming_Flow_Matching_for_Unsupervised_Anomaly_Detection_and_Localization.pdf">How and Why: Taming Flow Matching for Unsupervised Anomaly Detection and Localization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>本文提出了基于流匹配（Flow Matching, FM）及其变体“Worst Transport Flow Matching”（WT-Flow）的无监督异常检测与定位新范式，通过理论和实证分析揭示了传统流方法在高维空间中的不可逆性和表达能力限制，并构建了非概率性路径控制机制。WT-Flow方法通过对样本归一化和潜在井构建，实现了对异常样本的有效分离，在MVTec工业视觉异常检测数据集上达到了单尺度下的SOTA性能，显著提升了检测效率和精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/How_and_Why_Taming_Flow_Matching_for_Unsupervised_Anomaly_Detection_and_Localization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-Explaining-Similarity-in-Vision–Language-Encoders-with-Weighted-Banzhaf-Interactions"><a href="#35-Explaining-Similarity-in-Vision–Language-Encoders-with-Weighted-Banzhaf-Interactions" class="headerlink" title="35. Explaining Similarity in Vision–Language Encoders with Weighted Banzhaf Interactions"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Explaining_Similarity_in_Vision-Language_Encoders_with_Weighted_Banzhaf_Interactions.pdf">Explaining Similarity in Vision–Language Encoders with Weighted Banzhaf Interactions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Warsaw</span></p>
<p>本文提出了FIXLIP，一种基于加权Banzhaf交互指数的视觉-语言编码器解释方法，通过游戏理论建模和高效的跨模态采样策略，能够分解图像-文本对在编码器中的相似性得分。实验结果表明，FIXLIP在MS COCO和ImageNet-1k等基准上比一阶归因方法更准确、可扩展，并支持模型间对比分析。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Explaining_Similarity_in_Vision%E2%80%93Language_Encoders_with_Weighted_Banzhaf_Interactions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation"><a href="#36-UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation" class="headerlink" title="36. UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/UNCAGE__Contrastive_Attention_Guidance_for_Masked_Generative_Transformers_in_Text-to-Image_Generatio.pdf">UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Seoul National University</span></p>
<p>该论文提出UNCAGE（Unmasking with Contrastive Attention Guidance），为Masked Generative Transformers（MGTs）在文本到图像生成中的多物体和属性绑定问题提供首个训练免方法。UNCAGE利用注意力图，通过正负对比机制指导token解遮顺序，优先解锁能清晰代表单一对象的token，提升合成一致性且推理开销极小。实验表明UNCAGE在多基准和多指标下均优于现有方法，提升了文本-图像对齐和人类偏好。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/UNCAGE_Contrastive_Attention_Guidance_for_Masked_Generative_Transformers_in_Text-to-Image_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-MagicHOI-Leveraging-3D-Priors-for-Accurate-Hand-object-Reconstruction-from-Short-Monocular-Video-Clips"><a href="#37-MagicHOI-Leveraging-3D-Priors-for-Accurate-Hand-object-Reconstruction-from-Short-Monocular-Video-Clips" class="headerlink" title="37. MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MagicHOI__Leveraging_3D_Priors_for_Accurate_Hand-object_Reconstruction_from_Short_Monocular_Video_Cl.pdf">MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>MagicHOI提出了一种结合结构光恢复、NVS扩散模型和可见性加权策略的方法，从短时单目视频中实现准确的手与物体三维重建，包括处理遮挡区域。实验证明，该方法在有限视角和遮挡严重的场景下，重建质量和手物体空间关系均优于当前主流方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MagicHOI_Leveraging_3D_Priors_for_Accurate_Hand-object_Reconstruction_from_Short_Monocular_Video_Clips.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-F2PASeg-Feature-Fusion-for-Pituitary-Anatomy-Segmentation-in-Endoscopic-Surgery"><a href="#38-F2PASeg-Feature-Fusion-for-Pituitary-Anatomy-Segmentation-in-Endoscopic-Surgery" class="headerlink" title="38. F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/F2PASeg__Feature_Fusion_for_Pituitary_Anatomy_Segmentation_in_Endoscopic_Surgery.pdf">F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science &amp; Innovation, Chinese Academy of Sciences</span></p>
<p>该论文提出了F2PASeg方法，通过在视频级语义分割中引入特征融合模块和LoRA分支，结合高分辨率图像特征与深层语义信息，实现了对复杂术中场景下垂体解剖结构的高效分割。实验表明，F2PASeg在新构建的PAS数据集上取得了优于现有方法的精度和实时性，有效提升了手术关键结构的分割准确率并满足术中应用需求。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/F2PASeg_Feature_Fusion_for_Pituitary_Anatomy_Segmentation_in_Endoscopic_Surgery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-Smoothing-Slot-Attention-Iterations-and-Recurrences"><a href="#39-Smoothing-Slot-Attention-Iterations-and-Recurrences" class="headerlink" title="39. Smoothing Slot Attention Iterations and Recurrences"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Smoothing_Slot_Attention_Iterations_and_Recurrences.pdf">Smoothing Slot Attention Iterations and Recurrences</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Aalto University</span></p>
<p>该论文提出SmoothSA方法，通过预热冷启动查询和区分不同帧的聚合变换，解决Slot Attention在图像和视频对象聚合中的冷启动和变换同质性问题。实验表明，SmoothSA在对象发现、识别和视觉问答任务上取得了新的SOTA表现，显著提升了对象表示质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Smoothing_Slot_Attention_Iterations_and_Recurrences.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-CT-GRAPH-Hierarchical-Graph-Attention-Network-for-Anatomy-Guided-CT-Report-Generation"><a href="#40-CT-GRAPH-Hierarchical-Graph-Attention-Network-for-Anatomy-Guided-CT-Report-Generation" class="headerlink" title="40. CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT Report Generation"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/CT-GRAPH__Hierarchical_Graph_Attention_Network_for_Anatomy-Guided_CT_Report_Generation.pdf">CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT Report Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute for AI in Medicine (IKIM), University Hospital Essen</span></p>
<p>论文提出了CT-GRAPH，一种结合预训练3D医疗特征编码器和解剖结构分层图注意力网络的方法，通过解剖掩码提取细粒度器官及全局特征，并构建多层次解剖学图，利用图注意力机制进行特征聚合，最终将结构化特征输入大语言模型生成详细医学报告。实验表明，该方法在CT报告生成任务上显著提升了临床实体F1分数（提升7.9%），在细粒度和全局诊断上均优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/CT-GRAPH_Hierarchical_Graph_Attention_Network_for_Anatomy-Guided_CT_Report_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-PriorRG-Prior-Guided-Contrastive-Pre-training-and-Coarse-to-Fine-Decoding-for-Chest-X-ray-Report-Generation"><a href="#41-PriorRG-Prior-Guided-Contrastive-Pre-training-and-Coarse-to-Fine-Decoding-for-Chest-X-ray-Report-Generation" class="headerlink" title="41. PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/PriorRG__Prior-Guided_Contrastive_Pre-training_and_Coarse-to-Fine_Decoding_for_Chest_X-ray_Report_Ge.pdf">PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xidian University</span></p>
<p>PriorRG提出了一个针对胸部X光报告生成的两阶段框架，结合了患者特定的临床先验信息，包括临床背景和近期影像。其方法包括先验引导的对比预训练，提升图像与文本的语义对齐能力，以及先验感知的粗到细解码策略，逐步融合临床语境、疾病进展和分层视觉特征，显著提升生成报告的临床准确性与流畅度。实验结果表明，PriorRG在MIMIC-CXR及MIMIC-ABN等主流医学数据集上，报告生成和医学图像-文本检索任务均优于现有方法，具有更强的泛化能力和部署效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/PriorRG_Prior-Guided_Contrastive_Pre-training_and_Coarse-to-Fine_Decoding_for_Chest_X-ray_Report_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-Textual-Inversion-for-Efficient-Adaptation-of-Open-Vocabulary-Object-Detectors-Without-Forgetting"><a href="#42-Textual-Inversion-for-Efficient-Adaptation-of-Open-Vocabulary-Object-Detectors-Without-Forgetting" class="headerlink" title="42. Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Textual_Inversion_for_Efficient_Adaptation_of_Open-Vocabulary_Object_Detectors_Without_Forgetting.pdf">Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">TNO, Intelligent Imaging, the Hague, the Netherlands</span></p>
<p>本文提出了一种将文本倒置（Textual Inversion, TI）方法应用于视觉-语言模型（VLM）开放词汇目标检测的新方法。通过仅优化新词嵌入向量、冻结原有模型权重，实现以极少样本（如3张图片）高效扩展检测词汇，同时显著减缓遗忘问题，保留原模型零样本泛化能力。实验表明TI在few-shot和fine-grained检测任务中性能接近甚至超越常规提示微调（Prompt Tuning）方法，且可将已学类别迁移到新领域，显著减少对全模型微调和原始数据的依赖。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Textual_Inversion_for_Efficient_Adaptation_of_Open-Vocabulary_Object_Detectors_Without_Forgetting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-mKG-RAG-Multimodal-Knowledge-Graph-Enhanced-RAG-for-Visual-Question-Answering"><a href="#43-mKG-RAG-Multimodal-Knowledge-Graph-Enhanced-RAG-for-Visual-Question-Answering" class="headerlink" title="43. mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/mKG-RAG__Multimodal_Knowledge_Graph-Enhanced_RAG_for_Visual_Question_Answering.pdf">mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong Polytechnic University</span></p>
<p>本文提出mKG-RAG，一种结合检索增强生成（RAG）和多模态知识图谱（KG）的新框架，用于提升多模态大语言模型（MLLMs）在知识密集型视觉问答（VQA）任务中的推理能力。方法包括利用MLLM进行多模态文档的实体和关系抽取，构建结构化知识图谱，并通过双阶段检索（向量检索和图检索）精准筛选相关知识，显著提升VQA准确率，实验结果优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/mKG-RAG_Multimodal_Knowledge_Graph-Enhanced_RAG_for_Visual_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion"><a href="#44-SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion" class="headerlink" title="44. SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SGDFuse__SAM-Guided_Diffusion_for_High-Fidelity_Infrared_and_Visible_Image_Fusion.pdf">SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shandong Technology and Business University</span></p>
<p>该论文提出了一种名为SGDFuse的红外与可见光图像融合方法，通过Segment Anything Model (SAM)生成高质量语义掩码，引导条件扩散模型，实现高保真、语义一致的图像融合。方法采用两阶段架构：第一阶段多模态特征融合，第二阶段结合语义掩码和扩散生成进行结构细化和语义增强，显著提升融合图像在目标检测和语义分割等下游任务中的表现。实验结果显示SGDFuse在主流数据集和多项指标上均优于现有方法，结论表明该方法能有效统一语义一致性与视觉质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SGDFuse_SAM-Guided_Diffusion_for_High-Fidelity_Infrared_and_Visible_Image_Fusion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-CF3-Compact-and-Fast-3D-Feature-Fields"><a href="#45-CF3-Compact-and-Fast-3D-Feature-Fields" class="headerlink" title="45. CF3: Compact and Fast 3D Feature Fields"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/CF3__Compact_and_Fast_3D_Feature_Fields.pdf">CF3: Compact and Fast 3D Feature Fields</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Seoul National University</span></p>
<p>该论文提出CF3，一种从多视角2D视觉基础模型特征通过加权融合提升到3D高斯特征场的方法，并在每个高斯点上训练自适应自动编码器进行特征压缩，再通过自适应稀疏化策略合并和剪枝冗余高斯点，大幅降低存储和计算成本。实验表明，CF3在多种3D场景分割与定位任务中，能以仅5%高斯数量达到与主流方法相当的性能，且渲染与查询速度显著提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/CF3_Compact_and_Fast_3D_Feature_Fields.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-A-Study-of-Gender-Classification-Techniques-Based-on-Iris-Images-A-Deep-Survey-and-Analysis"><a href="#46-A-Study-of-Gender-Classification-Techniques-Based-on-Iris-Images-A-Deep-Survey-and-Analysis" class="headerlink" title="46. A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Study_of_Gender_Classification_Techniques_Based_on_Iris_Images__A_Deep_Survey_and_Analysis.pdf">A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical College of Informatics Akre, Duhok Polytechnic University</span></p>
<p>本文系统综述了基于虹膜图像的性别分类技术，涵盖了传统方法与深度学习（如CNN）在图像采集、分割、特征提取和分类等流程中的应用，并对主流算法（Gabor滤波、DWT、LBP、PCA等）及其在实际数据集上的表现进行比较分析。结论认为，深度学习方法在特征自动提取和分类准确率方面较传统方法有显著提升，但仍面临图像质量、特征选择等挑战，未来可通过优化特征选择与分类器融合进一步提高实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Study_of_Gender_Classification_Techniques_Based_on_Iris_Images_A_Deep_Survey_and_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-Coarse-to-Fine-Joint-Registration-of-MR-and-Ultrasound-Images-via-Imaging-Style-Transfer"><a href="#47-Coarse-to-Fine-Joint-Registration-of-MR-and-Ultrasound-Images-via-Imaging-Style-Transfer" class="headerlink" title="47. Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Coarse-to-Fine_Joint_Registration_of_MR_and_Ultrasound_Images_via_Imaging_Style_Transfer.pdf">Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>本文提出了一种结合3D CycleGAN无监督风格迁移、分层块匹配仿射变换与SynthMorph局部形变估计的粗到细多阶段MR与超声图像配准方法。方法显著增强了MR与US图像的一致性，但在部分情况下会导致目标配准误差（TRE）增加，因此未来需研究更一致的上下文描述指标。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Coarse-to-Fine_Joint_Registration_of_MR_and_Ultrasound_Images_via_Imaging_Style_Transfer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-ReasoningTrack-Chain-of-Thought-Reasoning-for-Long-term-Vision-Language-Tracking"><a href="#48-ReasoningTrack-Chain-of-Thought-Reasoning-for-Long-term-Vision-Language-Tracking" class="headerlink" title="48. ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/ReasoningTrack__Chain-of-Thought_Reasoning_for_Long-term_Vision-Language_Tracking.pdf">ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Anhui University</span></p>
<p>该论文提出了ReasoningTrack，一种基于链式推理（Chain-of-Thought, CoT）的视觉-语言长期跟踪框架，利用预训练大规模视觉语言模型Qwen2.5-VL，通过两阶段的有监督微调（SFT）和强化学习（GRPO）优化模型的推理与语言生成能力。方法结合动态语言描述和视觉特征，通过推理链不断更新目标描述，并在统一的跟踪骨干网络中实现高鲁棒性跟踪。在多个数据集上的实验显示，ReasoningTrack显著提升了跟踪精度，并提出了包含200个视频序列的TNLLT长视频视觉语言跟踪数据集，为领域后续研究奠定基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/ReasoningTrack_Chain-of-Thought_Reasoning_for_Long-term_Vision-Language_Tracking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-SPA-Generalized-Graph-Spectral-Alignment-for-Versatile-Domain-Adaptation"><a href="#49-SPA-Generalized-Graph-Spectral-Alignment-for-Versatile-Domain-Adaptation" class="headerlink" title="49. SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SPA++__Generalized_Graph_Spectral_Alignment_for_Versatile_Domain_Adaptation.pdf">SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>SPA++提出了一种基于图谱对齐和邻居感知传播机制的通用域自适应方法，通过在特征空间进行图谱对齐、利用邻居信息提升目标域判别能力，并结合数据增强和一致性正则化，适配多种复杂域自适应场景（如UDA、SSDA、MSDA、MTDA、长尾&#x2F;子群分布）。实验证明SPA++在多种视觉域自适应基准上的表现优于现有方法，具有更强的鲁棒性和适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SPA%2B%2B_Generalized_Graph_Spectral_Alignment_for_Versatile_Domain_Adaptation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-Wavelet-Guided-Dual-Frequency-Encoding-for-Remote-Sensing-Change-Detection"><a href="#50-Wavelet-Guided-Dual-Frequency-Encoding-for-Remote-Sensing-Change-Detection" class="headerlink" title="50. Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Wavelet-Guided_Dual-Frequency_Encoding_for_Remote_Sensing_Change_Detection.pdf">Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shandong Technology and Business University</span></p>
<p>该论文提出了Wavelet-Guided Dual-Frequency Encoding (WGDF)方法，通过在输入阶段利用离散小波变换（DWT）将遥感影像分解为高低频分支，分别提取局部细节和全局语义：高频分支利用DFFE和FDID模块增强边缘变化建模，低频分支结合Transformer和PCDM捕捉全局语义变化，最终融合获得变化检测结果。实验在多个遥感数据集上验证了WGDF在复杂场景下的边缘消歧和检测准确性优于现有方法，具有较高的鲁棒性和效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Wavelet-Guided_Dual-Frequency_Encoding_for_Remote_Sensing_Change_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding"><a href="#51-RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding" class="headerlink" title="51. RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/RegionMed-CLIP__A_Region-Aware_Multimodal_Contrastive_Learning_Pre-trained_Model_for_Medical_Image_U.pdf">RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Anhui Polytechnic University</span></p>
<p>该论文提出RegionMed-CLIP，一种结合全局与局部特征的区域感知多模态对比学习框架，核心包括ROI（感兴趣区域）处理器和分阶段训练策略，有效融合医学影像的细粒度病灶信息与语义文本。通过自建高质量区域标注的MedRegion-500k数据集，实验在零样本分类、图文检索和医学VQA任务上均显著优于现有SOTA模型，证明区域感知对比预训练对于医学图像理解的关键作用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/RegionMed-CLIP_A_Region-Aware_Multimodal_Contrastive_Learning_Pre-trained_Model_for_Medical_Image_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-ArbiViewGen-Controllable-Arbitrary-Viewpoint-Camera-Data-Generation-for-Autonomous-Driving-via-Stable-Diffusion-Models"><a href="#52-ArbiViewGen-Controllable-Arbitrary-Viewpoint-Camera-Data-Generation-for-Autonomous-Driving-via-Stable-Diffusion-Models" class="headerlink" title="52. ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/ArbiViewGen__Controllable_Arbitrary_Viewpoint_Camera_Data_Generation_for_Autonomous_Driving_via_Stab.pdf">ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本论文提出了ArbiViewGen，一种基于Stable Diffusion模型的可控任意视角相机图像生成框架，专为自动驾驶场景下多车辆多摄像头系统设计。方法核心包括特征感知自适应视图拼接（FAVS）与跨视图一致性自监督学习（CVC-SSL）：FAVS通过分层几何与特征匹配生成高质量伪标签，CVC-SSL利用扩散模型实现闭环自监督训练，无需真实外插视角监督，仅依赖多摄像头图像和姿态信息。实验表明，该方法在nuScenes自动驾驶数据集上实现了高于现有方法的任意视角合成质量，显著提升了跨平台数据复用能力。结论指出，ArbiViewGen有效解决了缺乏外插视角监督的难题，提升了自动驾驶感知系统的适应性和鲁棒性，但在高度动态环境下细节还需提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/ArbiViewGen_Controllable_Arbitrary_Viewpoint_Camera_Data_Generation_for_Autonomous_Driving_via_Stable_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-Segmenting-the-Complex-and-Irregular-in-Two-Phase-Flows-A-Real-World-Empirical-Study-with-SAM2"><a href="#53-Segmenting-the-Complex-and-Irregular-in-Two-Phase-Flows-A-Real-World-Empirical-Study-with-SAM2" class="headerlink" title="53. Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Segmenting_the_Complex_and_Irregular_in_Two-Phase_Flows__A_Real-World_Empirical_Study_with_SAM2.pdf">Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Delft University of Technology</span></p>
<p>本论文将复杂气泡分割任务视为迁移学习问题，首次对Segment Anything Model (SAM v2.1)进行细致微调，并通过数据增强策略，实现在真实多相流场景下对高度不规则、非凸形状气泡的高精度分割。实验表明，仅用100张标注图像即可获得F1和Dice分数高达0.95，优于以往需要大规模数据训练的方法，特别在中、大气泡分割上表现突出，显著降低了人工标注成本。结论指出，该方法数据效率高，泛化性强，并公开了数据集和微调流程，助力多相流领域的后续研究。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Segmenting_the_Complex_and_Irregular_in_Two-Phase_Flows_A_Real-World_Empirical_Study_with_SAM2.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-Beyond-Pixels-Medical-Image-Quality-Assessment-with-Implicit-Neural-Representations"><a href="#54-Beyond-Pixels-Medical-Image-Quality-Assessment-with-Implicit-Neural-Representations" class="headerlink" title="54. Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Beyond_Pixels__Medical_Image_Quality_Assessment_with_Implicit_Neural_Representations.pdf">Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Istanbul Technical University</span></p>
<p>该论文提出使用基于SIREN的隐式神经表示（INR）对心脏MRI图像质量进行评估，通过两阶段流程：首先将2D切片拟合为INR参数，然后利用深度神经网络（如多层感知机、图神经网络、关系注意力Transformer等）直接在INR参数空间进行分类。实验表明，该方法在ACDC数据集上实现了优于传统像素级方法的准确率，并大幅减少了参数量，尤其Transformer（Probe&#x3D;32）模型在Good vs. Poor任务上达到94.4%准确率，展现了INR作为医学图像质量描述符的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Beyond_Pixels_Medical_Image_Quality_Assessment_with_Implicit_Neural_Representations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-X-MoGen-Unified-Motion-Generation-across-Humans-and-Animals"><a href="#55-X-MoGen-Unified-Motion-Generation-across-Humans-and-Animals" class="headerlink" title="55. X-MoGen: Unified Motion Generation across Humans and Animals"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/X-MoGen__Unified_Motion_Generation_across_Humans_and_Animals.pdf">X-MoGen: Unified Motion Generation across Humans and Animals</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本论文提出X-MoGen，首次实现跨人类与动物统一的文本驱动动作生成框架。方法采用两阶段架构：第一阶段通过条件图变分自编码器（CGAE）学习T-pose骨架先验及自编码器（AE）编码动作至共享潜在空间，并引入形态损失正则化；第二阶段利用掩码变换器（Masked Transformer）结合扩散模型，根据文本描述生成动作嵌入，训练时形态一致性模块（MCM）保证结构合理性。作者还构建了包含115种物种、119k序列的UniMo4D统一数据集，支持跨物种联合建模。实验显示X-MoGen在已见及未见物种上均优于现有方法，生成动作真实且具良好泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/X-MoGen_Unified_Motion_Generation_across_Humans_and_Animals.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-FedGIN-Federated-Learning-with-Dynamic-Global-Intensity-Non-linear-Augmentation-for-Organ-Segmentation-using-Multi-modal-Images"><a href="#56-FedGIN-Federated-Learning-with-Dynamic-Global-Intensity-Non-linear-Augmentation-for-Organ-Segmentation-using-Multi-modal-Images" class="headerlink" title="56. FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FedGIN__Federated_Learning_with_Dynamic_Global_Intensity_Non-linear_Augmentation_for_Organ_Segmentat.pdf">FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Norwegian University of Science and Technology</span></p>
<p>该论文提出FedGIN，一种结合了全局强度非线性（GIN）增强的联邦学习框架，用于多模态医学图像（CT和MRI）在隐私保护下的器官分割。FedGIN在本地训练阶段通过GIN模块动态对CT和MRI图像进行强度域增强，促使模型学习模态无关的特征，并通过模型参数聚合实现跨机构的协同建模，显著提升了在复杂和低对比度器官上的分割表现。实验结果表明，FedGIN在多模态数据下接近中心化训练性能，特别是在胰腺和胆囊等难分割器官上优于单模态和无GIN增强的联邦学习基线，验证了其在隐私保护、多机构、多模态实际应用场景下的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FedGIN_Federated_Learning_with_Dynamic_Global_Intensity_Non-linear_Augmentation_for_Organ_Segmentation_using_Multi-modal_Images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-PoseGen-In-Context-LoRA-Finetuning-for-Pose-Controllable-Long-Human-Video-Generation"><a href="#57-PoseGen-In-Context-LoRA-Finetuning-for-Pose-Controllable-Long-Human-Video-Generation" class="headerlink" title="57. PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/PoseGen__In-Context_LoRA_Finetuning_for_Pose-Controllable_Long_Human_Video_Generation.pdf">PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiaoice</span></p>
<p>该论文提出PoseGen，一种高效且创新的人体视频生成框架，实现了从单张参考图像和动作姿态序列生成任意长度、高保真、姿态可控的人体视频。方法核心包括双重条件机制：用LoRA微调视频扩散模型，参考信息在token级注入、姿态信息在channel级注入，并采用分段生成与KV缓存共享技术，实现视频片段无缝拼接，保证身份一致性和时序连贯性。实验结果显示，PoseGen在身份保持、动作控制和长视频一致性方面均优于现有方法，且仅需极少训练数据。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/PoseGen_In-Context_LoRA_Finetuning_for_Pose-Controllable_Long_Human_Video_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-AdaFusion-Prompt-Guided-Inference-with-Adaptive-Fusion-of-Pathology-Foundation-Models"><a href="#58-AdaFusion-Prompt-Guided-Inference-with-Adaptive-Fusion-of-Pathology-Foundation-Models" class="headerlink" title="58. AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/AdaFusion__Prompt-Guided_Inference_with_Adaptive_Fusion_of_Pathology_Foundation_Models.pdf">AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China University of Technology</span></p>
<p>本文提出AdaFusion，一种面向病理图像分析的多模型特征自适应融合推理框架。其方法是在无需微调基础模型的前提下，采用轻量级注意力机制对多种预训练病理基础模型（PFMs）的低维特征进行压缩、对齐和基于组织表型上下文的动态加权融合。对治疗响应预测、肿瘤分级和空间基因表达推断等多项真实世界任务的实验表明，AdaFusion在分类和回归任务上均优于单一PFM和主流融合基线，并能可解释性地量化各模型对不同组织表型的贡献，提升鲁棒性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/AdaFusion_Prompt-Guided_Inference_with_Adaptive_Fusion_of_Pathology_Foundation_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-AU-IQA-A-Benchmark-Dataset-for-Perceptual-Quality-Assessment-of-AI-Enhanced-User-Generated-Content"><a href="#59-AU-IQA-A-Benchmark-Dataset-for-Perceptual-Quality-Assessment-of-AI-Enhanced-User-Generated-Content" class="headerlink" title="59. AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/AU-IQA__A_Benchmark_Dataset_for_Perceptual_Quality_Assessment_of_AI-Enhanced_User-Generated_Content.pdf">AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了AU-IQA，这是首个面向AI增强用户生成内容（AI-UGC）感知质量评估的基准数据集，涵盖超分辨率、低光增强和去噪三类AI增强方式，共4800张图像，并采用主观均值评分（MOS）进行标注。作者系统评测了传统IQA模型和大规模多模态模型在该数据集上的表现，发现以UGC为目标的模型效果最佳，现有模型在不同AI-UGC类型间表现差异大且不稳定，强调了开发专用AI-UGC质量评估模型的必要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/AU-IQA_A_Benchmark_Dataset_for_Perceptual_Quality_Assessment_of_AI-Enhanced_User-Generated_Content.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-Multimodal-Causal-Driven-Representation-Learning-for-Generalizable-Medical-Image-Segmentation"><a href="#60-Multimodal-Causal-Driven-Representation-Learning-for-Generalizable-Medical-Image-Segmentation" class="headerlink" title="60. Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Multimodal_Causal-Driven_Representation_Learning_for_Generalizable_Medical_Image_Segmentation.pdf">Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hong Kong Institute of Science &amp; Innovation</span></p>
<p>该论文提出了一种多模态因果驱动表征学习（MCDRL）框架，通过结合因果推断和视觉-语言模型（如CLIP），分两步实现医学图像分割的领域泛化：首先利用CLIP跨模态能力识别病灶区域并通过文本提示构建混杂因素字典，然后训练因果干预网络，消除领域特异变异影响，保留关键解剖结构信息。实验结果表明，MCDRL在多个医学图像分割数据集上优于现有方法，具备更强领域泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Multimodal_Causal-Driven_Representation_Learning_for_Generalizable_Medical_Image_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-Propagating-Sparse-Depth-via-Depth-Foundation-Model-for-Out-of-Distribution-Depth-Completion"><a href="#61-Propagating-Sparse-Depth-via-Depth-Foundation-Model-for-Out-of-Distribution-Depth-Completion" class="headerlink" title="61. Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Propagating_Sparse_Depth_via_Depth_Foundation_Model_for_Out-of-Distribution_Depth_Completion.pdf">Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Dalian University of Technology</span></p>
<p>该论文提出了PSD深度补全框架，通过利用预训练的深度基础模型从RGB图像中提取结构和语义线索，引导稀疏深度向缺失区域传播，包含无参数的3D&#x2F;2D双空间传播模块及可学习的残差校正模块。实验显示，该方法在16个OOD场景下超越现有方法，具备极强泛化性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Propagating_Sparse_Depth_via_Depth_Foundation_Model_for_Out-of-Distribution_Depth_Completion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-Rotation-Equivariant-Arbitrary-scale-Image-Super-Resolution"><a href="#62-Rotation-Equivariant-Arbitrary-scale-Image-Super-Resolution" class="headerlink" title="62. Rotation Equivariant Arbitrary-scale Image Super-Resolution"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Rotation_Equivariant_Arbitrary-scale_Image_Super-Resolution.pdf">Rotation Equivariant Arbitrary-scale Image Super-Resolution</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong University</span></p>
<p>本文提出了一种端到端旋转等变的任意尺度图像超分辨率(ASISR)方法，通过重新设计隐式神经表示(INR)和编码器模块，嵌入旋转等变性，理论分析了等变误差并实现了对现有ASISR方法的可插拔增强。实验结果表明，该方法能更好地保持图像几何结构及旋转对称性，有效提升超分辨率恢复质量和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Rotation_Equivariant_Arbitrary-scale_Image_Super-Resolution.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-Decoupling-Continual-Semantic-Segmentation"><a href="#63-Decoupling-Continual-Semantic-Segmentation" class="headerlink" title="63. Decoupling Continual Semantic Segmentation"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Decoupling_Continual_Semantic_Segmentation.pdf">Decoupling Continual Semantic Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-sen University</span></p>
<p>该论文提出了DecoupleCSS，一个用于持续语义分割（CSS）的两阶段框架，将类别感知检测与类别无关分割解耦。方法中，第一阶段利用预训练视觉-语言模型并结合LoRA适配器进行类别检测和定位提示生成，第二阶段用Segment Anything Model（SAM）实现精细分割，显著缓解遗忘问题并提升新旧知识平衡。实验表明，DecoupleCSS在多个常用CSS任务上取得了当前最优表现，具备较强的泛化能力，但推理时多任务切换带来一定延迟。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Decoupling_Continual_Semantic_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-Modeling-Rapid-Contextual-Learning-in-the-Visual-Cortex-with-Fast-Weight-Deep-Autoencoder-Networks"><a href="#64-Modeling-Rapid-Contextual-Learning-in-the-Visual-Cortex-with-Fast-Weight-Deep-Autoencoder-Networks" class="headerlink" title="64. Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Modeling_Rapid_Contextual_Learning_in_the_Visual_Cortex_with_Fast-Weight_Deep_Autoencoder_Networks.pdf">Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carnegie Mellon University</span></p>
<p>本文提出了一种基于Vision Transformer（ViT）自编码器的架构，利用低秩适配（LoRA）模块模拟大脑视觉皮层中的快速权重机制，实现熟悉上下文的快速学习。实验表明，熟悉训练促使早期层对全局上下文敏感，压缩任务无关的变异，同时LoRA进一步增强了这一效果，有助于实现更鲁棒的视觉表示和更清晰的图像分割。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Modeling_Rapid_Contextual_Learning_in_the_Visual_Cortex_with_Fast-Weight_Deep_Autoencoder_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-UGOD-Uncertainty-Guided-Differentiable-Opacity-and-Soft-Dropout-for-Enhanced-Sparse-View-3DGS"><a href="#65-UGOD-Uncertainty-Guided-Differentiable-Opacity-and-Soft-Dropout-for-Enhanced-Sparse-View-3DGS" class="headerlink" title="65. UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/UGOD__Uncertainty-Guided_Differentiable_Opacity_and_Soft_Dropout_for_Enhanced_Sparse-View_3DGS.pdf">UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Manchester Metropolitan University</span></p>
<p>该论文提出了一种集成神经网络的不确定性学习模块，通过结合多层HashGrid编码，对3D Gaussian Splatting（3DGS）中的每个高斯进行视点相关的不确定性建模。方法利用学习到的不确定性分别指导高斯的不透明度调节及可微分软丢弃机制，有效减少稀疏视角场景下的过拟合。实验表明，该方法在多个3D重建数据集上均优于现有方法，能以更少的高斯实现更高的渲染质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/UGOD_Uncertainty-Guided_Differentiable_Opacity_and_Soft_Dropout_for_Enhanced_Sparse-View_3DGS.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-Open-world-Point-Cloud-Semantic-Segmentation-A-Human-in-the-loop-Framework"><a href="#66-Open-world-Point-Cloud-Semantic-Segmentation-A-Human-in-the-loop-Framework" class="headerlink" title="66. Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Open-world_Point_Cloud_Semantic_Segmentation__A_Human-in-the-loop_Framework.pdf">Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Science and Technology</span></p>
<p>该论文提出了HOW-Seg框架，实现了点云的开放世界语义分割，通过在查询样本上以稀疏人工标注为指导，直接构建类别原型并结合交互式原型消歧和基于CRF的原型标签分配机制，避免了跨样本分布偏移和原型偏差。实验表明，HOW-Seg无需额外支持样本或离线增量学习，仅靠极少量人工干预即可在S3DIS和ScanNetv2等数据集上显著优于现有方法，实现高质量的基类与新类点云分割。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Open-world_Point_Cloud_Semantic_Segmentation_A_Human-in-the-loop_Framework.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-Laplacian-Analysis-Meets-Dynamics-Modelling-Gaussian-Splatting-for-4D-Reconstruction"><a href="#67-Laplacian-Analysis-Meets-Dynamics-Modelling-Gaussian-Splatting-for-4D-Reconstruction" class="headerlink" title="67. Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Laplacian_Analysis_Meets_Dynamics_Modelling__Gaussian_Splatting_for_4D_Reconstruction.pdf">Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>本论文提出一种动态3D高斯投影(3DGS)的混合显式-隐式编码框架，融合多尺度哈希编码与拉普拉斯频域分析，有效解耦并控制复杂动态场景中不同频率的运动分量。方法还包括增强的高斯动态属性和基于KDTree的自适应高斯划分策略，实现对高动态区域的精细建模和高效优化。实验结果在三个主流动态场景数据集上验证了所提方法在重建精度与运动一致性上的领先性能，优于现有动态重建方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Laplacian_Analysis_Meets_Dynamics_Modelling_Gaussian_Splatting_for_4D_Reconstruction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="68-H-NET-Hierarchical-Dynamic-Chunking-for-Tokenizer-Free-Language-Modelling-in-Morphologically-Rich-Languages"><a href="#68-H-NET-Hierarchical-Dynamic-Chunking-for-Tokenizer-Free-Language-Modelling-in-Morphologically-Rich-Languages" class="headerlink" title="68. H-NET++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/H-Net++__Hierarchical_Dynamic_Chunking_for_Tokenizer-Free_Language_Modelling_in_Morphologically-Rich.pdf">H-NET++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent (no institution listed)</span></p>
<p>该论文提出了H-NET++，一种针对形态复杂语言（如波斯语）的分层动态分块语言建模方法，核心创新包括层次化GRU路由器、轻量Transformer上下文混合器、文档级变分超先验、特殊处理零宽非连接字符（ZWNJ）以及阶段式课程训练。实验结果显示H-NET++在语料压缩、下游任务准确率、对正字法噪声的鲁棒性和形态边界识别方面均超越现有模型，并能自动学习符合语言学规律的分块，无需手动标注。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/H-NET%2B%2B_Hierarchical_Dynamic_Chunking_for_Tokenizer-Free_Language_Modelling_in_Morphologically-Rich_Languages.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-Simulating-Human-Like-Learning-Dynamics-with-LLM-Empowered-Agents"><a href="#69-Simulating-Human-Like-Learning-Dynamics-with-LLM-Empowered-Agents" class="headerlink" title="69. Simulating Human-Like Learning Dynamics with LLM-Empowered Agents"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Simulating_Human-Like_Learning_Dynamics_with_LLM-Empowered_Agents.pdf">Simulating Human-Like Learning Dynamics with LLM-Empowered Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>该论文提出LearnerAgent框架，利用多智能体系统和大语言模型（LLM）模拟具有不同心理画像（深度、表层、懒惰、无画像）的学生在为期一年的真实教学环境中的学习行为。通过周期性知识获取、测试与互动，揭示只有深度型学习者具备长期认知成长，LLM默认行为为“勤奋但脆弱的表层学习者”，存在对表层模式的依赖，缺乏深层泛化能力。结论：LearnerAgent能高保真复现人类学习行为，揭示LLM短板并为教育与AI认知提供新洞察。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Simulating_Human-Like_Learning_Dynamics_with_LLM-Empowered_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle"><a href="#70-Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle" class="headerlink" title="70. Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Shuffle-R1__Efficient_RL_framework_for_Multimodal_Large_Language_Models_via_Data-centric_Dynamic_Shu.pdf">Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huazhong University of Science and Technology</span></p>
<p>该论文提出了Shuffle-R1框架，通过Pairwise Trajectory Sampling（结构化高对比轨迹采样）和Advantage-based Batch Shuffle（优势感知批次重构）优化多模态大语言模型的强化学习微调流程，有效解决了优势塌缩和回合静默问题，提高了训练效率和泛化能力。实验结果表明，Shuffle-R1在多项主流推理任务上超越现有RL方法，且具备较低计算开销。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Shuffle-R1_Efficient_RL_framework_for_Multimodal_Large_Language_Models_via_Data-centric_Dynamic_Shuffle.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-LLaVA-RE-Binary-Image-Text-Relevancy-Evaluation-with-Multimodal-Large-Language-Model"><a href="#71-LLaVA-RE-Binary-Image-Text-Relevancy-Evaluation-with-Multimodal-Large-Language-Model" class="headerlink" title="71. LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/LLaVA-RE__Binary_Image-Text_Relevancy_Evaluation_with_Multimodal_Large_Language_Model.pdf">LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stony Brook University</span></p>
<p>该论文提出LLaVA-RE框架，基于LLaVA 1.5多模态大语言模型，通过引入详细任务指令和多模态in-context learning，实现复杂图文对二元相关性评估，并构建了多任务的二元相关性数据集。实验表明该方法在训练及未见任务上均显著优于主流模型，具备良好泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/LLaVA-RE_Binary_Image-Text_Relevancy_Evaluation_with_Multimodal_Large_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-Iterative-Learning-of-Computable-Phenotypes-for-Treatment-Resistant-Hypertension-using-Large-Language-Models"><a href="#72-Iterative-Learning-of-Computable-Phenotypes-for-Treatment-Resistant-Hypertension-using-Large-Language-Models" class="headerlink" title="72. Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Iterative_Learning_of_Computable_Phenotypes_for_Treatment_Resistant_Hypertension_using_Large_Languag.pdf">Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Federal University of ABC</span></p>
<p>该论文提出使用大型语言模型（LLM）自动生成高血压及其复杂变型的可计算表型，通过“synthesize, execute, debug, instruct（SEDI）”迭代策略对模型进行数据驱动反馈优化。实验结果表明，SEDI策略下的LLM生成模型在准确性和可解释性方面接近甚至优于传统机器学习方法，但所需专家标注样本更少，且模型更简洁易于临床应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Iterative_Learning_of_Computable_Phenotypes_for_Treatment_Resistant_Hypertension_using_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-MV-Debate-Multi-view-Agent-Debate-with-Dynamic-Reflection-Gating-for-Multimodal-Harmful-Content-Detection-in-Social-Media"><a href="#73-MV-Debate-Multi-view-Agent-Debate-with-Dynamic-Reflection-Gating-for-Multimodal-Harmful-Content-Detection-in-Social-Media" class="headerlink" title="73. MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MV-Debate__Multi-view_Agent_Debate_with_Dynamic_Reflection_Gating_for_Multimodal_Harmful_Content_Det.pdf">MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shandong University</span></p>
<p>本文提出MV-Debate多视角多智能体辩论框架，通过组装表层分析、深度推理、模态对比和社会语境四类异构智能体，结合动态反思门控机制，实现多模态有害内容（如讽刺、仇恨言论、虚假信息）的统一检测。实验显示该方法在多项基准数据集上显著优于单模型和现有多智能体方法，提升了准确性、效率和可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MV-Debate_Multi-view_Agent_Debate_with_Dynamic_Reflection_Gating_for_Multimodal_Harmful_Content_Detection_in_Social_Media.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification"><a href="#74-On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification" class="headerlink" title="74. On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/On_the_Generalization_of_SFT__A_Reinforcement_Learning_Perspective_with_Reward_Rectification.pdf">On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southeast University</span></p>
<p>本文从理论上揭示了大语言模型（LLM）中常用的Supervised Fine-Tuning（SFT）方法的泛化能力受限，原因在于其梯度本质上具有隐式的奖励结构，导致优化不稳定和过拟合。为此，提出了一种简单的动态重加权方法Dynamic Fine-Tuning（DFT），通过对每个token的目标函数动态缩放，大幅提升了SFT的泛化能力，并在多个数学推理基准和离线RL场景下超越了标准SFT及多种RL方法。结论：DFT显著提升了SFT的泛化和训练稳定性，为LLM微调提供了高效实用的新范式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/On_the_Generalization_of_SFT_A_Reinforcement_Learning_Perspective_with_Reward_Rectification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-How-Do-LLMs-Persuade-Linear-Probes-Can-Uncover-Persuasion-Dynamics-in-Multi-Turn-Conversations"><a href="#75-How-Do-LLMs-Persuade-Linear-Probes-Can-Uncover-Persuasion-Dynamics-in-Multi-Turn-Conversations" class="headerlink" title="75. How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/How_Do_LLMs_Persuade__Linear_Probes_Can_Uncover_Persuasion_Dynamics_in_Multi-Turn_Conversations.pdf">How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mila</span></p>
<p>本论文提出了一种利用线性探针（linear probes）分析大型语言模型（LLMs）在多轮对话中说服能力的方法。作者设计了可高效检测说服结果、被说服者个性和说服策略的探针，并在真实及合成数据集上验证其能捕捉对话中的关键说服动态，发现探针不仅效率远超提示（prompting），并在部分任务上效果更优。结论显示，线性探针能够高效揭示LLM说服行为细节，适用于大规模对话分析与复杂行为研究。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/How_Do_LLMs_Persuade_Linear_Probes_Can_Uncover_Persuasion_Dynamics_in_Multi-Turn_Conversations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-The-Missing-Reward-Active-Inference-in-the-Era-of-Experience"><a href="#76-The-Missing-Reward-Active-Inference-in-the-Era-of-Experience" class="headerlink" title="76. The Missing Reward: Active Inference in the Era of Experience"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/The_Missing_Reward__Active_Inference_in_the_Era_of_Experience.pdf">The Missing Reward: Active Inference in the Era of Experience</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IBM T.J. Watson Research Center</span></p>
<p>本文提出将Active Inference（AIF）与大规模语言模型（LLM）结合，作为实现AI自主学习与经验驱动智能的理论基础。方法上，以AIF的自由能最小化为内在目标，利用LLM作为生成式世界模型，统一探索与利用，并消除对人工奖励工程的依赖。结论认为该架构能提升AI自主性与能源效率，推动经验驱动的可持续AI发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/The_Missing_Reward_Active_Inference_in_the_Era_of_Experience.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-COOPER-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models"><a href="#77-COOPER-Co-Optimizing-Policy-and-Reward-Models-in-Reinforcement-Learning-for-Large-Language-Models" class="headerlink" title="77. COOPER: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Cooper__Co-Optimizing_Policy_and_Reward_Models_in_Reinforcement_Learning_for_Large_Language_Models.pdf">COOPER: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出Cooper框架，在强化学习中同步优化大语言模型的策略模型和奖励模型。方法上，通过参考式奖励模型（VerifyRM）、混合注释策略和正负对比学习动态更新奖励模型，结合规则型高精度奖励筛选正样本和助手LLM生成负样本，极大提升了奖励模型的鲁棒性、减少reward hacking。实验证明，Cooper在多个数学推理任务上优于仅用规则或静态奖励模型，动态更新奖励模型能有效抑制reward hacking并提升RL效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/COOPER_Co-Optimizing_Policy_and_Reward_Models_in_Reinforcement_Learning_for_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-Follow-Your-Instruction-A-Comprehensive-MLLM-Agent-for-World-Data-Synthesis"><a href="#78-Follow-Your-Instruction-A-Comprehensive-MLLM-Agent-for-World-Data-Synthesis" class="headerlink" title="78. Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Follow-Your-Instruction__A_Comprehensive_MLLM_Agent_for_World_Data_Synthesis.pdf">Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HKUST(GZ)</span></p>
<p>本文提出了Follow-Your-Instruction框架，一种基于多模态大语言模型（MLLM）的数据合成代理，支持2D、3D和4D数据的自动高质量合成。该方法通过MLLM-Collector收集多模态资产，MLLM-Generator生成3D布局，MLLM-Optimizer多视图优化场景，MLLM-Planner结合VLM实现时序连贯的视频生成。实验表明，该框架合成的数据能显著提升多项AIGC下游任务性能，具有良好可扩展性和实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Follow-Your-Instruction_A_Comprehensive_MLLM_Agent_for_World_Data_Synthesis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-Conformal-Sets-in-Multiple-Choice-Question-Answering-under-Black-Box-Settings-with-Provable-Coverage-Guarantees"><a href="#79-Conformal-Sets-in-Multiple-Choice-Question-Answering-under-Black-Box-Settings-with-Provable-Coverage-Guarantees" class="headerlink" title="79. Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Conformal_Sets_in_Multiple-Choice_Question_Answering_under_Black-Box_Settings_with_Provable_Coverage.pdf">Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Jinan</span></p>
<p>本文提出了一种基于频率的不确定性量化方法，结合多次独立采样和模态输出作为参考，通过经验频率计算预测熵，并利用Conformal Prediction理论构建具有可证明覆盖率保证的预测集。实验结果表明，该方法在多种LLM和MCQA数据集上较传统logit概率方法更有效地区分正确与错误预测，并能在不同风险级别下有效控制误覆盖率，提升LLM在黑盒场景下的不确定性量化可靠性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Conformal_Sets_in_Multiple-Choice_Question_Answering_under_Black-Box_Settings_with_Provable_Coverage_Guarantees.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-AI-vs-Human-Moderators-A-Comparative-Evaluation-of-Multimodal-LLMs-in-Content-Moderation-for-Brand-Safety"><a href="#80-AI-vs-Human-Moderators-A-Comparative-Evaluation-of-Multimodal-LLMs-in-Content-Moderation-for-Brand-Safety" class="headerlink" title="80. AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/AI_vs._Human_Moderators__A_Comparative_Evaluation_of_Multimodal_LLMs_in_Content_Moderation_for_Brand.pdf">AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zefr Inc</span></p>
<p>本文提出了一个多模态和多语言视频数据集，用于品牌安全内容审核，并系统评估了多模态大语言模型（MLLMs，如Gemini、GPT、Llama）在视频内容审核任务中的表现。方法包括多模态信息处理、零样本分类和精细化提示设计，结果显示MLLMs可显著降低人工审核成本，且多模态模型优于文本模型，但准确率仍不及专业人工审核，尤其在复杂和细微分类上表现不足。结论指出MLLMs具备提升内容审核效率的潜力，未来可通过微调和多语言优化进一步提升效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/AI_vs._Human_Moderators_A_Comparative_Evaluation_of_Multimodal_LLMs_in_Content_Moderation_for_Brand_Safety.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-Leveraging-AI-to-Accelerate-Clinical-Data-Cleaning-A-Comparative-Study-of-AI-Assisted-vs-Traditional-Methods"><a href="#81-Leveraging-AI-to-Accelerate-Clinical-Data-Cleaning-A-Comparative-Study-of-AI-Assisted-vs-Traditional-Methods" class="headerlink" title="81. Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Leveraging_AI_to_Accelerate_Clinical_Data_Cleaning__A_Comparative_Study_of_AI-Assisted_vs._Tradition.pdf">Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Octozi</span></p>
<p>该论文提出了Octozi平台，通过结合大语言模型（如Llama 4微调）和临床领域启发式算法，实现了临床数据清洗流程的自动化和智能化。实验结果显示，AI辅助不仅提升了数据清洗速度6倍，还将错误率从54.67%降至8.48%，显著减轻了人工负担并保证合规性和准确性，验证了AI在药物临床试验数据管理中的变革潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Leveraging_AI_to_Accelerate_Clinical_Data_Cleaning_A_Comparative_Study_of_AI-Assisted_vs._Traditional_Methods.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMs’-Entity-Deduction-Capabilities"><a href="#82-The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMs’-Entity-Deduction-Capabilities" class="headerlink" title="82. The World According to LLMs: How Geographic Origin Influences LLMs’ Entity Deduction Capabilities"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/The_World_According_to_LLMs__How_Geographic_Origin_Influences_LLMs'_Entity_Deduction_Capabilities.pdf">The World According to LLMs: How Geographic Origin Influences LLMs’ Entity Deduction Capabilities</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">BITS, Pilani</span></p>
<p>本文提出了一种基于“20问游戏”的新型评测框架，通过让大型语言模型（LLMs）自发地提出问题并推理，系统性地揭示了模型在地理实体推理任务中的隐性地理偏见。研究发现，主流LLMs在推理来自全球北方和西方地区的知名人物和事物时表现更优，而实体的流行度和训练语料频次对这种差异的解释力有限，游戏语言影响也极小，凸显了模型推理路径中的地理和文化不平等。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/The_World_According_to_LLMs_How_Geographic_Origin_Influences_LLMs%E2%80%99_Entity_Deduction_Capabilities.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-Streamlining-Admission-with-LOR-Insights-AI-Based-Leadership-Assessment-in-Online-Master’s-Program"><a href="#83-Streamlining-Admission-with-LOR-Insights-AI-Based-Leadership-Assessment-in-Online-Master’s-Program" class="headerlink" title="83. Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master’s Program"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Streamlining_Admission_with_LOR_Insights__AI-Based_Leadership_Assessment_in_Online_Master's_Program.pdf">Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master’s Program</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Georgia Institute of Technology</span></p>
<p>本论文提出LORI系统，通过结合RoBERTa弱监督模型和LLAMA2等大语言模型，对硕士申请者推荐信中的领导力特质（如团队协作、沟通、创新）进行自动化检测与微标签分类。实验证明RoBERTa模型在领导力句子识别上取得了91.6%的F1分数，并通过LLM实现更深层次的短语提取与验证，有效提升了招生流程效率和公平性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Streamlining_Admission_with_LOR_Insights_AI-Based_Leadership_Assessment_in_Online_Master%27s_Program.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-Auto-Eval-Judge-Towards-a-General-Agentic-Framework-for-Task-Completion-Evaluation"><a href="#84-Auto-Eval-Judge-Towards-a-General-Agentic-Framework-for-Task-Completion-Evaluation" class="headerlink" title="84. Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Auto-Eval_Judge__Towards_a_General_Agentic_Framework_for_Task_Completion_Evaluation.pdf">Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Massachusetts Amherst</span></p>
<p>本文提出了一个通用、模块化的Agent任务完成评价框架，能够自动分解任务为子任务，通过LLM和多Agent模块验证每一步推理和最终结果。实验在GAIA和BigCodeBench数据集上进行，与GPT-4o LLM-as-a-Judge基线相比，该Judge Agent与人类评估的对齐度提升4.76%和10.52%。结论显示该框架能更全面、可扩展地评价复杂Agent系统，减少人工依赖。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Auto-Eval_Judge_Towards_a_General_Agentic_Framework_for_Task_Completion_Evaluation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities"><a href="#85-InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities" class="headerlink" title="85. InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/InfiAlign__A_Scalable_and_Sample-Efficient_Framework_for_Aligning_LLMs_to_Enhance_Reasoning_Capabili.pdf">InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">InfiX.ai</span></p>
<p>本文提出了InfiAlign框架，通过结合多维度自动化数据筛选、监督微调（SFT）和直接偏好优化（DPO），实现大语言模型在推理任务上的高效对齐。核心方法包括多维质量指标驱动的数据采样（涵盖多样性、难度、质量等），并采用模块化和可扩展设计，实现小数据量下的高性能推理能力。结论显示，InfiAlign在仅用12%数据量下达到与主流强基线相当性能，DPO进一步提升数学推理表现，实现了高数据利用率和易扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/InfiAlign_A_Scalable_and_Sample-Efficient_Framework_for_Aligning_LLMs_to_Enhance_Reasoning_Capabilities.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-Can-Large-Language-Models-Generate-Effective-Datasets-for-Emotion-Recognition-in-Conversations"><a href="#86-Can-Large-Language-Models-Generate-Effective-Datasets-for-Emotion-Recognition-in-Conversations" class="headerlink" title="86. Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Can_Large_Language_Models_Generate_Effective_Datasets_for_Emotion_Recognition_in_Conversations_.pdf">Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Hamburg</span></p>
<p>本文提出利用小型、通用的大语言模型（Vicuna 1.5-13B）结合结构化提示工程，自动合成多样化的对话情绪识别（ERC）数据集，包括自然分布和均衡分布两种类型，针对主流ERC基准（MELD、EmoryNLP、IEMOCAP）分别生成相应的数据集，并通过多种现有ERC分类模型验证其有效性。实验结果和统计检验显示，基于LLM生成的数据集能提升情感识别模型的鲁棒性和性能，并对标签分布失衡等问题具有积极影响，结论为该方法能够高效、可复现地生成高质量ERC数据集，促进下游模型性能提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Can_Large_Language_Models_Generate_Effective_Datasets_for_Emotion_Recognition_in_Conversations_.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-Group-Causal-Policy-Optimization-for-Post-Training-Large-Language-Models"><a href="#87-Group-Causal-Policy-Optimization-for-Post-Training-Large-Language-Models" class="headerlink" title="87. Group Causal Policy Optimization for Post-Training Large Language Models"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Group_Causal_Policy_Optimization_for_Post-Training_Large_Language_Models.pdf">Group Causal Policy Optimization for Post-Training Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Software, Chinese Academy of Sciences</span></p>
<p>本论文提出Group Causal Policy Optimization（GCPO）方法，通过引入结构化因果模型（SCM），发现LLM候选响应间因最终集成输出而产生因果关联，进而将候选输出投影到因果子空间，并结合KL正则项，引导策略优化在结构上更一致。实验结果表明，GCPO在多项数学与代码推理基准上均优于GRPO等现有方法，验证了因果结构建模对LLM后训练效果的提升和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Group_Causal_Policy_Optimization_for_Post-Training_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-LLM-based-Multi-Agent-Copilot-for-Quantum-Sensor"><a href="#88-LLM-based-Multi-Agent-Copilot-for-Quantum-Sensor" class="headerlink" title="88. LLM-based Multi-Agent Copilot for Quantum Sensor"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/LLM-based_Multi-Agent_Copilot_for_Quantum_Sensor.pdf">LLM-based Multi-Agent Copilot for Quantum Sensor</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Defense Technology</span></p>
<p>本文提出QCopilot，一个基于大语言模型（LLM）的多智能体框架，集成外部知识访问、主动学习和不确定性量化，用于量子传感器的设计与诊断。QCopilot通过专用智能体实现实验参数自适应优化、自动建模分析和独立故障诊断，实验在冷原子原型系统中实现了无人工干预下的高效制冷和异常参数自主识别，大幅提升了实验效率和智能化水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/LLM-based_Multi-Agent_Copilot_for_Quantum_Sensor.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-StructVRM-Aligning-Multimodal-Reasoning-with-Structured-and-Verifiable-Reward-Models"><a href="#89-StructVRM-Aligning-Multimodal-Reasoning-with-Structured-and-Verifiable-Reward-Models" class="headerlink" title="89. StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/StructVRM__Aligning_Multimodal_Reasoning_with_Structured_and_Verifiable_Reward_Models.pdf">StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ByteDance Seed China</span></p>
<p>StructVRM提出了一种基于模型的细粒度奖励机制，通过训练可验证的奖励模型（verifier），在多模态推理任务中对多子问题进行结构化、可验证的评分，实现了部分正确性反馈，并结合PPO强化学习优化模型推理能力。实验证明，StructVRM在六个公开多模态基准和高难度STEM-Bench上取得SOTA表现，显著提升了复杂现实推理任务中的多模态模型能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/StructVRM_Aligning_Multimodal_Reasoning_with_Structured_and_Verifiable_Reward_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-The-Term-‘Agent’-Has-Been-Diluted-Beyond-Utility-and-Requires-Redefinition"><a href="#90-The-Term-‘Agent’-Has-Been-Diluted-Beyond-Utility-and-Requires-Redefinition" class="headerlink" title="90. The Term ‘Agent’ Has Been Diluted Beyond Utility and Requires Redefinition"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/The_Term_'Agent'_Has_Been_Diluted_Beyond_Utility_and_Requires_Redefinition.pdf">The Term ‘Agent’ Has Been Diluted Beyond Utility and Requires Redefinition</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Duke University</span></p>
<p>本文提出了针对“agent”一词在人工智能领域被广泛滥用和定义模糊的重新定义框架。通过历史和现有用法分析，作者提出了五维度（环境交互、目标导向、时间一致性、学习适应性和自主性）评价体系，并明确了成为‘agent’的最低要求。结论指出，该框架有助于提升研究方法的严谨性、沟通的清晰度和政策制定的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/The_Term_%E2%80%98Agent%E2%80%99_Has_Been_Diluted_Beyond_Utility_and_Requires_Redefinition.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-LAG-Logic-Augmented-Generation-from-a-Cartesian-Perspective"><a href="#91-LAG-Logic-Augmented-Generation-from-a-Cartesian-Perspective" class="headerlink" title="91. LAG: Logic-Augmented Generation from a Cartesian Perspective"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/LAG__Logic-Augmented_Generation_from_a_Cartesian_Perspective.pdf">LAG: Logic-Augmented Generation from a Cartesian Perspective</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong Polytechnic University</span></p>
<p>该论文提出了LAG（Logic-Augmented Generation），一种针对大型语言模型检索增强生成（RAG）系统的创新推理管道。通过自适应问题分解、逻辑链推理、逻辑终止机制，LAG能将复杂问题拆解为原子子问题，按逻辑依赖顺序逐步解决，并在各环节有效防止错误传递。实验表明LAG在多跳问答和复杂推理任务上显著提升准确率和合理性，优于主流RAG方法，实现更接近人类认知的推理过程。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/LAG_Logic-Augmented_Generation_from_a_Cartesian_Perspective.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs"><a href="#92-MELLA-Bridging-Linguistic-Capability-and-Cultural-Groundedness-for-Low-Resource-Language-MLLMs" class="headerlink" title="92. MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MELLA__Bridging_Linguistic_Capability_and_Cultural_Groundedness_for_Low-Resource_Language_MLLMs.pdf">MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Artificial Intelligence Laboratory</span></p>
<p>本文提出一种针对低资源语言多模态大语言模型（MLLM）的双目标与双源数据框架，强调语言能力和文化扎根性。通过构建MELLA大规模多语种多模态数据集，分别采集本地网页原生alt-text（文化知识）和MLLM生成描述（语言能力），并联合训练，显著提升模型在八种低资源语言上的描述准确度和文化认知能力。实验表明，经过MELLA微调后，多模态模型在各项指标上均优于现有方法，实现了更丰富的“厚描述”。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MELLA_Bridging_Linguistic_Capability_and_Cultural_Groundedness_for_Low-Resource_Language_MLLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-GRAIL-Learning-to-Interact-with-Large-Knowledge-Graphs-for-Retrieval-Augmented-Reasoning"><a href="#93-GRAIL-Learning-to-Interact-with-Large-Knowledge-Graphs-for-Retrieval-Augmented-Reasoning" class="headerlink" title="93. GRAIL: Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/GRAIL_Learning_to_Interact_with_Large_Knowledge_Graphs_for_Retrieval_Augmented_Reasoning.pdf">GRAIL: Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute for AI Industry Research (AIR), Tsinghua University</span></p>
<p>GRAIL提出了一个针对大型知识图谱的交互式检索增强推理框架，通过LLM引导的数据合成、两阶段（监督微调+强化学习）训练和动态交互检索机制，实现多步高效、结构感知的信息检索与推理。实验结果显示，GRAIL在多个知识图谱问答基准上取得了显著性能提升，平均准确率提升21.01%、F1提升22.43%，同时大幅减少冗余检索信息，实现检索深度与精度的高效平衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/GRAIL_Learning_to_Interact_with_Large_Knowledge_Graphs_for_Retrieval_Augmented_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-MoMA-A-Mixture-of-Multimodal-Agents-Architecture-for-Enhancing-Clinical-Prediction-Modelling"><a href="#94-MoMA-A-Mixture-of-Multimodal-Agents-Architecture-for-Enhancing-Clinical-Prediction-Modelling" class="headerlink" title="94. MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MoMA__A_Mixture-of-Multimodal-Agents_Architecture_for_Enhancing_Clinical_Prediction_Modelling.pdf">MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Wisconsin-Madison</span></p>
<p>该论文提出了MoMA架构，通过多智能体协作，利用多个大语言模型（LLM）将多模态电子健康记录（EHR）数据中的非文本模态（如医学影像、实验室结果）转化为结构化文本摘要，再由聚合智能体整合所有文本后交给预测智能体用于临床预测。评测结果表明，MoMA在多个多模态临床任务上超越了现有主流方法，表现出更高的预测准确性和灵活性，且可灵活适配新模态，降低对大规模配对数据的依赖。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MoMA_A_Mixture-of-Multimodal-Agents_Architecture_for_Enhancing_Clinical_Prediction_Modelling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-MyCulture-Exploring-Malaysia’s-Diverse-Culture-under-Low-Resource-Language-Constraints"><a href="#95-MyCulture-Exploring-Malaysia’s-Diverse-Culture-under-Low-Resource-Language-Constraints" class="headerlink" title="95. MyCulture: Exploring Malaysia’s Diverse Culture under Low-Resource Language Constraints"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MyCulture__Exploring_Malaysia's_Diverse_Culture_under_Low-Resource_Language_Constraints.pdf">MyCulture: Exploring Malaysia’s Diverse Culture under Low-Resource Language Constraints</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universiti Malaya</span></p>
<p>本文提出MyCulture基准，是首个面向马来西亚、以马来语呈现的多元文化LLM评测基准，覆盖艺术、服饰、习俗、娱乐、食物和宗教六大领域，并采用创新的无标准选项的开放式多选题格式（包括多答案、排序、匹配三种类型）以减少猜测和格式偏倚。实验结果显示，传统封闭式多选题高估了模型的文化理解能力，采用开放式题型后主流LLM的成绩平均下降至少17%，凸显更严格、文化嵌入型评测的重要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MyCulture_Exploring_Malaysia%E2%80%99s_Diverse_Culture_under_Low-Resource_Language_Constraints.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-Large-Language-Models-Transform-Organic-Synthesis-From-Reaction-Prediction-to-Automation"><a href="#96-Large-Language-Models-Transform-Organic-Synthesis-From-Reaction-Prediction-to-Automation" class="headerlink" title="96. Large Language Models Transform Organic Synthesis: From Reaction Prediction to Automation"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Large_Language_Models_Transform_Organic_Synthesis_From_Reaction_Prediction_to_Automation.pdf">Large Language Models Transform Organic Synthesis: From Reaction Prediction to Automation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Government Boys Higher Secondary School, Bukera Sharif, Tando Allahyar, Affiliated with BISE Hyderabad, Sindh, Pakistan</span></p>
<p>本文系统综述了大型语言模型（LLMs）在有机合成中的应用进展，包括化学反应预测、逆合成规划、条件优化及与机器人平台的自动化集成。通过结合LLMs与图神经网络、强化学习和多模态数据，显著提升了反应预测准确率和实验自动化效率，同时讨论了数据偏差、模型解释性和安全风险等挑战。结论认为LLMs正在推动化学合成向高效、自动化和可持续方向变革，但仍需解决可靠性与可解释性等关键问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Large_Language_Models_Transform_Organic_Synthesis_From_Reaction_Prediction_to_Automation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-NomicLaw-Emergent-Trust-and-Strategic-Argumentation-in-LLMs-During-Collaborative-Law-Making"><a href="#97-NomicLaw-Emergent-Trust-and-Strategic-Argumentation-in-LLMs-During-Collaborative-Law-Making" class="headerlink" title="97. NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/NomicLaw__Emergent_Trust_and_Strategic_Argumentation_in_LLMs_During_Collaborative_Law-Making.pdf">NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Jyvaskyla</span></p>
<p>该论文提出NomicLaw框架，让多个开源大语言模型（LLMs）在法律治理场景下以“提案-论证-投票”循环协作立法，量化分析信任、互惠和联盟形成等社会动态，并通过主题分析揭示模型在多智能体协作中呈现的法律推理和说服能力。实验显示，模型异质性能减少自投、增加联盟切换并丰富法律论证类型，强调多样化模型组合可提升AI辅助立法的合理性和多元性，但当前LLM推理多为统计模式而非真正理解，需人类监管。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/NomicLaw_Emergent_Trust_and_Strategic_Argumentation_in_LLMs_During_Collaborative_Law-Making.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-A-Novel-Architecture-for-Symbolic-Reasoning-with-Decision-Trees-and-LLM-Agents"><a href="#98-A-Novel-Architecture-for-Symbolic-Reasoning-with-Decision-Trees-and-LLM-Agents" class="headerlink" title="98. A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Novel_Architecture_for_Symbolic_Reasoning_with_Decision_Trees_and_LLM_Agents.pdf">A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harvard University</span></p>
<p>本文提出一种结合决策树符号推理与大型语言模型（LLM）生成推理能力的多智能体系统架构。方法包括将决策树作为可调用的符号推理模块嵌入到由LLM驱动的协调推理流程中，通过中央编排器维护信念一致性并动态调用工具，实现结构化和非结构化知识的跨模态推理。实验表明该架构在ProofWriter、GSM8k和ARC等基准任务上推理一致性和准确率均超过现有方法，且系统具备高度可解释性和可扩展性，适用于医疗决策支持和科学发现等领域。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Novel_Architecture_for_Symbolic_Reasoning_with_Decision_Trees_and_LLM_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-B4DL-A-Benchmark-for-4D-LiDAR-LLM-in-Spatio-Temporal-Understanding"><a href="#99-B4DL-A-Benchmark-for-4D-LiDAR-LLM-in-Spatio-Temporal-Understanding" class="headerlink" title="99. B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding"></a>99. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/B4DL__A_Benchmark_for_4D_LiDAR_LLM_in_Spatio-Temporal_Understanding.pdf">B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea Advanced Institute of Science and Technology</span></p>
<p>该论文提出B4DL基准与数据集，专为训练和评估多模态大语言模型在4D LiDAR时空理解任务中而设计。方法包括创新的数据生成管线（融合多视角图像与人工注释，自动生成LiDAR时空场景的问答对），以及提出B4DL模型，通过LiDAR编码器、对齐层和元信息模块实现4D LiDAR与语言的高效对齐，采用分阶段训练策略提升模型空间与时序推理能力。实验证明B4DL模型在空间及时序理解上显著优于现有3D或视频多模态LLM，在多项任务和跨数据集泛化方面表现突出。结论：B4DL为4D LiDAR多模态理解提供了首个高质量基准和方法体系，有效推动了真实世界时空动态理解领域的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/B4DL_A_Benchmark_for_4D_LiDAR_LLM_in_Spatio-Temporal_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-Resource-Limited-Joint-Multimodal-Sentiment-Reasoning-and-Classification-via-Chain-of-Thought-Enhancement-and-Distillation"><a href="#100-Resource-Limited-Joint-Multimodal-Sentiment-Reasoning-and-Classification-via-Chain-of-Thought-Enhancement-and-Distillation" class="headerlink" title="100. Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation"></a>100. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Resource-Limited_Joint_Multimodal_Sentiment_Reasoning_and_Classification_via_Chain-of-Thought_Enhanc.pdf">Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northeastern University</span></p>
<p>本文提出MulCoT-RD框架，针对资源受限场景下的多模态情感推理与分类（JMSRC）任务，采用“教师-助理-学生”三阶段推理蒸馏范式，通过多任务学习及结构化CoT（Chain-of-Thought）增强和软硬标签联合优化，使轻量级多模态大模型能自主生成高质量情感推理链并实现高效情感分类。实验表明，MulCoT-RD在多个公开数据集上实现了较优的情感分类性能和推理解释性，且模型仅3B参数，具备良好的部署适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Resource-Limited_Joint_Multimodal_Sentiment_Reasoning_and_Classification_via_Chain-of-Thought_Enhancement_and_Distillation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="101-VFlowOpt-A-Token-Pruning-Framework-for-LMMs-with-Visual-Information-Flow-Guided-Optimization"><a href="#101-VFlowOpt-A-Token-Pruning-Framework-for-LMMs-with-Visual-Information-Flow-Guided-Optimization" class="headerlink" title="101. VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization"></a>101. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/VFlowOpt__A_Token_Pruning_Framework_for_LMMs_with_Visual_Information_Flow-Guided_Optimization.pdf">VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai AI Laboratory</span></p>
<p>本文提出VFlowOpt，一种针对大型多模态模型（LMMs）的可微视觉信息流引导的视觉token裁剪框架。方法通过结合注意力校准和信息熵，分阶段评估视觉token重要性，采用回收机制减少信息损失，并利用视觉信息流优化各阶段裁剪超参数。实验表明，VFlowOpt可在保留10%视觉token的情况下仍维持90%模型性能，显著降低显存占用和推理时延，适用于实际部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/VFlowOpt_A_Token_Pruning_Framework_for_LMMs_with_Visual_Information_Flow-Guided_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="102-FAITH-A-Framework-for-Assessing-Intrinsic-Tabular-Hallucinations-in-Finance"><a href="#102-FAITH-A-Framework-for-Assessing-Intrinsic-Tabular-Hallucinations-in-Finance" class="headerlink" title="102. FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in Finance"></a>102. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FAITH__A_Framework_for_Assessing_Intrinsic_Tabular_Hallucinations_in_finance.pdf">FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in Finance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Asian Institute of Digital Finance, National University of Singapore</span></p>
<p>该论文提出了用于金融领域评估大型语言模型（LLM）内在幻觉的新框架，包括自动化数据集构建、基于真实年报的金融幻觉基准数据集，以及四类金融推理复杂度的分类方法。实验结果表明，主流LLM在复杂金融表格推理任务中依然存在较高幻觉率，准确性随推理复杂度下降，表明当前模型在高精度金融应用部署上仍有显著挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FAITH_A_Framework_for_Assessing_Intrinsic_Tabular_Hallucinations_in_Finance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="103-Incident-Response-Planning-Using-a-Lightweight-Large-Language-Model-with-Reduced-Hallucination"><a href="#103-Incident-Response-Planning-Using-a-Lightweight-Large-Language-Model-with-Reduced-Hallucination" class="headerlink" title="103. Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination"></a>103. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Incident_Response_Planning_Using_a_Lightweight_Large_Language_Model_with_Reduced_Hallucination.pdf">Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Melbourne</span></p>
<p>本文提出了一种集成轻量级大语言模型（LLM）用于网络安全事件响应的方法，包括指令微调、信息检索和决策规划三步。该方法通过理论证明有效减少幻觉概率，并在多数据集上实验显示比前沿LLM恢复时间缩短最高22%，泛化能力强且资源消耗低。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Incident_Response_Planning_Using_a_Lightweight_Large_Language_Model_with_Reduced_Hallucination.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="104-RLHF-Fine-Tuning-of-LLMs-for-Alignment-with-Implicit-User-Feedback-in-Conversational-Recommenders"><a href="#104-RLHF-Fine-Tuning-of-LLMs-for-Alignment-with-Implicit-User-Feedback-in-Conversational-Recommenders" class="headerlink" title="104. RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders"></a>104. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/RLHF_Fine-Tuning_of_LLMs_for_Alignment_with_Implicit_User_Feedback_in_Conversational_Recommenders.pdf">RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northeastern University</span></p>
<p>该论文提出了一种将RLHF（基于人类反馈的强化学习）用于大语言模型（LLM）在多轮对话推荐系统中的微调方法，通过隐式用户反馈（如停留时间、情感变化、语义一致性）构建奖励模型，并用PPO优化LLM生成的推荐结果。实验结果表明，该方法在REDIAL和OpenDialKG等数据集上显著提升了推荐准确率、对话流畅性和用户满意度，验证了隐式信号指导下的自适应推荐能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/RLHF_Fine-Tuning_of_LLMs_for_Alignment_with_Implicit_User_Feedback_in_Conversational_Recommenders.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="105-Understanding-and-Mitigating-Errors-of-LLM-Generated-RTL-Code"><a href="#105-Understanding-and-Mitigating-Errors-of-LLM-Generated-RTL-Code" class="headerlink" title="105. Understanding and Mitigating Errors of LLM-Generated RTL Code"></a>105. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Understanding_and_Mitigating_Errors_of_LLM-Generated_RTL_Code.pdf">Understanding and Mitigating Errors of LLM-Generated RTL Code</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Computing Technology, Chinese Academy of Sciences</span></p>
<p>本论文系统分析LLM生成的RTL代码错误类型，发现主要问题为缺乏RTL编程知识、理解电路概念不足、描述歧义和多模态输入误解。作者提出RAG知识补充、规则描述修正、多模态转换及仿真调试等纠错机制，并集成至代码生成流程，在VerilogEval基准上准确率达91%，显著优于基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Understanding_and_Mitigating_Errors_of_LLM-Generated_RTL_Code.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="106-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs"><a href="#106-MoBE-Mixture-of-Basis-Experts-for-Compressing-MoE-based-LLMs" class="headerlink" title="106. MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs"></a>106. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MoBE__Mixture-of-Basis-Experts_for_Compressing_MoE-based_LLMs.pdf">MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Inclusion AI</span></p>
<p>该论文提出Mixture-of-Basis-Experts (MoBE) 方法，通过将专家模块中的权重矩阵分解为专家特有的变换矩阵和共享的基矩阵线性组合，实现大规模MoE语言模型的高效参数压缩。实验表明，MoBE在主流MoE模型上能减少24%-30%的参数，仅带来1%-2%的准确率下降，显著优于现有压缩方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MoBE_Mixture-of-Basis-Experts_for_Compressing_MoE-based_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="107-Pruning-Large-Language-Models-by-Identifying-and-Preserving-Functional-Networks"><a href="#107-Pruning-Large-Language-Models-by-Identifying-and-Preserving-Functional-Networks" class="headerlink" title="107. Pruning Large Language Models by Identifying and Preserving Functional Networks"></a>107. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Pruning_Large_Language_Models_by_Identifying_and_Preserving_Functional_Networks.pdf">Pruning Large Language Models by Identifying and Preserving Functional Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Automation, Northwestern Polytechnical University</span></p>
<p>本文提出了一种受神经科学启发的大语言模型（LLM）结构化剪枝新方法，通过独立成分分析（ICA）分解每层神经元信号，识别并保留LLM中的功能网络，实现高效剪枝。实验结果表明，该方法在不同模型和剪枝率下均优于现有主流剪枝方法，有效降低计算和内存消耗，同时保持模型性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Pruning_Large_Language_Models_by_Identifying_and_Preserving_Functional_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="108-Cross-LoRA-A-Data-Free-LoRA-Transfer-Framework-across-Heterogeneous-LLMs"><a href="#108-Cross-LoRA-A-Data-Free-LoRA-Transfer-Framework-across-Heterogeneous-LLMs" class="headerlink" title="108. Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs"></a>108. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Cross-LoRA__A_Data-Free_LoRA_Transfer_Framework_across_Heterogeneous_LLMs.pdf">Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Baidu Inc</span></p>
<p>本文提出Cross-LoRA，一种无需数据和训练即可将LoRA适配器在不同大语言模型间迁移的框架。方法包括通过截断奇异值分解（SVD）和Frobenius最优线性变换对源、目标模型进行子空间对齐（LoRA-Align），然后将源LoRA权重更新投影到目标模型参数空间（LoRA-Shift），实现架构无关的迁移。实验显示Cross-LoRA在多个NLP推理基准上实现了对基线模型的稳定提升，性能接近于直接训练的LoRA适配器，且在单卡20分钟内完成迁移，适合低资源和实际应用场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Cross-LoRA_A_Data-Free_LoRA_Transfer_Framework_across_Heterogeneous_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="109-SPEX-A-Vision-Language-Model-for-Land-Cover-Extraction-on-Spectral-Remote-Sensing-Images"><a href="#109-SPEX-A-Vision-Language-Model-for-Land-Cover-Extraction-on-Spectral-Remote-Sensing-Images" class="headerlink" title="109. SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images"></a>109. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SPEX__A_Vision-Language_Model_for_Land_Cover_Extraction_on_Spectral_Remote_Sensing_Images.pdf">SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xinjiang University</span></p>
<p>该论文提出了SPEX模型，通过融合多尺度视觉特征聚合、Token上下文压缩和多光谱视觉预训练，实现了基于指令的多模态大语言模型，用于遥感多光谱影像的地表覆盖物提取。构建了SPIE数据集，将地物光谱先验编码到可被LLM识别的文本属性，显著提升了对植被、水体、建筑等类别的像素级识别精度。实验结果表明，SPEX在五个公开多光谱数据集上均优于现有方法，且生成结果更具解释性与交互性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SPEX_A_Vision-Language_Model_for_Land_Cover_Extraction_on_Spectral_Remote_Sensing_Images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="110-EvoGraph-Hybrid-Directed-Graph-Evolution-toward-Software-3-0"><a href="#110-EvoGraph-Hybrid-Directed-Graph-Evolution-toward-Software-3-0" class="headerlink" title="110. EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0"></a>110. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EvoGraph__Hybrid_Directed_Graph_Evolution_toward_Software_3.0.pdf">EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AutoHand AI</span></p>
<p>EvoGraph提出了一个利用专门小语言模型（SLM）驱动的混合定向图进化框架，用于自动演化企业软件系统的代码、文档、构建流程和工单等全部软件制品。其方法包括基于有类型定向图的统一表征、多种SLM引导的变异算子、带安全约束的多目标进化选择，并在多语言现代化（COBOL、.NET、Lisp等）任务中实现了82-96%语义等价和90%算力节省。结论显示，EvoGraph有效支持企业级遗留系统现代化，SLM优于大模型并经济实用，推进了自主演化的软件3.0路径。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EvoGraph_Hybrid_Directed_Graph_Evolution_toward_Software_3.0.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="111-QA-Dragon-Query-Aware-Dynamic-RAG-System-for-Knowledge-Intensive-Visual-Question-Answering"><a href="#111-QA-Dragon-Query-Aware-Dynamic-RAG-System-for-Knowledge-Intensive-Visual-Question-Answering" class="headerlink" title="111. QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering"></a>111. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/QA-Dragon__Query-Aware_Dynamic_RAG_System_for_Knowledge-Intensive_Visual_Question_Answering.pdf">QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong Polytechnic University</span></p>
<p>该论文提出了QA-Dragon系统，通过引入领域路由器、搜索路由器、工具路由器以及多阶段重排序机制，实现了动态多模态检索增强生成（RAG）以提升知识密集型视觉问答（VQA）的准确性。其方法结合了图像和文本的混合检索、领域感知推理和多轮多跳推理，有效提高了复杂VQA任务中的答案准确率和知识重叠度，在KDD Cup 2025 Meta CRAG-MM Challenge中全面超越现有强基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/QA-Dragon_Query-Aware_Dynamic_RAG_System_for_Knowledge-Intensive_Visual_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="112-Attention-Basin-Why-Contextual-Position-Matters-in-Large-Language-Models"><a href="#112-Attention-Basin-Why-Contextual-Position-Matters-in-Large-Language-Models" class="headerlink" title="112. Attention Basin: Why Contextual Position Matters in Large Language Models"></a>112. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Attention_Basin__Why_Contextual_Position_Matters_in_Large_Language_Models.pdf">Attention Basin: Why Contextual Position Matters in Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-sen University</span></p>
<p>本文系统揭示了大语言模型（LLM）在处理结构化输入时存在“attention basin”现象，即模型会对输入序列开头和结尾的信息分配更高关注，中间部分则被忽视。为此，作者提出了一种无需训练、模型无关的Attention-Driven Reranking（AttnRank）方法，通过少量校准集估算模型固有的注意力位置偏好，并据此重排关键内容，使其位于模型关注度高的位置。实验表明，AttnRank在多跳问答和few-shot学习等任务中，无需修改模型参数便能显著提升10种主流LLM的表现。结论：AttnRank高效缓解了位置偏差，提高了信息利用率，且与现有推理加速框架兼容。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Attention_Basin_Why_Contextual_Position_Matters_in_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="113-Beyond-Automation-Socratic-AI-Epistemic-Agency-and-the-Implications-of-the-Emergence-of-Orchestrated-Multi-Agent-Learning-Architectures"><a href="#113-Beyond-Automation-Socratic-AI-Epistemic-Agency-and-the-Implications-of-the-Emergence-of-Orchestrated-Multi-Agent-Learning-Architectures" class="headerlink" title="113. Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures"></a>113. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Beyond_Automation__Socratic_AI,_Epistemic_Agency,_and_the_Implications_of_the_Emergence_of_Orchestra.pdf">Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Kassel</span></p>
<p>该论文提出并实证验证了一种基于Socratic方法的对话式大语言模型AI导师，通过结构化提问促进学生批判性、独立性和反思性思维，并与通用AI聊天机器人进行了对比实验。结果显示，Socratic AI显著提升了学生的元认知参与和高阶思维，推动了AI教育从单一工具向多智能体协作生态系统转变，并对高等教育的教师角色、课程设计、基础设施及评估体系提出了系统性建议。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Beyond_Automation_Socratic_AI%2C_Epistemic_Agency%2C_and_the_Implications_of_the_Emergence_of_Orchestrated_Multi-Agent_Learning_Architectures.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="114-Align-Don’t-Divide-Revisiting-the-LoRA-Architecture-in-Multi-Task-Learning"><a href="#114-Align-Don’t-Divide-Revisiting-the-LoRA-Architecture-in-Multi-Task-Learning" class="headerlink" title="114. Align, Don’t Divide: Revisiting the LoRA Architecture in Multi-Task Learning"></a>114. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Align,_Don't_Divide__Revisiting_the_LoRA_Architecture_in_Multi-Task_Learning.pdf">Align, Don’t Divide: Revisiting the LoRA Architecture in Multi-Task Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Jilin University</span></p>
<p>本文系统分析了参数高效微调（PEFT）方法LoRA在多任务学习（MTL）中的主流多头&#x2F;多适配器结构，发现高结构复杂度和任务特异性分离并未带来更优泛化性能，反而简单单头高秩LoRA表现更优。基于此，作者提出Align-LoRA，通过在训练目标中引入KL或MMD对齐损失，显式增强任务间共享表征，无需增加推理开销且获得了显著优于复杂结构的多任务泛化效果。结论：对齐表征、强化任务共享知识比结构分离更关键，Align-LoRA为多任务PEFT提供了更高效新范式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Align%2C_Don%27t_Divide_Revisiting_the_LoRA_Architecture_in_Multi-Task_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="115-Aligning-LLMs-on-a-Budget-Inference-Time-Alignment-with-Heuristic-Reward-Models"><a href="#115-Aligning-LLMs-on-a-Budget-Inference-Time-Alignment-with-Heuristic-Reward-Models" class="headerlink" title="115. Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models"></a>115. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Aligning_LLMs_on_a_Budget__Inference-Time_Alignment_with_Heuristic_Reward_Models.pdf">Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Massachusetts Amherst</span></p>
<p>该论文提出HIA（Heuristic-Guided Inference-time Alignment）方法，通过轻量级启发式奖励模型和两阶段筛选机制，在不需微调和仅需黑盒访问的前提下，实现大语言模型推理时的多目标、个性化对齐。实验证明，在HELPSTEER和COMPRED等真实数据集上，HIA在相同推理预算下优于BoN采样、束搜索和贪心搜索，尤其在推理预算低至1-2次查询时提升显著。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Aligning_LLMs_on_a_Budget_Inference-Time_Alignment_with_Heuristic_Reward_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="116-Speech-LLMs-in-Low-Resource-Scenarios-Data-Volume-Requirements-and-the-Impact-of-Pretraining-on-High-Resource-Languages"><a href="#116-Speech-LLMs-in-Low-Resource-Scenarios-Data-Volume-Requirements-and-the-Impact-of-Pretraining-on-High-Resource-Languages" class="headerlink" title="116. Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages"></a>116. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Speech_LLMs_in_Low-Resource_Scenarios__Data_Volume_Requirements_and_the_Impact_of_Pretraining_on_Hig.pdf">Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Trento</span></p>
<p>该论文基于SLAM-ASR框架，研究了在低资源语音识别场景中，训练数据量及高资源语言预训练对模型性能的影响。作者通过将Whisper-large-v3-turbo语音编码器、轻量线性投影器和多语种LLM（EuroLLM&#x2F;Salamandra）结合，实验证明：需100-200小时训练数据才能达到Whisper-only模型效果，且投影器在高资源语言上预训练并迁移到低资源语言，尤其是在仅有10-15小时微调数据时，能显著提升识别准确率，多语种投影器进一步提高泛化能力。结论指出，预训练和微调策略为低资源语种语音识别提供了优化方向，但对数据量和跨领域泛化仍存在挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Speech_LLMs_in_Low-Resource_Scenarios_Data_Volume_Requirements_and_the_Impact_of_Pretraining_on_High-Resource_Languages.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="117-Towards-Assessing-Medical-Ethics-from-Knowledge-to-Practice"><a href="#117-Towards-Assessing-Medical-Ethics-from-Knowledge-to-Practice" class="headerlink" title="117. Towards Assessing Medical Ethics from Knowledge to Practice"></a>117. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Towards_Assessing_Medical_Ethics_from_Knowledge_to_Practice.pdf">Towards Assessing Medical Ethics from Knowledge to Practice</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong, Shenzhen</span></p>
<p>该论文提出了PrinciplismQA基准，用于系统性评估大语言模型（LLM）在医疗伦理领域的知识掌握与实际应用能力，结合多项选择题和开放式案例分析，采用专家校验与LLM自动评分流程。实验结果显示所有模型均存在“知识-实践鸿沟”，尤其在实际情境动态应用伦理原则（如仁慈性）方面表现较弱，医疗领域微调虽提升实际伦理表现但略有知识遗忘，未来需强化模型伦理对齐。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Towards_Assessing_Medical_Ethics_from_Knowledge_to_Practice.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="118-Exploring-Superior-Function-Calls-via-Reinforcement-Learning"><a href="#118-Exploring-Superior-Function-Calls-via-Reinforcement-Learning" class="headerlink" title="118. Exploring Superior Function Calls via Reinforcement Learning"></a>118. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Exploring_Superior_Function_Calls_via_Reinforcement_Learning.pdf">Exploring Superior Function Calls via Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AWorld Team, Inclusion AI</span></p>
<p>本文提出了FunRL，一种针对大语言模型函数调用任务的强化学习框架。方法创新性地将Chain-of-Thought（CoT）熵纳入GRPO优势估计，通过两阶段数据准备（LLM和AST评估）与精确的二元奖励函数，促进模型在推理和参数验证上探索多样化思路。实验表明，FunRL在BFCLv2基准上实现了开源模型最优的86.02%总体准确率，显著优于标准GRPO，特别是在代码预训练模型上提升效果明显。结论：FunRL显著提升了LLM工具调用的复杂推理能力和实际应用可靠性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Exploring_Superior_Function_Calls_via_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="119-EasySize-Elastic-Analog-Circuit-Sizing-via-LLM-Guided-Heuristic-Search"><a href="#119-EasySize-Elastic-Analog-Circuit-Sizing-via-LLM-Guided-Heuristic-Search" class="headerlink" title="119. EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search"></a>119. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EasySize__Elastic_Analog_Circuit_Sizing_via_LLM-Guided_Heuristic_Search.pdf">EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出EasySize，一种基于微调Qwen3-8B模型的轻量化模拟电路尺寸调整框架，通过结合大语言模型生成动态损失函数和启发式搜索（DE与PSO），实现跨工艺节点和电路拓扑的通用高效优化。实验表明EasySize在无需额外训练的情况下性能优于AutoCkt，并大幅减少仿真资源消耗，显著提升模拟电路设计自动化的效率与适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EasySize_Elastic_Analog_Circuit_Sizing_via_LLM-Guided_Heuristic_Search.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="120-JPS-Jailbreak-Multimodal-Large-Language-Models-with-Collaborative-Visual-Perturbation-and-Textual-Steering"><a href="#120-JPS-Jailbreak-Multimodal-Large-Language-Models-with-Collaborative-Visual-Perturbation-and-Textual-Steering" class="headerlink" title="120. JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering"></a>120. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/JPS__Jailbreak_Multimodal_Large_Language_Models_with_Collaborative_Visual_Perturbation_and_Textual_S.pdf">JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出JPS方法，通过协同优化视觉扰动与多智能体系统生成的文本引导，实现对多模态大语言模型（MLLM）的高质量越狱攻击。JPS将安全绕过任务分配给目标引导的图像对抗扰动，将高效恶意意图实现交由多智能体优化的引导提示，两者迭代协同提升攻击效果。实验结果表明，JPS在多种模型和基准上，越狱成功率（ASR）和恶意意图实现率（MIFR）均达到最新水平，有效提升了实际攻击效用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/JPS_Jailbreak_Multimodal_Large_Language_Models_with_Collaborative_Visual_Perturbation_and_Textual_Steering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="121-MedMKEB-A-Comprehensive-Knowledge-Editing-Benchmark-for-Medical-Multimodal-Large-Language-Models"><a href="#121-MedMKEB-A-Comprehensive-Knowledge-Editing-Benchmark-for-Medical-Multimodal-Large-Language-Models" class="headerlink" title="121. MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models"></a>121. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MedMKEB__A_Comprehensive_Knowledge_Editing_Benchmark_for_Medical_Multimodal_Large_Language_Models.pdf">MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>该论文提出MedMKEB，这是首个专为医学多模态大语言模型设计的知识编辑综合基准，涵盖视觉和文本信息的系统性编辑评估，包含可靠性、局部性、泛化性、可迁移性和鲁棒性五大指标。实验表明，现有知识编辑方法在医学多模态场景下表现有限，需要针对医学领域开发更精细化的编辑算法，MedMKEB将推动医学知识编辑技术的进步。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MedMKEB_A_Comprehensive_Knowledge_Editing_Benchmark_for_Medical_Multimodal_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="122-Finding-Needles-in-Images-Can-Multimodal-LLMs-Locate-Fine-Details"><a href="#122-Finding-Needles-in-Images-Can-Multimodal-LLMs-Locate-Fine-Details" class="headerlink" title="122. Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?"></a>122. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Finding_Needles_in_Images__Can_Multimodal_LLMs_Locate_Fine_Details_.pdf">Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fujitsu Research India</span></p>
<p>本论文提出NiM-Benchmark，用于评估多模态大语言模型（MLLMs）在复杂文档中定位和推理细粒度信息的能力，并提出Spot-IT方法：结合语义相似性分块和自适应高斯注意力，引导模型关注关键区域。实验表明，Spot-IT在多个DocVQA数据集上均显著优于现有方法，有效提升了复杂布局中文档细节提取的准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Finding_Needles_in_Images_Can_Multimodal_LLMs_Locate_Fine_Details_.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="123-Evaluation-of-Large-Language-Model-Architectures-in-Abstract-Meaning-Representation-Parsing"><a href="#123-Evaluation-of-Large-Language-Model-Architectures-in-Abstract-Meaning-Representation-Parsing" class="headerlink" title="123. Evaluation of Large Language Model Architectures in Abstract Meaning Representation Parsing"></a>123. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Evaluation_of_LLMs_in_AMR_Parsing.pdf">Evaluation of Large Language Model Architectures in Abstract Meaning Representation Parsing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University College London</span></p>
<p>本文系统评估了四种开源解码器式大语言模型（Phi-3.5、Gemma-2、LLaMA-3.2、DeepSeek-R1-LLaMA-Distilled）在AMR语义解析任务中的微调表现，采用LoRA高效参数微调和SMATCH图对齐指标，在LDC2020T02 Gold AMR3.0等数据集上进行对比分析。结果显示，经过简单微调后，LLM模型（尤其是LLaMA-3.2）在语义解析性能上接近专用SOTA解析器，Phi-3.5则在结构有效性上表现最优，证明LLM无需复杂架构即可高效完成AMR解析任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Evaluation_of_Large_Language_Model_Architectures_in_Abstract_Meaning_Representation_Parsing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="124-Skin-SOAP-A-Weakly-Supervised-Framework-for-Generating-Structured-SOAP-Notes"><a href="#124-Skin-SOAP-A-Weakly-Supervised-Framework-for-Generating-Structured-SOAP-Notes" class="headerlink" title="124. Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes"></a>124. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Skin-SOAP__A_Weakly_Supervised_Framework_for_Generating_Structured_SOAP_Notes.pdf">Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland, Baltimore County</span></p>
<p>该论文提出了Skin-SOAP，一种弱监督多模态框架，通过结合皮损图像和稀疏临床文本，利用生成式语言模型生成临床描述、检索增强知识整合，并对Vision-LLaMA模型进行参数高效微调，实现结构化SOAP病历自动生成。实验结果显示，Skin-SOAP在临床相关性和结构一致性上与最先进的LLM模型（如GPT-4o等）性能相当，有助于缓解医生文档负担并提升皮肤科临床流程效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Skin-SOAP_A_Weakly_Supervised_Framework_for_Generating_Structured_SOAP_Notes.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="125-SPaRFT-Self-Paced-Reinforcement-Fine-Tuning-for-Large-Language-Models"><a href="#125-SPaRFT-Self-Paced-Reinforcement-Fine-Tuning-for-Large-Language-Models" class="headerlink" title="125. SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models"></a>125. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/SPaRFT__Self-Paced_Reinforcement_Fine-Tuning_for_Large_Language_Models.pdf">SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Deakin University</span></p>
<p>本论文提出SPaRFT，一种自适应的强化学习微调框架，针对大语言模型的高效训练。方法首先使用语义和难度结合的聚类进行数据精简，再以多臂赌博机动态分配训练样本，实现性能驱动的课程学习。实验显示，SPaRFT在数学推理任务中以极少样本达到或超越现有方法，显著提升小模型推理能力，同时资源消耗极低。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/SPaRFT_Self-Paced_Reinforcement_Fine-Tuning_for_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="126-Can-Large-Language-Models-Integrate-Spatial-Data-Empirical-Insights-into-Reasoning-Strengths-and-Computational-Weaknesses"><a href="#126-Can-Large-Language-Models-Integrate-Spatial-Data-Empirical-Insights-into-Reasoning-Strengths-and-Computational-Weaknesses" class="headerlink" title="126. Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses"></a>126. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Can_Large_Language_Models_Integrate_Spatial_Data__Empirical_Insights_into_Reasoning_Strengths_and_Co.pdf">Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Washington</span></p>
<p>本论文系统评估了大语言模型（LLM）在城市空间数据整合中的能力，提出通过引入几何特征（如最小角度、最小距离、最大重叠面积）显著提升LLM在空间数据匹配、融合等任务的表现。实验发现，LLM难以仅凭自然语言指令完成空间推理，但若给定相关特征，则可推断出合理阈值，准确率最高可达99.5%，提出的“review-and-refine”两步自我修正方法进一步提升了结果的可靠性。结论认为，LLM能有效辅助空间数据集成，但尚不适合复杂计算几何任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Can_Large_Language_Models_Integrate_Spatial_Data_Empirical_Insights_into_Reasoning_Strengths_and_Computational_Weaknesses.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="127-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data"><a href="#127-R-Zero-Self-Evolving-Reasoning-LLM-from-Zero-Data" class="headerlink" title="127. R-Zero: Self-Evolving Reasoning LLM from Zero Data"></a>127. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/R-Zero__Self-Evolving_Reasoning_LLM_from_Zero_Data.pdf">R-Zero: Self-Evolving Reasoning LLM from Zero Data</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent AI Seattle Lab</span></p>
<p>本文提出了R-Zero框架，实现了无需任何外部数据、完全自主进化的推理型大语言模型。方法采用双模型协同进化机制（Challenger和Solver），通过Group Relative Policy Optimization（GRPO）实现模型自我生成、过滤与解答难题的循环，完全摆脱人工标注与现有任务依赖。实验证明，R-Zero能显著提升不同基础LLM在数学及通用推理领域的能力，并可作为有监督微调的有效增强方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/R-Zero_Self-Evolving_Reasoning_LLM_from_Zero_Data.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="128-A-Multi-Stage-Large-Language-Model-Framework-for-Extracting-Suicide-Related-Social-Determinants-of-Health"><a href="#128-A-Multi-Stage-Large-Language-Model-Framework-for-Extracting-Suicide-Related-Social-Determinants-of-Health" class="headerlink" title="128. A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health"></a>128. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Multi-Stage_Large_Language_Model_Framework_for_Extracting_Suicide-Related_Social_Determinants_of_H.pdf">A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Cockrell School of Engineering, The University of Texas at Austin</span></p>
<p>本论文提出一种多阶段大语言模型（LLM）框架，包括上下文检索、相关性验证和SDoH因子抽取三个步骤，实现对自杀相关社会健康决定因素（SDoH）从非结构化文本中的高效提取。方法在零样本设定下，对比BioBERT、GPT-3.5-turbo和DeepSeek-R1等基线，在提取少见和常见SDoH因子上取得更高的准确率和召回率，并通过提供中间解释提高了模型可解释性和人工标注效率。结论：本方法提升了从文本中提取自杀相关SDoH因子的准确性与透明度，有助于早期风险识别和干预策略制定。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Multi-Stage_Large_Language_Model_Framework_for_Extracting_Suicide-Related_Social_Determinants_of_Health.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="129-Situated-Epistemic-Infrastructures-A-Diagnostic-Framework-for-Post-Coherence-Knowledge"><a href="#129-Situated-Epistemic-Infrastructures-A-Diagnostic-Framework-for-Post-Coherence-Knowledge" class="headerlink" title="129. Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge"></a>129. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Situated_Epistemic_Infrastructures__A_Diagnostic_Framework_for_Post-Coherence_Knowledge.pdf">Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Toronto</span></p>
<p>本文提出了“Situated Epistemic Infrastructures (SEI)”框架，用以诊断和分析在大语言模型（LLM）等AI技术推动下，知识生产、流通和权威认定过程中的基础结构变革。该方法强调通过基础设施、权力签名、符号压缩和故障动态四个维度，分析知识权威是如何在算法、机构、人类协作等混合系统中不断被重塑。结论指出，SEI能够帮助学者和机构识别和适应由AI驱动的知识体系不稳定性，促进更具反思性和适应性的知识组织模式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Situated_Epistemic_Infrastructures_A_Diagnostic_Framework_for_Post-Coherence_Knowledge.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="130-A-Metric-for-MLLM-Alignment-in-Large-scale-Recommendation"><a href="#130-A-Metric-for-MLLM-Alignment-in-Large-scale-Recommendation" class="headerlink" title="130. A Metric for MLLM Alignment in Large-scale Recommendation"></a>130. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/A_Metric_for_MLLM_Alignment_in_Large-scale_Recommendation.pdf">A Metric for MLLM Alignment in Large-scale Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiaohongshu Inc.</span></p>
<p>本文提出了Leakage Impact Score (LIS)，一种用于多模态大语言模型（MLLM）在大规模推荐系统中对齐评价的新指标。LIS通过预先量化偏好数据的上界，辅助MLLM对齐流程，显著提升了多模态推荐部署效率。实验在小红书真实场景下，包括内容推荐和广告推荐，A&#x2F;B测试显示用户时长、广告价值等指标均有显著提升，验证了方法的实际价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/A_Metric_for_MLLM_Alignment_in_Large-scale_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="131-Learning-Geometric-Aware-Quadrature-Rules-for-Functional-Minimization"><a href="#131-Learning-Geometric-Aware-Quadrature-Rules-for-Functional-Minimization" class="headerlink" title="131. Learning Geometric-Aware Quadrature Rules for Functional Minimization"></a>131. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Learning_Geometric-Aware_Quadrature_Rules_for_Functional_Minimization.pdf">Learning Geometric-Aware Quadrature Rules for Functional Minimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of the Aegean</span></p>
<p>该论文提出了QuadrANN，一种基于图神经网络(GNN)的新型架构，用于从非均匀点云几何中直接学习最优积分权重，实现对变分原理下泛函最小化问题的高效数值积分。通过多层消息传递结合局部密度和全局域形状特征，QuadrANN显著降低了积分估计的方差，并在多种积分和PDE求解任务中优于传统QMC方法，提升了深度学习变分求解器的稳定性和精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Learning_Geometric-Aware_Quadrature_Rules_for_Functional_Minimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="132-MolSnap-Snap-Fast-Molecular-Generation-with-Latent-Variational-Mean-Flow"><a href="#132-MolSnap-Snap-Fast-Molecular-Generation-with-Latent-Variational-Mean-Flow" class="headerlink" title="132. MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow"></a>132. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MolSnap__Snap-Fast_Molecular_Generation_with_Latent_Variational_Mean_Flow.pdf">MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Kentucky</span></p>
<p>该论文提出了MolSnap框架，通过创新的因果感知Transformer（CAT）和变分均值流（VMF）方法，实现了基于文本描述的高质量、多样化分子生成。CAT显式建模分子图与文本间的因果依赖，VMF将潜在空间建模为高斯混合分布，支持高效一步推断。实验表明方法在新颖性、多样性和有效性上均优于SOTA模型，且推理速度提升显著。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MolSnap_Snap-Fast_Molecular_Generation_with_Latent_Variational_Mean_Flow.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="133-Multi-Modal-Multi-Behavior-Sequential-Recommendation-with-Conditional-Diffusion-Based-Feature-Denoising"><a href="#133-Multi-Modal-Multi-Behavior-Sequential-Recommendation-with-Conditional-Diffusion-Based-Feature-Denoising" class="headerlink" title="133. Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising"></a>133. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Multi-Modal_Multi-Behavior_Sequential_Recommendation_with_Conditional_Diffusion-Based_Feature_Denois.pdf">Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Takway.AI</span></p>
<p>本文提出了一种多模态多行为序列推荐模型M3BSR，通过条件扩散模型分别对多模态特征和行为特征进行去噪，并引入多专家兴趣提取层以建模用户在不同模态和行为下的共性和特性兴趣。实验证明，M3BSR在公开数据集上显著优于现有先进方法，提升了偏好建模和推荐准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Multi-Modal_Multi-Behavior_Sequential_Recommendation_with_Conditional_Diffusion-Based_Feature_Denoising.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="134-EnergyPatchTST-Multi-scale-Time-Series-Transformers-with-Uncertainty-Estimation-for-Energy-Forecasting"><a href="#134-EnergyPatchTST-Multi-scale-Time-Series-Transformers-with-Uncertainty-Estimation-for-Energy-Forecasting" class="headerlink" title="134. EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting"></a>134. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/EnergyPatchTST__Multi-scale_Time_Series_Transformers_with_Uncertainty_Estimation_for_Energy_Forecast.pdf">EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai University</span></p>
<p>该论文提出EnergyPatchTST模型，用于能源领域的时间序列预测。方法包括多尺度特征提取、蒙特卡罗Dropout不确定性估计、未来变量集成以及预训练-微调迁移学习，有效提升能源预测的准确性和可靠性。实验结果显示，在多个能源数据集上，该方法较主流方法误差降低7-12%，并提供了可靠的不确定性区间预测，显著促进了能源领域的时间序列分析性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/EnergyPatchTST_Multi-scale_Time_Series_Transformers_with_Uncertainty_Estimation_for_Energy_Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="135-Harmonic-fractal-transformation-for-modeling-complex-neuronal-effects-from-bursting-and-noise-shaping-to-waveform-sensitivity-and-noise-induced-subthreshold-spiking"><a href="#135-Harmonic-fractal-transformation-for-modeling-complex-neuronal-effects-from-bursting-and-noise-shaping-to-waveform-sensitivity-and-noise-induced-subthreshold-spiking" class="headerlink" title="135. Harmonic fractal transformation for modeling complex neuronal effects: from bursting and noise shaping to waveform sensitivity and noise-induced subthreshold spiking"></a>135. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Harmonic_fractal_transformation_for_modeling_complex_neuronal_effects__from_bursting_and_noise_shapi.pdf">Harmonic fractal transformation for modeling complex neuronal effects: from bursting and noise shaping to waveform sensitivity and noise-induced subthreshold spiking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">90 Navigation street, Birmingham, UK</span></p>
<p>本文提出了谐波分形变换（HFT）作为一种全新的频率映射方法，能够以简单形式复刻神经元复杂效应，包括信号激发、噪声整形、波形敏感性和亚阈值噪声诱导脉冲。HFT通过分形重组输入频谱，激发新的谐波分量，实现对噪声的鲁棒性和信号放大，结论表明该方法能灵活建模多种脉冲形态与神经元特性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Harmonic_fractal_transformation_for_modeling_complex_neuronal_effects_from_bursting_and_noise_shaping_to_waveform_sensitivity_and_noise-induced_subthreshold_spiking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="136-Estimating-Musical-Surprisal-from-Audio-in-Autoregressive-Diffusion-Model-Noise-Spaces"><a href="#136-Estimating-Musical-Surprisal-from-Audio-in-Autoregressive-Diffusion-Model-Noise-Spaces" class="headerlink" title="136. Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces"></a>136. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Estimating_Musical_Surprisal_from_Audio_in_Autoregressive_Diffusion_Model_Noise_Spaces.pdf">Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Johannes Kepler University Linz</span></p>
<p>该论文提出利用自回归扩散模型（EDM和RFF）在连续音频嵌入空间估计音乐惊奇性（surprisal），通过信息含量（IC）和负对数似然（NLL）衡量模型性能，并在单音高惊奇性和多轨音频分段检测两任务上与GIVT模型对比。实验结果显示，扩散模型在描述音乐数据和捕捉音高惊奇性方面优于GIVT模型，且在适当噪声水平下对音色具有更高不变性，提升分段检测精度与召回率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Estimating_Musical_Surprisal_from_Audio_in_Autoregressive_Diffusion_Model_Noise_Spaces.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="137-FlowState-Sampling-Rate-Invariant-Time-Series-Forecasting"><a href="#137-FlowState-Sampling-Rate-Invariant-Time-Series-Forecasting" class="headerlink" title="137. FlowState: Sampling Rate Invariant Time Series Forecasting"></a>137. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FlowState__Sampling_Rate_Invariant_Time_Series_Forecasting.pdf">FlowState: Sampling Rate Invariant Time Series Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IBM Research Europe – Zurich, Switzerland</span></p>
<p>该论文提出了FlowState，一种结合S5状态空间模型（SSM）编码器和功能基解码器（FBD）的时间序列基础模型，实现了对不同采样率的自适应与连续时间建模。通过并行预测训练方案和因果归一化，FlowState在GIFT-ZS和Chronos-ZS基准上达到新的SOTA，优于体积更大的模型，具备出色的泛化能力和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FlowState_Sampling_Rate_Invariant_Time_Series_Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="138-FDC-Net-Rethinking-the-association-between-EEG-artifact-removal-and-multi-dimensional-affective-computing"><a href="#138-FDC-Net-Rethinking-the-association-between-EEG-artifact-removal-and-multi-dimensional-affective-computing" class="headerlink" title="138. FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing"></a>138. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/FDC-Net__Rethinking_the_association_between_EEG_artifact_removal_and_multi-dimensional_affective_com.pdf">FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing University of Technology</span></p>
<p>本文提出了一种端到端的EEG情感识别与去伪影协同框架FDC-Net，采用双向梯度反馈优化和频带可学习编码的Transformer（EEGSPTransformer），实现了去噪与情感识别的深度耦合与动态协作。实验结果表明，在强噪声干扰下，该方法在DEAP和DREAMER数据集上的去噪和情感识别性能均优于现有主流方法，验证了其鲁棒性和有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/FDC-Net_Rethinking_the_association_between_EEG_artifact_removal_and_multi-dimensional_affective_computing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="139-Balancing-Accuracy-and-Novelty-with-Sub-Item-Popularity"><a href="#139-Balancing-Accuracy-and-Novelty-with-Sub-Item-Popularity" class="headerlink" title="139. Balancing Accuracy and Novelty with Sub-Item Popularity"></a>139. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Balancing_Accuracy_and_Novelty_with_Sub-Item_Popularity.pdf">Balancing Accuracy and Novelty with Sub-Item Popularity</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Politecnico di Bari</span></p>
<p>本文提出在Transformer-based推荐系统RecJPQ中引入个性化流行度信号，包括基于item-ID与sub-ID的个性化流行度分数（PPS和sPPS），通过对sub-ID流行度建模，实现更细粒度的用户重复行为捕捉。实验证明，sub-ID粒度的流行度建模在提升推荐新颖性的同时不损失准确性，并可灵活调控准确性与新颖性的权衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Balancing_Accuracy_and_Novelty_with_Sub-Item_Popularity.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="140-Hybrid-quantum-tensor-networks-for-aeroelastic-applications"><a href="#140-Hybrid-quantum-tensor-networks-for-aeroelastic-applications" class="headerlink" title="140. Hybrid quantum tensor networks for aeroelastic applications"></a>140. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Hybrid_quantum_tensor_networks_for_aeroelastic_applications.pdf">Hybrid quantum tensor networks for aeroelastic applications</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute for AI Safety and Security, German Aerospace Center (DLR)</span></p>
<p>本文提出了一种面向气动弹性问题的混合量子张量网络算法，集成了张量网络（TN）降维、数据编码和张量网络启发的可训练变分量子电路（VQC），实现端到端训练。方法在气动弹性时序数据的二分类任务中取得了极高准确率（F1&gt;0.99），并在多&#x2F;单变量回归任务中表现出有前景的性能，但模型的超参数选择依然是主要挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Hybrid_quantum_tensor_networks_for_aeroelastic_applications.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="141-Salt-Rock-Creep-Deformation-Forecasting-Using-Deep-Neural-Networks-and-Analytical-Models-for-Subsurface-Energy-Storage-Applications"><a href="#141-Salt-Rock-Creep-Deformation-Forecasting-Using-Deep-Neural-Networks-and-Analytical-Models-for-Subsurface-Energy-Storage-Applications" class="headerlink" title="141. Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications"></a>141. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Salt-Rock_Creep_Deformation_Forecasting_Using_Deep_Neural_Networks_and_Analytical_Models_for_Subsurf.pdf">Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IIT (ISM) Dhanbad</span></p>
<p>该论文提出了融合深度神经网络（N-BEATS、TCN、RNN、Transformer）与分析模型（如Power Law、Burger等）的方法，对盐岩在不同压力下的蠕变变形进行时间序列预测。结果显示，深度神经网络（尤其N-BEATS和TCN）在预测精度上优于传统统计和分析模型，准确率提升15-20%，为地下盐穴能源储存的安全管理提供了更可靠的建模基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Salt-Rock_Creep_Deformation_Forecasting_Using_Deep_Neural_Networks_and_Analytical_Models_for_Subsurface_Energy_Storage_Applications.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="142-High-Dimensional-Differentially-Private-Quantile-Regression-Distributed-Estimation-and-Statistical-Inference"><a href="#142-High-Dimensional-Differentially-Private-Quantile-Regression-Distributed-Estimation-and-Statistical-Inference" class="headerlink" title="142. High-Dimensional Differentially Private Quantile Regression: Distributed Estimation and Statistical Inference"></a>142. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/High-Dimensional_Differentially_Private_Quantile_Regression__Distributed_Estimation_and_Statistical_.pdf">High-Dimensional Differentially Private Quantile Regression: Distributed Estimation and Statistical Inference</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Statistics and Data Science, Shanghai University of Finance and Economics</span></p>
<p>该论文提出了一种适用于高维分布式数据的差分隐私分位数回归方法，核心包括利用Newton型变换将非光滑分位数回归问题转化为普通最小二乘问题，并结合Noisy Hard Thresholding算法实现迭代式分布式参数估计，保证稀疏性和差分隐私；在推断阶段，提出了差分隐私去偏估计和分布式私有bootstrap方法用于多重假设检验和置信区间构建。理论分析给出了估计误差界和推断一致性，实验证明算法在异质和重尾分布下具有稳健性和有效性，且揭示了隐私保护与统计精度之间的基本权衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/High-Dimensional_Differentially_Private_Quantile_Regression_Distributed_Estimation_and_Statistical_Inference.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="143-Graph-based-Event-Log-Repair"><a href="#143-Graph-based-Event-Log-Repair" class="headerlink" title="143. Graph-based Event Log Repair"></a>143. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Graph-based_Event_Log_Repair.pdf">Graph-based Event Log Repair</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Trento</span></p>
<p>该论文提出了SANAGRAPH，一种利用异构图神经网络（HGNN）编码事件日志痕迹并修复其中缺失数据的方法。方法将日志中的每个事件和属性映射为图节点，并通过SAGEConv卷积操作实现信息传播，使用多种损失函数同时优化分类和回归任务。实验对比了现有的自编码器方法，结果显示SANAGRAPH在重建活动和时间戳方面准确率更高，对所有属性的修复也表现良好，显著提升了流程挖掘数据质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Graph-based_Event_Log_Repair.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="144-Align-for-Fusion-Harmonizing-Triple-Preferences-via-Dual-oriented-Diffusion-for-Cross-domain-Sequential-Recommendation"><a href="#144-Align-for-Fusion-Harmonizing-Triple-Preferences-via-Dual-oriented-Diffusion-for-Cross-domain-Sequential-Recommendation" class="headerlink" title="144. Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation"></a>144. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Align-for-Fusion__Harmonizing_Triple_Preferences_via_Dual-oriented_Diffusion_for_Cross-domain_Sequen.pdf">Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Defense Technology</span></p>
<p>本文提出了一种面向跨域序列推荐的新框架HorizonRec，结合混合条件分布检索（MDR）和双向扩散偏好建模（DPD），利用扩散模型在源域、目标域和混合域间进行细粒度的用户兴趣融合。实验表明该方法在多个真实数据集上显著提升了跨域推荐的准确性和效率，验证了MDR和DPD模块对于多域偏好对齐及鲁棒建模的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Align-for-Fusion_Harmonizing_Triple_Preferences_via_Dual-oriented_Diffusion_for_Cross-domain_Sequential_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="145-Q-DPTS-Quantum-Differentially-Private-Time-Series-Forecasting-via-Variational-Quantum-Circuits"><a href="#145-Q-DPTS-Quantum-Differentially-Private-Time-Series-Forecasting-via-Variational-Quantum-Circuits" class="headerlink" title="145. Q-DPTS: Quantum Differentially Private Time Series Forecasting via Variational Quantum Circuits"></a>145. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Q-DPTS__Quantum_Differentially_Private_Time_Series_Forecasting_via_Variational_Quantum_Circuits.pdf">Q-DPTS: Quantum Differentially Private Time Series Forecasting via Variational Quantum Circuits</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent Researcher</span></p>
<p>本文提出Q-DPTS框架，将变分量子电路（VQC）与微分隐私（Differential Privacy, DP）机制结合，用于时序数据预测。采用DP-SGD算法中的每样本梯度裁剪与高斯噪声注入，并用Rényi微分隐私会计，为量子-经典混合模型提供严格的隐私保障。实验在ETT数据集上，系统对比了LSTM、QASA、QRWKV、QLSTM等模型在不同隐私预算下的预测误差，结果表明，Q-DPTS量子模型在牺牲最小精度的前提下实现优良的隐私-效用权衡，尤其QASA和QRWKV在高噪声环境下仍优于经典模型。结论：量子差分隐私预测模型不仅可行且有效，能在敏感领域实现安全高效的时序建模。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Q-DPTS_Quantum_Differentially_Private_Time_Series_Forecasting_via_Variational_Quantum_Circuits.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="146-Fairness-in-Dysarthric-Speech-Synthesis-Understanding-Intrinsic-Bias-in-Dysarthric-Speech-Cloning-using-F5-TTS"><a href="#146-Fairness-in-Dysarthric-Speech-Synthesis-Understanding-Intrinsic-Bias-in-Dysarthric-Speech-Cloning-using-F5-TTS" class="headerlink" title="146. Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS"></a>146. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Fairness_in_Dysarthric_Speech_Synthesis__Understanding_Intrinsic_Bias_in_Dysarthric_Speech_Cloning_u.pdf">Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">International Institute of Information Technology-Hyderabad</span></p>
<p>本文提出了一个框架，利用F5-TTS零样本语音克隆技术和TORGO数据库，系统评估了合成dysarthric（构音障碍）语音在可懂度、说话人相似性和韵律保留方面的效果，并通过Parity Difference和Disparate Impact等公平性指标分析不同病情严重程度下的偏差。研究发现，F5-TTS对语音可懂度偏置较大，尤其对高严重度患者，导致数据增强时对相关辅助语音识别任务效果不佳，强调了在语音数据增强中需考虑公平性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Fairness_in_Dysarthric_Speech_Synthesis_Understanding_Intrinsic_Bias_in_Dysarthric_Speech_Cloning_using_F5-TTS.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="147-Disentangling-Bias-by-Modeling-Intra-and-Inter-modal-Causal-Attention-for-Multimodal-Sentiment-Analysis"><a href="#147-Disentangling-Bias-by-Modeling-Intra-and-Inter-modal-Causal-Attention-for-Multimodal-Sentiment-Analysis" class="headerlink" title="147. Disentangling Bias by Modeling Intra- and Inter-modal Causal Attention for Multimodal Sentiment Analysis"></a>147. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Disentangling_Bias_by_Modeling_Intra-_and_Inter-modal_Causal_Attention_for_Multimodal_Sentiment_Anal.pdf">Disentangling Bias by Modeling Intra- and Inter-modal Causal Attention for Multimodal Sentiment Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science, South China Normal University</span></p>
<p>本文提出了一种多关系多模态因果干预模型（MMCI），利用因果理论中的背门调整机制，通过构建多关系图和图注意力网络显式建模文本、音频、视觉模态间的内外部依赖，分别估算和解耦因果特征与捷径特征，并动态结合以抵御数据分布偏移。实验表明，MMCI在多项基准和OOD测试数据上有效去偏、提升了多模态情感分析的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Disentangling_Bias_by_Modeling_Intra-_and_Inter-modal_Causal_Attention_for_Multimodal_Sentiment_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="148-Compressed-Decentralized-Momentum-Stochastic-Gradient-Methods-for-Nonconvex-Optimization"><a href="#148-Compressed-Decentralized-Momentum-Stochastic-Gradient-Methods-for-Nonconvex-Optimization" class="headerlink" title="148. Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization"></a>148. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/Compressed_Decentralized_Momentum_Stochastic_Gradient_Methods_for_Nonconvex_Optimization.pdf">Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Rensselaer Polytechnic Institute</span></p>
<p>本文提出了两种用于多智能体非凸随机优化的压缩去中心化动量随机梯度算法（DAMSCo和DaSHCo），分别针对梯度有界和数据异质两种场景，结合了动量&#x2F;自适应技术与通信压缩以加速收敛并降低通信成本。理论上，两种方法均证明能达到最优收敛速率，且在一定误差容忍区间内实现线性加速和算法参数的拓扑无关性，实验上在训练深度神经网络和Transformer时优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/Compressed_Decentralized_Momentum_Stochastic_Gradient_Methods_for_Nonconvex_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="149-MENDR-Manifold-Explainable-Neural-Data-Representations"><a href="#149-MENDR-Manifold-Explainable-Neural-Data-Representations" class="headerlink" title="149. MENDR: Manifold Explainable Neural Data Representations"></a>149. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-08/MENDR__Manifold_Explainable_Neural_Data_Representations.pdf">MENDR: Manifold Explainable Neural Data Representations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Georgia Institute of Technology</span></p>
<p>该论文提出MENDR，一种用于脑电（EEG）信号的基础模型，创新性地结合离散小波包变换、图神经网络（GNN）空间归一化、自监督对比学习和Riemannian流形Transformer结构，实现对EEG信号的可解释和高效表示。实验显示，MENDR在多个EEG下游任务上参数量更低但表现接近主流模型，同时提升了表示可解释性和重建能力，展现出高效、可解释、临床适用的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-11/MENDR_Manifold_Explainable_Neural_Data_Representations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/11报纸</div>
      <div>http://ywfhhh.github.io/2025/08/2025-08-11_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>具身ADT</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月11日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/2025-08-12_article/" title="2025/08/12报纸">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">2025/08/12报纸</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/01/2024-01-04_article/" title="2024/01/04报纸">
                        <span class="hidden-mobile">2024/01/04报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<!-- hexo injector body_end start -->
<script>
  function download(url) {
    // 检查 URL 是否有效
    if (!url) {
      console.error('下载 URL 无效');
      return;
    }

    // 创建一个隐藏隐藏的 <a> 元素用于下载
    const a = document.createElement('a');
    a.href = url;
    a.download = url.split('/').pop(); // 设置下载的文件名（默认为 URL 的最后一部分）
    document.body.appendChild(a); // 添加到 DOM
    a.click(); // 触发点击事件
    document.body.removeChild(a); // 下载完成后移除 <a> 元素
  }
</script>
<!-- hexo injector body_end end --></body>
</html>
