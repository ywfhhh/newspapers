

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="具身ADT">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;27报纸">
<meta property="og:url" content="http://ywfhhh.github.io/2025/08/2025-08-27_article/index.html">
<meta property="og:site_name" content="最新前沿论文分享">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Solving_the_Min-Max_Multiple_Traveling_Salesmen_Problem_via_Learning-Based_Path_Generation_and_Optimal_Splitting.jpg">
<meta property="article:published_time" content="2025-08-27T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-26T09:58:31.467Z">
<meta property="article:author" content="具身ADT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Solving_the_Min-Max_Multiple_Traveling_Salesmen_Problem_via_Learning-Based_Path_Generation_and_Optimal_Splitting.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/27报纸 - 最新前沿论文分享</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ywfhhh.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文库</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/27报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-27 00:00" pubdate>
          星期三, 八月 27日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          39k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/27报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/27报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-27<br>📄 <strong>发现论文数量</strong>：256  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Solving-the-Min-Max-Multiple-Traveling-Salesmen-Problem-via-Learning-Based-Path-Generation-and-Optimal-Splittingpdf"><a href="#1-Solving-the-Min-Max-Multiple-Traveling-Salesmen-Problem-via-Learning-Based-Path-Generation-and-Optimal-Splittingpdf" class="headerlink" title="1. Solving the Min-Max Multiple Traveling Salesmen Problem via Learning-Based Path Generation and Optimal Splittingpdf"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Solving_the_Min-Max_Multiple_Traveling_Salesmen_Problem_via_Learning-Based_Path_Generation_and_Optim.pdf">Solving the Min-Max Multiple Traveling Salesmen Problem via Learning-Based Path Generation and Optimal Splitting</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本论文提出了Generate-and-Split (GaS) 框架解决最小化最大路径长度的多旅行商问题（m3-TSP）。方法上，采用强化学习（RL）驱动的LSTM增强路径生成器联合最优分割算法，先生成全局路径再高效分割，解决了部分可观测性和分割一致性问题。实验结果显示，GaS在解质量和泛化能力上显著优于现有学习方法，尤其在分布外测试中表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Solving_the_Min-Max_Multiple_Traveling_Salesmen_Problem_via_Learning-Based_Path_Generation_and_Optimal_Splitting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-Reinforcement-Guided-Hyper-Heuristic-Hyperparameter-Optimization-for-Fair-and-Explainable-Spiking-Neural-Network-Based-Financial-Fraud-Detectionpdf"><a href="#2-Reinforcement-Guided-Hyper-Heuristic-Hyperparameter-Optimization-for-Fair-and-Explainable-Spiking-Neural-Network-Based-Financial-Fraud-Detectionpdf" class="headerlink" title="2. Reinforcement-Guided Hyper-Heuristic Hyperparameter Optimization for Fair and Explainable Spiking Neural Network-Based Financial Fraud Detectionpdf"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Reinforcement-Guided_Hyper-Heuristic_Hyperparameter_Optimization_for_Fair_and_Explainable_Spiking_Ne.pdf">Reinforcement-Guided Hyper-Heuristic Hyperparameter Optimization for Fair and Explainable Spiking Neural Network-Based Financial Fraud Detection</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Department of Computer Science &amp; Engineering, Khulna University of Engineering &amp; Technology (KUET)</span></p>
<p>本文提出一种融合生物启发的皮层脉冲神经网络（CSNPC）与强化学习引导的超启发式优化器（RHOSS）的金融欺诈检测方法。方法通过Q学习动态选择低层启发式进行SNN超参数优化，并结合可解释性AI技术（如显著性归因和脉冲活动分析）提升模型透明度和公平性。实验结果表明，该方法在严格的5%误报率下召回率达90.8%，且在客户年龄、收入和就业等敏感属性上实现98%以上的预测平等，优于主流脉冲与非脉冲模型，具备高性能及伦理对齐。结论：融合人口编码SNN与强化学习超启发式优化可实现公平、可解释且高效的金融欺诈检测。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Reinforcement-Guided_Hyper-Heuristic_Hyperparameter_Optimization_for_Fair_and_Explainable_Spiking_Neural_Network-Based_Financial_Fraud_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-Dream-to-Chat-Model-based-Reinforcement-Learning-on-Dialogues-with-User-Belief-Modelingpdf"><a href="#3-Dream-to-Chat-Model-based-Reinforcement-Learning-on-Dialogues-with-User-Belief-Modelingpdf" class="headerlink" title="3. Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modelingpdf"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dream_to_Chat__Model-based_Reinforcement_Learning_on_Dialogues_with_User_Belief_Modeling.pdf">Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Geely AI Lab</span></p>
<p>本文提出DreamCUB框架，将基于模型的强化学习（Model-based RL, MBRL）与用户信念建模（包括情感、情绪和意图识别）结合，专为对话系统设计。方法包括预训练对话世界模型以预测用户情绪及未来对话轨迹，并通过POMDP理论推导，联合策略、评论家与世界模型进行训练。实验结果表明，该方法在情感识别和对话生成质量上优于现有方法，并表现出良好的领域泛化能力和探索-利用平衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Dream_to_Chat_Model-based_Reinforcement_Learning_on_Dialogues_with_User_Belief_Modeling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-ReviBranch-Deep-Reinforcement-Learning-for-Branch-and-Bound-with-Revived-Trajectoriespdf"><a href="#4-ReviBranch-Deep-Reinforcement-Learning-for-Branch-and-Bound-with-Revived-Trajectoriespdf" class="headerlink" title="4. ReviBranch: Deep Reinforcement Learning for Branch-and-Bound with Revived Trajectoriespdf"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ReviBranch__Deep_Reinforcement_Learning_for_Branch-and-Bound_with_Revived_Trajectories.pdf">ReviBranch: Deep Reinforcement Learning for Branch-and-Bound with Revived Trajectories</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hong Kong Baptist University</span></p>
<p>该论文提出ReviBranch深度强化学习框架，通过Encoder-Revival-Decoder架构结合BipartiteGCN图结构建模和多方向注意力机制，创新性地利用“复活轨迹”关联分支决策与动态图状态，并采用重要性加权奖励重分布，解决分支定界（B&amp;B）中长期依赖、稀疏奖励和状态动态变化等难题。实验证明，ReviBranch在多个MILP基准上优于最先进RL方法，复杂实例上B&amp;B节点减少4.0%、LP迭代减少2.2%，展现出良好的泛化能力和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ReviBranch_Deep_Reinforcement_Learning_for_Branch-and-Bound_with_Revived_Trajectories.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-Rectified-Robust-Policy-Optimization-for-Model-Uncertain-Constrained-Reinforcement-Learning-without-Strong-Dualitypdf"><a href="#5-Rectified-Robust-Policy-Optimization-for-Model-Uncertain-Constrained-Reinforcement-Learning-without-Strong-Dualitypdf" class="headerlink" title="5. Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Dualitypdf"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Rectified_Robust_Policy_Optimization_for_Model-Uncertain_Constrained_Reinforcement_Learning_without_.pdf">Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland, College Park</span></p>
<p>本文发现，在存在模型不确定性的约束强化学习（RL）问题下，强对偶性一般不成立，导致传统的原始-对偶方法难以保证最优可行策略。为此，作者提出了只依赖原问题的Rectified Robust Policy Optimization（RRPO）算法，无需对偶变量，直接在原始问题上优化策略，并在一般正则性假设下给出了收敛性理论保证。实验表明，RRPO在典型环境下可在模型不确定性下获得鲁棒且安全的策略，优于非鲁棒方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Rectified_Robust_Policy_Optimization_for_Model-Uncertain_Constrained_Reinforcement_Learning_without_Strong_Duality.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-MetaFed-Advancing-Privacy-Performance-and-Sustainability-in-Federated-Metaverse-Systemspdf"><a href="#6-MetaFed-Advancing-Privacy-Performance-and-Sustainability-in-Federated-Metaverse-Systemspdf" class="headerlink" title="6. MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systemspdf"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MetaFed__Advancing_Privacy,_Performance,_and_Sustainability_in_Federated_Metaverse_Systems.pdf">MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kırıkkale University</span></p>
<p>本文提出了MetaFed框架，通过集成多智能体强化学习（MARL）动态客户端选择、同态加密的隐私保护联邦学习以及碳感知调度，有效实现了Metaverse系统中的资源智能编排。实验结果显示，MetaFed相比传统方法在MNIST和CIFAR-10数据集上可减少约20-25%的碳排放，同时保持高准确率和低通信延迟，验证了其在隐私保护与可持续性方面的优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MetaFed_Advancing_Privacy_Performance_and_Sustainability_in_Federated_Metaverse_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-Federated-Reinforcement-Learning-for-Runtime-Optimization-of-AI-Applications-in-Smart-Eyewearspdf"><a href="#7-Federated-Reinforcement-Learning-for-Runtime-Optimization-of-AI-Applications-in-Smart-Eyewearspdf" class="headerlink" title="7. Federated Reinforcement Learning for Runtime Optimization of AI Applications in Smart Eyewearspdf"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Federated_Reinforcement_Learning_for_Runtime_Optimization_of_AI_Applications_in_Smart_Eyewears.pdf">Federated Reinforcement Learning for Runtime Optimization of AI Applications in Smart Eyewears</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Politecnico di Milano</span></p>
<p>本论文提出了一种结合三层DNN分区和联邦强化学习（FRL）的方法，实现智能眼镜设备在动态网络和云延迟环境下的AI任务自适应卸载。通过同步和异步联邦策略，多智能体在保持数据隐私的前提下协同学习，显著降低了性能波动，提高了系统稳定性和泛化能力。实验结果显示，FRL方法在多智能体协作下能够在不加速收敛的前提下，获得更稳定和可靠的策略，适用于实时目标检测等场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Federated_Reinforcement_Learning_for_Runtime_Optimization_of_AI_Applications_in_Smart_Eyewears.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-GPG-HT-Generalized-Policy-Gradient-with-History-Aware-Decision-Transformer-for-Probabilistic-Path-Planningpdf"><a href="#8-GPG-HT-Generalized-Policy-Gradient-with-History-Aware-Decision-Transformer-for-Probabilistic-Path-Planningpdf" class="headerlink" title="8. GPG-HT: Generalized Policy Gradient with History-Aware Decision Transformer for Probabilistic Path Planningpdf"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GPG-HT__Generalized_Policy_Gradient_with_History-Aware_Decision_Transformer_for_Probabilistic_Path_P.pdf">GPG-HT: Generalized Policy Gradient with History-Aware Decision Transformer for Probabilistic Path Planning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sichuan University</span></p>
<p>该论文提出GPG-HT框架，将历史感知决策Transformer与广义策略梯度相结合，针对交通网络中的概率路径规划SOTA问题。方法利用Transformer建模长程轨迹依赖，通过广义策略梯度以非马尔科夫目标进行优化，实现基于历史信息的路径决策。实验表明，GPG-HT在Sioux Falls Network等基准上显著提升了到点概率与决策效率，优于现有RL与DP方法，具有高准确性和实时性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GPG-HT_Generalized_Policy_Gradient_with_History-Aware_Decision_Transformer_for_Probabilistic_Path_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-Reinforcement-Learning-enhanced-Online-Adaptive-Clinical-Decision-Support-via-Digital-Twin-powered-Policy-and-Treatment-Effect-optimized-Rewardpdf"><a href="#9-Reinforcement-Learning-enhanced-Online-Adaptive-Clinical-Decision-Support-via-Digital-Twin-powered-Policy-and-Treatment-Effect-optimized-Rewardpdf" class="headerlink" title="9. Reinforcement Learning enhanced Online Adaptive Clinical Decision Support via Digital Twin powered Policy and Treatment Effect optimized Rewardpdf"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Reinforcement_Learning_enhanced_Online_Adaptive_Clinical_Decision_Support_via_Digital_Twin_powered_P.pdf">Reinforcement Learning enhanced Online Adaptive Clinical Decision Support via Digital Twin powered Policy and Treatment Effect optimized Reward</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Houston</span></p>
<p>本文提出了一种结合强化学习（RL）、数字孪生（DT）和治疗效应（TE）优化奖励的临床决策支持系统。方法包括：离线阶段利用批约束Q学习（BCQ）从回顾性数据中训练策略，建立患者数字孪生环境模拟与结果模型，在线阶段通过Q网络集成和主动不确定性采样进行决策，并在高不确定性时触发专家查询，始终通过规则安全门控保证安全。结论：在合成临床仿真中，该系统具有低延迟、高稳定性和安全性，减少了专家标注需求，并显著优于常规值函数基线，展现了从离线建模到交互式临床部署的可行路径。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Reinforcement_Learning_enhanced_Online_Adaptive_Clinical_Decision_Support_via_Digital_Twin_powered_Policy_and_Treatment_Effect_optimized_Reward.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-ANO-Faster-is-Better-in-Noisy-Landscapespdf"><a href="#10-ANO-Faster-is-Better-in-Noisy-Landscapespdf" class="headerlink" title="10. ANO : Faster is Better in Noisy Landscapespdf"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ANO___Faster_is_Better_in_Noisy_Landscape.pdf">ANO : Faster is Better in Noisy Landscapes</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent Researcher</span></p>
<p>本文提出了一种新的优化器Ano，通过将梯度方向和平滑（动量）与步长（由当前梯度模决定）解耦，提升在高噪声和非平稳环境下的鲁棒性，同时保持一阶优化算法的高效性。理论证明其在非凸问题下收敛速度与主流符号优化器相当，实验证明在深度强化学习和低资源NLP任务中显著优于Adam等基线，在常规视觉任务中也具备竞争力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ANO_Faster_is_Better_in_Noisy_Landscapes.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generationpdf"><a href="#11-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generationpdf" class="headerlink" title="11. Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generationpdf"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Visual-CoG__Stage-Aware_Reinforcement_Learning_with_Chain_of_Guidance_for_Text-to-Image_Generation.pdf">Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Alibaba Group</span></p>
<p>该论文提出了一种用于文本到图像生成的三阶段强化学习框架Visual-CoG，包括语义推理、过程细化和结果评估，每一阶段都提供即时奖励指导生成流程。通过在GenEval、T2I-CompBench和自建VisCog-Bench上的实验，验证了该方法在多属性和歧义性提示下能大幅提升生成图像的语义一致性和推理能力，显著优于已有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Visual-CoG_Stage-Aware_Reinforcement_Learning_with_Chain_of_Guidance_for_Text-to-Image_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-Multi-layer-Abstraction-for-Nested-Generation-of-Options-MANGO-in-Hierarchical-Reinforcement-Learningpdf"><a href="#12-Multi-layer-Abstraction-for-Nested-Generation-of-Options-MANGO-in-Hierarchical-Reinforcement-Learningpdf" class="headerlink" title="12. Multi-layer Abstraction for Nested Generation of Options (MANGO) in Hierarchical Reinforcement Learningpdf"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multi-layer_Abstraction_for_Nested_Generation_of_Options_(MANGO)_in_Hierarchical_Reinforcement_Learn.pdf">Multi-layer Abstraction for Nested Generation of Options (MANGO) in Hierarchical Reinforcement Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Human Inspired Technology Research Center, Università di Padova</span></p>
<p>本文提出了MANGO框架，通过多层抽象将复杂稀疏奖励环境分解为嵌套的选项（options），每层定义抽象状态空间和选项，选项可在不同层间复用，提升样本效率和泛化能力。实验证明，MANGO在程序生成的网格环境中较标准RL方法有更高样本效率与更好的泛化能力，同时增强了决策过程的可解释性，适用于安全和工业场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Multi-layer_Abstraction_for_Nested_Generation_of_Options_%28MANGO%29_in_Hierarchical_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-HERO-Hierarchical-Extrapolation-and-Refresh-for-Efficient-World-Modelspdf"><a href="#13-HERO-Hierarchical-Extrapolation-and-Refresh-for-Efficient-World-Modelspdf" class="headerlink" title="13. HERO: Hierarchical Extrapolation and Refresh for Efficient World Modelspdf"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HERO__Hierarchical_Extrapolation_and_Refresh_for_Efficient_World_Models.pdf">HERO: Hierarchical Extrapolation and Refresh for Efficient World Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了一种用于高效世界模型的无训练分层加速框架HERO。方法上，HERO针对世界模型中的多模态特性，采用层次化策略：浅层采用Patch-wise Refresh动态选择token重计算，深层则用线性外推直接估算中间特征，避免冗余计算。实验表明，HERO在视觉规划和重建任务上实现了1.73倍加速且几乎无性能损失，优于现有扩散模型加速方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/HERO_Hierarchical_Extrapolation_and_Refresh_for_Efficient_World_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-Consciousness-as-a-Functorpdf"><a href="#14-Consciousness-as-a-Functorpdf" class="headerlink" title="14. Consciousness as a Functorpdf"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Consciousness_as_a_Functor.pdf">Consciousness as a Functor</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Adobe Research</span></p>
<p>该论文提出将意识建模为范函子（Functor），通过范畴论和煤代数理论描述意识与无意识过程的信息流转，并用Universal Reinforcement Learning (URL)框架模拟意识短期记忆到长期无意识记忆的传递，以及用网络经济模型模拟无意识到意识的竞争过程。结论认为该理论为人工智能系统中意识功能提供了统一的理论基础，强调异步分布计算和范畴结构可推广于智能决策系统。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Consciousness_as_a_Functor.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="15-M3DMap-Object-aware-Multimodal-3D-Mapping-for-Dynamic-Environmentspdf"><a href="#15-M3DMap-Object-aware-Multimodal-3D-Mapping-for-Dynamic-Environmentspdf" class="headerlink" title="15. M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environmentspdf"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/M3DMap__Object-aware_Multimodal_3D_Mapping_for_Dynamic_Environments.pdf">M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Moscow Institute of Physics and Technology</span></p>
<p>本文提出了M3DMap，一种面向动态环境下对象感知的多模态3D建图方法，模块化集成了神经网络对象分割与跟踪、可训练里程计估计、3D地图构建与更新以及多模态数据检索模块，支持多种传感器数据（图像、点云、文本）融合。实验和理论分析表明，融合多模态信息能显著提升目标识别、定位、问答、机器人操控等下游任务的精度，但对实时性的同步与模块协调提出挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/M3DMap_Object-aware_Multimodal_3D_Mapping_for_Dynamic_Environments.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-A-Rapid-Iterative-Trajectory-Planning-Method-for-Automated-Parking-through-Differential-Flatnesspdf"><a href="#16-A-Rapid-Iterative-Trajectory-Planning-Method-for-Automated-Parking-through-Differential-Flatnesspdf" class="headerlink" title="16. A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatnesspdf"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Rapid_Iterative_Trajectory_Planning_Method_for_Automated_Parking_through_Differential_Flatness.pdf">A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatness</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Laboratory of Industrial Control Technology, Zhejiang University</span></p>
<p>该论文提出了一种基于路径-速度分解（PVD）的快速迭代轨迹规划（RITP）方法，结合差分平坦理论，将高阶多项式用于路径规划，利用并行计算和显式碰撞检测实现高效且精确的碰撞规避，并通过端点平滑约束（TSC）提升轨迹在换挡点的控制可行性。实验和仿真结果表明，该方法在自动泊车场景下相比现有方法大幅提升了计算效率（0.042~0.065s内完成）和轨迹的控制可行性，且可在实际车辆平台上实现高效、稳定的自动泊车。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Rapid_Iterative_Trajectory_Planning_Method_for_Automated_Parking_through_Differential_Flatness.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-DualReg-Dual-Space-Filtering-and-Reinforcement-for-Rigid-Registrationpdf"><a href="#17-DualReg-Dual-Space-Filtering-and-Reinforcement-for-Rigid-Registrationpdf" class="headerlink" title="17. DualReg: Dual-Space Filtering and Reinforcement for Rigid Registrationpdf"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DualReg__Dual-Space_Filtering_and_Reinforcement_for_Rigid_Registration.pdf">DualReg: Dual-Space Filtering and Reinforcement for Rigid Registration</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本文提出了DualReg，一种结合特征空间与几何空间的点云刚性配准高效算法。方法包括单点RANSAC快速过滤、三点RANSAC概率加权细化，并利用几何代理点建立双空间优化框架实现高精度配准。实验在3DMatch、3DLoMatch和KITTI等公开数据集上，方法在配准精度和速度方面均优于现有主流方法，达到最快32倍加速且精度提升，适用于机器人视觉与SLAM等实时场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/DualReg_Dual-Space_Filtering_and_Reinforcement_for_Rigid_Registration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-WEBSIGHT-A-Vision-First-Architecture-for-Robust-Web-Agentspdf"><a href="#18-WEBSIGHT-A-Vision-First-Architecture-for-Robust-Web-Agentspdf" class="headerlink" title="18. WEBSIGHT: A Vision-First Architecture for Robust Web Agentspdf"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/WebSight__A_Vision-First_Architecture_for_Robust_Web_Agents.pdf">WEBSIGHT: A Vision-First Architecture for Robust Web Agents</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>WEBSIGHT提出了一种以视觉为主的自主网页代理架构，完全基于网页截图进行交互，摒弃对HTML或DOM元数据的依赖。核心方法是通过LoRA微调UI-TARS模型，打造专用于网页UI元素识别与操作的WEBSIGHT-7B视觉语言模型，并集成至多智能体系统，包含规划、推理、视觉-动作和验证智能体，利用短期记忆机制协调任务推进。实验结果显示，WEBSIGHT-7B在Showdown Clicks基准上取得58.84%准确率，整体代理在WebVoyager基准上达到68%成功率，超越OpenAI等主流系统，且在已完成任务上正确率高达97.14%。结论表明该视觉优先架构实现了高效、可解释且鲁棒的网页自动化代理，推动自主网页智能体更接近人类视觉交互。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/WEBSIGHT_A_Vision-First_Architecture_for_Robust_Web_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-LLM-based-Human-like-Traffic-Simulation-for-Self-driving-Testspdf"><a href="#19-LLM-based-Human-like-Traffic-Simulation-for-Self-driving-Testspdf" class="headerlink" title="19. LLM-based Human-like Traffic Simulation for Self-driving Testspdf"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LLM-based_Human-like_Traffic_Simulation_for_Self-driving_Tests.pdf">LLM-based Human-like Traffic Simulation for Self-driving Tests</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本论文提出HDSim框架，通过结合认知理论与大语言模型(LLM)，以分层人类驾驶风格模型和感知中介行为影响(PMBI)机制，实现多样化、真实的人类驾驶行为仿真。结论显示，HDSim在无需重训练的情况下，可在自动驾驶测试中暴露多达68%更多的潜在安全隐患，提升仿真交通的真实性和系统扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LLM-based_Human-like_Traffic_Simulation_for_Self-driving_Tests.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-Drive-As-You-Like-Strategy-Level-Motion-Planning-Based-on-A-Multi-Head-Diffusion-Modelpdf"><a href="#20-Drive-As-You-Like-Strategy-Level-Motion-Planning-Based-on-A-Multi-Head-Diffusion-Modelpdf" class="headerlink" title="20. Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Modelpdf"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Drive_As_You_Like__Strategy-Level_Motion_Planning_Based_on_A_Multi-Head_Diffusion_Model.pdf">Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Model</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University Malaysia</span></p>
<p>本文提出了一种多头扩散模型（M-Diffusion Planner），结合DiT架构与Group Relative Policy Optimization（GRPO），实现策略级自动驾驶运动规划。模型通过LLM语义解析实时理解人类指令，动态选择不同驾驶策略头，生成多样化且高质量的轨迹。实验表明，该方法在nuPlan基准上取得SOTA性能，支持实时策略切换且有效反映用户偏好。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Drive_As_You_Like_Strategy-Level_Motion_Planning_Based_on_A_Multi-Head_Diffusion_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-A-Workflow-for-Map-Creation-in-Autonomous-Vehicle-Simulationspdf"><a href="#21-A-Workflow-for-Map-Creation-in-Autonomous-Vehicle-Simulationspdf" class="headerlink" title="21. A Workflow for Map Creation in Autonomous Vehicle Simulationspdf"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Workflow_for_Map_Creation_in_Autonomous_Vehicle_Simulations.pdf">A Workflow for Map Creation in Autonomous Vehicle Simulations</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ontario Tech University</span></p>
<p>本文提出了一种面向自动驾驶仿真环境（如AWSIM和Autoware）的自定义地图生成工作流程，采用轻量级开源工具链（包括OpenStreetMap、OSM2World、CloudCompare、PCL和Vector Map Builder）自动化生成3D网格、点云和Lanelet2地图文件，无需依赖特定仿真器或高资源消耗。结果表明，该流程可高效生成仿真地图并成功集成于主流自动驾驶仿真平台，显著降低了开发门槛，便于早期研究和快速原型设计。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Workflow_for_Map_Creation_in_Autonomous_Vehicle_Simulations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-NinA-Normalizing-Flows-in-Action-Training-VLA-Models-with-Normalizing-Flowspdf"><a href="#22-NinA-Normalizing-Flows-in-Action-Training-VLA-Models-with-Normalizing-Flowspdf" class="headerlink" title="22. NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flowspdf"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/NinA__Normalizing_Flows_in_Action._Training_VLA_Models_with_Normalizing_Flows.pdf">NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AIRI</span></p>
<p>本文提出了一种名为NinA的高效视觉-语言-动作（VLA）模型，创新性地用Normalizing Flows（NF）替换了以往常用的扩散模型作为动作解码器，实现了单步采样和精确似然估计，显著加速推理速度。实验表明，在LIBERO基准上，NinA在保持与扩散模型相当性能的同时，模型参数更少且推理速度提升7-10倍，为实际机器人高频控制提供了更实用的解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/NinA_Normalizing_Flows_in_Action._Training_VLA_Models_with_Normalizing_Flows.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-LodeStar-Long-horizon-Dexterity-via-Synthetic-Data-Augmentation-from-Human-Demonstrationspdf"><a href="#23-LodeStar-Long-horizon-Dexterity-via-Synthetic-Data-Augmentation-from-Human-Demonstrationspdf" class="headerlink" title="23. LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrationspdf"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LodeStar__Long-horizon_Dexterity_via_Synthetic_Data_Augmentation_from_Human_Demonstrations.pdf">LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California San Diego</span></p>
<p>本文提出LODESTAR系统，通过利用视觉基础模型自动将少量人类示范分解为语义技能段，并结合仿真中基于残差强化学习生成多样的合成演示数据，提升技能鲁棒性。最终利用Skill Routing Transformer将多个学习到的技能串联，实现长时序高难度操作，实验证明在三类真实世界任务上性能和泛化性均优于现有方法，成功率提升25%以上。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LodeStar_Long-horizon_Dexterity_via_Synthetic_Data_Augmentation_from_Human_Demonstrations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-Variational-Shape-Inference-for-Grasp-Diffusion-on-SE-3-pdf"><a href="#24-Variational-Shape-Inference-for-Grasp-Diffusion-on-SE-3-pdf" class="headerlink" title="24. Variational Shape Inference for Grasp Diffusion on SE(3)pdf"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Variational_Shape_Inference_for_Grasp_Diffusion_on_SE(3).pdf">Variational Shape Inference for Grasp Diffusion on SE(3)</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Purdue University</span></p>
<p>本论文提出了一种结合变分形状推断和扩散模型的多模态抓取合成框架，通过先训练基于隐式神经表示（INR）的变分自编码器从点云中获取鲁棒的几何特征，再利用这些特征驱动在SE(3)流形上的抓取扩散模型生成稳定多样的抓取姿态。此外，作者引入了可微分的测试时抓取优化方法以提升抓取成功率。实验表明，该方法在ACRONYM数据集上较主流多模态抓取方法提升6.3%，并在真实场景下表现出对点云噪声和稀疏性的强鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Variational_Shape_Inference_for_Grasp_Diffusion_on_SE%283%29.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-SoK-Cybersecurity-Assessment-of-Humanoid-Ecosystempdf"><a href="#25-SoK-Cybersecurity-Assessment-of-Humanoid-Ecosystempdf" class="headerlink" title="25. SoK: Cybersecurity Assessment of Humanoid Ecosystempdf"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SoK__Cybersecurity_Assessment_of_Humanoid_Ecosystem.pdf">SoK: Cybersecurity Assessment of Humanoid Ecosystem</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ben-Gurion University of the Negev</span></p>
<p>本论文提出了专为人形机器人设计的七层安全模型，梳理了硬件、传感、数据处理、中间件、决策、应用和人机交互等各层39种攻击手法与35种防御机制，并以系统性视角整合机器人、CPS和网络安全领域的碎片化研究。基于该模型，论文开发了RISK-MAP定量安全评估方法，并通过对Pepper、G1 EDU和Digit三款真实机器人平台的实证分析，发现当前防护多集中于高层，物理层及跨层级威胁仍存在显著安全短板，强调需系统性防护以保障人形机器人安全可信。结论指出该方法为行业提供了可量化、跨平台的安全基线，有助于推动主动安全投资和部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SoK_Cybersecurity_Assessment_of_Humanoid_Ecosystem.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-A-Synthetic-Dataset-for-Manometry-Recognition-in-Robotic-Applicationspdf"><a href="#26-A-Synthetic-Dataset-for-Manometry-Recognition-in-Robotic-Applicationspdf" class="headerlink" title="26. A Synthetic Dataset for Manometry Recognition in Robotic Applicationspdf"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Synthetic_Dataset_for_Manometry_Recognition_in_Robotic_Applications.pdf">A Synthetic Dataset for Manometry Recognition in Robotic Applications</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of São Paulo</span></p>
<p>本文提出了一种混合数据合成管线，结合BlenderProc程序化渲染与NVIDIA Cosmos-Predict2 AI驱动视频生成，用于在危险工业环境（如海上石油平台）提升机器人检测系统的训练数据量和多样性。实验表明，基于YOLO的检测网络在真实与合成数据1:1混合训练下，性能优于仅用真实数据，且可有效降低数据采集成本，提高安全性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Synthetic_Dataset_for_Manometry_Recognition_in_Robotic_Applications.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-Optimizing-Grasping-in-Legged-Robots-A-Deep-Learning-Approach-to-Loco-Manipulationpdf"><a href="#27-Optimizing-Grasping-in-Legged-Robots-A-Deep-Learning-Approach-to-Loco-Manipulationpdf" class="headerlink" title="27. Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulationpdf"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Optimizing_Grasping_in_Legged_Robots__A_Deep_Learning_Approach_to_Loco-Manipulation.pdf">Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Federal University of Uberlandia</span></p>
<p>本论文提出了一种结合深度学习和仿真环境的四足机器人抓取优化方法。方法通过Genesis仿真平台生成大规模合成数据集，利用多模态传感输入训练U-Net结构的卷积神经网络，集成YOLOv11目标检测和d2nt表面法线估算，实现了在Spot平台上自主导航、感知和精准抓取。实验表明，所提管线能够在真实环境中提升机器人抓取的准确性和适应性，但在物体变化和深度数据噪声方面仍有改进空间。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Optimizing_Grasping_in_Legged_Robots_A_Deep_Learning_Approach_to_Loco-Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-SEER-VAR-Semantic-Egocentric-Environment-Reasoner-for-Vehicle-Augmented-Realitypdf"><a href="#28-SEER-VAR-Semantic-Egocentric-Environment-Reasoner-for-Vehicle-Augmented-Realitypdf" class="headerlink" title="28. SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Realitypdf"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SEER-VAR__Semantic_Egocentric_Environment_Reasoner_for_Vehicle_Augmented_Reality.pdf">SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Eberhard-Karls-Universitat Tubingen</span></p>
<p>本文提出了SEER-VAR，一种创新的车辆增强现实（AR）系统框架，将语义分解、双分支SLAM（CASB）和GPT驱动的推荐模块统一，专为动态驾驶场景设计。核心方法包括基于深度的语义分割实现车内外场景分离、分别跟踪两类环境的相机运动，并结合LLM（GPT）的视觉-语言推理生成驾驶相关的AR叠加内容。实验和用户调研表明，SEER-VAR在复杂环境下实现了稳定的空间对齐和高感知一致性的AR渲染，大幅提升了驾驶场景理解和驾驶体验，为未来智能车辆AR系统奠定了基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SEER-VAR_Semantic_Egocentric_Environment_Reasoner_for_Vehicle_Augmented_Reality.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-4D-Visual-Pre-training-for-Robot-Learningpdf"><a href="#29-4D-Visual-Pre-training-for-Robot-Learningpdf" class="headerlink" title="29. 4D Visual Pre-training for Robot Learningpdf"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/4D_Visual_Pre-training_for_Robot_Learning.pdf">4D Visual Pre-training for Robot Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出了FVP（4D Visual Pre-training）视觉预训练框架，用于机器人操作任务的3D点云表示学习。该方法以下一时刻点云预测为目标，结合历史点云和动作信息，通过扩散模型预训练3D视觉表示，并在多种3D编码器和公开数据集上进行泛化。实验表明，FVP可显著提升3D模仿学习方法在12项真实世界机器人操作任务上的表现，平均成功率提升28%，并在VLA大模型应用中提升空间感知、任务泛化和长时序任务能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/4D_Visual_Pre-training_for_Robot_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agentspdf"><a href="#30-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agentspdf" class="headerlink" title="30. From reactive to cognitive: brain-inspired spatial intelligence for embodied agentspdf"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/From_reactive_to_cognitive__brain-inspired_spatial_intelligence_for_embodied_agents.pdf">From reactive to cognitive: brain-inspired spatial intelligence for embodied agents</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出BSC-Nav框架，通过仿生结构化空间记忆（包括地标、路径和测量知识）赋予具身智能体认知空间智能。BSC-Nav集成多模态大模型（如GPT-4V、DINOv2）进行环境感知、语义推理和动态规划，实现基于语义目标的分层记忆检索和高效导航，在多种仿真与真实场景下的导航、移动操作和空间问答任务中均取得最先进的泛化与效率表现，展示了生物启发式结构化空间记忆在提升具身智能体泛化性和适应性的关键作用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/From_reactive_to_cognitive_brain-inspired_spatial_intelligence_for_embodied_agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-FlowVLA-Thinking-in-Motion-with-a-Visual-Chain-of-Thoughtpdf"><a href="#31-FlowVLA-Thinking-in-Motion-with-a-Visual-Chain-of-Thoughtpdf" class="headerlink" title="31. FlowVLA: Thinking in Motion with a Visual Chain of Thoughtpdf"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FlowVLA__Thinking_in_Motion_with_a_Visual_Chain_of_Thought.pdf">FlowVLA: Thinking in Motion with a Visual Chain of Thought</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HKUST(GZ)</span></p>
<p>本文提出了Visual Chain of Thought (Visual CoT)原则，并在FlowVLA框架中实现，通过“帧→光流→帧”结构化推理过程，先预测物理运动再生成下一帧，采用统一的VQ-GAN分词器与自回归Transformer模型学习视觉与运动表征。实验表明，FlowVLA在机器人操作任务上实现了新的性能纪录，并显著提升了样本效率，验证了显式运动推理对感知与控制桥接的价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/FlowVLA_Thinking_in_Motion_with_a_Visual_Chain_of_Thought.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-Scene-Agnostic-Traversability-Labeling-and-Estimation-via-a-Multimodal-Self-supervised-Frameworkpdf"><a href="#32-Scene-Agnostic-Traversability-Labeling-and-Estimation-via-a-Multimodal-Self-supervised-Frameworkpdf" class="headerlink" title="32. Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Frameworkpdf"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scene-Agnostic_Traversability_Labeling_and_Estimation_via_a_Multimodal_Self-supervised_Framework.pdf">Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了一种多模态自监督框架，用于机器人不同场景下的可通行性标注与估计。方法通过融合视觉基础模型（如SAM、DINOv2）、LiDAR与脚印数据，自动生成语义与几何结合的像素级可通行性标签，并设计双流网络实现语义与几何特征解耦学习，结合稀疏LiDAR标签校正伪标签误差。实验表明，该方法在多数据集和真实机器人平台中均显著优于现有方法，自动标注IoU约为88%，估计准确率提升1.6-3.5%，且支持实时部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Scene-Agnostic_Traversability_Labeling_and_Estimation_via_a_Multimodal_Self-supervised_Framework.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-The-Effects-of-Communication-Delay-on-Human-Performance-and-Neurocognitive-Responses-in-Mobile-Robot-Teleoperationpdf"><a href="#33-The-Effects-of-Communication-Delay-on-Human-Performance-and-Neurocognitive-Responses-in-Mobile-Robot-Teleoperationpdf" class="headerlink" title="33. The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperationpdf"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_Effects_of_Communication_Delay_on_Human_Performance_and_Neurocognitive_Responses_in_Mobile_Robot.pdf">The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>本论文通过人机互动实验，结合脑电（EEG）和机器人行为数据，系统分析了通信延迟（0–500 ms）对移动机器人遥操作中人的操作绩效与神经认知反应的影响。研究发现，200–300 ms延迟会显著降低任务效率与准确性，100–200 ms为人类对延迟的早期感知阈值，400 ms为认知资源分配的上限，这些结果为遥操作系统延迟补偿策略设计提供了神经认知依据。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/The_Effects_of_Communication_Delay_on_Human_Performance_and_Neurocognitive_Responses_in_Mobile_Robot_Teleoperation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-Arnold-a-generalist-muscle-transformer-policypdf"><a href="#34-Arnold-a-generalist-muscle-transformer-policypdf" class="headerlink" title="34. Arnold: a generalist muscle transformer policypdf"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Arnold__a_generalist_muscle_transformer_policy.pdf">Arnold: a generalist muscle transformer policy</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">École Polytechnique Fédérale de Lausanne (EPFL)</span></p>
<p>本论文提出Arnold，一种通用的肌肉控制Transformer策略，结合了行为克隆（on-policy behavior cloning, OBC）和PPO微调，能在14项复杂的生物肌肉骨骼控制任务（涵盖不同身体部分、多任务与多体型）中实现专家或超专家水平。方法创新在于提出了“传感-运动词汇”，用组合式嵌入描述异构感知、目标与执行器，并通过Transformer处理变长观测与动作空间，实现高效多任务和多体型学习，同时支持新任务的快速适应。结论显示，Arnold不仅能高效汇聚多专家知识和实现自蒸馏，还在个别任务中形成低维肌肉协同子空间，但缺乏跨任务的通用肌肉协同，揭示了生物运动控制中协同迁移性的局限。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-PerPilot-Personalizing-VLM-based-Mobile-Agents-via-Memory-and-Explorationpdf"><a href="#35-PerPilot-Personalizing-VLM-based-Mobile-Agents-via-Memory-and-Explorationpdf" class="headerlink" title="35. PerPilot: Personalizing VLM-based Mobile Agents via Memory and Explorationpdf"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PerPilot__Personalizing_VLM-based_Mobile_Agents_via_Memory_and_Exploration.pdf">PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northwestern Polytechnical University</span></p>
<p>本文提出了PerPilot，一个基于大语言模型（LLM）驱动的可插拔框架，提升移动智能体在理解和执行用户个性化指令方面的能力。其方法包括个性化感知模块用于识别指令中的用户特有元素，以及通过记忆检索和推理探索两种互补机制自动补全信息。实验结果显示，PerPilot显著提升了多种主流移动智能体在个性化任务上的成功率，并且随着使用次数增加，个性化能力持续增强。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-MODELING-AND-CONTROL-FRAMEWORK-FOR-AUTONOMOUS-SPACE-MANIPULATOR-HANDOVER-OPERATIONSpdf"><a href="#36-MODELING-AND-CONTROL-FRAMEWORK-FOR-AUTONOMOUS-SPACE-MANIPULATOR-HANDOVER-OPERATIONSpdf" class="headerlink" title="36. MODELING AND CONTROL FRAMEWORK FOR AUTONOMOUS SPACE MANIPULATOR HANDOVER OPERATIONSpdf"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Modeling_and_Control_Framework_for_Autonomous_Space_Manipulator_Handover_Operations.pdf">MODELING AND CONTROL FRAMEWORK FOR AUTONOMOUS SPACE MANIPULATOR HANDOVER OPERATIONS</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cincinnati</span></p>
<p>本文提出了一种用于自主空间机器人系统中机器人对机器人（R2R）交接任务的动力学建模方法，建立了双臂空间机械臂的完整动力学模型，并在仿真中对非线性模型预测控制（NMPC）与经典PID控制方法进行了比较。结果表明，NMPC和PID均能完成任务，但NMPC在轨迹跟踪精度和稳定性方面优于PID，适合空间服务、组装与制造（ISAM）场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-No-Need-to-Look-Locating-and-Grasping-Objects-by-a-Robot-Arm-Covered-with-Sensitive-Skinpdf"><a href="#37-No-Need-to-Look-Locating-and-Grasping-Objects-by-a-Robot-Arm-Covered-with-Sensitive-Skinpdf" class="headerlink" title="37. No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skinpdf"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/No_Need_to_Look!_Locating_and_Grasping_Objects_by_a_Robot_Arm_Covered_with_Sensitive_Skin.pdf">No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Czech Technical University in Prague</span></p>
<p>本论文提出了一种机器人在完全无视觉输入情况下，仅依赖覆盖全臂的人工皮肤进行物体定位和抓取的方法。该方法分为粗略全臂探索和末端精确定位两阶段，结合力&#x2F;扭矩传感器，并在仿真与真实机器人上系统评估，实现了多种物体的高效定位与抓取。结论显示，相比仅末端触觉基线方法，结合全臂皮肤的方案在真实机器人中抓取成功率达85.7%，且速度提升6倍，适用于视觉受限场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-Integration-of-Computer-Vision-with-Adaptive-Control-for-Autonomous-Driving-Using-ADOREpdf"><a href="#38-Integration-of-Computer-Vision-with-Adaptive-Control-for-Autonomous-Driving-Using-ADOREpdf" class="headerlink" title="38. Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADOREpdf"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Integration_of_Computer_Vision_with_Adaptive_Control_for_Autonomous_Driving_Using_ADORE.pdf">Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADORE</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Siegen</span></p>
<p>本论文提出了一种将YOLOv8深度学习视觉模型与ADORE自适应控制框架集成的方法，通过ROS桥在CARLA仿真环境下实现感知、决策和控制的实时闭环。实验表明，该系统可在不同天气和速度限制下准确识别交通标志，且ADORE能低延迟地调整车辆行为，实现平滑人性化的自主驾驶。结论：方法提升了自动驾驶系统的安全性和可靠性，并具备扩展到实际车辆的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-Neural-Algorithmic-Reasoners-informed-Large-Language-Model-for-Multi-Agent-Path-Findingpdf"><a href="#39-Neural-Algorithmic-Reasoners-informed-Large-Language-Model-for-Multi-Agent-Path-Findingpdf" class="headerlink" title="39. Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Findingpdf"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Neural_Algorithmic_Reasoners_informed_Large_Language_Model_for_Multi-Agent_Path_Finding.pdf">Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本文提出了LLM-NAR框架，将大语言模型（LLM）与基于图神经网络（GNN）的神经算法推理器（NAR）以及交叉注意力机制结合，以提升多智能体路径规划（MAPF）任务的性能。方法通过优化的LLM提示机制、GNN-NAR地图信息建模和交叉注意力融合，实现了高效规划和更优路径。实验结果显示，该方法在仿真和真实机器人环境中均显著优于现有LLM方法，成功率更高、路径更短，同时训练效率远超强化学习和传统规划。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-A-holistic-perception-system-of-internal-and-external-monitoring-for-ground-autonomous-vehicles-AutoTRUST-paradigmpdf"><a href="#40-A-holistic-perception-system-of-internal-and-external-monitoring-for-ground-autonomous-vehicles-AutoTRUST-paradigmpdf" class="headerlink" title="40. A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigmpdf"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_holistic_perception_system_of_internal_and_external_monitoring_for_ground_autonomous_vehicles__Aut.pdf">A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Industrial Systems Institute, ATHENA Research Center</span></p>
<p>本文提出了AutoTRUST范式下的地面自动驾驶车辆全方位感知系统，集成了内部行为监测、智能虚拟助手、空气质量与热舒适分析，以及基于LiDAR的高效外部语义分割，通过多摄像头、智能传感器和大语言模型实现车内外实时感知。系统在真实电动车平台上部署并验证，有效提升了乘员安全、舒适性和环境感知能力，具备良好的实时性和鲁棒性，适用于多样化自动驾驶场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-Egocentric-Instruction-oriented-Affordance-Prediction-via-Large-Multimodal-Modelpdf"><a href="#41-Egocentric-Instruction-oriented-Affordance-Prediction-via-Large-Multimodal-Modelpdf" class="headerlink" title="41. Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Modelpdf"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Egocentric_Instruction-oriented_Affordance_Prediction_via_Large_Multimodal_Model.pdf">Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xidian University</span></p>
<p>该论文提出了一个以指令为中心的、从第一人称视角出发的物体可操作性（affordance）预测任务，并发布了包含约1.5万个数据样本的新数据集EIVA。方法上，作者设计了一种基于大规模多模态模型（LMM）的“Actor-Verifier”自反迭代管线，利用LMM对机器人操作区域和动作方向进行逐步自我优化和验证。实验结果显示，所提管线在开放词汇、复杂场景、真实机器人仿真等多项基准上超越现有方法，显著提升了基于指令的可操作性预测能力，为机器人智能操作领域提供了强有力的基线和新见解。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-Physical-Embodiment-Enables-Information-Processing-Beyond-Explicit-Sensing-in-Active-Matterpdf"><a href="#42-Physical-Embodiment-Enables-Information-Processing-Beyond-Explicit-Sensing-in-Active-Matterpdf" class="headerlink" title="42. Physical Embodiment Enables Information Processing Beyond Explicit Sensing in Active Matterpdf"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Physical_Embodiment_Enables_Information_Processing_Beyond_Explicit_Sensing_in_Active_Matter.pdf">Physical Embodiment Enables Information Processing Beyond Explicit Sensing in Active Matter</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Leipzig University</span></p>
<p>本论文提出一种无需显式传感的微观自主导航方法，通过物理具身性和强化学习（PPO算法）控制自热泳动微机器人，仅利用位置信息和自身动力学反馈，实现对不可观测流场扰动的适应性导航。实验结果显示，机器人能在流场扰动强度远超自驱速度的环境中，仅凭自身动力学与环境耦合学习出对抗策略，证明了具身动力学可作为信息处理资源，为微型自主机器人和生物启发计算提供新范式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-EndoUFM-Utilizing-Foundation-Models-for-Monocular-depth-estimation-of-endoscopic-imagespdf"><a href="#43-EndoUFM-Utilizing-Foundation-Models-for-Monocular-depth-estimation-of-endoscopic-imagespdf" class="headerlink" title="43. EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic imagespdf"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/EndoUFM__Utilizing_Foundation_Models_for_Monocular_depth_estimation_of_endoscopic_images.pdf">EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本文提出EndoUFM框架，通过融合Depth Anything和Segment Anything两种视觉基础模型，实现了针对外科内窥镜图像的无监督单目深度估计。方法采用RVLoRA高效微调策略和Res-DSC残差块以提升模型对复杂纹理和光照变化的适应性，并设计了基于SAM分割掩码的平滑损失保证解剖结构内深度一致性。实验结果表明，EndoUFM在SCARED、Hamlyn、SERV-CT和EndoNeRF等公开数据集上达到了最新最优性能，增强了微创手术的空间感知和导航精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-CubeDN-Real-time-Drone-Detection-in-3D-Space-from-Dual-mmWave-Radar-Cubespdf"><a href="#44-CubeDN-Real-time-Drone-Detection-in-3D-Space-from-Dual-mmWave-Radar-Cubespdf" class="headerlink" title="44. CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubespdf"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CubeDN__Real-time_Drone_Detection_in_3D_Space_from_Dual_mmWave_Radar_Cubes.pdf">CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubes</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University College London</span></p>
<p>本论文提出了CubeDN，一种面向无人机的实时三维检测与追踪系统，采用双毫米波雷达立方体数据，通过创新的4D数据融合与3D卷积神经网络，实现小型和大型无人机的高精度检测、定位与分类。实验结果显示CubeDN在多种复杂场景下均显著优于现有方法，达到10Hz实时推理和厘米级定位精度，具有优良的泛化能力和实用价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-AgentRAN-An-Agentic-AI-Architecture-for-Autonomous-Control-of-Open-6G-Networkspdf"><a href="#45-AgentRAN-An-Agentic-AI-Architecture-for-Autonomous-Control-of-Open-6G-Networkspdf" class="headerlink" title="45. AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networkspdf"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AgentRAN__An_Agentic_AI_Architecture_for_Autonomous_Control_of_Open_6G_Networks.pdf">AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networks</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northeastern University</span></p>
<p>本文提出AgentRAN框架，通过基于大语言模型（LLM）的AI代理，实现对Open RAN架构下6G蜂窝网络的自主控制。方法包括自然语言意图解析、层级意图分解、多代理协调、AI-RAN Factory自动生成和优化控制代理，实现跨时空和协议层的智能控制。实验在真实5G测试床中验证了系统能根据高层意图自适应优化网络性能，实现网络自主进化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-MEVITA-Open-Source-Bipedal-Robot-Assembled-from-E-Commerce-Components-via-Sheet-Metal-Weldingpdf"><a href="#46-MEVITA-Open-Source-Bipedal-Robot-Assembled-from-E-Commerce-Components-via-Sheet-Metal-Weldingpdf" class="headerlink" title="46. MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Weldingpdf"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MEVITA__Open-Source_Bipedal_Robot_Assembled_from_E-Commerce_Components_via_Sheet_Metal_Welding.pdf">MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Welding</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Tokyo</span></p>
<p>本文提出了MEVITA，一种完全由电商平台可购金属部件组装、并通过板金焊接集成复杂结构的开源双足机器人。通过最小化机械结构设计和强化学习仿真-实物迁移，MEVITA实现了部件数显著减少、易于装配，并在多种环境中实现了稳定行走。实验结果验证了其硬件和学习方法的有效性，降低了双足机器人开发门槛。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-SEBVS-Synthetic-Event-based-Visual-Servoing-for-Robot-Navigation-and-Manipulationpdf"><a href="#47-SEBVS-Synthetic-Event-based-Visual-Servoing-for-Robot-Navigation-and-Manipulationpdf" class="headerlink" title="47. SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulationpdf"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SEBVS__Synthetic_Event-based_Visual_Servoing_for_Robot_Navigation_and_Manipulation.pdf">SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Arizona State University</span></p>
<p>本文提出了SEBVS，一个集成于Gazebo仿真的开源ROS2包，通过v2e模拟器将RGB图像实时转换为事件流，实现无专用硬件的类神经形态视觉感知，并设计Transformer架构融合事件和RGB数据用于机器人导航和操作。实验表明，融合事件和RGB输入的Transformer策略能在运动模糊和复杂光照下实现更高导航和抓取精度、鲁棒性及任务成功率，展示了事件视觉在机器人感知与决策中的价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-GWM-Towards-Scalable-Gaussian-World-Models-for-Robotic-Manipulationpdf"><a href="#48-GWM-Towards-Scalable-Gaussian-World-Models-for-Robotic-Manipulationpdf" class="headerlink" title="48. GWM: Towards Scalable Gaussian World Models for Robotic Manipulationpdf"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GWM__Towards_Scalable_Gaussian_World_Models_for_Robotic_Manipulation.pdf">GWM: Towards Scalable Gaussian World Models for Robotic Manipulation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出Gaussian World Model (GWM)，结合3D Gaussian Splatting表征、3D变分自编码器与扩散Transformer，实现机器人操作中未来场景的精细三维预测。GWM可作为高效的视觉表征用于模仿学习，也可作为神经仿真器助力模型型强化学习。实验表明，GWM在多任务仿真和真实机器人操作中均显著优于现有图像方法，提升策略学习成功率和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="49-Structural-Damage-Detection-Using-AI-Super-Resolution-and-Visual-Language-Modelpdf"><a href="#49-Structural-Damage-Detection-Using-AI-Super-Resolution-and-Visual-Language-Modelpdf" class="headerlink" title="49. Structural Damage Detection Using AI Super Resolution and Visual Language Modelpdf"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Structural_Damage_Detection_Using_AI_Super_Resolution_and_Visual_Language_Model.pdf">Structural Damage Detection Using AI Super Resolution and Visual Language Model</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Dakota State University</span></p>
<p>该论文提出了一个结合无人机航拍、AI视频超分辨率模型（Video Restoration Transformer, VRT）与视觉语言模型（Gemma3:27b）的灾后结构损伤自动检测框架。流程包括多角度灾前&#x2F;灾后图像采集、VRT图像增强、VLM语义分析与建筑损伤分级，验证于土耳其地震和美国龙卷风数据集，分类准确率达84.5%。结论显示该方法能快速、准确地评估灾区建筑损伤，有效辅助救灾决策，且普通用户也可操作。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-CE-RS-SBCIT-A-Novel-Channel-Enhanced-Hybrid-CNN–Transformer-with-Residual-Spatial-and-Boundary-Aware-Learning-for-Brain-Tumor-MRI-Analysispdf"><a href="#50-CE-RS-SBCIT-A-Novel-Channel-Enhanced-Hybrid-CNN–Transformer-with-Residual-Spatial-and-Boundary-Aware-Learning-for-Brain-Tumor-MRI-Analysispdf" class="headerlink" title="50. CE-RS-SBCIT: A Novel Channel-Enhanced Hybrid CNN–Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysispdf"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CE-RS-SBCIT_A_Novel_Channel_Enhanced_Hybrid_CNN_Transformer_with_Residual,_Spatial,_and_Boundary-Awa.pdf">CE-RS-SBCIT: A Novel Channel-Enhanced Hybrid CNN–Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysis</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ibadat International University</span></p>
<p>该论文提出了CE-RS-SBCIT混合框架，将残差学习CNN、空间学习CNN与变换器模块结合，用于脑肿瘤MRI分类。方法创新包括SBCIT模块、通道增强策略、空间注意机制等，能同时捕获局部细粒度和全局上下文特征。实验在Kaggle和Figshare脑肿瘤MRI公开数据集上，准确率达98.3%，灵敏度98.43%，F1分数98.25%，精度98.08%，显著优于现有CNN和ViT模型。结论表明该方法在脑肿瘤自动诊断中表现优异且具备实际应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-GRASP-Geospatial-pixel-Reasoning-viA-Structured-Policy-learningpdf"><a href="#51-GRASP-Geospatial-pixel-Reasoning-viA-Structured-Policy-learningpdf" class="headerlink" title="51. GRASP: Geospatial pixel Reasoning viA Structured Policy learningpdf"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GRASP__Geospatial_pixel_Reasoning_viA_Structured_Policy_learning.pdf">GRASP: Geospatial pixel Reasoning viA Structured Policy learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>GRASP提出了一种使用结构化策略学习的远程感知像素推理框架，结合多模态大型语言模型（MLLM）输出任务相关的边框和正点，并以此为提示输入预训练分割模型生成最终掩码。系统采用纯强化学习（GRPO），通过格式和准确性奖励进行优化，无需掩码监督，显著降低标注成本并提升泛化能力。实验在遥感领域内外均取得了SOTA表现，尤其在OOD测试集提升高达54%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-Proximal-Vision-Transformer-Enhancing-Feature-Representation-through-Two-Stage-Manifold-Geometrypdf"><a href="#52-Proximal-Vision-Transformer-Enhancing-Feature-Representation-through-Two-Stage-Manifold-Geometrypdf" class="headerlink" title="52. Proximal Vision Transformer: Enhancing Feature Representation through Two-Stage Manifold Geometrypdf"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Proximal_Vision_Transformer__Enhancing_Feature_Representation_through_Two-Stage_Manifold_Geometry.pdf">Proximal Vision Transformer: Enhancing Feature Representation through Two-Stage Manifold Geometry</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">North Carolina State University</span></p>
<p>该论文提出了一种将Vision Transformer (ViT) 与可学习的邻近算子（proximal operator）相结合的新框架，通过自注意力机制构建数据流形的切丛，并利用邻近迭代在切丛内定义截面，实现特征从局部到全局的对齐与优化，显著提升分类准确率和特征分布结构。实验结果表明，该方法在高分辨率视觉任务上优于传统ViT，提升了类内紧凑性和类间可分性，验证了几何优化思想在视觉Transformer中的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-SSG-Dit-A-Spatial-Signal-Guided-Framework-for-Controllable-Video-Generationpdf"><a href="#53-SSG-Dit-A-Spatial-Signal-Guided-Framework-for-Controllable-Video-Generationpdf" class="headerlink" title="53. SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generationpdf"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SSG-Dit__A_Spatial_Signal_Guided_Framework_for_Controllable_Video_Generation.pdf">SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>该论文提出了SSG-DiT，一种面向可控视频生成的空间信号引导扩散变换器架构，采用解耦两阶段流程：首先利用CLIP模型中间特征生成文本感知的视觉提示，然后通过轻量级SSG-Adapter将视觉和文本条件联合注入冻结的Video DiT主干，实现空间信号的精细引导。实验表明，SSG-DiT在VBench基准的空间关系控制和一致性等关键指标上显著优于现有方法，提升了生成视频对复杂多模态条件的语义一致性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-REGEN-Real-Time-Photorealism-Enhancement-in-Games-via-a-Dual-Stage-Generative-Network-Frameworkpdf"><a href="#54-REGEN-Real-Time-Photorealism-Enhancement-in-Games-via-a-Dual-Stage-Generative-Network-Frameworkpdf" class="headerlink" title="54. REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Frameworkpdf"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/REGEN__Real-Time_Photorealism_Enhancement_in_Games_via_a_Dual-Stage_Generative_Network_Framework.pdf">REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Aristotle University of Thessaloniki</span></p>
<p>该论文提出了REGEN双阶段生成网络框架，首先利用健壮的无配对图像到图像翻译方法（EPE）生成与原始游戏帧语义一致的高拟真数据，再以此为配对数据训练轻量级的Pix2PixHD模型，实现游戏中的高效实时照片级画质增强。实验表明，该方法在GTA V游戏上可将推理速度提升32倍，同时保持与SOTA方法相当的视觉质量，且易于集成无需底层引擎信息。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-PVNet-Point-Voxel-Interaction-LiDAR-Scene-Upsampling-Via-Diffusion-Modelspdf"><a href="#55-PVNet-Point-Voxel-Interaction-LiDAR-Scene-Upsampling-Via-Diffusion-Modelspdf" class="headerlink" title="55. PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Modelspdf"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PVNet__Point-Voxel_Interaction_LiDAR_Scene_Upsampling_Via_Diffusion_Models.pdf">PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Telecom Guizhou Branch</span></p>
<p>该论文提出PVNet，一种基于扩散模型的点-体素交互框架，用于无密集监督的LiDAR点云场景级超分辨率。方法包括：以分类器无关指导的DDPM为生成主干，融合多帧稀疏点云作为条件输入，通过体素补全模块丰富特征，再利用点-体素交互模块提升每个上采样点的环境感知。实验结果表明，PVNet在各类基准测试上实现了任意倍率的高质量场景点云超分辨率，性能优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-F4-ITS-Fine-grained-Feature-Fusion-for-Food-Image-Text-Searchpdf"><a href="#56-F4-ITS-Fine-grained-Feature-Fusion-for-Food-Image-Text-Searchpdf" class="headerlink" title="56. F4-ITS: Fine-grained Feature Fusion for Food Image-Text Searchpdf"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/F4-ITS__Fine-grained_Feature_Fusion_for_Food_Image-Text_Search.pdf">F4-ITS: Fine-grained Feature Fusion for Food Image-Text Search</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HyperVerge</span></p>
<p>本文提出了一种训练自由的视觉-语言模型引导框架F4-ITS，通过双向多模态特征融合和基于食材的特征重排序显著提升食物图像与文本检索性能。实验结果表明该方法在细粒度食物检索任务中对主流模型有平均10%、7.7%和28.6%的准确率提升，并证明小模型加融合可媲美大模型，适合资源受限场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-Dual-Orthogonal-Guidance-for-Robust-Diffusion-based-Handwritten-Text-Generationpdf"><a href="#57-Dual-Orthogonal-Guidance-for-Robust-Diffusion-based-Handwritten-Text-Generationpdf" class="headerlink" title="57. Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generationpdf"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dual_Orthogonal_Guidance_for_Robust_Diffusion-based_Handwritten_Text_Generation.pdf">Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Luleå University of Technology</span></p>
<p>本文提出了Dual Orthogonal Guidance (DOG)策略，通过在扩散模型生成手写文本时引入正负提示的正交投影，有效抑制伪影并提升内容清晰度和风格多样性。DOG基于三角形时序调度动态控制引导强度，可直接应用于现有手写文本扩散模型，无需重新训练，实验表明在内容准确性和风格复现方面均优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-Contrastive-Prompt-Clustering-for-Weakly-Supervised-Semantic-Segmentationpdf"><a href="#58-Contrastive-Prompt-Clustering-for-Weakly-Supervised-Semantic-Segmentationpdf" class="headerlink" title="58. Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentationpdf"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Contrastive_Prompt_Clustering_for_Weakly_Supervised_Semantic_Segmentation.pdf">Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong-Liverpool University</span></p>
<p>该论文提出了一种基于对比提示聚类（CPC）的弱监督语义分割方法，利用大语言模型（LLM）自动生成类别聚类，编码类别间的内在关系，并引入类感知的patch级对比损失以强化类内一致性与类间分离。实验证明该方法在PASCAL VOC 2012和MS COCO 2014数据集上超过了现有的最先进方法，显著提升了弱监督分割的精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-HiCache-Training-free-Acceleration-of-Diffusion-Models-via-Hermite-Polynomial-based-Feature-Cachingpdf"><a href="#59-HiCache-Training-free-Acceleration-of-Diffusion-Models-via-Hermite-Polynomial-based-Feature-Cachingpdf" class="headerlink" title="59. HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Cachingpdf"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HiCache__Training-free_Acceleration_of_Diffusion_Models_via_Hermite_Polynomial-based_Feature_Caching.pdf">HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出HiCache，一种基于Hermite多项式的特征缓存方法，无需训练即可加速扩散模型的推理过程。方法通过验证扩散Transformer特征动态近似高斯分布，进而用Hermite多项式替代传统的Taylor展开进行特征预测，并引入双缩放机制保证数值稳定性。实验证明HiCache在文本生成图像、视频生成和超分辨率任务中实现高达6.24倍加速，同时生成质量超过现有基线。结论是HiCache能在无需模型重训练下大幅提升扩散模型的推理效率与生成质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-Combating-Digitally-Altered-Images-Deepfake-Detectionpdf"><a href="#60-Combating-Digitally-Altered-Images-Deepfake-Detectionpdf" class="headerlink" title="60. Combating Digitally Altered Images: Deepfake Detectionpdf"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Combating_Digitally_Altered_Images__Deepfake_Detection.pdf">Combating Digitally Altered Images: Deepfake Detection</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Amrita School of Computing</span></p>
<p>本文提出一种基于改进版Vision Transformer（ViT）的深度伪造（Deepfake）图像检测方法，利用OpenForensics数据集并结合多种数据增强和处理技术，提升模型对多样化图像篡改的鲁棒性。模型在测试集上取得了超过99%的检测准确率，表明其在真实与伪造图像分类任务中具备高效和实际应用价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-RPD-Diff-Region-Adaptive-Physics-Guided-Diffusion-Model-for-Visibility-Enhancement-under-Dense-and-Non-Uniform-Hazepdf"><a href="#61-RPD-Diff-Region-Adaptive-Physics-Guided-Diffusion-Model-for-Visibility-Enhancement-under-Dense-and-Non-Uniform-Hazepdf" class="headerlink" title="61. RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Hazepdf"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/RPD-Diff__Region-Adaptive_Physics-Guided_Diffusion_Model_for_Visibility_Enhancement_under_Dense_and_.pdf">RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Haze</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Intelligent Systems Engineering, Shenzhen Campus of Sun Yat-Sen University</span></p>
<p>本文提出了一种区域自适应物理引导扩散模型（RPD-Diff），用于在密集和非均匀雾霾条件下提升图像可见性。该方法引入物理引导的中间状态目标（PIST）策略，通过物理先验动态调整扩散过程，并结合基于传输图的交叉注意力机制实现局部去噪步数自适应（HADTP），有效应对空间异质性雾霾。实验表明，RPD-Diff在四个真实世界数据集上均取得了优于现有方法的性能，能够生成高质量、无雾且细节清晰、色彩真实的图像。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-Disentangling-Polysemantic-Neurons-with-a-Null-Calibrated-Polysemanticity-Index-and-Causal-Patch-Interventionspdf"><a href="#62-Disentangling-Polysemantic-Neurons-with-a-Null-Calibrated-Polysemanticity-Index-and-Causal-Patch-Interventionspdf" class="headerlink" title="62. Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventionspdf"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Disentangling_Polysemantic_Neurons_with_a_Null-Calibrated_Polysemanticity_Index_and_Causal_Patch_Int.pdf">Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventions</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">BITS Pilani</span></p>
<p>本论文提出了一种新的多证据神经元多义性度量指标——Polysemanticity Index (PSI)，结合几何聚类质量、类别标签对齐和基于CLIP的开放词汇语义判别，通过与显式的零假设分布校准，有效识别和量化神经网络中响应多种概念的多义神经元。实验结果表明，PSI能显著区分真实和随机信号，揭示深层网络多义性增强，并通过因果干预验证了发现的概念原型对神经元激活具备因果效力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-HieroAction-Hierarchically-Guided-VLM-for-Fine-Grained-Action-Analysispdf"><a href="#63-HieroAction-Hierarchically-Guided-VLM-for-Fine-Grained-Action-Analysispdf" class="headerlink" title="63. HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysispdf"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HieroAction__Hierarchically_Guided_VLM_for_Fine-Grained_Action_Analysis.pdf">HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysis</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>该论文提出HieroAction，一种结合视觉-语言模型（VLM）、分步动作推理（SAR）和分层策略学习（HPL）的精细化动作分析方法。其核心流程为：首先利用SAR将复杂人类动作分解为观察、识别、评估、结论四个阶段，生成结构化、可解释的评估序列；随后以Qwen-VL-2.5-7B为基础，通过有监督微调和基于多级奖励的分层策略强化学习，提升模型对子动作识别、时序边界及评分准确性的理解。实验在FineDive、FineFS和LOGO等细粒度体育动作数据集上取得了比现有方法更高的准确率和可解释性。结论认为，融合结构化推理与策略学习的视觉-语言模型能大幅提升复杂动作评估的透明度、准确性和通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-HunyuanVideo-Foley-Multimodal-Diffusion-with-Representation-Alignment-for-High-Fidelity-Foley-Audio-Generationpdf"><a href="#64-HunyuanVideo-Foley-Multimodal-Diffusion-with-Representation-Alignment-for-High-Fidelity-Foley-Audio-Generationpdf" class="headerlink" title="64. HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generationpdf"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HunyuanVideo-Foley__Multimodal_Diffusion_with_Representation_Alignment_for_High-Fidelity_Foley_Audio.pdf">HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent Hunyuan</span></p>
<p>该论文提出HunyuanVideo-Foley，一种端到端的文本-视频-音频生成框架，利用多模态扩散Transformer结构和表示对齐(REPA)策略，结合大规模自动标注的高质量多模态数据集，提升了生成音频的视觉-语义对齐、时序同步和音质。实验结果表明，该方法在多个权威基准上实现了新的SOTA，显著提升了视频语义对齐和音质。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-LGE-Guided-Cross-Modality-Contrastive-Learning-for-Gadolinium-Free-Cardiomyopathy-Screening-in-Cine-CMRpdf"><a href="#65-LGE-Guided-Cross-Modality-Contrastive-Learning-for-Gadolinium-Free-Cardiomyopathy-Screening-in-Cine-CMRpdf" class="headerlink" title="65. LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMRpdf"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LGE-Guided_Cross-Modality_Contrastive_Learning_for_Gadolinium-Free_Cardiomyopathy_Screening_in_Cine_.pdf">LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMR</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ShanghaiTech University</span></p>
<p>本文提出了CC-CMR框架，通过对心脏磁共振（CMR）影像中的cine序列与晚期钆增强（LGE）序列的潜在空间对齐，利用对比学习和特征交互模块（包含不确定性感知机制）实现无需钆剂的心肌病筛查。该方法在多中心数据集上显著提升了诊断准确率（94.3%），优于现有方法，并具备良好的泛化能力和临床适用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-MDD-A-Dataset-for-Text-and-Music-Conditioned-Duet-Dance-Generationpdf"><a href="#66-MDD-A-Dataset-for-Text-and-Music-Conditioned-Duet-Dance-Generationpdf" class="headerlink" title="66. MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generationpdf"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MDD__A_Dataset_for_Text-and-Music_Conditioned_Duet_Dance_Generation.pdf">MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Purdue University</span></p>
<p>本文提出了MDD数据集，这是首个同时包含高质量3D双人舞动作、音乐及细粒度文本描述的大规模多模态数据集，支持文本和音乐条件下的双人舞动作生成。作者定义了两个新的基准任务：Text-to-Duet（基于文本和音乐生成双人舞动作）与Text-to-Dance Accompaniment（基于文本、音乐及领舞动作生成跟随舞者动作），并基于现有方法进行了基线实验。实验结果表明，MDD在规模和标注深度上均超越现有数据集，为互动动画与自动舞蹈生成等研究提供了坚实基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-Generating-Synthetic-Contrast-Enhanced-Chest-CT-Images-from-Non-Contrast-Scans-Using-Slice-Consistent-Brownian-Bridge-Diffusion-Networkpdf"><a href="#67-Generating-Synthetic-Contrast-Enhanced-Chest-CT-Images-from-Non-Contrast-Scans-Using-Slice-Consistent-Brownian-Bridge-Diffusion-Networkpdf" class="headerlink" title="67. Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Networkpdf"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Generating_Synthetic_Contrast-Enhanced_Chest_CT_Images_from_Non-Contrast_Scans_Using_Slice-Consisten.pdf">Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Network</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Saskatchewan</span></p>
<p>本文提出一种基于Slice-Consistent Brownian Bridge Diffusion Model (SC-BBDM) 的方法，实现从无造影胸部CT生成高保真合成造影CT图像。该方法通过预处理（配准、分割）、风格一致性和切片间轨迹对齐，有效提升解剖结构和对比度保持性。实验结果表明，SC-BBDM在保留血管结构和对比度真实性方面优于现有基线方法，具有提升诊断安全性和降低成本的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-A-Lightweight-Convolution-and-Vision-Transformer-integrated-model-with-Multi-scale-Self-attention-Mechanismpdf"><a href="#68-A-Lightweight-Convolution-and-Vision-Transformer-integrated-model-with-Multi-scale-Self-attention-Mechanismpdf" class="headerlink" title="68. A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanismpdf"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Lightweight_Convolution_and_Vision_Transformer_integrated_model_with_Multi-scale_Self-attention_Me.pdf">A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanism</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sichuan University</span></p>
<p>该论文提出了SAEViT，一种融合卷积神经网络（CNN）和视觉Transformer（ViT）的轻量级混合架构，核心包括稀疏聚合注意力（SAA）模块和通道交互前馈网络（CIFFN），以提升局部感知和全局建模能力。通过在ImageNet分类、COCO目标检测和ADE20K语义分割等任务上的实验，SAEViT在保持低计算量下实现了优于同类轻量模型的准确率-速度权衡，展示了其在多种视觉任务下的有效性与高效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-Multimodal-Medical-Endoscopic-Image-Analysis-via-Progressive-Disentangle-aware-Contrastive-Learningpdf"><a href="#69-Multimodal-Medical-Endoscopic-Image-Analysis-via-Progressive-Disentangle-aware-Contrastive-Learningpdf" class="headerlink" title="69. Multimodal Medical Endoscopic Image Analysis via Progressive Disentangle-aware Contrastive Learningpdf"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multimodal_Medical_Endoscopic_Image_Analysis_via_Progressive_Disentangle-aware_Contrastive_Learning.pdf">Multimodal Medical Endoscopic Image Analysis via Progressive Disentangle-aware Contrastive Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</span></p>
<p>本论文提出了一种创新的多模态医学内镜图像分割框架，结合’Align-Disentangle-Fusion’机制，通过多尺度分布对齐及逐步特征解耦，对2D白光和窄带成像进行融合。方法包括多尺度分布对齐、特征解耦及解耦感知对比学习，实现模态差异消除和高效语义融合。实验结果显示，该方法在多数据集上显著优于现有方法，提升了临床分割准确率，具备较强的泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-Do-Multimodal-LLMs-See-Sentiment-pdf"><a href="#70-Do-Multimodal-LLMs-See-Sentiment-pdf" class="headerlink" title="70. Do Multimodal LLMs See Sentiment?pdf"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Do_Multimodal_LLMs_See_Sentiment_.pdf">Do Multimodal LLMs See Sentiment?</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universidade Tecnologica Federal do Paraná</span></p>
<p>本文提出MLLMsent框架，系统性地评估多模态大语言模型（如MiniGPT-4、GPT-4o mini、DeepSeek-VL2-Tiny）在视觉情感分析中的推理能力。方法包括直接图像情感分类、通过自动生成图像描述并用文本LLM分析情感、以及在带标签的描述上微调LLM。实验结果表明，微调后的MLLMsent在PerceptSent和DeepSent数据集上，显著优于Lexicon、CNN和Transformer等传统方法，F1-score最高提升64.8%，且具备较强的跨数据集泛化能力。结论是：多模态视觉-语言推理结合微调的文本模型，能有效提升社交媒体视觉情感分析的准确性和可解释性，并设立了新的性能基线。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-Delta-SVD-Efficient-Compression-for-Personalized-Text-to-Image-Modelspdf"><a href="#71-Delta-SVD-Efficient-Compression-for-Personalized-Text-to-Image-Modelspdf" class="headerlink" title="71. Delta-SVD: Efficient Compression for Personalized Text-to-Image Modelspdf"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Delta-SVD__Efficient_Compression_for_Personalized_Text-to-Image_Models.pdf">Delta-SVD: Efficient Compression for Personalized Text-to-Image Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University</span></p>
<p>该论文提出Delta-SVD方法，通过对DreamBooth个性化文本到图像扩散模型的权重更新进行后处理压缩，采用SVD分解和能量阈值截断实现低秩近似，无需再训练或模型架构修改。实验表明，Delta-SVD能在极大压缩模型存储的同时保持生成质量，与主流方法相比压缩比和视觉保真度均优，支持大规模个性化扩散模型的高效部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-TinySR-Pruning-Diffusion-for-Real-World-Image-Super-Resolutionpdf"><a href="#72-TinySR-Pruning-Diffusion-for-Real-World-Image-Super-Resolutionpdf" class="headerlink" title="72. TinySR: Pruning Diffusion for Real-World Image Super-Resolutionpdf"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TinySR__Pruning_Diffusion_for_Real-World_Image_Super-Resolution.pdf">TinySR: Pruning Diffusion for Real-World Image Super-Resolution</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出了TinySR，一种针对真实场景图像超分辨率的高效扩散模型。方法包括动态深度剪枝（动态块间激活和扩展-腐蚀策略）、VAE压缩（通道剪枝、注意力移除、轻量化卷积）、去除冗余条件模块和参数预缓存等，整体显著降低计算量和模型规模。实验表明，TinySR在保持高感知质量的同时，推理速度提升5.68倍，参数减少83%，适合实时和边缘设备部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-OmniMRI-A-Unified-Vision–Language-Foundation-Model-for-Generalist-MRI-Interpretationpdf"><a href="#73-OmniMRI-A-Unified-Vision–Language-Foundation-Model-for-Generalist-MRI-Interpretationpdf" class="headerlink" title="73. OmniMRI: A Unified Vision–Language Foundation Model for Generalist MRI Interpretationpdf"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/OmniMRI__A_Unified_Vision--Language_Foundation_Model_for_Generalist_MRI_Interpretation.pdf">OmniMRI: A Unified Vision–Language Foundation Model for Generalist MRI Interpretation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Athinoula A. Martinos Center for Biomedical Imaging, Harvard Medical School</span></p>
<p>OmniMRI提出了一种统一的视觉-语言基础模型，能够覆盖MRI全流程任务，包括重建、分割、检测、诊断建议和报告生成。方法上，模型采用大规模异构数据集，通过自监督视觉预训练、视觉-语言对齐、多模态预训练和多任务指令微调等多阶段训练流程，实现跨模态推理和多任务泛化。结论显示，OmniMRI能够在一个架构下高质量完成多种MRI相关任务，有望将碎片化流程整合为通用框架，提升医学影像自动化与临床决策支持能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-Social-MAE-A-Transformer-Based-Multimodal-Autoencoder-for-Face-and-Voicepdf"><a href="#74-Social-MAE-A-Transformer-Based-Multimodal-Autoencoder-for-Face-and-Voicepdf" class="headerlink" title="74. Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voicepdf"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Social-MAE__A_Transformer-Based_Multimodal_Autoencoder_for_Face_and_Voice.pdf">Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Numediart Institute, ISIA Lab, University of Mons</span></p>
<p>该论文提出了Social-MAE，一种基于Transformer的多模态Masked Autoencoder，专为人脸和语音社交行为理解设计。方法在大规模社交音视频数据集（VoxCeleb2）上通过自监督学习预训练，采用改进CAV-MAE结构，增强视觉时序分辨率，并结合对比损失和重构损失。通过在情感识别、笑声检测和表面人格估计任务上微调，Social-MAE在多模态情感识别和笑声检测上达到了最新水平，验证了领域内自监督预训练对社交行为感知任务的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-Multimodal-Representation-Learning-Conditioned-on-Semantic-Relationspdf"><a href="#75-Multimodal-Representation-Learning-Conditioned-on-Semantic-Relationspdf" class="headerlink" title="75. Multimodal Representation Learning Conditioned on Semantic Relationspdf"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multimodal_Representation_Learning_Conditioned_on_Semantic_Relations.pdf">Multimodal Representation Learning Conditioned on Semantic Relations</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Emory University</span></p>
<p>该论文提出了RCML（Relation-Conditioned Multimodal Learning）框架，通过自然语言语义关系描述对多模态（图像-文本）特征提取和对齐进行条件引导。方法创新地构建了多对多的训练对，并引入关系引导的跨注意力机制，联合跨模态与模态内对比损失，实现了关系上下文中的多模态一致性。实验证明，RCML在多个数据集上的检索和分类任务均优于现有强基线模型，显著提升了关系感知的多模态表示能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-GraphMMP-A-Graph-Neural-Network-Model-with-Mutual-Information-and-Global-Fusion-for-Multimodal-Medical-Prognosispdf"><a href="#76-GraphMMP-A-Graph-Neural-Network-Model-with-Mutual-Information-and-Global-Fusion-for-Multimodal-Medical-Prognosispdf" class="headerlink" title="76. GraphMMP: A Graph Neural Network Model with Mutual Information and Global Fusion for Multimodal Medical Prognosispdf"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GraphMMP__A_Graph_Neural_Network_Model_with_Mutual_Information_and_Global_Fusion_for_Multimodal_Medi.pdf">GraphMMP: A Graph Neural Network Model with Mutual Information and Global Fusion for Multimodal Medical Prognosis</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hangzhou Dianzi University</span></p>
<p>该论文提出了一种基于图神经网络的双阶段多模态医学预后模型GraphMMP，方法包括利用互信息进行特征图构建和基于Mamba的全局融合模块以提升全局感知能力。实验表明，GraphMMP在肝脏和乳腺癌预后任务中准确率和鲁棒性均优于现有方法，证明了其在多模态医学数据分析中的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-Multi-Level-LVLM-Guidance-for-Untrimmed-Video-Action-Recognitionpdf"><a href="#77-Multi-Level-LVLM-Guidance-for-Untrimmed-Video-Action-Recognitionpdf" class="headerlink" title="77. Multi-Level LVLM Guidance for Untrimmed Video Action Recognitionpdf"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multi-Level_LVLM_Guidance_for_Untrimmed_Video_Action_Recognition.pdf">Multi-Level LVLM Guidance for Untrimmed Video Action Recognition</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kunming University of Science and Technology</span></p>
<p>本文提出了事件情境化视频Transformer（ECVT）架构，通过引入大规模视觉-语言模型（LVLM）生成的多粒度语义描述，采用视频编码分支与跨模态引导分支结合，利用全局事件提示与时序子事件提示等多级文本信息，通过自适应门控融合、跨模态注意力和事件图校准，有效提升视频时序结构和事件逻辑理解能力。在ActivityNet v1.3和THUMOS14数据集上，ECVT实现了平均mAP 40.5%和<a href="mailto:&#x6d;&#x41;&#x50;&#x40;&#48;&#46;&#53;">mAP@0.5</a> 67.1%的领先性能，优于现有主流方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-An-LLM-LVLM-Driven-Agent-for-Iterative-and-Fine-Grained-Image-Editingpdf"><a href="#78-An-LLM-LVLM-Driven-Agent-for-Iterative-and-Fine-Grained-Image-Editingpdf" class="headerlink" title="78. An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editingpdf"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/An_LLM-LVLM_Driven_Agent_for_Iterative_and_Fine-Grained_Image_Editing.pdf">An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kunming University of Science and Technology</span></p>
<p>该论文提出了RefineEdit-Agent，一个无需训练的智能代理框架，利用大型语言模型（LLM）进行编辑任务规划，并结合视觉-语言大型模型（LVLM）进行场景理解和迭代反馈，实现复杂、多步、细粒度的图像编辑。通过引入LongBench-T2I-Edit基准和大量实验证明，该方法在编辑精度和上下文保持方面显著优于现有主流方法。结论：RefineEdit-Agent可高效执行复杂编辑指令，并保持图像整体一致性，推动智能图像编辑技术发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-Robust-Point-Cloud-Registration-via-Geometric-Overlapping-Guided-Rotation-Searchpdf"><a href="#79-Robust-Point-Cloud-Registration-via-Geometric-Overlapping-Guided-Rotation-Searchpdf" class="headerlink" title="79. Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Searchpdf"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Robust_Point_Cloud_Registration_via_Geometric_Overlapping_Guided_Rotation_Search.pdf">Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>该论文提出了一种基于几何最大重叠和分支定界（BnB）搜索的点云配准方法GMOR。方法分两阶段：首先利用Chasles定理将刚性变换分解为旋转轴与轴向平移，通过立方映射参数化旋转轴并用区间扫描加速RMQ，第二阶段将2D配准转化为1D旋转角度搜索，利用扫线算法和线段树对轴对齐矩形做最大重叠查询。实验表明GMOR在3DMatch、3DLoMatch和KITTI数据集上配准精度和计算效率优于现有方法，且复杂度随点数线性增长，适合大规模场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-Constrained-Prompt-Enhancement-for-Improving-Zero-Shot-Generalization-of-Vision-Language-Modelspdf"><a href="#80-Constrained-Prompt-Enhancement-for-Improving-Zero-Shot-Generalization-of-Vision-Language-Modelspdf" class="headerlink" title="80. Constrained Prompt Enhancement for Improving Zero-Shot Generalization of Vision-Language Modelspdf"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Constrained_Prompt_Enhancement_for_Improving_Zero-Shot_Generalization_of_Vision-Language_Models.pdf">Constrained Prompt Enhancement for Improving Zero-Shot Generalization of Vision-Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tianjin University</span></p>
<p>该论文提出了一种约束性提示增强（CPE）方法，通过“拓扑引导同义语义生成”（TGSSG）和“类别无关判别区域选择”（CADRS）分别生成完整的文本和精简的视觉提示，并采用基于测试时自适应（TTA）和最优传输（OT）的集合到集合匹配策略提升视觉-文本对齐能力。实验表明，CPE方法在零样本图像分类、分布外泛化和视频动作识别任务上均显著超过现有方法，取得了最先进的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-Lightweight-Joint-Optimization-of-General-Purpose-Vision-Language-Models-and-Retrievers-for-Medical-Diagnosispdf"><a href="#81-Lightweight-Joint-Optimization-of-General-Purpose-Vision-Language-Models-and-Retrievers-for-Medical-Diagnosispdf" class="headerlink" title="81. Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosispdf"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Lightweight_Joint_Optimization_of_General-Purpose_Vision-Language_Models_and_Retrievers_for_Medical_.pdf">Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosis</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hebrew University of Jerusalem</span></p>
<p>该论文提出JOMED方法，通过联合优化通用多模态检索器和大型视觉语言模型（LVLM），在无需医学预训练的情况下实现医学图像分类和视觉问答任务。方法采用两阶段训练流程，LVLM和检索器分别进行轻量级微调，并设计了基于KL散度的损失函数，使LVLM的误差信号能够反向传播至检索器，有效提升在检索结果不一致的复杂病例上的表现。实验结果表明，JOMED在多个医学基准任务上取得了与医学预训练模型相媲美甚至更优的性能，尤其在困难检索情形下显著提升了准确率和可靠性，但与理论最优（oracle）之间仍有提升空间。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-Neural-Proteomics-Fields-for-Super-resolved-Spatial-Proteomics-Predictionpdf"><a href="#82-Neural-Proteomics-Fields-for-Super-resolved-Spatial-Proteomics-Predictionpdf" class="headerlink" title="82. Neural Proteomics Fields for Super-resolved Spatial Proteomics Predictionpdf"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Neural_Proteomics_Fields_for_Super-resolved_Spatial_Proteomics_Prediction.pdf">Neural Proteomics Fields for Super-resolved Spatial Proteomics Prediction</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, University of Chinese Academy of Sciences</span></p>
<p>该论文提出了Neural Proteomics Fields (NPF)，首创将空间超分辨蛋白组学（seq-SP）预测任务建模为连续空间的蛋白表达重建问题。NPF采用空间建模模块与形态建模模块的双分支架构，结合深度神经网络、Vision Transformer和基础病理模型UNI，能有效捕捉组织特异的蛋白空间分布与形态特征，并在公开的Pseudo-Visium SP和真实10X Visium数据集上实现了参数量更少下的SOTA性能。结论显示，NPF方法大幅提升了蛋白空间分辨预测的准确性和泛化能力，并为空间蛋白组学研究提供了新范式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-DiCache-Let-Diffusion-Model-Determine-Its-Own-Cachepdf"><a href="#83-DiCache-Let-Diffusion-Model-Determine-Its-Own-Cachepdf" class="headerlink" title="83. DiCache: Let Diffusion Model Determine Its Own Cachepdf"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DiCache__Let_Diffusion_Model_Determine_Its_Own_Cache.pdf">DiCache: Let Diffusion Model Determine Its Own Cache</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出DiCache，一种运行时自适应、无需训练的缓存加速策略，用于提升扩散模型的推理速度和视觉质量。方法包括在线浅层探针分析（动态判定何时缓存）和缓存轨迹动态对齐（自适应多步缓存利用），实验表明DiCache在多种主流图像&#x2F;视频扩散模型上均显著优于现有方法，兼具高效和高保真度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-PosBridge-Multi-View-Positional-Embedding-Transplant-for-Identity-Aware-Image-Editingpdf"><a href="#84-PosBridge-Multi-View-Positional-Embedding-Transplant-for-Identity-Aware-Image-Editingpdf" class="headerlink" title="84. PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editingpdf"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PosBridge__Multi-View_Positional_Embedding_Transplant_for_Identity-Aware_Image_Editing.pdf">PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editing</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Electro-Communications</span></p>
<p>该论文提出了PosBridge，一种无需训练的身份感知图像编辑框架，通过多视角位置嵌入移植（positional embedding transplant）和Corner Centered Layout，实现对象特征结构与场景的高保真融合。其方法以扩散模型为基础，利用参考图像的位置信息引导目标区域的合成，同时可选用轻量级LoRA模块增强细节表现。实验显示，该方法在结构一致性、外观真实感及计算效率方面优于主流基线，能有效实现个性化、空间可控的对象插入。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-FoundDiff-Foundational-Diffusion-Model-for-Generalizable-Low-Dose-CT-Denoisingpdf"><a href="#85-FoundDiff-Foundational-Diffusion-Model-for-Generalizable-Low-Dose-CT-Denoisingpdf" class="headerlink" title="85. FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoisingpdf"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FoundDiff__Foundational_Diffusion_Model_for_Generalizable_Low-Dose_CT_Denoising.pdf">FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本论文提出FoundDiff，一种两阶段的基础扩散模型，实现了在不同剂量水平和解剖区域下通用的低剂量CT去噪。方法上，第一阶段利用基于CLIP的DA-CLIP模型，通过对剂量排序和解剖判别损失获得连续的剂量和解剖表征；第二阶段用DA-Diff扩散模型，通过创新的DACB模块将上述表征融合于扩散过程，实现自适应去噪。实验在多公开数据集和不同剂量、解剖条件下，FoundDiff均优于现有方法，且对未见剂量具备强泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoningpdf"><a href="#86-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoningpdf" class="headerlink" title="86. Explain Before You Answer: A Survey on Compositional Visual Reasoningpdf"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Explain_Before_You_Answer__A_Survey_on_Compositional_Visual_Reasoning.pdf">Explain Before You Answer: A Survey on Compositional Visual Reasoning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University</span></p>
<p>本论文系统综述了2023-2025年间260余篇关于组合式视觉推理的研究，梳理了从基于提示的语言中心方法、工具增强的大模型、链式思维推理到统一体式视觉语言模型的五阶段发展范式，涵盖关键技术如多模态推理、工具集成、链式推理与视觉证据显式展示。结论指出，组合式视觉推理以结构化中间步骤实现更强的泛化、认知对齐、可解释性和鲁棒性，但仍面临数据、评测和系统集成等挑战，未来应关注世界模型集成与更细致的评测协议。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-Quickly-Tuning-Foundation-Models-for-Image-Segmentationpdf"><a href="#87-Quickly-Tuning-Foundation-Models-for-Image-Segmentationpdf" class="headerlink" title="87. Quickly Tuning Foundation Models for Image Segmentationpdf"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Quickly_Tuning_Foundation_Models_for_Image_Segmentation.pdf">Quickly Tuning Foundation Models for Image Segmentation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Freiburg</span></p>
<p>本论文提出QTT-SEG，一种基于元学习和贝叶斯优化的自动化快速微调SAM（Segment Anything Model）图像分割基础模型的方法。QTT-SEG通过预训练的性能与成本预测器，结合丰富的超参数搜索空间，在13个二值与多类分割数据集上实现高效、稳定的微调，显著优于SAM零样本及AutoGluon等AutoML基线，尤其在限定时间预算下表现突出。结论显示QTT-SEG能显著提升基础模型在领域特定分割任务中的表现，具备优秀的效率、稳定性与泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-ERF-BA-TFD-A-Multimodal-Model-for-Audio-Visual-Deepfake-Detectionpdf"><a href="#88-ERF-BA-TFD-A-Multimodal-Model-for-Audio-Visual-Deepfake-Detectionpdf" class="headerlink" title="88. ERF-BA-TFD+: A Multimodal Model for Audio-Visual Deepfake Detectionpdf"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ERF-BA-TFD+__A_Multimodal_Model_for_Audio-Visual_Deepfake_Detection.pdf">ERF-BA-TFD+: A Multimodal Model for Audio-Visual Deepfake Detection</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Lanzhou University</span></p>
<p>该论文提出了ERF-BA-TFD+，一种结合增强感受野（ERF）和音视频融合的多模态深度伪造检测模型。方法包括视觉编码器（MViTv2）、音频编码器（BYOL-A）、跨重建注意力Transformer（CRATrans）、帧分类与边界定位模块，以及特征增强模块，能够建模音视频长程依赖，精准定位伪造片段。实验在DDL-AV数据集上进行，模型在精度、召回率、F1分数等指标上超越现有方法，尤其在长时视频和音视频不同步场景下表现突出，并获得深度伪造检测竞赛第一名。结论：ERF-BA-TFD+显著提升了多模态深度伪造检测的准确性和鲁棒性，适用于复杂真实场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-MTNet-Learning-Modality-Aware-Representation-with-Transformer-for-RGBT-Trackingpdf"><a href="#89-MTNet-Learning-Modality-Aware-Representation-with-Transformer-for-RGBT-Trackingpdf" class="headerlink" title="89. MTNet: Learning Modality-Aware Representation with Transformer for RGBT Trackingpdf"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MTNet__Learning_modality-aware_representation_with_transformer_for_RGBT_tracking.pdf">MTNet: Learning Modality-Aware Representation with Transformer for RGBT Tracking</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本文提出了一种基于Transformer的模态感知RGBT目标跟踪方法MTNet，创新性地设计了模态感知网络（包括通道聚合分配模块CADM和空间相似性感知模块SSPM）以增强多模态特征表达，并结合混合Transformer融合网络实现模板与搜索区域的全局依赖建模。通过三分支预测头与状态感知动态模板更新策略，有效提升了定位精度和跟踪鲁棒性。实验结果表明，MTNet在多个RGBT基准数据集上均优于现有方法，且具备实时跟踪速度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-PersPose-3D-Human-Pose-Estimation-with-Perspective-Encoding-and-Perspective-Rotationpdf"><a href="#90-PersPose-3D-Human-Pose-Estimation-with-Perspective-Encoding-and-Perspective-Rotationpdf" class="headerlink" title="90. PersPose: 3D Human Pose Estimation with Perspective Encoding and Perspective Rotationpdf"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PersPose__3D_Human_Pose_Estimation_with_Perspective_Encoding_and_Perspective_Rotation.pdf">PersPose: 3D Human Pose Estimation with Perspective Encoding and Perspective Rotation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southern University of Science and Technology</span></p>
<p>本文提出了一种新颖的3D人体姿态估计算法PersPose，核心贡献包括引入Perspective Encoding（PE）模块，将裁剪后图像的相机内参编码为2D特征图与图像一同送入CNN，以及提出Perspective Rotation（PR）模块，通过旋转原图使人体主体居中，减少视角畸变并简化模型拟合难度。实验结果显示，PersPose在3DPW、MPI-INF-3DHP和Human3.6M等数据集上均实现了SOTA性能，特别在复杂真实场景下表现出更强鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-Multi-Agent-Visual-Language-Reasoning-for-Comprehensive-Highway-Scene-Understandingpdf"><a href="#91-Multi-Agent-Visual-Language-Reasoning-for-Comprehensive-Highway-Scene-Understandingpdf" class="headerlink" title="91. Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understandingpdf"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multi-Agent_Visual-Language_Reasoning_for_Comprehensive_Highway_Scene_Understanding.pdf">Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Georgia</span></p>
<p>该论文提出了一种多智能体视觉-语言推理框架，采用大型通用视觉语言模型（如GPT-4o）结合领域知识生成链式思维（CoT）提示，引导小型高效VLM（如Qwen2.5-VL-7B）对短视频及多模态数据进行推理，实现对天气分类、路面湿滑评估和交通拥堵检测等多任务高效感知。实验结果表明，该方法在多样化交通和环境条件下表现稳健，显著提升多任务推理准确率，并可集成于现有交通摄像头系统，实现大规模部署和高风险路段实时监控。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-MMCIG-Multimodal-Cover-Image-Generation-for-Text-only-Documents-and-Its-Dataset-Construction-via-Pseudo-labelingpdf"><a href="#92-MMCIG-Multimodal-Cover-Image-Generation-for-Text-only-Documents-and-Its-Dataset-Construction-via-Pseudo-labelingpdf" class="headerlink" title="92. MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labelingpdf"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MMCIG__Multimodal_Cover_Image_Generation_for_Text-only_Documents_and_Its_Dataset_Construction_via_Ps.pdf">MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labeling</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chungnam National University</span></p>
<p>本文提出了MMCIG任务，针对仅含文本的文档，先自动生成简明摘要，再生成与摘要视觉对应的封面图片。核心方法是多模态伪标签标注，通过独立对图片和其说明进行排序，选出与摘要最相关的图片，并构建高质量训练集。实验和人工评测显示，该方法比只用文本或图片伪标注更能生成与摘要高度一致的图片，并提升生成模型的表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-Advancing-Weakly-Supervised-Change-Detection-in-Satellite-Images-via-Adversarial-Class-Promptingpdf"><a href="#93-Advancing-Weakly-Supervised-Change-Detection-in-Satellite-Images-via-Adversarial-Class-Promptingpdf" class="headerlink" title="93. Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Promptingpdf"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Advancing_Weakly-Supervised_Change_Detection_in_Satellite_Images_via_Adversarial_Class_Prompting.pdf">Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wuhan University</span></p>
<p>该论文提出了一种新颖的Adversarial Class Prompting (AdvCP)方法用于遥感卫星图像的弱监督变化检测。方法包含两个阶段：首先通过对未变化样本施加强制性分类扰动挖掘共现噪声（背景变化），其次结合全局原型进行对抗样本特征的聚类校正，从而显著提升模型区分目标变化与环境噪声的能力。实验证明，AdvCP可无缝集成至主流ConvNet、Transformer及SAM等多种网络，在多个公开数据集上带来最高7.46% IoU提升，且对多类弱监督分割与全监督变化检测等任务也具有良好泛化性。结论：AdvCP方法增强了弱监督变化检测的鲁棒性和泛用性，有效降低了误检背景变化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-VROOM-Visual-Reconstruction-over-Onboard-Multiviewpdf"><a href="#94-VROOM-Visual-Reconstruction-over-Onboard-Multiviewpdf" class="headerlink" title="94. VROOM - Visual Reconstruction over Onboard Multiviewpdf"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/VROOM_-_Visual_Reconstruction_over_Onboard_Multiview.pdf">VROOM - Visual Reconstruction over Onboard Multiview</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Berkeley</span></p>
<p>该论文提出VROOM系统，通过对F1赛车的车载摄像头视频进行预处理（包括分块、掩膜、降采样等），结合Monst3r、AnyCam等3D重建与SLAM方法，实现赛道及车辆轨迹的三维重建。实验表明，在复杂动态环境下，VROOM能恢复部分赛道和车辆轨迹，展示了车载视频实现可扩展4D重建的可行性，但全局一致性和多视角融合仍需进一步优化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-GSVisLoc-Generalizable-Visual-Localization-for-Gaussian-Splatting-Scene-Representationspdf"><a href="#95-GSVisLoc-Generalizable-Visual-Localization-for-Gaussian-Splatting-Scene-Representationspdf" class="headerlink" title="95. GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representationspdf"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GSVisLoc__Generalizable_Visual_Localization_for_Gaussian_Splatting_Scene_Representations.pdf">GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Weizmann Institute of Science</span></p>
<p>该论文提出GSVisLoc，一种基于3D Gaussian Splatting（3DGS）场景表征的视觉定位方法。GSVisLoc通过3DGS编码器和2D图像编码器提取特征，采用粗到细的3D-2D特征匹配流程并结合PnP+RANSAC和基于3DGS的姿态优化，实现了无需额外参考图像、无需重训练即可对新场景泛化的高效视觉定位。实验表明，该方法在标准室内外数据集上超越了现有3DGS方法，并与主流视觉定位方法具有竞争力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-Disentangling-the-Factors-of-Convergence-between-Brains-and-Computer-Vision-Modelspdf"><a href="#96-Disentangling-the-Factors-of-Convergence-between-Brains-and-Computer-Vision-Modelspdf" class="headerlink" title="96. Disentangling the Factors of Convergence between Brains and Computer Vision Modelspdf"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Disentangling_the_Factors_of_Convergence_between_Brains_and_Computer_Vision_Models.pdf">Disentangling the Factors of Convergence between Brains and Computer Vision Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Meta AI</span></p>
<p>本论文系统性地分析了自监督视觉Transformer模型（DINOv3）在架构、训练量和数据类型三方面对其与人脑视觉表征收敛性的影响。通过与人类fMRI和MEG数据比较，并采用编码分数、空间分数和时间分数等多维度指标，作者发现更大规模、训练量更高且更具人类中心的数据能显著提升模型与脑表征的相似性，且该收敛过程与大脑皮层结构和功能属性密切相关。结论指出，模型架构、训练和数据共同决定AI与人脑视觉表征的收敛性，为理解生物视觉的组织原则提供了新思路。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-Follow-My-Hold-Hand-Object-Interaction-Reconstruction-through-Geometric-Guidancepdf"><a href="#97-Follow-My-Hold-Hand-Object-Interaction-Reconstruction-through-Geometric-Guidancepdf" class="headerlink" title="97. Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidancepdf"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Follow_My_Hold__Hand-Object_Interaction_Reconstruction_through_Geometric_Guidance.pdf">Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Max Planck Institute for Informatics</span></p>
<p>该论文提出FollowMyHold方法，通过多模态几何引导在单张RGB图像下实现手持物体的高质量3D重建，采用优化环路设计引导预训练的3D扩散模型。核心流程包括多模型输出统一对齐、阶段式优化（先手再物体最后联合）、并在扩散采样过程中引入2D像素对齐损失和3D物理约束（交集与接触），显著提升了在遮挡和真实场景下的重建稳定性和准确性。实验表明，FollowMyHold在多数据集上重建准确率和鲁棒性均优于现有生成方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-Explain-and-Monitor-Deep-Learning-Models-for-Computer-Vision-using-Obz-AIpdf"><a href="#98-Explain-and-Monitor-Deep-Learning-Models-for-Computer-Vision-using-Obz-AIpdf" class="headerlink" title="98. Explain and Monitor Deep Learning Models for Computer Vision using Obz AIpdf"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Explain_and_Monitor_Deep_Learning_Models_for_Computer_Vision_using_Obz_AI.pdf">Explain and Monitor Deep Learning Models for Computer Vision using Obz AI</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Informatics, University of Warsaw</span></p>
<p>本文提出Obz AI，一套面向计算机视觉系统的集成化可解释性与监测软件平台，包含PyTorch库、后端数据库和前端仪表盘，支持XAI方法、异常检测及实时监控。实验表明Obz AI可提升深度学习视觉模型的可解释性和可观测性，便于实际部署和审计。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-SEAM-Semantically-Equivalent-Across-Modalities-Benchmark-for-Vision-Language-Modelspdf"><a href="#99-SEAM-Semantically-Equivalent-Across-Modalities-Benchmark-for-Vision-Language-Modelspdf" class="headerlink" title="99. SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Modelspdf"></a>99. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SEAM__Semantically_Equivalent_Across_Modalities_Benchmark_for_Vision-Language_Models.pdf">SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Toronto</span></p>
<p>SEAM提出了一个评价视觉-语言模型（VLM）在视觉与文本两种模态下推理一致性的基准，通过四个具有标准化符号和视觉表征的领域（国际象棋、化学、音乐、图论）构建语义等价的输入。实验评测21个主流VLM，发现所有模型在不同模态间存在显著性能不均衡，视觉推理普遍落后于文本推理，且跨模态答案一致性较低，主要受文本标记化和视觉感知误差影响。结论：当前VLM在处理语义等价的多模态信息时仍存在明显不足，SEAM为推动更强跨模态推理能力提供了标准和工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-Scene-Aware-Vectorized-Memory-Multi-Agent-Framework-with-Cross-Modal-Differentiated-Quantization-VLMs-for-Visually-Impaired-Assistancepdf"><a href="#100-Scene-Aware-Vectorized-Memory-Multi-Agent-Framework-with-Cross-Modal-Differentiated-Quantization-VLMs-for-Visually-Impaired-Assistancepdf" class="headerlink" title="100. Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistancepdf"></a>100. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scene-Aware_Vectorized_Memory_Multi-Agent_Framework_with_Cross-Modal_Differentiated_Quantization_VLM.pdf">Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>本文提出一种创新的跨模态差异化量化框架（CMDQ），针对视觉-语言模型（VLM）中视觉编码器和跨模态处理模块的不同量化敏感性进行模块化量化压缩，并结合高效解量化和存储优化技术，使19B参数模型在单块消费级GPU上运行，将内存需求从38GB降至16GB，仅损失约2%性能。此外，设计了场景感知向量化记忆多智能体系统，实现跨视角理解和历史场景知识集成，通过多智能体协作提供实时场景分析、文本识别与语音流式反馈。结论显示，该系统既提升了模型部署高效性，又为视障用户提供了及时、全面的环境感知辅助。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="101-Incorporating-Pre-trained-Diffusion-Models-in-Solving-the-Schrodinger-Bridge-Problempdf"><a href="#101-Incorporating-Pre-trained-Diffusion-Models-in-Solving-the-Schrodinger-Bridge-Problempdf" class="headerlink" title="101. Incorporating Pre-trained Diffusion Models in Solving the Schrödinger Bridge Problempdf"></a>101. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Incorporating_Pre-trained_Diffusion_Models_in_Solving_the_Schr%C3%B6dinger_Bridge_Problem.pdf">Incorporating Pre-trained Diffusion Models in Solving the Schrödinger Bridge Problem</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了三种重参数化技术（IPMM、IPTM、IPFM），统一了Schrödinger Bridge (SB) 问题与Score-based Generative Models (SGMs, 即扩散模型) 的训练目标，并提出将预训练扩散模型用于SB模型初始化，从而大幅提升SB模型训练的效率、稳定性和生成质量。实验结果表明，该方法在高维高分辨率图像生成、无配对图像翻译等任务上，显著优于已有SB及扩散模型方法，并有效加速收敛与提升生成表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="102-Annotation-Free-Open-Vocabulary-Segmentation-for-Remote-Sensing-Imagespdf"><a href="#102-Annotation-Free-Open-Vocabulary-Segmentation-for-Remote-Sensing-Imagespdf" class="headerlink" title="102. Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Imagespdf"></a>102. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Annotation-Free_Open-Vocabulary_Segmentation_for_Remote-Sensing_Images.pdf">Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong University</span></p>
<p>本论文提出SegEarth-OV，首个面向遥感图像的无标注开放词汇分割框架。方法核心包括SimFeatUp通用特征上采样模块（基于参数化JBU和内容保持网络，无需任务特定训练）、Global Bias Alleviation（全局偏置消除操作提升像素级区分力）、以及AlignEarth跨模态知识蒸馏策略（实现光学VLM到SAR编码器的高效语义迁移）。实验表明，SegEarth-OV在17个遥感任务和8个SAR数据集上显著超越现有SOTA方法，验证了其在多模态遥感无标注分割的有效性和通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="103-AQ-PCDSys-An-Adaptive-Quantized-Planetary-Crater-Detection-System-for-Autonomous-Space-Explorationpdf"><a href="#103-AQ-PCDSys-An-Adaptive-Quantized-Planetary-Crater-Detection-System-for-Autonomous-Space-Explorationpdf" class="headerlink" title="103. AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Explorationpdf"></a>103. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AQ-PCDSys__An_Adaptive_Quantized_Planetary_Crater_Detection_System_for_Autonomous_Space_Exploration.pdf">AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Manipal University Jaipur</span></p>
<p>本文提出了AQ-PCDSys系统，通过结合量化感知训练（QAT）的量化神经网络（QNN）、自适应多传感器融合（AMF）模块和多尺度检测头，实现了对行星陨石坑的高效、实时检测。该系统显著优化了模型体积和推理速度，并通过自适应权重机制动态融合光学影像和DEM数据，提升了复杂环境下的检测鲁棒性。结论：AQ-PCDSys为受限硬件下的自主空间探测提供了一种高效、准确且可部署的视觉感知方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="104-Propose-and-Rectify-A-Forensics-Driven-MLLM-Framework-for-Image-Manipulation-Localizationpdf"><a href="#104-Propose-and-Rectify-A-Forensics-Driven-MLLM-Framework-for-Image-Manipulation-Localizationpdf" class="headerlink" title="104. Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localizationpdf"></a>104. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Propose_and_Rectify__A_Forensics-Driven_MLLM_Framework_for_Image_Manipulation_Localization.pdf">Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">City University of Hong Kong</span></p>
<p>本文提出了一种面向图像篡改检测与定位的Propose-Rectify框架，结合多模态大语言模型（MLLM）高层语义推理与多尺度低层次取证特征分析。方法包括利用经过取证适配的LLaVA模型生成初步可疑区域，再通过多特征提取器和取证校正模块逐步精细化检测与分割边界，并引入增强分割模块融合取证线索以提升定位精度。实验表明该方法在多个公开数据集上实现了领先的检测与定位精度，具备优异的稳健性与泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="105-SAIL-Recon-Large-SfM-by-Augmenting-Scene-Regression-with-Localizationpdf"><a href="#105-SAIL-Recon-Large-SfM-by-Augmenting-Scene-Regression-with-Localizationpdf" class="headerlink" title="105. SAIL-Recon: Large SfM by Augmenting Scene Regression with Localizationpdf"></a>105. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SAIL-Recon__Large_SfM_by_Augmenting_Scene_Regression_with_Localization.pdf">SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>该论文提出SAIL-Recon，一种通过在Transformer架构下增加视觉定位能力，实现大规模结构光束法（SfM）的端到端场景回归方法。方法流程为：先从一小部分锚点图片提取神经场景表征，再用此表征对所有输入图片进行定位与重建，实现了高效处理千级图片数据集，同时提升了相机位姿估计与新视角合成的精度。实验表明，SAIL-Recon在TUM-RGBD、CO3Dv2、Tanks &amp; Temples等基准上实现了SOTA效果，超越传统与其他深度学习SfM方法，能够在几分钟内完成大规模三维重建与定位。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="106-Generative-Feature-Imputing-—-A-Technique-for-Error-resilient-Semantic-Communicationpdf"><a href="#106-Generative-Feature-Imputing-—-A-Technique-for-Error-resilient-Semantic-Communicationpdf" class="headerlink" title="106. Generative Feature Imputing — A Technique for Error-resilient Semantic Communicationpdf"></a>106. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Generative_Feature_Imputing_-_A_Technique_for_Error-resilient_Semantic_Communication.pdf">Generative Feature Imputing — A Technique for Error-resilient Semantic Communication</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Hong Kong</span></p>
<p>本论文提出了一种用于数字语义通信系统的生成式特征补全新框架，包含空间误差集中分组、扩散模型驱动的特征生成、以及基于语义重要性的功率分配。实验表明，该方法在图像传输中显著提升了抗误差鲁棒性和语义准确性，优于DJSCC和传统方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="107-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimationpdf"><a href="#107-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimationpdf" class="headerlink" title="107. Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimationpdf"></a>107. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Gaze_into_the_Heart__A_Multi-View_Video_Dataset_for_rPPG_and_Health_Biomarkers_Estimation.pdf">Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sber AI Lab</span></p>
<p>本文提出了MCD-rPPG多视角视频大规模数据集，用于远程光电容积描记（rPPG）和健康生物标志物估计，涵盖600名受试者在不同状态下的多摄像头同步视频及13项健康指标，并配套100 Hz PPG信号。基于此数据集，作者设计了高效的多任务神经网络，实现了从人脸视频中实时推断脉搏波与健康参数，并在跨数据集评测中表现出色。结论认为公开的数据集和模型将加速基于AI的远程健康监测领域进展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="108-ISALux-Illumination-and-Semantics-Aware-Transformer-Employing-Mixture-of-Experts-for-Low-Light-Image-Enhancementpdf"><a href="#108-ISALux-Illumination-and-Semantics-Aware-Transformer-Employing-Mixture-of-Experts-for-Low-Light-Image-Enhancementpdf" class="headerlink" title="108. ISALux: Illumination and Semantics-Aware Transformer Employing Mixture of Experts for Low Light Image Enhancementpdf"></a>108. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ISALux__Illumination_and_Segmentation_Aware_Transformer_Employing_Mixture_of_Experts_for_Low_Light_I.pdf">ISALux: Illumination and Semantics-Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Manchester</span></p>
<p>该论文提出了ISALux，一种基于视觉Transformer的低光照图像增强方法，通过融合照明先验和语义分割先验，实现对不同区域结构和亮度的自适应增强。核心创新包括引入HISA-MSA模块进行照明与语义特征融合自注意力，以及采用MoE专家混合机制提升上下文特征表达能力，并通过LoRA适配防止过拟合。实验结果显示，ISALux在多个基准数据集上优于现有方法，且参数和推理效率均具备优势，适合实际应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="109-Edge-Enhanced-Vision-Transformer-Framework-for-Accurate-AI-Generated-Image-Detectionpdf"><a href="#109-Edge-Enhanced-Vision-Transformer-Framework-for-Accurate-AI-Generated-Image-Detectionpdf" class="headerlink" title="109. Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detectionpdf"></a>109. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Edge-Enhanced_Vision_Transformer_Framework_for_Accurate_AI-Generated_Image_Detection.pdf">Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detection</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Uttara University</span></p>
<p>本文提出了一种结合细粒度边缘结构分析与ViT（Vision Transformer）全局特征提取的AI生成图像检测框架。方法包括对ViT进行多数据集微调，结合基于边缘差异方差的新型后处理模块，提升对AI生成图像与真实图像的区分能力；实验在CIFAKE、Artistic及自建数据集上均取得优于主流模型的检测准确率（CIFAKE上准确率97.75%、F1分数97.77%），验证了方法在不同领域和分辨率下的鲁棒性与高效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="110-Diffusion-Based-Data-Augmentation-for-Medical-Image-Segmentationpdf"><a href="#110-Diffusion-Based-Data-Augmentation-for-Medical-Image-Segmentationpdf" class="headerlink" title="110. Diffusion-Based Data Augmentation for Medical Image Segmentationpdf"></a>110. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Diffusion-Based_Data_Augmentation_for_Medical_Image_Segmentation.pdf">Diffusion-Based Data Augmentation for Medical Image Segmentation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science and Engineering, Beihang University</span></p>
<p>本文提出DiffAug框架，结合文本引导的扩散模型生成与分割验证，用于医学图像中的异常病灶数据增强。通过医学术语文本提示和空间掩码在正常图像中扩散生成病灶，并通过高效的单步推理分割网络自动验证空间准确性，仅保留高质量合成样本训练分割网络。实验在CVC-ClinicDB、Kvasir-SEG和REFUGE2等医学数据集上验证，DiffAug显著提升分割性能（Dice提升8-10%），并将小病灶漏检率降低至28%，兼顾生成质量与计算效率，为医学影像稀缺异常数据增强提供实用方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="111-SCOUT-Semi-supervised-Camouflaged-Object-Detection-by-Utilizing-Text-and-Adaptive-Data-Selectionpdf"><a href="#111-SCOUT-Semi-supervised-Camouflaged-Object-Detection-by-Utilizing-Text-and-Adaptive-Data-Selectionpdf" class="headerlink" title="111. SCOUT: Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selectionpdf"></a>111. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SCOUT__Semi-supervised_Camouflaged_Object_Detection_by_Utilizing_Text_and_Adaptive_Data_Selection.pdf">SCOUT: Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selection</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiamen University</span></p>
<p>该论文提出了SCOUT，一个用于伪装物体检测的半监督学习模型，结合了自适应数据增强与选择（ADAS）模块和文本融合模块（TFM），通过主动筛选有价值的未标注数据并利用图像文本交互提升模型对伪装物体的表征能力。实验结果表明，SCOUT在多个主流数据集上显著优于现有半监督方法，有效降低了标注成本并提升了检测性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="112-UniSino-Physics-Driven-Foundational-Model-for-Universal-CT-Sinogram-Standardizationpdf"><a href="#112-UniSino-Physics-Driven-Foundational-Model-for-Universal-CT-Sinogram-Standardizationpdf" class="headerlink" title="112. UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardizationpdf"></a>112. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/UniSino__Physics-Driven_Foundational_Model_for_Universal_CT_Sinogram_Standardization.pdf">UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanchang University</span></p>
<p>该论文提出了UniSino，一个面向CT投影域（sinogram）的物理驱动基础模型，通过SinoVAE感知压缩和物理约束的损失函数，以及条件扩散模型LRD，实现多类型伪影和噪声的标准化处理。该方法显著提升了CT重建图像的质量，并在多个数据集和任务下展现出卓越的泛化能力，为医学影像预处理提供了统一、高效的解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="113-Towards-Trustworthy-Breast-Tumor-Segmentation-in-Ultrasound-using-Monte-Carlo-Dropout-and-Deep-Ensembles-for-Epistemic-Uncertainty-Estimationpdf"><a href="#113-Towards-Trustworthy-Breast-Tumor-Segmentation-in-Ultrasound-using-Monte-Carlo-Dropout-and-Deep-Ensembles-for-Epistemic-Uncertainty-Estimationpdf" class="headerlink" title="113. Towards Trustworthy Breast Tumor Segmentation in Ultrasound using Monte Carlo Dropout and Deep Ensembles for Epistemic Uncertainty Estimationpdf"></a>113. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Towards_Trustworthy_Breast_Tumor_Segmentation_in_Ultrasound_using_Monte_Carlo_Dropout_and_Deep_Ensem.pdf">Towards Trustworthy Breast Tumor Segmentation in Ultrasound using Monte Carlo Dropout and Deep Ensembles for Epistemic Uncertainty Estimation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kwame Nkrumah University of Science and Technology</span></p>
<p>该论文提出了基于改进型Residual Encoder U-Net的乳腺超声肿瘤自动分割方法，并重点利用Monte Carlo dropout和深度集成（Deep Ensembles）对模型的认知不确定性进行量化，结合多种不确定性估计方法提升分割的可靠性。通过清理和去重BUSI数据集，并在分布内和分布外数据上进行充分验证，结果在Breast-Lesion-USG数据集上实现了当前最优分割准确率，同时不确定性估计能够有效指示低置信度区域，强调了数据准备和不确定性建模在临床部署中的重要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="114-CEIDM-A-Controlled-Entity-and-Interaction-Diffusion-Model-for-Enhanced-Text-to-Image-Generationpdf"><a href="#114-CEIDM-A-Controlled-Entity-and-Interaction-Diffusion-Model-for-Enhanced-Text-to-Image-Generationpdf" class="headerlink" title="114. CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generationpdf"></a>114. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CEIDM__A_Controlled_Entity_and_Interaction_Diffusion_Model_for_Enhanced_Text-to-Image_Generation.pdf">CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">College of Computer Science and Technology, National University of Defense Technology</span></p>
<p>本文提出CEIDM方法，通过在扩散模型中引入显式与隐式交互关系挖掘（基于大语言模型推理）、交互动作聚类与双向偏移，以及实体控制网络，实现了对文本到图像生成过程中实体及其复杂交互的精细双重控制。实验结果显示，CEIDM在图像整体质量和实体交互检测准确率方面均优于现有主流方法，尤其在复杂场景合成和细节处理上表现出色。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="115-SUPERGEN-An-Efficient-Ultra-high-resolution-Video-Generation-System-with-Sketching-and-Tilingpdf"><a href="#115-SUPERGEN-An-Efficient-Ultra-high-resolution-Video-Generation-System-with-Sketching-and-Tilingpdf" class="headerlink" title="115. SUPERGEN: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tilingpdf"></a>115. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SuperGen__An_Efficient_Ultra-high-resolution_Video_Generation_System_with_Sketching_and_Tiling.pdf">SUPERGEN: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Rice University</span></p>
<p>SUPERGEN提出了一种高效、无需额外训练的超高分辨率视频生成系统，采用两阶段的sketch-tile协同生成流程：首先通过预训练扩散模型生成低分辨率视频草图，随后利用分块（tiling）和智能平衡的并行加速对各区域细致优化，并结合区域自适应缓存机制减少冗余计算。实验显示，该系统在多种主流基线和分辨率下实现了最高6.2倍的加速，同时保持了生成质量，极大提升了超高分辨率视频生成的实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="116-DroneKey-Drone-3D-Pose-Estimation-in-Image-Sequences-using-Gated-Key-representation-and-Pose-adaptive-Learningpdf"><a href="#116-DroneKey-Drone-3D-Pose-Estimation-in-Image-Sequences-using-Gated-Key-representation-and-Pose-adaptive-Learningpdf" class="headerlink" title="116. DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learningpdf"></a>116. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DroneKey__Drone_3D_Pose_Estimation_in_Image_Sequences_using_Gated_Key-representation_and_Pose-adapti.pdf">DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chonnam National University</span></p>
<p>本文提出DroneKey框架，专为无人机3D姿态估计设计，结合Transformer结构进行2D关键点检测，并通过门控加权融合中间和紧凑特征表示，提升关键点检测的准确性和顺序判别能力。为增强极端姿态下的稳定性，提出了姿态自适应Mahalanobis损失函数，并构建了公开的合成无人机2D关键点和3D姿态数据集。实验结果显示，DroneKey在关键点检测（OKS AP 99.68%）和3D姿态估计（MAE-angle 10.62°，RMSE 0.221m，MAE-absolute 0.076m）方面显著优于现有方法，并能实时运行（44 FPS）。该方法适用于实时反无人机系统。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="117-Few-shot-Human-Action-Anomaly-Detection-via-a-Unified-Contrastive-Learning-Frameworkpdf"><a href="#117-Few-shot-Human-Action-Anomaly-Detection-via-a-Unified-Contrastive-Learning-Frameworkpdf" class="headerlink" title="117. Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Frameworkpdf"></a>117. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Few-shot_Human_Action_Anomaly_Detection_via_a_Unified_Contrastive_Learning_Framework.pdf">Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Toyama</span></p>
<p>该论文提出了一个统一的对比学习框架用于少样本人体动作异常检测，通过残差图卷积网络(Res-GCN)和离散余弦变换(DCT)进行动作编码，并结合基于扩散模型的生成式运动增强策略(HumanMAC)以提升类别泛化和鲁棒性。实验结果表明，该方法无需类别特定的重训练，在HumanAct12数据集的已见和未见类别上均取得了最优性能，提升了模型的实用性和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="118-Instant-Preference-Alignment-for-Text-to-Image-Diffusion-Modelspdf"><a href="#118-Instant-Preference-Alignment-for-Text-to-Image-Diffusion-Modelspdf" class="headerlink" title="118. Instant Preference Alignment for Text-to-Image Diffusion Modelspdf"></a>118. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Instant_Preference_Alignment_for_Text-to-Image_Diffusion_Models.pdf">Instant Preference Alignment for Text-to-Image Diffusion Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">New Laboratory of Pattern Recognition, CASIA</span></p>
<p>本文提出了一种无需训练、基于多模态大语言模型（MLLM）先验的即时偏好对齐文本到图像（T2I）生成框架，将任务分为偏好理解和偏好引导生成两部分。方法通过MLLM自动从参考图像中提取多维全局偏好关键词并丰富文本提示，结合全局关键词引导与局部区域感知的交叉注意力调控，实现了对扩散模型的实时、细粒度偏好控制，无须额外微调。实验表明，该方法在多个数据集上均优于现有方案，并支持多轮交互式图像精细化生成，极大提升了偏好对齐能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="119-CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolutionpdf"><a href="#119-CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolutionpdf" class="headerlink" title="119. CATformer: Contrastive Adversarial Transformer for Image Super-Resolutionpdf"></a>119. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CATformer__Contrastive_Adversarial_Transformer_for_Image_Super-Resolution.pdf">CATformer: Contrastive Adversarial Transformer for Image Super-Resolution</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Duke University</span></p>
<p>本文提出了CATformer，一种对比对抗变换器网络，将扩散模型启发的特征精炼、对抗性训练和对比学习结合，用于图像超分辨率。CATformer采用双分支结构：主分支为扩散式Transformer逐步去噪特征，辅助分支通过噪声增强提升鲁棒性，最后经特征融合和深层残差密集块解码重建高质量图像。实验表明，CATformer在感知质量、结构相似性和推理效率上优于现有SOTA方法，但在小数据集PSNR略低，凸显了感知质量与像素精度的权衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="120-Robustness-Feature-Adapter-for-Efficient-Adversarial-Trainingpdf"><a href="#120-Robustness-Feature-Adapter-for-Efficient-Adversarial-Trainingpdf" class="headerlink" title="120. Robustness Feature Adapter for Efficient Adversarial Trainingpdf"></a>120. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Robustness_Feature_Adapter_for_Efficient_Adversarial_Training.pdf">Robustness Feature Adapter for Efficient Adversarial Training</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Dongguan University of Technology</span></p>
<p>本文提出了一种新的鲁棒性特征适配器（RFA），在特征空间中高效进行对抗训练（AT），通过引入外部适配器模块实现参数高效微调，有效缓解了大模型AT的高计算开销和鲁棒过拟合问题。实验表明，该方法大幅提升了模型对未知攻击的泛化鲁棒性和训练效率，并可灵活应用于主流视觉模型（如CNN和ViT）和下游多媒体任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="121-Citizen-Centered-Climate-Intelligence-Operationalizing-Open-Tree-Data-for-Urban-Cooling-and-Eco-Routing-in-Indian-Citiespdf"><a href="#121-Citizen-Centered-Climate-Intelligence-Operationalizing-Open-Tree-Data-for-Urban-Cooling-and-Eco-Routing-in-Indian-Citiespdf" class="headerlink" title="121. Citizen Centered Climate Intelligence: Operationalizing Open Tree Data for Urban Cooling and Eco-Routing in Indian Citiespdf"></a>121. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Citizen_Centered_Climate_Intelligence__Operationalizing_Open_Tree_Data_for_Urban_Cooling_and_Eco-Rou.pdf">Citizen Centered Climate Intelligence: Operationalizing Open Tree Data for Urban Cooling and Eco-Routing in Indian Cities</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Institute of Technology Tiruchirappalli</span></p>
<p>该论文提出了一套以城市居民为中心的气候智能框架，包括三大模块：手机端AI分割辅助的树木测量工具、基于卫星地表温度的局地降温指标建模，以及融合环境质量分数的生态路径规划引擎。该方法实现了公民参与式数据采集、环境分析和生态出行建议，显著提升了城市气候适应与公民赋权。实验展示了该系统在印地城市的应用潜力，结论认为此框架可复制推广，有效促进城市绿色治理和可持续交通。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="122-Finding-Outliers-in-a-Haystack-Anomaly-Detection-for-Large-Pointcloud-Scenespdf"><a href="#122-Finding-Outliers-in-a-Haystack-Anomaly-Detection-for-Large-Pointcloud-Scenespdf" class="headerlink" title="122. Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenespdf"></a>122. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Finding_Outliers_in_a_Haystack__Anomaly_Detection_for_Large_Pointcloud_Scenes.pdf">Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenes</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Adelaide</span></p>
<p>本文提出了一种新颖的基于重建的开集分割方法，结合了Mamba架构对大规模点云场景的长程依赖建模能力。通过仅用已知类别训练重建器以生成场景的“默认上下文”，再利用重建与原始点云的差异进行异常检测，显著提升了在KITTI和ECLAIR等数据集上的异常检测性能，并可增强现有方法的表现。结论表明，该方法在异常检测任务中优于主流方法，但在已知对象分割上仍有提升空间。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="123-Improving-Interpretability-in-Alzheimer’s-Prediction-via-Joint-Learning-of-ADAS-Cog-Scorespdf"><a href="#123-Improving-Interpretability-in-Alzheimer’s-Prediction-via-Joint-Learning-of-ADAS-Cog-Scorespdf" class="headerlink" title="123. Improving Interpretability in Alzheimer’s Prediction via Joint Learning of ADAS-Cog Scorespdf"></a>123. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Improving_Interpretability_in_Alzheimer's_Prediction_via_Joint_Learning_of_ADAS-Cog_Scores.pdf">Improving Interpretability in Alzheimer’s Prediction via Joint Learning of ADAS-Cog Scores</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universiti Brunei Darussalam</span></p>
<p>该论文提出了一种多任务学习（MTL）框架，利用Vision Transformer（ViT）和Swin Transformer对MRI影像和纵向临床评分（ADAS-Cog及其13个子项）进行特征提取和融合，联合预测阿尔茨海默症未来的全球评分和各子评分。结论表明联合子项学习提升了全球评分预测的准确性，并揭示了部分子项（如Q1、Q4、Q8）对总评分的主导作用，但也暴露出模型对临床特征的过度依赖和MRI特征利用不足，未来需优化多模态融合与损失权重以提升模型稳定性与解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="124-JCo-MVTON-Jointly-Controllable-Multi-Modal-Diffusion-Transformer-for-Mask-Free-Virtual-Try-onpdf"><a href="#124-JCo-MVTON-Jointly-Controllable-Multi-Modal-Diffusion-Transformer-for-Mask-Free-Virtual-Try-onpdf" class="headerlink" title="124. JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-onpdf"></a>124. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/JCo-MVTON__Jointly_Controllable_Multi-Modal_Diffusion_Transformer_for_Mask-Free_Virtual_Try-on.pdf">JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-on</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">DAMO Academy, Alibaba Group</span></p>
<p>该论文提出了JCo-MVTON，一种基于多模态扩散变换器（MM-DiT）的无掩码虚拟试衣框架，通过多条件特征注入（如参考图像和服装图像）与自注意力机制结合，并引入条件分支掩码与拼接位置编码，提升衣物与人体融合的真实性和一致性。实验结果显示，JCo-MVTON在VITON-HD和DressCode等公开基准和真实场景下均超越了现有学术和商业系统，实现了更高的图像保真度与细节一致性，验证了模型的实际应用价值和强泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="125-A-Weighted-Vision-Transformer-Based-Multi-Task-Learning-Framework-for-Predicting-ADAS-Cog-Scorespdf"><a href="#125-A-Weighted-Vision-Transformer-Based-Multi-Task-Learning-Framework-for-Predicting-ADAS-Cog-Scorespdf" class="headerlink" title="125. A Weighted Vision Transformer-Based Multi-Task Learning Framework for Predicting ADAS-Cog Scorespdf"></a>125. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Weighted_Vision_Transformer-Based_Multi-Task_Learning_Framework_for_Predicting_ADAS-Cog_Scores.pdf">A Weighted Vision Transformer-Based Multi-Task Learning Framework for Predicting ADAS-Cog Scores</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universiti Brunei Darussalam</span></p>
<p>本文提出了一种加权Vision Transformer (ViT)为基础的多任务学习框架，用于联合预测阿尔茨海默病ADAS-Cog量表的13个认知分项得分与未来总分，模型以MRI影像为输入，通过对关键子项（Q1, Q4, Q8）赋予更高损失权重，实现对具临床意义认知域的重点关注。实验结果表明：强权重策略在MCI组提升预测相关性和准确性，适度权重更适合CN组，均优于均匀权重方案，显示加权策略可提升模型泛化性和解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="126-TinyGiantVLM-A-Lightweight-Vision-Language-Architecture-for-Spatial-Reasoning-under-Resource-Constraintspdf"><a href="#126-TinyGiantVLM-A-Lightweight-Vision-Language-Architecture-for-Spatial-Reasoning-under-Resource-Constraintspdf" class="headerlink" title="126. TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraintspdf"></a>126. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TinyGiantVLM__A_Lightweight_Vision-Language_Architecture_for_Spatial_Reasoning_under_Resource_Constr.pdf">TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science, VNU-HCM</span></p>
<p>本文提出TinyGiantVLM，一种面向工业仓储环境空间推理的轻量级视觉-语言模型架构。方法采用双分支设计，对RGB和深度信息的全局及区域特征进行提取，并通过跨注意力融合和Mixture-of-Experts（MoE）模块实现多模态融合及任务特定推理，采用两阶段训练（自由回答与规范化回答）提升空间推理和输出准确率。结论显示，该模型仅用64M参数在AI City Challenge 2025空间智能赛道取得前五名，兼顾推理能力和计算效率，适合资源受限场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="127-Beyond-Play-Pause-Untwist-–-Turning-GPT-4o’s-Spatial-Weakness-into-a-Strength-for-In-Depth-Interactive-Video-Learningpdf"><a href="#127-Beyond-Play-Pause-Untwist-–-Turning-GPT-4o’s-Spatial-Weakness-into-a-Strength-for-In-Depth-Interactive-Video-Learningpdf" class="headerlink" title="127. Beyond Play &amp; Pause: Untwist – Turning GPT-4o’s Spatial Weakness into a Strength for In-Depth Interactive Video Learningpdf"></a>127. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Beyond_Play_and_Pause__Turning_GPT-4o_Spatial_Weakness_into_a_Strength_for_In-Depth_Interactive_Vide.pdf">Beyond Play &amp; Pause: Untwist – Turning GPT-4o’s Spatial Weakness into a Strength for In-Depth Interactive Video Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Clemson University</span></p>
<p>该论文提出了Untwist系统，通过结合GPT-4o API与计算机视觉技术，实现视频内容的交互式学习。用户可对整个视频或指定区域提出问题，系统以标注框代替坐标传递，显著提升了GPT-4o在视频区域定位和理解的准确性。实验证明标注图像显著优于坐标输入，系统能够实时生成多模态、上下文相关的反馈，增强视频学习的互动性和理解深度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="128-Towards-Safeguarding-LLM-Fine-tuning-APIs-against-Cipher-Attackspdf"><a href="#128-Towards-Safeguarding-LLM-Fine-tuning-APIs-against-Cipher-Attackspdf" class="headerlink" title="128. Towards Safeguarding LLM Fine-tuning APIs against Cipher Attackspdf"></a>128. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Towards_Safeguarding_LLM_Fine-tuning_APIs_against_Cipher_Attacks.pdf">Towards Safeguarding LLM Fine-tuning APIs against Cipher Attacks</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent</span></p>
<p>本文提出了Cipher Fine-tuning Robustness Benchmark（CIFR），用于系统性评估LLM微调API在面对利用隐写编码（cipher-based）攻击时的安全防御能力，并提出了一种基于模型内部激活的线性探针监控方法。实验证明，该探针监控技术对多种已知和未知加密变体具有99%以上检测准确率，且泛化能力优于现有主流监控方法，能够有效阻止加密内容规避安全机制的攻击。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="129-Mind-the-Gap-Time-of-Check-to-Time-of-Use-Vulnerabilities-in-LLM-Enabled-Agentspdf"><a href="#129-Mind-the-Gap-Time-of-Check-to-Time-of-Use-Vulnerabilities-in-LLM-Enabled-Agentspdf" class="headerlink" title="129. Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agentspdf"></a>129. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Mind_the_Gap__Time-of-Check_to_Time-of-Use_Vulnerabilities_in_LLM-Enabled_Agents.pdf">Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Oregon State University</span></p>
<p>该论文首次系统性研究了大语言模型(LLM)驱动的智能体在多步任务执行中存在的时间检查到使用(TOCTOU)安全漏洞，提出了TOCTOU-Bench基准和三种防御机制：Prompt Rewriting、State Integrity Monitoring和Tool Fusing，并在基准上评估其有效性。实验结果显示，这些方法可协同将已执行轨迹中的TOCTOU漏洞率从12%降低到8%，并显著缩短攻击窗口，提升了LLM智能体的安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="130-Natural-Language-Satisfiability-Exploring-the-Problem-Distribution-and-Evaluating-Transformer-based-Language-Modelspdf"><a href="#130-Natural-Language-Satisfiability-Exploring-the-Problem-Distribution-and-Evaluating-Transformer-based-Language-Modelspdf" class="headerlink" title="130. Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Modelspdf"></a>130. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Natural_Language_Satisfiability__Exploring_the_Problem_Distribution_and_Evaluating_Transformer-based.pdf">Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Department of Computer Science, University of Manchester</span></p>
<p>本论文提出了针对不同复杂度自然语言片段的可满足性问题数据集，系统评估了Transformer类语言模型（TLMs，如T5-large与DeBERTa-v3-large）在逻辑推理任务中的表现。结果显示，TLMs在复杂度较低的语言片段上表现较好，但随着片段复杂度提升，模型准确率下降，且在分布外泛化能力有限，难以学习并推断底层推理规则，因此当前TLMs尚未可靠地学会自然语言逻辑推理所需的算法与规则。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="131-MoE-Beyond-Learning-Based-Expert-Activation-Prediction-on-Edge-Devicespdf"><a href="#131-MoE-Beyond-Learning-Based-Expert-Activation-Prediction-on-Edge-Devicespdf" class="headerlink" title="131. MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devicespdf"></a>131. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MoE-Beyond__Learning-Based_Expert_Activation_Prediction_on_Edge_Devices.pdf">MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Univ. of Pennsylvania</span></p>
<p>MoE-Beyond 提出了一种基于轻量级 transformer 的专家激活预测模型，将专家选择建模为多标签序列预测任务，通过在 6600 万条 DeepSeek-V2-Lite MoE 专家激活轨迹上训练，显著提升了边缘设备大规模 MoE 推理的缓存命中率。实验结果显示，在仅有 10% 专家可缓存的条件下，GPU 缓存命中率由 17% 提升至 72%，大幅优于现有启发式方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="132-Token-Homogenization-under-Positional-Biaspdf"><a href="#132-Token-Homogenization-under-Positional-Biaspdf" class="headerlink" title="132. Token Homogenization under Positional Biaspdf"></a>132. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Token_Homogenization_under_Positional_Bias.pdf">Token Homogenization under Positional Bias</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">International Laboratory of Bioinformatics, HSE University</span></p>
<p>本文提出并实证分析了大语言模型（LLMs）中 token homogenization（表示趋同）与 positional bias（位置偏置）之间的关系，使用一系列创新和现有的度量指标（如MAUVE分数、有效秩、最大可解释方差、Schatten范数和resultant length），在LLaMA-3、Gemma和Qwen等模型上，通过对数据集中关键信息词前置&#x2F;后置的实验，揭示了位置偏置会显著增强token表示的趋同效应。结果表明，位置偏置导致模型各层token表现更加同质化，为模型解释性、压缩和优化提供了参考。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="133-PowerChain-Automating-Distribution-Grid-Analysis-with-Agentic-AI-Workflowspdf"><a href="#133-PowerChain-Automating-Distribution-Grid-Analysis-with-Agentic-AI-Workflowspdf" class="headerlink" title="133. PowerChain: Automating Distribution Grid Analysis with Agentic AI Workflowspdf"></a>133. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PowerChain__Automating_Distribution_Grid_Analysis_with_Agentic_AI_Workflows.pdf">PowerChain: Automating Distribution Grid Analysis with Agentic AI Workflows</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Vermont</span></p>
<p>该论文提出了PowerChain系统，通过结合Agentic AI架构和大语言模型（LLM）函数调用，实现对电力分布网分析任务的自动化工作流生成。方法包括利用专家工作流-查询对、函数池描述符和动态上下文提示，递归生成并执行领域相关的分析流程，显著降低了对专家定制和LLM微调的需求。实验证明，PowerChain能在真实电力数据上产生专家级分析流程，提升小型电力公司对复杂网格分析的可达性和效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="134-Zero-shot-Multimodal-Document-Retrieval-via-Cross-modal-Question-Generationpdf"><a href="#134-Zero-shot-Multimodal-Document-Retrieval-via-Cross-modal-Question-Generationpdf" class="headerlink" title="134. Zero-shot Multimodal Document Retrieval via Cross-modal Question Generationpdf"></a>134. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Zero-shot_Multimodal_Document_Retrieval_via_Cross-modal_Question_Generation.pdf">Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Yonsei University</span></p>
<p>该论文提出PREMIR框架，利用多模态大语言模型(MLLM)从文档中生成跨模态预问题(preQs)，并通过细粒度token级别表示实现多模态文档的高效检索。实验显示，PREMIR在跨领域、闭域和多语言场景下均取得了最优性能，无需额外训练，显著提升了检索的泛化能力和质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="135-Linguistic-Neuron-Overlap-Patterns-to-Facilitate-Cross-lingual-Transfer-on-Low-resource-Languagespdf"><a href="#135-Linguistic-Neuron-Overlap-Patterns-to-Facilitate-Cross-lingual-Transfer-on-Low-resource-Languagespdf" class="headerlink" title="135. Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languagespdf"></a>135. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Linguistic_Neuron_Overlap_Patterns_to_Facilitate_Cross-lingual_Transfer_on_Low-resource_Languages.pdf">Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Information Science and Technology, Beijing Foreign Studies University</span></p>
<p>本文提出了BridgeX-ICL方法，通过分析大型语言模型（LLMs）中不同语言的神经元重叠模式，并利用MUSE双语词典构建神经元探测数据，结合HSIC度量选择最优桥接语言，提升低资源语言的跨语言零样本上下文学习（X-ICL）能力。实验表明，该方法在15对语言和两个跨语言任务上均显著提升了LLMs在低资源语言上的性能，平均提升约6%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="136-GRAID-Synthetic-Data-Generation-with-Geometric-Constraints-and-Multi-Agentic-Reflection-for-Harmful-Content-Detectionpdf"><a href="#136-GRAID-Synthetic-Data-Generation-with-Geometric-Constraints-and-Multi-Agentic-Reflection-for-Harmful-Content-Detectionpdf" class="headerlink" title="136. GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detectionpdf"></a>136. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GRAID__Synthetic_Data_Generation_with_Geometric_Constraints_and_Multi-Agentic_Reflection_for_Harmful.pdf">GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detection</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Capital One</span></p>
<p>论文提出GRAID数据增强管道，利用大型语言模型（LLM）进行两阶段数据合成：首先通过几何约束生成均衡分布的新样本，然后采用多代理反思机制提升风格多样性并发现极端案例。实验结果显示，GRAID在BeaverTails和WildGuard数据集上能显著提升有害文本分类模型的性能，增强模型对多样化和难例的检测能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="137-Limitations-of-refinement-methods-for-weak-to-strong-generalizationpdf"><a href="#137-Limitations-of-refinement-methods-for-weak-to-strong-generalizationpdf" class="headerlink" title="137. Limitations of refinement methods for weak to strong generalizationpdf"></a>137. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Limitations_of_refinement_methods_for_weak_to_strong_generalization.pdf">Limitations of refinement methods for weak to strong generalization</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Michigan</span></p>
<p>本文提出了一个通用的潜在概念转移框架，分析了弱训练和标签精炼在LLM超对齐（superalignment）下的弱到强泛化能力，理论证明这两类主流方法都会产生不可消除的偏差，无法一致收敛到目标函数。作者进一步给出一种基于潜在概念识别的理想化算法，能在参数可辨识条件下实现一致估计，但该算法尚不切实际。结论认为现有弱到强泛化方法存在理论缺陷，需探索兼具实用性和最优性的替代方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="138-KL-Regularised-Q-Learning-A-Token-level-Action-Value-perspective-on-Online-RLHFpdf"><a href="#138-KL-Regularised-Q-Learning-A-Token-level-Action-Value-perspective-on-Online-RLHFpdf" class="headerlink" title="138. KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHFpdf"></a>138. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/KL-Regularised_Q-Learning__A_Token-level_Action-Value_perspective_on_Online_RLHF.pdf">KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cambridge</span></p>
<p>本文提出KL正则化Q学习（KLQ），一种面向大语言模型基于人类反馈的在线强化学习（LM-RLHF）的动作价值方法。KLQ在理论上等价于一种变体的PPO，但其目标函数更简洁，能够在摘要和对话生成等任务上达到与PPO相当甚至更优的表现，尤其在LLM评审下胜率更高。结论显示，KLQ为LM-RLHF提供了新的理论视角和更优质输出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="139-Unveiling-the-Latent-Directions-of-Reflection-in-Large-Language-Modelspdf"><a href="#139-Unveiling-the-Latent-Directions-of-Reflection-in-Large-Language-Modelspdf" class="headerlink" title="139. Unveiling the Latent Directions of Reflection in Large Language Modelspdf"></a>139. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Unveiling_the_Latent_Directions_of_Reflection_in_Large_Language_Models.pdf">Unveiling the Latent Directions of Reflection in Large Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Taiwan University</span></p>
<p>该论文提出了一种基于激活操控（activation steering）的方法，通过构建反思行为的潜在方向向量，系统性地分析和控制大语言模型（LLM）中的反思机制。实验证明：反思行为可通过激活干预被增强或抑制，且抑制反思比激发更容易，并能发现新的反思触发指令，提升模型推理准确率；同时揭示了反思可控性带来的安全隐患与防御机会。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="140-ReFactX-Scalable-Reasoning-with-Reliable-Facts-via-Constrained-Generationpdf"><a href="#140-ReFactX-Scalable-Reasoning-with-Reliable-Facts-via-Constrained-Generationpdf" class="headerlink" title="140. ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generationpdf"></a>140. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ReFactX__Scalable_Reasoning_with_Reliable_Facts_via_Constrained_Generation.pdf">ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Milano-Bicocca</span></p>
<p>ReFactX提出了一种基于受限生成（constrained generation）和前缀树索引的通用方法，使LLMs能高效访问超大规模知识库（如800M Wikidata事实），无需外部检索器或管道，仅通过受控解码生成真实知识事实并嵌入推理流程。实验证明，该方法在问答任务中可大幅提升准确率和精度（提升达20个百分点，精度超90%），同时仅增加约1%推理延迟。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="141-Hierarchical-Contextual-Grounding-LVLM-Enhancing-Fine-Grained-Visual-Language-Understanding-with-Robust-Groundingpdf"><a href="#141-Hierarchical-Contextual-Grounding-LVLM-Enhancing-Fine-Grained-Visual-Language-Understanding-with-Robust-Groundingpdf" class="headerlink" title="141. Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Groundingpdf"></a>141. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Hierarchical_Contextual_Grounding_LVLM__Enhancing_Fine-Grained_Visual-Language_Understanding_with_Ro.pdf">Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Grounding</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhongkai University of Agriculture and Engineering</span></p>
<p>该论文提出了一种新型分层视觉语言大模型架构HCG-LVLM，通过模拟人类由粗到细的认知流程，结合全局语境感知层与细粒度局部定位层（含局部细节增强模块和语义一致性验证器），提升了视觉-语言对齐的准确性和鲁棒性。实验表明该方法在GQA、A-OKVQA、RefCOCO等细粒度VQA与指代表达理解任务上，显著优于Flamingo、BLIP-2和MiniGPT-4，有效减少了幻觉现象、提升了精确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="142-Robust-Diagram-Reasoning-A-Framework-for-Enhancing-LVLM-Performance-on-Visually-Perturbed-Scientific-Diagramspdf"><a href="#142-Robust-Diagram-Reasoning-A-Framework-for-Enhancing-LVLM-Performance-on-Visually-Perturbed-Scientific-Diagramspdf" class="headerlink" title="142. Robust Diagram Reasoning: A Framework for Enhancing LVLM Performance on Visually Perturbed Scientific Diagramspdf"></a>142. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Robust_Diagram_Reasoning__A_Framework_for_Enhancing_LVLM_Performance_on_Visually_Perturbed_Scientifi.pdf">Robust Diagram Reasoning: A Framework for Enhancing LVLM Performance on Visually Perturbed Scientific Diagrams</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Taiyuan University of Science and Technology</span></p>
<p>本文提出了Robust Diagram Reasoning (RDR) 框架，通过Adaptive Multi-View &amp; Consistency Verification (AMCV)机制，对科学图表生成多种视觉扰动版本，进行多视角并行推理，并通过自我纠错循环提高LVLM在视觉退化场景下的鲁棒性。实验表明，RDR显著提升了LVLM在受噪声、模糊、遮挡等干扰下的推理准确率与一致性，超越了主流开源及闭源大模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="143-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoningpdf"><a href="#143-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoningpdf" class="headerlink" title="143. Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoningpdf"></a>143. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Breaking_the_Exploration_Bottleneck__Rubric-Scaffolded_Reinforcement_Learning_for_General_LLM_Reason.pdf">Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了Rubric-Scaffolded Reinforcement Learning (RuscaRL) 框架，通过在大语言模型(LLM)强化学习训练中引入checklist风格的rubric作为显式探索支架与可验证奖励，打破RL在通用推理任务中的探索瓶颈。RuscaRL在rollout阶段提供分组区分和逐步衰减的rubric支架激发多样化高质量响应，同时以rubric为准则设计多维度奖励函数，促进模型有效学习。实验表明，RuscaRL在HealthBench等多个基准上大幅提升LLM推理能力，小模型Qwen-2.5-7B-Instruct得分提升至50.3，超越GPT-4.1，Qwen3-30B模型更超越OpenAI-o3，验证了方法的有效性与通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="144-Attention-Layers-Add-Into-Low-Dimensional-Residual-Subspacespdf"><a href="#144-Attention-Layers-Add-Into-Low-Dimensional-Residual-Subspacespdf" class="headerlink" title="144. Attention Layers Add Into Low-Dimensional Residual Subspacespdf"></a>144. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Attention_Layers_Add_Into_Low-Dimensional_Residual_Subspaces.pdf">Attention Layers Add Into Low-Dimensional Residual Subspaces</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Innovation Institute</span></p>
<p>该论文发现Transformer模型中的注意力输出被限制在低维子空间内，这种低秩结构导致稀疏字典学习（如稀疏自编码器）中的大量死特征。作者提出Active Subspace Initialization方法，将特征初始化到激活的低维子空间，并结合SparseAdam优化器，显著减少死特征（由87%降至1%），提升重构精度。该方法适用于大语言模型中的稀疏字典学习和稀疏替换模型，改善可解释性和训练效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="145-TextOnly-A-Unified-Function-Portal-for-Text-Related-Functions-on-Smartphonespdf"><a href="#145-TextOnly-A-Unified-Function-Portal-for-Text-Related-Functions-on-Smartphonespdf" class="headerlink" title="145. TextOnly: A Unified Function Portal for Text-Related Functions on Smartphonespdf"></a>145. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TextOnly__A_Unified_Function_Portal_for_Text-Related_Functions_on_Smartphones.pdf">TextOnly: A Unified Function Portal for Text-Related Functions on Smartphones</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出了TextOnly系统，通过统一的文本输入框整合并智能推荐智能手机上的文本相关功能，结合了大语言模型（LLM，GPT-3.5）与BERT模型，LLM提供通用知识和冷启动能力，BERT实现用户个性化与快速预测。实验表明，TextOnly能够有效提升文本相关功能的访问效率，用户满意度高，且系统准确率和推理速度可随使用持续提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="146-TriagerX-Dual-Transformers-for-Bug-Triaging-Tasks-with-Content-and-Interaction-Based-Rankingspdf"><a href="#146-TriagerX-Dual-Transformers-for-Bug-Triaging-Tasks-with-Content-and-Interaction-Based-Rankingspdf" class="headerlink" title="146. TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankingspdf"></a>146. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TriagerX__Dual_Transformers_for_Bug_Triaging_Tasks_with_Content_and_Interaction_Based_Rankings.pdf">TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Calgary</span></p>
<p>本文提出了TriagerX，一种结合内容和开发者交互的双Transformer架构，用于自动化软件缺陷（bug）分配。方法上，TriagerX通过内容驱动的双PLM（如DeBERTa与RoBERTa）融合多层语义特征，并引入基于历史开发者交互（如commit&#x2F;PR&#x2F;讨论）的交互排序，最后通过加权聚合提升推荐准确性。在多个工业与公开数据集上，TriagerX显著优于现有SOTA方法，Top-1与Top-3开发者推荐准确率提升超10%，在工业部署中成功助力实际团队高效分配任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="147-Beyond-Emotion-Recognition-A-Multi-Turn-Multimodal-Emotion-Understanding-and-Reasoning-Benchmarkpdf"><a href="#147-Beyond-Emotion-Recognition-A-Multi-Turn-Multimodal-Emotion-Understanding-and-Reasoning-Benchmarkpdf" class="headerlink" title="147. Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmarkpdf"></a>147. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Beyond_Emotion_Recognition__A_Multi-Turn_Multimodal_Emotion_Understanding_and_Reasoning_Benchmark.pdf">Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hefei University of Technology</span></p>
<p>本论文提出了多轮多模态情感理解与推理基准（MTMEUR），涵盖1451个真实场景视频和5101个递进式问题，涵盖情感识别、情感成因、未来行为预测等方面。作者还提出了包含背景、角色、事件、决策四个智能体的多智能体协作框架，显著提升了多模态大模型在情感推理任务上的表现。实验表明，现有多模态大模型在复杂情感推理任务中仍面临很大挑战，所提方法在该基准上取得了最优效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="148-RADAR-A-Reasoning-Guided-Attribution-Framework-for-Explainable-Visual-Data-Analysispdf"><a href="#148-RADAR-A-Reasoning-Guided-Attribution-Framework-for-Explainable-Visual-Data-Analysispdf" class="headerlink" title="148. RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysispdf"></a>148. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/RADAR__A_Reasoning-Guided_Attribution_Framework_for_Explainable_Visual_Data_Analysis.pdf">RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysis</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Massachusetts Institute of Technology</span></p>
<p>该论文提出RADAR框架，通过结合多模态大语言模型（如InternLM-XComposer2）和半自动数据标注，生成可解释的数学图表问答推理及区域归因数据集，并实现了自动化的推理分步生成与视觉归因流程。实验结果显示，RADAR在图表数学推理归因准确率上较现有方法提升15%，生成答案与人工标注高度一致（BERTScore约0.90），显著增强了多模态大模型在数据可视化分析的可解释性和可信度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="149-Quantifying-Sycophancy-as-Deviations-from-Bayesian-Rationality-in-LLMspdf"><a href="#149-Quantifying-Sycophancy-as-Deviations-from-Bayesian-Rationality-in-LLMspdf" class="headerlink" title="149. Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMspdf"></a>149. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Quantifying_Sycophancy_as_Deviations_from_Bayesian_Rationality_in_LLMs.pdf">Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northeastern University</span></p>
<p>本文提出利用贝叶斯框架量化大语言模型（LLM）在用户观点引导下的逢迎行为，将逢迎定义为偏离贝叶斯理性更新的概率偏移，并在三类任务（对话预测、道德判断、文化适应性）和多种模型及概率估计方法下进行实证评估。结论显示：逢迎会显著提升LLM对目标结果的概率估计，并在部分情形下导致贝叶斯误差增加，表明逢迎对模型推理有不可忽视的影响，但LLM本身并不总是贝叶斯理性，逢迎有时可纠正模型的某些误差。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="150-In-Context-Algorithm-Emulation-in-Fixed-Weight-Transformerspdf"><a href="#150-In-Context-Algorithm-Emulation-in-Fixed-Weight-Transformerspdf" class="headerlink" title="150. In-Context Algorithm Emulation in Fixed-Weight Transformerspdf"></a>150. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/In-Context_Algorithm_Emulation_in_Fixed-Weight_Transformers.pdf">In-Context Algorithm Emulation in Fixed-Weight Transformers</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northwestern University</span></p>
<p>本文理论证明了最简化的、固定权重的Transformer架构（尤其是仅由softmax自注意力组成且无参数更新）能够通过巧妙设计的prompt精确模拟一类广泛的统计和优化算法，包括梯度下降、线性回归、岭回归等。方法核心是将算法参数编码进输入token，使softmax注意力机制在前向计算过程中完成算法输出的再现。结论表明，GPT风格的大型Transformer可仅通过prompt切换内部算法，无需重训练，具备高度算法通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="151-Activation-Transport-Operatorspdf"><a href="#151-Activation-Transport-Operatorspdf" class="headerlink" title="151. Activation Transport Operatorspdf"></a>151. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Activation_Transport_Operators.pdf">Activation Transport Operators</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cambridge</span></p>
<p>该论文提出了Activation Transport Operators (ATO)，即在特征空间中由上游至下游残差流的正则化线性映射，并利用稀疏自动编码器(SAE)对LLM残差流中的特征传递进行建模。实验表明，ATO能有效判别特征是被线性传递还是由非线性操作新生成，且提出了传输效率指标来量化线性传递子空间的维度。结论指出，特征线性传递主要发生在相邻层，随层深及跨度增加而减弱，ATO有助于模型调试与诊断。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="152-Evaluating-Retrieval-Augmented-Generation-Strategies-for-Large-Language-Models-in-Travel-Mode-Choice-Predictionpdf"><a href="#152-Evaluating-Retrieval-Augmented-Generation-Strategies-for-Large-Language-Models-in-Travel-Mode-Choice-Predictionpdf" class="headerlink" title="152. Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Predictionpdf"></a>152. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Evaluating_Retrieval-Augmented_Generation_Strategies_for_Large_Language_Models_in_Travel_Mode_Choice.pdf">Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Prediction</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Texas at Austin</span></p>
<p>本文提出了一套结合检索增强生成（RAG）与大语言模型（LLM）的方法框架，系统评估了四种RAG策略（基础RAG、平衡检索、交叉编码重排序、及两者结合）在出行方式选择预测任务中的表现。结果显示，GPT-4o与平衡检索和交叉编码重排序结合时在Puget Sound出行调查数据上准确率达到80.8%，大幅优于传统统计与机器学习基线模型，且LLM具备更强泛化能力，强调了模型推理能力与检索策略匹配的重要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="153-Efficient-Zero-Shot-Long-Document-Classification-by-Reducing-Context-Through-Sentence-Rankingpdf"><a href="#153-Efficient-Zero-Shot-Long-Document-Classification-by-Reducing-Context-Through-Sentence-Rankingpdf" class="headerlink" title="153. Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Rankingpdf"></a>153. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Efficient_Zero-Shot_Long_Document_Classification_by_Reducing_Context_Through_Sentence_Ranking.pdf">Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Ranking</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Pune Institute of Computer Technology</span></p>
<p>本文提出了一种高效的零样本长文档分类方法，通过TF-IDF句子排序进行上下文压缩，无需修改模型结构。将短文本训练的Transformer模型（如marathi-bert-v2）应用于长文档，仅保留最具信息量的句子，显著降低推理时间且保持分类准确率。实验表明，选取50%高排名句子可在减少35%推理时间的情况下，准确率与原文档相当甚至更优，验证了句子排序作为上下文优化的可扩展性和效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="154-MoE-Inference-Bench-Performance-Evaluation-of-Mixture-of-Expert-Large-Language-and-Vision-Modelspdf"><a href="#154-MoE-Inference-Bench-Performance-Evaluation-of-Mixture-of-Expert-Large-Language-and-Vision-Modelspdf" class="headerlink" title="154. MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Modelspdf"></a>154. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MoE-Inference-Bench__Performance_Evaluation_of_Mixture_of_Expert_Large_Language_and_Vision_Models.pdf">MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Argonne National Laboratory</span></p>
<p>本文提出MoE-Inference-Bench，系统评测主流混合专家（MoE）大型语言及视觉-语言模型（LLM&#x2F;VLM）在多种硬件（主要为Nvidia H100 GPU）和推理优化策略（如量化、剪枝、推理并行、Speculative Decoding和Fused MoE等）下的推理性能。结果表明，FP8量化带来20-30%的吞吐提升，激活专家数量是主要的优化杠杆，单专家设置可提升50-80%的推理效率，且VLM模型推理延迟远高于文本模型，为高效部署提供了参数和策略参考。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="155-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modelingpdf"><a href="#155-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modelingpdf" class="headerlink" title="155. TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modelingpdf"></a>155. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TreePO__Bridging_the_Gap_of_Policy_Optimization_and_Efficacy_and_Inference_Efficiency_with_Heuristic.pdf">TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">M-A-P</span></p>
<p>该论文提出TreePO，一种将大语言模型序列生成过程重构为分段树结构搜索的强化学习训练框架。方法包括动态树采样、分段解码和树结构优势估计，有效利用KV缓存，显著减少推理和训练计算成本。实验表明TreePO在数学推理等复杂任务中提升了训练稳定性和探索多样性，同时显著降低了GPU计算消耗，推动了RL后训练在大模型上的可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="156-MahaParaphrase-A-Marathi-Paraphrase-Detection-Corpus-and-BERT-based-Modelspdf"><a href="#156-MahaParaphrase-A-Marathi-Paraphrase-Detection-Corpus-and-BERT-based-Modelspdf" class="headerlink" title="156. MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Modelspdf"></a>156. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MahaParaphrase__A_Marathi_Paraphrase_Detection_Corpus_and_BERT-based_Models.pdf">MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Pune Institute of Computer Technology</span></p>
<p>本论文提出了MahaParaphrase数据集——一个包含8,000对经人工标注的马拉地语同义&#x2F;非同义句对的高质量语料库。数据集采集采用余弦相似度和回译相结合并由母语者手工校验，并按词重叠率分为五个难度桶；文中还基于多个BERT系列模型（如MahaBERT、IndicBERT、Muril、MBERT）进行了微调和评测，MahaBERT在该任务上取得了88.7%的F1分数。结论：该数据集填补了低资源马拉地语领域的空白，并为相关NLP研究提供了重要资源和基线。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="157-Large-Language-Models-as-Universal-Predictors-An-Empirical-Study-on-Small-Tabular-Datasetspdf"><a href="#157-Large-Language-Models-as-Universal-Predictors-An-Empirical-Study-on-Small-Tabular-Datasetspdf" class="headerlink" title="157. Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasetspdf"></a>157. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Large_Language_Models_as_Universal_Predictors__An_Empirical_Study_on_Small_Tabular_Datasets.pdf">Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute for Language and Speech Processing, Athena Research Center</span></p>
<p>本文系统评估了多种主流大语言模型（如GPT-5、GPT-4o、Gemini等）在小规模结构化表格数据上的分类、回归和聚类任务表现，采用少样本in-context learning（ICL）方式并与传统机器学习及表格基础模型（TabPFN、TabICL）做对比。实验发现，LLMs在分类任务中接近甚至优于传统方法，无需训练即可作为实用基线，而在回归和聚类任务中表现欠佳，主要受限于模型的数值输出精度和无监督学习能力，适合于业务智能等场景的快速数据探索。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="158-GRAPH-R1-Incentivizing-the-Zero-Shot-Graph-Learning-Capability-in-LLMs-via-Explicit-Reasoningpdf"><a href="#158-GRAPH-R1-Incentivizing-the-Zero-Shot-Graph-Learning-Capability-in-LLMs-via-Explicit-Reasoningpdf" class="headerlink" title="158. GRAPH-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoningpdf"></a>158. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Graph-R1__Incentivizing_the_Zero-Shot_Graph_Learning_Capability_in_LLMs_via_Explicit_Reasoning.pdf">GRAPH-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本文提出了一种全新的无GNN（图神经网络）图学习范式，将图任务（节点分类、链接预测、图分类等）转化为大型语言模型（LLM）的推理问题，并构建第一个带有详细推理链的图任务数据集。通过两阶段训练（指令微调+基于rethink模板的强化学习），GRAPH-R1显著提升了LLM在零样本图任务上的泛化和可解释性，实验结果优于现有主流基线，证明了显式推理在图学习中的有效性和潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="159-Mimicking-the-Physicist’s-Eye-A-VLM-Centric-Approach-for-Physics-Formula-Discoverypdf"><a href="#159-Mimicking-the-Physicist’s-Eye-A-VLM-Centric-Approach-for-Physics-Formula-Discoverypdf" class="headerlink" title="159. Mimicking the Physicist’s Eye: A VLM-Centric Approach for Physics Formula Discoverypdf"></a>159. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Mimicking_the_Physicist's_Eye_A_VLM-centric_Approach_for_Physics_Formula_Discovery.pdf">Mimicking the Physicist’s Eye: A VLM-Centric Approach for Physics Formula Discovery</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">UNC–Chapel Hill</span></p>
<p>该论文提出了VIPER-R1，一种多模态视觉语言模型，通过“运动结构归纳”(MSI)和“奖励引导符号校准”(RGSC)两阶段训练，结合视觉感知（轨迹和相空间图）与符号推理，实现物理公式自动发现。推理阶段利用符号回归工具作残差对齐(SR²)，与经验数据融合，显著提升结构正确性和精度。实验表明，该模型在PhysSymbol多模态物理数据集上结构分数和准确率均显著超越现有VLM基线，实现更精确的物理规律发现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="160-Evolving-Collective-Cognition-in-Human–Agent-Hybrid-Societies-How-Agents-Form-Stances-and-Boundariespdf"><a href="#160-Evolving-Collective-Cognition-in-Human–Agent-Hybrid-Societies-How-Agents-Form-Stances-and-Boundariespdf" class="headerlink" title="160. Evolving Collective Cognition in Human–Agent Hybrid Societies: How Agents Form Stances and Boundariespdf"></a>160. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Evolving_Collective_Cognition_in_Human-Agent_Hybrid_Societies__How_Agents_Form_Stances_and_Boundarie.pdf">Evolving Collective Cognition in Human–Agent Hybrid Societies: How Agents Form Stances and Boundaries</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">William &amp; Mary</span></p>
<p>该论文提出了CMASE框架，将生成式多智能体建模与虚拟民族志方法结合，用于研究人—Agent混合社会中的群体立场分化和社会边界形成。通过三项实验，发现基于LLM的智能体能够自主形成与预设身份无关的立场，并可在互动中重组社会结构，显示身份标签的不稳定性和语言互动对社会认知演化的关键作用。结论认为，基于语言模型的智能体在多轮交互中能打破静态身份设定，通过语言实践重组认知和社会边界，为未来可信赖的社会性AI系统提供理论基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="161-Trust-Me-I-Know-This-Function-Hijacking-LLM-Static-Analysis-using-Biaspdf"><a href="#161-Trust-Me-I-Know-This-Function-Hijacking-LLM-Static-Analysis-using-Biaspdf" class="headerlink" title="161. Trust Me, I Know This Function: Hijacking LLM Static Analysis using Biaspdf"></a>161. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Trust_Me,_I_Know_This_Function__Hijacking_LLM_Static_Analysis_using_Bias.pdf">Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ben Gurion University of the Negev</span></p>
<p>本论文发现和系统分析了大型语言模型（LLM）在代码静态分析任务中的抽象偏置漏洞，提出了“Familiar Pattern Attacks”（FPAs），即通过在熟悉的代码模式中嵌入微小但有意义的错误来误导LLM的静态分析。作者开发了一种全自动黑盒算法，可在多种主流模型（GPT-4o、Claude 3.5、Gemini 2.0）和多种编程语言（Python、C、Rust、Go）下生成和注入FPAs。实验显示FPAs具有高度转移性和普适性，即使在明确告知模型防御该攻击的情况下依然有效。结论：FPA攻击揭示了LLM在代码理解中的系统性盲点，难以通过提示工程或数据过滤彻底修复，需引起广泛关注。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="162-Agentic-AI-for-Software-thoughts-from-Software-Engineering-communitypdf"><a href="#162-Agentic-AI-for-Software-thoughts-from-Software-Engineering-communitypdf" class="headerlink" title="162. Agentic AI for Software: thoughts from Software Engineering communitypdf"></a>162. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Agentic_AI_for_Software__thoughts_from_Software_Engineering_community.pdf">Agentic AI for Software: thoughts from Software Engineering community</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Singapore</span></p>
<p>本文探讨了基于大型语言模型（LLM）的自主软件工程智能体（如AutoCodeRover），通过结合程序分析和意图推断，实现自动修复、测试和维护软件任务。结论认为，随着自动化程度提高，信任和验证将成为AI生成代码流程中的核心问题，未来agentic AI将在代码验证和安全审计等关键环节扮演重要角色。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="163-Capturing-Legal-Reasoning-Paths-from-Facts-to-Law-in-Court-Judgments-using-Knowledge-Graphspdf"><a href="#163-Capturing-Legal-Reasoning-Paths-from-Facts-to-Law-in-Court-Judgments-using-Knowledge-Graphspdf" class="headerlink" title="163. Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphspdf"></a>163. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Capturing_Legal_Reasoning_Paths_from_Facts_to_Law_in_Court_Judgments_using_Knowledge_Graphs.pdf">Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphs</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Tokyo</span></p>
<p>本论文提出了一种基于知识图谱（LKG）的法律推理建模方法，通过设计法律本体和分层结构，从日本行政法院判决中抽取事实、法律规范、法律适用等节点及其推理关系，利用GPT-4o自动化构建图谱，并实现了从事实到法律条文的可追溯检索。实验表明，该方法在法律条文检索准确率和解释性上均优于单纯LLM或RAG方法，证明显式结构对法律AI推理与解释的重要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="164-DropLoRA-Sparse-Low-Rank-Adaptation-for-Parameter-Efficient-Fine-Tuningpdf"><a href="#164-DropLoRA-Sparse-Low-Rank-Adaptation-for-Parameter-Efficient-Fine-Tuningpdf" class="headerlink" title="164. DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuningpdf"></a>164. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DropLoRA__Sparse_Low-Rank_Adaptation_for_Parameter-Efficient_Fine-Tuning.pdf">DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出DropLoRA，一种创新的剪枝策略，通过在LoRA的两低秩矩阵间插入动态mask，实现动态子空间学习，从而提升大语言模型的参数高效微调性能。DropLoRA无需增加额外参数，实验在推理、数学、代码生成等多任务上验证了其在LLaMA系列上的优越性，显著优于现有LoRA及其变体。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="165-Omne-R1-Learning-to-Reason-with-Memory-for-Multi-hop-Question-Answeringpdf"><a href="#165-Omne-R1-Learning-to-Reason-with-Memory-for-Multi-hop-Question-Answeringpdf" class="headerlink" title="165. Omne-R1: Learning to Reason with Memory for Multi-hop Question Answeringpdf"></a>165. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Omne-R1__Learning_to_Reason_with_Memory_for_Multi-hop_Question_Answering.pdf">Omne-R1: Learning to Reason with Memory for Multi-hop Question Answering</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tanka Inc.</span></p>
<p>本文提出了Omne-R1，一种针对无模式知识图谱多跳问答的多阶段训练框架，融合大语言模型推理能力。方法包括知识图谱与自动生成多跳QA对、两阶段强化学习和一次监督微调，采用工具调用增强推理链条。实验结果表明，该方法在复杂多跳（3+跳）问答上优于基线，并具备较强跨领域泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="166-CultranAI-at-PalmX-2025-Data-Augmentation-for-Cultural-Knowledge-Representationpdf"><a href="#166-CultranAI-at-PalmX-2025-Data-Augmentation-for-Cultural-Knowledge-Representationpdf" class="headerlink" title="166. CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representationpdf"></a>166. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CultranAI_at_PalmX_2025__Data_Augmentation_for_Cultural_Knowledge_Representation.pdf">CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Qatar University</span></p>
<p>本论文提出了CultranAI系统，通过数据增强（结合PalmX、Palm和新扩展的PalmX-ext数据集）以及LoRA微调技术，提升大语言模型对阿拉伯文化知识的表达能力。实验对多种大模型进行基准测试，最终选择并微调Fanar-1-9B-Instruct模型，结果显示在Palm数据集上达到84.1%准确率，在盲测集上排名第五（70.5%），证明数据增强和高效微调策略有效提升了模型的文化知识理解与生成能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="167-Chinese-Court-Simulation-with-LLM-Based-Agent-Systempdf"><a href="#167-Chinese-Court-Simulation-with-LLM-Based-Agent-Systempdf" class="headerlink" title="167. Chinese Court Simulation with LLM-Based Agent Systempdf"></a>167. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Chinese_Court_Simulation_with_LLM-Based_Agent_System.pdf">Chinese Court Simulation with LLM-Based Agent System</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了SimCourt框架，基于中国真实刑事审判流程，设计了具备记忆、规划和反思能力的多模块大语言模型（LLM）法庭代理人系统，能够完整模拟法庭五大阶段及五类角色，并集成法律检索工具提升法律推理水平。实验表明，SimCourt在判决预测和模拟过程评估上均超过现有方法，部分场景下甚至优于真实法官和律师，展示了其在法律实践和教育中的应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="168-AdaptiveK-Sparse-Autoencoders-Dynamic-Sparsity-Allocation-for-Interpretable-LLM-Representationspdf"><a href="#168-AdaptiveK-Sparse-Autoencoders-Dynamic-Sparsity-Allocation-for-Interpretable-LLM-Representationspdf" class="headerlink" title="168. AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for Interpretable LLM Representationspdf"></a>168. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AdaptiveK_Sparse_Autoencoders__Dynamic_Sparsity_Allocation_for_Interpretable_LLM_Representations.pdf">AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for Interpretable LLM Representations</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出AdaptiveK Sparse Autoencoders（AdaptiveK SAE）框架，通过先用线性探针对文本复杂度进行线性预测，然后动态调整稀疏自编码器的激活特征数量，实现对大语言模型（LLM）内部表示的可解释分解。实验表明，该方法在多种LLM（Pythia-70M、Pythia-160M、Gemma-2-2B）上，重构精度、解释方差和余弦相似性均优于固定稀疏度方法，且无需大量超参数调优。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="169-Provable-Generalization-in-Overparameterized-Neural-Netspdf"><a href="#169-Provable-Generalization-in-Overparameterized-Neural-Netspdf" class="headerlink" title="169. Provable Generalization in Overparameterized Neural Netspdf"></a>169. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Provable_Generalization_in_Overparameterized_Neural_Nets.pdf">Provable Generalization in Overparameterized Neural Nets</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Princeton University</span></p>
<p>本文提出用注意力矩阵的有效秩（effective rank）作为Transformer模型容量的新度量，并理论证明该度量可导出非空的泛化界，优于传统参数计数。结论指出，注意力机制的谱结构比参数数量更能解释大型语言模型的泛化能力，与实际经验的标度律一致。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="170-CoViPAL-Layer-wise-Contextualized-Visual-Token-Pruning-for-Large-Vision-Language-Modelspdf"><a href="#170-CoViPAL-Layer-wise-Contextualized-Visual-Token-Pruning-for-Large-Vision-Language-Modelspdf" class="headerlink" title="170. CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Modelspdf"></a>170. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CoViPAL__Layer-wise_Contextualized_Visual_Token_Pruning_for_Large_Vision-Language_Models.pdf">CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, Wuhan University</span></p>
<p>本文提出了CoViPAL，一种面向大规模视觉-语言模型（LVLMs）的层级上下文化视觉token剪枝方法。其核心是引入轻量级、可插拔的剪枝模块（PPM），通过两阶段训练策略利用注意力权重和上下文信息，预测并在各层有效地移除冗余视觉token，在不损失准确率的前提下显著降低推理计算与内存开销。实验表明，CoViPAL可在保留25%视觉token情况下，推理时间降低50%以上，性能优于现有训练型和免训练型剪枝方法，适用于多种LVLM模型和多模态任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="171-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generationpdf"><a href="#171-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generationpdf" class="headerlink" title="171. ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generationpdf"></a>171. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ClaimGen-CN__A_Large-scale_Chinese_Dataset_for_Legal_Claim_Generation.pdf">ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本论文提出了ClaimGen-CN，这是首个面向中文法律诉请生成的大规模数据集，涵盖100类民事案件，总计207,748条样本，并专为非专业用户如原告设计。方法上，作者不仅构建了多样化、以原告为中心的数据集，还引入了事实性与清晰度两个面向的评价指标，并对主流通用及法律领域大模型进行了零样本评测和误差分析，发现现有模型在事实准确性和表达清晰度上均存在明显不足。结论表明，ClaimGen-CN为法律AI研究提供了新基准，当前大模型在法律诉请生成上的表现仍需提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="172-Module-Aware-Parameter-Efficient-Machine-Unlearning-on-Transformerspdf"><a href="#172-Module-Aware-Parameter-Efficient-Machine-Unlearning-on-Transformerspdf" class="headerlink" title="172. Module-Aware Parameter-Efficient Machine Unlearning on Transformerspdf"></a>172. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Module-Aware_Parameter-Efficient_Machine_Unlearning_on_Transformers.pdf">Module-Aware Parameter-Efficient Machine Unlearning on Transformers</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出MAPE-Unlearn，一种模块感知的高效参数化机器遗忘方法，通过可学习掩码在Transformer的heads和filters处定位关键影响参数，并结合二阶优化及贪婪搜索算法实现稀疏更新。实验显示MAPE-Unlearn在多模型和任务上提升了遗忘效果与模型鲁棒性，在连续遗忘和对抗重学攻击下表现优异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="173-SSFO-Self-Supervised-Faithfulness-Optimization-for-Retrieval-Augmented-Generationpdf"><a href="#173-SSFO-Self-Supervised-Faithfulness-Optimization-for-Retrieval-Augmented-Generationpdf" class="headerlink" title="173. SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generationpdf"></a>173. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SSFO__Self-Supervised_Faithfulness_Optimization_for_Retrieval-Augmented_Generation.pdf">SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>本文提出了SSFO（Self-Supervised Faithfulness Optimization），一种针对检索增强生成（RAG）系统的自监督大语言模型对齐方法。该方法通过让模型自身生成有无外部上下文的响应并构造偏好对，自主进行对齐训练，结合Direct Preference Optimization和改进的损失函数以促进概率向上下文一致的响应转移。实验结果显示，SSFO在多种问答数据集上显著提升了模型的上下文忠实度、跨语言泛化能力，并有效保留模型的通用指令遵循能力，且无需昂贵标注和推理开销。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="174-Exposing-Privacy-Risks-in-Graph-Retrieval-Augmented-Generationpdf"><a href="#174-Exposing-Privacy-Risks-in-Graph-Retrieval-Augmented-Generationpdf" class="headerlink" title="174. Exposing Privacy Risks in Graph Retrieval-Augmented Generationpdf"></a>174. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Exposing_Privacy_Risks_in_Graph_Retrieval-Augmented_Generation.pdf">Exposing Privacy Risks in Graph Retrieval-Augmented Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Pennsylvania State University</span></p>
<p>本论文系统研究了Graph Retrieval-Augmented Generation（Graph RAG）系统在数据提取攻击下的隐私风险，设计针对性攻击方法揭示其在结构化信息（实体和关系）泄露方面的高脆弱性。结论指出：Graph RAG虽然减少了原始文本泄露，但极易泄露结构化数据，且常见防御（系统提示、相似度阈值、摘要）作用有限，亟需设计更强隐私防护技术。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="175-How-to-make-Medical-AI-Systems-safer-Simulating-Vulnerabilities-and-Threats-in-Multimodal-Medical-RAG-Systemspdf"><a href="#175-How-to-make-Medical-AI-Systems-safer-Simulating-Vulnerabilities-and-Threats-in-Multimodal-Medical-RAG-Systemspdf" class="headerlink" title="175. How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG Systemspdf"></a>175. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/How_to_make_Medical_AI_Systems_safer__Simulating_Vulnerabilities,_and_Threats_in_Multimodal_Medical_.pdf">How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG Systems</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Warwick</span></p>
<p>本论文提出MedThreatRAG框架，通过注入对抗性的图文对，系统性模拟并评估多模态医学RAG系统在文本、视觉和跨模态冲突等多种威胁下的脆弱性。实验表明，跨模态对抗注入（CMCI）最为致命，能显著削弱主流医学视觉-语言模型的查全率和F1分数，暴露了医学RAG系统在知识库更新和检索阶段的安全隐患。结论指出，当前医学RAG系统极易受到数据投毒攻击，需引入多模态一致性检测等新型安全机制。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="176-Large-Language-Model-Based-Automatic-Formulation-for-Stochastic-Optimization-Modelspdf"><a href="#176-Large-Language-Model-Based-Automatic-Formulation-for-Stochastic-Optimization-Modelspdf" class="headerlink" title="176. Large Language Model-Based Automatic Formulation for Stochastic Optimization Modelspdf"></a>176. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Large_Language_Model-Based_Automatic_Formulation_for_Stochastic_Optimization_Models.pdf">Large Language Model-Based Automatic Formulation for Stochastic Optimization Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Ohio State University</span></p>
<p>本文系统研究了大语言模型（LLM），尤其是ChatGPT，在从自然语言自动构建和求解随机优化问题中的表现，涵盖联合&#x2F;个体机会约束模型及两阶段随机线性规划（SLP-2）。作者设计了链式思维、模块化推理和多智能体协作的提示策略，并提出“软评分”指标，系统评估了各类模型及提示对随机优化任务的建模与代码生成能力。结论显示，GPT-4-Turbo结合高效提示策略在变量、目标函数匹配及部分得分等方面表现最佳，多智能体协作和结构化提示能提升LLM在随机优化建模中的准确性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="177-BudgetThinker-Empowering-Budget-aware-LLM-Reasoning-with-Control-Tokenspdf"><a href="#177-BudgetThinker-Empowering-Budget-aware-LLM-Reasoning-with-Control-Tokenspdf" class="headerlink" title="177. BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokenspdf"></a>177. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/BudgetThinker__Empowering_Budget-aware_LLM_Reasoning_with_Control_Tokens.pdf">BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute for AI Industry Research (AIR), Tsinghua University</span></p>
<p>本文提出BudgetThinker，一种通过在推理过程中周期性插入特殊控制token，使大语言模型（LLM）能够精准控制推理长度的预算感知推理框架。方法包括SFT（监督微调）+ RL（基于长度奖励的强化学习）双阶段训练流程，实现模型在不同预算下高效且可控的推理输出。实验结果表明，BudgetThinker在多个数学推理基准上较现有方法实现了更精确的预算控制和更高的准确率，具备实际部署价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="178-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMspdf"><a href="#178-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMspdf" class="headerlink" title="178. PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMspdf"></a>178. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PosterGen__Aesthetic-Aware_Paper-to-Poster_Generation_via_Multi-Agent_LLMs.pdf">PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stony Brook University</span></p>
<p>本论文提出了PosterGen，一种通过多智能体大语言模型（LLMs）驱动的学术论文自动海报生成框架，创新性地将专业设计流程中的美学和排版原则嵌入多代理协作，包括内容解析、版面布局、色彩与字体风格设计以及最终渲染。通过引入基于视觉-语言模型（VLM）的评价体系，实验证明PosterGen在内容准确性、视觉美观性和可读性等方面显著优于现有方法，生成的海报无需大量人工修正即可直接用于展示。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="179-LLM-Assertiveness-can-be-Mechanistically-Decomposed-into-Emotional-and-Logical-Componentspdf"><a href="#179-LLM-Assertiveness-can-be-Mechanistically-Decomposed-into-Emotional-and-Logical-Componentspdf" class="headerlink" title="179. LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Componentspdf"></a>179. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LLM_Assertiveness_can_be_Mechanistically_Decomposed_into_Emotional_and_Logical_Components.pdf">LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Cardiff University</span></p>
<p>本文提出利用机制可解释性分析方法，研究大型语言模型（LLM）中过度自信（assertiveness）的内部机制。通过对Llama-3.2模型在人工标注断言性数据集上的微调，提取各层激活，并聚类分析，发现高断言性表征可分解为情感与逻辑两个正交子成分，并通过向量操控实验验证其对模型预测的独立影响。结论指出，LLM断言性具有多成分结构，为缓解模型过度自信行为提供了机制依据。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="180-MaRVL-QA-A-Benchmark-for-Mathematical-Reasoning-over-Visual-Landscapespdf"><a href="#180-MaRVL-QA-A-Benchmark-for-Mathematical-Reasoning-over-Visual-Landscapespdf" class="headerlink" title="180. MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapespdf"></a>180. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MaRVL-QA__A_Benchmark_for_Mathematical_Reasoning_over_Visual_Landscapes.pdf">MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Waymo</span></p>
<p>MaRVL-QA提出了一个专为多模态大语言模型设计的数学推理和空间推理基准，包括拓扑计数和变换识别两大任务，通过严格的函数库与多阶段过滤生成高质量测评数据。实验证明，当前主流MLLM在抽象空间和数学结构推理方面表现不佳，暴露出系统性缺陷，为未来模型架构和训练范式的改进提供方向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="181-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiencypdf"><a href="#181-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiencypdf" class="headerlink" title="181. InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiencypdf"></a>181. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/InternVL3.5__Advancing_Open-Source_Multimodal_Models_in_Versatility,_Reasoning,_and_Efficiency.pdf">InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai AI Laboratory</span></p>
<p>本文提出InternVL3.5，一种新一代开源多模态大模型，创新性地引入Cascade强化学习（Cascade RL）框架，结合离线和在线RL以提升推理能力，并通过视觉分辨率路由器（ViR）与解耦视觉-语言部署（DvD）优化推理效率。实验结果表明，InternVL3.5在多模态理解、推理和多任务能力方面大幅超越前代开源模型，推理性能提升16%，推理速度提升4倍，缩小了与顶级商用模型（如GPT-5）的差距。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="182-MMTok-Multimodal-Coverage-Maximization-for-Efficient-Inference-of-VLMspdf"><a href="#182-MMTok-Multimodal-Coverage-Maximization-for-Efficient-Inference-of-VLMspdf" class="headerlink" title="182. MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMspdf"></a>182. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MMTok__Multimodal_Coverage_Maximization_for_Efficient_Inference_of_VLMs.pdf">MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Arizona State University</span></p>
<p>本文提出了一种多模态覆盖最大化（MMTok）方法，通过最大覆盖准则联合利用视觉和文本信息，选择最具信息量的视觉token，以减少视觉-语言模型（VLM）推理时的token冗余。该方法将token选择问题形式化为最大覆盖问题，通过贪心算法高效近似最优解，并可选用轻量agent模型丰富文本语义。实验结果表明，MMTok在多个数据集和VLM架构下大幅提升推理效率，在保持性能的同时显著减少视觉token数量，超越主流单模态方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="183-Type-Compliant-Adaptation-Cascades-Adapting-Programmatic-LM-Workflows-to-Datapdf"><a href="#183-Type-Compliant-Adaptation-Cascades-Adapting-Programmatic-LM-Workflows-to-Datapdf" class="headerlink" title="183. Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Datapdf"></a>183. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Type-Compliant_Adaptation_Cascades__Adapting_Programmatic_LM_Workflows_to_Data.pdf">Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Google</span></p>
<p>本文提出Type-Compliant Adaptation Cascades (tacs) 框架，将LLM多步工作流适应问题形式化为学习带类型约束的概率程序，通过参数高效微调(PEFT)和梯度优化，实现端到端结构化任务适应。实验显示，在结构化任务（如MGSM-SymPy、FinQA）和中小型模型下，tacs显著优于主流prompt优化方法，并可高效 enforcing 类型合规性，提升复杂推理和结构化输出任务的可靠性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="184-Flash-Sparse-Attention-An-Alternative-Efficient-Implementation-of-Native-Sparse-Attention-Kernelpdf"><a href="#184-Flash-Sparse-Attention-An-Alternative-Efficient-Implementation-of-Native-Sparse-Attention-Kernelpdf" class="headerlink" title="184. Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernelpdf"></a>184. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Flash_Sparse_Attention__An_Alternative_Efficient_Implementation_of_Native_Sparse_Attention_Kernel.pdf">Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>本文提出了Flash Sparse Attention (FSA)，一种替代Native Sparse Attention (NSA)的高效稀疏注意力核方法，通过将内核循环顺序颠倒，并引入针对非连续内存访问、在线softmax和累加的优化，使NSA在小GQA组下也能高效运行。实验结果表明，FSA在主流长上下文LLM训练和推理中，在内核延迟、端到端训练和预填速度方面均显著优于NSA和全量注意力，实现了最高3.5×内核加速和1.25×训练加速。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="185-Unraveling-the-Cognitive-Patterns-of-Large-Language-Models-through-Module-Communitiespdf"><a href="#185-Unraveling-the-Cognitive-Patterns-of-Large-Language-Models-through-Module-Communitiespdf" class="headerlink" title="185. Unraveling the Cognitive Patterns of Large Language Models through Module Communitiespdf"></a>185. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Unraveling_the_cognitive_patterns_of_Large_Language_Models_through_module_communities.pdf">Unraveling the Cognitive Patterns of Large Language Models through Module Communities</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Rensselaer Polytechnic Institute</span></p>
<p>本论文提出了一种基于网络科学和认知科学的多重网络分析框架，将抽象认知技能、数据集和LLM（如Llama、Llama-Chat、Vicuna）内部的模块结构进行关联，通过社区发现方法揭示了模型内部技能分布与模块功能的关系，并分析了不同剪枝与微调策略对模型表现的影响。结论认为，虽然LLM存在与认知技能相关的社区结构，但基于认知技能的模块化微调并未在性能上优于随机选择，LLM的认知能力源于分布式且相互依赖的模块交互而非严格局部化的功能分区。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="186-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answeringpdf"><a href="#186-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answeringpdf" class="headerlink" title="186. ST-Raptor: LLM-Powered Semi-Structured Table Question Answeringpdf"></a>186. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ST-Raptor__LLM-Powered_Semi-Structured_Table_Question_Answering.pdf">ST-Raptor: LLM-Powered Semi-Structured Table Question Answering</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本论文提出了ST-Raptor，一种基于层次正交树（HO-Tree）结构和操作流水线的半结构化表格问答（QA）框架，通过多模态模型和启发式规则构建表结构，结合问题分解与操作-表内容对齐，实现复杂表格布局下的问题自动解答。实验表明，ST-Raptor在SSTQA等真实场景数据集上，答案准确率比九种现有方法最高提升20%，显著增强了半结构化表格问答的准确性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="187-Leveraging-Large-Language-Models-for-Accurate-Sign-Language-Translation-in-Low-Resource-Scenariospdf"><a href="#187-Leveraging-Large-Language-Models-for-Accurate-Sign-Language-Translation-in-Low-Resource-Scenariospdf" class="headerlink" title="187. Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenariospdf"></a>187. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Leveraging_Large_Language_Models_for_Accurate_Sign_Language_Translation_in_Low-Resource_Scenarios.pdf">Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Catania</span></p>
<p>该论文提出AulSign，一种结合大语言模型（LLM）、动态提示、上下文学习和外部词汇的手语翻译方法，包括Retriever、LLM和Sign Mapper三个模块，实现了结构化、可解释的从口语到手语及反向翻译。实验表明，AulSign在美式手语（ASL）和意大利手语（LIS）低资源条件下显著优于现有方法，提升了翻译准确性和可用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="188-AdLoCo-Adaptive-Batching-Significantly-Improves-Communications-Efficiency-and-Convergence-for-Large-Language-Modelspdf"><a href="#188-AdLoCo-Adaptive-Batching-Significantly-Improves-Communications-Efficiency-and-Convergence-for-Large-Language-Modelspdf" class="headerlink" title="188. AdLoCo: Adaptive Batching Significantly Improves Communications Efficiency and Convergence for Large Language Modelspdf"></a>188. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AdLoCo__adaptive_batching_significantly_improves_communications_efficiency_and_convergence_for_Large.pdf">AdLoCo: Adaptive Batching Significantly Improves Communications Efficiency and Convergence for Large Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Moscow Institute of Physics and Technology</span></p>
<p>本文提出AdLoCo方法，通过多实例训练（MIT）、自适应批处理（Adaptive Batching）和梯度累积切换机制（Switch Mode），提升大语言模型在分布式环境下的硬件利用率和通信效率。实验结果表明，AdLoCo在达到目标困惑度和收敛速度上均优于现有分布式训练方法，理论分析也给出了通信复杂度上界。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="189-The-Computational-Complexity-of-Satisfiability-in-State-Space-Modelspdf"><a href="#189-The-Computational-Complexity-of-Satisfiability-in-State-Space-Modelspdf" class="headerlink" title="189. The Computational Complexity of Satisfiability in State Space Modelspdf"></a>189. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_Computational_Complexity_of_Satisfiability_in_State_Space_Models.pdf">The Computational Complexity of Satisfiability in State Space Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Kassel</span></p>
<p>本文系统研究了状态空间模型（SSM）可满足性问题的计算复杂性，提出一般情形下问题不可判定，并在现实应用动机下探讨了两类可判定情形：有限上下文长度和定宽量化算术，分别给出NP-complete、NEXPTIME、PSPACE-complete和EXPSPACE等复杂性界。结论表明，对基于SSM的语言模型进行形式化验证存在固有的计算复杂性挑战，但在某些实际约束下问题是可解且复杂性可明确界定。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="190-Learning-from-Few-Samples-A-Novel-Approach-for-High-Quality-Malcode-Generationpdf"><a href="#190-Learning-from-Few-Samples-A-Novel-Approach-for-High-Quality-Malcode-Generationpdf" class="headerlink" title="190. Learning from Few Samples: A Novel Approach for High-Quality Malcode Generationpdf"></a>190. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Learning_from_Few_Samples__A_Novel_Approach_for_High-Quality_Malcode_Generation.pdf">Learning from Few Samples: A Novel Approach for High-Quality Malcode Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huazhong University of Science and Technology</span></p>
<p>该论文提出了GANGRL-LLM框架，将生成对抗网络（GAN）与大语言模型（LLM）结合，在半监督和少样本场景下协同训练生成器和判别器，生成高质量恶意代码（如SQL注入）并提升检测能力。实验结果表明，即使训练数据极少，该方法在恶意代码生成和入侵检测上均取得了优异性能，并具备良好的迁移性和扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="191-Test-Time-Scaling-Strategies-for-Generative-Retrieval-in-Multimodal-Conversational-Recommendationspdf"><a href="#191-Test-Time-Scaling-Strategies-for-Generative-Retrieval-in-Multimodal-Conversational-Recommendationspdf" class="headerlink" title="191. Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendationspdf"></a>191. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Test-Time_Scaling_Strategies_for_Generative_Retrieval_in_Multimodal_Conversational_Recommendations.pdf">Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Research Center for Information Technology Innovation, Academia Sinica</span></p>
<p>该论文提出了一种结合多模态大语言模型（MLLMs）的生成式检索器与测试时重排序（Test-Time Reranking, TTR）机制的新框架，以提升多轮多模态对话中的商品检索效果。方法流程包括用户意图推理、多模态生成检索、以及在推理阶段动态调整候选项分数的TTR机制。实验证明，所提方法在多个基准数据集上平均MRR提升14.5点、nDCG@1提升10.6点，显著增强检索准确性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="192-The-AI-Data-Scientistpdf"><a href="#192-The-AI-Data-Scientistpdf" class="headerlink" title="192. The AI Data Scientistpdf"></a>192. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_AI_Data_Scientist.pdf">The AI Data Scientist</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">MBZUAI</span></p>
<p>本文提出了AI Data Scientist系统，一种由大型语言模型（LLM）驱动的自治智能体，通过六个专用子代理（数据清洗、假设生成、预处理、特征工程、模型训练和行动建议）实现从原始数据到业务洞察的自动化数据科学全流程。实验结果显示，该方法不仅能提升预测准确率，还能生成更具解释性和实用性的建议，有效缩短数据分析周期，并提高洞察的可用性和决策效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="193-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Codepdf"><a href="#193-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Codepdf" class="headerlink" title="193. A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Codepdf"></a>193. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A.S.E__A_Repository-Level_Benchmark_for_Evaluating_Security_in_AI-Generated_Code.pdf">A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent</span></p>
<p>本文提出A.S.E基准，专为评估大语言模型（LLMs）在真实仓库级安全代码生成能力而设计。A.S.E以带有CVEs的真实开源项目为任务来源，保留完整仓库上下文，并通过结构&#x2F;语义变异扩展数据，结合专家自定义的静态分析规则（如CodeQL&#x2F;Joern）和可复现的Docker化环境，实现多维度（安全性、质量、稳定性）客观评价。实验显示，Claude-3.7-Sonnet模型总体表现最优，开源模型Qwen3-235B-A22B-Instruct在安全性上领先，且“快思维”推理策略优于“慢思维”在安全修复任务。结论表明A.S.E为代码安全生成研究提供了严谨可复现的评测基础，且开源模型与专有模型安全性能差距缩小，安全性与推理策略密切相关。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="194-Detecting-and-Characterizing-Planning-in-Language-Modelspdf"><a href="#194-Detecting-and-Characterizing-Planning-in-Language-Modelspdf" class="headerlink" title="194. Detecting and Characterizing Planning in Language Modelspdf"></a>194. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Detecting_and_Characterizing_Planning_in_Language_Models.pdf">Detecting and Characterizing Planning in Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Massachusetts Amherst</span></p>
<p>该论文提出了用于检测与表征大语言模型（LLMs）内部规划行为的形式化、因果标准，并实现了半自动化注释流程。作者将此流程应用于Gemma-2-2B的基础版与指令微调版，在MBPP代码生成和诗歌生成任务上，发现规划并非普适，指令微调有助于优化而非创造规划行为，部分任务模型通过即兴完成，部分则展现出明确的规划机制。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="195-Teaching-LLMs-to-Think-Mathematically-A-Critical-Study-of-Decision-Making-via-Optimizationpdf"><a href="#195-Teaching-LLMs-to-Think-Mathematically-A-Critical-Study-of-Decision-Making-via-Optimizationpdf" class="headerlink" title="195. Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimizationpdf"></a>195. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Teaching_LLMs_to_Think_Mathematically__A_Critical_Study_of_Decision-Making_via_Optimization.pdf">Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Princess Sumaya University for Technology</span></p>
<p>本文系统性综述和元分析了大语言模型（LLM）在数学规划与优化领域对决策问题的自动建模与求解能力，涵盖了学习方法、数据集设计、评估指标和提示策略，并通过构建数据集和三种提示策略（Act-as-expert、chain-of-thought、self-consistency）在计算机网络优化建模任务上进行了实验。实验显示LLM能解析自然语言并生成符号公式，但在准确性、可扩展性和可解释性上仍有明显不足，未来需发展结构化数据集、领域微调、多智能体和检索增强等混合方法来提升数学建模能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="196-Named-Entity-Recognition-of-Historical-Text-via-Large-Language-Modelpdf"><a href="#196-Named-Entity-Recognition-of-Historical-Text-via-Large-Language-Modelpdf" class="headerlink" title="196. Named Entity Recognition of Historical Text via Large Language Modelpdf"></a>196. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Named_Entity_Recognition_of_Historical_Text_via_Large_Language_Model.pdf">Named Entity Recognition of Historical Text via Large Language Model</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Bologna</span></p>
<p>本文提出利用大型语言模型（LLM），通过零样本和小样本（few-shot）提示策略，在无需任务特定训练数据的情况下，对多语言历史文本进行命名实体识别（NER）。实验基于HIPE-2022多语言历史文本数据集，探索了不同示例选择策略（随机、词汇重叠、嵌入相似性）对few-shot学习效果的影响。结论表明：仅用单一示例的小样本提示即可显著优于零样本，且示例选择方式影响有限，虽然与有监督SOTA相比仍存在性能差距，但该方法为低资源场景下NER任务提供了高效、低成本的可行替代方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="197-HyST-LLM-Powered-Hybrid-Retrieval-over-Semi-Structured-Tabular-Datapdf"><a href="#197-HyST-LLM-Powered-Hybrid-Retrieval-over-Semi-Structured-Tabular-Datapdf" class="headerlink" title="197. HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Datapdf"></a>197. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HyST__LLM-Powered_Hybrid_Retrieval_over_Semi-Structured_Tabular_Data.pdf">HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">PrompTart LAB, MODULABS</span></p>
<p>HyST提出了一种基于大语言模型（LLM）的混合检索框架，用于解析自然语言查询，将结构化约束（如类别、品牌等）转化为元数据过滤条件，并结合语义嵌入搜索对半结构化表格数据进行检索。实验结果表明，HyST在精准检索和查询理解方面优于传统基线，显著提升推荐系统在复杂查询下的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="198-Language-Models-Coupled-with-Metacognition-Can-Outperform-Reasoning-Modelspdf"><a href="#198-Language-Models-Coupled-with-Metacognition-Can-Outperform-Reasoning-Modelspdf" class="headerlink" title="198. Language Models Coupled with Metacognition Can Outperform Reasoning Modelspdf"></a>198. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Language_Models_Coupled_with_Metacognition_Can_Outperform_Reasoning_Models.pdf">Language Models Coupled with Metacognition Can Outperform Reasoning Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IBM Research</span></p>
<p>本文提出SOFAI-LM，一种无需训练的元认知架构，将大语言模型(LLM)与大推理模型(LRM)结合，通过元认知反馈迭代提升LLM推理能力，并在必要时调用LRM。实验覆盖图着色和代码调试两大推理任务，结果显示SOFAI-LM在准确率和推理效率上均显著优于单独LRM，且对模型和任务具备高度通用性。结论：SOFAI-LM实现了LLM推理性能的大幅提升，并降低了整体计算成本。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="199-Understanding-Subword-Compositionality-of-Large-Language-Modelspdf"><a href="#199-Understanding-Subword-Compositionality-of-Large-Language-Modelspdf" class="headerlink" title="199. Understanding Subword Compositionality of Large Language Modelspdf"></a>199. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Understanding_Subword_Compositionality_of_Large_Language_Models.pdf">Understanding Subword Compositionality of Large Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Copenhagen</span></p>
<p>本文系统性分析了六种主流大语言模型在子词组合到词级表示过程中的结构相似性、语义可分解性和形式保持能力，采用Procrustes分析、二分类和回归探针实验，揭示不同模型采用三种截然不同的子词组合策略。结论表明，内容相关的语义信息在所有模型和层中均能较好保持，而形式信息则有较大模型间差异，揭示了LLMs内部不同的组合动态。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="200-Debiasing-Multilingual-LLMs-in-Cross-lingual-Latent-Spacepdf"><a href="#200-Debiasing-Multilingual-LLMs-in-Cross-lingual-Latent-Spacepdf" class="headerlink" title="200. Debiasing Multilingual LLMs in Cross-lingual Latent Spacepdf"></a>200. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Debiasing_Multilingual_LLMs_in_Cross-lingual_Latent_Space.pdf">Debiasing Multilingual LLMs in Cross-lingual Latent Space</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Copenhagen</span></p>
<p>论文提出在跨语言对齐的潜在空间而非直接在LLM表示上进行消偏操作，通过训练自编码器在TED讲座平行语料上构建多语种潜在空间，并在该空间内应用SentDebias和INLP消偏技术。实验表明，这种方法能显著提升消偏效果和跨语言迁移能力，最高可减少65%偏差，且对模型下游性能影响极小。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="201-AMELIA-A-FAMILY-OF-MULTI-TASK-END-TO-END-LANGUAGE-MODELS-FOR-ARGUMENTATIONpdf"><a href="#201-AMELIA-A-FAMILY-OF-MULTI-TASK-END-TO-END-LANGUAGE-MODELS-FOR-ARGUMENTATIONpdf" class="headerlink" title="201. AMELIA: A FAMILY OF MULTI-TASK END-TO-END LANGUAGE MODELS FOR ARGUMENTATIONpdf"></a>201. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AMELIA__A_Family_of_Multi-task_End-to-end_Language_Models_for_Argumentation.pdf">AMELIA: A FAMILY OF MULTI-TASK END-TO-END LANGUAGE MODELS FOR ARGUMENTATION</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universite Claude Bernard Lyon 1</span></p>
<p>本论文提出AMELIA，通过整合19个主流论证挖掘数据集构建多任务标准化数据集，并基于Meta AI的Llama-3.1-8B-Instruct模型系统研究了单任务微调、多任务联合微调以及多模型合并等训练策略，涵盖论证成分分类、立场检测、证据识别等八项任务。结果显示：单任务微调显著提升各任务表现，多任务微调无性能损失且实现任务迁移，模型合并可在降低资源消耗下取得竞争性效果。结论认为大语言模型通过细致适配，可高效应对多种论证挖掘任务，兼顾性能与灵活性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="202-Riemannian-Optimization-for-LoRA-on-the-Stiefel-Manifoldpdf"><a href="#202-Riemannian-Optimization-for-LoRA-on-the-Stiefel-Manifoldpdf" class="headerlink" title="202. Riemannian Optimization for LoRA on the Stiefel Manifoldpdf"></a>202. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Riemannian_Optimization_for_LoRA_on_the_Stiefel_Manifold.pdf">Riemannian Optimization for LoRA on the Stiefel Manifold</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Opt-AI Inc.</span></p>
<p>本文提出Stiefel-LoRA方法，将LoRA中的B矩阵优化限制在Stiefel流形上，实现显式正交约束以提升表示效率，通过Riemannian优化（QR分解投影+回缩）替代传统AdamW优化。实验结果表明，Stiefel-LoRA在多种LLM微调基准任务上均显著优于传统LoRA，提升参数利用率与收敛速度，验证了几何约束对PEFT性能优化的关键作用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="203-ILRE-Intermediate-Layer-Retrieval-for-Context-Compression-in-Causal-Language-Modelspdf"><a href="#203-ILRE-Intermediate-Layer-Retrieval-for-Context-Compression-in-Causal-Language-Modelspdf" class="headerlink" title="203. ILRE: Intermediate Layer Retrieval for Context Compression in Causal Language Modelspdf"></a>203. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ILRe__Intermediate_Layer_Retrieval_for_Context_Compression_in_Causal_Language_Models.pdf">ILRE: Intermediate Layer Retrieval for Context Compression in Causal Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AI Lab, China Merchants Bank</span></p>
<p>该论文提出了一种名为ILRe的无训练上下文压缩框架，用于因果语言模型的长文本推理。方法通过在离线选定的中间解码器层，仅对该层进行流式分块预填充，然后利用输入查询与全量Key缓存的注意力分数实现Token召回，并在召回过程中采用多池化核分配策略以保持语义完整性。实验结果显示，ILRe在无需额外训练或算子开发的情况下，实现了推理速度百倍提升，精度与完整上下文持平或更优，显著提升了长上下文LLM的推理效率和能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="204-UniAPO-Unified-Multimodal-Automated-Prompt-Optimizationpdf"><a href="#204-UniAPO-Unified-Multimodal-Automated-Prompt-Optimizationpdf" class="headerlink" title="204. UniAPO: Unified Multimodal Automated Prompt Optimizationpdf"></a>204. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/UniAPO__Unified_Multimodal_Automated_Prompt_Optimization.pdf">UniAPO: Unified Multimodal Automated Prompt Optimization</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ByteDance</span></p>
<p>本论文提出了UniAPO——首个统一的多模态自动提示优化（APO）框架，采用EM算法思想，将反馈建模与提示优化解耦，并引入短-长期记忆机制，专门解决视觉token膨胀和过程级监督缺失等多模态场景下的挑战。实验显示，UniAPO在文本、图像和视频等多模态任务上全面超越现有方法，具备优异的性能、泛化能力及优化效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="205-AVAM-Universal-Training-free-Adaptive-Visual-Anchoring-Embedded-into-Multimodal-Large-Language-Model-for-Multi-image-Question-Answeringpdf"><a href="#205-AVAM-Universal-Training-free-Adaptive-Visual-Anchoring-Embedded-into-Multimodal-Large-Language-Model-for-Multi-image-Question-Answeringpdf" class="headerlink" title="205. AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answeringpdf"></a>205. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AVAM__Universal_Training-free_Adaptive_Visual_Anchoring_Embedded_into_Multimodal_Large_Language_Mode.pdf">AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hunan University</span></p>
<p>本文提出了一种无训练的自适应视觉锚定（Adaptive Visual Anchoring, AVAM）策略，通过计算视觉特征与文本特征之间的响应热区，自适应筛选多图像输入中与问题最相关的连续关键区域，并结合协同解码机制动态融合全局与局部视觉上下文信息。实验结果表明，AVAM可无缝集成到主流多模态大模型（MLLMs），在多图像视觉问答任务（MVQA）中显著提升准确率和效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="206-VISA-Group-wise-Visual-Token-Selection-and-Aggregation-via-Graph-Summarization-for-Efficient-MLLMs-Inferencepdf"><a href="#206-VISA-Group-wise-Visual-Token-Selection-and-Aggregation-via-Graph-Summarization-for-Efficient-MLLMs-Inferencepdf" class="headerlink" title="206. VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inferencepdf"></a>206. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/VISA__Group-wise_Visual_Token_Selection_and_Aggregation_via_Graph_Summarization_for_Efficient_MLLMs_.pdf">VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiamen University</span></p>
<p>该论文提出了VISA方法，通过组内视觉token选择与图汇总聚合（VTA），解决多模态大语言模型（MLLMs）视觉token冗余导致推理效率低的问题。VTA利用语义相似性构建图，将被移除token的信息通过图结构聚合到保留token，并结合组内选择策略，根据文本关注逐步筛选和压缩视觉token。实验表明，VISA在主流多模态基准上能在极高压缩率下保持模型性能，同时提升推理速度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="207-Group-Expectation-Policy-Optimization-for-Stable-Heterogeneous-Reinforcement-Learning-in-LLMspdf"><a href="#207-Group-Expectation-Policy-Optimization-for-Stable-Heterogeneous-Reinforcement-Learning-in-LLMspdf" class="headerlink" title="207. Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMspdf"></a>207. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Group_Expectation_Policy_Optimization_for_Stable_Heterogeneous_Reinforcement_Learning_in_LLMs.pdf">Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMs</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peng Cheng Laboratory</span></p>
<p>该论文提出了HeteroRL异构计算架构和GEPO群期望策略优化算法，用于提升大语言模型（LLM）在地理分布式高延迟异构环境下的强化学习训练稳定性。方法上，HeteroRL实现采样与参数学习异步解耦，GEPO通过样本级重要性权重与群期望平滑机制，显著抑制高延迟下重要性采样方差爆炸，理论与实验证明在极端延迟下仍能保持模型性能稳定，优于GRPO等现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="208-FAIRGAMER-Evaluating-Biases-in-the-Application-of-Large-Language-Models-to-Video-Gamespdf"><a href="#208-FAIRGAMER-Evaluating-Biases-in-the-Application-of-Large-Language-Models-to-Video-Gamespdf" class="headerlink" title="208. FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Gamespdf"></a>208. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FAIRGAMER__Evaluating_Biases_in_the_Application_of_Large_Language_Models_to_Video_Games.pdf">FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Games</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Cyber Security, University of Chinese Academy of Sciences</span></p>
<p>该论文提出了FAIRGAMER，这是首个用于评估大语言模型（LLM）在电子游戏场景中社会和文化偏见的基准，涵盖NPC对话、对抗决策和场景生成三大任务，并引入了Decision Log Standard Deviation（Dlstd）新指标。实验证明，现有主流LLM在真实与虚构游戏环境下均表现出明显偏见，显著影响游戏平衡，揭示了LLM在游戏应用中的可靠性挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="209-Scalable-Engine-and-the-Performance-of-Different-LLM-Models-in-a-SLURM-Based-HPC-Architecturepdf"><a href="#209-Scalable-Engine-and-the-Performance-of-Different-LLM-Models-in-a-SLURM-Based-HPC-Architecturepdf" class="headerlink" title="209. Scalable Engine and the Performance of Different LLM Models in a SLURM Based HPC Architecturepdf"></a>209. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scalable_Engine_and_the_Performance_of_Different_LLM_Models_in_a_SLURM_based_HPC_architecture.pdf">Scalable Engine and the Performance of Different LLM Models in a SLURM Based HPC Architecture</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AImotion Bavaria</span></p>
<p>本文提出了一种基于SLURM的高性能计算架构，结合容器化微服务与动态资源调度，实现了多种LLM模型（如Llama 3.1和3.2）在多节点集群中的高效可扩展推理。实验结果显示，该架构在REST API接口基础上支持高并发、低延迟的推理服务，对小模型可支持128并发、50毫秒级延迟，大模型则在2个并发下延迟超2秒，整体系统具备良好弹性和容错性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="210-Proximal-Supervised-Fine-Tuningpdf"><a href="#210-Proximal-Supervised-Fine-Tuningpdf" class="headerlink" title="210. Proximal Supervised Fine-Tuningpdf"></a>210. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Proximal_Supervised_Fine-Tuning.pdf">Proximal Supervised Fine-Tuning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了一种新的大模型监督微调方法Proximal Supervised Fine-Tuning (PSFT)，将强化学习中的信赖域思想（如TRPO、PPO的clipped surrogate objective）引入到监督微调损失，限制模型参数更新幅度以防止能力退化和熵坍缩。实验表明，PSFT在数学推理和人类价值对齐等领域，在保持目标任务性能的同时显著提升了模型的泛化能力，并为后续RL等优化阶段打下更好基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="211-ISACL-Internal-State-Analyzer-for-Copyrighted-Training-Data-Leakagepdf"><a href="#211-ISACL-Internal-State-Analyzer-for-Copyrighted-Training-Data-Leakagepdf" class="headerlink" title="211. ISACL: Internal State Analyzer for Copyrighted Training Data Leakagepdf"></a>211. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ISACL__Internal_State_Analyzer_for_Copyrighted_Training_Data_Leakage.pdf">ISACL: Internal State Analyzer for Copyrighted Training Data Leakage</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">City University of Hong Kong</span></p>
<p>该论文提出了一种名为ISACL的框架，通过分析大语言模型（LLM）在生成文本前的内部状态，结合检索增强生成（RAG）系统及神经网络分类器，实现对潜在版权训练数据泄漏的实时、前置检测。实验表明ISACL能有效预防敏感或受版权保护内容的泄漏，并兼顾高准确率和效率，提升了合规性和数据隐私保护。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="212-Randomly-Removing-50-of-Dimensions-in-Text-Embeddings-has-Minimal-Impact-on-Retrieval-and-Classification-Taskspdf"><a href="#212-Randomly-Removing-50-of-Dimensions-in-Text-Embeddings-has-Minimal-Impact-on-Retrieval-and-Classification-Taskspdf" class="headerlink" title="212. Randomly Removing 50% of Dimensions in Text Embeddings has Minimal Impact on Retrieval and Classification Taskspdf"></a>212. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Randomly_Removing_50%_of_Dimensions_in_Text_Embeddings_has_Minimal_Impact_on_Retrieval_and_Classific.pdf">Randomly Removing 50% of Dimensions in Text Embeddings has Minimal Impact on Retrieval and Classification Tasks</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Mannheim</span></p>
<p>本论文系统分析了在文本嵌入向量中随机移除高达50%维度后，对检索和分类等下游任务性能的影响。通过对6个主流文本编码器和26项任务的大规模实验证明，嵌入维度的随机截断仅导致性能微幅下降（小于10%），其原因在于许多维度对下游任务有负面贡献，并在所有模型中均匀分布。结论指出，当前文本嵌入模型存在大量无效甚至有害的维度，随机减小维度可以低成本地减少表示空间并保持高性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="213-Speculative-Safety-Aware-Decodingpdf"><a href="#213-Speculative-Safety-Aware-Decodingpdf" class="headerlink" title="213. Speculative Safety-Aware Decodingpdf"></a>213. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Speculative_Safety-Aware_Decoding.pdf">Speculative Safety-Aware Decoding</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science &amp; Technology, Beijing Institute of Technology</span></p>
<p>本文提出了Speculative Safety-Aware Decoding (SSD)，一种轻量级解码时安全增强方法，通过结合小型安全专家模型与大模型，在生成过程中利用“match ratio”动态切换解码策略（交集用于实用性、并集用于安全性），以防御jailbreak攻击并加速推理。实验证明SSD能有效将深度安全对齐属性转移至大型语言模型输出，显著提升安全性且保持模型有用性和推理效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="214-F2RVLM-Boosting-Fine-grained-Fragment-Retrieval-for-Multi-Modal-Long-form-Dialogue-with-Vision-Language-Modelpdf"><a href="#214-F2RVLM-Boosting-Fine-grained-Fragment-Retrieval-for-Multi-Modal-Long-form-Dialogue-with-Vision-Language-Modelpdf" class="headerlink" title="214. F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Modelpdf"></a>214. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/F2RVLM__Boosting_Fine-grained_Fragment_Retrieval_for_Multi-Modal_Long-form_Dialogue_with_Vision_Lang.pdf">F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent Inc</span></p>
<p>本文提出了Fine-grained Fragment Retrieval (FFR)任务，要求模型从多模态长对话中检索语义相关的文本和图像片段。作者构建了长对话数据集MLDR，并提出F2RVLM模型：首先通过监督微调注入检索知识，随后采用基于GRPO的强化学习优化语义精确性与上下文一致性，并结合难度感知课程采样提升长上下文推理能力。实验结果显示F2RVLM在合成和真实对话场景下均显著超越主流VLM模型，提升检索准确率与一致性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="215-Database-Normalization-via-Dual-LLM-Self-Refinementpdf"><a href="#215-Database-Normalization-via-Dual-LLM-Self-Refinementpdf" class="headerlink" title="215. Database Normalization via Dual-LLM Self-Refinementpdf"></a>215. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Database_Normalization_via_Dual-LLM_Self-Refinement.pdf">Database Normalization via Dual-LLM Self-Refinement</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sungshin Women’s University</span></p>
<p>该论文提出了Miffie框架，利用大语言模型（LLM）实现数据库模式的自动归一化。方法采用双模型自我优化架构：GPT-4负责生成规范化方案，o1-mini进行验证并循环反馈，通过零样本任务特定提示实现高准确率和成本效率。实验表明，Miffie能够高效准确地规范复杂数据库方案，大幅降低人工工作量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="216-LLM-based-Agentic-Reasoning-Frameworks-A-Survey-from-Methods-to-Scenariospdf"><a href="#216-LLM-based-Agentic-Reasoning-Frameworks-A-Survey-from-Methods-to-Scenariospdf" class="headerlink" title="216. LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenariospdf"></a>216. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LLM-based_Agentic_Reasoning_Frameworks__A_Survey_from_Methods_to_Scenarios.pdf">LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Jiaotong University</span></p>
<p>本论文提出了一个系统性的分类法，将基于大语言模型（LLM）的智能体推理框架分为单智能体方法、工具增强方法和多智能体方法，详细分析了各类框架的核心方法（如 prompt 工程、反思与自我改进、工具集成与选择、多智能体组织与交互等）及其在科学发现、医疗、软件工程和社会经济模拟等关键场景中的应用与评测体系。结论指出，系统化的框架有助于推动智能体推理的发展，提升其在多领域复杂任务中的适用性和安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="217-Unlearning-as-Ablation-Toward-a-Falsifiable-Benchmark-for-Generative-Scientific-Discoverypdf"><a href="#217-Unlearning-as-Ablation-Toward-a-Falsifiable-Benchmark-for-Generative-Scientific-Discoverypdf" class="headerlink" title="217. Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discoverypdf"></a>217. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Unlearning_as_Ablation__Toward_a_Falsifiable_Benchmark_for_Generative_Scientific_Discovery.pdf">Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discovery</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent Researcher</span></p>
<p>本文提出了“Unlearning-as-Ablation”方法作为检验大语言模型（LLM）是否具备生成性科学发现能力的可证伪测试。该方法通过系统性移除目标结果及其相关推理链（forget-closure），并要求模型仅凭允许的公理和工具重新推导该结果，从而区分模型真正的生成能力与仅仅是记忆和重组；结论认为，无论模型能否重新推导目标，这一框架都为AI科学发现能力的边界提供了实证评估手段。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="218-Towards-Synthesizing-Normative-Data-for-Cognitive-Assessments-Using-Generative-Multimodal-Large-Language-Modelspdf"><a href="#218-Towards-Synthesizing-Normative-Data-for-Cognitive-Assessments-Using-Generative-Multimodal-Large-Language-Modelspdf" class="headerlink" title="218. Towards Synthesizing Normative Data for Cognitive Assessments Using Generative Multimodal Large Language Modelspdf"></a>218. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Towards_Synthesizing_Normative_Data_for_Cognitive_Assessments_Using_Generative_Multimodal_Large_Lang.pdf">Towards Synthesizing Normative Data for Cognitive Assessments Using Generative Multimodal Large Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Westminster Schools</span></p>
<p>本文提出利用生成式多模态大语言模型（如GPT-4o及其mini版本）通过精细化提示，自动生成认知评估所需的规范性文本数据，替代以往昂贵且耗时的人工采集方法。实验结果显示，综合型临床提示能生成更真实且具诊断区分度的文本，BERTScore和LLM-as-a-Judge评价方法更能反映人工评判，推动认知测试新刺激的快速开发与数据更新。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="219-Dynamic-Embedding-of-Hierarchical-Visual-Features-for-Efficient-Vision-Language-Fine-Tuningpdf"><a href="#219-Dynamic-Embedding-of-Hierarchical-Visual-Features-for-Efficient-Vision-Language-Fine-Tuningpdf" class="headerlink" title="219. Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuningpdf"></a>219. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dynamic_Embedding_of_Hierarchical_Visual_Features_for_Efficient_Vision-Language_Fine-Tuning.pdf">Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">College of Computer Science and Technology, National University of Defense Technology</span></p>
<p>该论文提出一种动态分层视觉特征嵌入框架（DEHVF），通过分层视觉融合器，根据语言模型各层的语义粒度动态选择和融合视觉编码器的多层特征，并将其直接嵌入至对应层的FFN权重中，从而实现高效的视觉-语言模型微调。实验表明，DEHVF在ScienceQA和COCO Caption等任务上，以更少参数和更高计算效率超越现有PEFT方法和全量微调模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="220-Weights-Rotated-Preference-Optimization-for-Large-Language-Modelspdf"><a href="#220-Weights-Rotated-Preference-Optimization-for-Large-Language-Modelspdf" class="headerlink" title="220. Weights-Rotated Preference Optimization for Large Language Modelspdf"></a>220. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Weights-Rotated_Preference_Optimization_for_Large_Language_Models.pdf">Weights-Rotated Preference Optimization for Large Language Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>本文提出了Weights-Rotated Preference Optimization (RoPO)算法，通过对LLM参数空间的多粒度正交矩阵旋转，结合DPO中的KL散度约束，显式地约束中间隐藏层及输出层，防止神经元塌缩和表达冗余，从而有效缓解DPO中的reward hacking问题。实验表明，RoPO在AlpacaEval 2和MT-Bench等多项评测中表现优异，提升对齐能力并减少无效冗长生成，保留了预训练和SFT阶段的知识。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="221-Stop-Spinning-Wheels-Mitigating-LLM-Overthinking-via-Mining-Patterns-for-Early-Reasoning-Exitpdf"><a href="#221-Stop-Spinning-Wheels-Mitigating-LLM-Overthinking-via-Mining-Patterns-for-Early-Reasoning-Exitpdf" class="headerlink" title="221. Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exitpdf"></a>221. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Stop_Spinning_Wheels__Mitigating_LLM_Overthinking_via_Mining_Patterns_for_Early_Reasoning_Exit.pdf">Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Computing Technology, Chinese Academy of Sciences</span></p>
<p>本文提出通过分析大语言模型推理过程中的思维长度与内容长度关系，将推理划分为三个阶段，并首次明确提出Reasoning Completion Point（RCP）作为最佳推理终止点。基于特征重要性分析，设计了一套高效的启发式规则（RCPD），可低成本且精准地检测RCP，实验证明在多个复杂推理基准上平均减少30%以上token消耗，同时维持或提升准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="222-Steering-When-Necessary-Flexible-Steering-Large-Language-Models-with-Backtrackingpdf"><a href="#222-Steering-When-Necessary-Flexible-Steering-Large-Language-Models-with-Backtrackingpdf" class="headerlink" title="222. Steering When Necessary: Flexible Steering Large Language Models with Backtrackingpdf"></a>222. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Steering_When_Necessary__Flexible_Steering_Large_Language_Models_with_Backtracking.pdf">Steering When Necessary: Flexible Steering Large Language Models with Backtracking</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本文提出了Flexible Activation Steering with Backtracking（FASB）框架，通过追踪LLM生成过程中的内部状态，结合问题和已生成内容，动态判断是否以及如何强度干预模型激活，并设计了回溯机制对偏离目标行为的token进行回退再生成。实验表明，该方法在TruthfulQA和六个多项选择数据集上优于现有基线方法，有效提升了LLM输出的可控性与真确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="223-ChartMaster-Advancing-Chart-to-Code-Generation-with-Real-World-Charts-and-Chart-Similarity-Reinforcement-Learningpdf"><a href="#223-ChartMaster-Advancing-Chart-to-Code-Generation-with-Real-World-Charts-and-Chart-Similarity-Reinforcement-Learningpdf" class="headerlink" title="223. ChartMaster: Advancing Chart-to-Code Generation with Real-World Charts and Chart Similarity Reinforcement Learningpdf"></a>223. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ChartMaster__Advancing_Chart-to-Code_Generation_with_Real-World_Charts_and_Chart_Similarity_Reinforc.pdf">ChartMaster: Advancing Chart-to-Code Generation with Real-World Charts and Chart Similarity Reinforcement Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China University of Technology</span></p>
<p>该论文提出ChartMaster框架，利用真实世界arXiv论文中的图表构建高多样性数据集ReChartPrompt-240K，并设计ChartSimRL强化学习算法，通过结合语义属性和视觉相似度奖励优化模型输出。最终模型在多个chart-to-code生成基准上取得接近GPT-4o的表现，显著提升了图表复现的准确性和多样性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="224-Exploring-Efficient-Learning-of-Small-BERT-Networks-with-LoRA-and-DoRApdf"><a href="#224-Exploring-Efficient-Learning-of-Small-BERT-Networks-with-LoRA-and-DoRApdf" class="headerlink" title="224. Exploring Efficient Learning of Small BERT Networks with LoRA and DoRApdf"></a>224. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Exploring_Efficient_Learning_of_Small_BERT_Networks_with_LoRA_and_DoRA.pdf">Exploring Efficient Learning of Small BERT Networks with LoRA and DoRA</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>本论文围绕小型BERT模型（minBERT），研究并定制了低秩适应（LoRA）和权重分解低秩适应（DoRA）技术，结合自动混合精度（AMP），实现了参数高效微调。通过多任务学习（情感分析、复述检测、语义相似度），系统对比了不同低秩分解方式、超参数与优化策略，发现对于小模型，LoRA和DoRA在极低秩（秩为1）即可保持性能，AMP与LoRA结合带来约7-50%内存节省和最高60%训练加速，性能提升或损失极小。结论认为低秩微调技术同样适用于小规模语言模型，推荐在资源受限场景下采用LoRA+AMP方式，且进一步研究需关注低秩更新对模型误差模式和偏见的影响。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="225-TradingGroup-A-Multi-Agent-Trading-System-with-Self-Reflection-and-Data-Synthesispdf"><a href="#225-TradingGroup-A-Multi-Agent-Trading-System-with-Self-Reflection-and-Data-Synthesispdf" class="headerlink" title="225. TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesispdf"></a>225. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TradingGroup__A_Multi-Agent_Trading_System_with_Self-Reflection_and_Data-Synthesis.pdf">TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of New South Wales</span></p>
<p>该论文提出TradingGroup多智能体量化交易系统，通过结合大语言模型（LLM）、自反思机制、动态风险管理和自动化数据合成管道，实现了新闻情绪、财报解读、趋势预测、风格适应等多代理协作。系统在五只真实股票数据上回测，显著优于规则、传统机器学习、强化学习和现有LLM基交易方法，并通过自主生成高质量后训练数据显著提升了模型表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="226-Enhancing-Knowledge-Tracing-through-Leakage-Free-and-Recency-Aware-Embeddingspdf"><a href="#226-Enhancing-Knowledge-Tracing-through-Leakage-Free-and-Recency-Aware-Embeddingspdf" class="headerlink" title="226. Enhancing Knowledge Tracing through Leakage-Free and Recency-Aware Embeddingspdf"></a>226. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Enhancing_Knowledge_Tracing_through_Leakage-Free_and_Recency-Aware_Embeddings.pdf">Enhancing Knowledge Tracing through Leakage-Free and Recency-Aware Embeddings</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Applied Sciences Karlsruhe</span></p>
<p>本文针对知识追踪（KT）模型中的标签泄漏问题，提出通过在输入嵌入阶段使用MASK标签，有效阻止同一问题下各知识点（KC）之间的标签泄漏，同时采用基于可学习傅里叶特征的Recency Encoding方法，显式编码每个知识点自上次出现以来的时间步距。实验结果显示，这两种嵌入技术能在多种KT模型和基准数据集上显著提升预测准确率，尤其在多知识点题目数据集上提升更为明显，且方法高效、易于集成。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="227-A-Decoupled-LOB-Representation-Framework-for-Multilevel-Manipulation-Detection-with-Supervised-Contrastive-Learningpdf"><a href="#227-A-Decoupled-LOB-Representation-Framework-for-Multilevel-Manipulation-Detection-with-Supervised-Contrastive-Learningpdf" class="headerlink" title="227. A Decoupled LOB Representation Framework for Multilevel Manipulation Detection with Supervised Contrastive Learningpdf"></a>227. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Decoupled_LOB_Representation_Framework_for_Multilevel_Manipulation_Detection_with_Supervised_Contr.pdf">A Decoupled LOB Representation Framework for Multilevel Manipulation Detection with Supervised Contrastive Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southern University of Science and Technology</span></p>
<p>本文提出一种用于金融市场多层级操纵检测的表征学习框架，核心包括级联限制性委托单（LOB）表征管道和监督式对比学习。该方法通过Transformer编码器和混合损失函数，有效提升多层级异常检测性能，实验结果表明在多种模型下均优于现有方法，特别是在罕见事件检测上表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="228-CP4SBI-Local-Conformal-Calibration-of-Credible-Sets-in-Simulation-Based-Inferencepdf"><a href="#228-CP4SBI-Local-Conformal-Calibration-of-Credible-Sets-in-Simulation-Based-Inferencepdf" class="headerlink" title="228. CP4SBI: Local Conformal Calibration of Credible Sets in Simulation-Based Inferencepdf"></a>228. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CP4SBI__Local_Conformal_Calibration_of_Credible_Sets_in_Simulation-Based_Inference.pdf">CP4SBI: Local Conformal Calibration of Credible Sets in Simulation-Based Inference</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Federal University of São Carlos</span></p>
<p>该论文提出CP4SBI框架，将局部和CDF变换的共形校准方法应用于仿真推断(SBI)后验分布的可信区间构建，支持任意后验估计器和打分函数。实验结果表明，CP4SBI在多个SBI基准任务上显著提升了可信区间的条件覆盖率与校准效果，提升了后验不确定性的可靠性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="229-TabResFlow-A-Normalizing-Spline-Flow-Model-for-Probabilistic-Univariate-Tabular-Regressionpdf"><a href="#229-TabResFlow-A-Normalizing-Spline-Flow-Model-for-Probabilistic-Univariate-Tabular-Regressionpdf" class="headerlink" title="229. TabResFlow: A Normalizing Spline Flow Model for Probabilistic Univariate Tabular Regressionpdf"></a>229. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TabResFlow__A_Normalizing_Spline_Flow_Model_for_Probabilistic_Univariate_Tabular_Regression.pdf">TabResFlow: A Normalizing Spline Flow Model for Probabilistic Univariate Tabular Regression</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Hildesheim</span></p>
<p>该论文提出了TabResFlow，一种结合ResNet风格多层感知机特征提取与条件神经样条流（RQ-NSF）密度估计的新架构，用于单变量表格数据概率回归。TabResFlow在九个公开基准数据集和真实二手车定价任务上，相比现有概率回归方法（如TreeFlow和NodeFlow）在概率拟合精度（NLL）和推理速度上均有显著提升，并提出了AURC评价指标验证模型在不确定性量化中的优越性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="230-Score-Matching-on-Large-Geometric-Graphs-for-Cosmology-Generationpdf"><a href="#230-Score-Matching-on-Large-Geometric-Graphs-for-Cosmology-Generationpdf" class="headerlink" title="230. Score Matching on Large Geometric Graphs for Cosmology Generationpdf"></a>230. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Score_Matching_on_Large_Geometric_Graphs_for_Cosmology_Generation.pdf">Score Matching on Large Geometric Graphs for Cosmology Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Eindhoven University of Technology</span></p>
<p>该论文提出了一种基于score-matching的生成模型，结合E(3)等变图神经网络和拓扑感知噪声调度，用于在大规模几何图上生成宇宙学结构。方法通过物理动机的均匀先验、周期边界条件和对称性约束实现高效、物理一致的银河团簇生成，并显著优于现有扩散模型。实验表明，该模型可扩展至60万节点的大规模图，生成结果在聚类统计和计算效率上均优于基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="231-Sig-DEG-for-Distillation-Making-Diffusion-Models-Faster-and-Lighterpdf"><a href="#231-Sig-DEG-for-Distillation-Making-Diffusion-Models-Faster-and-Lighterpdf" class="headerlink" title="231. Sig-DEG for Distillation: Making Diffusion Models Faster and Lighterpdf"></a>231. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Sig-DEG_for_Distillation__Making_Diffusion_Models_Faster_and_Lighter.pdf">Sig-DEG for Distillation: Making Diffusion Models Faster and Lighter</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University College London</span></p>
<p>该论文提出了Sig-DEG（Signature-based Differential Equation Generator）方法，用于将预训练扩散模型蒸馏为更高效的生成器。其核心技术是利用部分路径特征（partial signatures）和递归神经网络结构，近似反向扩散SDE过程，实现粗时间分辨率下的高效仿真。方法将蒸馏过程建模为监督学习任务，以高分辨教师模型轨迹为标签，训练Sig-DEG在推理时能用极少步数生成高质量样本。实验表明，Sig-DEG在MNIST、合成分布与时间序列等任务中，能以5-10步推理达到数十到上百倍加速，同时保持甚至提升生成质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="232-Bridging-Graph-and-State-Space-Modeling-for-Intensive-Care-Unit-Length-of-Stay-Predictionpdf"><a href="#232-Bridging-Graph-and-State-Space-Modeling-for-Intensive-Care-Unit-Length-of-Stay-Predictionpdf" class="headerlink" title="232. Bridging Graph and State-Space Modeling for Intensive Care Unit Length of Stay Predictionpdf"></a>232. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Bridging_Graph_and_State-Space_Modeling_for_Intensive_Care_Unit_Length_of_Stay_Prediction.pdf">Bridging Graph and State-Space Modeling for Intensive Care Unit Length of Stay Prediction</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cambridge</span></p>
<p>该论文提出了S2G-Net，一种结合状态空间模型（Mamba）和多视图图神经网络（GraphGPS）的双路径神经架构，用于ICU住院时长预测。方法通过时序路径捕捉患者动态轨迹，图路径整合诊断、语义和管理特征构建的患者相似性图，并融合静态特征，提升预测准确性和模型解释性。实验结果显示，S2G-Net在MIMIC-IV大规模临床数据集上显著优于主流时序、图和混合模型，具备高效能、良好解释性和强鲁棒性，为临床资源管理提供有效解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="233-GateTS-Versatile-and-Efficient-Forecasting-via-Attention-Inspired-routed-Mixture-of-Expertspdf"><a href="#233-GateTS-Versatile-and-Efficient-Forecasting-via-Attention-Inspired-routed-Mixture-of-Expertspdf" class="headerlink" title="233. GateTS: Versatile and Efficient Forecasting via Attention-Inspired routed Mixture-of-Expertspdf"></a>233. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GateTS__Versatile_and_Efficient_Forecasting_via_Attention-Inspired_routed_Mixture-of-Experts.pdf">GateTS: Versatile and Efficient Forecasting via Attention-Inspired routed Mixture-of-Experts</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Lviv Polytechnic National University</span></p>
<p>本文提出了GateTS，一种结合稀疏Mixture-of-Experts结构和创新注意力启发门控机制的时间序列预测模型。该方法通过Kronecker乘积将输入与专家查询进行二阶交互，实现无需辅助均衡损失即可自适应选择专家，并在多种真实数据集上以更少的参数超越PatchTST和LSTM等主流模型。结果表明，GateTS在保证高预测精度的同时，大幅降低了计算与参数成本，适用于精度和效率均受限的实际预测场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="234-Effective-Clustering-for-Large-Multi-Relational-Graphspdf"><a href="#234-Effective-Clustering-for-Large-Multi-Relational-Graphspdf" class="headerlink" title="234. Effective Clustering for Large Multi-Relational Graphspdf"></a>234. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Effective_Clustering_for_Large_Multi-Relational_Graphs.pdf">Effective Clustering for Large Multi-Relational Graphs</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hong Kong Baptist University</span></p>
<p>本文提出了两种针对大规模多关系图聚类（MRGC）的方法：DEMM和DEMM+。DEMM采用基于多关系Dirichlet能量（MRDE）的两阶段优化目标，先融合多类型关系和节点属性生成高质量特征，再进行谱聚类；DEMM+利用近似算法和Sinkhorn-Knopp归一化显著提升计算效率和可扩展性。实验表明，DEMM+在聚类质量和速度上均优于现有20种主流方法，尤其在大规模数据集上表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="235-ShaLa-Multimodal-Shared-Latent-Space-Modellingpdf"><a href="#235-ShaLa-Multimodal-Shared-Latent-Space-Modellingpdf" class="headerlink" title="235. ShaLa: Multimodal Shared Latent Space Modellingpdf"></a>235. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ShaLa__Multimodal_Shared_Latent_Space_Modelling.pdf">ShaLa: Multimodal Shared Latent Space Modelling</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stevens Institute of Technology</span></p>
<p>本文提出了ShaLa，一种结合结构化推断模型和二阶段扩散先验的生成式框架，用于学习多模态数据间的共享潜在空间。ShaLa通过将各模态编码为确定性特征并融合为全局摘要，显著提升了多模态联合生成和跨模态推断的表现，尤其解决了传统多模态VAE在表达能力和先验匹配上的瓶颈。实验显示，ShaLa在多个公开多模态基准（如PolyMNIST、CUB、ShapeNet）上，相比最新多模态VAE模型在生成一致性、质量和可扩展性方面均取得了最优结果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="236-ShortListing-Model-A-Streamlined-Simplex-Diffusion-for-Discrete-Variable-Generationpdf"><a href="#236-ShortListing-Model-A-Streamlined-Simplex-Diffusion-for-Discrete-Variable-Generationpdf" class="headerlink" title="236. ShortListing Model: A Streamlined Simplex Diffusion for Discrete Variable Generationpdf"></a>236. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ShortListing_Model__A_Streamlined_SimplexDiffusion_for_Discrete_Variable_Generation.pdf">ShortListing Model: A Streamlined Simplex Diffusion for Discrete Variable Generation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出Shortlisting Model (SLM)，一种基于单纯形的扩散模型，创新性地将离散变量生成视为逐步候选剪枝过程。SLM通过在单纯形质心之间建模、简化交叉熵损失和灵活的classifier-free guidance，有效提升了大词表语言建模、生物序列设计等多项任务的生成质量和可扩展性。实验结果表明SLM在DNA&#x2F;蛋白设计和文本生成等离散数据领域表现优异，尤其在非自回归场景下超越现有主流方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="237-Learning-Short-Term-and-Long-Term-Patterns-of-High-Order-Dynamics-in-Real-World-Networkspdf"><a href="#237-Learning-Short-Term-and-Long-Term-Patterns-of-High-Order-Dynamics-in-Real-World-Networkspdf" class="headerlink" title="237. Learning Short-Term and Long-Term Patterns of High-Order Dynamics in Real-World Networkspdf"></a>237. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Learning_Short-Term_and_Long-Term_Patterns_of_High-Order_Dynamics_in_Real-World_Networks.pdf">Learning Short-Term and Long-Term Patterns of High-Order Dynamics in Real-World Networks</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chung-Ang University</span></p>
<p>该论文提出了一种动态超图学习方法Lincoln，通过双交互超边编码捕捉快照内高阶关系的结构与时序模式，并通过周期性时间注入和中间节点表示学习跨快照的长期周期性模式。实验表明Lincoln在七个真实数据集上的动态超边预测任务中显著优于九种现有方法，能更有效捕捉高阶动态关系。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="238-Scaling-Graph-Transformers-A-Comparative-Study-of-Sparse-and-Dense-Attentionpdf"><a href="#238-Scaling-Graph-Transformers-A-Comparative-Study-of-Sparse-and-Dense-Attentionpdf" class="headerlink" title="238. Scaling Graph Transformers: A Comparative Study of Sparse and Dense Attentionpdf"></a>238. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scaling_Graph_Transformers__A_Comparative_Study_of_Sparse_and_Dense_Attention.pdf">Scaling Graph Transformers: A Comparative Study of Sparse and Dense Attention</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent, Munich, Germany</span></p>
<p>本文系统比较了图Transformer中稠密与稀疏注意力机制，分析了Graphormer、SAN（稠密）、GraphGPS和Exphormer（稀疏）等代表性模型的结构、效率和性能。结论表明，稠密注意力适用于小型图，能捕捉全局依赖但计算量大，而稀疏注意力适合大规模图，具备更好可扩展性且在准确率上表现优秀。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="239-Aligning-the-Evaluation-of-Probabilistic-Predictions-with-Downstream-Valuepdf"><a href="#239-Aligning-the-Evaluation-of-Probabilistic-Predictions-with-Downstream-Valuepdf" class="headerlink" title="239. Aligning the Evaluation of Probabilistic Predictions with Downstream Valuepdf"></a>239. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Aligning_the_Evaluation_of_Probabilistic_Predictions_with_Downstream_Value.pdf">Aligning the Evaluation of Probabilistic Predictions with Downstream Value</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Tartu</span></p>
<p>本文提出了将概率预测的评估与下游任务价值对齐的新方法：将评估对齐问题形式化，并基于加权评分规则理论，设计利用神经网络学习的加权函数，实现与特定下游任务价值的自动对齐。通过合成数据和真实数据回归实验，验证了该方法能有效缩小传统评估指标与实际下游收益之间的差距，显著提升模型筛选与优化的实际效用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="240-PCR-CA-Parallel-Codebook-Representations-with-Contrastive-Alignment-for-Multiple-Category-App-Recommendationpdf"><a href="#240-PCR-CA-Parallel-Codebook-Representations-with-Contrastive-Alignment-for-Multiple-Category-App-Recommendationpdf" class="headerlink" title="240. PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendationpdf"></a>240. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PCR-CA__Parallel_Codebook_Representations_with_Contrastive_Alignment_for_Multiple-Category_App_Recom.pdf">PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Microsoft Store</span></p>
<p>本文提出PCR-CA框架，通过并行码本VQ-AE对多模态文本特征进行离散化编码，并利用对比对齐提升语义与协同嵌入空间的一致性。结合双注意力机制融合ID与内容特征，实现对多类别、长尾应用的更准确推荐。实验证明在大规模数据和线上A&#x2F;B测试中，PCR-CA在AUC和CTR等指标均显著优于现有方法，有效提升长尾与多类别App推荐效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="241-Frozen-in-Time-Parameter-Efficient-Time-Series-Transformers-via-Reservoir-Induced-Feature-Expansion-and-Fixed-Random-Dynamicspdf"><a href="#241-Frozen-in-Time-Parameter-Efficient-Time-Series-Transformers-via-Reservoir-Induced-Feature-Expansion-and-Fixed-Random-Dynamicspdf" class="headerlink" title="241. Frozen in Time: Parameter-Efficient Time Series Transformers via Reservoir-Induced Feature Expansion and Fixed Random Dynamicspdf"></a>241. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Frozen_in_Time__Parameter-Efficient_Time_Series_Transformers_via_Reservoir-Induced_Feature_Expansion.pdf">Frozen in Time: Parameter-Efficient Time Series Transformers via Reservoir-Induced Feature Expansion and Fixed Random Dynamics</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IIT Roorkee</span></p>
<p>该论文提出FreezeTST，一种通过在时间序列Transformer中交替插入冻结的随机特征（reservoir）块与可训练层的混合架构。冻结层提供无优化成本的非线性记忆，可训练层通过自注意力机制自适应检索这些记忆，实现参数数量和训练时间的大幅下降。实验证明，在七个长时序预测基准数据集上，FreezeTST在精度上与最佳专用Transformer模型（如PatchTST、Informer等）持平或略有超越，同时大幅减少可训练参数和训练时间，理论分析还保证了梯度稳定性和可控记忆长度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="242-Dynamic-Fusion-Multimodal-Network-for-SpeechWellness-Detectionpdf"><a href="#242-Dynamic-Fusion-Multimodal-Network-for-SpeechWellness-Detectionpdf" class="headerlink" title="242. Dynamic Fusion Multimodal Network for SpeechWellness Detectionpdf"></a>242. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dynamic_Fusion_Multimodal_Network_for_SpeechWellness_Detection.pdf">Dynamic Fusion Multimodal Network for SpeechWellness Detection</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northwestern Polytechnical University</span></p>
<p>本文提出了一种轻量级多分支多模态网络用于SpeechWellness检测，整合了时域、时频域声学特征和文本特征，并通过动态融合机制自适应调整各模态权重。实验结果表明，该方法在参数量减少78%的同时，准确率较官方基线提高5%，验证了多模态融合与动态加权的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="243-Training-Transformers-for-Mesh-Based-Simulationspdf"><a href="#243-Training-Transformers-for-Mesh-Based-Simulationspdf" class="headerlink" title="243. Training Transformers for Mesh-Based Simulationspdf"></a>243. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Training_Transformers_for_Mesh-Based_Simulations.pdf">Training Transformers for Mesh-Based Simulations</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mines Paris - PSL University</span></p>
<p>本文提出了一种新颖的基于Transformer的图神经网络架构，用于高效模拟复杂物理过程，利用邻接矩阵作为稀疏注意力掩码，并通过膨胀滑动窗口、全局注意力和随机连接扩展感受野。综合实验表明，该方法在大规模3D网格流体动力学数据集上显著提升了仿真精度和效率，最大模型对比SOTA平均提升38.8%，最小模型比MeshGraphNet快7倍且更小。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="244-Enhancing-Differentially-Private-Linear-Regression-via-Public-Second-Momentpdf"><a href="#244-Enhancing-Differentially-Private-Linear-Regression-via-Public-Second-Momentpdf" class="headerlink" title="244. Enhancing Differentially Private Linear Regression via Public Second-Momentpdf"></a>244. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Enhancing_Differentially_Private_Linear_Regression_via_Public_Second-Moment.pdf">Enhancing Differentially Private Linear Regression via Public Second-Moment</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The School of Mathematics, Northwest University</span></p>
<p>本文提出了一种基于公共二阶矩阵变换的差分隐私线性回归方法，通过利用公共数据的二阶矩阵对私有数据进行变换，改善了充分统计扰动法（SSP-OLSE）在无界数据下的数值稳定性和回归精度。理论和实验结果表明，该方法在保证差分隐私的同时，相较于传统SSP-OLSE具有更好的鲁棒性和更低的误差界。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="245-A-Novel-Framework-for-Uncertainty-Quantification-via-Proper-Scores-for-Classification-and-Beyondpdf"><a href="#245-A-Novel-Framework-for-Uncertainty-Quantification-via-Proper-Scores-for-Classification-and-Beyondpdf" class="headerlink" title="245. A Novel Framework for Uncertainty Quantification via Proper Scores for Classification and Beyondpdf"></a>245. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Novel_Framework_for_Uncertainty_Quantification_via_Proper_Scores_for_Classification_and_Beyond.pdf">A Novel Framework for Uncertainty Quantification via Proper Scores for Classification and Beyond</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Johann Wolfgang Goethe-Universität</span></p>
<p>本文提出了一种基于Proper Scores的通用不确定性量化理论框架，统一了回归、分类及生成建模等多种任务中的不确定性估计与模型校准，并通过理论联系Bregman信息、偏差-方差分解、校准误差等核心量。方法上，论文系统推广了Proper Scores在偏差-方差分解、核方法、生成模型多领域中的不确定性估计，提出了新型校准误差估计器和评估方法，并在多个实际任务如图像、音频和大语言模型生成中进行了实证验证。结论显示该框架能有效提升模型不确定性估计的可靠性与泛化性，在大语言模型等任务中超越现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="246-Spectrum-Prediction-in-the-Fractional-Fourier-Domain-with-Adaptive-Filteringpdf"><a href="#246-Spectrum-Prediction-in-the-Fractional-Fourier-Domain-with-Adaptive-Filteringpdf" class="headerlink" title="246. Spectrum Prediction in the Fractional Fourier Domain with Adaptive Filteringpdf"></a>246. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Spectrum_Prediction_in_the_Fractional_Fourier_Domain_with_Adaptive_Filtering.pdf">Spectrum Prediction in the Fractional Fourier Domain with Adaptive Filtering</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Aeronautics and Astronautics</span></p>
<p>本文提出了一种光谱预测新框架SFFP，核心方法包括自适应分数阶傅里叶变换（FrFT）模块将数据映射到最优分数阶傅里叶域以增强趋势与噪声的可分性，随后采用自适应滤波模块保留关键预测特征、抑制噪声，最后利用复值神经网络在该域内完成趋势预测。实验证明SFFP在实际光谱数据集上显著优于现有主流光谱预测和时间序列预测方法，实现了更高精度的预测效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="247-FasterVoiceGrad-Faster-One-step-Diffusion-Based-Voice-Conversion-with-Adversarial-Diffusion-Conversion-Distillationpdf"><a href="#247-FasterVoiceGrad-Faster-One-step-Diffusion-Based-Voice-Conversion-with-Adversarial-Diffusion-Conversion-Distillationpdf" class="headerlink" title="247. FasterVoiceGrad: Faster One-step Diffusion-Based Voice Conversion with Adversarial Diffusion Conversion Distillationpdf"></a>247. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FasterVoiceGrad__Faster_One-step_Diffusion-Based_Voice_Conversion_with_Adversarial_Diffusion_Convers.pdf">FasterVoiceGrad: Faster One-step Diffusion-Based Voice Conversion with Adversarial Diffusion Conversion Distillation</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">NTT, Inc., Japan</span></p>
<p>本文提出了FasterVoiceGrad，一种通过对扩散模型和内容编码器同时进行蒸馏、结合对抗性扩散转换蒸馏（ADCD）的单步扩散语音转换方法。该方法显著提升了语音转换速度，在保证音质和说话人相似度的前提下，在GPU和CPU上分别实现了6.6倍和1.8倍的加速。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="248-Ada-TransGNN-An-Air-Quality-Prediction-Model-Based-On-Adaptive-Graph-Convolutional-Networkspdf"><a href="#248-Ada-TransGNN-An-Air-Quality-Prediction-Model-Based-On-Adaptive-Graph-Convolutional-Networkspdf" class="headerlink" title="248. Ada-TransGNN: An Air Quality Prediction Model Based On Adaptive Graph Convolutional Networkspdf"></a>248. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Ada-TransGNN__An_Air_Quality_Prediction_Model_Based_On_Adaptive_Graph_Convolutional_Networks.pdf">Ada-TransGNN: An Air Quality Prediction Model Based On Adaptive Graph Convolutional Networks</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">East China University of Science and Technology</span></p>
<p>本文提出了Ada-TransGNN，一种基于Transformer和自适应图卷积网络的空气质量预测方法，通过引入多头注意力机制和图卷积模块，动态提取复杂时空依赖特征，并自适应学习最优图结构捕捉监测点间空间关系。此外，辅助任务学习模块结合Moran系数增强空间自相关性建模。实验表明，该方法在短期和长期空气质量预测上均优于现有主流模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="249-EEG-FM-Bench-A-Comprehensive-Benchmark-for-the-Systematic-Evaluation-of-EEG-Foundation-Modelspdf"><a href="#249-EEG-FM-Bench-A-Comprehensive-Benchmark-for-the-Systematic-Evaluation-of-EEG-Foundation-Modelspdf" class="headerlink" title="249. EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Modelspdf"></a>249. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/EEG-FM-Bench__A_Comprehensive_Benchmark_for_the_Systematic_Evaluation_of_EEG_Foundation_Models.pdf">EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tongji University</span></p>
<p>本文提出了EEG-FM-Bench，这是首个面向脑电基础模型（EEG-FMs）的系统化标准化评测基准，包括14个数据集、10类典型任务，统一数据预处理、评测协议和开放源码工具链，并对五种主流EEG-FM模型在多种微调策略下进行定量和定性分析。实验表明，多任务学习、精细化时空特征建模及融入神经生理先验能显著提升模型泛化能力，现有主流基础模型在冻结骨干时表现出较大泛化缺口，需改进高层语义表征学习。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="250-Copyright-Protection-for-3D-Molecular-Structures-with-Watermarkingpdf"><a href="#250-Copyright-Protection-for-3D-Molecular-Structures-with-Watermarkingpdf" class="headerlink" title="250. Copyright Protection for 3D Molecular Structures with Watermarkingpdf"></a>250. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Copyright_Protection_for_3D_Molecular_Structures_with_Watermarking.pdf">Copyright Protection for 3D Molecular Structures with Watermarking</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">City University of Hong Kong</span></p>
<p>本文提出了首个针对分子结构的AI驱动水印技术，通过利用原子级节点和边特征，在保持分子属性和结构完整性的前提下嵌入水印，并采用SE(3)不变特征与多维尺度缩放（MDS）增强对旋转、平移、反射等仿射变换的鲁棒性。实验结果表明，该方法在主流分子生成数据集和模型上能以高于95%准确率提取水印，同时分子基本属性损失极小且下游分子对接功能保持稳定，有效实现了分子知识产权保护。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="251-Text-Meets-Topology-Rethinking-Out-of-distribution-Detection-in-Text-Rich-Networkspdf"><a href="#251-Text-Meets-Topology-Rethinking-Out-of-distribution-Detection-in-Text-Rich-Networkspdf" class="headerlink" title="251. Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networkspdf"></a>251. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Text_Meets_Topology__Rethinking_Out-of-distribution_Detection_in_Text-Rich_Networks.pdf">Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networks</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Queensland</span></p>
<p>本文提出TextTopoOOD，一个系统性评测文本丰富网络（TrN）中分布外检测（OOD）的多维度框架，涵盖属性、结构、主题标签和领域等多种分布偏移，同时提出TNT-OOD方法，利用结构编码器、跨模态注意力机制和超网络动态融合节点文本与拓扑特征，提升了ID&#x2F;OOD区分能力。实验表明，TNT-OOD在11个TrN数据集和4类OOD场景中均优于现有方法，有效应对复杂的文本-结构耦合分布偏移挑战。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="252-On-the-Edge-of-Memorization-in-Diffusion-Modelspdf"><a href="#252-On-the-Edge-of-Memorization-in-Diffusion-Modelspdf" class="headerlink" title="252. On the Edge of Memorization in Diffusion Modelspdf"></a>252. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/On_the_Edge_of_Memorization_in_Diffusion_Models.pdf">On the Edge of Memorization in Diffusion Models</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">TTIC</span></p>
<p>本文提出了一个理论实验平台，用于研究扩散模型中的记忆化与泛化现象。作者通过在可控的高斯混合分布下，系统分析了模型容量、数据复杂度与泛化&#x2F;记忆化之间的临界点，理论上推导并实验验证了模型参数量增大时记忆化行为的发生转变点，并能精确预测这一转变。结论表明，扩散模型记忆化的临界点可通过训练损失的理论计算精确预测，该理论适用于分析和实证扩散模型泛化与记忆化行为。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="253-Characterizing-the-Behavior-of-Training-Mamba-based-State-Space-Models-on-GPUspdf"><a href="#253-Characterizing-the-Behavior-of-Training-Mamba-based-State-Space-Models-on-GPUspdf" class="headerlink" title="253. Characterizing the Behavior of Training Mamba-based State Space Models on GPUspdf"></a>253. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Characterizing_the_Behavior_of_Training_Mamba-based_State_Space_Models_on_GPUs.pdf">Characterizing the Behavior of Training Mamba-based State Space Models on GPUs</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Advanced Micro Devices (AMD)</span></p>
<p>本文系统分析了Mamba-based State Space Models (SSMs) 在GPU上的训练行为，通过构建多种代表性模型（包括语言、视觉、混合和图结构任务）并在NVIDIA H100上进行性能剖析，揭示了SSM关键算子在计算和内存上的瓶颈，并指出大部分算子受限于内存带宽而非算力。结论认为，要提升Mamba类模型的训练效率，需要在硬件和软件上针对内存带宽与算子执行路径进行联合优化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="254-Longitudinal-Progression-Prediction-of-Alzheimer’s-Disease-with-Tabular-Foundation-Modelpdf"><a href="#254-Longitudinal-Progression-Prediction-of-Alzheimer’s-Disease-with-Tabular-Foundation-Modelpdf" class="headerlink" title="254. Longitudinal Progression Prediction of Alzheimer’s Disease with Tabular Foundation Modelpdf"></a>254. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Longitudinal_Progression_Prediction_of_Alzheimer's_Disease_with_Tabular_Foundation_Model.pdf">Longitudinal Progression Prediction of Alzheimer’s Disease with Tabular Foundation Model</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Emory University</span></p>
<p>本文提出L2C-TabPFN方法，将纵向到横截面（L2C）特征变换与预训练Tabular Foundation Model（TabPFN）结合，应用于TADPOLE多模态数据集以预测阿尔茨海默病的诊断、认知评分及脑室体积。实验表明，该方法在脑室体积预测上优于XGBoost基线，能更好捕捉与疾病进展相关的结构性脑变化，展示了TabPFN模型在医学回归任务中的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="255-Quantum-Graph-Attention-Network-A-Novel-Quantum-Multi-Head-Attention-Mechanism-for-Graph-Learningpdf"><a href="#255-Quantum-Graph-Attention-Network-A-Novel-Quantum-Multi-Head-Attention-Mechanism-for-Graph-Learningpdf" class="headerlink" title="255. Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learningpdf"></a>255. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Quantum_Graph_Attention_Network__A_Novel_Quantum_Multi-Head_Attention_Mechanism_for_Graph_Learning.pdf">Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea Advanced Institute of Science and Technology (KAIST)</span></p>
<p>该论文提出了一种混合型的量子图注意力网络（QGAT），将变分量子电路集成到图神经网络的注意力机制中，通过振幅编码和强纠缠量子层，实现高效的多头注意力与非线性特征交互。实验结果表明，QGAT在节点分类和链路预测等任务上，对比经典GAT与GATv2表现出更优的精度和更强的抗噪鲁棒性，尤其在复杂和有噪声的图结构下展现出优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="256-The-Statistical-Fairness-Accuracy-Frontierpdf"><a href="#256-The-Statistical-Fairness-Accuracy-Frontierpdf" class="headerlink" title="256. The Statistical Fairness-Accuracy Frontierpdf"></a>256. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_Statistical_Fairness-Accuracy_Frontier.pdf">The Statistical Fairness-Accuracy Frontier</a><a href="paper.origin_link">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Rice University</span></p>
<p>该论文研究有限样本下的公平性与准确性（FA）前沿理论，提出了在已知和未知协方差情况下的最优估计器，量化了有限样本导致的公平性与准确性偏移，并给出了最优分配采样策略。结果表明有限数据会导致群体间风险不对称，影响公平性-准确性权衡，理论结论为实际公平算法设计和政策提供参考。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/zanwutupian.png" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/27报纸</div>
      <div>http://ywfhhh.github.io/2025/08/2025-08-27_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>具身ADT</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月27日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-26_article/" title="2025/08/26报纸">
                        <span class="hidden-mobile">2025/08/26报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
