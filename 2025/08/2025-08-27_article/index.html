

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="å…·èº«ADT">
  <meta name="keywords" content="">
  
    <meta name="description" content="æ¯æ—¥æœ€æ–°è®ºæ–‡åˆ†äº«">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;27æŠ¥çº¸">
<meta property="og:url" content="http://ywfhhh.github.io/2025/08/2025-08-27_article/index.html">
<meta property="og:site_name" content="æœ€æ–°å‰æ²¿è®ºæ–‡åˆ†äº«">
<meta property="og:description" content="æ¯æ—¥æœ€æ–°è®ºæ–‡åˆ†äº«">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GRASP_Geospatial_pixel_Reasoning_viA_Structured_Policy_learning.jpg">
<meta property="article:published_time" content="2025-08-27T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-27T07:07:26.354Z">
<meta property="article:author" content="å…·èº«ADT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GRASP_Geospatial_pixel_Reasoning_viA_Structured_Policy_learning.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/27æŠ¥çº¸ - æœ€æ–°å‰æ²¿è®ºæ–‡åˆ†äº«</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- ä¸»é¢˜ä¾èµ–çš„å›¾æ ‡åº“ï¼Œä¸è¦è‡ªè¡Œä¿®æ”¹ -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ywfhhh.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<!-- hexo injector head_end start -->
<script src="/js/custom.js"></script>
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>è®ºæ–‡åº“</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/27æŠ¥çº¸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-27 00:00" pubdate>
          æ˜ŸæœŸä¸‰, å…«æœˆ 27æ—¥ 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          44k å­—
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> æ¬¡
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/27æŠ¥çº¸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/27æŠ¥çº¸</h1></center>

<p>ğŸ“… <strong>æ—¥æœŸ</strong>ï¼š2025-08-27<br>ğŸ“„ <strong>å‘ç°è®ºæ–‡æ•°é‡</strong>ï¼š256  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17102v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      1. [GRASP: Geospatial pixel Reasoning viA Structured Policy learning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GRASP__Geospatial_pixel_Reasoning_viA_Structured_Policy_learning.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tsinghua University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºGRASPæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¾“å‡ºä¸ä»»åŠ¡ç›¸å…³çš„æ¡†æ¡†å’Œæ­£æ ·æœ¬ç‚¹ï¼Œä½œä¸ºæç¤ºä¼ é€’ç»™é¢„è®­ç»ƒåˆ†å‰²æ¨¡å‹ï¼Œæœ€ç»ˆç”Ÿæˆé¥æ„Ÿå›¾åƒçš„åˆ†å‰²æ©ç ã€‚ä¸ä»¥å¾€éœ€åƒç´ çº§ç›‘ç£çš„æ–¹æ¡ˆä¸åŒï¼ŒGRASPå®Œå…¨é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆGRPOç®—æ³•ï¼‰ä»…ç”¨æ¡†å’Œç‚¹çš„å¥–åŠ±ä¿¡å·ä¼˜åŒ–ï¼Œæ— éœ€æ©ç ç›‘ç£ï¼Œæå¤§é™ä½æ ‡æ³¨æˆæœ¬å¹¶æå‡æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒGRASPåœ¨å†…å¤–åŸŸæ•°æ®é›†ä¸Šå‡è¾¾SOTAï¼Œå†…åŸŸæå‡çº¦4%ï¼Œå¤–åŸŸæœ€é«˜æå‡54%ï¼Œè¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–å’Œç¨³å¥æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GRASP_Geospatial_pixel_Reasoning_viA_Structured_Policy_learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17087v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      2. [Solving the Min-Max Multiple Traveling Salesmen Problem via Learning-Based Path Generation and Optimal Splitting]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Solving_the_Min-Max_Multiple_Traveling_Salesmen_Problem_via_Learning-Based_Path_Generation_and_Optim.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Nanjing University</span></p>
<p>æœ¬æ–‡æå‡ºäº†Generate-and-Split (GaS) æ¡†æ¶ï¼Œé€šè¿‡LSTMå¢å¼ºçš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ç”ŸæˆåŸå¸‚è®¿é—®è·¯å¾„ï¼Œå¹¶ç»“åˆç¡®å®šæ€§è¿‘ä¼¼æœ€ä¼˜çš„åˆ†å‰²ç®—æ³•ï¼Œå°†è·¯å¾„åˆ†é…ç»™å¤šä½é”€å”®å‘˜ä»¥æœ€å°åŒ–æœ€é•¿æ—…è¡Œè·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGaSåœ¨è§£è´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰å­¦ä¹ å‹æ–¹æ³•ï¼Œå°¤å…¶åœ¨ä¸åŒè§„æ¨¡å’Œåˆ†å¸ƒçš„æ•°æ®ä¸Šè¡¨ç°æ›´ä½³ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Solving_the_Min-Max_Multiple_Traveling_Salesmen_Problem_via_Learning-Based_Path_Generation_and_Optimal_Splitting.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16915v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      3. [Reinforcement-Guided Hyper-Heuristic Hyperparameter Optimization for Fair and Explainable Spiking Neural Network-Based Financial Fraud Detection]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Reinforcement-Guided_Hyper-Heuristic_Hyperparameter_Optimization_for_Fair_and_Explainable_Spiking_Ne.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Khulna University of Engineering &amp; Technology (KUET)</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆçš®å±‚è„‰å†²ç¥ç»ç½‘ç»œï¼ˆCSNPCï¼‰ã€åŸºäºQ-learningçš„å¼ºåŒ–å¼•å¯¼è¶…å¯å‘å¼ä¼˜åŒ–å™¨ï¼ˆRHOSSï¼‰å’Œå¯è§£é‡Šæ€§AIï¼ˆXAIï¼‰æŠ€æœ¯çš„é‡‘èæ¬ºè¯ˆæ£€æµ‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡RHOSSåŠ¨æ€ä¼˜åŒ–SNNè¶…å‚æ•°ï¼ŒåµŒå…¥MoSSTIæ¡†æ¶ï¼Œåˆ©ç”¨æ˜¾è‘—æ€§å½’å› å’Œç¥ç»å…ƒè„‰å†²æ´»åŠ¨åˆ†æå®ç°æ¨¡å‹å¯è§£é‡Šä¸å…¬å¹³æ€§ï¼Œå®éªŒè¯æ˜åœ¨BAFæ•°æ®é›†ä¸‹æ¨¡å‹åœ¨5% FPRçº¦æŸä¸‹å¬å›ç‡è¾¾90.8%ï¼Œå…¬å¹³æ€§æŒ‡æ ‡è¶…98%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç»“è®ºè¡¨æ˜è¯¥æ–¹æ³•åœ¨æå‡è„‰å†²ç¥ç»ç½‘ç»œå…¬å¹³æ€§ã€æ€§èƒ½å’Œå¯è§£é‡Šæ€§ä¸Šæ•ˆæœçªå‡ºã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Reinforcement-Guided_Hyper-Heuristic_Hyperparameter_Optimization_for_Fair_and_Explainable_Spiking_Neural_Network-Based_Financial_Fraud_Detection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16876v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      4. [Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dream_to_Chat__Model-based_Reinforcement_Learning_on_Dialogues_with_User_Belief_Modeling.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Geely AI Lab</span></p>
<p>æœ¬æ–‡æå‡ºDreamCUBæ¡†æ¶ï¼Œå°†æ¨¡å‹é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ ï¼ˆMBRLï¼‰åº”ç”¨äºå¯¹è¯ç³»ç»Ÿï¼Œå¹¶å¼•å…¥ç”¨æˆ·ä¿¡å¿µå»ºæ¨¡ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿã€æƒ…ç»ªå’Œæ„å›¾è¯†åˆ«ã€‚å…³é”®æ–¹æ³•æ˜¯æ„å»ºå¯¹è¯ä¸–ç•Œæ¨¡å‹ï¼ˆDWMï¼‰ï¼Œé€šè¿‡POMDPç†è®ºå’Œä¿¡æ¯ç“¶é¢ˆä¼˜åŒ–å®ç°ç”¨æˆ·éšçŠ¶æ€æ¨ç†ï¼Œå¹¶ä¸å¯¹è¯ç­–ç•¥è”åˆè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œé¢„è®­ç»ƒçš„DWMåœ¨æƒ…æ„Ÿä¸æƒ…ç»ªè¯†åˆ«ä»»åŠ¡ä¸Šè¾¾åˆ°SOTAæ°´å¹³ï¼ŒDreamCUBåœ¨æ—¥å¸¸åŠåŒç†å¿ƒå¯¹è¯ä¸­å‡æ˜¾è‘—æå‡æƒ…æ„Ÿé¢„æµ‹ä¸å›å¤è´¨é‡ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Dream_to_Chat_Model-based_Reinforcement_Learning_on_Dialogues_with_User_Belief_Modeling.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17452v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      5. [ReviBranch: Deep Reinforcement Learning for Branch-and-Bound with Revived Trajectories]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ReviBranch__Deep_Reinforcement_Learning_for_Branch-and-Bound_with_Revived_Trajectories.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Hong Kong Baptist University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ReviBranchï¼Œä¸€ä¸ªé’ˆå¯¹æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆMILPï¼‰åˆ†æ”¯å®šç•Œç®—æ³•çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨Encoder-Revival-Decoderç»“æ„ï¼Œé€šè¿‡å†å²è½¨è¿¹å¤åŸå’Œå›¾-åºåˆ—è§£ç å™¨æ•æ‰åˆ†æ”¯å†³ç­–ä¸çŠ¶æ€çš„æ—¶ç©ºä¾èµ–ï¼Œå¹¶å¼•å…¥é‡è¦æ€§åŠ æƒå¥–åŠ±é‡åˆ†é…æœºåˆ¶ï¼Œå°†ç¨€ç–å¥–åŠ±è½¬åŒ–ä¸ºå¯†é›†åé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡å®ä¾‹ä¸Šä¼˜äºç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ç°äº†å¹³å‡B&amp;BèŠ‚ç‚¹æ•°é™ä½4.0%ã€LPè¿­ä»£æ¬¡æ•°å‡å°‘2.2%ï¼Œå…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ReviBranch_Deep_Reinforcement_Learning_for_Branch-and-Bound_with_Revived_Trajectories.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17448v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      6. [Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Rectified_Robust_Policy_Optimization_for_Model-Uncertain_Constrained_Reinforcement_Learning_without_.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Maryland, College Park</span></p>
<p>æœ¬æ–‡é’ˆå¯¹å­˜åœ¨æ¨¡å‹ä¸ç¡®å®šæ€§çš„é²æ£’çº¦æŸå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é—®é¢˜ï¼Œé¦–æ¬¡ç†è®ºæ€§åœ°è¯æ˜äº†è¯¥ç±»é—®é¢˜ä¸€èˆ¬ä¸æ»¡è¶³å¼ºå¯¹å¶æ€§ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„åŸå§‹-å¯¹å¶æ–¹æ³•å¯èƒ½æ— æ³•è·å¾—æœ€ä¼˜å¯è¡Œç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä»…åŸºäºåŸå§‹é—®é¢˜çš„ç®—æ³•RRPOï¼ˆRectified Robust Policy Optimizationï¼‰ï¼Œç›´æ¥åœ¨åŸå§‹ç©ºé—´ä¸Šè¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œå¹¶åœ¨æ¸©å’Œå‡è®¾ä¸‹ç»™å‡ºå…¨å±€æ”¶æ•›æ€§ç†è®ºä¿è¯ï¼Œå®éªŒéªŒè¯RRPOåœ¨ä¸åŒç¯å¢ƒä¸‹å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§å’Œå®‰å…¨æ€§ï¼Œèƒ½æœ‰æ•ˆè§„é¿æœ€åæƒ…å†µä¸‹çš„çº¦æŸè¿èƒŒã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Rectified_Robust_Policy_Optimization_for_Model-Uncertain_Constrained_Reinforcement_Learning_without_Strong_Duality.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17445v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      7. [TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TreePO__Bridging_the_Gap_of_Policy_Optimization_and_Efficacy_and_Inference_Efficiency_with_Heuristic.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">M-A-P</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†TreePOæ¡†æ¶ï¼Œå°†å¤§è¯­è¨€æ¨¡å‹çš„åºåˆ—ç”Ÿæˆè¿‡ç¨‹è½¬åŒ–ä¸ºæ ‘ç»“æ„æœç´¢ï¼Œç»“åˆåˆ†æ®µé‡‡æ ·ä¸åˆ†å±‚ä¼˜åŠ¿ä¼°è®¡ï¼Œå®ç°é«˜æ•ˆçš„æ¢ç´¢ä¸ç²¾ç¡®çš„ä¿¡ç”¨åˆ†é…ã€‚å®éªŒæ˜¾ç¤ºï¼ŒTreePOåœ¨å¤šé¡¹æ¨ç†åŸºå‡†ä¸Šåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒç¨³å®šæ€§å¹¶å‡å°‘äº†22%~43%çš„GPUè®¡ç®—æ¶ˆè€—ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ‰©å±•RLåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TreePO_Bridging_the_Gap_of_Policy_Optimization_and_Efficacy_and_Inference_Efficiency_with_Heuristic_Tree-based_Modeling.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17341v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      8. [MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MetaFed__Advancing_Privacy,_Performance,_and_Sustainability_in_Federated_Metaverse_Systems.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">KÄ±rÄ±kkale University</span></p>
<p>æœ¬æ–‡æå‡ºäº†MetaFedæ¡†æ¶ï¼Œå°†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰é©±åŠ¨çš„åŠ¨æ€èµ„æºç¼–æ’ã€åŒæ€åŠ å¯†ä¸å·®åˆ†éšç§ç»“åˆçš„è”é‚¦å­¦ä¹ ï¼Œä»¥åŠç¢³æ„ŸçŸ¥è°ƒåº¦é›†æˆäºå…ƒå®‡å®™ç¯å¢ƒä¸‹ï¼Œå®ç°éšç§ä¿æŠ¤ã€é«˜æ•ˆæ€§èƒ½å’Œç¯å¢ƒå¯æŒç»­æ€§ã€‚å®éªŒè¯æ˜ï¼ŒMetaFedåœ¨MNISTå’ŒCIFAR-10ç­‰åŸºå‡†æ•°æ®é›†ä¸Šèƒ½åœ¨ç»´æŒé«˜å‡†ç¡®ç‡å’Œä½é€šä¿¡å»¶è¿Ÿçš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘ç¢³æ’æ”¾ï¼ˆç›¸è¾ƒä¼ ç»Ÿæ–¹æ³•æœ€é«˜é™å¹…è¾¾25%ï¼‰ï¼Œä¸ºç°å®å…ƒå®‡å®™åº”ç”¨æä¾›å¯æ‰©å±•ã€ç»¿è‰²ä¸”éšç§åˆè§„çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MetaFed_Advancing_Privacy_Performance_and_Sustainability_in_Federated_Metaverse_Systems.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17262v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      9. [Federated Reinforcement Learning for Runtime Optimization of AI Applications in Smart Eyewears]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Federated_Reinforcement_Learning_for_Runtime_Optimization_of_AI_Applications_in_Smart_Eyewears.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Politecnico di Milano</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè”é‚¦å¼ºåŒ–å­¦ä¹ ï¼ˆFRLï¼‰çš„ä¸‰å±‚DNNåˆ†å±‚åˆ†å‰²ä¸ä»»åŠ¡å¸è½½ä¼˜åŒ–æ¡†æ¶ï¼Œé’ˆå¯¹æ™ºèƒ½çœ¼é•œï¼ˆSEWï¼‰åœ¨æœ‰é™ç®—åŠ›ã€èƒ½è€—å’Œç½‘ç»œæ³¢åŠ¨ä¸‹çš„AIåº”ç”¨è¿è¡Œæ—¶ä¼˜åŒ–ã€‚æ–¹æ³•é‡‡ç”¨åŒæ­¥ä¸å¼‚æ­¥FRLç­–ç•¥ï¼Œåˆ†å¸ƒå¼å¤šä»£ç†é€šè¿‡æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ååŒè®­ç»ƒï¼Œå®ç°å¤šç›®æ ‡ï¼ˆå»¶è¿Ÿã€èƒ½è€—ã€è´¹ç”¨ï¼‰è‡ªé€‚åº”å†³ç­–ã€‚å®éªŒè¡¨æ˜ï¼ŒFRLæ–¹æ³•ç¨³å®šæ€§é«˜ã€æ³›åŒ–èƒ½åŠ›å¼ºã€é€‚åº”å¤šç§AIæ¨¡å‹ï¼ˆå¦‚YOLOv5&#x2F;YOLOv8ï¼‰ï¼Œä¸”åœ¨å¼‚æ­¥ç¯å¢ƒä¸‹ä¹Ÿèƒ½ä¿æŒä½å»¶è¿Ÿè¿è§„ç‡ï¼Œä¼˜äºä¼ ç»Ÿåˆ†å‰²åŸºçº¿ç®—æ³•ã€‚ç»“è®ºï¼šFRLå¯åœ¨åŠ¨æ€ç¯å¢ƒä¸‹æ˜¾è‘—æå‡å¯ç©¿æˆ´AIè®¾å¤‡çš„å®æ—¶æ€§ä¸ç¨³å®šæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Federated_Reinforcement_Learning_for_Runtime_Optimization_of_AI_Applications_in_Smart_Eyewears.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17218v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      10. [GPG-HT: Generalized Policy Gradient with History-Aware Decision Transformer for Probabilistic Path Planning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GPG-HT__Generalized_Policy_Gradient_with_History-Aware_Decision_Transformer_for_Probabilistic_Path_P.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Sichuan University</span></p>
<p>æœ¬æ–‡æå‡ºGPG-HTæ¡†æ¶ï¼Œå°†å†å²æ„ŸçŸ¥å†³ç­–Transformerä¸å¹¿ä¹‰ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ç»“åˆï¼Œé’ˆå¯¹å…·æœ‰ä¾èµ–å…³ç³»çš„éšæœºäº¤é€šç½‘ç»œä¸­çš„æ¦‚ç‡è·¯å¾„è§„åˆ’é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡Transformerå»ºæ¨¡è½¦è¾†å†å²è½¨è¿¹å’Œå‰©ä½™æ—¶é—´é¢„ç®—ï¼Œé‡‡ç”¨GPGä¼˜åŒ–å®ç°éé©¬å°”å¯å¤«ç¯å¢ƒä¸‹é«˜æ•ˆè·¯å¾„å†³ç­–ï¼Œå¹¶åœ¨Sioux Falls Networkç­‰åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†å‡†æ—¶åˆ°è¾¾æ¦‚ç‡ï¼Œä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GPG-HT_Generalized_Policy_Gradient_with_History-Aware_Decision_Transformer_for_Probabilistic_Path_Planning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17212v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      11. [Reinforcement Learning enhanced Online Adaptive Clinical Decision Support via Digital Twin powered Policy and Treatment Effect optimized Reward]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Reinforcement_Learning_enhanced_Online_Adaptive_Clinical_Decision_Support_via_Digital_Twin_powered_P.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Houston</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ ã€æ•°å­—å­ªç”Ÿå’Œæ²»ç–—æ•ˆæœä¼˜åŒ–çš„æ–°å‹åœ¨çº¿è‡ªé€‚åº”ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿã€‚æ–¹æ³•æµç¨‹åŒ…æ‹¬ï¼šç¦»çº¿ç”¨æ‰¹çº¦æŸå¼ºåŒ–å­¦ä¹ ï¼ˆBCQï¼‰å’Œæ•°å­—å­ªç”Ÿè®­ç»ƒç­–ç•¥å’Œç¯å¢ƒï¼Œåœ¨çº¿é˜¶æ®µé‡‡ç”¨Qç½‘ç»œé›†æˆåˆ¤å®šä¸ç¡®å®šæ€§ï¼Œç»“åˆä¸»åŠ¨å­¦ä¹ ä¸å®‰å…¨è§„åˆ™å®ç°é«˜æ•ˆä½é£é™©çš„äººæœºåä½œå†³ç­–ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨åˆæˆä¸´åºŠç¯å¢ƒä¸­å®ç°äº†ä½å»¶è¿Ÿã€é«˜å®‰å…¨æ€§ã€ä¸“å®¶æŸ¥è¯¢ç‡ä½å’Œæ”¶ç›Šä¼˜äºä¸»æµåŸºçº¿æ–¹æ³•ã€‚ç»“è®ºï¼šè¯¥ç³»ç»Ÿèƒ½åœ¨ä¿è¯å®‰å…¨çš„å‰æä¸‹å¿«é€Ÿé€‚åº”åˆ†å¸ƒå˜åŒ–ï¼Œæ˜¾è‘—é™ä½ä¸“å®¶è´Ÿæ‹…å¹¶æå‡ä¸´åºŠå†³ç­–æ•ˆç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Reinforcement_Learning_enhanced_Online_Adaptive_Clinical_Decision_Support_via_Digital_Twin_powered_Policy_and_Treatment_Effect_optimized_Reward.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18258v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      12. [ANO: Faster is Better in Noisy Landscapes]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ANO___Faster_is_Better_in_Noisy_Landscape.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Independent Researcher</span></p>
<p>æœ¬æ–‡æå‡ºäº†Anoï¼Œä¸€ç§æ–°çš„ä¼˜åŒ–å™¨ï¼Œé€šè¿‡å°†æ›´æ–°çš„æ–¹å‘ï¼ˆç”±åŠ¨é‡å¹³æ»‘ï¼‰ä¸æ­¥é•¿ï¼ˆç”±ç¬æ—¶æ¢¯åº¦å¹…å€¼å†³å®šï¼‰è§£è€¦ï¼Œå¢å¼ºäº†å¯¹å™ªå£°å’Œéå¹³ç¨³æ¢¯åº¦çš„é²æ£’æ€§ï¼Œä¿ç•™äº†ä¸€é˜¶è‡ªé€‚åº”æ–¹æ³•çš„é«˜æ•ˆä¸ç®€æ´ã€‚æ­¤å¤–ï¼Œæå‡ºäº†Anologï¼Œé€šè¿‡å¯¹åŠ¨é‡ç³»æ•°é‡‡ç”¨å¯¹æ•°è°ƒåº¦è¿›ä¸€æ­¥æå‡è¶…å‚æ•°é²æ£’æ€§ã€‚ç†è®ºä¸Šï¼ŒAnoåœ¨éå‡¸éšæœºä¼˜åŒ–ä¸‹å…·æœ‰O(K^{-1&#x2F;4})çš„æ”¶æ•›ç‡ï¼Œå¹¶åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ å’Œé«˜å™ªå£°NLPä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¸»æµä¼˜åŒ–å™¨ï¼ŒåŒæ—¶åœ¨æ ‡å‡†è§†è§‰ä»»åŠ¡ä¸Šä¹Ÿå…·å¤‡ç«äº‰åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ANO_Faster_is_Better_in_Noisy_Landscapes.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18032v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      13. [Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Visual-CoG__Stage-Aware_Reinforcement_Learning_with_Chain_of_Guidance_for_Text-to-Image_Generation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Alibaba Group</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ æ–°æ¡†æ¶Visual-CoGï¼Œé€šè¿‡è¯­ä¹‰æ¨ç†ã€è¿‡ç¨‹ç»†åŒ–å’Œç»“æœè¯„ä¼°ä¸‰ä¸ªé˜¶æ®µï¼Œç»“åˆé˜¶æ®µæ€§å¥–åŠ±æœºåˆ¶å®ç°å…¨æµç¨‹å³æ—¶ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨GenEvalã€T2I-CompBenchå’ŒVisCog-Benchç­‰å¤šé¡¹åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†å¤šå±æ€§å’Œæ¨¡ç³ŠæŒ‡ä»¤ä¸‹çš„ç”Ÿæˆæ•ˆæœï¼Œå°¤å…¶å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†ä¸è¯­ä¹‰å¯¹é½èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Visual-CoG_Stage-Aware_Reinforcement_Learning_with_Chain_of_Guidance_for_Text-to-Image_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17751v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      14. [Multi-layer Abstraction for Nested Generation of Options (MANGO) in Hierarchical Reinforcement Learning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multi-layer_Abstraction_for_Nested_Generation_of_Options_(MANGO)_in_Hierarchical_Reinforcement_Learn.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Human Inspired Technology Research Center, UniversitÃ  di Padova</span></p>
<p>æœ¬æ–‡æå‡ºäº†MANGOï¼ˆä¸€ç§å¤šå±‚æŠ½è±¡çš„åµŒå¥—é€‰é¡¹ç”Ÿæˆå±‚æ¬¡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼‰ï¼Œé€šè¿‡å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤šå±‚æŠ½è±¡ï¼Œæ¯å±‚å®šä¹‰æŠ½è±¡çŠ¶æ€ç©ºé—´å¹¶ç”¨é€‰é¡¹æ¨¡å—åŒ–è½¨è¿¹ï¼Œå®ç°äº†è·¨å±‚é€‰é¡¹åµŒå¥—å’Œé«˜æ•ˆé‡ç”¨ã€‚å®éªŒè¡¨æ˜ï¼ŒMANGOåœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹æ˜¾è‘—æå‡äº†æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¢å¼ºäº†å†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Multi-layer_Abstraction_for_Nested_Generation_of_Options_%28MANGO%29_in_Hierarchical_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17044v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      15. [M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/M3DMap__Object-aware_Multimodal_3D_Mapping_for_Dynamic_Environments.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Moscow Institute of Physics and Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†M3DMapï¼Œä¸€ç§é¢å‘åŠ¨æ€ç¯å¢ƒçš„å¯¹è±¡æ„ŸçŸ¥å¤šæ¨¡æ€3Då»ºå›¾çš„æ¨¡å—åŒ–æ–¹æ³•ï¼Œæ•´åˆäº†ç¥ç»ç½‘ç»œå®ç°çš„å¤šæ¨¡æ€å¯¹è±¡åˆ†å‰²ä¸è·Ÿè¸ªã€å¯è®­ç»ƒçš„é‡Œç¨‹è®¡ä¼°è®¡ã€3Dåœ°å›¾æ„å»ºä¸æ›´æ–°ä»¥åŠå¤šæ¨¡æ€æ•°æ®æ£€ç´¢æ¨¡å—ã€‚å®éªŒå’Œç†è®ºè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶ç­‰å¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ç›®æ ‡è¯†åˆ«ã€å®šä½ã€å¯¼èˆªå’Œæ“ä½œï¼‰ä¸­èƒ½æ˜¾è‘—æå‡æ€§èƒ½ï¼Œå°¤å…¶åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®ä¸åŸºç¡€æ¨¡å‹èåˆå¸¦æ¥æ›´ä¼˜è¯†åˆ«å’Œå»ºå›¾æ•ˆæœã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/M3DMap_Object-aware_Multimodal_3D_Mapping_for_Dynamic_Environments.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17038v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      16. [A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatness]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Rapid_Iterative_Trajectory_Planning_Method_for_Automated_Parking_through_Differential_Flatness.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">State Key Laboratory of Industrial Control Technology, Zhejiang University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¾®åˆ†å¹³å¦æ€§çš„å¿«é€Ÿè¿­ä»£è½¨è¿¹è§„åˆ’ï¼ˆRITPï¼‰æ–¹æ³•ï¼Œç”¨äºè‡ªåŠ¨æ³Šè½¦åœºæ™¯ä¸­çš„è·¯å¾„-é€Ÿåº¦åˆ†è§£ã€‚è¯¥æ–¹æ³•é€šè¿‡å¹¶è¡Œè®¡ç®—å’Œæ˜¾å¼ç¢°æ’æ£€æµ‹å®ç°é«˜æ•ˆã€å®æ—¶çš„ç¢°æ’è§„é¿ï¼Œåˆ©ç”¨å¤šé¡¹å¼è·¯å¾„è§„åˆ’ç¡®ä¿è½¦è¾†è¿åŠ¨å­¦æ¨¡å‹çº¦æŸï¼Œå¹¶åœ¨æ¢æŒ¡ç‚¹å¼•å…¥æœ«ç«¯å¹³æ»‘çº¦æŸæå‡æ§åˆ¶å¯è¡Œæ€§ã€‚ä»¿çœŸå’Œå®è½¦å®éªŒç»“æœè¡¨æ˜ï¼ŒRITPæ–¹æ³•åœ¨ä¿è¯å¼ºæ§åˆ¶å¯è¡Œæ€§å’Œç¢°æ’æ— çš„åŒæ—¶ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•çš„è®¡ç®—æ•ˆç‡å’Œè½¨è¿¹è·Ÿè¸ªæ€§èƒ½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Rapid_Iterative_Trajectory_Planning_Method_for_Automated_Parking_through_Differential_Flatness.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17034v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      17. [DualReg: Dual-Space Filtering and Reinforcement for Rigid Registration]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DualReg__Dual-Space_Filtering_and_Reinforcement_for_Rigid_Registration.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Science and Technology of China</span></p>
<p>æœ¬æ–‡æå‡ºDualRegï¼Œä¸€ç§é¢å‘åˆšæ€§ç‚¹äº‘é…å‡†çš„é«˜æ•ˆé²æ£’çš„åŒç©ºé—´ä¼˜åŒ–æ¡†æ¶ã€‚æ–¹æ³•ä¸Šï¼Œé¦–å…ˆè®¾è®¡äº†ä¸€ç§ç»“åˆå•ç‚¹RANSACå¿«é€Ÿè¿‡æ»¤å’Œä¸‰ç‚¹RANSACæ¦‚ç‡åŠ æƒé‡‡æ ·çš„ç‰¹å¾å¯¹åº”ç‚¹æ¸è¿›è¿‡æ»¤ç®—æ³•ï¼Œéšåä»¥è¿‡æ»¤åçš„ç‰¹å¾å¯¹åº”ç‚¹ä¸ºé”šç‚¹ï¼Œæ„å»ºå±€éƒ¨å‡ ä½•ä»£ç†ç‚¹é›†ï¼Œæå‡ºç‰¹å¾ç©ºé—´ä¸å‡ ä½•ç©ºé—´ååŒçš„ä¼˜åŒ–ç›®æ ‡å¹¶é«˜æ•ˆè¿­ä»£æ±‚è§£åˆšæ€§å˜æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨3DMatchã€3DLoMatchå’ŒKITTIç­‰æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡é…å‡†ç²¾åº¦å’Œé€Ÿåº¦ï¼Œéƒ¨åˆ†åœºæ™¯ä¸‹ç›¸æ¯”SOTAæ–¹æ³•æå‡32å€é€Ÿåº¦å¹¶ä¿æŒé«˜ç²¾åº¦ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/DualReg_Dual-Space_Filtering_and_Reinforcement_for_Rigid_Registration.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16987v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      18. [WEBSIGHT: A Vision-First Architecture for Robust Web Agents]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/WebSight__A_Vision-First_Architecture_for_Robust_Web_Agents.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Stanford University</span></p>
<p>WEBSIGHTæå‡ºäº†ä¸€ç§ä»¥è§†è§‰ä¸ºæ ¸å¿ƒçš„è‡ªä¸»ç½‘é¡µä»£ç†æ¶æ„ï¼Œé€šè¿‡è§†è§‰-è¯­è¨€æ¨¡å‹WEBSIGHT-7Bï¼Œä»…ä¾èµ–ç½‘é¡µæˆªå›¾å®ç°UIå…ƒç´ è¯†åˆ«ä¸äº¤äº’ï¼Œå®Œå…¨æ‘†è„±HTMLæˆ–DOMè¾“å…¥ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å¤šä»£ç†æ¡†æ¶ï¼ˆè§„åˆ’ã€æ¨ç†ã€è§†è§‰åŠ¨ä½œã€éªŒè¯ï¼‰ï¼Œé€šè¿‡æ¨¡å—åŒ–åä½œå’ŒçŸ­æœŸè®°å¿†æœºåˆ¶å®ç°äººç±»å¼ç½‘é¡µå¯¼èˆªï¼Œåœ¨Showdown Clickså’ŒWebVoyageråŸºå‡†ä¸Šè¶…è¶Šå¤šé¡¹ä¸šç•Œæ–¹æ¡ˆï¼Œä»»åŠ¡å‡†ç¡®ç‡è¾¾97.14%ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/WEBSIGHT_A_Vision-First_Architecture_for_Robust_Web_Agents.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16962v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      19. [LLM-based Human-like Traffic Simulation for Self-driving Tests]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LLM-based_Human-like_Traffic_Simulation_for_Self-driving_Tests.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Nanjing University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºHDSimï¼Œä¸€ç§ç»“åˆè®¤çŸ¥ç†è®ºä¸å¤§è¯­è¨€æ¨¡å‹(LLM)çš„é«˜çœŸå®äº¤é€šä»¿çœŸæ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚é©¾é©¶å‘˜é£æ ¼æ¨¡å‹å’Œæ„ŸçŸ¥ä»‹å¯¼è¡Œä¸ºå½±å“æœºåˆ¶ï¼ˆPMBIï¼‰ç”Ÿæˆå¤šæ ·ä¸”å¯æ‰©å±•çš„äººç±»é©¾é©¶è¡Œä¸ºã€‚å®éªŒè¯æ˜ï¼ŒHDSimå¯æå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨ä»¿çœŸæµ‹è¯•ä¸­å‘ç°å®‰å…¨éšæ‚£çš„èƒ½åŠ›ï¼ˆå¤±è´¥æ£€æµ‹æå‡è¾¾68%ï¼‰ï¼Œä¸”ç”Ÿæˆçš„äº‹æ•…ä¸çœŸå®ä¸–ç•Œé«˜åº¦ä¸€è‡´ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LLM-based_Human-like_Traffic_Simulation_for_Self-driving_Tests.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16947v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      20. [Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Model]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Drive_As_You_Like__Strategy-Level_Motion_Planning_Based_on_A_Multi-Head_Diffusion_Model.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Monash University Malaysia</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šå¤´æ‰©æ•£æ¨¡å‹ï¼ˆM-Diffusion Plannerï¼‰çš„äº¤äº’å¼è‡ªåŠ¨é©¾é©¶è¿åŠ¨è§„åˆ’æ¡†æ¶ï¼Œé€šè¿‡DiTå˜ä½“å®ç°å¤šç­–ç•¥è¾“å‡ºï¼Œå¹¶ç»“åˆGRPOï¼ˆGroup Relative Policy Optimizationï¼‰è¿›è¡Œç­–ç•¥å±‚é¢å¾®è°ƒï¼Œä½¿å„è¾“å‡ºå¤´å­¦ä¹ ä¸åŒé©¾é©¶é£æ ¼ã€‚æ¨ç†æ—¶å¼•å…¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯­ä¹‰è§£æå™¨ï¼Œå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„ä¸ºç­–ç•¥æ ‡è¯†ï¼Œå®ç°äº†ç”¨æˆ·æŒ‡ä»¤é©±åŠ¨çš„å®æ—¶å¤šç­–ç•¥åˆ‡æ¢ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨nuPlanåŸºå‡†ä¸Šè·å¾—SOTAæ€§èƒ½ï¼Œæ”¯æŒå¤šæ ·åŒ–ã€åå¥½åŒ–çš„å®æ—¶è½¨è¿¹ç”Ÿæˆã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Drive_As_You_Like_Strategy-Level_Motion_Planning_Based_on_A_Multi-Head_Diffusion_Model.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16856v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      21. [A Workflow for Map Creation in Autonomous Vehicle Simulations]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Workflow_for_Map_Creation_in_Autonomous_Vehicle_Simulations.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Ontario Tech University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘è‡ªåŠ¨é©¾é©¶ä»¿çœŸçš„åœ°å›¾åˆ›å»ºè‡ªå®šä¹‰å·¥ä½œæµï¼Œåˆ©ç”¨OpenStreetMapæ•°æ®å’Œè½»é‡çº§å¼€æºå·¥å…·ï¼ˆå¦‚OSM2Worldã€CloudCompareä¸Point Cloud Libraryï¼‰è‡ªåŠ¨ç”Ÿæˆ3Dç½‘æ ¼ã€ç‚¹äº‘å’ŒLanelet2åœ°å›¾ï¼Œç®€åŒ–äº†AWSIMä¸Autowareç­‰ä»¿çœŸå¹³å°çš„é«˜ç²¾åº¦åœ°å›¾é›†æˆæµç¨‹ã€‚å®éªŒåœ¨å®‰çœç†å·¥å¤§å­¦åœè½¦åœºç¯å¢ƒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“è®ºè®¤ä¸ºè¯¥æµç¨‹èƒ½é«˜æ•ˆã€ä½èµ„æºåœ°æ”¯æŒä»¿çœŸæµ‹è¯•ï¼Œæœªæ¥å°†å¼•å…¥SLAMå’Œå¢å¼ºå…¼å®¹æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Workflow_for_Map_Creation_in_Autonomous_Vehicle_Simulations.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16845v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      22. [NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/NinA__Normalizing_Flows_in_Action._Training_VLA_Models_with_Normalizing_Flows.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">AIRI</span></p>
<p>è¯¥è®ºæ–‡æå‡ºNinAï¼Œå°†ä¼ ç»ŸVision-Language-Action (VLA)æ¶æ„ä¸­çš„æ‰©æ•£æ¨¡å‹åŠ¨ä½œè§£ç å™¨æ›¿æ¢ä¸ºNormalizing Flowï¼ˆNFï¼‰æ¨¡å‹ï¼Œæ”¯æŒé«˜æ•ˆå•æ­¥é‡‡æ ·ï¼Œé€šè¿‡MLPå’ŒTransformerä¸¤ç§ç»“æ„å®ç°ï¼Œæå¤§æå‡æ¨ç†é€Ÿåº¦å¹¶å‡å°‘å‚æ•°é‡ã€‚å®éªŒè¯æ˜ï¼ŒNinAåœ¨LIBEROæœºå™¨äººåŸºå‡†ä¸Šå¯ä¸æ‰©æ•£æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œä½†æ¨ç†é€Ÿåº¦å¿«7-10å€ï¼Œæ¨¡å‹æ›´å°ï¼Œé€‚åˆå®æ—¶æœºå™¨äººæ§åˆ¶ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/NinA_Normalizing_Flows_in_Action._Training_VLA_Models_with_Normalizing_Flows.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17482v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      23. [Variational Shape Inference for Grasp Diffusion on SE(3)]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Variational_Shape_Inference_for_Grasp_Diffusion_on_SE(3).pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Purdue University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå˜åˆ†å½¢çŠ¶æ¨æ–­ä¸æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œå®ç°äº†åŸºäºSE(3)æµå½¢çš„å¤šæ¨¡æ€æœºæ¢°æ‰‹æŠ“å–å§¿æ€ç”Ÿæˆã€‚å…·ä½“æµç¨‹ä¸ºï¼šå…ˆåˆ©ç”¨åŸºäºPointNetçš„å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨å¯¹ç‚¹äº‘è¿›è¡Œå½¢çŠ¶éšç©ºé—´å»ºæ¨¡ï¼Œè·å¾—ç¨³å¥çš„å‡ ä½•ç‰¹å¾ï¼Œå†ç”¨è¿™äº›ç‰¹å¾å¼•å¯¼æ‰©æ•£æ¨¡å‹å®ç°æŠ“å–å§¿æ€çš„ç”Ÿæˆï¼Œå¹¶å¼•å…¥åŸºäºç¥ç»SDFçš„æµ‹è¯•æ—¶æŠ“å–ä¼˜åŒ–ä»¥æå‡æŠ“å–è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ACRONYMæ•°æ®é›†ä¸ŠæŠ“å–æˆåŠŸç‡æå‡6.3%ï¼Œä¸”å¯¹ç‚¹äº‘ç¨€ç–å’Œå™ªå£°å…·å¤‡å¼ºé²æ£’æ€§ï¼Œåœ¨ç°å®åœºæ™¯ä¸‹å®ç°äº†é›¶æ ·æœ¬è¿ç§»ï¼ŒæŠ“å–æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Variational_Shape_Inference_for_Grasp_Diffusion_on_SE%283%29.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17481v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      24. [SoK: Cybersecurity Assessment of Humanoid Ecosystem]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SoK__Cybersecurity_Assessment_of_Humanoid_Ecosystem.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Ben-Gurion University of the Negev</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†é’ˆå¯¹äººå½¢æœºå™¨äººç”Ÿæ€ç³»ç»Ÿçš„ç³»ç»Ÿæ€§å®‰å…¨è¯„ä¼°æ–¹æ³•ï¼Œæ„å»ºäº†ä¸ƒå±‚å®‰å…¨æ¨¡å‹ï¼Œå°†39ç±»æ”»å‡»ä¸35ç±»é˜²å¾¡æœºåˆ¶æ˜ å°„åˆ°ä¸åŒæ¶æ„å±‚ï¼Œå¹¶å¼€å‘äº†RISK-MAPå®šé‡å®‰å…¨è¯„ä¼°ä½“ç³»ï¼ˆå«è’™ç‰¹å¡æ´›ä»¿çœŸï¼‰ï¼Œç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒäººå½¢æœºå™¨äººçš„å®‰å…¨æ°´å¹³ã€‚å®è¯åˆ†ææ­ç¤ºäº†ä¸‰ç§ä¸»æµå¹³å°åœ¨ç‰©ç†å±‚å’Œä¸Šå±‚é˜²æŠ¤ä¸Šçš„ç³»ç»Ÿæ€§æ¼æ´ï¼Œç»“è®ºå¼ºè°ƒç³»ç»ŸåŒ–ã€å¤šå±‚æ¬¡å®‰å…¨é˜²æŠ¤å¯¹äººå½¢æœºå™¨äººå®‰å…¨éƒ¨ç½²è‡³å…³é‡è¦ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SoK_Cybersecurity_Assessment_of_Humanoid_Ecosystem.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17468v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      25. [A Synthetic Dataset for Manometry Recognition in Robotic Applications]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Synthetic_Dataset_for_Manometry_Recognition_in_Robotic_Applications.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of SÃ£o Paulo</span></p>
<p>è¯¥è®ºæ–‡æå‡ºå¹¶éªŒè¯äº†ä¸€ç§ç”¨äºå·¥ä¸šåœºæ™¯ï¼ˆå¦‚æµ·ä¸ŠçŸ³æ²¹å¹³å°ï¼‰ä¸­å‹åŠ›è®¡è¯†åˆ«çš„æ··åˆæ•°æ®ç”Ÿæˆç®¡çº¿ï¼Œç»“åˆäº†BlenderProcç¨‹åºåŒ–æ¸²æŸ“ä¸NVIDIA Cosmos-Predict2 AIé©±åŠ¨è§†é¢‘æ‰©å±•ï¼Œç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒå’Œè§†é¢‘ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºçœŸå®ä¸åˆæˆæ•°æ®1:1æ··åˆè®­ç»ƒçš„YOLOæ£€æµ‹æ¨¡å‹åœ¨å‡†ç¡®ç‡å’Œè®­ç»ƒæ”¶æ•›é€Ÿåº¦å‡ä¼˜äºä»…ç”¨çœŸå®æ•°æ®ï¼Œåˆæˆæ•°æ®å¯æœ‰æ•ˆç¼“è§£æ•°æ®é‡‡é›†éš¾é¢˜å¹¶æå‡å·¥ä¸šæœºå™¨äººæ„ŸçŸ¥æ€§èƒ½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Synthetic_Dataset_for_Manometry_Recognition_in_Robotic_Applications.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17466v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      26. [Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Optimizing_Grasping_in_Legged_Robots__A_Deep_Learning_Approach_to_Loco-Manipulation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Federal University of Uberlandia</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å››è¶³æœºå™¨äººæŠ“å–ä¼˜åŒ–æ–¹æ³•ï¼Œæ ¸å¿ƒä¸ºåœ¨Genesisä»¿çœŸç¯å¢ƒä¸­ç”ŸæˆæŠ“å–æ•°æ®é›†ï¼Œé€šè¿‡èåˆRGB-Då›¾åƒã€åˆ†å‰²æ©ç å’Œæ³•çº¿å›¾è®­ç»ƒU-Netç»“æ„å·ç§¯ç¥ç»ç½‘ç»œï¼Œé¢„æµ‹åƒç´ çº§æŠ“å–è´¨é‡çƒ­åŠ›å›¾ã€‚è¯¥æ¨¡å‹åœ¨Boston Dynamics Spotå®ç‰©å¹³å°å®ç°ç«¯åˆ°ç«¯è‡ªä¸»å¯¼èˆªã€è¯†åˆ«ã€æŠ“å–ï¼ŒéªŒè¯äº†ä»¿çœŸåˆ°ç°å®ï¼ˆsim-to-realï¼‰ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œæå‡äº†æœºå™¨äººåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸‹çš„æŠ“å–ç²¾åº¦ä¸é€‚åº”æ€§ã€‚ç»“è®ºè¯å®æ‰€æç³»ç»Ÿèƒ½åœ¨ç°å®ç¯å¢ƒä¸­å®ç°é«˜æ•ˆçµæ´»çš„ç‰©ä½“æŠ“å–ï¼Œä½†æ¨¡å‹æ³›åŒ–æ€§å’Œæ„ŸçŸ¥é²æ£’æ€§æœ‰å¾…è¿›ä¸€æ­¥æå‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Optimizing_Grasping_in_Legged_Robots_A_Deep_Learning_Approach_to_Loco-Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17230v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      27. [4D Visual Pre-training for Robot Learning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/4D_Visual_Pre-training_for_Robot_Learning.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Peking University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘æœºå™¨äººæ“æ§çš„4Dè§†è§‰é¢„è®­ç»ƒæ¡†æ¶ï¼ˆFVPï¼‰ï¼Œé€šè¿‡å°†å†å²å¸§ç‚¹äº‘å’Œæœºå™¨äººåŠ¨ä½œä½œä¸ºæ¡ä»¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹é¢„æµ‹æœªæ¥ç‚¹äº‘ï¼Œä»è€Œé¢„è®­ç»ƒ3Dè§†è§‰è¡¨å¾ã€‚å®éªŒè¡¨æ˜ï¼ŒFVPèƒ½æ˜¾è‘—æå‡3Dæ¨¡ä»¿å­¦ä¹ ï¼ˆå¦‚DP3ã€RISEï¼‰å’Œè§†è§‰-è¯­è¨€-åŠ¨ä½œå¤§æ¨¡å‹ï¼ˆå¦‚RDT-1Bï¼‰åœ¨å¤šç§ç°å®æœºå™¨äººä»»åŠ¡ä¸­çš„æˆåŠŸç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºç°æœ‰2D&#x2F;3Dé¢„è®­ç»ƒæ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/4D_Visual_Pre-training_for_Robot_Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17198v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      28. [From reactive to cognitive: brain-inspired spatial intelligence for embodied agents]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/From_reactive_to_cognitive__brain-inspired_spatial_intelligence_for_embodied_agents.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tsinghua University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºBSC-Navæ¡†æ¶ï¼Œç»“åˆç”Ÿç‰©å¯å‘çš„ç»“æ„åŒ–ç©ºé—´è®°å¿†ï¼ˆåŒ…æ‹¬åœ°æ ‡ã€è·¯å¾„å’Œåœ°å›¾çŸ¥è¯†ï¼‰ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ç°äº†å…·å¤‡è®¤çŸ¥ç©ºé—´æ™ºèƒ½çš„è‡ªä¸»ä½“ã€‚æ–¹æ³•é€šè¿‡æ˜¾å¼æ„å»ºåœ°æ ‡è®°å¿†å’Œè®¤çŸ¥åœ°å›¾ï¼Œå¹¶åˆ©ç”¨å·¥ä½œè®°å¿†æ¨¡å—å®ç°åˆ†å±‚æ£€ç´¢ï¼Œä»¥æ”¯æŒå¤šæ¨¡æ€ã€å¤šç²’åº¦çš„å¯¼èˆªã€æŒ‡ä»¤è·Ÿéšã€é—®ç­”å’Œç§»åŠ¨æ“ä½œä»»åŠ¡ã€‚å®éªŒæ˜¾ç¤ºï¼ŒBSC-Navåœ¨å¤šé¡¹ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒä¸‹çš„å¯¼èˆªä¸æ“ä½œä»»åŠ¡ä¸­ï¼Œè¡¨ç°å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ä¸æ•ˆç‡ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæ¨åŠ¨äº†æœºå™¨äººä»ååº”å¼å‘è®¤çŸ¥ç©ºé—´æ™ºèƒ½çš„è½¬å˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/From_reactive_to_cognitive_brain-inspired_spatial_intelligence_for_embodied_agents.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17547v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      29. [LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LodeStar__Long-horizon_Dexterity_via_Synthetic_Data_Augmentation_from_Human_Demonstrations.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of California San Diego</span></p>
<p>æœ¬è®ºæ–‡æå‡ºLODESTARç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹è‡ªåŠ¨åˆ†å‰²äººç±»æ¼”ç¤ºä¸ºæŠ€èƒ½ç‰‡æ®µï¼Œå¹¶åˆ©ç”¨æ®‹å·®å¼ºåŒ–å­¦ä¹ åœ¨ä»¿çœŸä¸­åˆæˆå¤šæ ·åŒ–æŠ€èƒ½æ•°æ®ï¼Œå®ç°ä»å°‘é‡äººç±»æ¼”ç¤ºä¸­å­¦ä¹ é²æ£’çš„é•¿æ—¶åºçµå·§æ“ä½œã€‚Skill Routing Transformerç­–ç•¥å°†è¿™äº›æŠ€èƒ½é«˜æ•ˆä¸²è”ï¼Œå®ç°æœºå™¨äººåœ¨ç°å®ç¯å¢ƒä¸­æ›´å¼ºçš„ä»»åŠ¡å®Œæˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå®éªŒåœ¨ä¸‰ä¸ªå¤æ‚çœŸå®ä»»åŠ¡ä¸Šå¹³å‡æå‡æˆåŠŸç‡25%ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LodeStar_Long-horizon_Dexterity_via_Synthetic_Data_Augmentation_from_Human_Demonstrations.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18249v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      30. [Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scene-Agnostic_Traversability_Labeling_and_Estimation_via_a_Multimodal_Self-supervised_Framework.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Jiao Tong University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è‡ªç›‘ç£æ¡†æ¶ï¼Œç»“åˆè¶³è¿¹ã€LiDARå’Œç›¸æœºä¿¡æ¯ï¼Œé€šè¿‡è§†è§‰åŸºç¡€æ¨¡å‹ä¸å‡ ä½•å…ˆéªŒè‡ªåŠ¨ç”Ÿæˆåƒç´ çº§å¯é€šè¡Œæ€§æ ‡ç­¾ï¼Œå¹¶é‡‡ç”¨è¯­ä¹‰-å‡ ä½•è§£è€¦çš„åŒæµç¥ç»ç½‘ç»œå®ç°å¤šæ¨¡æ€å¯é€šè¡Œæ€§ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ç¯å¢ƒä¸‹è‡ªåŠ¨æ ‡æ³¨å’Œå¯é€šè¡Œæ€§ä¼°è®¡å‡æ˜¾è‘—ä¼˜äºç°æœ‰è‡ªç›‘ç£æ–¹æ³•ï¼ŒIoUæå‡1.6â€“3.5%ï¼Œèƒ½æœ‰æ•ˆåŠ©åŠ›ä¸‹æ¸¸æœºå™¨äººå¯¼èˆªä»»åŠ¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Scene-Agnostic_Traversability_Labeling_and_Estimation_via_a_Multimodal_Self-supervised_Framework.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18066v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      31. [Arnold: a generalist muscle transformer policy]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Arnold__a_generalist_muscle_transformer_policy.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne (EPFL)</span></p>
<p>è¯¥è®ºæ–‡æå‡ºArnoldï¼Œä¸€ç§åŸºäºTransformerçš„å¤§è§„æ¨¡é€šç”¨è‚Œè‚‰æ§åˆ¶ç­–ç•¥ï¼Œèƒ½åŒæ—¶å¤„ç†14é¡¹æ¶‰åŠä¸åŒäººä½“éƒ¨ä½çš„ç”Ÿç‰©åŠ›å­¦ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç²¾ç»†æ“ä½œä¸è¡Œèµ°ã€‚Arnoldé€šè¿‡ç»„åˆä¼ æ„Ÿ-è¿åŠ¨è¯æ±‡è¡¨ã€è¡Œä¸ºå…‹éš†ï¼ˆOBCï¼‰ã€PPOå¾®è°ƒå’Œè‡ªè’¸é¦ï¼Œç»Ÿä¸€å­¦ä¹ å¤šä»»åŠ¡å¤šä½“æ€æ§åˆ¶ï¼Œç»“æœæ˜¾ç¤ºåœ¨å¤šæ•°ä»»åŠ¡ä¸Šè¾¾åˆ°ç”šè‡³è¶…è¶Šä¸“å®¶çº§è¡¨ç°ï¼Œå¹¶èƒ½é«˜æ•ˆé€‚åº”æ–°ä»»åŠ¡ï¼Œä½†å…¶å­¦ä¹ åˆ°çš„è‚Œè‚‰ååŒä¸»è¦ä¸ºä»»åŠ¡ç‰¹å®šï¼Œç¼ºä¹é€šç”¨æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Arnold_a_generalist_muscle_transformer_policy.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18040v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      32. [PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PerPilot__Personalizing_VLM-based_Mobile_Agents_via_Memory_and_Exploration.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Northwestern Polytechnical University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºPerPilotï¼Œä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯æ’æ‹”æ¡†æ¶ï¼Œé€šè¿‡ä¸ªæ€§åŒ–æ„ŸçŸ¥æ¨¡å—è¯†åˆ«å¹¶æå–ç”¨æˆ·æŒ‡ä»¤ä¸­çš„ä¸ªæ€§åŒ–å…ƒç´ ï¼Œåˆ©ç”¨è®°å¿†æ£€ç´¢ä¸æ¨ç†æ¢ç´¢ä¸¤ç§æ–¹å¼è‡ªåŠ¨è¡¥å…¨å’Œæ‰§è¡Œä¸ªæ€§åŒ–æŒ‡ä»¤ï¼Œå¹¶æ„å»ºäº†é¦–ä¸ªä¸ªæ€§åŒ–ç§»åŠ¨åœºæ™¯æ•°æ®é›†PerInstructã€‚å®éªŒè¡¨æ˜ï¼ŒPerPilotå¤§å¹…æå‡äº†å¤šç§ç§»åŠ¨ä»£ç†ç³»ç»Ÿåœ¨ä¸ªæ€§åŒ–åœºæ™¯ä¸‹çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œä¸”éšä½¿ç”¨æ¬¡æ•°æå‡ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PerPilot_Personalizing_VLM-based_Mobile_Agents_via_Memory_and_Exploration.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18039v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      33. [MODELING AND CONTROL FRAMEWORK FOR AUTONOMOUS SPACE MANIPULATOR HANDOVER OPERATIONS]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Modeling_and_Control_Framework_for_Autonomous_Space_Manipulator_Handover_Operations.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Cincinnati</span></p>
<p>æœ¬æ–‡æå‡ºäº†é€‚ç”¨äºç©ºé—´åœ¨è½¨æœåŠ¡ã€ç»„è£…å’Œåˆ¶é€ åœºæ™¯çš„é«˜è‡ªç”±åº¦åŒè‡‚ç©ºé—´æœºæ¢°è‡‚ç³»ç»ŸåŠ¨åŠ›å­¦å»ºæ¨¡æ–¹æ³•ï¼Œå¹¶å¯¹åŸºäºéçº¿æ€§æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆNMPCï¼‰ä¸ä¼ ç»ŸPIDæ§åˆ¶çš„è½¨è¿¹è·Ÿè¸ªæ€§èƒ½è¿›è¡Œäº†å¯¹æ¯”ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œä¸¤ç§æ§åˆ¶å™¨å‡èƒ½å®ç°è‡ªä¸»æœºå™¨äººé—´æ¥åŠ›ä¼ é€’ä»»åŠ¡ï¼Œä½†NMPCè¡¨ç°å‡ºæ›´ä¼˜çš„è½¨è¿¹è·Ÿè¸ªç²¾åº¦å’ŒåŠ¨æ€å“åº”ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ç©ºé—´æœºå™¨äººè‡ªä¸»æ“ä½œä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MODELING_AND_CONTROL_FRAMEWORK_FOR_AUTONOMOUS_SPACE_MANIPULATOR_HANDOVER_OPERATIONS.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17986v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      34. [No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/No_Need_to_Look!_Locating_and_Grasping_Objects_by_a_Robot_Arm_Covered_with_Sensitive_Skin.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Czech Technical University in Prague</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å®Œå…¨åŸºäºè§¦è§‰åé¦ˆã€æ— éœ€è§†è§‰è¾“å…¥çš„æœºå™¨äººæŠ“å–æ–¹æ¡ˆï¼Œæ ¸å¿ƒåˆ›æ–°åœ¨äºç”¨è¦†ç›–å…¨è‡‚çš„äººå·¥çš®è‚¤è¿›è¡Œç²—ç•¥æ‰«æï¼Œç»“åˆæœ«ç«¯åŠ›&#x2F;æ‰­çŸ©ä¼ æ„Ÿå™¨å®ç°ç²¾ç¡®å®šä½ä¸æŠ“å–ã€‚ç³»ç»Ÿåœ¨çœŸå®å’Œä»¿çœŸç¯å¢ƒä¸­è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå¯¹ä¸åŒå½¢çŠ¶ã€ä½ç½®åŠå¤šç‰©ä½“åœºæ™¯å‡æœ‰è¾ƒé«˜æˆåŠŸç‡ï¼Œä¸”é€Ÿåº¦ä¸ºåªç”¨æœ«ç«¯è§¦è§‰çš„6å€ï¼Œé€‚ç”¨äºè§†è§‰æ„ŸçŸ¥å—é™åœºæ™¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/No_Need_to_Look%21_Locating_and_Grasping_Objects_by_a_Robot_Arm_Covered_with_Sensitive_Skin.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17971v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      35. [Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Neural_Algorithmic_Reasoners_informed_Large_Language_Model_for_Multi-Agent_Path_Finding.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">State Key Laboratory of Complex &amp; Critical Software Environment, Beihang University</span></p>
<p>æœ¬æ–‡æå‡ºLLM-NARæ¡†æ¶ï¼Œå°†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä¸ºæ ¸å¿ƒçš„ç¥ç»ç®—æ³•æ¨ç†å™¨ï¼ˆNARï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶èåˆï¼Œé’ˆå¯¹å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’ï¼ˆMAPFï¼‰ä»»åŠ¡ã€‚æ–¹æ³•åŒ…æ‹¬åŸºäºåœºæ™¯çš„LLMæç¤ºå·¥ç¨‹ã€GNNé¢„è®­ç»ƒè¡¨å¾ç©ºé—´ä¿¡æ¯ã€ä»¥åŠè·¨æ³¨æ„åŠ›èåˆï¼Œæå¤§æå‡äº†MAPFä»»åŠ¡ä¸­LLMçš„ç©ºé—´ç†è§£ä¸å†³ç­–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œæ— è®ºåœ¨ä»¿çœŸè¿˜æ˜¯å®é™…æœºå™¨äººå®éªŒä¸­ï¼ŒLLM-NARåœ¨æˆåŠŸç‡å’Œè·¯å¾„æ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰LLMæ–¹æ³•ï¼Œä¸”è®­ç»ƒæ­¥éª¤å’Œæ‰§è¡Œæ—¶é—´å‡å¤§å¹…å‡å°‘ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Neural_Algorithmic_Reasoners_informed_Large_Language_Model_for_Multi-Agent_Path_Finding.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17922v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      36. [Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Egocentric_Instruction-oriented_Affordance_Prediction_via_Large_Multimodal_Model.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Xidian University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä»¥ä»»åŠ¡&#x2F;æŒ‡ä»¤ä¸ºå¯¼å‘çš„ä»¿ç”Ÿè§†è§’å¯ä¾›æ€§é¢„æµ‹æ–°ä»»åŠ¡ï¼Œå¼ºè°ƒåŒä¸€ç‰©ä½“åœ¨ä¸åŒæ“ä½œæŒ‡ä»¤ä¸‹çš„å¯ä¾›æ€§åº”åŠ¨æ€å˜åŒ–ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ„å»ºäº†åŒ…å«è¿‘1.5ä¸‡ä¸ªå¯¹è±¡-æŒ‡ä»¤-å¯ä¾›æ€§ä¸‰å…ƒç»„çš„å¤§è§„æ¨¡EIVAæ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†åŸºäºå¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„â€œsearch against verifiersâ€è‡ªåå¼æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡Actorä¸Verifierçš„äº¤äº’è¿­ä»£ä¼˜åŒ–å¯ä¾›æ€§é¢„æµ‹ï¼Œæ˜¾è‘—æå‡äº†LMMåœ¨å®é™…æœºå™¨äººæ“ä½œåœºæ™¯ä¸‹çš„æ³›åŒ–å’ŒæŒ‡ä»¤é€‚åº”èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨æ–°ä»»åŠ¡ä¸Šä¸ºæŒ‡ä»¤å¯¼å‘çš„å¯ä¾›æ€§é¢„æµ‹å»ºç«‹äº†å¼ºåŸºçº¿ï¼Œå¹¶åœ¨çœŸå®æœºå™¨äººä»¿çœŸä¸­è·å¾—äº†ä¼˜å¼‚è¡¨ç°ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Egocentric_Instruction-oriented_Affordance_Prediction_via_Large_Multimodal_Model.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17921v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      37. [Physical Embodiment Enables Information Processing Beyond Explicit Sensing in Active Matter]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Physical_Embodiment_Enables_Information_Processing_Beyond_Explicit_Sensing_in_Active_Matter.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Leipzig University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é€šè¿‡ç‰©ç†ä½“ç°ï¼ˆembodimentï¼‰å®ç°ä¿¡æ¯å¤„ç†çš„æ–°èŒƒå¼ï¼Œä¸ä¾èµ–æ˜¾å¼ä¼ æ„Ÿï¼Œä»…åˆ©ç”¨è‡ªèº«åŠ¨åŠ›å­¦ä¸ç¯å¢ƒäº¤äº’è¿›è¡Œé€‚åº”ã€‚ç ”ç©¶ä¸­ï¼Œä½œè€…é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆPPOç®—æ³•ï¼‰å®æ—¶æ§åˆ¶è‡ªé©±åŠ¨å¾®å‹æœºå™¨äººï¼ˆå…‰é©±åŠ¨è‡ªçƒ­æ³³ç²’å­ï¼‰ï¼Œå®éªŒéªŒè¯å…¶å¯åœ¨æœªçŸ¥æµåœºæ‰°åŠ¨ä¸‹ï¼Œä»…å‡­ä½ç½®å’Œä½ç§»åé¦ˆå­¦ä¹ åˆ°å¯¹æŠ—æµåœºçš„å¯¼èˆªç­–ç•¥ï¼Œå®ç°å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªã€‚ç»“è®ºï¼šç‰©ç†ä½“ç°å¯ä½œä¸ºå¾®è§‚æœºå™¨äººä¿¡æ¯å¤„ç†çš„èµ„æºï¼Œæ— éœ€å¤æ‚ä¼ æ„Ÿå™¨å³å¯é€‚åº”é™æ€åŠåŠ¨æ€æµåœºï¼Œä¸ºå¾®å‹è‡ªä¸»ç³»ç»Ÿå’Œä»¿ç”Ÿè®¡ç®—å¼€è¾Ÿæ–°è·¯å¾„ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Physical_Embodiment_Enables_Information_Processing_Beyond_Explicit_Sensing_in_Active_Matter.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17831v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      38. [CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubes]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CubeDN__Real-time_Drone_Detection_in_3D_Space_from_Dual_mmWave_Radar_Cubes.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University College London</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºåŒæ¯«ç±³æ³¢é›·è¾¾çš„3Dæ— äººæœºæ£€æµ‹ç³»ç»ŸCubeDNï¼Œé‡‡ç”¨åˆ›æ–°çš„4Dé›·è¾¾æ•°æ®èåˆå’Œ3Då·ç§¯ç¥ç»ç½‘ç»œï¼Œå®ç°äº†å¯¹ä¸åŒå°ºå¯¸æ— äººæœºçš„å®æ—¶æ£€æµ‹ã€å®šä½ä¸åˆ†ç±»ã€‚å®éªŒæ˜¾ç¤ºCubeDNåœ¨å¤æ‚ç¯å¢ƒä¸‹å¹³å‡ç²¾åº¦è¾¾95%ã€å¬å›ç‡85%ï¼Œä¸”è¿½è¸ªè¯¯å·®ä½è‡³åˆ†ç±³çº§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å…·å¤‡10Hzå®æ—¶æ¨ç†èƒ½åŠ›ï¼Œé€‚ç”¨äºå®é™…æ— äººæœºç›‘æ§åœºæ™¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CubeDN_Real-time_Drone_Detection_in_3D_Space_from_Dual_mmWave_Radar_Cubes.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17778v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      39. [AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networks]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AgentRAN__An_Agentic_AI_Architecture_for_Autonomous_Control_of_Open_6G_Networks.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Northeastern University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºAgentRANï¼Œä¸€ç§é¢å‘å¼€æ”¾6Gç½‘ç»œçš„AIåŸç”Ÿagentæ¶æ„ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰æ„å›¾è§£é‡Šå’Œåˆ†è§£ï¼Œå®ç°æ— çº¿æ¥å…¥ç½‘ï¼ˆRANï¼‰ä¸­è·¨æ—¶ç©ºå’Œåè®®å±‚çš„è‡ªæ²»æ§åˆ¶ã€‚æ–¹æ³•åŒ…æ‹¬å¤šå±‚æ¬¡agentåä½œã€AI-RAN Factoryè‡ªåŠ¨åˆæˆæ–°agentåŠæŒç»­è‡ªå­¦ä¹ æœºåˆ¶ï¼Œå®éªŒåœ¨çœŸå®5Gæµ‹è¯•åºŠä¸­éªŒè¯äº†ç³»ç»Ÿèƒ½æ ¹æ®NLæ„å›¾åŠ¨æ€ä¼˜åŒ–ä¸Šè¡Œè°ƒåº¦å’ŒåŠŸæ§ï¼Œæ˜¾è‘—æå‡äº†ç½‘ç»œè‡ªé€‚åº”æ€§å’Œå¯è§£é‡Šæ€§ã€‚ç»“è®ºï¼šAgentRANèƒ½å¤Ÿå®ç°æ— çº¿ç½‘ç»œåŸºäºè‡ªç„¶è¯­è¨€è‡ªé€‚åº”ã€é«˜æ•ˆä¸é€æ˜çš„è‡ªåŠ¨åŒ–æ§åˆ¶ï¼Œæ¨åŠ¨AI-for-RANæ„¿æ™¯è½åœ°ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/AgentRAN_An_Agentic_AI_Architecture_for_Autonomous_Control_of_Open_6G_Networks.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17746v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      40. [DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DroneKey__Drone_3D_Pose_Estimation_in_Image_Sequences_using_Gated_Key-representation_and_Pose-adapti.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Chonnam National University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†DroneKeyæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆtransformerç¼–ç å™¨çš„ä¸­é—´å’Œç´§å‡‘ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨é—¨æ§åŠ æƒæ±‚å’Œæ–¹å¼ï¼Œå®ç°äº†æ— äººæœºèºæ—‹æ¡¨å…³é”®ç‚¹çš„é«˜ç²¾åº¦æ£€æµ‹ï¼Œå¹¶é€šè¿‡PnPæ±‚è§£å™¨å®Œæˆæ— äººæœº3Då§¿æ€ä¼°è®¡ã€‚æå‡ºçš„å§¿æ€è‡ªé€‚åº”MahalanobisæŸå¤±å‡½æ•°æå‡äº†æç«¯å§¿æ€ä¸‹å…³é”®ç‚¹é¢„æµ‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ–¹æ³•åœ¨å…³é”®ç‚¹æ£€æµ‹å’Œ3Då§¿æ€ä¼°è®¡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·å¤‡å®æ—¶æ€§å’Œé«˜å‡†ç¡®ç‡ï¼Œé€‚ç”¨äºåæ— äººæœºç³»ç»Ÿã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/DroneKey_Drone_3D_Pose_Estimation_in_Image_Sequences_using_Gated_Key-representation_and_Pose-adaptive_Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17684v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      41. [MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Welding]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MEVITA__Open-Source_Bipedal_Robot_Assembled_from_E-Commerce_Components_via_Sheet_Metal_Welding.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The University of Tokyo</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å¼€æºåŒè¶³æœºå™¨äººMEVITAï¼Œé‡‡ç”¨ç”µå•†å¯é‡‡è´­çš„é‡‘å±éƒ¨ä»¶ï¼Œé€šè¿‡é’£é‡‘ç„Šæ¥æŠ€æœ¯æå¤§å‡å°‘é›¶ä»¶æ•°é‡ï¼Œå®ç°äº†ç®€æ˜“ç»„è£…ã€‚è¯¥æœºå™¨äººç»“åˆå¼ºåŒ–å­¦ä¹ ä»¿çœŸä¸Sim-to-Realè¿ç§»ï¼Œåœ¨å¤šç§ç¯å¢ƒä¸‹å®ç°äº†ç¨³å®šæ­¥è¡Œï¼ŒéªŒè¯äº†ç»“æ„ä¸æ§åˆ¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MEVITA_Open-Source_Bipedal_Robot_Assembled_from_E-Commerce_Components_via_Sheet_Metal_Welding.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17643v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      42. [SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SEBVS__Synthetic_Event-based_Visual_Servoing_for_Robot_Navigation_and_Manipulation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Arizona State University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†SEBVSï¼Œä¸€ä¸ªé›†æˆäºGazeboä»¿çœŸçš„å¼€æºROS2åŒ…ï¼Œå®ç°äº†åŸºäºv2eçš„è§†é¢‘è½¬äº‹ä»¶æµä»¿çœŸï¼Œæ”¯æŒæœºå™¨äººåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸‹æ— ä¸“ç”¨ç¡¬ä»¶åœ°è·å–äº‹ä»¶æµã€‚ä½œè€…è®¾è®¡äº†èåˆRGBå’Œäº‹ä»¶å¸§çš„Transformeræ¶æ„ï¼Œç”¨äºç«¯åˆ°ç«¯å­¦ä¹ å¯¼èˆªä¸æ“ä½œç­–ç•¥ï¼ˆERPNavå’ŒERPArmï¼‰ï¼Œé€šè¿‡è¡Œä¸ºå…‹éš†è®­ç»ƒå¹¶åœ¨ä¸¤é¡¹ä»»åŠ¡ï¼ˆç§»åŠ¨æœºå™¨äººè·Ÿè¸ªä¸æœºæ¢°è‡‚æŠ“å–ï¼‰ä¸Šè¯„æµ‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œèåˆäº‹ä»¶æµçš„ç­–ç•¥åœ¨è¿½è¸ªç²¾åº¦ã€æŠ“å–æˆåŠŸç‡å’Œé²æ£’æ€§ä¸Šå‡ä¼˜äºä»…ç”¨RGBæˆ–ä»…ç”¨äº‹ä»¶æµçš„åŸºçº¿æ¨¡å‹ï¼ŒéªŒè¯äº†äº‹ä»¶è§†è§‰åœ¨æœºå™¨äººå®æ—¶æ§åˆ¶ä¸­çš„ä¼˜åŠ¿ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SEBVS_Synthetic_Event-based_Visual_Servoing_for_Robot_Navigation_and_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17600v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      43. [GWM: Towards Scalable Gaussian World Models for Robotic Manipulation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GWM__Towards_Scalable_Gaussian_World_Models_for_Robotic_Manipulation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tsinghua University</span></p>
<p>æœ¬æ–‡æå‡ºäº†Gaussian World Model (GWM)ï¼Œé€šè¿‡ç»“åˆ3D Gaussian Splattingè¡¨ç¤ºã€3Då˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’Œæ½œå˜é‡æ‰©æ•£Transformerï¼ˆDiTï¼‰ï¼Œå®ç°å¯¹æœºå™¨äººæ“ä½œä¸­æœªæ¥åœºæ™¯çš„é«˜ç²¾åº¦ä¸‰ç»´åŠ¨æ€å»ºæ¨¡ã€‚GWMä¸ä»…æå‡äº†ä»¿çœŸå’ŒçœŸå®åœºæ™¯ä¸‹çš„åŠ¨ä½œæ¡ä»¶ä¸‰ç»´è§†é¢‘é¢„æµ‹èƒ½åŠ›ï¼Œè¿˜æ˜¾è‘—å¢å¼ºäº†æ¨¡ä»¿å­¦ä¹ ä¸åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥å­¦ä¹ è¡¨ç°ï¼Œå®éªŒæ˜¾ç¤ºåœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå…·å¤‡è¾ƒå¼ºçš„æ•°æ®æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GWM_Towards_Scalable_Gaussian_World_Models_for_Robotic_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18269v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      44. [FlowVLA: Thinking in Motion with a Visual Chain of Thought]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FlowVLA__Thinking_in_Motion_with_a_Visual_Chain_of_Thought.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">HKUST(GZ)</span></p>
<p>æœ¬æ–‡æå‡ºäº†Visual Chain of Thought (Visual CoT) é¢„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ä¸­é—´å…‰æµï¼ˆmotion dynamicsï¼‰é¢„æµ‹æ­¥éª¤ï¼ˆvt â†’ ft â†’ vt+1ï¼‰ï¼Œåœ¨å•ä¸€è‡ªå›å½’Transformerä¸­å®ç°æ˜¾å¼è¿åŠ¨æ¨ç†ä¸è§†è§‰è¡¨å¾è§£è€¦ã€‚æœ€ç»ˆï¼ŒFlowVLAåœ¨æœºå™¨äººæ“ä½œåŸºå‡†ä¸Šå–å¾—äº†æœ€ä¼˜è¡¨ç°ï¼Œå¹¶æ˜¾è‘—æå‡äº†æ ·æœ¬æ•ˆç‡ï¼ŒéªŒè¯äº†æ˜¾å¼è¿åŠ¨æ¨ç†å¯¹æå‡æ„ŸçŸ¥ä¸æ§åˆ¶æ¡¥æ¥èƒ½åŠ›çš„ä»·å€¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/FlowVLA_Thinking_in_Motion_with_a_Visual_Chain_of_Thought.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18074v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      45. [The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_Effects_of_Communication_Delay_on_Human_Performance_and_Neurocognitive_Responses_in_Mobile_Robot.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Beijing Institute of Technology</span></p>
<p>æœ¬æ–‡é€šè¿‡äººæœºå›è·¯å®éªŒï¼Œç»“åˆç§»åŠ¨æœºå™¨äººè¡Œä¸ºæ•°æ®å’Œè„‘ç”µï¼ˆEEGï¼‰ä¿¡å·ï¼Œç³»ç»Ÿåˆ†æé€šä¿¡å»¶è¿Ÿï¼ˆ0-500msï¼‰å¯¹è¿œç¨‹æ“ä½œä»»åŠ¡ä¸­çš„äººç±»æ“ä½œæ€§èƒ½ä¸ç¥ç»è®¤çŸ¥å“åº”çš„å½±å“ã€‚å®éªŒå‘ç°ï¼Œåœ¨200-300mså»¶è¿ŸèŒƒå›´å†…ï¼Œä»»åŠ¡æ•ˆç‡å’Œå‡†ç¡®æ€§é¦–æ¬¡å‡ºç°æ˜¾è‘—ä¸‹é™ï¼ŒEEGåˆ†ææ­ç¤ºäº†ä¸å»¶è¿Ÿé«˜åº¦ç›¸å…³çš„è„‘åŒºç‰¹å¾ï¼ˆé¢å¶Î¸&#x2F;Î²æ³¢ã€é¡¶å¶Î±æ³¢ï¼‰ï¼Œå¹¶ç•Œå®šäº†äººç±»å¯¹å»¶è¿Ÿçš„æ—©æœŸæ„ŸçŸ¥é˜ˆå€¼ï¼ˆ100-200msï¼‰å’Œè®¤çŸ¥èµ„æºåˆ†é…æé™ï¼ˆ400msï¼‰ã€‚ç»“è®ºä¸ºè¿œç¨‹æ“ä½œç³»ç»Ÿå»¶è¿Ÿè¡¥å¿æœºåˆ¶å’Œå®‰å…¨å‚æ•°è®¾è®¡æä¾›äº†ç¥ç»è®¤çŸ¥ä¾æ®ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/The_Effects_of_Communication_Delay_on_Human_Performance_and_Neurocognitive_Responses_in_Mobile_Robot_Teleoperation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17985v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      46. [Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADORE]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Integration_of_Computer_Vision_with_Adaptive_Control_for_Autonomous_Driving_Using_ADORE.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Siegen</span></p>
<p>è¯¥è®ºæ–‡æå‡ºå°†YOLOv8æ·±åº¦å­¦ä¹ è§†è§‰æ¨¡å‹ä¸ADOREè‡ªé€‚åº”æ§åˆ¶æ¡†æ¶ç›¸ç»“åˆï¼Œé€šè¿‡CARLAä»¿çœŸå¹³å°å’ŒROSæ¡¥å®ç°æ„ŸçŸ¥ã€å†³ç­–ä¸æ§åˆ¶æ¨¡å—çš„å®æ—¶ååŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç³»ç»Ÿåœ¨ä¸åŒå¤©æ°”æ¡ä»¶ä¸‹èƒ½é«˜ç²¾åº¦æ£€æµ‹äº¤é€šæ ‡å¿—å¹¶å¿«é€Ÿé€‚åº”è½¦é€Ÿå˜åŒ–ï¼Œæå‡äº†è‡ªåŠ¨é©¾é©¶å®‰å…¨æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Integration_of_Computer_Vision_with_Adaptive_Control_for_Autonomous_Driving_Using_ADORE.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17969v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      47. [A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_holistic_perception_system_of_internal_and_external_monitoring_for_ground_autonomous_vehicles__Aut.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Industrial Systems Institute, ATHENA Research Center</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†AutoTRUSTæ¡†æ¶ï¼Œèåˆäº†è½¦å†…å¤šæ‘„åƒå¤´æƒ…æ„Ÿè¯†åˆ«ã€é©¾é©¶å‘˜åˆ†å¿ƒæ£€æµ‹ã€äººè„¸è¯†åˆ«ã€æ™ºèƒ½ç©ºæ°”è´¨é‡ä¸çƒ­èˆ’é€‚ç›‘æµ‹ï¼Œä»¥åŠåŸºäºLiDARçš„é«˜æ•ˆå¤–éƒ¨ç¯å¢ƒæ„ŸçŸ¥ï¼ˆä½æˆæœ¬LiDARè¶…åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²ï¼‰ï¼Œè¿˜é›†æˆäº†è™šæ‹ŸåŠ©æ‰‹ï¼Œå…¨éƒ¨åœ¨åµŒå…¥å¼è®¾å¤‡ä¸Šå®ç°ã€‚å®è½¦éƒ¨ç½²ä¸å®éªŒæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿèƒ½åœ¨å¤šæ ·ç¯å¢ƒä¸‹å®æ—¶ã€é«˜æ•ˆæå‡è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„å®‰å…¨æ€§ã€èˆ’é€‚åº¦å’Œä½“éªŒï¼Œå…·å¤‡è‰¯å¥½å¯æ‰©å±•æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_holistic_perception_system_of_internal_and_external_monitoring_for_ground_autonomous_vehicles_AutoTRUST_paradigm.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17130v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      48. [Structural Damage Detection Using AI Super Resolution and Visual Language Model]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Structural_Damage_Detection_Using_AI_Super_Resolution_and_Visual_Language_Model.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Dakota State University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ— äººæœºèˆªæ‹ã€AIè§†é¢‘è¶…åˆ†è¾¨ç‡ï¼ˆVideo Restoration Transformer, VRTï¼‰ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆGemma3:27bï¼‰çš„ç»“æ„æŸä¼¤æ£€æµ‹æ¡†æ¶ã€‚æ–¹æ³•æµç¨‹åŒ…æ‹¬ä½åˆ†è¾¨ç‡ç¾éš¾ç°åœºè§†é¢‘å¢å¼ºã€ç»“æ„æŸä¼¤è¯†åˆ«ä¸å››ç±»æŸä¼¤åˆ†çº§ï¼ˆæ— &#x2F;è½»å¾®ã€ä¸­åº¦ã€é‡åº¦ã€å®Œå…¨æ¯åï¼‰ï¼Œå¹¶åœ¨åœŸè€³å…¶åœ°éœ‡å’Œç¾å›½é¾™å·é£æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œåˆ†ç±»å‡†ç¡®ç‡è¾¾84.5%ã€‚ç»“è®ºæ˜¾ç¤ºè¯¥ç³»ç»Ÿèƒ½å¿«é€Ÿã€ä½æˆæœ¬åœ°æå‡ç¾å®³æŸä¼¤è¯„ä¼°çš„ç²¾åº¦ä¸å“åº”æ•ˆç‡ï¼Œä¸”æ™®é€šç”¨æˆ·ä¹Ÿå¯ä½¿ç”¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Structural_Damage_Detection_Using_AI_Super_Resolution_and_Visual_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17128v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      49. [CE-RS-SBCIT: A Novel Channel-Enhanced Hybrid CNNâ€“Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysis]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CE-RS-SBCIT_A_Novel_Channel_Enhanced_Hybrid_CNN_Transformer_with_Residual,_Spatial,_and_Boundary-Awa.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Ibadat International University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†CE-RS-SBCITæ··åˆè¯Šæ–­æ¡†æ¶ï¼Œç»“åˆæ®‹å·®å­¦ä¹ ä¸ç©ºé—´å­¦ä¹ CNNã€è¾¹ç•Œä¸å¹³æ»‘æ“ä½œçš„å®šåˆ¶Transformeræ¨¡å—ï¼Œå¹¶åˆ›æ–°å¼•å…¥é€šé“å¢å¼ºä¸ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆèåˆå±€éƒ¨ç»†ç²’åº¦å’Œå…¨å±€è¯­å¢ƒç‰¹å¾ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šç±»è„‘è‚¿ç˜¤MRIåˆ†ç±»ä¸­å‡†ç¡®ç‡è¾¾98.3%ï¼Œæ•æ„Ÿæ€§ã€F1åˆ†æ•°å’Œç²¾åº¦å‡æ˜¾è‘—ä¼˜äºç°æœ‰CNNã€ViTåŠæ··åˆæ¨¡å‹ï¼Œå…·å¤‡é«˜æ•ˆã€ç¨³å¥å’Œæ³›åŒ–çš„åŒ»å­¦å›¾åƒè‡ªåŠ¨è¯Šæ–­æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CE-RS-SBCIT_A_Novel_Channel-Enhanced_Hybrid_CNN%E2%80%93Transformer_with_Residual_Spatial_and_Boundary-Aware_Learning_for_Brain_Tumor_MRI_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17081v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      50. [Proximal Vision Transformer: Enhancing Feature Representation through Two-Stage Manifold Geometry]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Proximal_Vision_Transformer__Enhancing_Feature_Representation_through_Two-Stage_Manifold_Geometry.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">North Carolina State University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†Proximal Vision Transformerï¼ˆViT+Proxï¼‰æ¡†æ¶ï¼Œå°†Vision Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æ„é€ çš„æµå½¢åˆ‡ä¸›ä¸å¯å­¦ä¹ çš„è¿‘ç«¯ç®—å­ç»“åˆï¼Œå®ç°äº†å±€éƒ¨ä¸å…¨å±€å‡ ä½•ç‰¹å¾çš„ç»Ÿä¸€ä¼˜åŒ–ã€‚é€šè¿‡åœ¨ViTè¾“å‡ºåå¼•å…¥è¿‘ç«¯è¿­ä»£ï¼Œå®šä¹‰åˆ‡ä¸›æˆªé¢å¹¶æŠ•å½±æ•°æ®è‡³åŸºç©ºé—´ï¼Œæœ‰æ•ˆæå‡äº†åˆ†ç±»å‡†ç¡®ç‡ã€ç‰¹å¾åˆ†å¸ƒçš„ç»“æ„åŒ–è¡¨ç°ï¼Œå°¤å…¶åœ¨é«˜åˆ†è¾¨ç‡è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Proximal_Vision_Transformer_Enhancing_Feature_Representation_through_Two-Stage_Manifold_Geometry.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17062v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      51. [SSG-DiT: A Spatial Signal Guided Framework for Controllable Video Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SSG-Dit__A_Spatial_Signal_Guided_Framework_for_Controllable_Video_Generation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Electronic Science and Technology of China</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†SSG-DiTæ¡†æ¶ï¼Œç”¨äºå¯æ§è§†é¢‘ç”Ÿæˆï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆé€šè¿‡é¢„è®­ç»ƒCLIPæ¨¡å‹æå–ä¸­é—´ç‰¹å¾å¹¶èåˆï¼Œç”Ÿæˆè¯­ä¹‰ç›¸å…³çš„ç©ºé—´è§†è§‰æç¤ºï¼›ç„¶ååˆ©ç”¨è½»é‡å‹SSG-Adapterï¼Œå°†è§†è§‰æç¤ºä¸æ–‡æœ¬æ¡ä»¶æ³¨å…¥å†»ç»“çš„Diffusion Transformerï¼ˆDiTï¼‰ä¸»å¹²ï¼Œå®ç°ç²¾å‡†è§†é¢‘å†…å®¹æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSSG-DiTåœ¨VBenchåŸºå‡†çš„ç©ºé—´å…³ç³»æ§åˆ¶å’Œä¸€è‡´æ€§æ–¹é¢è¶…è¿‡ç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘ç”Ÿæˆçš„è¯­ä¹‰ä¸€è‡´æ€§ä¸ç»†èŠ‚è¿˜åŸèƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SSG-DiT_A_Spatial_Signal_Guided_Framework_for_Controllable_Video_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17061v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      52. [REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/REGEN__Real-Time_Photorealism_Enhancement_in_Games_via_a_Dual-Stage_Generative_Network_Framework.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Aristotle University of Thessaloniki</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†REGENæ¡†æ¶ï¼Œé€šè¿‡å…ˆç”¨é²æ£’çš„éå®æ—¶æ— ç›‘ç£Im2Imç”Ÿæˆé«˜è´¨é‡å…‰ç…§çœŸå®æ„Ÿå¯¹é½æ•°æ®ï¼Œç„¶åå°†ä»»åŠ¡è½¬åŒ–ä¸ºæ›´ç®€å•çš„æœ‰ç›‘ç£é…å¯¹Im2Imä»»åŠ¡ï¼Œåˆ©ç”¨è½»é‡çº§Pix2PixHDæ¨¡å‹å®ç°æ¸¸æˆç”»é¢å®æ—¶å…‰ç…§çœŸå®æ„Ÿå¢å¼ºã€‚å®éªŒåœ¨GTA Vä¸Cityscapesæ•°æ®é›†ä¸ŠéªŒè¯ï¼ŒREGENåœ¨ä¿æŒä¸SOTAæ–¹æ³•ç›¸å½“çš„è§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œæ¨ç†é€Ÿåº¦æå‡32å€ï¼Œå¯å®æ—¶åº”ç”¨äºæ¸¸æˆç”»é¢å¢å¼ºã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/REGEN_Real-Time_Photorealism_Enhancement_in_Games_via_a_Dual-Stage_Generative_Network_Framework.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17050v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      53. [PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PVNet__Point-Voxel_Interaction_LiDAR_Scene_Upsampling_Via_Diffusion_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Telecom Guizhou Branch</span></p>
<p>è¯¥è®ºæ–‡æå‡ºPVNetï¼Œä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç‚¹-ä½“ç´ äº¤äº’æ¡†æ¶ï¼Œå®ç°å®¤å¤–LiDARç‚¹äº‘çš„æ— å¯†é›†ç›‘ç£åœºæ™¯çº§æ’å€¼ã€‚æ–¹æ³•åŒ…æ‹¬åˆ†ç±»å™¨æ— å…³æŒ‡å¯¼çš„DDPMï¼Œä½“ç´ è¡¥å…¨æ¨¡å—å’Œç‚¹-ä½“ç´ äº¤äº’æ¨¡å—ï¼Œæœ‰æ•ˆæå‡å¤æ‚åœºæ™¯çš„ç‚¹äº‘å¯†åº¦å’Œç¯å¢ƒæ„ŸçŸ¥ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†å’Œä»»æ„æ’å€¼å€ç‡ä¸‹å‡è¾¾åˆ°äº†æœ€ä¼˜æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†3Dåœºæ™¯æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PVNet_Point-Voxel_Interaction_LiDAR_Scene_Upsampling_Via_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17037v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      54. [F4-ITS: Fine-grained Feature Fusion for Food Image-Text Search]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/F4-ITS__Fine-grained_Feature_Fusion_for_Food_Image-Text_Search.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">HyperVerge</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒã€ç”±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æŒ‡å¯¼çš„å¤šæ¨¡æ€ç‰¹å¾èåˆæ¡†æ¶F4-ITSï¼Œç”¨äºæå‡é£Ÿç‰©å›¾åƒä¸æ–‡æœ¬æè¿°çš„æ£€ç´¢æ€§èƒ½ã€‚å…¶ä¸»è¦æ–¹æ³•åŒ…æ‹¬å›¾åƒä¸VLMç”Ÿæˆæ–‡æœ¬æè¿°çš„å•å‘å’ŒåŒå‘èåˆï¼Œä»¥åŠåŸºäºé£Ÿæçº§åˆ«ç‰¹å¾çš„æ£€ç´¢å€™é€‰é‡æ’åºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ£€ç´¢å‡†ç¡®ç‡ï¼Œå°¤å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹å°æ¨¡å‹ä¹Ÿèƒ½è¾¾åˆ°ç”šè‡³è¶…è¿‡å¤§æ¨¡å‹ã€‚ç»“è®ºè®¤ä¸ºF4-ITSèƒ½æœ‰æ•ˆæ”¹å–„ç»†ç²’åº¦é£Ÿç‰©å›¾åƒæ–‡æœ¬æ£€ç´¢ï¼Œå¹¶å¯æ¨å¹¿è‡³å…¶å®ƒå¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/F4-ITS_Fine-grained_Feature_Fusion_for_Food_Image-Text_Search.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17009v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      55. [Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Contrastive_Prompt_Clustering_for_Weakly_Supervised_Semantic_Segmentation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Xiâ€™an Jiaotong-Liverpool University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²æ¡†æ¶CPCï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆç±»åˆ«èšç±»ï¼Œä½œä¸ºè¯­ä¹‰å…ˆéªŒå¼•å…¥ViTä¸»å¹²ç½‘ç»œï¼Œå¹¶è®¾è®¡äº†ç±»æ„ŸçŸ¥çš„patchçº§å¯¹æ¯”æŸå¤±ä»¥å¢å¼ºç±»åˆ«å†…ä¸€è‡´æ€§å’ŒåŒºåˆ†ç±»é—´è¾¹ç•Œã€‚å®éªŒåœ¨PASCAL VOC 2012ä¸MS COCO 2014æ•°æ®é›†ä¸ŠéªŒè¯äº†CPCåœ¨å¼±ç›‘ç£åˆ†å‰²ä»»åŠ¡ä¸­çš„é¢†å…ˆæ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†ä¼ªæ ‡ç­¾è´¨é‡å’Œåˆ†å‰²ç²¾åº¦ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Contrastive_Prompt_Clustering_for_Weakly_Supervised_Semantic_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16984v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      56. [HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HiCache__Training-free_Acceleration_of_Diffusion_Models_via_Hermite_Polynomial-based_Feature_Caching.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Jiao Tong University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºHiCacheï¼Œä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åŠ é€Ÿæ¡†æ¶ï¼Œé€šè¿‡å¯¹ç‰¹å¾æ¼”åŒ–çš„é«˜æ–¯åˆ†å¸ƒæ€§è¿›è¡Œç†è®ºéªŒè¯ï¼Œå°†Hermiteå¤šé¡¹å¼å¼•å…¥ç‰¹å¾ç¼“å­˜é¢„æµ‹ï¼Œå¹¶è®¾è®¡äº†åŒç¼©æ”¾æœºåˆ¶ä»¥æå‡æ•°å€¼ç¨³å®šæ€§å’Œé¢„æµ‹ç²¾åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒHiCacheåœ¨æ–‡æœ¬ç”Ÿæˆå›¾åƒã€è§†é¢‘ç”Ÿæˆã€è¶…åˆ†è¾¨ç‡ç­‰ä»»åŠ¡ä¸Šå®ç°æœ€é«˜6.24å€åŠ é€Ÿï¼Œå¹¶åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¼˜äºåŒç±»æ–¹æ³•ï¼Œè¯æ˜äº†æŒ‰æ•°æ®ç»Ÿè®¡æ€§è´¨é€‰æ‹©é¢„æµ‹åŸºå‡½æ•°çš„ç†è®ºå’Œå®è·µä»·å€¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/HiCache_Training-free_Acceleration_of_Diffusion_Models_via_Hermite_Polynomial-based_Feature_Caching.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16975v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      57. [Combating Digitally Altered Images: Deepfake Detection]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Combating_Digitally_Altered_Images__Deepfake_Detection.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Amrita School of Computing</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºVision Transformerï¼ˆViTï¼‰æ¨¡å‹çš„æ·±åº¦ä¼ªé€ ï¼ˆDeepfakeï¼‰å›¾åƒæ£€æµ‹æ–¹æ³•ï¼Œé‡‡ç”¨OpenForensicsæ•°æ®é›†å¹¶ç»“åˆå¤šç§æ•°æ®å¢å¼ºæŠ€æœ¯ä»¥æå‡æ¨¡å‹å¯¹å¤šæ ·åŒ–å›¾åƒç¯¡æ”¹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†â‰¥99%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œè¡¨ç°å‡ºæé«˜çš„æ£€æµ‹æ€§èƒ½å’Œå®é™…åº”ç”¨æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Combating_Digitally_Altered_Images_Deepfake_Detection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16974v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      58. [Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Grounding]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Hierarchical_Contextual_Grounding_LVLM__Enhancing_Fine-Grained_Visual-Language_Understanding_with_Ro.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zhongkai University of Agriculture and Engineering</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†å±‚æ¬¡åŒ–ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†è§‰è¯­è¨€å¤§æ¨¡å‹ï¼ˆHCG-LVLMï¼‰ï¼Œé€šè¿‡å…¨çƒä¸Šä¸‹æ–‡æ„ŸçŸ¥å±‚å’Œç»†ç²’åº¦å±€éƒ¨å®šä½å±‚ç›¸ç»“åˆï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç²¾ç»†è§†è§‰æ¨ç†å’Œç²¾å‡†åŒºåŸŸå®šä½æ–¹é¢çš„èƒ½åŠ›ã€‚æ–¹æ³•åŒ…æ‹¬å±€éƒ¨ç»†èŠ‚å¢å¼ºæ¨¡å—å’Œè¯­ä¹‰ä¸€è‡´æ€§éªŒè¯å™¨ï¼Œæœ‰æ•ˆå‡å°‘å¹»è§‰ç°è±¡å¹¶æå‡æ¨¡å‹åœ¨GQAã€A-OKVQAå’ŒRefCOCOç­‰æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ï¼Œç»“è®ºè¯å®è¯¥ç»“æ„åœ¨ç»†ç²’åº¦è§†è§‰è¯­è¨€ç†è§£ä¸­ä¼˜äºç°æœ‰ä¸»æµæ¨¡å‹ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Hierarchical_Contextual_Grounding_LVLM_Enhancing_Fine-Grained_Visual-Language_Understanding_with_Robust_Grounding.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16950v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      59. [Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventions]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Disentangling_Polysemantic_Neurons_with_a_Null-Calibrated_Polysemanticity_Index_and_Causal_Patch_Int.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">BITS Pilani</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†Polysemanticity Index (PSI)â€”â€”ä¸€ç§ç»“åˆå‡ ä½•èšç±»è´¨é‡ã€ç±»åˆ«æ ‡ç­¾å¯¹é½ä¸CLIPå¼€æ”¾è¯æ±‡è¯­ä¹‰åŒºåˆ†åº¦çš„å¤šè¯æ®ã€æ˜¾è‘—æ€§æ ¡å‡†æŒ‡æ ‡ï¼Œç”¨äºç³»ç»Ÿé‡åŒ–å’Œå‘ç°ç¥ç»ç½‘ç»œä¸­çš„å¤šè¯­ä¹‰ç¥ç»å…ƒã€‚é€šè¿‡å¯¹ResNet-50åœ¨Tiny-ImageNetæ•°æ®é›†ä¸Šçš„å®éªŒï¼ŒPSIä¸ä»…èƒ½åŒºåˆ†çœŸå®ç»“æ„ä¸å™ªå£°ï¼Œè¿˜æ­ç¤ºäº†æ·±å±‚ç½‘ç»œæ›´æ˜“å‡ºç°å¤šè¯­ä¹‰æ€§ï¼Œå¹¶é€šè¿‡å› æœæ€§patch-swapå®éªŒè¯æ˜äº†å‘ç°åŸå‹çš„åŠŸèƒ½ç›¸å…³æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Disentangling_Polysemantic_Neurons_with_a_Null-Calibrated_Polysemanticity_Index_and_Causal_Patch_Interventions.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16942v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      60. [HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysis]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HieroAction__Hierarchically_Guided_VLM_for_Fine-Grained_Action_Analysis.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Beihang University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºHieroActionï¼Œä¸€ç§ç»“åˆè§†è§‰-è¯­è¨€æ¨¡å‹çš„åˆ†å±‚åŠ¨ä½œåˆ†ææ–¹æ³•ï¼ŒåŒ…æ‹¬â€œé€æ­¥åŠ¨ä½œæ¨ç†â€ï¼ˆSARï¼‰ä¸â€œåˆ†å±‚ç­–ç•¥å­¦ä¹ â€ï¼ˆHPLï¼‰ã€‚SARæ¨¡å—å°†å¤æ‚åŠ¨ä½œåˆ†è§£ä¸ºè§‚å¯Ÿã€è¯†åˆ«ã€è¯„ä¼°ä¸ç»“è®ºå››é˜¶æ®µï¼ŒHPLåˆ©ç”¨å¤šç»´å¥–åŠ±å‡½æ•°é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–åˆ†é˜¶æ®µåŠ¨ä½œè´¨é‡è¯„åˆ†ã€‚å®éªŒæ˜¾ç¤ºï¼ŒHieroActionåœ¨FineDiveã€FineFSå’ŒLOGOæ•°æ®é›†ä¸Šå®ç°äº†é«˜å‡†ç¡®ç‡å’Œå¯è§£é‡Šæ€§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/HieroAction_Hierarchically_Guided_VLM_for_Fine-Grained_Action_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16927v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      61. [LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMR]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LGE-Guided_Cross-Modality_Contrastive_Learning_for_Gadolinium-Free_Cardiomyopathy_Screening_in_Cine_.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">ShanghaiTech University</span></p>
<p>æœ¬æ–‡æå‡ºäº†CC-CMRæ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œè·¨æ¨¡æ€ç‰¹å¾å¯¹é½ï¼Œå°†LGEï¼ˆæ™šæœŸé’†å¢å¼ºï¼‰åºåˆ—çš„ç—…ç†ç‰¹å¾éšå¼ç¼–ç è¿›æ— é’†cine CMRåºåˆ—ä¸­ï¼Œåˆ©ç”¨Video Swin Transformerè¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶é€šè¿‡ç‰¹å¾äº¤äº’æ¨¡å—è”åˆä¼˜åŒ–åˆ†ç±»ä¸ç‰¹å¾å¯¹é½ï¼ŒåŒæ—¶å¼•å…¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥æœºåˆ¶åŠ¨æ€è°ƒæ•´ç›®æ ‡æƒé‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸­å¿ƒæ•°æ®ä¸Šæ˜¾è‘—æå‡äº†æ— é’†æ¡ä»¶ä¸‹å¿ƒè‚Œç—…ç­›æŸ¥çš„å‡†ç¡®æ€§ï¼ˆ94.3%ï¼‰ï¼Œä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå…·å¤‡ä¸´åºŠå¯è¡Œæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LGE-Guided_Cross-Modality_Contrastive_Learning_for_Gadolinium-Free_Cardiomyopathy_Screening_in_Cine_CMR.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16911v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      62. [MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MDD__A_Dataset_for_Text-and-Music_Conditioned_Duet_Dance_Generation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Purdue University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†MDDï¼Œä¸€ä¸ªç»“åˆæ–‡æœ¬ã€éŸ³ä¹å’ŒåŠ¨ä½œæ•°æ®çš„å¤§è§„æ¨¡å¤šæ¨¡æ€åŒäººèˆè¹ˆç”Ÿæˆæ•°æ®é›†ã€‚MDDæ¶µç›–15ç§èˆè¹ˆç±»å‹ï¼ŒåŒ…å«620åˆ†é’Ÿé«˜è´¨é‡åŠ¨ä½œæ•æ‰æ•°æ®å’Œè¶…1ä¸‡æ¡ç»†ç²’åº¦æ–‡æœ¬æ³¨é‡Šï¼Œå¹¶æ”¯æŒä¸¤ä¸ªæ–°ä»»åŠ¡ï¼šText-to-Duetï¼ˆæ–‡æœ¬+éŸ³ä¹æ¡ä»¶ä¸‹ç”ŸæˆåŒäººèˆåŠ¨ä½œï¼‰å’ŒText-to-Dance Accompanimentï¼ˆæ–‡æœ¬+éŸ³ä¹+é¢†èˆåŠ¨ä½œæ¡ä»¶ä¸‹ç”Ÿæˆè·Ÿéšè€…åŠ¨ä½œï¼‰ã€‚å®éªŒè¡¨æ˜ï¼ŒMDDæå¤§æå‡äº†å¤šæ¨¡æ€äº¤äº’å¼èˆè¹ˆç”Ÿæˆçš„åŸºå‡†å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MDD_A_Dataset_for_Text-and-Music_Conditioned_Duet_Dance_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16897v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      63. [Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Network]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Generating_Synthetic_Contrast-Enhanced_Chest_CT_Images_from_Non-Contrast_Scans_Using_Slice-Consisten.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Saskatchewan</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºSlice-Consistent Brownian Bridge Diffusion Model (SC-BBDM)çš„æ¡¥æ‰©æ•£æ–¹æ³•ï¼Œå¯ä»éå¢å¼ºCTç”Ÿæˆåˆæˆçš„å¢å¼ºèƒ¸éƒ¨CTå½±åƒã€‚é€šè¿‡å¼•å…¥é£æ ¼é”®æ¡ä»¶ï¼ˆSKCï¼‰å’Œåˆ‡ç‰‡é—´è½¨è¿¹å¯¹é½ï¼ˆISTAï¼‰æœºåˆ¶ï¼Œç»“åˆä¸¥æ ¼çš„é¢„å¤„ç†ï¼ˆé…å‡†å’Œåˆ†å‰²æ©ç ï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨Coltea-Lungæ•°æ®é›†ä¸Šå¯¹æ¯”åŸºçº¿æ¨¡å‹ï¼Œåœ¨ä¿æŒè§£å‰–ç»“æ„å’Œå¢å¼ºå¯¹æ¯”åº¦æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚ç»“è®ºè¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¸´åºŠç›¸å…³åŒºåŸŸæå‡äº†å›¾åƒä¿çœŸåº¦å’Œç©ºé—´ä¸€è‡´æ€§ï¼Œæœ‰æœ›æé«˜æ— é€ å½±å‰‚CTçš„ä¸´åºŠå¯ç”¨æ€§å¹¶é™ä½åŒ»ç–—æˆæœ¬ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Generating_Synthetic_Contrast-Enhanced_Chest_CT_Images_from_Non-Contrast_Scans_Using_Slice-Consistent_Brownian_Bridge_Diffusion_Network.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16884v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      64. [A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanism]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Lightweight_Convolution_and_Vision_Transformer_integrated_model_with_Multi-scale_Self-attention_Me.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Sichuan University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§èåˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰Transformerï¼ˆViTï¼‰çš„è½»é‡çº§æ¨¡å‹SAEViTï¼Œé‡‡ç”¨å¤šå°ºåº¦ç¨€ç–è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSAAï¼‰å’Œé€šé“äº¤äº’å‰é¦ˆç½‘ç»œï¼ˆCIFFNï¼‰ï¼Œä»¥æå‡ç‰¹å¾è¡¨è¾¾èƒ½åŠ›å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒSAEViTåœ¨ImageNet-1Kåˆ†ç±»ã€COCOç›®æ ‡æ£€æµ‹å’ŒADE20Kè¯­ä¹‰åˆ†å‰²ç­‰ä¸»æµè§†è§‰ä»»åŠ¡ä¸Šå®ç°äº†é«˜å‡†ç¡®ç‡å’Œé«˜æ¨ç†é€Ÿåº¦ï¼Œä¼˜äºåŒç±»è½»é‡çº§æ¨¡å‹ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Lightweight_Convolution_and_Vision_Transformer_integrated_model_with_Multi-scale_Self-attention_Mechanism.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16882v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      65. [Multimodal Medical Endoscopic Image Analysis via Progressive Disentangle-aware Contrastive Learning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multimodal_Medical_Endoscopic_Image_Analysis_via_Progressive_Disentangle-aware_Contrastive_Learning.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºâ€œå¯¹é½-è§£è€¦-èåˆâ€(Align-Disentangle-Fusion)æœºåˆ¶çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç”¨äº2Då†…é•œåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œé›†æˆäº†ç™½å…‰æˆåƒ(WLI)ä¸çª„å¸¦æˆåƒ(NBI)ã€‚æ–¹æ³•åŒ…æ‹¬å¤šå°ºåº¦åˆ†å¸ƒå¯¹é½ã€å¤šé˜¶æ®µç‰¹å¾è§£è€¦ä¸è§£è€¦æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ï¼Œå®ç°äº†æ¨¡æ€å·®å¼‚æ¶ˆé™¤å’Œé²æ£’çš„è¯­ä¹‰èåˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä¸´åºŠæ•°æ®é›†ä¸Šåˆ†å‰²ç²¾åº¦ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹æ³›åŒ–å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Multimodal_Medical_Endoscopic_Image_Analysis_via_Progressive_Disentangle-aware_Contrastive_Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16873v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      66. [Do Multimodal LLMs See Sentiment?]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Do_Multimodal_LLMs_See_Sentiment_.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Universidade Tecnologica Federal do ParanÃ¡</span></p>
<p>æœ¬æ–‡æå‡ºMLLMsentæ¡†æ¶ï¼Œç³»ç»Ÿæ€§æ¢ç´¢å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚MiniGPT-4ã€GPT-4o miniã€DeepSeek-VL2-Tinyï¼‰åœ¨å›¾åƒæƒ…æ„Ÿåˆ†æä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æ–¹æ³•åŒ…å«ä¸‰ç§æµç¨‹ï¼šç›´æ¥ç”¨MLLMå¯¹å›¾ç‰‡æƒ…æ„Ÿåˆ†ç±»ï¼›åˆ©ç”¨MLLMè‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡æè¿°å¹¶ç”¨LLMåˆ†ææƒ…æ„Ÿï¼›å¯¹LLMåœ¨æƒ…æ„Ÿæ ‡æ³¨å›¾åƒæè¿°ä¸Šè¿›è¡Œå¾®è°ƒã€‚å®éªŒè¯æ˜ï¼Œå¾®è°ƒåæ¨¡å‹åœ¨ä¸»æµåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºè¯å…¸ã€CNNã€Transformerç­‰ä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨è·¨æ•°æ®é›†æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“è®ºï¼šMLLMsentåœ¨è§†è§‰æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸Šè¾¾åˆ°æ–°SOTAï¼Œå¹¶æå‡äº†è§£é‡Šæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Do_Multimodal_LLMs_See_Sentiment.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16863v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      67. [Delta-SVD: Efficient Compression for Personalized Text-to-Image Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Delta-SVD__Efficient_Compression_for_Personalized_Text-to-Image_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Monash University</span></p>
<p>æœ¬æ–‡æå‡ºDelta-SVDï¼Œä¸€ç§é’ˆå¯¹DreamBoothç­‰ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒåã€å…è®­ç»ƒå‹ç¼©æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹å¾®è°ƒåä¸é¢„è®­ç»ƒæ¨¡å‹æƒé‡çš„å·®åˆ†ï¼ˆdelta weightsï¼‰è¿›è¡Œé€å±‚SVDåˆ†è§£ï¼Œå¹¶é‡‡ç”¨èƒ½é‡é˜ˆå€¼è‡ªé€‚åº”æˆªæ–­ï¼Œå¾—åˆ°ä½ç§©è¿‘ä¼¼è¡¨ç¤ºï¼Œå®ç°å¤§å¹…åº¦å‡å°å­˜å‚¨ç©ºé—´ã€å®Œå…¨ä¿ç•™åŸæœ‰æ¨¡å‹ç»“æ„ä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤§å¹…å‹ç¼©å­˜å‚¨éœ€æ±‚çš„åŒæ—¶ï¼ŒDelta-SVDå‡ ä¹ä¸æŸå¤±å›¾åƒç”Ÿæˆè´¨é‡ï¼ˆåœ¨CLIPã€SSIMã€FIDç­‰æŒ‡æ ‡ä¸Šä¸å…¨é‡å¾®è°ƒæ¨¡å‹ç›¸å½“ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºLoRAå’Œ8bité‡åŒ–ç­‰æ–¹æ³•ï¼Œé€‚åˆé«˜æ•ˆéƒ¨ç½²å’Œåˆ†å‘å¤§é‡ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Delta-SVD_Efficient_Compression_for_Personalized_Text-to-Image_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17017v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      68. [Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dual_Orthogonal_Guidance_for_Robust_Diffusion-based_Handwritten_Text_Generation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">LuleÃ¥ University of Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡‡æ ·æŒ‡å¯¼ç­–ç•¥Dual Orthogonal Guidance (DOG)ï¼Œç”¨äºæå‡æ‰©æ•£æ¨¡å‹åœ¨æ‰‹å†™æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„å†…å®¹æ¸…æ™°åº¦å’Œé£æ ¼å¤šæ ·æ€§ã€‚DOGé€šè¿‡è´Ÿå‘æ‰°åŠ¨æ¡ä»¶çš„æ­£äº¤æŠ•å½±å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œç»“åˆä¸‰è§’è°ƒåº¦æœºåˆ¶æ§åˆ¶æŒ‡å¯¼å¼ºåº¦ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯é›†æˆç°æœ‰æ¨¡å‹ã€‚åœ¨DiffusionPenå’ŒOne-DMç­‰ä¸»æµæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDOGæ–¹æ³•èƒ½æ˜¾è‘—å¢å¼ºç”Ÿæˆæ–‡æœ¬çš„å¯è¯»æ€§å’Œé£æ ¼å¤šæ ·æ€§ï¼Œå°¤å…¶åœ¨é¢å¯¹è®­ç»ƒé›†ä¸­æœªè§è¿‡çš„è¯æˆ–é£æ ¼æ—¶è¡¨ç°ä¼˜å¼‚ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Dual_Orthogonal_Guidance_for_Robust_Diffusion-based_Handwritten_Text_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16956v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      69. [RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Haze]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/RPD-Diff__Region-Adaptive_Physics-Guided_Diffusion_Model_for_Visibility_Enhancement_under_Dense_and_.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Intelligent Systems Engineering, Shenzhen Campus of Sun Yat-Sen University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†RPD-Diffï¼Œä¸€ç§åŒºåŸŸè‡ªé€‚åº”ç‰©ç†å¼•å¯¼å»é›¾æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆç‰©ç†å¼•å¯¼çš„ä¸­é—´çŠ¶æ€ç›®æ ‡ï¼ˆPISTï¼‰å’Œé›¾æ„ŸçŸ¥å»å™ªæ­¥é•¿é¢„æµ‹å™¨ï¼ˆHADTPï¼‰ï¼Œä»¥è§£å†³å¯†é›†å’Œéå‡åŒ€é›¾æ¡ä»¶ä¸‹çš„å•å¹…å›¾åƒå»é›¾é—®é¢˜ã€‚å®éªŒè¡¨æ˜RPD-Diffåœ¨å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒç»†èŠ‚å’Œè‰²å½©è¿˜åŸèƒ½åŠ›ï¼Œé€‚ç”¨äºå¤æ‚çœŸå®åœºæ™¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/RPD-Diff_Region-Adaptive_Physics-Guided_Diffusion_Model_for_Visibility_Enhancement_under_Dense_and_Non-Uniform_Haze.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16930v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      70. [HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HunyuanVideo-Foley__Multimodal_Diffusion_with_Representation_Alignment_for_High-Fidelity_Foley_Audio.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tencent Hunyuan</span></p>
<p>æœ¬æ–‡æå‡ºHunyuanVideo-Foleyï¼Œä¸€ä¸ªç«¯åˆ°ç«¯æ–‡æœ¬-è§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆè‡ªåŠ¨åŒ–é«˜è´¨é‡æ•°æ®ç®¡é“ã€è¡¨ç¤ºå¯¹é½æŸå¤±ï¼ˆREPAï¼‰å’Œå¤šæ¨¡æ€æ‰©æ•£Transformerï¼Œå®ç°éŸ³é¢‘ä¸è§†è§‰åŠ¨æ€å’Œè¯­ä¹‰ç²¾å‡†å¯¹é½ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨éŸ³é¢‘ä¿çœŸåº¦ã€è§†è¯­å¯¹é½ã€æ—¶é—´åŒæ­¥å’Œåˆ†å¸ƒåŒ¹é…ç­‰æ–¹é¢å‡è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°è§†é¢‘è¯­ä¹‰é©±åŠ¨éŸ³é¢‘ç”Ÿæˆçš„SOTAæ°´å¹³ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/HunyuanVideo-Foley_Multimodal_Diffusion_with_Representation_Alignment_for_High-Fidelity_Foley_Audio_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17434v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      71. [TinySR: Pruning Diffusion for Real-World Image Super-Resolution]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TinySR__Pruning_Diffusion_for_Real-World_Image_Super-Resolution.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zhejiang University</span></p>
<p>æœ¬æ–‡æå‡ºäº†TinySRï¼Œä¸€ç§é’ˆå¯¹çœŸå®åœºæ™¯å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰çš„é«˜æ•ˆç´§å‡‘æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡åŠ¨æ€åˆ†å—æ¿€æ´»å’Œæ‰©å±•-è…èš€ç­–ç•¥å®ç°æ·±åº¦å‰ªæï¼Œå¹¶ç»“åˆVAEé€šé“å‰ªæã€æ³¨æ„åŠ›ç§»é™¤å’Œè½»é‡åŒ–å·ç§¯ç­‰ç»„ä»¶ä¼˜åŒ–ï¼Œæœ‰æ•ˆå‹ç¼©æ¨¡å‹ä½“ç§¯å’Œè®¡ç®—é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒTinySRåœ¨ä¿æŒæ„ŸçŸ¥è´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†5.68å€åŠ é€Ÿå’Œ83%å‚æ•°é‡å‡å°‘ï¼Œæ˜¾è‘—ä¼˜äºåŒç±»æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TinySR_Pruning_Diffusion_for_Real-World_Image_Super-Resolution.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17524v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      72. [OmniMRI: A Unified Visionâ€“Language Foundation Model for Generalist MRI Interpretation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/OmniMRI__A_Unified_Vision--Language_Foundation_Model_for_Generalist_MRI_Interpretation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Athinoula A. Martinos Center for Biomedical Imaging, Harvard Medical School</span></p>
<p>OmniMRIæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œé’ˆå¯¹MRIä¸´åºŠæµç¨‹ä¸­çš„å›¾åƒé‡å»ºã€åˆ†å‰²ã€å¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­å»ºè®®å’ŒæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ï¼Œé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒï¼ˆè‡ªç›‘ç£è§†è§‰é¢„è®­ç»ƒã€è§†è§‰-è¯­è¨€å¯¹é½ã€å¤šæ¨¡æ€é¢„è®­ç»ƒå’Œå¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒï¼‰ï¼Œèåˆäº†60ä¸ªå…¬å¼€æ•°æ®é›†çš„å¤§è§„æ¨¡å¼‚æ„MRIæ•°æ®ï¼Œå®ç°äº†è·¨ä»»åŠ¡å’Œè·¨è§£å‰–éƒ¨ä½çš„æ³›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒOmniMRIåœ¨å•ä¸€æ¶æ„ä¸‹å¯å®ç°å¤šç§ä»»åŠ¡çš„é«˜è´¨é‡è¾“å‡ºï¼Œæœ‰æ•ˆæ•´åˆäº†å½±åƒç†è§£ä¸ä¸´åºŠè¯­ä¹‰æ¨ç†ï¼Œä¸ºåŒ»å­¦å½±åƒè‡ªåŠ¨åŒ–å’Œæ™ºèƒ½åŒ–å¥ å®šäº†åŸºç¡€ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/OmniMRI_A_Unified_Vision%E2%80%93Language_Foundation_Model_for_Generalist_MRI_Interpretation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17502v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      73. [Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Social-MAE__A_Transformer-Based_Multimodal_Autoencoder_for_Face_and_Voice.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Numediart Institute, University of Mons</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†Social-MAEæ¨¡å‹ï¼Œå°†Transformeræ¶æ„ä¸è‡ªç›‘ç£Masked Autoencoderæ–¹æ³•ç›¸ç»“åˆï¼Œä¸“ä¸ºå¤šå¸§è¾“å…¥çš„éŸ³è§†é¢‘ç¤¾äº¤æ•°æ®è®¾è®¡ï¼Œé€šè¿‡åœ¨å¤§è§„æ¨¡äººç±»ç¤¾äº¤æ•°æ®é›†ï¼ˆVoxCeleb2ï¼‰ä¸Šé¢„è®­ç»ƒï¼Œå®ç°äº†é«˜æ—¶åºè§†è§‰ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSocial-MAEåœ¨æƒ…æ„Ÿè¯†åˆ«ã€ç¬‘å£°æ£€æµ‹ç­‰å¤šæ¨¡æ€ç¤¾äº¤ä»»åŠ¡ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼ŒéªŒè¯äº†é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒçš„é‡è¦æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Social-MAE_A_Transformer-Based_Multimodal_Autoencoder_for_Face_and_Voice.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17497v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      74. [Multimodal Representation Learning Conditioned on Semantic Relations]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multimodal_Representation_Learning_Conditioned_on_Semantic_Relations.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Emory University</span></p>
<p>æœ¬æ–‡æå‡ºäº†RCMLæ¡†æ¶ï¼Œå°†è¯­ä¹‰å…³ç³»ä½œä¸ºæ¡ä»¶å¼•å…¥å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼Œé€šè¿‡æ„é€ è‡ªç„¶è¯­è¨€å…³ç³»æè¿°çš„å¤šå¯¹å¤šè®­ç»ƒæ ·æœ¬ï¼Œå¹¶é‡‡ç”¨å…³ç³»å¼•å¯¼çš„è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°å¤šæ¨¡æ€ç‰¹å¾åœ¨ç‰¹å®šè¯­ä¹‰ä¸Šä¸‹æ–‡ä¸‹çš„æå–ä¸å¯¹é½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRCMLåœ¨ä¸ƒä¸ªäº§å“é¢†åŸŸçš„æ£€ç´¢å’Œåˆ†ç±»ä»»åŠ¡ä¸­å‡ä¼˜äºä¸»æµåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†åˆ©ç”¨è¯­ä¹‰å…³ç³»ä¿ƒè¿›å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Multimodal_Representation_Learning_Conditioned_on_Semantic_Relations.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17442v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      75. [Multi-Level LVLM Guidance for Untrimmed Video Action Recognition]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multi-Level_LVLM_Guidance_for_Untrimmed_Video_Action_Recognition.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Kunming University of Science and Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†Event-Contextualized Video Transformer (ECVT) æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ç”Ÿæˆçš„å¤šå±‚æ¬¡è¯­ä¹‰æè¿°ï¼ˆå…¨å±€äº‹ä»¶æç¤ºå’Œæ—¶åºå­äº‹ä»¶æç¤ºï¼‰ï¼Œä»¥å¼•å¯¼è§†é¢‘Transformerå¯¹æœªå‰ªè¾‘è§†é¢‘ä¸­çš„åŠ¨ä½œè¿›è¡Œè¯†åˆ«å’Œå®šä½ã€‚å¤šå±‚æ¬¡æ–‡æœ¬æç¤ºä¸è§†é¢‘ç‰¹å¾èåˆï¼Œåˆ©ç”¨è‡ªé€‚åº”é—¨æ§ã€é«˜çº§è¯­ä¹‰èåˆã€è·¨æ¨¡æ€æ³¨æ„åŠ›å’Œäº‹ä»¶å›¾æ¨¡å—è¿›è¡Œæ—¶åºæ ¡å‡†ï¼Œæå‡äº†æ¨¡å‹å¯¹å¤æ‚åŠ¨ä½œæ—¶åºç»“æ„å’Œäº‹ä»¶é€»è¾‘çš„ç†è§£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒECVTåœ¨ActivityNet v1.3å’ŒTHUMOS14æ•°æ®é›†ä¸Šå‡å–å¾—äº†å½“å‰æœ€ä¼˜æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Multi-Level_LVLM_Guidance_for_Untrimmed_Video_Action_Recognition.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17435v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      76. [An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/An_LLM-LVLM_Driven_Agent_for_Iterative_and_Fine-Grained_Image_Editing.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Kunming University of Science and Technology</span></p>
<p>æœ¬æ–‡æå‡ºRefineEdit-Agentï¼Œä¸€ä¸ªæ— è®­ç»ƒã€ä»¥å¤§è¯­è¨€æ¨¡å‹(LLM)å’Œè§†è§‰-è¯­è¨€å¤§æ¨¡å‹(LVLM)ä¸ºæ ¸å¿ƒçš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œå®ç°å¤æ‚ã€å¤šè½®ã€ç»†ç²’åº¦çš„å›¾åƒç¼–è¾‘ã€‚æ–¹æ³•åŒ…æ‹¬LVLMè§£ææŒ‡ä»¤å’Œåœºæ™¯ã€LLMåˆ†è§£ç›®æ ‡å’Œå·¥å…·é€‰æ‹©ã€æ‰§è¡Œç¼–è¾‘ã€LVLMåé¦ˆè¯„ä¼°ï¼Œå½¢æˆé—­ç¯è¿­ä»£ä¼˜åŒ–ã€‚å®éªŒåœ¨æ–°æ„å»ºçš„LongBench-T2I-EditåŸºå‡†ï¼ˆå«ä¹å¤§è§†è§‰ç»´åº¦ã€500ç»„å¤æ‚æŒ‡ä»¤ï¼‰ä¸Šï¼ŒRefineEdit-Agentåœ¨ç¼–è¾‘ä¿çœŸåº¦å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¸»æµæ–¹æ³•ï¼Œå¹³å‡å¾—åˆ†3.67ï¼Œå±•ç°å‡ºå¼ºå¤§å¤šè½®ç»†ç²’åº¦ç¼–è¾‘èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/An_LLM-LVLM_Driven_Agent_for_Iterative_and_Fine-Grained_Image_Editing.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17394v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      77. [Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosis]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Lightweight_Joint_Optimization_of_General-Purpose_Vision-Language_Models_and_Retrievers_for_Medical_.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Computer Science and Engineering, The Hebrew University of Jerusalem</span></p>
<p>æœ¬è®ºæ–‡æå‡ºJOMEDæ–¹æ³•ï¼Œå®ç°é€šç”¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ä¸å¤šæ¨¡æ€æ£€ç´¢å™¨çš„è”åˆä¼˜åŒ–ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ä¸è§†è§‰é—®ç­”ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸€èˆ¬æ€§ä¸»å¹²ç½‘ç»œï¼Œé€šè¿‡è½»é‡çº§å¾®è°ƒï¼Œé¦–å…ˆç”¨å†»ç»“æ£€ç´¢å™¨å¯¹LVLMè¿›è¡Œå¸¦æ£€ç´¢å¢å¼ºçš„å¾®è°ƒï¼Œç„¶åå†»ç»“LVLMå¯¹æ£€ç´¢å™¨è¿›è¡ŒKLæ•£åº¦æŸå¤±ä¼˜åŒ–ï¼Œæå‡æ£€ç´¢å†…å®¹ä¸ä¸‹æ¸¸ä»»åŠ¡çš„å…³è”æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤šé¡¹åŒ»å­¦å½±åƒåˆ†ç±»ä¸è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šï¼ŒJOMEDåœ¨æ— åŒ»å­¦é¢„è®­ç»ƒæƒ…å†µä¸‹å–å¾—ä¸åŒ»å­¦é¢„è®­ç»ƒæ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ•ˆæœï¼Œå°¤å…¶åœ¨ä¸åŒæ£€ç´¢å†…å®¹å¯¼è‡´é¢„æµ‹ä¸ä¸€è‡´çš„æŒ‘æˆ˜æ€§åœºæ™¯ä¸‹æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚ç»“è®ºæ˜¯ï¼šè”åˆä¼˜åŒ–çš„è½»é‡çº§æ–¹æ³•ï¼Œåœ¨åŒ»å­¦è¯Šæ–­ä»»åŠ¡å®ç°äº†é«˜æ•ˆä¸”å…·ç«äº‰åŠ›çš„è¡¨ç°ï¼Œå¹¶ä¸ºæœªæ¥ä¼˜åŒ–æ£€ç´¢-æ¨ç†æµç¨‹æä¾›äº†æ–¹å‘ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Lightweight_Joint_Optimization_of_General-Purpose_Vision-Language_Models_and_Retrievers_for_Medical_Diagnosis.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17389v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      78. [Neural Proteomics Fields for Super-resolved Spatial Proteomics Prediction]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Neural_Proteomics_Fields_for_Super-resolved_Spatial_Proteomics_Prediction.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Artificial Intelligence, University of Chinese Academy of Sciences</span></p>
<p>è¯¥è®ºæ–‡æå‡ºNeural Proteomics Fields (NPF)ï¼Œä¸€ç§é’ˆå¯¹æµ‹åºå‹ç©ºé—´è›‹ç™½ç»„ï¼ˆseq-SPï¼‰è¶…åˆ†è¾¨ç‡é¢„æµ‹çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚NPFé‡‡ç”¨ç©ºé—´å»ºæ¨¡æ¨¡å—å’Œå½¢æ€å»ºæ¨¡æ¨¡å—ï¼Œé€šè¿‡ä½ç½®é¢‘è°±ç¼–ç å’Œç—…ç†å›¾åƒç‰¹å¾èåˆï¼Œå®ç°è›‹ç™½è¡¨è¾¾åœ¨è¿ç»­ç©ºé—´ä¸Šçš„é‡å»ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNPFåœ¨å‚æ•°é‡æ›´å°‘çš„æƒ…å†µä¸‹è¾¾åˆ°ä¸šç•Œé¢†å…ˆçš„é¢„æµ‹æ€§èƒ½ï¼Œå¹¶é¦–æ¬¡å»ºç«‹äº†å¼€æ”¾åŸºå‡†æ•°æ®é›†ï¼Œæ¨åŠ¨ç©ºé—´è›‹ç™½ç»„ç ”ç©¶å‘å±•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Neural_Proteomics_Fields_for_Super-resolved_Spatial_Proteomics_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17356v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      79. [DiCache: Let Diffusion Model Determine Its Own Cache]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DiCache__Let_Diffusion_Model_Determine_Its_Own_Cache.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Jiao Tong University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€æ¨¡å‹æ— å…³çš„è‡ªé€‚åº”ç¼“å­˜åŠ é€Ÿæ–¹æ³•DiCacheï¼Œç”¨äºæå‡æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚WAN 2.1ã€HunyuanVideoå’ŒFluxï¼‰çš„æ¨ç†é€Ÿåº¦å’Œç”Ÿæˆè´¨é‡ã€‚å…¶æ ¸å¿ƒæŠ€æœ¯åŒ…æ‹¬åœ¨çº¿æµ…å±‚æ¢é’ˆå‰–é¢æ–¹æ¡ˆï¼Œå¯å®æ—¶æ ¹æ®æµ…å±‚ç‰¹å¾å˜åŒ–ä¼°ç®—ç¼“å­˜è¯¯å·®è‡ªé€‚åº”å†³å®šç¼“å­˜æ—¶æœºï¼Œä»¥åŠåŠ¨æ€ç¼“å­˜è½¨è¿¹å¯¹é½ï¼Œé€šè¿‡æµ…å±‚ç‰¹å¾è½¨è¿¹ç»„åˆå¤šæ­¥ç¼“å­˜ä»¥ç²¾ç¡®é€¼è¿‘å½“å‰ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼ŒDiCacheåœ¨å¤šç§ä¸»æµè§†è§‰ç”Ÿæˆä»»åŠ¡ä¸Šç›¸è¾ƒç°æœ‰æ–¹æ³•å®ç°äº†æ›´é«˜çš„æ¨ç†æ•ˆç‡å’Œæ›´ä¼˜çš„è§†è§‰ä¿çœŸåº¦ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/DiCache_Let_Diffusion_Model_Determine_Its_Own_Cache.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17302v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      80. [PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editing]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PosBridge__Multi-View_Positional_Embedding_Transplant_for_Identity-Aware_Image_Editing.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The University of Electro-Communications</span></p>
<p>æœ¬æ–‡æå‡ºäº†PosBridgeï¼Œä¸€ç§è®­ç»ƒæ— å…³çš„å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œé€šè¿‡ç§»æ¤å‚è€ƒå›¾åƒçš„ä½ç½®ä¿¡åµŒå…¥å’ŒCorner Centered Layoutï¼Œå®ç°äº†é«˜ä¿çœŸã€ç»“æ„ä¸€è‡´çš„ç›®æ ‡å¯¹è±¡æ’å…¥ã€‚å®éªŒè¡¨æ˜ï¼ŒPosBridgeåœ¨è¯­ä¹‰ä¸€è‡´æ€§ã€å¤–è§‚é€¼çœŸåº¦å’Œç”Ÿæˆé²æ£’æ€§ä¸Šå‡ä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ï¼Œä¸”å¯é€‰çš„LoRAæ¨¡å—è¿›ä¸€æ­¥æå‡ç»†èŠ‚è¡¨ç°ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PosBridge_Multi-View_Positional_Embedding_Transplant_for_Identity-Aware_Image_Editing.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17298v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      81. [Explain Before You Answer: A Survey on Compositional Visual Reasoning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Explain_Before_You_Answer__A_Survey_on_Compositional_Visual_Reasoning.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Monash University</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿç»¼è¿°äº†2023-2025å¹´é—´260ä½™ç¯‡å…³äºç»„åˆå¼è§†è§‰æ¨ç†ï¼ˆCompositional Visual Reasoning, CVRï¼‰çš„å‰æ²¿è®ºæ–‡ï¼Œæ¢³ç†äº†ä»å•ä½“è§†è§‰-è¯­è¨€æ¨¡å‹åˆ°é“¾å¼æ€ç»´å’Œè‡ªä¸»ä½“VLMçš„äº”é˜¶æ®µèŒƒå¼æ¼”è¿›ï¼Œè¯¦ç»†åˆ†æäº†CVRåœ¨è®¤çŸ¥å¯¹é½ã€å¯è§£é‡Šæ€§ã€æ³›åŒ–æ€§åŠæ•°æ®æ•ˆç‡ç­‰æ–¹é¢çš„ä¼˜åŠ¿ã€‚ç»“è®ºæŒ‡å‡ºï¼Œç»„åˆå¼è§†è§‰æ¨ç†é€šè¿‡æ˜¾å¼ä¸­é—´æ¨ç†æ­¥éª¤æå‡äº†è§†è§‰æ¨ç†ç³»ç»Ÿçš„é€æ˜æ€§ã€é²æ£’æ€§åŠé€šç”¨æ€§ï¼Œå¹¶æå‡ºäº†æœªæ¥åœ¨ä¸–ç•Œæ¨¡å‹é›†æˆã€äººæœºååŒæ¨ç†åŠæ›´ç»†è‡´è¯„æµ‹åè®®ä¸Šçš„ç ”ç©¶æ–¹å‘ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Explain_Before_You_Answer_A_Survey_on_Compositional_Visual_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17283v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      82. [Quickly Tuning Foundation Models for Image Segmentation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Quickly_Tuning_Foundation_Models_for_Image_Segmentation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Freiburg</span></p>
<p>æœ¬æ–‡æå‡ºQTT-SEGæ–¹æ³•ï¼Œé€šè¿‡åŸºäºå…ƒå­¦ä¹ çš„æ€§èƒ½ä¸æˆæœ¬é¢„æµ‹å™¨ç»“åˆè´å¶æ–¯ä¼˜åŒ–ï¼Œå®ç°å¯¹SAMï¼ˆSegment Anything Modelï¼‰åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸‹çš„å¿«é€Ÿè‡ªåŠ¨åŒ–å¾®è°ƒã€‚QTT-SEGèƒ½åœ¨200ä¸‡ä»¥ä¸Šé…ç½®ç©ºé—´ä¸­é«˜æ•ˆæœç´¢ï¼Œåœ¨13ä¸ªåˆ†å‰²æ•°æ®é›†ä¸Šï¼Œæ˜¾è‘—ä¼˜äºSAMé›¶æ ·æœ¬è¡¨ç°ï¼Œå¹¶åœ¨ç»å¤§å¤šæ•°äºŒåˆ†ç±»ä»»åŠ¡ä¸‹è¶…è¿‡AutoGluon Multimodalçš„è¡¨ç°ï¼Œä¸”æ”¶æ•›é€Ÿåº¦å¿«ã€‚ç»“è®ºè¡¨æ˜QTT-SEGå…·å¤‡é«˜æ•ˆã€é²æ£’ã€ç¨³å®šçš„æ¨¡å‹é€‚åº”èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å¤šé¢†åŸŸåˆ†å‰²ä»»åŠ¡ä¸‹è¡¨ç°çªå‡ºã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Quickly_Tuning_Foundation_Models_for_Image_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17282v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      83. [ERF-BA-TFD+: A Multimodal Model for Audio-Visual Deepfake Detection]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ERF-BA-TFD+__A_Multimodal_Model_for_Audio-Visual_Deepfake_Detection.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Lanzhou University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºERF-BA-TFD+çš„å¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹ï¼Œé€šè¿‡èåˆå¢å¼ºæ„Ÿå—é‡ï¼ˆERFï¼‰æ¨¡å—ä¸éŸ³è§†é¢‘ç‰¹å¾ï¼Œåˆ©ç”¨MViTv2è§†è§‰ç¼–ç å™¨å’ŒBYOL-AéŸ³é¢‘ç¼–ç å™¨æå–ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨CRATransè·¨æ¨¡æ€é‡å»ºæ³¨æ„åŠ›æœºåˆ¶å¯¹æ—¶åºä¸€è‡´æ€§è¿›è¡Œæ£€æµ‹ï¼Œå®ç°äº†å¯¹é•¿æ—¶é•¿ã€éŸ³è§†é¢‘ä¸åŒæ­¥ç­‰å¤æ‚æƒ…å†µçš„æ·±åº¦ä¼ªé€ å†…å®¹ç²¾ç¡®æ£€æµ‹ä¸å®šä½ã€‚åœ¨DDL-AVæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®ç‡ã€å¬å›ç‡ç­‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å›½é™…ç«èµ›ä¸­è·å¾—ç¬¬ä¸€åï¼ŒéªŒè¯äº†å…¶å®ç”¨æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ERF-BA-TFD%2B_A_Multimodal_Model_for_Audio-Visual_Deepfake_Detection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17280v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      84. [MTNet: Learning Modality-Aware Representation with Transformer for RGBT Tracking]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MTNet__Learning_modality-aware_representation_with_transformer_for_RGBT_tracking.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Nanjing University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„æ¨¡æ€æ„ŸçŸ¥RGBTç›®æ ‡è·Ÿè¸ªæ–¹æ³•MTNetã€‚æ–¹æ³•åˆ›æ–°æ€§åœ°è®¾è®¡äº†æ¨¡æ€æ„ŸçŸ¥ç½‘ç»œï¼ˆåŒ…æ‹¬é€šé“èšåˆä¸åˆ†å¸ƒæ¨¡å—CADMå’Œç©ºé—´ç›¸ä¼¼æ€§æ„ŸçŸ¥æ¨¡å—SSPMï¼‰ï¼Œåˆ©ç”¨æ··åˆTransformerèåˆç½‘ç»œå®ç°å¤šæ¨¡æ€ç‰¹å¾çš„å…¨å±€ä¾èµ–å»ºæ¨¡ï¼Œå¹¶é€šè¿‡ä¸‰åˆ†æ”¯é¢„æµ‹å¤´å’ŒåŠ¨æ€æ¨¡æ¿æ›´æ–°ç­–ç•¥æå‡å®šä½ç²¾åº¦ä¸ç¨³å¥æ€§ã€‚å®éªŒåœ¨ä¸‰å¤§RGBTåŸºå‡†ä¸Šå‡è¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶å®ç°å®æ—¶é€Ÿåº¦ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MTNet_Learning_Modality-Aware_Representation_with_Transformer_for_RGBT_Tracking.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17255v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      85. [SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SEER-VAR__Semantic_Egocentric_Environment_Reasoner_for_Vehicle_Augmented_Reality.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Eberhard-Karls-Universitat TÃ¼bingen</span></p>
<p>è¯¥è®ºæ–‡æå‡ºSEER-VARæ¡†æ¶ï¼Œé›†æˆæ·±åº¦å¼•å¯¼çš„è¯­ä¹‰åˆ†è§£ã€åŒåˆ†æ”¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥SLAMå’ŒGPTé©±åŠ¨çš„æ¨èç³»ç»Ÿï¼Œå®ç°è½¦è¾†é©¾é©¶åœºæ™¯ä¸‹è½¦å†…å¤–ç¯å¢ƒçš„åˆ†ç¦»å’Œå®æ—¶å®šä½ï¼Œå¹¶åŸºäºè§†è§‰-è¯­è¨€ç†è§£ç”Ÿæˆæƒ…å¢ƒç›¸å…³ARä¿¡æ¯å åŠ ã€‚å®éªŒè¡¨æ˜ï¼ŒSEER-VARåœ¨å¤šåœºæ™¯ä¸‹å®ç°äº†ç¨³å¥çš„ç©ºé—´å¯¹é½å’Œé«˜æ„ŸçŸ¥ä¸€è‡´æ€§çš„ARæ¸²æŸ“ï¼Œæå‡äº†é©¾é©¶å‘˜æ„ŸçŸ¥ä¸ä½“éªŒã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SEER-VAR_Semantic_Egocentric_Environment_Reasoner_for_Vehicle_Augmented_Reality.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17239v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      86. [PersPose: 3D Human Pose Estimation with Perspective Encoding and Perspective Rotation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PersPose__3D_Human_Pose_Estimation_with_Perspective_Encoding_and_Perspective_Rotation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Southern University of Science and Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†PersPoseæ¡†æ¶ï¼Œåˆ›æ–°æ€§åœ°å¼•å…¥äº†Perspective Encodingï¼ˆPEï¼‰æ¨¡å—ï¼Œå°†è£å‰ªå›¾åƒçš„ç›¸æœºå†…å‚ç¼–ç ä¸º2Dæ˜ å°„ï¼Œå¹¶Perspective Rotationï¼ˆPRï¼‰æ¨¡å—å°†äººä½“å±…ä¸­ä»¥å‡å°‘é€è§†å¤±çœŸï¼ŒäºŒè€…è”åˆè¾“å…¥åˆ°CNNä¸­è¿›è¡Œ3Däººä½“å§¿æ€ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPersPoseåœ¨3DPWã€MPI-INF-3DHPå’ŒHuman3.6Mç­‰æ•°æ®é›†ä¸Šå‡å–å¾—äº†å½“å‰æœ€ä¼˜æ€§èƒ½ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨å˜åŒ–é€è§†å…³ç³»ä¸‹çš„é²æ£’æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PersPose_3D_Human_Pose_Estimation_with_Perspective_Encoding_and_Perspective_Rotation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17205v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      87. [Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multi-Agent_Visual-Language_Reasoning_for_Comprehensive_Highway_Scene_Understanding.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Georgia</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“è§†è§‰-è¯­è¨€æ¨ç†æ¡†æ¶ï¼Œç»“åˆå¤§å‹é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰åˆ©ç”¨é¢†åŸŸçŸ¥è¯†ç”Ÿæˆä»»åŠ¡ç‰¹å®šçš„Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºï¼Œå¼•å¯¼å°å‹é«˜æ•ˆVLMï¼ˆå¦‚Qwen2.5-VL-7Bï¼‰å¯¹çŸ­è§†é¢‘å’Œå¤šæ¨¡æ€æ•°æ®è¿›è¡Œæ¨ç†ï¼Œå®ç°å¯¹å¤©æ°”åˆ†ç±»ã€è·¯é¢æ¹¿æ»‘è¯„ä¼°å’Œäº¤é€šæ‹¥å µæ£€æµ‹ç­‰å¤šä»»åŠ¡ç»¼åˆåœºæ™¯ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šæ ·çš„äº¤é€šå’Œç¯å¢ƒæ¡ä»¶ä¸‹è¡¨ç°å‡ºå¼ºå¥ä¸”é«˜æ•ˆçš„å¤šä»»åŠ¡æ¨ç†èƒ½åŠ›ï¼Œå¤§å¹…æå‡å‡†ç¡®ç‡ï¼Œå¹¶æ˜“äºé›†æˆè¿›ç°æœ‰äº¤é€šæ‘„åƒå¤´ç³»ç»Ÿï¼Œæœ‰åŠ©äºæå‡é“è·¯å®‰å…¨å’Œè‡ªåŠ¨åŒ–ç›‘æµ‹èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Multi-Agent_Visual-Language_Reasoning_for_Comprehensive_Highway_Scene_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17199v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      88. [MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labeling]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MMCIG__Multimodal_Cover_Image_Generation_for_Text-only_Documents_and_Its_Dataset_Construction_via_Ps.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Chungnam National University</span></p>
<p>æœ¬æ–‡æå‡ºäº†MMCIGä»»åŠ¡ï¼šé’ˆå¯¹ä»…å«æ–‡æœ¬çš„æ–‡æ¡£ï¼Œè‡ªåŠ¨ç”Ÿæˆç²¾ç‚¼æ‘˜è¦å’Œä¸ä¹‹è§†è§‰ä¸€è‡´çš„å°é¢å›¾ç‰‡ã€‚æ ¸å¿ƒæ–¹æ³•ä¸ºå¤šæ¨¡æ€ä¼ªæ ‡ç­¾æ•°æ®é›†æ„å»ºï¼Œç»“åˆå›¾åƒä¸å›¾ç‰‡è¯´æ˜çš„ç‹¬ç«‹æ’åºï¼Œç­›é€‰é«˜ä¸€è‡´æ€§æ ·æœ¬ï¼Œå®éªŒå’Œäººå·¥è¯„æµ‹å‡è¡¨æ˜è¯¥æ–¹æ³•æ„å»ºçš„æ•°æ®é›†æ›´é«˜è´¨é‡ï¼Œæ¨¡å‹ç”Ÿæˆçš„å›¾ç‰‡ä¸æ‘˜è¦é«˜åº¦åŒ¹é…ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MMCIG_Multimodal_Cover_Image_Generation_for_Text-only_Documents_and_Its_Dataset_Construction_via_Pseudo-labeling.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17186v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      89. [Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Advancing_Weakly-Supervised_Change_Detection_in_Satellite_Images_via_Adversarial_Class_Prompting.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Wuhan University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAdversarial Class Promptingï¼ˆAdvCPï¼‰çš„æ–°æ–¹æ³•ï¼Œä¸“æ³¨äºå«æ˜Ÿå›¾åƒä¸­çš„å¼±ç›‘ç£å˜åŒ–æ£€æµ‹ï¼ˆWSCDï¼‰ï¼Œæ ¸å¿ƒæµç¨‹åŒ…æ‹¬å¯¹æœªå˜åœºæ™¯æ–½åŠ å¯¹æŠ—æ€§æ ‡ç­¾æ‰°åŠ¨ä»¥æŒ–æ˜ä¼ªå˜åŒ–å™ªå£°æ ·æœ¬ï¼Œå¹¶ç»“åˆå…¨å±€åœ¨çº¿åŸå‹è¿›è¡Œå¯¹æŠ—æ ·æœ¬æ ¡æ­£ã€‚å®éªŒè¡¨æ˜ï¼ŒAdvCPå¯æ— ç¼é›†æˆåˆ°å¤šç§ä¸»æµWSCDæ–¹æ³•ä¸­ï¼Œåœ¨å¤šä¸ªé¥æ„Ÿå˜åŒ–æ£€æµ‹æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡IoUå’ŒF1åˆ†æ•°ï¼Œä¸”æ— éœ€å¢åŠ æ¨ç†å¼€é”€ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Advancing_Weakly-Supervised_Change_Detection_in_Satellite_Images_via_Adversarial_Class_Prompting.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17172v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      90. [VROOM - Visual Reconstruction over Onboard Multiview]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/VROOM_-_Visual_Reconstruction_over_Onboard_Multiview.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of California, Berkeley</span></p>
<p>è¯¥è®ºæ–‡æå‡ºVROOMç³»ç»Ÿï¼Œåˆ©ç”¨F1èµ›è½¦è½¦è½½æ‘„åƒå¤´è§†é¢‘ï¼Œé€šè¿‡é›†æˆDROID-SLAMã€AnyCamå’ŒMonst3rç­‰æ–¹æ³•ï¼Œç»“åˆåˆ†å—ã€åˆ†è¾¨ç‡è°ƒæ•´å’Œæ©ç é¢„å¤„ç†ï¼Œå®ç°èµ›é“åŠè½¦è¾†3Dé‡å»ºã€‚å®éªŒè¡¨æ˜ï¼ŒVROOMåœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹èƒ½å¤Ÿéƒ¨åˆ†è¿˜åŸèµ›é“åŠè½¦è¾†è½¨è¿¹ï¼ŒéªŒè¯äº†åœ¨çœŸå®åœºæ™¯ä¸‹ç”¨è½¦è½½è§†é¢‘å®ç°å¯æ‰©å±•4Dé‡å»ºçš„å¯è¡Œæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/VROOM_-_Visual_Reconstruction_over_Onboard_Multiview.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17427v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      91. [Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Robust_Point_Cloud_Registration_via_Geometric_Overlapping_Guided_Rotation_Search.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Beijing Institute of Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‡ ä½•æœ€å¤§é‡å ï¼ˆGMORï¼‰çš„ç‚¹äº‘é…å‡†ç®—æ³•ï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„æ—‹è½¬åˆ†æ”¯å®šç•Œï¼ˆBnBï¼‰æœç´¢å®ç°åˆšä½“å˜æ¢çš„å…¨å±€æœ€ä¼˜ä¼°è®¡ã€‚æ–¹æ³•åˆ©ç”¨Chasleså®šç†å°†åˆšä½“å˜æ¢åˆ†è§£ä¸ºæ—‹è½¬è½´æœç´¢ï¼ˆé‡‡ç”¨ç«‹æ–¹æ˜ å°„å’ŒåŒºé—´æˆ³ç®—æ³•è¿›è¡Œ1DèŒƒå›´æœ€å¤§æŸ¥è¯¢ï¼‰å’Œæ—‹è½¬è§’åº¦æœç´¢ï¼ˆåŸºäºæ‰«çº¿å’Œçº¿æ®µæ ‘è¿›è¡Œ2Dæœ€å¤§é‡å æŸ¥è¯¢ï¼‰ï¼Œç¡®ä¿å¤šé¡¹å¼æ—¶é—´å’Œçº¿æ€§ç©ºé—´å¤æ‚åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªçœŸå®ä¸åˆæˆæ•°æ®é›†ä¸Šé…å‡†ç²¾åº¦å’Œæ•ˆç‡ä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ã€‚ç»“è®ºï¼šGMORåœ¨ç‚¹äº‘é…å‡†ä»»åŠ¡ä¸­å…¼å…·é«˜æ•ˆæ€§å’Œé²æ£’æ€§ï¼Œé€‚ç”¨äºè½»é‡çº§è§†è§‰å’Œæœºå™¨äººåº”ç”¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Robust_Point_Cloud_Registration_via_Geometric_Overlapping_Guided_Rotation_Search.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17417v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      92. [Constrained Prompt Enhancement for Improving Zero-Shot Generalization of Vision-Language Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Constrained_Prompt_Enhancement_for_Improving_Zero-Shot_Generalization_of_Vision-Language_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tianjin University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†çº¦æŸæ€§æç¤ºå¢å¼ºï¼ˆCPEï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ‹“æ‰‘å¼•å¯¼åŒä¹‰è¯­ä¹‰ç”Ÿæˆï¼ˆTGSSGï¼‰å’Œç±»åˆ«æ— å…³åˆ¤åˆ«åŒºåŸŸé€‰æ‹©ï¼ˆCADRSï¼‰ï¼Œåˆ†åˆ«æ„å»ºå…¨é¢çš„æ–‡æœ¬æç¤ºå’Œç´§å‡‘çš„è§†è§‰æç¤ºï¼Œå¹¶ç»“åˆæµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰ä¸æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰å®ç°è§†è§‰-æ–‡æœ¬é›†åˆå¯¹é›†åˆåŒ¹é…ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé›¶æ ·æœ¬å›¾åƒä¸è§†é¢‘ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Constrained_Prompt_Enhancement_for_Improving_Zero-Shot_Generalization_of_Vision-Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17299v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      93. [FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FoundDiff__Foundational_Diffusion_Model_for_Generalizable_Low-Dose_CT_Denoising.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Fudan University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†FoundDiffï¼Œä¸€ç§é¢å‘ä½å‰‚é‡CTï¼ˆLDCTï¼‰å»å™ªçš„åŸºç¡€æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼šé¦–å…ˆåˆ©ç”¨å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆDA-CLIPï¼‰æ„ŸçŸ¥å¹¶ç¼–ç å‰‚é‡å’Œè§£å‰–ä¿¡æ¯ï¼Œé€šè¿‡å‰‚é‡æ’åºå’Œè§£å‰–åˆ¤åˆ«æŸå¤±è·å¾—è¯­ä¹‰åµŒå…¥ï¼›ç„¶åå°†è¿™äº›åµŒå…¥é€šè¿‡æ–°å‹DACBæ¨¡å—é›†æˆåˆ°æ‰©æ•£æ¨¡å‹ï¼ˆDA-Diffï¼‰ä¸­ï¼Œå®ç°è‡ªé€‚åº”å»å™ªã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ã€ä¸åŒå‰‚é‡å’Œè§£å‰–åŒºåŸŸå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å¯¹æœªè§å‰‚é‡å…·æœ‰ä¼˜è‰¯æ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/FoundDiff_Foundational_Diffusion_Model_for_Generalizable_Low-Dose_CT_Denoising.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18213v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      94. [Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Follow_My_Hold__Hand-Object_Interaction_Reconstruction_through_Geometric_Guidance.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Max Planck Institute for Informatics</span></p>
<p>æœ¬è®ºæ–‡æå‡ºFollowMyHoldï¼Œä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å•å¼ RGBå›¾åƒæ‰‹æŒç‰©ä½“ä¸‰ç»´é‡å»ºæ–¹æ³•ã€‚æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å‡ ä½•å¼•å¯¼ï¼ŒåŒ…æ‹¬æ³•çº¿ã€æ·±åº¦ã€è½®å»“ä¸€è‡´æ€§å’Œæ‰‹-ç‰©ä½“äº¤äº’çº¦æŸï¼Œé€šè¿‡åˆ†é˜¶æ®µä¼˜åŒ–ï¼ˆå…ˆæ‰‹ã€åç‰©ä½“ã€å†è”åˆï¼‰ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å¯¼3Dç”Ÿæˆæ¨¡å‹ï¼Œå®ç°é«˜è´¨é‡ä¸”ç‰©ç†åˆç†çš„æ‰‹-ç‰©ä½“ä¸‰ç»´é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨OakInkã€Arcticå’ŒDexYCBç­‰åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰ç”Ÿæˆå¼æ–¹æ³•ï¼Œå…·å¤‡æ›´é«˜çš„å‡†ç¡®ç‡ã€ç¨³å¥æ€§å’Œå¹¿æ³›æ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Follow_My_Hold_Hand-Object_Interaction_Reconstruction_through_Geometric_Guidance.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18188v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      95. [Explain and Monitor Deep Learning Models for Computer Vision using Obz AI]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Explain_and_Monitor_Deep_Learning_Models_for_Computer_Vision_using_Obz_AI.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute of Informatics, University of Warsaw</span></p>
<p>æœ¬æ–‡æå‡ºäº†Obz AIï¼Œä¸€ä¸ªé›†æˆçš„å…¨æ ˆè½¯ä»¶å¹³å°ï¼Œä¸“ä¸ºæ·±åº¦å­¦ä¹ è§†è§‰æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯ç›‘æ§æ€§è€Œè®¾è®¡ï¼Œæ¶µç›–Pythonåº“ã€åç«¯æ•°æ®åº“å’Œå‰ç«¯ä»ªè¡¨ç›˜ã€‚Obz AIæ”¯æŒä¸»æµXAIç®—æ³•ï¼ˆå¦‚Grad-CAMã€Saliency Mapsã€Attention Mapç­‰ï¼‰ä¸å›¾åƒç‰¹å¾æå–å’Œå¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œå¹¶å®ç°äº†æ¨¡å‹é¢„æµ‹ä¸è§£é‡Šçš„å®æ—¶å¯è§†åŒ–å’Œæ•°æ®ç®¡ç†ã€‚ç»“è®ºæ˜¯Obz AIé¦–æ¬¡ä¸ºCVé¢†åŸŸæä¾›äº†é›†æˆåŒ–çš„è§£é‡Šä¸ç›‘æ§å·¥å…·ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹éƒ¨ç½²çš„é€æ˜åº¦ä¸å¯é æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Explain_and_Monitor_Deep_Learning_Models_for_Computer_Vision_using_Obz_AI.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18177v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      96. [Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scene-Aware_Vectorized_Memory_Multi-Agent_Framework_with_Cross-Modal_Differentiated_Quantization_VLM.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Electronic Science and Technology of China</span></p>
<p>æœ¬æ–‡æå‡ºäº†è·¨æ¨¡æ€å·®å¼‚åŒ–é‡åŒ–ç­–ç•¥ï¼Œä¸“ä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è®¾è®¡ï¼Œé€šè¿‡å¯¹è§†è§‰ç¼–ç å™¨å’Œè·¨æ¨¡æ€æ¨¡å—åˆ†åˆ«é‡åŒ–ï¼Œæœ‰æ•ˆå°†19Bå‚æ•°æ¨¡å‹çš„å†…å­˜éœ€æ±‚ä»38GBé™è‡³16GBï¼Œæ€§èƒ½ä¸‹é™ä»…2.05%ï¼›æ„å»ºäº†åœºæ™¯æ„ŸçŸ¥çŸ¢é‡åŒ–è®°å¿†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé›†æˆåœºæ™¯åˆ†ç±»ã€çŸ¢é‡è®°å¿†å’Œå¤šæ¨¡æ€äº¤äº’ï¼Œå®ç°å†å²åœºæ™¯çŸ¥è¯†çš„é«˜æ•ˆæ£€ç´¢ï¼Œæ˜¾è‘—æå‡å¯¹è§†éšœç”¨æˆ·çš„ç¯å¢ƒç†è§£å’ŒåŠ©å¯¼èˆªèƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ä¸»æµåŸºå‡†MMBenchä¸OCR-VQAä¸Šä¿æŒé«˜å‡†ç¡®ç‡ï¼Œå¹¶å°†è¯­éŸ³å“åº”å»¶è¿Ÿé™è‡³2.83-3.52ç§’ï¼Œå¤§å¹…ä¼˜äºä¼ ç»Ÿæ–¹æ¡ˆã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Scene-Aware_Vectorized_Memory_Multi-Agent_Framework_with_Cross-Modal_Differentiated_Quantization_VLMs_for_Visually_Impaired_Assistance.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18067v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      97. [Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Annotation-Free_Open-Vocabulary_Segmentation_for_Remote-Sensing_Images.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Xiâ€™an Jiaotong University</span></p>
<p>æœ¬æ–‡æå‡ºSegEarth-OVæ¡†æ¶ï¼Œå®ç°é¥æ„Ÿå›¾åƒçš„æ— æ ‡æ³¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼Œæ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬SimFeatUpé€šç”¨ç‰¹å¾ä¸Šé‡‡æ ·æ¨¡å—ï¼ˆåŸºäºå‚æ•°åŒ–è”åˆåŒè¾¹ä¸Šé‡‡æ ·å’Œå†…å®¹ä¿æŒç½‘ç»œï¼Œæ¢å¤VLMä¸‹é‡‡æ ·å¤±çœŸçš„ç©ºé—´ç»†èŠ‚ï¼‰å’Œå…¨å±€åç½®æ¶ˆé™¤æ“ä½œï¼ˆé€šè¿‡å‡å»VLMçš„[CLS] tokenæŠ‘åˆ¶å…¨å±€è¯­ä¹‰æ³„æ¼ï¼Œæå‡åƒç´ çº§åˆ¤åˆ«åŠ›ï¼‰ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¾®è°ƒã€‚ä¸ºæ”¯æŒSARç­‰å¤šæ¨¡æ€é¥æ„Ÿæ•°æ®ï¼Œæå‡ºAlignEarthè·¨æ¨¡æ€è’¸é¦ç­–ç•¥ï¼Œé€šè¿‡å…‰å­¦-SARé…å¯¹å›¾åƒå°†å…‰å­¦VLMçŸ¥è¯†é«˜æ•ˆè¿ç§»åˆ°SARç¼–ç å™¨ï¼Œè§£å†³SARé¢†åŸŸå¤§æ¨¡å‹éš¾ä»¥è®­ç»ƒå’Œæ ‡æ³¨æ˜‚è´µéš¾é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒSegEarth-OVåœ¨å¤šç§å…‰å­¦å’ŒSARé¥æ„Ÿåˆ†å‰²ä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰SOTAæ–¹æ³•ï¼Œé¦–æ¬¡éªŒè¯äº†é¥æ„Ÿé¢†åŸŸå¼€æ”¾è¯æ±‡åˆ†å‰²çš„å¯è¡Œæ€§ä¸é€šç”¨æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Annotation-Free_Open-Vocabulary_Segmentation_for_Remote-Sensing_Images.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18025v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      98. [AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AQ-PCDSys__An_Adaptive_Quantized_Planetary_Crater_Detection_System_for_Autonomous_Space_Exploration.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Manipal University Jaipur</span></p>
<p>æœ¬æ–‡æå‡ºäº†AQ-PCDSysï¼Œä¸€ç§é’ˆå¯¹èµ„æºå—é™çš„è¡Œæ˜Ÿæ¢æµ‹ä»»åŠ¡è®¾è®¡çš„è‡ªé€‚åº”é‡åŒ–é™¨çŸ³å‘æ£€æµ‹ç³»ç»Ÿã€‚æ–¹æ³•ä¸Šï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰çš„é«˜æ•ˆé‡åŒ–ç¥ç»ç½‘ç»œæ¶æ„ã€è‡ªé€‚åº”å¤šä¼ æ„Ÿå™¨èåˆæ¨¡å—ï¼ˆAMFï¼‰ä¸å¤šå°ºåº¦æ£€æµ‹å¤´ï¼Œèƒ½å¤Ÿæ™ºèƒ½èåˆå…‰å­¦å½±åƒä¸æ•°å­—é«˜ç¨‹æ¨¡å‹æ•°æ®ï¼Œé€šè¿‡è‡ªé€‚åº”æƒé‡æœºåˆ¶åŠ¨æ€è°ƒæ•´ä¸åŒä¼ æ„Ÿå™¨ç‰¹å¾çš„è´¡çŒ®ï¼Œå®ç°äº†åœ¨ä½åŠŸè€—ç¡¬ä»¶ä¸Šçš„é«˜ç²¾åº¦å®æ—¶æ£€æµ‹ã€‚ç»“è®ºè¡¨æ˜ï¼ŒAQ-PCDSysåœ¨å…¼é¡¾ç²¾åº¦ã€å»¶è¿Ÿå’ŒåŠŸè€—çš„åŒæ—¶ï¼Œä¸ºè¡Œæ˜Ÿè‡ªä¸»æ¢æµ‹å¹³å°æä¾›äº†ç†è®ºä¸Šé«˜é²æ£’æ€§çš„è¾¹ç¼˜AIæ„ŸçŸ¥è§£å†³æ–¹æ¡ˆã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/AQ-PCDSys_An_Adaptive_Quantized_Planetary_Crater_Detection_System_for_Autonomous_Space_Exploration.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17976v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      99. [Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Propose_and_Rectify__A_Forensics-Driven_MLLM_Framework_for_Image_Manipulation_Localization.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">City University of Hong Kong</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å’Œä½å±‚æ¬¡å–è¯ç‰¹å¾åˆ†æçš„Propose-Rectifyå›¾åƒç¯¡æ”¹æ£€æµ‹ä¸å®šä½æ¡†æ¶ã€‚æ–¹æ³•é¦–å…ˆç”¨ç»è¿‡å–è¯é€‚é…çš„LLaVAæ¨¡å‹ç”ŸæˆåŸºäºè¯­ä¹‰ç†è§£çš„åˆæ­¥ç¯¡æ”¹åŒºåŸŸæè®®ï¼Œå†é€šè¿‡å¤šå°ºåº¦å–è¯ç‰¹å¾ï¼ˆå¦‚SRMã€Bayarã€Sobelã€Noiseprint++ï¼‰åœ¨Forensics Rectification Moduleä¸­ç³»ç»ŸéªŒè¯å’Œä¿®æ­£è¿™äº›æè®®ï¼Œå¹¶åœ¨å¢å¼ºåˆ†å‰²æ¨¡å—å†…èåˆå–è¯çº¿ç´¢ä»¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚å®éªŒæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä¼˜çš„æ£€æµ‹å‡†ç¡®ç‡å’Œå®šä½ç²¾åº¦ï¼Œå…·å¤‡å‡ºè‰²çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Propose_and_Rectify_A_Forensics-Driven_MLLM_Framework_for_Image_Manipulation_Localization.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17972v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      100. [SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SAIL-Recon__Large_SfM_by_Augmenting_Scene_Regression_with_Localization.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Hong Kong University of Science and Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºSAIL-Reconæ–¹æ³•ï¼Œé€šè¿‡åœ¨ç»“æ„å…‰æ¢å¤ï¼ˆSfMï¼‰Transformerå›å½’ç½‘ç»œä¸­å¼•å…¥è§†è§‰å®šä½æ¨¡å—ï¼Œå®ç°å¤§è§„æ¨¡æ— åºå›¾åƒé›†åˆçš„é«˜æ•ˆä¸‰ç»´é‡å»ºã€‚å…¶æµç¨‹ä¸ºï¼šé¦–å…ˆä»é‡‡æ ·çš„é”šç‚¹å›¾åƒæ„å»ºç¥ç»åœºæ™¯è¡¨ç¤ºï¼Œå†åŸºäºè¯¥è¡¨ç¤ºå®ç°å¯¹æ‰€æœ‰è¾“å…¥å›¾åƒçš„ä½å§¿ä¸åœºæ™¯ç»“æ„å›å½’ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨TUM-RGBDã€CO3Dv2å’ŒTanks &amp; Templesç­‰æ•°æ®é›†ä¸Šï¼Œåœ¨ç›¸æœºä½å§¿ä¼°è®¡ä¸æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šæ•ˆç‡å’Œç²¾åº¦å‡è¶…è¶Šä¼ ç»Ÿä¸ç°æœ‰å­¦ä¹ å‹SfMæ–¹æ³•ï¼Œå…·å¤‡å¤§è§„æ¨¡åœºæ™¯æ‰©å±•èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SAIL-Recon_Large_SfM_by_Augmenting_Scene_Regression_with_Localization.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17957v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      101. [Generative Feature Imputing â€” A Technique for Error-resilient Semantic Communication]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Generative_Feature_Imputing_-_A_Technique_for_Error-resilient_Semantic_Communication.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The University of Hong Kong</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”Ÿæˆå¼ç‰¹å¾è¡¥å…¨ï¼ˆGenerative Feature Imputing, GFIï¼‰æ¡†æ¶ï¼Œç”¨äºæå‡æ•°å­—è¯­ä¹‰é€šä¿¡ç³»ç»Ÿåœ¨å›¾åƒä¼ è¾“ä¸­çš„æŠ—è¯¯å·®èƒ½åŠ›ã€‚æ–¹æ³•åŒ…æ‹¬ç©ºé—´è¯¯å·®é›†ä¸­åˆ†ç»„ã€åŸºäºæ‰©æ•£æ¨¡å‹çš„ç‰¹å¾ç”Ÿæˆä¿®å¤ä»¥åŠè¯­ä¹‰æ„ŸçŸ¥åŠŸç‡åˆ†é…ï¼Œåˆ†åˆ«å®ç°äº†ç‰¹å¾è¯¯å·®çš„ç©ºé—´å±€éƒ¨åŒ–ã€ä¸¢å¤±ç‰¹å¾çš„é«˜æ•ˆé‡æ„å’Œå¯¹è¯­ä¹‰é‡è¦ä¿¡æ¯çš„åŠ¨æ€ä¿æŠ¤ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä½ä¿¡å™ªæ¯”å’Œå—è¡°è½æ¡ä»¶ä¸‹ï¼Œè¾ƒDeep Joint Source-Channel Coding (DJSCC)å’ŒJPEG2000ç­‰ä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨æ„ŸçŸ¥è´¨é‡å’Œè¯­ä¹‰å‡†ç¡®æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Generative_Feature_Imputing_%E2%80%94_A_Technique_for_Error-resilient_Semantic_Communication.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17924v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      102. [Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Gaze_into_the_Heart__A_Multi-View_Video_Dataset_for_rPPG_and_Health_Biomarkers_Estimation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Sber AI Lab</span></p>
<p>æœ¬æ–‡æå‡ºäº†MCD-rPPGï¼Œä¸€ä¸ªåŒ…å«600åå—è¯•è€…ã€3600æ®µå¤šè§†è§’è§†é¢‘ã€åŒæ­¥PPGä¿¡å·åŠä¸°å¯Œç”Ÿç‰©å¥åº·æŒ‡æ ‡ï¼ˆå¦‚å¿ƒç”µã€è¡€å‹ã€è¡€æ°§ç­‰ï¼‰çš„å…¬å¼€å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†åŸºäºé¢éƒ¨è§†é¢‘çš„é«˜æ•ˆå¤šä»»åŠ¡rPPGç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå®ç°äº†å®æ—¶å¥åº·å‚æ•°ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è·¨æ•°æ®é›†ä¸Šå…·å¤‡è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ä¸æ¨ç†æ•ˆç‡ï¼Œæ¨åŠ¨äº†AIåŒ»ç–—åŠ©æ‰‹å’Œè¿œç¨‹å¥åº·ç›‘æµ‹çš„å‘å±•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Gaze_into_the_Heart_A_Multi-View_Video_Dataset_for_rPPG_and_Health_Biomarkers_Estimation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17916v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      103. [EndoUFM: Utilizing Foundation Models for Monocular Depth Estimation of Endoscopic Images]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/EndoUFM__Utilizing_Foundation_Models_for_Monocular_depth_estimation_of_endoscopic_images.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Beihang University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†EndoUFMï¼Œä¸€ç§èåˆDepth Anythingå’ŒSegment Anythingä¸¤å¤§è§†è§‰åŸºç¡€æ¨¡å‹çš„è‡ªç›‘ç£å•ç›®å†…çª¥é•œæ·±åº¦ä¼°è®¡æ¡†æ¶ã€‚æ–¹æ³•åŒ…æ‹¬å¼•å…¥éšæœºå‘é‡ä½ç§©é€‚åº”ï¼ˆRVLoRAï¼‰ä¸åŸºäºæ·±åº¦å¯åˆ†ç¦»å·ç§¯çš„æ®‹å·®æ¨¡å—ï¼ˆRes-DSCï¼‰ï¼Œå¹¶ç»“åˆæ©ç å¼•å¯¼çš„å¹³æ»‘æŸå¤±ä»¥æå‡ç«¯åˆ°ç«¯æ·±åº¦ä¸€è‡´æ€§ã€‚å®éªŒåœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„é¢†å…ˆæ€§èƒ½ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨ç²¾åº¦å’Œé²æ£’æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”æ¨¡å‹å‚æ•°é‡å°ï¼Œé€‚äºä¸´åºŠéƒ¨ç½²ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/EndoUFM_Utilizing_Foundation_Models_for_Monocular_Depth_Estimation_of_Endoscopic_Images.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17885v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      104. [ISALUX: Illumination and Semantics-Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ISALux__Illumination_and_Segmentation_Aware_Transformer_Employing_Mixture_of_Experts_for_Low_Light_I.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Manchester</span></p>
<p>è¯¥è®ºæ–‡æå‡ºISALuxï¼Œä¸€ç§åŸºäºVision Transformerï¼ˆViTï¼‰çš„ä½å…‰ç…§å›¾åƒå¢å¼ºæ–¹æ³•ï¼Œåˆ›æ–°æ€§åœ°èåˆäº†ç…§æ˜å’Œè¯­ä¹‰å…ˆéªŒã€‚å…¶æ ¸å¿ƒæŠ€æœ¯åŒ…æ‹¬æ··åˆç…§æ˜ä¸è¯­ä¹‰æ„ŸçŸ¥å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆHISA-MSAï¼‰æ¨¡å—ä¸ä¸“å®¶æ··åˆï¼ˆMoEï¼‰å‰é¦ˆç½‘ç»œï¼Œåˆ©ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æå‡æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒISALuxåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æ¯”ç°æœ‰æŠ€æœ¯æ›´ä¼˜çš„å¢å¼ºè´¨é‡å’Œæ›´é«˜çš„æ•ˆç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ISALUX_Illumination_and_Semantics-Aware_Transformer_Employing_Mixture_of_Experts_for_Low_Light_Image_Enhancement.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17877v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      105. [Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detection]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Edge-Enhanced_Vision_Transformer_Framework_for_Accurate_AI-Generated_Image_Detection.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Uttara University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆç»†ç²’åº¦ç»“æ„åˆ†æä¸å…¨å±€ç‰¹å¾æå–çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹æ¡†æ¶ï¼šé€šè¿‡å¯¹Vision Transformerï¼ˆViTï¼‰è¿›è¡Œç‰¹å®šæ•°æ®é›†çš„å¾®è°ƒï¼Œæ•è·é«˜å±‚è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶è®¾è®¡äº†è¾¹ç¼˜å·®å¼‚åˆ†ææ¨¡å—ï¼Œå°†åŸå§‹ä¸é«˜æ–¯æ¨¡ç³Šåçš„å›¾åƒè¾¹ç¼˜è¿›è¡Œå¯¹æ¯”ï¼Œè®¡ç®—è¾¹ç¼˜æ–¹å·®åˆ†æ•°ï¼Œæœ‰æ•ˆåŒºåˆ†çœŸå®ä¸AIç”Ÿæˆå›¾åƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨CIFAKEç­‰å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºä¸»æµCNNå’ŒViTæ¨¡å‹ï¼Œæœ€é«˜å‡†ç¡®ç‡è¾¾97.75%ï¼ŒF1åˆ†æ•°97.77%ï¼Œä¸”æ–¹æ³•è½»é‡ã€å¯è§£é‡Šï¼Œé€‚ç”¨äºå›¾åƒå’Œè§†é¢‘å†…å®¹çš„æ•°å­—å–è¯ä¸è‡ªåŠ¨åŒ–éªŒè¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Edge-Enhanced_Vision_Transformer_Framework_for_Accurate_AI-Generated_Image_Detection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17844v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      106. [Diffusion-Based Data Augmentation for Medical Image Segmentation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Diffusion-Based_Data_Augmentation_for_Medical_Image_Segmentation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Beihang University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºDiffAugæ¡†æ¶ï¼Œå°†æ–‡æœ¬å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆå’Œè‡ªåŠ¨åˆ†å‰²éªŒè¯ç»“åˆï¼Œç”¨äºåŒ»å­¦å›¾åƒä¸­ç½•è§å¼‚å¸¸çš„åˆæˆæ•°æ®å¢å¼ºã€‚æ–¹æ³•é‡‡ç”¨åŒ»å­¦æ–‡æœ¬æè¿°å’Œç©ºé—´æ©è†œæŒ‡å¯¼æ‰©æ•£æ¨¡å‹åœ¨æ­£å¸¸å›¾åƒä¸­ç”Ÿæˆç—…å˜ï¼Œå¹¶é€šè¿‡æ½œç©ºé—´åˆ†å‰²ç½‘ç»œå®æ—¶éªŒè¯ç”Ÿæˆæ ·æœ¬çš„ç©ºé—´å‡†ç¡®æ€§ï¼Œä»…ä¿ç•™é«˜è´¨é‡æ ·æœ¬ç”¨äºè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒDiffAugåœ¨CVC-ClinicDBã€Kvasir-SEGå’ŒREFUGE2ä¸‰ä¸ªåŒ»å­¦å½±åƒåˆ†å‰²åŸºå‡†ä¸Šæå‡Diceç³»æ•°8-10%ï¼Œå¹¶å°†å¯¹å°å‹æ¯è‚‰ç­‰éš¾æ£€æµ‹å¼‚å¸¸çš„æ¼æ£€ç‡é™ä½è‡³28%ã€‚ç»“è®ºï¼šè¯¥æ–¹æ³•é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®ï¼Œæœ‰æ•ˆç¼“è§£åŒ»å­¦å½±åƒä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Diffusion-Based_Data_Augmentation_for_Medical_Image_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17816v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      107. [UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/UniSino__Physics-Driven_Foundational_Model_for_Universal_CT_Sinogram_Standardization.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Nanchang University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†UniSinoï¼Œä¸€ç§åŸºäºç‰©ç†çŸ¥è¯†çš„é€šç”¨CTæ­£æŠ•å½±æ•°æ®ï¼ˆsinogramï¼‰æ ‡å‡†åŒ–åŸºç¡€æ¨¡å‹ã€‚æ–¹æ³•åŒ…æ‹¬SinoVAEå˜åˆ†è‡ªç¼–ç å™¨è¿›è¡Œé«˜é¢‘ä¸å…¨é¢‘ç‰¹å¾å‹ç¼©ï¼Œç»“åˆæ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨æŠ•å½±åŸŸè¿›è¡Œä¼ªå½±&#x2F;å™ªå£°æŠ‘åˆ¶ä¸é‡å»ºï¼Œå¹¶å¼•å…¥ç‰©ç†çº¦æŸæŸå¤±å‡½æ•°æå‡ç‰©ç†ä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUniSinoåœ¨å¤šç§ä¼ªå½±ç±»å‹ã€æ··åˆä¼ªå½±åŠå¤šæ•°æ®é›†ä¸Šæ ‡å‡†åŒ–è¡¨ç°æ˜¾è‘—ä¼˜äºU-Netã€ViTã€CycleGANã€DDPMç­‰ä¸»æµæ–¹æ³•ï¼Œå…·å¤‡å¾ˆå¼ºçš„æ³›åŒ–å’Œé²æ£’æ€§ã€‚ç»“è®ºï¼šUniSinoå¯ç»Ÿä¸€å¤„ç†å¤šç§CTåŸå§‹æ•°æ®ä¼ªå½±ï¼Œä¸ºåŒ»å­¦å½±åƒé¢„å¤„ç†å¸¦æ¥é«˜æ•ˆé€šç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/UniSino_Physics-Driven_Foundational_Model_for_Universal_CT_Sinogram_Standardization.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17760v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      108. [CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CEIDM__A_Controlled_Entity_and_Interaction_Diffusion_Model_for_Enhanced_Text-to-Image_Generation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">College of Computer Science and Technology, National University of Defense Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCEIDMçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹å¹¶å¼•å…¥äº†å®ä½“ä¸äº¤äº’çš„åŒé‡æ§åˆ¶ã€‚æ–¹æ³•ä¸Šï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æŒ–æ˜éšå¼äº¤äº’å…³ç³»ã€èšç±»åŠåç§»äº¤äº’åŠ¨ä½œç‰¹å¾ï¼Œå¹¶è®¾è®¡å®ä½“æ§åˆ¶ç½‘ç»œå®ç°è¯­ä¹‰å¼•å¯¼æ©ç ä¸å¤šå°ºåº¦å·ç§¯ç‰¹å¾èåˆï¼Œä»è€Œæå‡å®ä½“å’Œäº¤äº’çš„ç²¾ç¡®æ§åˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒCEIDMåœ¨å®ä½“ä¸äº¤äº’å¯æ§æ€§å’Œç”Ÿæˆå›¾åƒè´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤æ‚åœºæ™¯ä¸ç»†èŠ‚å¤„ç†ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CEIDM_A_Controlled_Entity_and_Interaction_Diffusion_Model_for_Enhanced_Text-to-Image_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17756v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      109. [SUPERGEN: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SuperGen__An_Efficient_Ultra-high-resolution_Video_Generation_System_with_Sketching_and_Tiling.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Rice University</span></p>
<p>æœ¬æ–‡æå‡ºSUPERGENç³»ç»Ÿï¼Œä¸€ç§é«˜æ•ˆã€è®­ç»ƒæ— å…³çš„è¶…é«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚å…¶æ–¹æ³•åŒ…æ‹¬ä¸¤é˜¶æ®µç”Ÿæˆæµç¨‹ï¼šé¦–å…ˆåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆä½åˆ†è¾¨ç‡å…¨å±€ç»“æ„è‰å›¾ï¼Œç„¶åé€šè¿‡åˆ†å—ç»†ç²’åº¦çš„å±€éƒ¨ç²¾åŒ–ï¼Œå®ç°é«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆã€‚è¯¥ç³»ç»Ÿè¿˜å¼•å…¥åŒºåŸŸè‡ªé€‚åº”ç¼“å­˜å’Œé€šä¿¡æœ€å°åŒ–çš„åˆ†å—å¹¶è¡Œæœºåˆ¶ï¼Œæœ‰æ•ˆå‡å°‘å†…å­˜å’Œè®¡ç®—æˆæœ¬å¹¶æå‡æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒSUPERGENåœ¨å¤šä¸ªåŸºå‡†ä¸Šè·å¾—æœ€é«˜è´¨é‡ï¼ŒåŒæ—¶åœ¨ä¸é™ä½è§†é¢‘è´¨é‡çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾6.2å€çš„ç”Ÿæˆé€Ÿåº¦æå‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SUPERGEN_An_Efficient_Ultra-high-resolution_Video_Generation_System_with_Sketching_and_Tiling.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17726v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      110. [Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Few-shot_Human_Action_Anomaly_Detection_via_a_Unified_Contrastive_Learning_Framework.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Toyama</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ç”¨äºå°æ ·æœ¬äººä½“åŠ¨ä½œå¼‚å¸¸æ£€æµ‹ï¼ˆHAADï¼‰ï¼Œé€šè¿‡å¯¹å¤šä¸ªç±»åˆ«åŠ¨ä½œæ•°æ®è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œæ„å»ºç±»åˆ«æ— å…³çš„è¡¨å¾ç©ºé—´ï¼Œå¹¶åˆ©ç”¨åŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆHumanMACï¼‰çš„ç”Ÿæˆå¼è¿åŠ¨å¢å¼ºç­–ç•¥ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µç”Ÿæˆå¤šæ ·ä¸”çœŸå®çš„åŠ¨ä½œæ ·æœ¬ï¼Œæå‡æ¨¡å‹æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœåœ¨HumanAct12æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•åœ¨è§ç±»å’Œæœªè§ç±»åœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡AUCæå‡2.6%ï¼Œå®ç°äº†æ— éœ€ç±»åˆ«é‡è®­ç»ƒçš„é«˜æ•ˆã€å¯æ‰©å±•å¼‚å¸¸æ£€æµ‹ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Few-shot_Human_Action_Anomaly_Detection_via_a_Unified_Contrastive_Learning_Framework.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17718v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      111. [Instant Preference Alignment for Text-to-Image Diffusion Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Instant_Preference_Alignment_for_Text-to-Image_Diffusion_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">New Laboratory of Pattern Recognition, CASIA</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è®­ç»ƒæ— å…³ã€å®æ—¶çš„ç”¨æˆ·åå¥½å¯¹é½æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¡†æ¶ï¼ŒåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å…ˆéªŒï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºåå¥½ç†è§£å’Œåå¥½å¼•å¯¼ä¸¤éƒ¨åˆ†ã€‚å…·ä½“æ–¹æ³•é€šè¿‡MLLMè‡ªåŠ¨ä»å‚è€ƒå›¾åƒä¸­æå–å¤šç»´åå¥½å…³é”®è¯ï¼Œå¹¶è®¾è®¡ç»“æ„åŒ–æŒ‡ä»¤æ‰©å±•åŸå§‹æç¤ºï¼Œç»“åˆå…¨å±€å…³é”®è¯æ§åˆ¶å’Œå±€éƒ¨åŒºåŸŸè‡ªæ³¨æ„åŠ›è°ƒåˆ¶ï¼Œå®ç°å¯¹æ‰©æ•£æ¨¡å‹çš„ç²¾å‡†åå¥½å¼•å¯¼ï¼Œå¹¶æ”¯æŒå¤šè½®äº¤äº’å¼å®æ—¶ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Viperç­‰æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ›´ç»†ç²’åº¦ã€åŠ¨æ€çš„ç”¨æˆ·åå¥½å¯¹é½å›¾åƒç”Ÿæˆã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Instant_Preference_Alignment_for_Text-to-Image_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17708v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      112. [CATformer: Contrastive Adversarial Transformer for Image Super-Resolution]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CATformer__Contrastive_Adversarial_Transformer_for_Image_Super-Resolution.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Duke University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºCATformerï¼Œä¸€ç§èåˆæ‰©æ•£å¼ç‰¹å¾ç²¾ç»†åŒ–ã€å¯¹æŠ—å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ çš„åŒåˆ†æ”¯Transformeræ¶æ„ï¼Œç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡ã€‚CATformeré€šè¿‡ä¸»æ‰©æ•£Transformerå’Œå™ªå£°é²æ£’è¾…åŠ©Transformeråˆ†æ”¯æå–äº’è¡¥ç‰¹å¾ï¼Œå¹¶ç»“åˆæ®‹å·®å¯†é›†å—è¿›è¡Œé«˜è´¨é‡å›¾åƒé‡å»ºã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨CelebA-HQã€DIV2Kå’ŒUrban100ç­‰æ•°æ®é›†ä¸Šæå‡äº†ç»“æ„ç›¸ä¼¼æ€§ã€æ„ŸçŸ¥è´¨é‡å’Œæ¨ç†æ•ˆç‡ï¼Œä¼˜äºå½“å‰ä¸»æµæ‰©æ•£ã€Transformerå’ŒGANæ–¹æ³•ï¼Œä½†åœ¨å°è§„æ¨¡æ•°æ®é›†ä¸ŠPSNRç•¥ä½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CATformer_Contrastive_Adversarial_Transformer_for_Image_Super-Resolution.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17680v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      113. [Robustness Feature Adapter for Efficient Adversarial Training]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Robustness_Feature_Adapter_for_Efficient_Adversarial_Training.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Dongguan University of Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¯¹æŠ—è®­ç»ƒæ–¹æ³•ï¼Œæ ¸å¿ƒä¸ºåœ¨ç‰¹å¾ç©ºé—´å†…å¼•å…¥é²æ£’æ€§ç‰¹å¾é€‚é…å™¨ï¼ˆRFAï¼‰ï¼Œé€šè¿‡å¤–éƒ¨æ¨¡å—åˆ†ç¦»é²æ£’ä¸éé²æ£’ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨åŒåˆ†ç±»å™¨ä¸ä¸‰å…ƒç»„æŸå¤±æé«˜æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹æœªçŸ¥æ”»å‡»çš„é²æ£’æ€§ï¼Œé™ä½äº†è¿‡æ‹Ÿåˆé£é™©ï¼Œä¸”åœ¨ä¸åŒæ¨¡å‹æ¶æ„ï¼ˆå¦‚CNNå’ŒViTï¼‰åŠå¤§è§„æ¨¡å¯¹æŠ—è®­ç»ƒä¸­å‡å…·å¤‡æé«˜æ•ˆç‡å’Œçµæ´»æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Robustness_Feature_Adapter_for_Efficient_Adversarial_Training.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17648v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      114. [Citizen Centered Climate Intelligence: Operationalizing Open Tree Data for Urban Cooling and Eco-Routing in Indian Cities]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Citizen_Centered_Climate_Intelligence__Operationalizing_Open_Tree_Data_for_Urban_Cooling_and_Eco-Rou.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">National Institute of Technology Tiruchirappalli</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä»¥å¸‚æ°‘ä¸ºä¸­å¿ƒçš„åŸå¸‚æ°”å€™æ™ºèƒ½æ¡†æ¶ï¼Œé€šè¿‡æ™ºèƒ½æ‰‹æœºAIåˆ†å‰²è¾…åŠ©çš„æ ‘æœ¨æµ‹é‡å·¥å…·ã€åŸºäºé¥æ„Ÿçš„å±€éƒ¨å†·å´æ•ˆèƒ½åˆ†æä»¥åŠæ•´åˆå¤šæºç¯å¢ƒæŒ‡æ ‡çš„ç”Ÿæ€è·¯å¾„è§„åˆ’å¼•æ“ï¼Œå®ç°å¸‚æ°‘å‚ä¸çš„ç¯å¢ƒæ•°æ®é‡‡é›†ã€åˆ†æä¸è¡ŒåŠ¨é—­ç¯ã€‚ç»“è®ºè®¤ä¸ºï¼Œè¯¥æ–¹æ³•å¯æå‡åŸå¸‚ç»¿åŒ–å¹²é¢„çš„ç§‘å­¦æ€§å’Œé€æ˜åº¦ï¼Œå¹¶æ¨åŠ¨åŸå¸‚æ°”å€™é€‚åº”å’Œç¯å¢ƒæ²»ç†çš„æ°‘ä¸»åŒ–ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Citizen_Centered_Climate_Intelligence_Operationalizing_Open_Tree_Data_for_Urban_Cooling_and_Eco-Routing_in_Indian_Cities.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17634v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      115. [Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenes]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Finding_Outliers_in_a_Haystack__Anomaly_Detection_for_Large_Pointcloud_Scenes.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Adelaide</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘å¤§è§„æ¨¡ç‚¹äº‘åœºæ™¯çš„å¼€æ”¾é›†åˆ†å‰²ä¸å¼‚å¸¸æ£€æµ‹æ–°æ–¹æ³•ï¼Œæ ¸å¿ƒåœ¨äºç»“åˆMambaæ¶æ„çš„é•¿è·ç¦»ä¾èµ–å»ºæ¨¡èƒ½åŠ›å’Œé‡å»ºè‡ªç¼–ç å™¨ï¼Œå…ˆé‡å»ºåœºæ™¯é»˜è®¤ä¸Šä¸‹æ–‡ï¼Œå†åˆ©ç”¨åŸå§‹ä¸é‡å»ºç‚¹äº‘çš„å·®å¼‚è¿›è¡Œå¼‚å¸¸æ£€æµ‹ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨KITTIå’ŒECLAIRä¸¤ä¸ªå¤§å‹ç‚¹äº‘æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†å¼‚å¸¸æ£€æµ‹æ€§èƒ½ï¼Œå°¤å…¶åœ¨æ£€æµ‹æœªçŸ¥å¼‚å¸¸ç‰©ä½“æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä½†å¯¹å·²çŸ¥ç‰©ä½“åˆ†å‰²ä»æœ‰æå‡ç©ºé—´ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Finding_Outliers_in_a_Haystack_Anomaly_Detection_for_Large_Pointcloud_Scenes.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17619v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      116. [Improving Interpretability in Alzheimerâ€™s Prediction via Joint Learning of ADAS-Cog Scores]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Improving_Interpretability_in_Alzheimer's_Prediction_via_Joint_Learning_of_ADAS-Cog_Scores.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Universiti Brunei Darussalam</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰çš„æ¡†æ¶ï¼ŒèåˆVision Transformerï¼ˆViTï¼‰å’ŒSwin Transformerä»MRIå½±åƒä¸­æå–ç‰¹å¾ï¼Œå¹¶ç»“åˆçºµå‘ä¸´åºŠè¯„åˆ†ï¼ŒåŒæ—¶é¢„æµ‹é˜¿å°”å…¹æµ·é»˜ç—‡ADAS-Cogå…¨å±€åˆ†æ•°åŠå…¶13é¡¹å­åˆ†æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå­åˆ†æ•°çš„è”åˆå­¦ä¹ å¯æå‡å…¨å±€åˆ†æ•°é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶æ­ç¤ºæ¨¡å‹å¯¹éƒ¨åˆ†è®¤çŸ¥åŸŸï¼ˆå¦‚è¯è¯­å›å¿†ç­‰ï¼‰çš„é«˜åº¦ä¾èµ–ï¼Œä½†ä¹Ÿå‘ç°éƒ¨åˆ†å…³é”®å­åˆ†æ•°é¢„æµ‹ä¸ç¨³å®šï¼Œä¸»è¦å› æ¨¡å‹è¿‡åº¦ä¾èµ–ä¸´åºŠç‰¹å¾è€Œå¿½è§†MRIç‰¹å¾ï¼Œæœªæ¥éœ€æ”¹è¿›å¤šæ¨¡æ€èåˆä¸æŸå¤±è‡ªé€‚åº”æƒé‡ä»¥æå‡æ¨¡å‹ç¨³å¥æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Improving_Interpretability_in_Alzheimer%E2%80%99s_Prediction_via_Joint_Learning_of_ADAS-Cog_Scores.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17614v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      117. [JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-on]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/JCo-MVTON__Jointly_Controllable_Multi-Modal_Diffusion_Transformer_for_Mask-Free_Virtual_Try-on.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">DAMO Academy, Alibaba Group</span></p>
<p>æœ¬è®ºæ–‡æå‡ºJCo-MVTONï¼Œä¸€ç§åŸºäºMulti-Modal Diffusion Transformer (MM-DiT)çš„æ— æ©ç è™šæ‹Ÿè¯•è¡£æ–¹æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€æ¡ä»¶èåˆï¼ˆåŒ…æ‹¬å‚è€ƒå›¾åƒå’Œæœè£…å›¾åƒï¼‰å¹¶åœ¨è‡ªæ³¨æ„åŠ›å±‚å¼•å…¥æ¡ä»¶åˆ†æ”¯å’Œæ©ç æœºåˆ¶ï¼Œå®ç°é«˜è´¨é‡ã€ç¨³å®šçš„æœè£…æ›´æ¢ã€‚è¯¥æ–¹æ³•ç»“åˆåˆ›æ–°çš„æ•°æ®ç”Ÿæˆä¸ç­›é€‰æµç¨‹ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨å¤šæ ·åœºæ™¯ä¸‹çš„å¯æ§æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨DressCodeç­‰å…¬å¼€åŸºå‡†åŠçœŸå®åº”ç”¨ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¿‡ä¸»æµå­¦æœ¯ä¸å•†ä¸šç³»ç»Ÿã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/JCo-MVTON_Jointly_Controllable_Multi-Modal_Diffusion_Transformer_for_Mask-Free_Virtual_Try-on.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17613v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      118. [A Weighted Vision Transformer-Based Multi-Task Learning Framework for Predicting ADAS-Cog Scores]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Weighted_Vision_Transformer-Based_Multi-Task_Learning_Framework_for_Predicting_ADAS-Cog_Scores.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Universiti Brunei Darussalam</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŠ æƒè§†è§‰Transformerï¼ˆViTï¼‰çš„å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰æ¡†æ¶ï¼Œç”¨äºåˆ©ç”¨åŸºçº¿MRIå½±åƒé¢„æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ADAS-Cogé‡è¡¨çš„13ä¸ªåˆ†é¡¹å¾—åˆ†åŠå…¶æ€»åˆ†ã€‚æ–¹æ³•é€šè¿‡å¯¹ä¸å…¨å±€è¯„åˆ†é«˜åº¦ç›¸å…³çš„åˆ†é¡¹ï¼ˆQ1ã€Q4ã€Q8ï¼‰èµ‹äºˆæ›´é«˜æŸå¤±æƒé‡ï¼Œæå‡æ¨¡å‹å¯¹é‡è¦è®¤çŸ¥åŸŸçš„å…³æ³¨åº¦ï¼Œå¹¶åœ¨ä¸åŒä¸´åºŠåˆ†ç»„ï¼ˆMCIå’ŒCNï¼‰ä¸­ç³»ç»Ÿåˆ†ææƒé‡ç­–ç•¥å¯¹é¢„æµ‹æ•ˆæœçš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼Œå¼ºæƒé‡ç­–ç•¥åœ¨MCIç»„è¡¨ç°æ›´ä¼˜ï¼Œé€‚åº¦æƒé‡åˆ™æ›´é€‚åˆCNç»„ã€‚ç»“è®ºæŒ‡å‡ºï¼Œåˆç†çš„ä»»åŠ¡åŠ æƒå¯æå‡é¢„æµ‹å‡†ç¡®æ€§å’Œæ¨¡å‹è§£é‡Šæ€§ï¼Œè€Œå‡åŒ€æƒé‡ç­–ç•¥ä¼šä½ä¼°å…³é”®åˆ†é¡¹ï¼Œé™åˆ¶æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Weighted_Vision_Transformer-Based_Multi-Task_Learning_Framework_for_Predicting_ADAS-Cog_Scores.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17595v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      119. [TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TinyGiantVLM__A_Lightweight_Vision-Language_Architecture_for_Spatial_Reasoning_under_Resource_Constr.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Science, VNU-HCM</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†TinyGiantVLMï¼Œä¸€ç§è½»é‡çº§ã€æ¨¡å—åŒ–çš„è§†è§‰-è¯­è¨€åŒé˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡é¢„è®­ç»ƒè§†è§‰ä¸»å¹²æå–RGBå’Œæ·±åº¦çš„å…¨å±€ä¸åŒºåŸŸç‰¹å¾ï¼Œå¹¶åˆ©ç”¨Mixture-of-Experts(MoE)èåˆæ¨¡å—å®ç°å¤šæ¨¡æ€ç©ºé—´æ¨ç†ã€‚åœ¨AI City Challenge 2025çš„å·¥ä¸šä»“å‚¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­ï¼ŒTinyGiantVLMä»¥64Må‚æ•°åŸºçº¿æ¨¡å‹è·å¾—ç¬¬äº”åï¼Œå…¼é¡¾å‡†ç¡®æ€§ä¸è®¡ç®—æ•ˆç‡ï¼ŒMoEå˜ä½“è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TinyGiantVLM_A_Lightweight_Vision-Language_Architecture_for_Spatial_Reasoning_under_Resource_Constraints.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17588v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      120. [HERO: Hierarchical Extrapolation and Refresh for Efficient World Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HERO__Hierarchical_Extrapolation_and_Refresh_for_Efficient_World_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tsinghua University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„åˆ†å±‚åŠ é€Ÿæ¡†æ¶HEROï¼Œç”¨äºæå‡ä¸–ç•Œæ¨¡å‹æ¨ç†æ•ˆç‡ã€‚æ–¹æ³•ä¸Šï¼ŒHEROé’ˆå¯¹å¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹çš„åˆ†å±‚ç‰¹æ€§ï¼Œåœ¨æµ…å±‚é‡‡ç”¨patch-wise refreshåŠ¨æ€é‡ç®—éƒ¨åˆ†tokenï¼Œåœ¨æ·±å±‚ç”¨çº¿æ€§å¤–æ¨ç›´æ¥ä¼°ç®—ä¸­é—´ç‰¹å¾ï¼Œé¿å…å†—ä½™è®¡ç®—å¹¶å…¼å®¹FlashAttentionã€‚å®éªŒè¯æ˜ï¼ŒHEROåœ¨ä¿æŒé«˜è´¨é‡è¾“å‡ºçš„åŒæ—¶ï¼Œå®ç°äº†1.73å€æ¨ç†åŠ é€Ÿï¼Œä¼˜äºç°æœ‰æ‰©æ•£æ¨¡å‹åŠ é€Ÿæ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/HERO_Hierarchical_Extrapolation_and_Refresh_for_Efficient_World_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18242v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      121. [GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GSVisLoc__Generalizable_Visual_Localization_for_Gaussian_Splatting_Scene_Representations.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Weizmann Institute of Science</span></p>
<p>æœ¬æ–‡æå‡ºGSVisLocï¼Œä¸€ç§åŸºäº3D Gaussian Splattingï¼ˆ3DGSï¼‰åœºæ™¯è¡¨ç¤ºçš„é€šç”¨è§†è§‰å®šä½æ–¹æ³•ã€‚å…¶æ ¸å¿ƒæµç¨‹åŒ…æ‹¬ï¼šç”¨KPConvç¼–ç å™¨å¯¹3Dé«˜æ–¯ç‚¹äº‘è¿›è¡Œç¼–ç ä¸é™é‡‡æ ·ï¼Œå›¾åƒä¾§ç”¨æ·±åº¦å­¦ä¹ æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œé€šè¿‡äº¤é”™è‡ªæ³¨æ„åŠ›ä¸äº¤å‰æ³¨æ„åŠ›å®ç°3D-2Dç‰¹å¾å¯¹é½ï¼Œå…ˆç²—åç»†åŒ¹é…ï¼Œå¹¶ç»“åˆPnP+RANSACåŠGS-CPRç²¾åŒ–å§¿æ€ä¼°è®¡ã€‚ç»“è®ºè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¡¹å…¬å¼€æ•°æ®é›†ä¸Šè¶…è¶Šä»¥å¾€3DGSå®šä½æ–¹æ³•ï¼Œå¹¶å…·å¤‡è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–å‚è€ƒå›¾ç‰‡æˆ–é‡è®­ç»ƒå³å¯åº”ç”¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GSVisLoc_Generalizable_Visual_Localization_for_Gaussian_Splatting_Scene_Representations.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18226v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      122. [Disentangling the Factors of Convergence between Brains and Computer Vision Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Disentangling_the_Factors_of_Convergence_between_Brains_and_Computer_Vision_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Meta AI</span></p>
<p>æœ¬è®ºæ–‡é€šè¿‡ç³»ç»Ÿæ€§åœ°è®­ç»ƒå¹¶æ¯”è¾ƒå¤šç§ä¸åŒç»“æ„ã€è®­ç»ƒé‡å’Œæ•°æ®ç±»å‹çš„è‡ªç›‘ç£è§†è§‰Transformerï¼ˆDINOv3ï¼‰ï¼Œé‡åŒ–å…¶å¯¹è‡ªç„¶å›¾åƒçš„è¡¨å¾ä¸äººè„‘fMRIå’ŒMEGå“åº”çš„ç›¸ä¼¼æ€§ï¼Œé‡‡ç”¨ç¼–ç åˆ†æ•°ã€ç©ºé—´åˆ†æ•°å’Œæ—¶é—´åˆ†æ•°ä¸‰ç§åº¦é‡ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹è§„æ¨¡ã€è®­ç»ƒæ•°æ®é‡å’Œå›¾åƒç±»å‹è¿™ä¸‰å¤§å› ç´ å‡ç‹¬ç«‹ä¸”äº¤äº’åœ°æå‡AIæ¨¡å‹ä¸äººè„‘åœ¨è§†è§‰è¡¨å¾ä¸Šçš„ç›¸ä¼¼æ€§ï¼Œä¸”è¿™ç§å¯¹é½å±•ç°å‡ºä¸äººè„‘å‘è‚²ç›¸ä¼¼çš„å…ˆåé¡ºåºï¼Œæ­ç¤ºäº†æ¶æ„ä¸ç»éªŒå…±åŒä½œç”¨äºäººå·¥ç¥ç»ç½‘ç»œè§†è§‰è®¤çŸ¥èƒ½åŠ›çš„å½¢æˆã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Disentangling_the_Factors_of_Convergence_between_Brains_and_Computer_Vision_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18095v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      123. [Incorporating Pre-trained Diffusion Models in Solving the SchrÃ¶dinger Bridge Problem]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Incorporating_Pre-trained_Diffusion_Models_in_Solving_the_SchrÃ¶dinger_Bridge_Problem.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tsinghua University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸‰ç§é‡å‚æ•°åŒ–æŠ€æœ¯ï¼ˆIPMMã€IPTMã€IPFMï¼‰ï¼Œå°†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼ˆSGMsï¼‰ä¸SchrÃ¶dinger Bridgeï¼ˆSBï¼‰é—®é¢˜ç»Ÿä¸€èµ·æ¥ï¼Œä»¥æå¤§åœ°åŠ é€Ÿå’Œç¨³å®šSBæ¨¡å‹çš„è®­ç»ƒï¼Œå¹¶æå‡ºåˆ›æ–°çš„åˆå§‹åŒ–ç­–ç•¥ï¼Œåˆ©ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒSGMsæ˜¾è‘—æå‡SBæ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆæ•°æ®ã€å›¾åƒç”Ÿæˆå’Œæ— é…å¯¹å›¾åƒè¿ç§»ç­‰ä»»åŠ¡ä¸­åœ¨è®­ç»ƒé€Ÿåº¦ã€ç¨³å®šæ€§å’Œç”Ÿæˆè´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰SBæ–¹æ³•ï¼Œå¹¶èƒ½æœ‰æ•ˆæå‡é¢„è®­ç»ƒSGMsçš„æ€§èƒ½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Incorporating_Pre-trained_Diffusion_Models_in_Solving_the_Schr%C3%B6dinger_Bridge_Problem.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17843v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      124. [SCOUT: Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selection]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SCOUT__Semi-supervised_Camouflaged_Object_Detection_by_Utilizing_Text_and_Adaptive_Data_Selection.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Xiamen University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºSCOUTï¼Œä¸€ç§åˆ›æ–°çš„åŠç›‘ç£ä¼ªè£…ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼ŒåŒ…å«è‡ªé€‚åº”æ•°æ®å¢å¼ºä¸é€‰æ‹©ï¼ˆADASï¼‰æ¨¡å—å’Œæ–‡æœ¬èåˆï¼ˆTFMï¼‰æ¨¡å—ã€‚ADASé€šè¿‡å¯¹æœªæ ‡æ³¨æ•°æ®çš„å¯¹æŠ—æ€§å¢å¼ºä¸è¯„åˆ†ï¼Œä¸»åŠ¨é€‰æ‹©æœ‰ä»·å€¼æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼ŒTFMç»“åˆå›¾åƒå’Œä¼ªè£…ç›¸å…³æ–‡æœ¬ä¿¡æ¯è¿›è¡Œç‰¹å¾èåˆï¼Œå¹¶æå‡ºRefTextCODæ–°æ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºSCOUTåœ¨å››ä¸ªä¸»æµæµ‹è¯•é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡åˆ†å‰²ç²¾åº¦ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SCOUT_Semi-supervised_Camouflaged_Object_Detection_by_Utilizing_Text_and_Adaptive_Data_Selection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17768v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      125. [Towards Trustworthy Breast Tumor Segmentation in Ultrasound using Monte Carlo Dropout and Deep Ensembles for Epistemic Uncertainty Estimation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Towards_Trustworthy_Breast_Tumor_Segmentation_in_Ultrasound_using_Monte_Carlo_Dropout_and_Deep_Ensem.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Kwame Nkrumah University of Science and Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆMonte Carlo Dropoutå’ŒDeep Ensemblesç”¨äºä¸ç¡®å®šæ€§ä¼°è®¡çš„æ”¹è¿›Residual Encoder U-Netï¼Œç”¨äºä¹³è…ºè¶…å£°å›¾åƒçš„è‡ªåŠ¨åˆ†å‰²ï¼Œå¹¶ç³»ç»Ÿæ¸…ç†å’Œè¯„ä¼°BUSIæ•°æ®é›†ä»¥é¿å…æ•°æ®æ³„æ¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¹³è…ºç—…å˜è¶…å£°åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°çš„åˆ†å‰²å‡†ç¡®ç‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿé€šè¿‡ä¸ç¡®å®šæ€§ä¼°è®¡æœ‰æ•ˆæ ‡è¯†æ¨¡å‹ä½ç½®ä¿¡åº¦åŒºåŸŸï¼Œå¯¹å®é™…ä¸´åºŠéƒ¨ç½²å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Towards_Trustworthy_Breast_Tumor_Segmentation_in_Ultrasound_using_Monte_Carlo_Dropout_and_Deep_Ensembles_for_Epistemic_Uncertainty_Estimation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17160v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      126. [Beyond Play & Pause: Untwist â€“ Turning GPT-4oâ€™s Spatial Weakness into a Strength for In-Depth Interactive Video Learning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Beyond_Play_and_Pause__Turning_GPT-4o_Spatial_Weakness_into_a_Strength_for_In-Depth_Interactive_Vide.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Clemson University</span></p>
<p>æœ¬æ–‡æå‡ºäº†Untwistç³»ç»Ÿï¼Œå°†GPT-4oä¸è®¡ç®—æœºè§†è§‰æŠ€æœ¯ç»“åˆï¼Œå®ç°å¯¹è§†é¢‘å†…å®¹çš„åŒºåŸŸåŒ–ã€å®æ—¶äº¤äº’å¼é—®ç­”ã€‚å…³é”®æ–¹æ³•åŒ…æ‹¬è§†é¢‘è½¬å½•ã€å…³é”®å¸§æå–ä¸ä¼˜åŒ–ã€åˆ©ç”¨æ¡†é€‰æ³¨é‡Šæ›¿ä»£åæ ‡è¾“å…¥ä»¥æå‡ç©ºé—´ç†è§£ï¼Œæœ€ç»ˆæ˜¾è‘—æå‡GPT-4oåœ¨è§†é¢‘å†…å®¹å®šä½å’Œè§£é‡Šçš„å‡†ç¡®åº¦ã€‚ç»“è®ºæ˜¾ç¤ºï¼Œæ³¨é‡Šå¸§æ–¹å¼å¤§å¹…ä¼˜äºåŸå§‹åæ ‡è¾“å…¥ï¼Œä¿ƒè¿›äº†å¤šæ¨¡æ€ã€å®æ—¶çš„æ™ºèƒ½è§†é¢‘å­¦ä¹ ä½“éªŒã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Beyond_Play_%26_Pause_Untwist_%E2%80%93_Turning_GPT-4o%E2%80%99s_Spatial_Weakness_into_a_Strength_for_In-Depth_Interactive_Video_Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17158v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      127. [Towards Safeguarding LLM Fine-tuning APIs against Cipher Attacks]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Towards_Safeguarding_LLM_Fine-tuning_APIs_against_Cipher_Attacks.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Independent</span></p>
<p>æœ¬æ–‡æå‡ºäº†Cipher Fine-tuning Robustness Benchmark (CIFR)ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒAPIåœ¨é¢å¯¹åŸºäºå¯†ç ç¼–ç æ”»å‡»æ—¶çš„å®‰å…¨é˜²æŠ¤èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºçº¿æ€§æ¢é’ˆçš„ç›‘æ§æ–¹æ³•ï¼Œé€šè¿‡åˆ†ææ¨¡å‹å†…éƒ¨æ¿€æ´»æœ‰æ•ˆæ£€æµ‹åŠ å¯†æ¶æ„è¾“å…¥ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¢é’ˆåœ¨å¤šç§å¯†ç å˜ä½“å’Œå®¶æ—ä¸Šå‡èƒ½å®ç°99%ä»¥ä¸Šçš„æ£€æµ‹å‡†ç¡®ç‡ï¼Œå¹¶ä¼˜äºç°æœ‰å‰æ²¿ç›‘æ§æ–¹æ³•ï¼Œå…·å¤‡è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…éƒ¨ç½²ä»·å€¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Towards_Safeguarding_LLM_Fine-tuning_APIs_against_Cipher_Attacks.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17155v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      128. [Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Mind_the_Gap__Time-of-Check_to_Time-of-Use_Vulnerabilities_in_LLM-Enabled_Agents.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Oregon State University</span></p>
<p>æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ™ºèƒ½ä½“ä¸­çš„â€œæ£€æŸ¥æ—¶åˆ°ä½¿ç”¨æ—¶ï¼ˆTOCTOUï¼‰â€å®‰å…¨æ¼æ´ï¼Œæå‡ºTOCTOU-BenchåŸºå‡†æµ‹è¯•é›†ï¼Œæ¶µç›–66é¡¹çœŸå®ç”¨æˆ·ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°æ­¤ç±»æ¼æ´ã€‚ä½œè€…å€Ÿé‰´ç³»ç»Ÿå®‰å…¨é¢†åŸŸçš„æ–¹æ³•ï¼Œæå‡ºä¸‰ç±»é˜²å¾¡æªæ–½ï¼šæç¤ºé‡å†™ã€çŠ¶æ€å®Œæ•´æ€§ç›‘æ§å’Œå·¥å…·èåˆï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯å…¶å¯æ˜¾è‘—å‡å°‘æ¼æ´æ•°é‡å’Œæ”»å‡»çª—å£ã€‚ç»“è®ºè¡¨æ˜ç»¼åˆé˜²å¾¡æªæ–½å¯å°†TOCTOUæ¼æ´å‘ç”Ÿç‡ä»12%é™è‡³8%ï¼Œæ”»å‡»çª—å£ç¼©çŸ­95%ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Mind_the_Gap_Time-of-Check_to_Time-of-Use_Vulnerabilities_in_LLM-Enabled_Agents.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17137v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      129. [MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MoE-Beyond__Learning-Based_Expert_Activation_Prediction_on_Edge_Devices.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Univ. of Pennsylvania</span></p>
<p>MoE-Beyondæå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„ä¸“å®¶æ¿€æ´»é¢„æµ‹æ–¹æ³•ï¼Œå°†ä¸“å®¶é€‰æ‹©å»ºæ¨¡ä¸ºå¤šæ ‡ç­¾åºåˆ—é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨è½»é‡çº§Transformeråœ¨6600ä¸‡ä¸“å®¶æ¿€æ´»è½¨è¿¹ä¸Šè®­ç»ƒï¼Œå®ç°äº†ä¸“å®¶æ¿€æ´»çš„é«˜å‡†ç¡®ç‡å’Œé«˜ç¼“å­˜å‘½ä¸­ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå¯å°†GPUç¼“å­˜å‘½ä¸­ç‡ä»17%æå‡è‡³72%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰å¯å‘å¼æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MoE-Beyond_Learning-Based_Expert_Activation_Prediction_on_Edge_Devices.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17094v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      130. [PowerChain: Automating Distribution Grid Analysis with Agentic AI Workflows]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PowerChain__Automating_Distribution_Grid_Analysis_with_Agentic_AI_Workflows.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The University of Vermont</span></p>
<p>æœ¬æ–‡æå‡ºäº†PowerChainç³»ç»Ÿï¼Œç»“åˆåŸºäºAgentçš„AIç¼–æ’æ¶æ„å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡½æ•°è°ƒç”¨ï¼Œå®ç°å¯¹é…ç”µç½‘åˆ†æä»»åŠ¡çš„è‡ªåŠ¨åŒ–ã€‚æ–¹æ³•é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ŒåŠ¨æ€ç”Ÿæˆå’Œæ‰§è¡Œä¸ç”µåŠ›ç³»ç»Ÿç›¸å…³çš„å‡½æ•°åºåˆ—ï¼Œæ— éœ€æ¨¡å‹å¾®è°ƒå³å¯å¤„ç†å¤æ‚çš„æœªè§ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒPowerChainåœ¨çœŸå®ç”µåŠ›æ•°æ®ä¸Šèƒ½æœ‰æ•ˆç”Ÿæˆä¸ä¸“å®¶æ°´å¹³ç›¸å½“çš„å·¥ä½œæµï¼Œå¹¶æ˜¾è‘—é™ä½äº†å°å‹ç”µåŠ›å…¬å¸çš„æŠ€æœ¯é—¨æ§›å’Œæˆæœ¬ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PowerChain_Automating_Distribution_Grid_Analysis_with_Agentic_AI_Workflows.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17078v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      131. [Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Linguistic_Neuron_Overlap_Patterns_to_Facilitate_Cross-lingual_Transfer_on_Low-resource_Languages.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Information Science and Technology, Beijing Foreign Studies University</span></p>
<p>æœ¬æ–‡æå‡ºBridgeX-ICLæ–¹æ³•ï¼Œé€šè¿‡æ¢ç©¶å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ä¸åŒè¯­è¨€ä¹‹é—´çš„ç¥ç»å…ƒé‡å æ¨¡å¼ï¼Œåˆ©ç”¨MUSEå­—å…¸æ„å»ºæ¢æµ‹æ•°æ®ï¼Œæ¿€æ´»å¹¶é‡åŒ–è¯­è¨€é‡å ç¥ç»å…ƒï¼Œå¹¶åŸºäºHSICæŒ‡æ ‡é€‰æ‹©æœ€ä¼˜æ¡¥æ¥è¯­è¨€ï¼Œæå‡ä½èµ„æºè¯­è¨€çš„é›¶æ ·æœ¬è·¨è¯­è¨€æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨15å¯¹è¯­è¨€å’Œä¸¤é¡¹è·¨è¯­è¨€ä»»åŠ¡ä¸­å¹³å‡æå‡å‡†ç¡®ç‡6%ï¼Œæ­ç¤ºäº†LLMså¤šè¯­è¨€å†…éƒ¨æœºåˆ¶å¹¶éªŒè¯äº†æ¡¥æ¥ç¥ç»å…ƒä¿ƒè¿›è·¨è¯­è¨€è¿ç§»çš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Linguistic_Neuron_Overlap_Patterns_to_Facilitate_Cross-lingual_Transfer_on_Low-resource_Languages.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17057v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      132. [GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detection]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GRAID__Synthetic_Data_Generation_with_Geometric_Constraints_and_Multi-Agentic_Reflection_for_Harmful.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Capital One</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†GRAIDï¼Œä¸€ç§ç»“åˆå‡ ä½•çº¦æŸå’Œå¤šæ™ºèƒ½ä½“åæ€æœºåˆ¶çš„æ•°æ®å¢å¼ºæµæ°´çº¿ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæœ‰å®³æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„æ•°æ®æ‰©å……ã€‚æ–¹æ³•åŒ…æ‹¬é€šè¿‡å—æ§LLMç”Ÿæˆå‡ ä½•å‡è¡¡æ ·æœ¬ï¼Œéšåé‡‡ç”¨å¤šæ™ºèƒ½ä½“åæ€æµç¨‹å¼•å…¥é£æ ¼å¤šæ ·æ€§å’Œè¾¹ç•Œæ ·æœ¬ï¼Œå®éªŒç»“æœè¡¨æ˜GRAIDå¯æå‡ä¸»æµæ–‡æœ¬åˆ†ç±»å™¨åœ¨æœ‰å®³å†…å®¹æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GRAID_Synthetic_Data_Generation_with_Geometric_Constraints_and_Multi-Agentic_Reflection_for_Harmful_Content_Detection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16989v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      133. [Unveiling the Latent Directions of Reflection in Large Language Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Unveiling_the_Latent_Directions_of_Reflection_in_Large_Language_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">National Taiwan University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¿€æ´»æ“æ§ï¼ˆactivation steeringï¼‰çš„æ–¹æ³•ï¼Œç³»ç»Ÿæ€§åˆ†æå’Œè§£é‡Šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åæ€ï¼ˆreflectionï¼‰æœºåˆ¶ï¼Œå°†åæ€åˆ’åˆ†ä¸ºæ— åæ€ã€å†…åœ¨åæ€å’Œè§¦å‘åæ€ä¸‰ç±»ï¼Œå¹¶é€šè¿‡æ„å»ºéšç©ºé—´æ–¹å‘å‘é‡æ­ç¤ºä¸åŒåæ€æ°´å¹³é—´çš„æ½œåœ¨è¿‡æ¸¡ã€‚å®éªŒè¡¨æ˜ï¼Œæ¿€æ´»æ“æ§ä¸ä»…èƒ½å‘ç°æ–°çš„åæ€è§¦å‘æŒ‡ä»¤ï¼Œè¿˜èƒ½åœ¨æ¨ç†æ—¶ç›´æ¥å¢å¼ºæˆ–æŠ‘åˆ¶åæ€è¡Œä¸ºï¼Œä¸”æŠ‘åˆ¶åæ€æ¯”å¢å¼ºæ›´å®¹æ˜“ã€‚ç»“è®ºæŒ‡å‡ºï¼Œåæ€èƒ½åŠ›ä¸ç‰¹å®šæ¿€æ´»æ¨¡å¼é«˜åº¦ç›¸å…³ï¼Œè¯¥æœºåˆ¶æ—¢å¯ç”¨äºæå‡æ¨¡å‹å®‰å…¨æ€§ï¼Œä¹Ÿå¯èƒ½è¢«æ¶æ„åˆ©ç”¨ä»¥è§„é¿å®‰å…¨æªæ–½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Unveiling_the_Latent_Directions_of_Reflection_in_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16983v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      134. [ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ReFactX__Scalable_Reasoning_with_Reliable_Facts_via_Constrained_Generation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Milano-Bicocca</span></p>
<p>æœ¬æ–‡æå‡ºReFactXæ–¹æ³•ï¼Œé€šè¿‡å—çº¦æŸç”Ÿæˆä¸é¢„æ„å»ºå‰ç¼€æ ‘ç´¢å¼•ï¼Œä½¿LLMåœ¨æ— éœ€å¤–éƒ¨æ£€ç´¢å™¨æˆ–è¾…åŠ©æ¨¡å‹æƒ…å†µä¸‹ï¼Œç›´æ¥é«˜æ•ˆæ¥å…¥è¶…å¤§è§„æ¨¡çŸ¥è¯†åº“ï¼Œæå‡çŸ¥è¯†é—®ç­”ä»»åŠ¡ä¸­çš„äº‹å®æ€§ä¸å¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨800Mäº‹å®è§„æ¨¡ä¸‹ä»…å¢åŠ çº¦1%å»¶è¿Ÿï¼Œåœ¨å¤šä¸ªQAåŸºå‡†ä¸Šå¯æå‡å‡†ç¡®ç‡è‡³90%ä»¥ä¸Šï¼Œå¹¶æ˜¾è‘—å‡å°‘å¹»è§‰ç°è±¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ReFactX_Scalable_Reasoning_with_Reliable_Facts_via_Constrained_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16972v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      135. [Robust Diagram Reasoning: A Framework for Enhancing LVLM Performance on Visually Perturbed Scientific Diagrams]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Robust_Diagram_Reasoning__A_Framework_for_Enhancing_LVLM_Performance_on_Visually_Perturbed_Scientifi.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Taiyuan University of Science and Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†Robust Diagram Reasoning (RDR) æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”å¤šè§†å›¾ä¸ä¸€è‡´æ€§éªŒè¯æœºåˆ¶ï¼ˆAMCVï¼‰ï¼Œå¯¹ç§‘å­¦å›¾è¡¨ç”Ÿæˆå¤šç§æ‰°åŠ¨è§†å›¾ï¼Œåœ¨æ¨ç†é˜¶æ®µå¹¶è¡Œæ¨ç†å¹¶è¿›è¡Œè‡ªçº æ­£ï¼Œä»¥æå‡å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨å«å™ªã€æ¨¡ç³Šã€é®æŒ¡ç­‰è§†è§‰é€€åŒ–ä¸‹çš„é²æ£’æ€§ã€‚å®éªŒåŸºäºæ–°æ„å»ºçš„SciDiagram-Robustæ•°æ®é›†ï¼Œç»“æœæ˜¾ç¤ºRDRæ¡†æ¶èƒ½æ˜¾è‘—æå‡LVLMçš„æ‰°åŠ¨é²æ£’æ€§ï¼Œè¶…è¶Šç°æœ‰å¼€æºå’Œé—­æºæ¨¡å‹ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Robust_Diagram_Reasoning_A_Framework_for_Enhancing_LVLM_Performance_on_Visually_Perturbed_Scientific_Diagrams.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16949v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      136. [Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Breaking_the_Exploration_Bottleneck__Rubric-Scaffolded_Reinforcement_Learning_for_General_LLM_Reason.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zhejiang University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†RuscaRLï¼Œä¸€ç§åˆ©ç”¨checklisté£æ ¼rubricsä½œä¸ºæ˜¾å¼æ¢ç´¢æ”¯æ¶å’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¸“ä¸ºæå‡LLMåœ¨å¼€æ”¾ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æ–¹æ³•åŒ…æ‹¬åœ¨ç”Ÿæˆé˜¶æ®µé€šè¿‡åˆ†ç»„å·®å¼‚å’Œé€æ­¥è¡°å‡çš„æ–¹å¼æä¾›rubricæŒ‡å¯¼ï¼Œè®­ç»ƒé˜¶æ®µç”¨å¤šç»´rubricæ‰“åˆ†ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚å®éªŒç»“æœæ˜¾ç¤ºRuscaRLåœ¨å¤šé¡¹åŒ»å­¦å’ŒSTEMåŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰RLæ–¹æ³•ï¼Œæå‡å°æ¨¡å‹èƒ½åŠ›ï¼Œéƒ¨åˆ†æŒ‡æ ‡è¶…è¶ŠGPT-4.1å’ŒOpenAI-o3ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Breaking_the_Exploration_Bottleneck_Rubric-Scaffolded_Reinforcement_Learning_for_General_LLM_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17018v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      137. [Limitations of refinement methods for weak to strong generalization]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Limitations_of_refinement_methods_for_weak_to_strong_generalization.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Michigan</span></p>
<p>æœ¬æ–‡é‡‡ç”¨å¹¿æ³›çš„æ¦‚ç‡æ¡†æ¶ï¼Œåˆ†æäº†å¼±è®­ç»ƒå’Œæ ‡ç­¾ä¿®æ­£æ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼±åˆ°å¼ºæ³›åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¯æ˜äºŒè€…éƒ½å­˜åœ¨ä¸å¯æ¶ˆé™¤çš„è¯¯å·®ï¼Œæ— æ³•è¾¾åˆ°ç†è®ºæœ€ä¼˜ã€‚ç»“è®ºæŒ‡å‡ºï¼Œç°æœ‰å¼±åˆ°å¼ºæ³›åŒ–æŠ€æœ¯ï¼ˆå¦‚å¼±è®­ç»ƒå’Œæ ‡ç­¾ä¿®æ­£ï¼‰æ— æ³•ä¸€è‡´æ”¶æ•›äºç›®æ ‡å‡½æ•°ï¼Œå¼ºè°ƒæœªæ¥éœ€å¼€å‘å…¼å…·å¯å®è·µæ€§å’Œç†è®ºæœ€ä¼˜æ€§çš„æ›¿ä»£æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Limitations_of_refinement_methods_for_weak_to_strong_generalization.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16929v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      138. [Attention Layers Add Into Low-Dimensional Residual Subspaces]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Attention_Layers_Add_Into_Low-Dimensional_Residual_Subspaces.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Innovation Institute</span></p>
<p>è¯¥è®ºæ–‡å‘ç°Transformerä¸­çš„æ³¨æ„åŠ›å±‚è¾“å‡ºè¢«é™åˆ¶åœ¨ä¸€ä¸ªä½ç»´å­ç©ºé—´ï¼Œè¡¨æ˜çº¦60%çš„æ–¹å‘åŒ…å«99%çš„æ–¹å·®ï¼Œä¸”è¿™ç§ä½ç§©ç»“æ„æ˜¯ç”±è¾“å‡ºæŠ•å½±çŸ©é˜µå¼•èµ·å¹¶å¯¼è‡´ç¨€ç–å­—å…¸å­¦ä¹ ä¸­å¤§é‡dead featureã€‚æ–‡ç« æå‡ºActive Subspace Initializationæ–¹æ³•ï¼Œå°†ç¨€ç–è‡ªç¼–ç å™¨çš„ç‰¹å¾åˆå§‹åŒ–åˆ°æ¿€æ´»ç©ºé—´çš„ä¸»å­ç©ºé—´ï¼Œæ˜¾è‘—å‡å°‘dead featureæ•°é‡ï¼ˆä»87%é™è‡³1%ä»¥ä¸‹ï¼‰å¹¶æå‡é‡å»ºè´¨é‡ï¼Œè¯¥æ–¹æ³•å¯æ¨å¹¿åˆ°å…¶ä»–ç¨€ç–å­—å…¸å­¦ä¹ æ¨¡å‹ï¼Œæœ‰æ•ˆæå‡å¤§è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ•ˆç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Attention_Layers_Add_Into_Low-Dimensional_Residual_Subspaces.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16926v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      139. [TextOnly: A Unified Function Portal for Text-Related Functions on Smartphones]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TextOnly__A_Unified_Function_Portal_for_Text-Related_Functions_on_Smartphones.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tsinghua University</span></p>
<p>æœ¬æ–‡æå‡ºäº†TextOnlyï¼Œä¸€ç§é›†æˆäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒBERTæ¨¡å‹çš„ç»Ÿä¸€åŠŸèƒ½å…¥å£ï¼Œç”¨æˆ·å¯é€šè¿‡åœ¨å•ä¸€æ–‡æœ¬æ¡†ä¸­è¾“å…¥åŸå§‹æ–‡æœ¬å³å¯è®¿é—®å¤šåº”ç”¨ä¸­çš„æ–‡æœ¬ç›¸å…³åŠŸèƒ½ã€‚æ–¹æ³•åŒ…æ‹¬åˆ©ç”¨LLMæä¾›é€šç”¨çŸ¥è¯†å’Œå†·å¯åŠ¨èƒ½åŠ›ï¼ŒBERTæ¨¡å‹å®ç°ç”¨æˆ·ä¸ªæ€§åŒ–å’Œå¿«é€Ÿæ¨æ–­ï¼Œå¹¶é‡‡ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æå‡é¢„æµ‹å‡†ç¡®ç‡ã€‚å®éªŒè¯æ˜ï¼ŒTextOnlyåœ¨çœŸå®ç”¨æˆ·ä¸­å–å¾—äº†71.35%çš„Top-1å‡†ç¡®ç‡ï¼Œæ”¯æŒæ›´å¤šåŠŸèƒ½ä¸”è¾“å…¥æ›´ç²¾ç®€ï¼Œç”¨æˆ·æ»¡æ„åº¦é«˜ï¼Œä¸”ç³»ç»Ÿèƒ½æŒç»­è‡ªæˆ‘ä¼˜åŒ–ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TextOnly_A_Unified_Function_Portal_for_Text-Related_Functions_on_Smartphones.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16859v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      140. [Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Beyond_Emotion_Recognition__A_Multi-Turn_Multimodal_Emotion_Understanding_and_Reasoning_Benchmark.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Hefei University of Technology</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†MTMEURå¤šè½®å¤šæ¨¡æ€æƒ…æ„Ÿç†è§£ä¸æ¨ç†åŸºå‡†ï¼Œæ¶µç›–1451ä¸ªçœŸå®åœºæ™¯è§†é¢‘åŠ5101ä¸ªé€’è¿›å¼å¤šç»´æƒ…æ„Ÿé—®é¢˜ï¼Œå¹¶è®¾è®¡å¤šæ™ºèƒ½ä½“æ¡†æ¶åˆ†åˆ«èšç„¦èƒŒæ™¯ã€è§’è‰²ã€äº‹ä»¶å’Œå†³ç­–ä¿¡æ¯åä½œæ¨ç†ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•æå‡äº†å¤šæ¨¡æ€å¤§æ¨¡å‹å¯¹å¤æ‚æƒ…æ„Ÿæ¨ç†ä»»åŠ¡çš„è¡¨ç°ï¼Œä½†ç°æœ‰MLLMåœ¨è´Ÿé¢æƒ…æ„Ÿæ¨ç†ä¸Šä»å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼ŒMTMEURå¯ä½œä¸ºæœªæ¥æ¨¡å‹æ”¹è¿›çš„é‡è¦èµ„æºã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Beyond_Emotion_Recognition_A_Multi-Turn_Multimodal_Emotion_Understanding_and_Reasoning_Benchmark.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16850v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      141. [RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysis]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/RADAR__A_Reasoning-Guided_Attribution_Framework_for_Explainable_Visual_Data_Analysis.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Massachusetts Institute of Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºRADARæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚InternLM-XComposer2ï¼‰ç”Ÿæˆå›¾è¡¨é—®é¢˜çš„é€æ­¥æ•°å­¦æ¨ç†ï¼Œå¹¶å®ç°ç­”æ¡ˆåŠæ¨ç†è¿‡ç¨‹çš„å¯è§†åŒ–å½’å› ã€‚æ–¹æ³•åŒ…æ‹¬æ„å»ºåŒ…å«é—®é¢˜ã€ç­”æ¡ˆã€æ¨ç†ã€å½’å› æ³¨é‡Šçš„é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨Partial-LoRAè§†è§‰è‡ªé€‚åº”æŠ€æœ¯ï¼Œé€šè¿‡æ»‘åŠ¨çª—å£ä¸å¤šæ¨¡æ€ç‰¹å¾å¯¹é½ç”Ÿæˆå›¾è¡¨åŒºåŸŸå½’å› ã€‚ç»“è®ºï¼šRADARå½’å› å‡†ç¡®ç‡æå‡15%ï¼Œè‡ªåŠ¨æ¨ç†ä¸å½’å› æ˜¾è‘—æé«˜å¤šæ¨¡æ€æ¨¡å‹çš„å¯ä¿¡åº¦å’Œè§£é‡Šæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/RADAR_A_Reasoning-Guided_Attribution_Framework_for_Explainable_Visual_Data_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16846v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      142. [Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Quantifying_Sycophancy_as_Deviations_from_Bayesian_Rationality_in_LLMs.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Northeastern University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºç”¨è´å¶æ–¯æ¡†æ¶é‡åŒ–å¤§è¯­è¨€æ¨¡å‹(LLM)ä¸­çš„è°„åªšï¼ˆsycophancyï¼‰ç°è±¡ï¼Œå°†å…¶è§†ä¸ºåç¦»è´å¶æ–¯ç†æ€§è¡Œä¸ºçš„è¡¨ç°ï¼ŒåŒºåˆ†åˆç†ä¸éç†æ€§çš„ä¿¡å¿µæ›´æ–°ã€‚ä½œè€…åœ¨ä¸‰ç±»ä»»åŠ¡ï¼ˆå¯¹è¯é¢„æµ‹ã€é“å¾·åˆ¤æ–­ã€æ–‡åŒ–å¯æ¥å—æ€§ï¼‰ä¸Šï¼Œåˆ©ç”¨ä¸åŒæ¨¡å‹å’Œæ¦‚ç‡åˆ¤æ–­æ–¹æ³•ï¼Œç³»ç»Ÿè¯„ä¼°äº†ç”¨æˆ·è§†è§’ä¸‹LLMçš„è°„åªšç¨‹åº¦åŠå¯¹è´å¶æ–¯è¯¯å·®çš„å½±å“ã€‚ç»“è®ºæ˜¾ç¤ºï¼šLLMæ™®ééè´å¶æ–¯ç†æ€§ï¼Œè°„åªšæ˜¾è‘—åŠ å‰§äº†æ¨¡å‹å¯¹ç”¨æˆ·å€¾å‘ç»“æœçš„æ¦‚ç‡åç§»ï¼Œå¹¶åœ¨å¤šä»»åŠ¡&#x2F;æ¨¡å‹ä¸‹å¢åŠ äº†è´å¶æ–¯è¯¯å·®ï¼Œä½“ç°å‡ºè°„åªšå¯¹æ¨¡å‹æ¨ç†çš„è´Ÿé¢å½±å“ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Quantifying_Sycophancy_as_Deviations_from_Bayesian_Rationality_in_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17153v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      143. [Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Natural_Language_Satisfiability__Exploring_the_Problem_Distribution_and_Evaluating_Transformer-based.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Department of Computer Science, University of Manchester</span></p>
<p>æœ¬æ–‡æå‡ºä»¥è‡ªç„¶è¯­è¨€ä¸ºè¡¨è¾¾çš„å¯æ»¡è¶³æ€§é—®é¢˜ï¼Œç³»ç»Ÿæ„å»ºæ¶µç›–å¤šç§è¯­è¨€ç‰‡æ®µå’Œå¤šç§è®¡ç®—å¤æ‚åº¦çš„æ•°æ®é›†ï¼Œå¹¶å®è¯è¯„ä¼°T5ã€DeBERTaç­‰ä¸»æµTransformerè¯­è¨€æ¨¡å‹åœ¨å½’çº³ã€æ¨ç†å’Œé›¶æ ·æœ¬è‡ªç„¶è¯­è¨€å¯æ»¡è¶³æ€§åˆ¤å®šä¸Šçš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨å¤æ‚åº¦æ›´é«˜çš„è¯­è¨€ç‰‡æ®µä¸Šå‡†ç¡®ç‡æ˜æ˜¾ä¸‹é™ï¼Œä¸”éš¾ä»¥æ³›åŒ–å’Œå­¦ä¹ åº•å±‚é€»è¾‘æ¨ç†è§„åˆ™ï¼Œéœ€è¿›ä¸€æ­¥ç ”ç©¶æå‡æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Natural_Language_Satisfiability_Exploring_the_Problem_Distribution_and_Evaluating_Transformer-based_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17126v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      144. [Token Homogenization under Positional Bias]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Token_Homogenization_under_Positional_Bias.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">HSE University</span></p>
<p>æœ¬è®ºæ–‡ç³»ç»Ÿæå‡ºå’Œå®è¯åˆ†æäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­tokenåŒè´¨åŒ–ï¼ˆtoken homogenizationï¼‰ç°è±¡åŠå…¶ä¸ä½ç½®åç½®ï¼ˆpositional biasï¼‰çš„å…³ç³»ã€‚é€šè¿‡å¤šç§æ–°é¢–ä¸å·²æœ‰çš„åº¦é‡æ–¹æ³•ï¼ˆå¦‚Effective Rankã€Maximum Explainable Varianceã€SchattenèŒƒæ•°ã€MAUVEåˆ†æ•°ç­‰ï¼‰ï¼Œåœ¨LLaMA-3ã€Gemmaå’ŒQwenæ¨¡å‹ä¸Šï¼Œä½œè€…å‘ç°éšç€å±‚æ•°åŠ æ·±ä¸”è¾“å…¥ä¸­å…³é”®ä¿¡æ¯å‡ºç°æ˜¾è‘—ä½ç½®åç½®æ—¶ï¼Œtokenè¡¨å¾è¶‹äºåŒè´¨åŒ–ï¼Œè¯´æ˜ä½ç½®åç½®ä¼šæ˜¾è‘—åŠ å‰§tokenä¿¡æ¯çš„ä¸§å¤±ã€‚ç»“è®ºè®¤ä¸ºï¼Œä½ç½®åç½®ä¸tokenåŒè´¨åŒ–å¯†åˆ‡ç›¸å…³ï¼Œè¿™ä¸€ç°è±¡å¯¹æ¨¡å‹è§£é‡Šæ€§ã€æ€§èƒ½åŠå‹ç¼©æœ‰é‡è¦å¯ç¤ºã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Token_Homogenization_under_Positional_Bias.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17079v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      145. [Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Zero-shot_Multimodal_Document_Retrieval_via_Cross-modal_Question_Generation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Yonsei University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºPREMIRæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)ç”Ÿæˆè·¨æ¨¡æ€é¢„é—®é¢˜(preQs)ï¼Œå°†æ–‡æ¡£åˆ†è§£ä¸ºç»†ç²’åº¦å¤šæ¨¡æ€tokençº§è¡¨ç¤ºï¼Œæ„å»ºå…±äº«åµŒå…¥ç©ºé—´ï¼Œå®ç°å¤šæ¨¡æ€æ–‡æ¡£çš„é›¶æ ·æœ¬æ£€ç´¢ã€‚PREMIRåœ¨é—­åŸŸå’Œå¤šè¯­è¨€åŸºå‡†ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰å¼ºåŸºçº¿ï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œå¼ºæ³›åŒ–ä¸”é€‚ç”¨äºçœŸå®åœºæ™¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Zero-shot_Multimodal_Document_Retrieval_via_Cross-modal_Question_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17000v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      146. [KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/KL-Regularised_Q-Learning__A_Token-level_Action-Value_perspective_on_Online_RLHF.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Cambridge</span></p>
<p>è¯¥è®ºæ–‡æå‡ºKL-regularised Q-Learning (KLQ)ç®—æ³•ï¼Œä½œä¸ºé€‚ç”¨äºå¤§è¯­è¨€æ¨¡å‹ä»äººç±»åé¦ˆä¸­å¼ºåŒ–å­¦ä¹ ï¼ˆLM-RLHFï¼‰çš„æ–°å‹tokençº§åŠ¨ä½œå€¼æ–¹æ³•ã€‚KLQé€šè¿‡Î»-returnå›å½’å’Œç‰¹æ®Šçš„åŠ¨ä½œå€¼å‡½æ•°åˆ†è§£å®ç°é«˜æ•ˆè®­ç»ƒï¼Œå¹¶ç†è®ºä¸Šè¯æ˜äº†å…¶ä¸PPOæ–¹æ³•åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ç­‰ä»·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKLQåœ¨æ‘˜è¦å’Œå¯¹è¯ä»»åŠ¡ä¸­å¯¹é½ç›®æ ‡ä¸PPOç›¸å½“ï¼Œä½†åœ¨LLMè¯„åˆ¤ä¸­è¡¨ç°æ›´ä¼˜ï¼Œå…·æœ‰æ›´æ¸…æ™°çš„ç†è®ºåŠ¨æœºã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/KL-Regularised_Q-Learning_A_Token-level_Action-Value_perspective_on_Online_RLHF.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16860v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      147. [TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TriagerX__Dual_Transformers_for_Bug_Triaging_Tasks_with_Content_and_Interaction_Based_Rankings.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Calgary</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºè‡ªåŠ¨åŒ–Bugåˆ†æ´¾çš„æ··åˆæ¡†æ¶TriagerXï¼Œé€šè¿‡é›†æˆä¸¤ä¸ªé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰çš„åŒtransformeræ¶æ„ï¼ˆCBRï¼‰å’ŒåŸºäºå¼€å‘è€…å†å²äº¤äº’çš„æ’åºæ–¹æ³•ï¼ˆIBRï¼‰ï¼Œå¢å¼ºäº†å¯¹bugå†…å®¹å’Œå¼€å‘è€…ç»éªŒçš„ç»¼åˆå»ºæ¨¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTriagerXåœ¨å¤šä¸ªå…¬å¼€å’Œå·¥ä¸šæ•°æ®é›†ä¸Šç›¸è¾ƒäºä¹ç§SOTAåŸºçº¿æ¨¡å‹ï¼Œåœ¨Top-1å’ŒTop-3å‡†ç¡®ç‡ä¸Šå‡æå‡10%ä»¥ä¸Šï¼Œå·¥ä¸šéƒ¨ç½²åé¦ˆä¹ŸéªŒè¯äº†å…¶å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TriagerX_Dual_Transformers_for_Bug_Triaging_Tasks_with_Content_and_Interaction_Based_Rankings.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17550v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      148. [In-Context Algorithm Emulation in Fixed-Weight Transformers]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/In-Context_Algorithm_Emulation_in_Fixed-Weight_Transformers.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Center for Foundation Models and Generative AI, Northwestern University</span></p>
<p>æœ¬æ–‡æå‡ºå¹¶ä¸¥æ ¼è¯æ˜äº†ï¼šå³ä½¿æ˜¯æœ€å°åŒ–çš„ã€å†»ç»“æƒé‡çš„Transformeræ¶æ„ï¼ˆä»…å«softmax self-attentionå±‚ï¼Œæ— éœ€å‰é¦ˆç½‘ç»œå’Œå‚æ•°æ›´æ–°ï¼‰ï¼Œä¹Ÿèƒ½é€šè¿‡é€‚å½“è®¾è®¡è¾“å…¥promptï¼Œå®ç°å¯¹ä¸€å¤§ç±»ç»å…¸ç®—æ³•ï¼ˆå¦‚æ¢¯åº¦ä¸‹é™ã€çº¿æ€§&#x2F;å²­å›å½’ï¼‰çš„é«˜ç²¾åº¦æ¨¡æ‹Ÿã€‚å…³é”®æ–¹æ³•æ˜¯åœ¨promptä¸­ç¼–ç ç®—æ³•å‚æ•°ï¼Œè¯±å¯¼æ³¨æ„åŠ›æœºåˆ¶å®Œæˆæ‰€éœ€è®¡ç®—ï¼Œç†è®ºä¸å®éªŒå‡éªŒè¯äº†é€šè¿‡æ”¹å˜promptå¯åœ¨æ— éœ€å†è®­ç»ƒçš„å‰æä¸‹åˆ‡æ¢å’Œæ¨¡æ‹Ÿä¸åŒç®—æ³•ã€‚ç»“è®ºï¼šTransformeræ¨¡å‹æœ¬è´¨ä¸Šå¯ä½œä¸ºå¯é€šè¿‡promptç¼–ç¨‹çš„é€šç”¨ç®—æ³•åº“ï¼Œè¿™ä¸ºin-context learningæä¾›äº†æ¸…æ™°çš„ç†è®ºåŸºç¡€ï¼Œå¹¶å¯¹ç†è§£å¤§æ¨¡å‹æ³›åŒ–èƒ½åŠ›åŠpromptè®¾è®¡å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/In-Context_Algorithm_Emulation_in_Fixed-Weight_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17540v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      149. [Activation Transport Operators]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Activation_Transport_Operators.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Cambridge</span></p>
<p>æœ¬æ–‡æå‡ºäº†Activation Transport Operatorsï¼ˆATOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åœ¨å˜æ¢å™¨è§£ç å™¨æ¨¡å‹ä¸­ï¼Œé€šè¿‡çº¿æ€§æ˜ å°„é¢„æµ‹ä¸‹æ¸¸æ®‹å·®æµå‘é‡çš„æ–°æ–¹æ³•ã€‚åˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰è¿›è¡Œç‰¹å¾ç©ºé—´æŠ•å½±ï¼Œå¹¶é€šè¿‡å›å½’ä¸å› æœéªŒè¯è¯„ä¼°ç‰¹å¾æ˜¯å¦çº¿æ€§ä¼ é€’æˆ–ç”±éçº¿æ€§å±‚é‡æ–°åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç‰¹å¾åœ¨çº¿æ€§é€šé“ä¸­ä¸»è¦åœ¨ç›¸é‚»å±‚é—´ä¼ é€’ï¼Œéšç€å±‚è·å¢åŠ çº¿æ€§ä¼ é€’èƒ½åŠ›å‡å¼±ï¼ŒATOæ–¹æ³•ä¸ºLLMå®‰å…¨ã€è°ƒè¯•å’Œç‰¹å¾æµåŠ¨æ˜ å°„æä¾›å®ç”¨å·¥å…·ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Activation_Transport_Operators.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17490v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      150. [Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Ranking]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Efficient_Zero-Shot_Long_Document_Classification_by_Reducing_Context_Through_Sentence_Ranking.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Pune Institute of Computer Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„é›¶æ ·æœ¬é•¿æ–‡æ¡£åˆ†ç±»æ–¹æ³•ï¼Œé€šè¿‡TF-IDFå¥å­æ’åºå®ç°ä¸Šä¸‹æ–‡å‹ç¼©ï¼Œæ— éœ€ä¿®æ”¹Transformeræ¨¡å‹ç»“æ„ã€‚è¯¥æ–¹æ³•å°†çŸ­æ–‡æœ¬è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚BERTï¼‰ç›´æ¥åº”ç”¨äºé•¿æ–‡æ¡£ï¼Œé€šè¿‡ç­›é€‰æœ€å…·ä¿¡æ¯æ€§çš„å¥å­ï¼Œæ˜¾è‘—å‡å°‘è¾“å…¥é•¿åº¦å’Œæ¨ç†æ—¶é—´è€Œä¿æŒåˆ†ç±»å‡†ç¡®ç‡ï¼ŒéªŒè¯äºMahaNewsé©¬æ‹‰åœ°è¯­é•¿æ–°é—»æ•°æ®é›†ï¼Œæœ€é«˜å¯æå‡æ¨ç†æ•ˆç‡35%ï¼Œéƒ¨åˆ†æƒ…å†µä¸‹ç²¾åº¦è¶…è¿‡å…¨æ–‡åŸºçº¿ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Efficient_Zero-Shot_Long_Document_Classification_by_Reducing_Context_Through_Sentence_Ranking.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17444v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      151. [MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MahaParaphrase__A_Marathi_Paraphrase_Detection_Corpus_and_BERT-based_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Pune Institute of Computer Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†MahaParaphraseæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªåŒ…å«8000å¯¹é©¬æ‹‰åœ°è¯­å¥å­ï¼ˆç­‰é‡çš„åŒä¹‰ä¸éåŒä¹‰å¥å¯¹ï¼‰å¹¶ç»è¿‡äººå·¥æ ‡æ³¨çš„é«˜è´¨é‡é‡Šä¹‰æ£€æµ‹è¯­æ–™ã€‚ä½œè€…åŸºäºè¯é‡å åº¦å°†æ•°æ®åˆ†ä¸ºäº”ä¸ªæ¡¶ï¼Œå¹¶è¯„æµ‹äº†Murilã€mBERTã€IndicBERTå’ŒMahaBERTç­‰å¤šç§BERTæ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºMahaBERTåœ¨è¯¥ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼ˆF1&#x3D;88.7%ï¼‰ã€‚ç»“è®ºï¼šMahaParaphraseä¸ºé©¬æ‹‰åœ°è¯­è‡ªç„¶è¯­è¨€å¤„ç†æä¾›äº†é‡è¦èµ„æºï¼Œæ¨åŠ¨äº†ä½èµ„æºè¯­è¨€é‡Šä¹‰æ£€æµ‹çš„ç ”ç©¶ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MahaParaphrase_A_Marathi_Paraphrase_Detection_Corpus_and_BERT-based_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17391v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      152. [Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Large_Language_Models_as_Universal_Predictors__An_Empirical_Study_on_Small_Tabular_Datasets.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute for Language and Speech Processing, Athena Research Center</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†ä¸»æµå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-5ã€GPT-4oã€Gemini-2.5-Flashç­‰ï¼‰åœ¨å°è§„æ¨¡ç»“æ„åŒ–è¡¨æ ¼æ•°æ®ä¸Šçš„åˆ†ç±»ã€å›å½’å’Œèšç±»ä»»åŠ¡è¡¨ç°ï¼Œé€šè¿‡å°‘æ ·æœ¬æç¤ºï¼ˆfew-shot promptingï¼‰å®ç°é›¶è®­ç»ƒé¢„æµ‹ã€‚å®éªŒå‘ç°ï¼ŒLLMsåœ¨åˆ†ç±»ä»»åŠ¡ä¸­å¯ä½œä¸ºæœ‰æ•ˆé›¶è®­ç»ƒåŸºçº¿ï¼Œè¡¨ç°æ¥è¿‘ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½†åœ¨å›å½’å’Œèšç±»ä»»åŠ¡ä¸­è¡¨ç°æ¬ ä½³ï¼Œä¸»è¦å—é™äºè¿ç»­æ•°å€¼é¢„æµ‹èƒ½åŠ›å’Œæ— ç›‘ç£èšç±»ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ ç¼ºå¤±ã€‚ç»“è®ºï¼šLLMsé€‚åˆäºè¡¨æ ¼åˆ†ç±»ä»»åŠ¡çš„å¿«é€ŸåŸå‹å¼€å‘ï¼Œä½†åœ¨å›å½’ä¸èšç±»ä»»åŠ¡ä»æœ‰é™åˆ¶ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Large_Language_Models_as_Universal_Predictors_An_Empirical_Study_on_Small_Tabular_Datasets.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17387v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      153. [GRAPH-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Graph-R1__Incentivizing_the_Zero-Shot_Graph_Learning_Capability_in_LLMs_via_Explicit_Reasoning.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Beihang University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å®Œå…¨ä¸ä¾èµ–GNNçš„å›¾å­¦ä¹ æ–¹æ³•GRAPH-R1ï¼Œå°†èŠ‚ç‚¹åˆ†ç±»ã€é“¾è·¯é¢„æµ‹å’Œå›¾åˆ†ç±»ç­‰ä»»åŠ¡è½¬åŒ–ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯å¤„ç†çš„æ–‡æœ¬æ¨ç†é—®é¢˜ã€‚é€šè¿‡æ„å»ºåŒ…å«è¯¦ç»†æ¨ç†é“¾æ¡çš„æ–°å‹æ•°æ®é›†ï¼Œå¹¶ç»“åˆä»»åŠ¡ç‰¹å®šçš„æ¨ç†æ¨¡æ¿å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–ï¼ŒGRAPH-R1å®ç°åœ¨è·¨ä»»åŠ¡ã€è·¨é¢†åŸŸä¸‹çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ¨ç†è¿‡ç¨‹å…·å¤‡å¯è§£é‡Šæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GRAPH-R1_Incentivizing_the_Zero-Shot_Graph_Learning_Capability_in_LLMs_via_Explicit_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17380v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      154. [Mimicking the Physicistâ€™s Eye: A VLM-centric Approach for Physics Formula Discovery]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Mimicking_the_Physicist's_Eye_A_VLM-centric_Approach_for_Physics_Formula_Discovery.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">UNCâ€“Chapel Hill</span></p>
<p>æœ¬æ–‡æå‡ºVIPER-R1ï¼Œä¸€ç§è§†è§‰-è¯­è¨€å¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡Motion Structure Inductionï¼ˆMSIï¼‰ä¸Causal Chain-of-Thoughtï¼ˆC-CoTï¼‰ç›‘ç£è®­ç»ƒï¼Œç»“åˆReward-Guided Symbolic Calibrationï¼ˆRGSCï¼‰å¼ºåŒ–å­¦ä¹ ï¼Œå®ç°ç‰©ç†å…¬å¼çš„è‡ªåŠ¨å‘ç°ã€‚æ¨¡å‹æ¨ç†é˜¶æ®µåˆ©ç”¨å¤–éƒ¨ç¬¦å·å›å½’å·¥å…·è¿›è¡ŒSymbolic Residual Realignmentï¼ˆSRÂ²ï¼‰ï¼Œå°†ç†è®ºå…¬å¼ä¸å®é™…æ•°æ®å¯¹é½ã€‚å®éªŒç»“æœæ˜¾ç¤ºVIPER-R1åœ¨PhysSymbolæ•°æ®é›†ç»“æ„åŒ¹é…å’Œç²¾åº¦æ–¹é¢å‡è¶…è¿‡SOTAåŸºçº¿ï¼Œå®ç°äº†æ›´ç²¾ç¡®å’Œå¯è§£é‡Šçš„ç‰©ç†è§„å¾‹å‘ç°ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Mimicking_the_Physicist%E2%80%99s_Eye_A_VLM-centric_Approach_for_Physics_Formula_Discovery.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17366v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      155. [Evolving Collective Cognition in Humanâ€“Agent Hybrid Societies: How Agents Form Stances and Boundaries]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Evolving_Collective_Cognition_in_Human-Agent_Hybrid_Societies__How_Agents_Form_Stances_and_Boundarie.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">William &amp; Mary</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†CMASEï¼ˆComputational Multi-Agent Society Experimentï¼‰æ¡†æ¶ï¼Œå°†ç”Ÿæˆå¼å¤šæ™ºèƒ½ä½“å»ºæ¨¡ä¸è™šæ‹Ÿæ°‘æ—å¿—æ–¹æ³•ç»“åˆï¼Œç³»ç»Ÿç ”ç©¶äº†äºº-æ™ºèƒ½ä½“æ··åˆç¤¾ä¼šä¸­ç«‹åœºåˆ†åŒ–å’Œç¤¾ä¼šè¾¹ç•Œå½¢æˆçš„æœºåˆ¶ã€‚é€šè¿‡ä¸‰é¡¹å®éªŒè¯æ˜ï¼Œæ™ºèƒ½ä½“å¯åŸºäºè¯­è¨€äº’åŠ¨è‡ªå‘å½¢æˆç«‹åœºå¹¶é‡å¡‘ç¤¾ä¼šç»“æ„ï¼Œå¯¹äººç±»å¹²é¢„è¡¨ç°å‡ºä¸åŒå“åº”ï¼Œç»“è®ºæŒ‡å‡ºï¼Œæ™ºèƒ½ä½“çš„ç¤¾ä¼šç»“æ„ä¸ç”±é¢„è®¾èº«ä»½å†³å®šï¼Œè€Œæ˜¯åœ¨äº’åŠ¨ä¸­åŠ¨æ€ç”Ÿæˆï¼Œæç¤ºæœªæ¥AIç¤¾ä¼šå»ºæ¨¡åº”å…³æ³¨å†…ç”Ÿè®¤çŸ¥æœºåˆ¶ä¸äº’åŠ¨åŠ¨åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Evolving_Collective_Cognition_in_Human%E2%80%93Agent_Hybrid_Societies_How_Agents_Form_Stances_and_Boundaries.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17361v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      156. [Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Trust_Me,_I_Know_This_Function__Hijacking_LLM_Static_Analysis_using_Bias.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Ben Gurion University of the Negev</span></p>
<p>è¯¥è®ºæ–‡æå‡ºå¹¶ç³»ç»Ÿæ€§åˆ†æäº†ä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç é™æ€åˆ†æçš„å…¨æ–°æ”»å‡»æ–¹å¼â€”â€”Familiar Pattern Attackï¼ˆFPAï¼‰ã€‚æ–¹æ³•ä¸Šï¼Œä½œè€…é€šè¿‡è‡ªåŠ¨åŒ–é»‘ç›’ç®—æ³•ï¼Œåˆ©ç”¨LLMå¯¹ç†Ÿæ‚‰ç¼–ç¨‹æ¨¡å¼çš„æŠ½è±¡åè§ï¼Œå®ç°äº†å¯¹å¤šç§ä¸»æµæ¨¡å‹ï¼ˆGPT-4oã€Claude 3.5ã€Gemini 2.0ï¼‰å’Œå¤šç§ç¼–ç¨‹è¯­è¨€çš„è·¨æ¨¡å‹ã€è·¨è¯­è¨€è¿ç§»æ”»å‡»ã€‚ç»“è®ºè¡¨æ˜ï¼Œè¿™ä¸€æ”»å‡»æ–¹å¼å¯ä»¥åœ¨ä¸å½±å“å®é™…ä»£ç è¿è¡Œçš„å‰æä¸‹ï¼Œè¯¯å¯¼LLMå¯¹ä»£ç è¡Œä¸ºçš„åˆ¤æ–­ï¼Œä¸”éš¾ä»¥é€šè¿‡æç¤ºå·¥ç¨‹ã€æ•°æ®å»é‡æˆ–ç®€å•é˜²å¾¡ç­–ç•¥æ¶ˆé™¤ï¼Œå¯¹è‡ªåŠ¨åŒ–ä»£ç å®¡è®¡ã€ä»£ç å®‰å…¨åŠæ•æ„Ÿé€»è¾‘é˜²æŠ¤å¸¦æ¥å¹¿æ³›æŒ‘æˆ˜å’Œå®‰å…¨éšæ‚£ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Trust_Me_I_Know_This_Function_Hijacking_LLM_Static_Analysis_using_Bias.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17343v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      157. [Agentic AI for Software: thoughts from Software Engineering community]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Agentic_AI_for_Software__thoughts_from_Software_Engineering_community.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">National University of Singapore</span></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä»¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ ¸å¿ƒé©±åŠ¨çš„AIè‡ªä¸»ä»£ç†åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒä¸ä»…é™äºä»£ç ç”Ÿæˆï¼Œè¿˜åŒ…æ‹¬è‡ªåŠ¨åŒ–çš„é—®é¢˜è§£å†³ã€æ„å›¾æ¨æ–­å’Œä»£ç éªŒè¯ã€‚ä½œè€…ä»¥AutoCodeRoverä¸ºä¾‹ï¼Œæå‡ºé€šè¿‡ç¨‹åºè¡¨ç¤ºå’Œåˆ†æå·¥å…·æ¨æ–­å¼€å‘è€…æ„å›¾ï¼Œå®ç°æ›´å¯ä¿¡çš„è‡ªåŠ¨åŒ–è½¯ä»¶å·¥ç¨‹ï¼Œå¹¶æŒ‡å‡ºæœªæ¥AIä»£ç†å°†åœ¨ä»£ç éªŒè¯å’Œå®‰å…¨å®¡è®¡ä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Agentic_AI_for_Software_thoughts_from_Software_Engineering_community.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17340v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      158. [Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphs]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Capturing_Legal_Reasoning_Paths_from_Facts_to_Law_in_Court_Judgments_using_Knowledge_Graphs.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The University of Tokyo</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼ˆLKGï¼‰å¯¹æ—¥æœ¬è¡Œæ”¿æ³•é™¢åˆ¤å†³ä¸­çš„æ³•å¾‹æ¨ç†è·¯å¾„è¿›è¡Œå»ºæ¨¡çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸‰æ­¥æµç¨‹ï¼š1ï¼‰ä½¿ç”¨GPT-4oæç¤ºæŠ½å–äº‹å®ã€æ³•å¾‹è§„èŒƒã€æ³•å¾‹åº”ç”¨å’Œæ¡æ–‡èŠ‚ç‚¹ï¼›2ï¼‰è§„èŒƒæ³•å¾‹æ¡æ–‡å¼•ç”¨å¹¶æ„å»ºèŠ‚ç‚¹é—´å…³ç³»ï¼›3ï¼‰åŸºäºçŸ¥è¯†å›¾è°±å®ç°ä»äº‹å®åˆ°æ³•å¾‹æ¡æ–‡çš„æ£€ç´¢ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLKGæ–¹æ³•åœ¨æ³•å¾‹æ¡æ–‡æ£€ç´¢å‡†ç¡®ç‡å’Œå¬å›ç‡ä¸Šæ˜¾è‘—ä¼˜äºä»…ç”¨å¤§è¯­è¨€æ¨¡å‹å’Œæ£€ç´¢å¢å¼ºæ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´æ¸…æ™°åœ°æ­ç¤ºå’Œè§£é‡Šå¸æ³•æ¨ç†ç»“æ„ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Capturing_Legal_Reasoning_Paths_from_Facts_to_Law_in_Court_Judgments_using_Knowledge_Graphs.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17337v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      159. [DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DropLoRA__Sparse_Low-Rank_Adaptation_for_Parameter-Efficient_Fine-Tuning.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Science and Technology of China</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•DropLoRAï¼Œé€šè¿‡åœ¨LoRAçš„ä¸¤ä¸ªä½ç§©çŸ©é˜µä¹‹é—´æ’å…¥åŠ¨æ€å‰ªææ¨¡å—ï¼Œæ¨¡æ‹Ÿä½ç§©å­ç©ºé—´å­¦ä¹ ï¼Œä»è€Œæå‡å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDropLoRAåœ¨å¸¸è¯†æ¨ç†ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’ŒæŒ‡ä»¤è·Ÿéšç­‰å¤šé¡¹LLMä»»åŠ¡ä¸Šå‡ä¼˜äºLoRAåŠå…¶ä»–ä¸»æµPEFTæ–¹æ³•ï¼Œä¸”ä¸å¢åŠ è®­ç»ƒå’Œæ¨ç†æˆæœ¬ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/DropLoRA_Sparse_Low-Rank_Adaptation_for_Parameter-Efficient_Fine-Tuning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17330v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      160. [Omne-R1: Learning to Reason with Memory for Multi-hop Question Answering]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Omne-R1__Learning_to_Reason_with_Memory_for_Multi-hop_Question_Answering.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tanka Inc.</span></p>
<p>Omne-R1æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆä¸¤è½®å¼ºåŒ–å­¦ä¹ å’Œä¸€æ¬¡ç›‘ç£å¾®è°ƒï¼Œé›†æˆå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œå®ç°å¯¹æ— æ¨¡å¼çŸ¥è¯†å›¾è°±çš„å¤šè·³é—®ç­”ã€‚é€šè¿‡è‡ªåŠ¨æ„å»ºé¢†åŸŸæ— å…³çŸ¥è¯†å›¾è°±å’Œç”Ÿæˆå¤šè·³QAæ•°æ®ï¼Œæ–¹æ³•åœ¨å¤æ‚3+è·³é—®é¢˜ä¸Šå–å¾—æ˜¾è‘—æ€§èƒ½æå‡ï¼Œå…·å¤‡è‰¯å¥½è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Omne-R1_Learning_to_Reason_with_Memory_for_Multi-hop_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17324v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      161. [CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CultranAI_at_PalmX_2025__Data_Augmentation_for_Cultural_Knowledge_Representation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Qatar University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†CultranAIç³»ç»Ÿï¼Œé€šè¿‡æ•°æ®å¢å¼ºå’ŒLoRAå¾®è°ƒæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯æ–‡åŒ–çŸ¥è¯†è¡¨è¾¾ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ–¹æ³•åŒ…æ‹¬åˆå¹¶PalmXã€PalmåŠæ–°æ‰©å±•çš„22Kå¤šé¡¹é€‰æ‹©é¢˜æ•°æ®é›†ï¼Œé‡‡ç”¨LoRAå’ŒQLoRAå¯¹Fanar-1-9B-Instructæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå®éªŒæ˜¾ç¤ºæ•°æ®å¢å¼ºæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æ–‡åŒ–ç›¸å…³ä»»åŠ¡çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨PalmXç«èµ›ä¸­å–å¾—ç¬¬5åã€‚ç»“è®ºï¼šå¤šæºæ•°æ®å¢å¼ºå’Œé€‚å½“çš„å¾®è°ƒæŠ€æœ¯èƒ½å¤Ÿæå‡LLMå¯¹é˜¿æ‹‰ä¼¯æ–‡åŒ–çŸ¥è¯†çš„ç†è§£å’Œè¡¨è¾¾èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CultranAI_at_PalmX_2025_Data_Augmentation_for_Cultural_Knowledge_Representation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17322v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      162. [Chinese Court Simulation with LLM-Based Agent System]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Chinese_Court_Simulation_with_LLM-Based_Agent_System.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tsinghua University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºSimCourtæ¡†æ¶ï¼ŒåŸºäºä¸­å›½çœŸå®åº­å®¡æµç¨‹ï¼Œåˆ©ç”¨å…·å¤‡è®°å¿†ã€è§„åˆ’å’Œåæ€èƒ½åŠ›çš„LLMæ³•åº­å¤šè§’è‰²æ™ºèƒ½ä½“ï¼ˆå¦‚æ³•å®˜ã€æ£€å¯Ÿå®˜ã€å¾‹å¸ˆç­‰ï¼‰ï¼Œç»“åˆæ³•å¾‹æ¡æ–‡å’Œæ¡ˆä¾‹æ£€ç´¢å·¥å…·ï¼Œè‡ªåŠ¨æ¨¡æ‹Ÿäº”é˜¶æ®µå®Œæ•´åº­å®¡è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜SimCourtåœ¨åˆ¤å†³é¢„æµ‹å’Œè¿‡ç¨‹è¡¨ç°ä¸Šä¼˜äºç°æœ‰äººç±»åŠè‡ªåŠ¨åŒ–åŸºçº¿ï¼Œæ˜¾ç¤ºåœ¨æ³•å¾‹æ•™è‚²å’Œå¸æ³•æ”¯æŒä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Chinese_Court_Simulation_with_LLM-Based_Agent_System.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17243v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      163. [CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CoViPAL__Layer-wise_Contextualized_Visual_Token_Pruning_for_Large_Vision-Language_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Computer Science, Wuhan University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†CoViPALï¼Œä¸€ç§å±‚çº§ä¸Šä¸‹æ–‡åŒ–çš„è§†è§‰tokenå‰ªææ–¹æ³•ï¼Œé€šè¿‡Plug-and-Play Pruning Moduleï¼ˆPPMï¼‰è½»é‡çº§åˆ†ç±»å™¨ï¼Œåœ¨å¤šå±‚æ¬¡ä¸Šé¢„æµ‹å¹¶ç§»é™¤å†—ä½™è§†è§‰tokenï¼Œä»è€Œå‡å°‘å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨æ¨ç†é˜¶æ®µçš„è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚ç»“æœæ˜¾ç¤ºï¼ŒCoViPALèƒ½åœ¨ä¸æ˜¾è‘—é™ä½æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œæœ€å¤šå‰ªæ75%çš„è§†è§‰tokenï¼Œä½¿é¢„å¡«å……æ—¶é—´æœ€å¤šç¼©çŸ­60%ï¼Œå¹¶è¶…è¶Šäº†ä¸»æµè®­ç»ƒ&#x2F;éè®­ç»ƒå‹å‰ªææ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CoViPAL_Layer-wise_Contextualized_Visual_Token_Pruning_for_Large_Vision-Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17233v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      164. [Module-Aware Parameter-Efficient Machine Unlearning on Transformers]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Module-Aware_Parameter-Efficient_Machine_Unlearning_on_Transformers.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zhejiang University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†MAPE-Unlearnæ¨¡å—æ„ŸçŸ¥å‚æ•°é«˜æ•ˆæœºå™¨é—å¿˜æ–¹æ³•ï¼Œåˆ©ç”¨å¯å­¦ä¹ æ©ç åœ¨Transformerçš„headså’Œfiltersæ¨¡å—å®šä½å½±å“å…³é”®å‚æ•°ï¼Œé€šè¿‡æ¨¡å—çº§ç¨€ç–æ›´æ–°å®ç°é«˜æ•ˆæ•°æ®é—å¿˜ï¼Œå¹¶å¯é›†æˆå¤šç§ä¸»æµé—å¿˜ç®—æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒMAPE-Unlearnåœ¨å„ç±»Transformeræ¨¡å‹å’Œä»»åŠ¡ä¸­å®ç°äº†ä¼˜å¼‚çš„é—å¿˜æ•ˆæœå’Œæ¨¡å‹é²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿ç»­é—å¿˜å’Œå¯¹æŠ—é‡å­¦æ”»å‡»åœºæ™¯ä¸‹è¡¨ç°çªå‡ºã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Module-Aware_Parameter-Efficient_Machine_Unlearning_on_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17225v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      165. [SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SSFO__Self-Supervised_Faithfulness_Optimization_for_Retrieval-Augmented_Generation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>æœ¬æ–‡æå‡ºäº†SSFOï¼ˆä¸€ç§è‡ªç›‘ç£çš„ä¿¡å®æ€§ä¼˜åŒ–æ–¹æ³•ï¼‰ï¼Œé€šè¿‡å¯¹æ¯”å¤§è¯­è¨€æ¨¡å‹åœ¨æœ‰æ— æ£€ç´¢ä¸Šä¸‹æ–‡æ¡ä»¶ä¸‹çš„å“åº”ï¼Œè‡ªåŠ¨ç”Ÿæˆåå¥½æ•°æ®å¯¹å¹¶åº”ç”¨Direct Preference Optimizationï¼ˆDPOï¼‰è¿›è¡Œå¯¹é½è®­ç»ƒï¼Œæ— éœ€äººå·¥æ ‡æ³¨æˆ–å¼ºAIç›‘ç£ï¼Œä»è€Œæå‡RAGç³»ç»Ÿçš„ä¿¡å®æ€§å¹¶å‡å°‘å¹»è§‰ã€‚ç»“è®ºè¡¨æ˜ï¼ŒSSFOåœ¨å¤šæ•°æ®é›†ã€å¤šè¯­è¨€ç¯å¢ƒä¸‹æ˜¾è‘—æå‡äº†ä¸Šä¸‹æ–‡ä¿¡å®æ€§ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”å‡ ä¹ä¸æŸå®³æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SSFO_Self-Supervised_Faithfulness_Optimization_for_Retrieval-Augmented_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17222v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      166. [Exposing Privacy Risks in Graph Retrieval-Augmented Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Exposing_Privacy_Risks_in_Graph_Retrieval-Augmented_Generation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Pennsylvania State University</span></p>
<p>æœ¬è®ºæ–‡ç³»ç»Ÿåˆ†æäº†Graph Retrieval-Augmented Generationï¼ˆGraph RAGï¼‰ç³»ç»Ÿåœ¨æ•°æ®éšç§æ–¹é¢çš„é£é™©ï¼Œæå‡ºå¹¶å®æ–½ä¸“é—¨çš„æ•°æ®æå–æ”»å‡»ï¼Œæ¯”è¾ƒäº†ä¸ä¼ ç»ŸRAGç³»ç»Ÿçš„ç»“æ„åŒ–ä¿¡æ¯æ³„æ¼å·®å¼‚ï¼Œå¹¶æ¢è®¨äº†é˜²å¾¡æœºåˆ¶ã€‚ç»“è®ºæ˜¾ç¤ºï¼Œå°½ç®¡Graph RAGå¯å‡å°‘åŸå§‹æ–‡æœ¬æ³„æ¼ï¼Œä½†å…¶ç»“æ„åŒ–å®ä½“å’Œå…³ç³»ä¿¡æ¯ææ˜“è¢«æå–ï¼Œç°æœ‰ç®€å•é˜²å¾¡æ‰‹æ®µéš¾ä»¥æœ‰æ•ˆé˜²æŠ¤ï¼ŒäºŸéœ€é’ˆå¯¹ç»“æ„ç‰¹æ€§çš„éšç§ä¿æŠ¤æŠ€æœ¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Exposing_Privacy_Risks_in_Graph_Retrieval-Augmented_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17215v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      167. [How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG Systems]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/How_to_make_Medical_AI_Systems_safer__Simulating_Vulnerabilities,_and_Threats_in_Multimodal_Medical_.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Warwick</span></p>
<p>æœ¬è®ºæ–‡æå‡ºMedThreatRAGæ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°æ¨¡æ‹Ÿå’Œè¯„ä¼°é’ˆå¯¹å¤šæ¨¡æ€åŒ»å­¦RAGç³»ç»Ÿçš„æ•°æ®æŠ•æ¯’æ”»å‡»ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ”»å‡»ã€è§†è§‰æ”»å‡»å’Œè·¨æ¨¡æ€å†²çªæ³¨å…¥ï¼ˆCMCIï¼‰ï¼Œé€šè¿‡åœ¨åŠå¼€æ”¾å¼çŸ¥è¯†åº“ä¸­æ³¨å…¥å¯¹æŠ—æ€§å›¾æ–‡å¯¹ï¼Œå¹²æ‰°æ£€ç´¢ã€é‡æ’åºå’Œç”Ÿæˆæµç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMedThreatRAGèƒ½æ˜¾è‘—é™ä½ä¸»æµåŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚LLaVA-Med-1.5ï¼‰åœ¨IU-Xrayå’ŒMIMIC-CXRæ•°æ®é›†ä¸Šçš„F1åˆ†æ•°ï¼Œæœ€å¤§é™å¹…è¾¾27.66%ï¼Œæš´éœ²äº†åŒ»å­¦RAGç³»ç»Ÿåœ¨å¤šæ¨¡æ€ä¸€è‡´æ€§å’Œå®‰å…¨æ€§ä¸Šçš„æ ¹æœ¬ç¼ºé™·ã€‚ç»“è®ºå¼ºè°ƒéœ€é‡‡ç”¨æ›´å…·å¨èƒæ„ŸçŸ¥å’Œå¤šæ¨¡æ€ä¸€è‡´æ€§æ ¡éªŒçš„é˜²å¾¡æœºåˆ¶ï¼Œä»¥ä¿éšœåŒ»ç–—AIçš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/How_to_make_Medical_AI_Systems_safer_Simulating_Vulnerabilities_and_Threats_in_Multimodal_Medical_RAG_Systems.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17200v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      168. [Large Language Model-Based Automatic Formulation for Stochastic Optimization Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Large_Language_Model-Based_Automatic_Formulation_for_Stochastic_Optimization_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Ohio State University</span></p>
<p>æœ¬è®ºæ–‡é¦–æ¬¡ç³»ç»Ÿæ€§ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå°¤å…¶æ˜¯ChatGPTï¼Œåœ¨å°†è‡ªç„¶è¯­è¨€æè¿°è‡ªåŠ¨è½¬åŒ–ä¸ºéšæœºä¼˜åŒ–æ¨¡å‹ï¼ˆå¦‚è”åˆä¸ä¸ªä½“æœºä¼šçº¦æŸæ¨¡å‹ã€ä¸¤é˜¶æ®µéšæœºçº¿æ€§è§„åˆ’SLP-2ï¼‰ä¸­çš„è¡¨ç°ã€‚ä½œè€…æå‡ºå¤šæ™ºèƒ½ä½“åä½œã€é“¾å¼æ€ç»´ã€å¤šæ ·åŒ–æç¤ºå·¥ç¨‹ï¼Œå¹¶è®¾è®¡äº†ç»“æ„åŒ–è½¯è¯„åˆ†ä½“ç³»ï¼Œè¾ƒå¥½åœ°åº¦é‡äº†ç”Ÿæˆæ¨¡å‹çš„ç»“æ„è´¨é‡å’Œéƒ¨åˆ†æ­£ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒGPT-4-Turboé…åˆcot_s_instructionsåŠagenticæç¤ºæ–¹æ³•èƒ½é«˜æ•ˆä¸”å‡†ç¡®åœ°å®Œæˆéšæœºä¼˜åŒ–å»ºæ¨¡ä»»åŠ¡ï¼Œå‡å°‘é”™è¯¯ï¼Œæ¨åŠ¨åŸºäºè¯­è¨€çš„æ™ºèƒ½å»ºæ¨¡æµç¨‹å‘å±•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Large_Language_Model-Based_Automatic_Formulation_for_Stochastic_Optimization_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17196v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      169. [BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/BudgetThinker__Empowering_Budget-aware_LLM_Reasoning_with_Control_Tokens.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute for AI Industry Research (AIR), Tsinghua University</span></p>
<p>BudgetThinkeræå‡ºäº†ä¸€ç§é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‘¨æœŸæ€§æ’å…¥å‰©ä½™é¢„ç®—æç¤ºæ§åˆ¶tokençš„æ–¹æ³•ï¼Œå®ç°äº†å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†é•¿åº¦çš„ç²¾ç»†æ§åˆ¶ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œå…ˆé€šè¿‡æœ‰ç›‘ç£å¾®è°ƒ(SFT)è®©æ¨¡å‹ç†Ÿæ‚‰é¢„ç®—çº¦æŸï¼Œå†ç”¨å¼•å…¥é•¿åº¦å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å…¶å…¼é¡¾å‡†ç¡®æ€§ä¸é¢„ç®—éµå¾ªã€‚å®éªŒè¯æ˜ï¼ŒBudgetThinkeråœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨ä¸åŒé¢„ç®—ä¸‹å‡ä¼˜äºç°æœ‰é«˜æ•ˆæ¨ç†æ–¹æ³•ï¼Œèƒ½æ›´å¥½åœ°å…¼é¡¾æ¨ç†å‡†ç¡®ç‡å’Œé¢„ç®—æ§åˆ¶ï¼Œé€‚åˆèµ„æºå—é™ä¸å®æ—¶åœºæ™¯éƒ¨ç½²ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/BudgetThinker_Empowering_Budget-aware_LLM_Reasoning_with_Control_Tokens.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17188v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      170. [PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PosterGen__Aesthetic-Aware_Paper-to-Poster_Generation_via_Multi-Agent_LLMs.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Stony Brook University</span></p>
<p>æœ¬æ–‡æå‡ºäº†PosterGenï¼Œä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å­¦æœ¯è®ºæ–‡è‡ªåŠ¨æµ·æŠ¥ç”Ÿæˆæ¡†æ¶ã€‚æ–¹æ³•ä¸Šï¼ŒPosterGené€šè¿‡è§£æã€ç­–å±•ã€ç©ºé—´å¸ƒå±€ã€é£æ ¼åŒ–å’Œæ¸²æŸ“ç­‰ä¸“ç”¨æ™ºèƒ½ä½“åä½œï¼Œç»“åˆè®¾è®¡ç¾å­¦åŸåˆ™ï¼ˆå™äº‹ç»“æ„ã€ä¸‰æ å¸ƒå±€ã€é…è‰²ã€å­—ä½“å±‚çº§ï¼‰ï¼Œè‡ªåŠ¨ç”Ÿæˆå…¼å…·å†…å®¹å‡†ç¡®æ€§å’Œè§†è§‰ç¾æ„Ÿçš„å­¦æœ¯æµ·æŠ¥ã€‚ç»“è®ºå®éªŒè¡¨æ˜ï¼ŒPosterGenåœ¨å†…å®¹å¿ å®åº¦ä¸è®¾è®¡ç¾è§‚æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆæµ·æŠ¥æ— éœ€å¤§é‡äººå·¥ä¼˜åŒ–ï¼Œå…·å¤‡å®ç”¨æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PosterGen_Aesthetic-Aware_Paper-to-Poster_Generation_via_Multi-Agent_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17182v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      171. [LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LLM_Assertiveness_can_be_Mechanistically_Decomposed_into_Emotional_and_Logical_Components.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Cardiff University</span></p>
<p>è¯¥è®ºæ–‡é€šè¿‡å¯¹Llama-3.2æ¨¡å‹åœ¨äººå·¥æ ‡æ³¨çš„è‡ªä¿¡è¡¨è¾¾æ•°æ®é›†ä¸Šçš„å¾®è°ƒï¼Œåˆ©ç”¨æœºåˆ¶è§£é‡Šæ–¹æ³•åˆ†ææ¨¡å‹å±‚çº§æ¿€æ´»ï¼Œå‘ç°é«˜è‡ªä¿¡è¡¨è¾¾åœ¨æ¨¡å‹å†…éƒ¨å¯åˆ†è§£ä¸ºæƒ…æ„Ÿå’Œé€»è¾‘ä¸¤ä¸ªæ­£äº¤å­æˆåˆ†ã€‚ç»“è®ºæŒ‡å‡ºä¸¤ç±»å­æˆåˆ†åœ¨æ¨¡å‹é¢„æµ‹ä¸­äº§ç”Ÿä¸åŒå› æœå½±å“ï¼Œä¸ºè§£é‡ŠLLMè¿‡åº¦è‡ªä¿¡è¡Œä¸ºåŠå¹²é¢„æœºåˆ¶æä¾›äº†æ–°æ€è·¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LLM_Assertiveness_can_be_Mechanistically_Decomposed_into_Emotional_and_Logical_Components.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17180v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      172. [MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MaRVL-QA__A_Benchmark_for_Mathematical_Reasoning_over_Visual_Landscapes.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Waymo</span></p>
<p>æœ¬æ–‡æå‡ºMaRVL-QAåŸºå‡†ï¼Œä¸“ä¸ºå®šé‡è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•°å­¦ä¸ç©ºé—´æ¨ç†ä¸­çš„èƒ½åŠ›ï¼ŒåŒ…å«æ‹“æ‰‘è®¡æ•°å’Œå˜æ¢è¯†åˆ«ä¸¤å¤§ä»»åŠ¡ï¼Œå¹¶é€šè¿‡ä¸¥æ ¼çš„å‡½æ•°åº“å’Œå¤šé‡è¿‡æ»¤ç”Ÿæˆé«˜è´¨é‡é—®ç­”å¯¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä¾¿æ˜¯æœ€å…ˆè¿›çš„MLLMsåœ¨æŠ½è±¡æ•°å­¦å¯è§†åŒ–æ¨ç†ä»»åŠ¡ä¸Šä»å­˜åœ¨æ˜¾è‘—çŸ­æ¿ï¼Œè¡¨æ˜å½“å‰æ¨¡å‹ç¼ºä¹ç³»ç»ŸåŒ–ç©ºé—´ä¸ç¨‹åºåŒ–æ¨ç†èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MaRVL-QA_A_Benchmark_for_Mathematical_Reasoning_over_Visual_Landscapes.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17527v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      173. [Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Prediction]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Evaluating_Retrieval-Augmented_Generation_Strategies_for_Large_Language_Models_in_Travel_Mode_Choice.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The University of Texas at Austin</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†å°†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆï¼Œç”¨äºæ—…è¡Œæ–¹å¼é€‰æ‹©é¢„æµ‹ï¼Œå¼€å‘äº†åŒ…æ‹¬é›¶æ ·æœ¬é¢„æµ‹ã€åŸºç¡€RAGã€å¹³è¡¡æ£€ç´¢ã€äº¤å‰ç¼–ç å™¨é‡æ’åºç­‰å¤šç§ç­–ç•¥ï¼Œå¹¶åœ¨GPT-4oã€o3ã€o4-miniä¸‰ç§æ¶æ„ä¸Šè¿›è¡Œç³»ç»Ÿæ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼ŒRAGæ˜¾è‘—æå‡äº†é¢„æµ‹å‡†ç¡®ç‡ï¼Œå°¤å…¶æ˜¯GPT-4oç»“åˆå¹³è¡¡æ£€ç´¢å’Œé‡æ’åºå‡†ç¡®ç‡è¾¾80.8%ï¼Œä¼˜äºä¼ ç»Ÿç»Ÿè®¡å’Œæœºå™¨å­¦ä¹ æ–¹æ³•ï¼ŒåŒæ—¶LLMå…·å¤‡æ›´å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œç»“è®ºå¼ºè°ƒæ£€ç´¢ç­–ç•¥éœ€ä¸æ¨¡å‹èƒ½åŠ›åŒ¹é…ä»¥å‘æŒ¥æœ€å¤§æ½œåŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Evaluating_Retrieval-Augmented_Generation_Strategies_for_Large_Language_Models_in_Travel_Mode_Choice_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17467v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      174. [MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MoE-Inference-Bench__Performance_Evaluation_of_Mixture_of_Expert_Large_Language_and_Vision_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Argonne National Laboratory</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†MoE-Inference-BenchåŸºå‡†å¥—ä»¶ï¼Œç³»ç»Ÿè¯„ä¼°å¤šç§Mixture of Expertsï¼ˆMoEï¼‰å¤§è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸åŒç¡¬ä»¶ï¼ˆä¸»è¦ä¸ºNvidia H100 GPUï¼‰åŠå¤šç§ç®—æ³•ä¼˜åŒ–ï¼ˆå¦‚é‡åŒ–ã€å‰ªæã€æ¨ç†å¹¶è¡Œã€æ¨æµ‹è§£ç å’ŒFused MoEï¼‰ä¸‹çš„æ¨ç†æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œè¶…å‚æ•°é…ç½®å’Œç¡¬ä»¶ä¼˜åŒ–å¯¹MoEæ¨ç†æ•ˆç‡å½±å“æ˜¾è‘—ï¼ŒFP8é‡åŒ–å’Œå•ä¸“å®¶æ¿€æ´»å¯æå‡ååé‡20-80%ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†å»¶è¿Ÿæ˜¾è‘—é«˜äºçº¯æ–‡æœ¬æ¨¡å‹ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MoE-Inference-Bench_Performance_Evaluation_of_Mixture_of_Expert_Large_Language_and_Vision_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17320v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      175. [AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for Interpretable LLM Representations]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AdaptiveK_Sparse_Autoencoders__Dynamic_Sparsity_Allocation_for_Interpretable_LLM_Representations.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zhejiang University</span></p>
<p>æœ¬æ–‡æå‡ºäº†AdaptiveK Sparse Autoencoderï¼ˆAdaptiveK SAEï¼‰æ¡†æ¶ï¼Œé€šè¿‡çº¿æ€§æ¢é’ˆé¢„æµ‹è¾“å…¥æ–‡æœ¬çš„å¤æ‚åº¦ï¼Œæ ¹æ®å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ç¨€ç–è‡ªç¼–ç å™¨çš„æ¿€æ´»ç‰¹å¾æ•°ï¼Œå®ç°å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å†…éƒ¨è¡¨ç¤ºçš„è‡ªé€‚åº”åˆ†è§£ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä¸»æµLLMï¼ˆPythia-70Mã€Pythia-160Mã€Gemma-2-2Bï¼‰ä¸Šï¼Œç›¸æ¯”äºå›ºå®šç¨€ç–åº¦æ–¹æ³•ï¼Œåœ¨é‡æ„ç²¾åº¦ã€è§£é‡Šæ–¹å·®åŠä½™å¼¦ç›¸ä¼¼åº¦ç­‰æŒ‡æ ‡ä¸Šå‡å–å¾—æ›´ä¼˜è¡¨ç°ï¼Œå¹¶å¤§å¹…å‡å°‘è¶…å‚æ•°è°ƒä¼˜æˆæœ¬ã€‚ç»“è®ºï¼šAdaptiveK SAEå¯æ˜¾è‘—æå‡LLMå¯è§£é‡Šæ€§ä¸æ•ˆèƒ½ï¼Œå…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§å’Œå®ç”¨ä»·å€¼ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/AdaptiveK_Sparse_Autoencoders_Dynamic_Sparsity_Allocation_for_Interpretable_LLM_Representations.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17234v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      176. [ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ClaimGen-CN__A_Large-scale_Chinese_Dataset_for_Legal_Claim_Generation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Zhejiang University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºClaimGen-CNï¼Œé¦–ä¸ªå¤§è§„æ¨¡ä¸­æ–‡æ°‘äº‹æ¡ˆä»¶æ³•å¾‹è¯‰è¯·ç”Ÿæˆæ•°æ®é›†ï¼Œæ¶µç›–100ç±»æ¡ˆç”±ï¼Œä»çœŸå®åˆ¤å†³æ–‡æ¡£ä¸­æŠ½å–åŸå‘Šäº‹å®ä¸è¯‰è¯·ï¼Œå¹¶æå‡ºåŸºäºäº‹å®æ€§ä¸æ¸…æ™°åº¦çš„ç»†ç²’åº¦è¯„ä»·æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œä½œè€…å¯¹ä¸»æµé€šç”¨åŠæ³•å¾‹é¢†åŸŸå¤§æ¨¡å‹åœ¨é›¶æ ·æœ¬æ³•å¾‹è¯‰è¯·ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°è¿›è¡Œäº†ç³»ç»Ÿè¯„æµ‹ï¼Œå‘ç°å½“å‰å¤§æ¨¡å‹åœ¨äº‹å®å‡†ç¡®æ€§å’Œè¡¨è¾¾æ¸…æ™°åº¦ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚ç»“è®ºæŒ‡å‡ºï¼Œè¯¥æ•°æ®é›†ä¸ºæ³•å¾‹AIç ”ç©¶æä¾›äº†åŸºç¡€ï¼Œæœªæ¥éœ€é’ˆå¯¹æ³•å¾‹æ–‡æœ¬ç”Ÿæˆç‰¹æ€§æ”¹è¿›å¤§æ¨¡å‹ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ClaimGen-CN_A_Large-scale_Chinese_Dataset_for_Legal_Claim_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18264v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      177. [MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MMTok__Multimodal_Coverage_Maximization_for_Efficient_Inference_of_VLMs.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Arizona State University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºMMTokæ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–è¦†ç›–çš„å¤šæ¨¡æ€å­é›†é€‰æ‹©ç­–ç•¥ï¼Œè”åˆåˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬Tokençš„ç›¸ä¼¼æ€§ï¼Œä»¥è´ªå¿ƒç®—æ³•è¿‘ä¼¼æ±‚è§£å­æ¨¡å—è¦†ç›–ä¼˜åŒ–é—®é¢˜ï¼Œå®ç°Vision-Language Model (VLM) é«˜æ•ˆæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒMMTokåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸åŒVLMæ¨¡å‹ä¸Šï¼Œèƒ½åœ¨æå¤§å‡å°‘è§†è§‰Tokenæ•°é‡çš„åŒæ—¶ä¿æŒåŸæœ‰æ€§èƒ½ï¼ˆå¦‚åœ¨POPEæ•°æ®é›†ä¸ŠåŠ é€Ÿ1.87å€ï¼Œä¿ç•™98.7%æ€§èƒ½ï¼‰ï¼Œä¼˜äºå•æ¨¡æ€å‰ªææ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MMTok_Multimodal_Coverage_Maximization_for_Efficient_Inference_of_VLMs.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18244v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      178. [Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Type-Compliant_Adaptation_Cascades__Adapting_Programmatic_LM_Workflows_to_Data.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Google</span></p>
<p>æœ¬è®ºæ–‡æå‡ºType-Compliant Adaptation Cascades (tacs) æ¡†æ¶ï¼Œå°†å¤æ‚LLMå·¥ä½œæµå»ºæ¨¡ä¸ºå¯æ¢¯åº¦è®­ç»ƒçš„ç±»å‹åŒ–æ¦‚ç‡ç¨‹åºï¼Œé€šè¿‡å‚æ•°é«˜æ•ˆçš„é€‚é…å’Œä¸¥æ ¼ç±»å‹çº¦æŸï¼Œå®ç°ç«¯åˆ°ç«¯ç»“æ„åŒ–ä»»åŠ¡é€‚åº”ã€‚å®éªŒè¯æ˜ï¼Œtacsåœ¨ç»“æ„åŒ–æ¨ç†ä»»åŠ¡ä¸Šæ€§èƒ½æ˜¾è‘—ä¼˜äºä¸»æµpromptä¼˜åŒ–æ–¹æ³•ï¼Œå°¤å…¶åœ¨æ¨¡å‹å®¹é‡è¾ƒå°æˆ–ä»»åŠ¡ç»“æ„è¦æ±‚ä¸¥æ ¼æ—¶æå‡æ›´å¤§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Type-Compliant_Adaptation_Cascades_Adapting_Programmatic_LM_Workflows_to_Data.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18224v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      179. [Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Flash_Sparse_Attention__An_Alternative_Efficient_Implementation_of_Native_Sparse_Attention_Kernel.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Hong Kong University of Science and Technology</span></p>
<p>æœ¬æ–‡æå‡ºFlash Sparse Attention (FSA)ï¼Œé€šè¿‡é‡æ„Native Sparse Attention (NSA)çš„å†…æ ¸å¾ªç¯é¡ºåºï¼Œå®ç°äº†å¯¹ä¸»æµLLMåœ¨ä¸åŒGQAåˆ†ç»„ä¸‹çš„é«˜æ•ˆç¨€ç–æ³¨æ„åŠ›è®¡ç®—ã€‚FSAé‡‡ç”¨KVå—å¤–å¾ªç¯å’ŒæŸ¥è¯¢tokenå†…å¾ªç¯ï¼Œå¹¶å¼•å…¥éè¿ç»­å†…å­˜è®¿é—®ä¼˜åŒ–ã€åœ¨çº¿softmaxåŠç´¯åŠ æœºåˆ¶ï¼Œæœ‰æ•ˆæ¶ˆé™¤NSAåœ¨GPUä¸Šçš„paddingä½æ•ˆã€‚å®éªŒè¡¨æ˜ï¼ŒFSAåœ¨è®­ç»ƒä¸æ¨ç†ç«¯æ˜¾è‘—æå‡ç³»ç»Ÿæ•ˆç‡ï¼Œæœ€é«˜å¯è¾¾3.5å€æ ¸çº§é€Ÿåº¦æå‡ï¼Œå¹¶ä¿æŒå‡†ç¡®æ€§ã€‚ç»“è®ºï¼šFSAä¸ºé•¿ä¸Šä¸‹æ–‡LLMæä¾›äº†æ›´é«˜æ•ˆçš„ç¨€ç–æ³¨æ„åŠ›ç³»ç»Ÿå®ç°ï¼Œæ˜¾è‘—åŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Flash_Sparse_Attention_An_Alternative_Efficient_Implementation_of_Native_Sparse_Attention_Kernel.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18192v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      180. [Unraveling the Cognitive Patterns of Large Language Models through Module Communities]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Unraveling_the_cognitive_patterns_of_Large_Language_Models_through_module_communities.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Rensselaer Polytechnic Institute</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç½‘ç»œç§‘å­¦çš„æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºå¤šé‡ç½‘ç»œå°†LLMçš„æ¶æ„æ¨¡å—ã€è®¤çŸ¥æŠ€èƒ½å’Œæ•°æ®é›†è”ç³»èµ·æ¥ï¼Œé‡‡ç”¨Louvainç¤¾åŒºå‘ç°ç­‰æŠ€æœ¯ï¼Œæ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹å†…éƒ¨æ¨¡å—ä¸è®¤çŸ¥æŠ€èƒ½ä¹‹é—´çš„å¯¹åº”å…³ç³»åŠå…¶åˆ†å¸ƒå¼ç‰¹æ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMçš„è®¤çŸ¥èƒ½åŠ›ä¾èµ–äºæ¨¡å—é—´çš„åŠ¨æ€äº¤äº’å’Œåˆ†å¸ƒå¼å­¦ä¹ ï¼Œè€Œéä¸¥æ ¼çš„åŠŸèƒ½æ¨¡å—åŒ–ï¼Œåˆ†å¸ƒå¼å¾®è°ƒç­–ç•¥ä¼˜äºä¸¥æ ¼çš„å±€éƒ¨å¾®è°ƒã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Unraveling_the_Cognitive_Patterns_of_Large_Language_Models_through_Module_Communities.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18190v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      181. [ST-Raptor: LLM-Powered Semi-Structured Table Question Answering]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ST-Raptor__LLM-Powered_Semi-Structured_Table_Question_Answering.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Jiao Tong University</span></p>
<p>æœ¬æ–‡æå‡ºST-Raptorï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åŠç»“æ„åŒ–è¡¨æ ¼é—®ç­”æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥åˆ†å±‚æ­£äº¤æ ‘ï¼ˆHO-Treeï¼‰å¯¹å¤æ‚è¡¨æ ¼ç»“æ„è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶è®¾è®¡åŸå­æ ‘æ“ä½œå®ç°é—®é¢˜åˆ†è§£ä¸æ“ä½œç®¡é“ç”Ÿæˆï¼Œç»“åˆä¸¤é˜¶æ®µéªŒè¯æœºåˆ¶æå‡ç­”æ¡ˆå‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚å®éªŒè¯æ˜ï¼ŒST-Raptoråœ¨SSTQAç­‰åŸºå‡†æ•°æ®é›†ä¸Šè¾ƒç°æœ‰æ–¹æ³•æœ€é«˜æå‡20%å‡†ç¡®ç‡ï¼Œæœ‰æ•ˆè§£å†³åŠç»“æ„åŒ–è¡¨æ ¼è§£æå’Œé—®ç­”éš¾é¢˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ST-Raptor_LLM-Powered_Semi-Structured_Table_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18183v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      182. [Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Leveraging_Large_Language_Models_for_Accurate_Sign_Language_Translation_in_Low-Resource_Scenarios.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Catania</span></p>
<p>è¯¥è®ºæ–‡æå‡ºAulSignï¼Œä¸€ç§ç»“åˆRetrieverã€LLMä¸Sign Mapperæ¨¡å—çš„æ‰‹è¯­ç¿»è¯‘æ–¹æ³•ï¼Œåˆ©ç”¨åŠ¨æ€promptå’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œé€šè¿‡å°†æ‰‹è¯­ä¸è‡ªç„¶è¯­è¨€ç´§å‡‘æè¿°å…³è”ï¼Œå®ç°ä½èµ„æºåœºæ™¯ä¸‹çš„é«˜æ•ˆç¿»è¯‘ã€‚å®éªŒåœ¨ASLå’ŒLISæ•°æ®é›†ä¸Šè¡¨æ˜ï¼ŒAulSignåœ¨spoken-to-signå’Œsign-to-spokenä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºæ•°æ®ç¨€ç¼ºç¯å¢ƒï¼Œå¹¶æå‡äº†å¯è§£é‡Šæ€§å’Œå¯ç”¨æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Leveraging_Large_Language_Models_for_Accurate_Sign_Language_Translation_in_Low-Resource_Scenarios.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18182v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      183. [ADLOCO: ADAPTIVE BATCHING SIGNIFICANTLY IMPROVES COMMUNICATIONS EFFICIENCY AND CONVERGENCE FOR LARGE LANGUAGE MODELS]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AdLoCo__adaptive_batching_significantly_improves_communications_efficiency_and_convergence_for_Large.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Moscow Institute of Physics and Technology</span></p>
<p>æœ¬æ–‡æå‡ºAdLoCoæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¤šå®ä¾‹è®­ç»ƒï¼ˆMITï¼‰ã€è‡ªé€‚åº”æ‰¹é‡è°ƒæ•´çš„DiLoCoå’Œæ¢¯åº¦ç´¯ç§¯åˆ‡æ¢æœºåˆ¶ï¼Œå®ç°äº†å¯¹å¤§è¯­è¨€æ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒçš„é€šä¿¡æ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦çš„æ˜¾è‘—æå‡ã€‚å®éªŒå’Œç†è®ºåˆ†æå‡è¡¨æ˜ï¼ŒAdLoCoåœ¨ç¡¬ä»¶èµ„æºåˆ©ç”¨ã€é€šä¿¡å¼€é”€å’Œè®­ç»ƒæ”¶æ•›æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶é€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ADLOCO_ADAPTIVE_BATCHING_SIGNIFICANTLY_IMPROVES_COMMUNICATIONS_EFFICIENCY_AND_CONVERGENCE_FOR_LARGE_LANGUAGE_MODELS.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18179v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      184. [SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SEAM__Semantically_Equivalent_Across_Modalities_Benchmark_for_Vision-Language_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Toronto</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†SEAMåŸºå‡†ï¼Œä¸“é—¨é’ˆå¯¹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è·¨æ¨¡æ€ä¸€è‡´æ€§è¯„ä»·ï¼Œæ„é€ äº†è¯­ä¹‰ç­‰ä»·çš„æ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼Œæ¶µç›–å›½é™…è±¡æ£‹ã€åŒ–å­¦ã€éŸ³ä¹å’Œå›¾è®ºå››å¤§é¢†åŸŸï¼Œä¿è¯ä¸åŒæ¨¡æ€é—´ä¿¡æ¯å®Œå…¨ç­‰ä»·ã€‚é€šè¿‡å¯¹21ä¸ªä¸»æµVLMsçš„ç³»ç»Ÿè¯„æµ‹ï¼Œå‘ç°æ‰€æœ‰æ¨¡å‹åœ¨è§†è§‰å’Œè¯­è¨€æ¨¡æ€é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œä¸”è·¨æ¨¡æ€ä¸€è‡´æ€§ä½ï¼Œä¸»è¦ç”±æ–‡æœ¬ç¬¦å·åŒ–å’Œè§†è§‰æ„ŸçŸ¥å¤±è¯¯å¯¼è‡´ã€‚ç»“è®ºæ˜¯å½“å‰VLMså°šæ— æ³•å®ç°æ¨¡æ€æ— å…³çš„ç»Ÿä¸€æ¨ç†ï¼ŒSEAMä¸ºåç»­æ¨¡å‹æ”¹è¿›å’Œå¯¹æ¯”æä¾›äº†ä¸¥è°¨åŸºå‡†ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SEAM_Semantically_Equivalent_Across_Modalities_Benchmark_for_Vision-Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18162v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      185. [The Computational Complexity of Satisfiability in State Space Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_Computational_Complexity_of_Satisfiability_in_State_Space_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Kassel</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†State Space Models (SSM)çš„å¯æ»¡è¶³æ€§åˆ¤å®šé—®é¢˜ï¼ˆssmSATï¼‰ï¼Œæ­ç¤ºå…¶åœ¨ä¸€èˆ¬æƒ…å†µä¸‹ä¸ºä¸å¯åˆ¤å®šï¼Œä½†åœ¨å®é™…åº”ç”¨å¸¸è§çš„æœ‰é™ä¸Šä¸‹æ–‡é•¿åº¦å’Œå®šå®½é‡åŒ–ç®—æœ¯ä¸¤ç§çº¦æŸä¸‹å¯åˆ¤å®šï¼Œå¹¶ç»™å‡ºäº†å…·ä½“çš„å¤æ‚æ€§ç•Œé™ï¼ˆå¦‚NP-Completeã€NEXPTIMEã€PSPACE-Completeã€EXPSPACEç­‰ï¼‰ã€‚ç»“è®ºæŒ‡å‡ºï¼ŒSSMçš„å½¢å¼åŒ–éªŒè¯é¢ä¸´æ˜¾è‘—çš„è®¡ç®—å¤æ‚æ€§æŒ‘æˆ˜ï¼Œå³ä½¿åœ¨å—é™ç¯å¢ƒä¸‹ä¹Ÿä¸å®¹å¿½è§†ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/The_Computational_Complexity_of_Satisfiability_in_State_Space_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18148v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      186. [Learning from Few Samples: A Novel Approach for High-Quality Malcode Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Learning_from_Few_Samples__A_Novel_Approach_for_High-Quality_Malcode_Generation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Huazhong University of Science and Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†GANGRL-LLMæ¡†æ¶ï¼Œå°†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆï¼Œé€šè¿‡ååŒè®­ç»ƒï¼Œåˆ©ç”¨åˆ¤åˆ«å™¨çš„è¾“å‡ºä½œä¸ºå¥–åŠ±ä¿¡å·ï¼ŒæŒ‡å¯¼ç”Ÿæˆå™¨åœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹ç”Ÿæˆé«˜è´¨é‡çš„æ¶æ„ä»£ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨SQLæ³¨å…¥æ”»å‡»ä»£ç ç”Ÿæˆå’Œæ£€æµ‹ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å…·å¤‡è‰¯å¥½çš„è¿ç§»èƒ½åŠ›ï¼Œèƒ½æœ‰æ•ˆæå‡å…¥ä¾µæ£€æµ‹ç³»ç»Ÿåœ¨æ ·æœ¬æœ‰é™æƒ…å†µä¸‹çš„é˜²å¾¡æ°´å¹³ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Learning_from_Few_Samples_A_Novel_Approach_for_High-Quality_Malcode_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18113v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      187. [The AI Data Scientist]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_AI_Data_Scientist.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Mohamed bin Zayed University of Artificial Intelligence</span></p>
<p>æœ¬æ–‡æå‡ºäº†AI Data Scientistï¼Œä¸€ç§ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„è‡ªæ²»ä»£ç†ç³»ç»Ÿï¼Œé€šè¿‡å…­ä¸ªä¸“ç”¨å­ä»£ç†è‡ªåŠ¨å®Œæˆæ•°æ®æ¸…æ´—ã€å‡è®¾ç”Ÿæˆä¸éªŒè¯ã€ç‰¹å¾å·¥ç¨‹ã€å»ºæ¨¡åŠä¸šåŠ¡å†³ç­–å»ºè®®ï¼Œå®ç°ä»åŸå§‹æ•°æ®åˆ°å¯æ“ä½œæ´å¯Ÿçš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¢†åŸŸçœŸå®æ•°æ®é›†ä¸Šä¼˜äºäººå·¥åŸºçº¿ï¼Œç»“æœæ›´å…·å¯è§£é‡Šæ€§ï¼Œæ˜¾è‘—æå‡æ•°æ®ç§‘å­¦æ•ˆç‡ä¸å†³ç­–è´¨é‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/The_AI_Data_Scientist.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18106v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      188. [A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A.S.E__A_Repository-Level_Benchmark_for_Evaluating_Security_in_AI-Generated_Code.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tencent</span></p>
<p>è¯¥è®ºæ–‡æå‡ºA.S.EåŸºå‡†ï¼Œä¸“ä¸ºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä»£ç ä»“åº“ä¸­çš„å®‰å…¨æ€§è®¾è®¡ï¼Œé€šè¿‡æ”¶é›†å¸¦æœ‰CVEçš„å¼€æºé¡¹ç›®ï¼Œé‡‡ç”¨è¯­ä¹‰å’Œç»“æ„å˜æ¢ç¡®ä¿æ•°æ®å¤šæ ·æ€§ï¼Œç»“åˆä¸“å®¶åˆ¶å®šçš„è§„åˆ™ä¸Dockerå®¹å™¨å®ç°å¯å¤ç°çš„å®‰å…¨ã€è´¨é‡å’Œç¨³å®šæ€§è¯„ä¼°ã€‚å®éªŒè¯æ˜ï¼ŒClaude-3.7-Sonnetæ•´ä½“è¡¨ç°æœ€ä½³ï¼Œå¼€æºæ¨¡å‹ä¸é—­æºæ¨¡å‹å®‰å…¨å·®è·ç¼©å°ï¼Œä¸”â€œå¿«æ€è€ƒâ€ç­–ç•¥ä¼˜äºâ€œæ…¢æ€è€ƒâ€åœ¨ä»“åº“çº§å®‰å…¨ä¿®å¤ä¸­ã€‚ç»“è®ºè¡¨æ˜A.S.EåŸºå‡†èƒ½æœ‰æ•ˆæ¨åŠ¨LLMå®‰å…¨ä»£ç ç”ŸæˆæŠ€æœ¯è¿›æ­¥å’Œè¯„ä¼°ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A.S.E_A_Repository-Level_Benchmark_for_Evaluating_Security_in_AI-Generated_Code.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18098v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      189. [Detecting and Characterizing Planning in Language Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Detecting_and_Characterizing_Planning_in_Language_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Massachusetts Amherst</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€å¥—åŸºäºå› æœå’Œå½¢å¼åŒ–æ ‡å‡†ï¼ˆæœªæ¥ä»¤ç‰Œç¼–ç Future-Token Encodingå’Œå‰é©±å½±å“Precursor Influenceï¼‰çš„åŠè‡ªåŠ¨ç®¡é“ï¼Œç”¨äºæ£€æµ‹å’Œåˆ†æå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆä»»åŠ¡ä¸­æ˜¯å¦è¿›è¡Œè®¡åˆ’ï¼ˆplanningï¼‰è€Œéå³å…´ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒGemma-2-2Båœ¨ä¸åŒä»»åŠ¡ä¸­ä¼šåœ¨è§„åˆ’å’Œå³å…´ä¹‹é—´åˆ‡æ¢ï¼Œä¸”æŒ‡ä»¤å¾®è°ƒèƒ½æå‡æ¨¡å‹é€‰æ‹©æ­£ç¡®è®¡åˆ’çš„èƒ½åŠ›ï¼Œä½†å¹¶éäº§ç”Ÿè§„åˆ’è¡Œä¸ºçš„æ ¹æœ¬æ¥æºã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Detecting_and_Characterizing_Planning_in_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18091v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      190. [Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Teaching_LLMs_to_Think_Mathematically__A_Critical_Study_of_Decision-Making_via_Optimization.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Princess Sumaya University for Technology</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿå›é¡¾å’Œå…ƒåˆ†æäº†å½“å‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦å»ºæ¨¡ä¸ä¼˜åŒ–å†³ç­–ä¸­çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å­¦ä¹ æ–¹æ³•ã€æ•°æ®é›†è®¾è®¡ã€è¯„ä¼°æŒ‡æ ‡åŠæç¤ºç­–ç•¥ï¼Œå¹¶é€šè¿‡è®¾è®¡é’ˆå¯¹è®¡ç®—æœºç½‘ç»œé¢†åŸŸçš„å®éªŒï¼Œæ¯”è¾ƒäº†å¤šç§ä¸»æµLLMåœ¨è‡ªåŠ¨ç”Ÿæˆä¼˜åŒ–æ¨¡å‹æ—¶çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜LLMåœ¨è§£æè‡ªç„¶è¯­è¨€å’Œç”Ÿæˆç¬¦å·åŒ–ä¼˜åŒ–å…¬å¼æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨å‡†ç¡®æ€§ã€å¯æ‰©å±•æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæœªæ¥éœ€é€šè¿‡ç»“æ„åŒ–æ•°æ®é›†ã€é¢†åŸŸå¾®è°ƒã€å¤šä»£ç†æ¨¡å—åŒ–ã€ç¥ç»ç¬¦å·æ··åˆç­‰æ–¹æ³•æå‡å…¶æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Teaching_LLMs_to_Think_Mathematically_A_Critical_Study_of_Decision-Making_via_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18090v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      191. [Named Entity Recognition of Historical Text via Large Language Model]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Named_Entity_Recognition_of_Historical_Text_via_Large_Language_Model.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Bologna</span></p>
<p>æœ¬è®ºæ–‡æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºç­–ç•¥ï¼Œé’ˆå¯¹å¤šè¯­è¨€å†å²æ–‡æœ¬çš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ï¼Œè®¾è®¡äº†åŒ…æ‹¬éšæœºã€è¯æ±‡é‡å å’ŒåµŒå…¥ç›¸ä¼¼åº¦çš„ç¤ºä¾‹æ£€ç´¢æ–¹æ³•ï¼Œå¹¶åœ¨HIPE-2022å¤šè¯­è¨€å†å²æ•°æ®é›†ä¸Šè¿›è¡Œç³»ç»Ÿå®éªŒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°‘æ ·æœ¬æç¤ºï¼ˆå°¤å…¶æ˜¯å•ç¤ºä¾‹ï¼‰æ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬ï¼Œä¸”ç®€å•çš„ç¤ºä¾‹é€‰æ‹©ç­–ç•¥å·²èƒ½å¸¦æ¥æå‡ï¼Œä½†æ•´ä½“æ€§èƒ½ä»ä½äºæœ‰ç›‘ç£SOTAæ¨¡å‹ï¼Œæç¤ºLLMæç¤ºåœ¨ä½èµ„æºå†å²è¯­æ–™ä¸Šå…·å¤‡å¯è¡Œæ€§å’Œå®ç”¨æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Named_Entity_Recognition_of_Historical_Text_via_Large_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18048v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      192. [HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HyST__LLM-Powered_Hybrid_Retrieval_over_Semi-Structured_Tabular_Data.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">PrompTart LAB, MODULABS</span></p>
<p>è¯¥è®ºæ–‡æå‡ºHySTæ¡†æ¶ï¼Œé€šè¿‡LLMè§£æè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œå°†ç»“æ„åŒ–çº¦æŸè½¬æ¢ä¸ºå…ƒæ•°æ®è¿‡æ»¤ï¼Œå¹¶åˆ©ç”¨è¯­ä¹‰åµŒå…¥æœç´¢å¤„ç†éç»“æ„åŒ–éƒ¨åˆ†ï¼Œå®ç°å¯¹åŠç»“æ„åŒ–è¡¨æ ¼æ•°æ®çš„é«˜ç²¾åº¦æ£€ç´¢ã€‚åœ¨STaRK AmazonåŸºå‡†ä¸‹ï¼ŒHySTåœ¨æ‰€æœ‰æ£€ç´¢æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œè¯æ˜ç»“æ„åŒ–è¿‡æ»¤ä¸è¯­ä¹‰æ£€ç´¢ç»“åˆèƒ½æœ‰æ•ˆæå‡æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/HyST_LLM-Powered_Hybrid_Retrieval_over_Semi-Structured_Tabular_Data.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17959v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      193. [Language Models Coupled with Metacognition Can Outperform Reasoning Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Language_Models_Coupled_with_Metacognition_Can_Outperform_Reasoning_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">IBM Research</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„SOFAI-LMæ¶æ„ï¼Œå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤§æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰é€šè¿‡æ— è®­ç»ƒçš„å…ƒè®¤çŸ¥åé¦ˆæ¨¡å—è€¦åˆï¼Œå®ç°äº†LLMçš„è‡ªæˆ‘çº é”™å’Œé€æ­¥æ”¹è¿›ï¼Œå¿…è¦æ—¶å†è°ƒç”¨LRMã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å›¾ç€è‰²å’Œä»£ç è°ƒè¯•ä¸¤ä¸ªé¢†åŸŸï¼ŒSOFAI-LMä¸ä»…åœ¨å‡†ç¡®ç‡ä¸Šå¯ä¸ç‹¬ç«‹çš„LRMåª²ç¾ç”šè‡³è¶…è¶Šï¼Œè€Œä¸”å¤§å¹…é™ä½æ¨ç†æ—¶é—´ï¼Œæå‡äº†æ•´ä½“è§£é¢˜æ•ˆç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Language_Models_Coupled_with_Metacognition_Can_Outperform_Reasoning_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17953v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      194. [Understanding Subword Compositionality of Large Language Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Understanding_Subword_Compositionality_of_Large_Language_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Copenhagen</span></p>
<p>æœ¬è®ºæ–‡ç³»ç»Ÿæ€§æ¢ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å­è¯ç»„åˆçš„æœºåˆ¶ï¼Œä»ç»“æ„ç›¸ä¼¼æ€§ã€è¯­ä¹‰å¯åˆ†è§£æ€§å’Œå½¢å¼ä¿æŒæ€§ä¸‰æ–¹é¢ï¼ŒåŸºäºå…­ä¸ªä¸»æµLLMçš„å¤šç»„å®éªŒï¼Œé‡‡ç”¨Procrustesåˆ†æä¸æ¢é’ˆä»»åŠ¡ï¼Œæ­ç¤ºä¸åŒæ¨¡å‹åœ¨å­è¯ç»„åˆç­–ç•¥ä¸Šå­˜åœ¨ä¸‰ç±»æˆªç„¶ä¸åŒçš„ç»“æ„åŠ¨æ€ã€‚ç»“è®ºè¡¨æ˜ï¼šå¤§éƒ¨åˆ†LLMçš„å­è¯ç»„åˆç»“æ„ä¸ç®€å•åŠ æ³•é«˜åº¦åŒæ„ï¼Œè¯­ä¹‰ä¿¡æ¯è·¨å±‚æ¬¡ä¿å­˜è‰¯å¥½ï¼Œä½†å½¢å¼ä¿¡æ¯çš„ä¿æŒæ¨¡å¼å› æ¨¡å‹è€Œå¼‚ï¼Œä¸”å·®å¼‚ä¸»è¦æºè‡ªé¢„è®­ç»ƒé˜¶æ®µã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Understanding_Subword_Compositionality_of_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17948v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      195. [Debiasing Multilingual LLMs in Cross-lingual Latent Space]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Debiasing_Multilingual_LLMs_in_Cross-lingual_Latent_Space.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Copenhagen</span></p>
<p>æœ¬æ–‡æå‡ºåœ¨è·¨è¯­ç§å¯¹é½çš„æ½œç©ºé—´ä¸­å¯¹å¤šè¯­è¨€å¤§æ¨¡å‹è¿›è¡Œå»åå¤„ç†ï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨æ¨¡å‹åŸå§‹è¡¨ç¤ºä¸Šæ“ä½œã€‚é€šè¿‡åœ¨TEDæ¼”è®²å¹³è¡Œè¯­æ–™ä¸Šè®­ç»ƒè‡ªç¼–ç å™¨æ„å»ºè·¨è¯­ç§æ½œç©ºé—´ï¼Œå¹¶åœ¨è¯¥ç©ºé—´åº”ç”¨SentDebiaså’ŒINLPå»åæŠ€æœ¯ï¼Œå®éªŒè¡¨æ˜æ­¤æ–¹æ³•æ˜¾è‘—æå‡äº†å»åæ€§èƒ½å’Œè·¨è¯­ç§è¿ç§»èƒ½åŠ›ï¼Œæœ€é«˜å¯å‡å°‘65%çš„åè§åˆ†æ•°ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Debiasing_Multilingual_LLMs_in_Cross-lingual_Latent_Space.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17926v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      196. [AMELIA: A FAMILY OF MULTI-TASK END-TO-END LANGUAGE MODELS FOR ARGUMENTATION]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AMELIA__A_Family_of_Multi-task_End-to-end_Language_Models_for_Argumentation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Universite Claude Bernard Lyon 1</span></p>
<p>è¯¥è®ºæ–‡æå‡ºAMELIAï¼Œä¸€ç³»åˆ—é¢å‘è®ºè¯æŒ–æ˜çš„å¤šä»»åŠ¡ç«¯åˆ°ç«¯å¤§è¯­è¨€æ¨¡å‹ã€‚æ–¹æ³•ä¸Šï¼Œä½œè€…é¦–å…ˆå°†19ä¸ªä¸»æµè®ºè¯æŒ–æ˜æ•°æ®é›†æ ‡å‡†åŒ–æ•´åˆï¼Œè¦†ç›–å…«ç±»è®ºè¯ä»»åŠ¡ï¼Œå¹¶åŸºäºMeta Llama-3.1-8B-Instructæ¨¡å‹ï¼Œç³»ç»Ÿè¯„ä¼°äº†å•ä»»åŠ¡å¾®è°ƒã€å¤šä»»åŠ¡å¾®è°ƒå’Œæ¨¡å‹åˆå¹¶ï¼ˆå¦‚DELLAæ–¹æ³•ï¼‰ä¸‰ç§è®­ç»ƒç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå•ä»»åŠ¡å¾®è°ƒå¯æ˜¾è‘—æå‡å„ä»»åŠ¡è¡¨ç°ï¼Œå¤šä»»åŠ¡å¾®è°ƒå¯å®ç°è·¨ä»»åŠ¡è¿ç§»ä¸”æ— æ€§èƒ½æŸå¤±ï¼Œæ¨¡å‹åˆå¹¶åˆ™åœ¨èŠ‚çº¦ç®—åŠ›çš„åŒæ—¶å…¼é¡¾å¤šä»»åŠ¡èƒ½åŠ›ã€‚ç»“è®ºï¼šå¤šä»»åŠ¡å¾®è°ƒåœ¨å¤šé¡¹è®ºè¯ä»»åŠ¡ä¸Šåˆ›é€ äº†æ–°çš„æ€§èƒ½æ ‡æ†ï¼Œæ¨¡å‹åˆå¹¶ä¸ºå®é™…éƒ¨ç½²æä¾›äº†é«˜æ•ˆå¯æ‰©å±•çš„æ–¹æ¡ˆã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/AMELIA_A_FAMILY_OF_MULTI-TASK_END-TO-END_LANGUAGE_MODELS_FOR_ARGUMENTATION.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17901v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      197. [Riemannian Optimization for LoRA on the Stiefel Manifold]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Riemannian_Optimization_for_LoRA_on_the_Stiefel_Manifold.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Opt-AI Inc.</span></p>
<p>è¯¥è®ºæ–‡æå‡ºStiefel-LoRAæ–¹æ³•ï¼Œå°†LoRAä¸­BçŸ©é˜µçš„ä¼˜åŒ–ä»ä¼ ç»Ÿçš„Euclideanç©ºé—´è¿ç§»åˆ°Stiefelæµå½¢ï¼Œé€šè¿‡å¼ºåˆ¶æ­£äº¤æ€§çº¦æŸå®ç°å‚æ•°é«˜æ•ˆä¸è¡¨è¾¾èƒ½åŠ›æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å‡ ä½•ä¼˜åŒ–æ–¹æ³•åœ¨å¤šç§LLMè§„æ¨¡å’ŒNLPä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºAdamWï¼Œå°¤å…¶æå‡äº†è¡¨ç°å’Œæ”¶æ•›é€Ÿåº¦ï¼Œæ­ç¤ºäº†å‡ ä½•çº¦æŸå¯¹PEFTè®¾è®¡çš„é‡è¦æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Riemannian_Optimization_for_LoRA_on_the_Stiefel_Manifold.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17890v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      198. [UniAPO: Unified Multimodal Automated Prompt Optimization]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/UniAPO__Unified_Multimodal_Automated_Prompt_Optimization.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">ByteDance</span></p>
<p>æœ¬æ–‡æå‡ºäº†UniAPOâ€”â€”é¦–ä¸ªé¢å‘å¤šæ¨¡æ€è‡ªåŠ¨åŒ–æç¤ºä¼˜åŒ–çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé‡‡ç”¨EMç®—æ³•æ€æƒ³å°†åé¦ˆå»ºæ¨¡å’Œæç¤ºä¼˜åŒ–è§£è€¦ï¼Œå¹¶è®¾è®¡äº†é•¿çŸ­æœŸè®°å¿†æœºåˆ¶ï¼Œæœ‰æ•ˆç¼“è§£è§†è§‰tokenè†¨èƒ€å’Œç¼ºä¹è¿‡ç¨‹çº§ç›‘ç£ç­‰å¤šæ¨¡æ€è‡ªåŠ¨æç¤ºä¼˜åŒ–éš¾é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒUniAPOåœ¨æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ä¸Šå‡å¤§å¹…ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·å¤‡å¼ºæ³›åŒ–ä¸é«˜æ•ˆç‡ï¼Œæ¨åŠ¨å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æç¤ºä¼˜åŒ–å‘æ›´ç¨³å¥ã€å¯æ‰©å±•æ–¹å‘å‘å±•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/UniAPO_Unified_Multimodal_Automated_Prompt_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17857v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      199. [VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/VISA__Group-wise_Visual_Token_Selection_and_Aggregation_via_Graph_Summarization_for_Efficient_MLLMs_.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Xiamen University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•VISAï¼Œç”¨äºé«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¨ç†ä¸­çš„è§†è§‰tokenå‹ç¼©ã€‚æ–¹æ³•åŒ…æ‹¬åŸºäºå›¾çš„è§†è§‰tokenèšåˆï¼ˆVTAï¼‰æ¨¡å—ï¼Œé€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§æ„å»ºtokenå›¾å¹¶å°†è¢«ç§»é™¤tokençš„ä¿¡æ¯æ±‡èšåˆ°ä¿ç•™tokenï¼Œä»¥åŠåˆ†ç»„çš„tokené€‰æ‹©ç­–ç•¥ï¼ˆGTSï¼‰ï¼Œåˆ©ç”¨æ–‡æœ¬tokenæ³¨æ„åŠ›å¼•å¯¼åˆ†ç»„å†…çš„è§†è§‰tokenä¿ç•™ä¸ç§»é™¤ï¼Œå®ç°é€æ­¥ç¨³å®šä¿¡æ¯å‹ç¼©ã€‚å®éªŒè¡¨æ˜ï¼ŒVISAåœ¨LLaVA-1.5ã€LLaVA-NeXTå’ŒVideo-LLaVAç­‰æ¨¡å‹å’Œå¤šé¡¹åŸºå‡†æµ‹è¯•ä¸Šï¼Œåœ¨åŠ é€Ÿæ¨ç†çš„åŒæ—¶ä¿æŒæˆ–æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå‹ç¼©ç‡é«˜æ—¶ä¾ç„¶ä¼˜äºåŒç±»æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/VISA_Group-wise_Visual_Token_Selection_and_Aggregation_via_Graph_Summarization_for_Efficient_MLLMs_Inference.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17850v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      200. [Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMs]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Group_Expectation_Policy_Optimization_for_Stable_Heterogeneous_Reinforcement_Learning_in_LLMs.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Peng Cheng Laboratory</span></p>
<p>æœ¬æ–‡æå‡ºäº†HeteroRLæ¡†æ¶ä¸GEPOç®—æ³•ï¼Œä¸“ä¸ºå¼‚æ„åˆ†å¸ƒå¼ç¯å¢ƒä¸‹å¤§è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†è®­ç»ƒè®¾è®¡ã€‚HeteroRLå®ç°äº†é‡‡æ ·å’Œå‚æ•°å­¦ä¹ çš„å¼‚æ­¥è§£è€¦ï¼ŒGEPOé€šè¿‡å¼•å…¥æ ·æœ¬çº§é‡è¦æ€§æƒé‡åŠç»„æœŸæœ›å¹³æ»‘æœºåˆ¶ï¼Œæœ‰æ•ˆå‡å°‘é«˜å»¶è¿Ÿä¸‹çš„æ¢¯åº¦æ–¹å·®ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œåœ¨æç«¯ç½‘ç»œå»¶è¿Ÿä¸‹GEPOæ€§èƒ½ä¸‹é™ä¸è¶…è¿‡3%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Group_Expectation_Policy_Optimization_for_Stable_Heterogeneous_Reinforcement_Learning_in_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17825v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      201. [FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Games]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FAIRGAMER__Evaluating_Biases_in_the_Application_of_Large_Language_Models_to_Video_Games.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Cyber Security, University of Chinese Academy of Sciences</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†FAIRGAMERåŸºå‡†ï¼Œä¸“ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§†é¢‘æ¸¸æˆåœºæ™¯ä¸­çš„ç¤¾ä¼šä¸æ–‡åŒ–åè§ï¼Œæ¶µç›–NPCå†³ç­–ã€å¯¹æŠ—ç©å®¶å’Œåœºæ™¯ç”Ÿæˆä¸‰å¤§ä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†DlstdæŒ‡æ ‡æ¥é‡åŒ–æ¨¡å‹åè§å¯¹æ¸¸æˆå¹³è¡¡çš„å½±å“ã€‚å®éªŒæ˜¾ç¤ºï¼Œå½“å‰ä¸»æµLLMå‡å­˜åœ¨æ˜æ˜¾åè§ï¼Œéƒ¨åˆ†å¯¹æ¸¸æˆå…¬å¹³æ€§å’Œç©å®¶ä½“éªŒé€ æˆä¸¥é‡å½±å“ï¼Œå°¤å…¶Grok-3åè§æœ€æ˜¾è‘—ï¼Œåªæœ‰æå°‘æ¨¡å‹åœ¨ä¸ªåˆ«ä»»åŠ¡ä¸Šè¡¨ç°æ— æ˜æ˜¾åè§ï¼›ç»“è®ºæŒ‡å‡ºLLMå›ºæœ‰åè§ä¼šæ˜¾è‘—ç ´åæ¸¸æˆå¹³è¡¡ï¼Œæœªæ¥éœ€å¤šè§’åº¦æ¶ˆé™¤åè§ä»¥æå‡å¯é æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/FAIRGAMER_Evaluating_Biases_in_the_Application_of_Large_Language_Models_to_Video_Games.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17814v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      202. [SCALABLE ENGINE AND THE PERFORMANCE OF DIFFERENT LLM MODELS IN A SLURM BASED HPC ARCHITECTURE]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scalable_Engine_and_the_Performance_of_Different_LLM_Models_in_a_SLURM_based_HPC_architecture.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">AImotion Bavaria</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºSLURMçš„é«˜æ€§èƒ½è®¡ç®—ï¼ˆHPCï¼‰æ¶æ„ï¼Œç”¨äºåœ¨å¤šèŠ‚ç‚¹é›†ç¾¤ä¸­é«˜æ•ˆéƒ¨ç½²å’Œæ¨ç†å¤šç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬åŠ¨æ€èµ„æºè°ƒåº¦ã€å®¹å™¨åŒ–å¾®æœåŠ¡é›†æˆã€REST APIæ¥å£ã€å¼‚æ­¥æ‰¹é‡æ¨ç†ä¸å¤šæ­¥â€œè£åˆ¤å›¢â€è¾“å‡ºä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨å°æ¨¡å‹ä¸‹å¯æ”¯æŒé«˜å¹¶å‘ä¸”ä½å»¶è¿Ÿæ¨ç†ï¼Œè€Œå¤§æ¨¡å‹åœ¨é«˜è´Ÿè½½ä¸‹æ˜“é¥±å’Œï¼Œä½†æ•´ä½“æ¶æ„å…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€é²æ£’æ€§å’Œçµæ´»æ€§ï¼Œé€‚åˆå¤§è§„æ¨¡LLMæ¨ç†ä»»åŠ¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SCALABLE_ENGINE_AND_THE_PERFORMANCE_OF_DIFFERENT_LLM_MODELS_IN_A_SLURM_BASED_HPC_ARCHITECTURE.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17784v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      203. [Proximal Supervised Fine-Tuning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Proximal_Supervised_Fine-Tuning.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai Jiao Tong University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†Proximal Supervised Fine-Tuning (PSFT)ï¼Œå—TRPOå’ŒPPOç­‰å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯å‘ï¼Œå°†ä¿¡ä»»åŸŸçº¦æŸå¼•å…¥å¤§æ¨¡å‹ç›‘ç£å¾®è°ƒè¿‡ç¨‹ï¼Œé€šè¿‡clipped surrogate objectiveæ–¹å¼é™åˆ¶ç­–ç•¥æ¼‚ç§»ï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œé¿å…ç†µå¡Œç¼©ã€‚å®éªŒè¡¨æ˜ï¼ŒPSFTåœ¨æ•°å­¦å’Œäººç±»ä»·å€¼å¯¹é½ç­‰å¤šç§ä»»åŠ¡ä¸­ï¼Œåœ¨åŸŸå†…å…·å¤‡ä¸ä¼ ç»ŸSFTç›¸å½“æ€§èƒ½ï¼ŒåŸŸå¤–æ³›åŒ–æ›´ä¼˜ï¼Œå¹¶ä¸ºåç»­RLä¼˜åŒ–æ‰“ä¸‹æ›´åšå®åŸºç¡€ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Proximal_Supervised_Fine-Tuning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17767v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      204. [ISACL: Internal State Analyzer for Copyrighted Training Data Leakage]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ISACL__Internal_State_Analyzer_for_Copyrighted_Training_Data_Leakage.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">City University of Hong Kong</span></p>
<p>æœ¬æ–‡æå‡ºäº†ISACLæ¡†æ¶ï¼Œé€šè¿‡åˆ†æå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆæ–‡æœ¬å‰çš„å†…éƒ¨çŠ¶æ€ï¼Œé¢„æµ‹å…¶æ˜¯å¦å­˜åœ¨è®­ç»ƒæ•°æ®æ³„æ¼é£é™©ã€‚è¯¥æ–¹æ³•ç»“åˆç¥ç»ç½‘ç»œåˆ†ç±»å™¨ä¸RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿï¼Œåˆ©ç”¨FAISSå’ŒSQLiteé«˜æ•ˆæ£€ç´¢å’Œæ¯”å¯¹å—ç‰ˆæƒä¿æŠ¤çš„å‚è€ƒææ–™ï¼Œå®ç°å®æ—¶ã€å¯æ‰©å±•çš„é£é™©è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒISACLåœ¨å¤šç§LLMï¼ˆå¦‚Llamaå’ŒMistralï¼‰åŠä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹å‡èƒ½é«˜æ•ˆå‡†ç¡®åœ°æ£€æµ‹æ³„æ¼é£é™©ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºäºè¾“å‡ºå†…å®¹çš„æ£€æµ‹æ–¹æ³•ï¼Œæå‡äº†åˆè§„æ€§ä¸æ£€æµ‹æ•ˆç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ISACL_Internal_State_Analyzer_for_Copyrighted_Training_Data_Leakage.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17744v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      205. [Randomly Removing 50% of Dimensions in Text Embeddings has Minimal Impact on Retrieval and Classification Tasks]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Randomly_Removing_50%_of_Dimensions_in_Text_Embeddings_has_Minimal_Impact_on_Retrieval_and_Classific.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Mannheim</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿæ€§åœ°åˆ†æäº†åœ¨6ç§ä¸»æµæ–‡æœ¬ç¼–ç å™¨å’Œ26ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œéšæœºç§»é™¤æ–‡æœ¬åµŒå…¥çš„50%ç»´åº¦å¯¹æ£€ç´¢å’Œåˆ†ç±»ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚é€šè¿‡å¯¹åµŒå…¥ç©ºé—´çš„å‡ ä½•å±æ€§ï¼ˆå¦‚å„å‘å¼‚æ€§ã€ç»´åº¦å†—ä½™ã€å¼‚å¸¸ç»´åº¦ï¼‰ä¸ä¸‹æ¸¸è¡¨ç°çš„å…³è”åˆ†æï¼Œå‘ç°è®¸å¤šåµŒå…¥ç»´åº¦å¯¹æ€§èƒ½æœ‰è´Ÿé¢å½±å“ï¼Œéšæœºç§»é™¤ç»´åº¦ä»…é€ æˆæå°æ€§èƒ½ä¸‹é™ï¼Œä¸”æŸäº›æƒ…å†µæ€§èƒ½è¿˜æå‡ã€‚ç»“è®ºæ˜¯ï¼Œå½“å‰ä¸»æµæ–‡æœ¬åµŒå…¥å™¨å¯¹è¡¨ç¤ºç©ºé—´åˆ©ç”¨ä½æ•ˆï¼Œå­˜åœ¨å¤§é‡â€œé™çº§ç»´åº¦â€ï¼Œæœªæ¥å¯é€šè¿‡ä¼˜åŒ–æ¨¡å‹ç»“æ„æˆ–ç›®æ ‡å‡½æ•°è¿›ä¸€æ­¥æå‡åµŒå…¥è´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Randomly_Removing_50%25_of_Dimensions_in_Text_Embeddings_has_Minimal_Impact_on_Retrieval_and_Classification_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17739v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      206. [Speculative Safety-Aware Decoding]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Speculative_Safety-Aware_Decoding.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">School of Computer Science &amp; Technology, Beijing Institute of Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§è§£ç æ—¶é˜²å¾¡æ–¹æ³•â€”â€”Speculative Safety-Aware Decoding (SSD)ï¼Œç»“åˆäº†å°å‹å…·å®‰å…¨å±æ€§è¯­è¨€æ¨¡å‹çš„æ¨æµ‹é‡‡æ ·ä¸å¤§æ¨¡å‹åŠ¨æ€åˆ‡æ¢è§£ç ç­–ç•¥ï¼Œé€šè¿‡åŒ¹é…ç‡é‡åŒ–è¶Šç‹±é£é™©å¹¶åœ¨å®ç”¨æ€§å’Œå®‰å…¨æ€§é—´è‡ªé€‚åº”åˆ‡æ¢è§£ç åˆ†å¸ƒã€‚å®éªŒè¡¨æ˜ï¼ŒSSDå¯æœ‰æ•ˆå¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ·±åº¦å®‰å…¨å¯¹æŠ—èƒ½åŠ›ï¼Œä¿æŒæœ‰ç”¨æ€§åŒæ—¶æå‡æ¨ç†æ•ˆç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Speculative_Safety-Aware_Decoding.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17693v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      207. [Database Normalization via Dual-LLM Self-Refinement]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Database_Normalization_via_Dual-LLM_Self-Refinement.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Sungshin Womenâ€™s University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†Miffieæ¡†æ¶ï¼Œåˆ©ç”¨åŒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªæˆ‘ä¼˜åŒ–æ¶æ„ï¼Œå®ç°æ•°æ®åº“è§„èŒƒåŒ–è‡ªåŠ¨åŒ–ã€‚å…¶æ–¹æ³•é‡‡ç”¨GPT-4è¿›è¡Œè§„èŒƒåŒ–ç”Ÿæˆï¼Œo1-miniè¿›è¡ŒéªŒè¯ï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„é›¶æ ·æœ¬æç¤ºå’Œåé¦ˆè¿­ä»£æå‡å‡†ç¡®ç‡ä¸æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒMiffieèƒ½å¤Ÿé«˜æ•ˆå‡†ç¡®åœ°è§„èŒƒåŒ–å¤æ‚æ•°æ®åº“æ¨¡å¼ï¼Œæ˜¾è‘—å‡å°‘äººå·¥æ“ä½œã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Database_Normalization_via_Dual-LLM_Self-Refinement.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17692v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      208. [LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LLM-based_Agentic_Reasoning_Frameworks__A_Survey_from_Methods_to_Scenarios.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Beijing Jiaotong University</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿæ¢³ç†äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„Agenticæ¨ç†æ¡†æ¶ï¼Œæå‡ºäº†ç»Ÿä¸€çš„ä¸‰å±‚åˆ†ç±»ä½“ç³»ï¼šå•ä½“æ™ºèƒ½ä½“æ–¹æ³•ã€å·¥å…·é›†æˆæ–¹æ³•ä¸å¤šæ™ºèƒ½ä½“æ–¹æ³•ï¼Œå¹¶ä»¥å½¢å¼åŒ–è¯­è¨€æè¿°å„ç±»æ¨ç†æµç¨‹ã€‚é€šè¿‡å¯¹ç§‘å­¦å‘ç°ã€åŒ»ç–—ã€è½¯ä»¶å·¥ç¨‹ã€ç¤¾ä¼šç»æµä»¿çœŸç­‰é¢†åŸŸçš„åº”ç”¨æ¢³ç†ï¼Œæ­ç¤ºäº†ä¸åŒAgenticæ¡†æ¶çš„æ ¸å¿ƒæœºåˆ¶ã€é€‚ç”¨åœºæ™¯åŠè¯„ä¼°å®è·µã€‚ç»“è®ºè®¤ä¸ºè¯¥ä½“ç³»æœ‰åŠ©äºç†æ¸…LLM-Agentçš„å‘å±•è„‰ç»œï¼Œä¿ƒè¿›æ ‡å‡†åŒ–å’Œå®‰å…¨æ¼”è¿›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LLM-based_Agentic_Reasoning_Frameworks_A_Survey_from_Methods_to_Scenarios.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17681v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      209. [Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discovery]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Unlearning_as_Ablation__Toward_a_Falsifiable_Benchmark_for_Generative_Scientific_Discovery.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Independent Researcher</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§â€œunlearning-as-ablationâ€æ–¹æ³•ï¼Œé€šè¿‡ç³»ç»Ÿæ€§ç§»é™¤å¤§è¯­è¨€æ¨¡å‹å…³äºæŸä¸ªç§‘å­¦ç»“æœåŠå…¶æ‰€æœ‰ç›¸å…³æ¨ç†é“¾ï¼ˆforget-closureï¼‰çš„çŸ¥è¯†ï¼Œå†æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½ä»…å‡­å‰©ä½™å…¬ç†å’Œå·¥å…·é‡æ–°æ¨å¯¼è¯¥ç»“æœã€‚æ–¹æ³•æ˜ç¡®å°†unlearningç”¨ä½œæ£€éªŒæ¨¡å‹æ˜¯å¦å…·å¤‡çœŸå®ç”Ÿæˆæ€§èƒ½åŠ›è€Œéä»…ä»…è®°å¿†ï¼Œå¹¶ä»¥æ•°å­¦å’Œç®—æ³•é¢†åŸŸä¸ºè¯•ç‚¹ã€‚ç»“è®ºè®¤ä¸ºè¯¥æ–¹æ³•å¯ä¸ºAIç§‘å­¦å‘ç°èƒ½åŠ›æä¾›å¯è¯ä¼ªçš„å®éªŒæ ‡å‡†ï¼Œå¹¶æœ‰åŠ©äºå˜æ¸…AIåœ¨ç§‘å­¦å‘ç°ä¸­çš„çœŸå®è¾¹ç•Œã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Unlearning_as_Ablation_Toward_a_Falsifiable_Benchmark_for_Generative_Scientific_Discovery.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17675v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      210. [Towards Synthesizing Normative Data for Cognitive Assessments Using Generative Multimodal Large Language Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Towards_Synthesizing_Normative_Data_for_Cognitive_Assessments_Using_Generative_Multimodal_Large_Lang.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The Westminster Schools</span></p>
<p>è¯¥è®ºæ–‡æå‡ºåˆ©ç”¨ç”Ÿæˆå¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oåŠå…¶miniç‰ˆæœ¬ï¼‰ï¼Œé€šè¿‡ç»¼åˆå’ŒåŸºç¡€æç¤ºè¯ç­–ç•¥ç”Ÿæˆè®¤çŸ¥è¯„ä¼°ä¸­çš„æ ‡å‡†åŒ–æ–‡æœ¬ååº”ï¼Œå®ç°å¯¹çœŸå®æ‚£è€…è¯­æ–™çš„æ¨¡æ‹Ÿã€‚ç ”ç©¶å‘ç°ï¼Œç»¼åˆæ€§æç¤ºè¯å’Œå¼ºæ¨¡å‹èƒ½ç”Ÿæˆæ›´çœŸå®ä¸”å¯åŒºåˆ†è®¤çŸ¥çŠ¶æ€ï¼ˆæ­£å¸¸ã€è½»åº¦è®¤çŸ¥éšœç¢ã€é˜¿å°”èŒ¨æµ·é»˜ç—‡ï¼‰çš„è¯­æ–™ï¼ŒBERTScoreå’ŒLLM-as-a-Judgeè¯„ä»·æ–¹æ³•æ›´æœ‰æ•ˆã€‚ç»“è®ºè®¤ä¸ºæ­¤æ–¹æ³•å¯ä¸ºè®¤çŸ¥æµ‹è¯•å¿«é€Ÿç”Ÿæˆæ ‡å‡†åŒ–åŸºçº¿æ•°æ®ï¼Œé™ä½ä¼ ç»Ÿæ•°æ®æ”¶é›†æˆæœ¬å¹¶æœ‰åŠ©äºæ–°æµ‹è¯•å¼€å‘ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Towards_Synthesizing_Normative_Data_for_Cognitive_Assessments_Using_Generative_Multimodal_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17638v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      211. [Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dynamic_Embedding_of_Hierarchical_Visual_Features_for_Efficient_Vision-Language_Fine-Tuning.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">National University of Defense Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†DEHVFæ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€åµŒå…¥å’Œèåˆè§†è§‰ç¼–ç å™¨çš„å¤šå±‚æ¬¡ç‰¹å¾ï¼Œå°†å…¶ç›´æ¥æ³¨å…¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å„å±‚çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼Œå®ç°è§†è§‰-è¯­è¨€ä¿¡æ¯åœ¨åŒè¯­ä¹‰ç²’åº¦ä¸Šçš„é«˜æ•ˆå¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒDEHVFåœ¨ScienceQAå’ŒCOCO Captionç­‰ä¸»æµè§†è§‰-è¯­è¨€åŸºå‡†ä¸Šï¼Œæ¯”ç°æœ‰é«˜æ•ˆå¾®è°ƒæ–¹æ³•æœ‰æ›´é«˜å‡†ç¡®ç‡å’Œæ›´ä¼˜è®­ç»ƒæ¨ç†é€Ÿåº¦ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Dynamic_Embedding_of_Hierarchical_Visual_Features_for_Efficient_Vision-Language_Fine-Tuning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17621v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      212. [Steering When Necessary: Flexible Steering Large Language Models with Backtracking]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Steering_When_Necessary__Flexible_Steering_Large_Language_Models_with_Backtracking.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Nanjing University</span></p>
<p>æœ¬æ–‡æå‡ºäº†Flexible Activation Steering with Backtracking (FASB) æ¡†æ¶ï¼Œé€šè¿‡è·Ÿè¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å†…éƒ¨çŠ¶æ€ï¼ŒåŠ¨æ€åˆ¤æ–­æ˜¯å¦éœ€è¦å¹²é¢„åŠå¹²é¢„å¼ºåº¦ï¼Œç»“åˆå›æº¯æœºåˆ¶å¯¹å·²åç¦»ç›®æ ‡è¡Œä¸ºçš„tokenè¿›è¡Œå†ç”Ÿæˆï¼Œå®ç°æ›´ç²¾å‡†çš„æ¨¡å‹è¡Œä¸ºå¼•å¯¼ã€‚å®éªŒè¯æ˜FASBåœ¨TruthfulQAåŠå…­ä¸ªå¤šé¡¹é€‰æ‹©æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„çœŸå®æ€§å’Œå¤šé¡¹é€‰æ‹©å‡†ç¡®ç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Steering_When_Necessary_Flexible_Steering_Large_Language_Models_with_Backtracking.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17608v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      213. [ChartMaster: Advancing Chart-to-Code Generation with Real-World Charts and Chart Similarity Reinforcement Learning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ChartMaster__Advancing_Chart-to-Code_Generation_with_Real-World_Charts_and_Chart_Similarity_Reinforc.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">South China University of Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºChartMasteræ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ReChartPromptè‡ªåŠ¨åŒ–ç®¡é“ä»arXivè®ºæ–‡æ”¶é›†çœŸå®ä¸–ç•Œå›¾è¡¨ï¼Œæ„å»ºé«˜å¤šæ ·æ€§ReChartPrompt-240Kæ•°æ®é›†ï¼Œå¹¶è®¾è®¡ChartSimRLå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç»“åˆè§†è§‰å’Œå±æ€§ç›¸ä¼¼æ€§å¥–åŠ±æå‡å›¾è¡¨ä¸ä»£ç çš„å¯¹é½æ•ˆæœã€‚å®éªŒè¡¨æ˜ï¼ŒChartMasteråœ¨å¤šé¡¹chart-to-codeç”ŸæˆåŸºå‡†ä¸Šæ¥è¿‘GPT-4oæ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰å¼€æº7Bæ¨¡å‹ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ChartMaster_Advancing_Chart-to-Code_Generation_with_Real-World_Charts_and_Chart_Similarity_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17586v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      214. [Exploring Efficient Learning of Small BERT Networks with LoRA and DoRA]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Exploring_Efficient_Learning_of_Small_BERT_Networks_with_LoRA_and_DoRA.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Stanford University</span></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹å°å‹BERTæ¨¡å‹ï¼ˆminBERTï¼‰ï¼Œæå‡ºå¹¶å®ç°äº†LoRAå’ŒDoRAä¸¤ç§ä½ç§©é€‚åº”æ–¹æ³•ï¼Œå¹¶ç»“åˆè‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰è®­ç»ƒï¼Œç³»ç»ŸåŸºå‡†æµ‹è¯•å…¶åœ¨å¤šä»»åŠ¡ï¼ˆæƒ…æ„Ÿåˆ†æã€å¤è¿°æ£€æµ‹ã€è¯­ä¹‰ç›¸ä¼¼æ€§ï¼‰åœºæ™¯ä¸‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRAå’ŒDoRAåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½äº†æ˜¾å­˜å’Œè®¡ç®—æ¶ˆè€—ï¼Œä¸”å¯¹äºå°æ¨¡å‹ï¼Œä½ç§©ï¼ˆrank&#x3D;1ï¼‰åˆ†è§£å³å¯è¾¾åˆ°æœ€ä½³æ€§ä»·æ¯”ï¼ŒAMPå’ŒLoRAç»“åˆæ•ˆæœæœ€ä¼˜ã€‚ç»“è®ºæŒ‡å‡ºï¼Œä½ç§©é€‚åº”æŠ€æœ¯ä¸ä»…é€‚ç”¨äºå¤§æ¨¡å‹ï¼Œå¯¹å°å‹è¯­è¨€æ¨¡å‹åŒæ ·æœ‰æ•ˆï¼Œå¹¶ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„æ¨¡å‹å¾®è°ƒæä¾›äº†å¯è¡Œé€”å¾„ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Exploring_Efficient_Learning_of_Small_BERT_Networks_with_LoRA_and_DoRA.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17565v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      215. [TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TradingGroup__A_Multi-Agent_Trading_System_with_Self-Reflection_and_Data-Synthesis.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The University of New South Wales</span></p>
<p>è¯¥è®ºæ–‡æå‡ºTradingGroupï¼Œä¸€ä¸ªé›†æˆäº†æ–°é—»æƒ…æ„Ÿåˆ†æã€è´¢æŠ¥è§£è¯»ã€è‚¡ç¥¨è¶‹åŠ¿é¢„æµ‹ã€äº¤æ˜“é£æ ¼é€‚é…åŠå†³ç­–çš„å¤šæ™ºèƒ½ä½“é‡‘èäº¤æ˜“ç³»ç»Ÿã€‚æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬å¤šå±‚æ¬¡è‡ªåæœºåˆ¶ã€åŠ¨æ€é£é™©ç®¡ç†æ¨¡å—ï¼Œä»¥åŠè‡ªåŠ¨æ•°æ®åˆæˆä¸æ ‡æ³¨æµç¨‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡åè®­ç»ƒæ•°æ®å¹¶å¾®è°ƒLLMã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTradingGroupåœ¨äº”ä¸ªçœŸå®è‚¡ç¥¨æ•°æ®é›†ä¸Šçš„æ”¶ç›Šå’Œé£é™©è¡¨ç°å‡ä¼˜äºç°æœ‰è§„åˆ™ã€æœºå™¨å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ä»¥åŠLLMäº¤æ˜“ç­–ç•¥ï¼ŒéªŒè¯äº†ç³»ç»ŸåŠåˆæˆæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TradingGroup_A_Multi-Agent_Trading_System_with_Self-Reflection_and_Data-Synthesis.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18265v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      216. [InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/InternVL3.5__Advancing_Open-Source_Multimodal_Models_in_Versatility,_Reasoning,_and_Efficiency.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Shanghai AI Laboratory</span></p>
<p>æœ¬æ–‡æå‡ºäº†InternVL3.5å®¶æ—ï¼Œä¸»æ‰“å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨é€šç”¨èƒ½åŠ›ã€æ¨ç†èƒ½åŠ›å’Œæ¨ç†æ•ˆç‡ä¸Šçš„æå‡ã€‚æ–¹æ³•ä¸Šï¼Œåˆ›æ–°æ€§åœ°å¼•å…¥Cascade Reinforcement Learningï¼ˆçº§è”å¼ºåŒ–å­¦ä¹ ï¼‰ä¸¤é˜¶æ®µæµç¨‹ï¼ˆç¦»çº¿RLé«˜æ•ˆæ”¶æ•›+åœ¨çº¿RLç²¾ç»†å¯¹é½ï¼‰ï¼Œå¹¶æå‡ºVisual Resolution Router (ViR)ç”¨äºåŠ¨æ€è§†è§‰åˆ†è¾¨ç‡é€‰æ‹©ï¼Œä»¥åŠDecoupled Vision-Language Deployment (DvD)å®ç°å¼‚æ„è®¾å¤‡åˆ†å¸ƒå¼æ¨ç†ï¼ŒåŠ é€Ÿå¤§æ¨¡å‹å¤šæ¨¡æ€æ¨ç†ã€‚ç»“è®ºæ˜¾ç¤ºï¼ŒInternVL3.5åœ¨å¤šæ¨¡æ€ç†è§£ã€æ¨ç†ã€æ–‡æœ¬ã€Agentç­‰ä»»åŠ¡ä¸Šå–å¾—é¢†å…ˆå¼€æºæ°´å¹³ï¼Œæ¨ç†èƒ½åŠ›ç›¸æ¯”å‰ä»£æå‡16%ã€æ¨ç†é€Ÿåº¦æå‡4å€ï¼Œæ˜¾è‘—ç¼©å°ä¸å•†ä¸šæ¨¡å‹å¦‚GPT-5çš„å·®è·ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/InternVL3.5_Advancing_Open-Source_Multimodal_Models_in_Versatility_Reasoning_and_Efficiency.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18132v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      217. [Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Test-Time_Scaling_Strategies_for_Generative_Retrieval_in_Multimodal_Conversational_Recommendations.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Academia Sinica</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é›†æˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç”Ÿæˆå¼æ£€ç´¢å™¨ä¸æ¨ç†æ—¶åŠ¨æ€å†æ’åºï¼ˆTest-Time Reranking, TTRï¼‰æœºåˆ¶çš„æ–°æ¡†æ¶ï¼Œç”¨äºå¤šè½®å¯¹è¯ä¸­çš„å¤šæ¨¡æ€å•†å“æ¨èæ£€ç´¢ã€‚æ–¹æ³•æµç¨‹åŒ…æ‹¬ç”¨æˆ·æ„å›¾æ¨æ–­ã€å¤šæ¨¡æ€ç”Ÿæˆå¼æ£€ç´¢ï¼ˆåŸºäºè¯­ä¹‰IDï¼‰ã€ä»¥åŠTTRæœºåˆ¶å¯¹å€™é€‰å•†å“è¿›è¡Œæ¨ç†æ—¶å†æ’åºï¼Œä»¥æ›´å¥½åœ°å¥‘åˆç”¨æˆ·åŠ¨æ€å˜åŒ–çš„æ£€ç´¢æ„å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTTRæœºåˆ¶æ˜¾è‘—æå‡äº†å¤šè½®å¤šæ¨¡æ€æ£€ç´¢çš„å‡†ç¡®ç‡ï¼Œåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå¹³å‡æå‡MRR 14.5åˆ†ã€nDCG@1 10.6åˆ†ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Test-Time_Scaling_Strategies_for_Generative_Retrieval_in_Multimodal_Conversational_Recommendations.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17892v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      218. [ILRE: Intermediate Layer Retrieval for Context Compression in Causal Language Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ILRe__Intermediate_Layer_Retrieval_for_Context_Compression_in_Causal_Language_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">China Merchants Bank</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¤§è¯­è¨€æ¨¡å‹é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æ— è®­ç»ƒä¸Šä¸‹æ–‡å‹ç¼©æ–¹æ³•ILReï¼Œé€šè¿‡æŒ‡å®šç¦»çº¿ä¸­é—´è§£ç å±‚ï¼Œä»…å¯¹è¯¥å±‚è¿›è¡Œæµå¼åˆ†å—é¢„å¡«å……å’ŒæŸ¥è¯¢-ä¸Šä¸‹æ–‡æ³¨æ„åŠ›æ£€ç´¢ï¼Œå¹¶é‡‡ç”¨å¤šæ± åŒ–æ ¸åˆ†é…ç­–ç•¥ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ILReåœ¨é™ä½è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜å ç”¨çš„åŒæ—¶ï¼Œèƒ½ä»¥æ¥è¿‘ç”šè‡³è¶…è¶Šå…¨ä¸Šä¸‹æ–‡çš„å‡†ç¡®ç‡å®Œæˆé•¿æ–‡æœ¬æ¨ç†ä»»åŠ¡ï¼Œå®ç°ç™¾å€æ¨ç†åŠ é€Ÿã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ILRE_Intermediate_Layer_Retrieval_for_Context_Compression_in_Causal_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17860v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      219. [AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AVAM__Universal_Training-free_Adaptive_Visual_Anchoring_Embedded_into_Multimodal_Large_Language_Mode.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Hunan University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è®­ç»ƒæ— å…³çš„è‡ªé€‚åº”è§†è§‰é”šå®šï¼ˆAdaptive Visual Anchoringï¼‰ç­–ç•¥ï¼Œé€šè¿‡åŸºäºè·¨æ¨¡æ€å“åº”å›¾è‡ªé€‚åº”å‹ç¼©è§†è§‰Tokenï¼Œæå–ä¸é—®é¢˜ç›¸å…³çš„å…³é”®è§†è§‰åŒºåŸŸï¼Œæœ‰æ•ˆè¿‡æ»¤å¤šå›¾åƒåœºæ™¯ä¸‹çš„å†—ä½™ä¿¡æ¯ã€‚å¹¶å¼•å…¥ååŒè§£ç æœºåˆ¶ï¼ŒåŠ¨æ€èåˆå…¨å±€ä¸å‹ç¼©è§†è§‰ä¿¡æ¯ï¼Œåœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¤šå›¾åƒé—®ç­”ä»»åŠ¡ä¸­æ˜¾è‘—æå‡å‡†ç¡®ç‡å’Œæ•ˆç‡ï¼Œç»ä¸»æµMLLMså’Œå¤šåŸºå‡†æµ‹è¯•éªŒè¯æ–¹æ³•çš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/AVAM_Universal_Training-free_Adaptive_Visual_Anchoring_Embedded_into_Multimodal_Large_Language_Model_for_Multi-image_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17714v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      220. [F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/F2RVLM__Boosting_Fine-grained_Fragment_Retrieval_for_Multi-Modal_Long-form_Dialogue_with_Vision_Lang.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tencent Inc</span></p>
<p>æœ¬æ–‡æå‡ºäº†F2RVLMæ¨¡å‹ï¼Œé’ˆå¯¹é•¿ç¯‡å¤šæ¨¡æ€å¯¹è¯ä¸­çš„ç»†ç²’åº¦ç‰‡æ®µæ£€ç´¢ä»»åŠ¡ï¼ˆFFRï¼‰ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒï¼šé¦–å…ˆåˆ©ç”¨ç›‘ç£å¾®è°ƒæ³¨å…¥ç‰‡æ®µçº§æ£€ç´¢çŸ¥è¯†ï¼Œéšåé‡‡ç”¨åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ ï¼Œç»“åˆå¤šç›®æ ‡å¥–åŠ±ï¼ˆè¯­ä¹‰ç²¾åº¦ã€ç›¸å…³æ€§å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼‰ï¼Œå¹¶å¼•å…¥éš¾åº¦æ„ŸçŸ¥è¯¾ç¨‹é‡‡æ ·é€æ­¥æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒF2RVLMåœ¨å¤§è§„æ¨¡MLDRåˆæˆæ•°æ®å’ŒçœŸå®å¾®ä¿¡å¯¹è¯æ•°æ®é›†ä¸Šå‡å¤§å¹…ä¼˜äºä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¯¹è¯æ£€ç´¢çš„å‡†ç¡®æ€§å’Œç‰‡æ®µä¸€è‡´æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/F2RVLM_Boosting_Fine-grained_Fragment_Retrieval_for_Multi-Modal_Long-form_Dialogue_with_Vision_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17637v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      221. [Weights-Rotated Preference Optimization for Large Language Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Weights-Rotated_Preference_Optimization_for_Large_Language_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>æœ¬æ–‡é’ˆå¯¹ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½ä¸­çš„â€œå¥–åŠ±æŠ•æœºâ€é—®é¢˜ï¼Œæå‡ºäº†æƒé‡æ—‹è½¬åå¥½ä¼˜åŒ–ï¼ˆRoPOï¼‰ç®—æ³•ã€‚RoPOæ–¹æ³•ç»“åˆäº†è¾“å‡ºå±‚KLæ•£åº¦çš„éšå¼çº¦æŸä¸ä¸­é—´éšè—å±‚çš„å¤šç²’åº¦æ­£äº¤çŸ©é˜µæ˜¾å¼çº¦æŸï¼Œé˜²æ­¢ç¥ç»å…ƒå¡Œé™·å¸¦æ¥çš„è¡¨è¾¾å†—ä½™ï¼Œä»è€Œä¿ç•™é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µä¹ å¾—çš„çŸ¥è¯†ä¸è¡¨è¾¾èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒRoPOåœ¨AlpacaEval 2å’ŒMT-Benchç­‰åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†å¯¹é½æ€§èƒ½ï¼Œå‡å°‘äº†é‡å¤å’ŒçŸ¥è¯†é—å¿˜ï¼Œä¸”åªéœ€0.015%çš„å¯è®­ç»ƒå‚æ•°ï¼Œæœ‰æ•ˆç¼“è§£äº†DPOçš„å¥–åŠ±æŠ•æœºé—®é¢˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Weights-Rotated_Preference_Optimization_for_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17627v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      222. [Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Stop_Spinning_Wheels__Mitigating_LLM_Overthinking_via_Mining_Patterns_for_Early_Reasoning_Exit.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute of Computing Technology, Chinese Academy of Sciences</span></p>
<p>æœ¬æ–‡æå‡ºé€šè¿‡åˆ†æå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†è¿‡ç¨‹ä¸­çš„æ€ç»´é•¿åº¦ä¸å†…å®¹é•¿åº¦å…³ç³»ï¼Œå°†æ¨ç†åˆ’åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œå¹¶é¦–æ¬¡æ˜ç¡®æå‡ºâ€œæ¨ç†å®Œæˆç‚¹â€ï¼ˆRCPï¼‰çš„æ¦‚å¿µã€‚é€šè¿‡CatBoostç‰¹å¾åˆ†æï¼Œä½œè€…æŒ–æ˜å‡ºè¯†åˆ«RCPçš„ç¨³å®šä¿¡å·ï¼Œå¹¶æå‡ºä¸€å¥—é«˜æ•ˆçš„å¯å‘å¼è§„åˆ™ï¼ˆRCPDï¼‰ï¼Œèƒ½å¤Ÿä½å¼€é”€åœ°å‡†ç¡®æ£€æµ‹RCPï¼Œå®ç°æ¨ç†æ—©åœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯åœ¨ä¿æŒç”šè‡³æå‡æ¨ç†å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå¹³å‡å‡å°‘30%ä»¥ä¸Šçš„Tokenæ¶ˆè€—ï¼Œæœ‰æ•ˆç¼“è§£äº†LLMçš„è¿‡åº¦æ€è€ƒé—®é¢˜ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Stop_Spinning_Wheels_Mitigating_LLM_Overthinking_via_Mining_Patterns_for_Early_Reasoning_Exit.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17092v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      223. [Enhancing Knowledge Tracing through Leakage-Free and Recency-Aware Embeddings]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Enhancing_Knowledge_Tracing_through_Leakage-Free_and_Recency-Aware_Embeddings.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Applied Sciences Karlsruhe</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•é«˜æ•ˆçš„çŸ¥è¯†è¿½è¸ªï¼ˆKTï¼‰è¾“å…¥åµŒå…¥æ”¹è¿›æ–¹æ³•ï¼Œé‡‡ç”¨MASKæ ‡ç­¾æœºåˆ¶é˜²æ­¢å¤šçŸ¥è¯†ç‚¹ï¼ˆKCï¼‰é—®é¢˜ä¸­çš„æ ‡ç­¾æ³„æ¼ï¼Œå¹¶é¦–æ¬¡å¼•å…¥åŸºäºå¯å­¦ä¹ å‚…é‡Œå¶ç‰¹å¾çš„Recency Encodingç”¨äºç¼–ç çŸ¥è¯†ç‚¹çš„æœ€è¿‘å‡ºç°è·ç¦»ã€‚è¿™äº›æ–¹æ³•å¯ç›´æ¥é›†æˆåˆ°ä¸»æµKTæ¨¡å‹ï¼ˆå¦‚DKTã€AKTã€SAKTï¼‰ä¸­ï¼Œä¸å¢åŠ æ˜¾è‘—è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†é¢„æµ‹å‡†ç¡®ç‡ã€‚ç»“è®ºæ˜¯ï¼šMASKæ ‡ç­¾å’ŒRecency Encodingèƒ½æœ‰æ•ˆæå‡KTæ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šKCé—®é¢˜æ•°æ®é›†ä¸Šï¼Œä¸”ä¼˜äºç°æœ‰é˜²æ³„æ¼æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Enhancing_Knowledge_Tracing_through_Leakage-Free_and_Recency-Aware_Embeddings.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17086v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      224. [A Decoupled LOB Representation Framework for Multilevel Manipulation Detection with Supervised Contrastive Learning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Decoupled_LOB_Representation_Framework_for_Multilevel_Manipulation_Detection_with_Supervised_Contr.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Southern University of Science and Technology</span></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„åˆ†çº§é™ä»·è®¢å•ç°¿ï¼ˆLOBï¼‰è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç»“åˆçº§è”LOBè¡¨ç¤ºæµæ°´çº¿å’Œæœ‰ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œç”¨äºå¤šå±‚æ¬¡é‡‘èå¸‚åœºæ“çºµï¼ˆå¦‚æ¬ºéª—äº¤æ˜“ï¼‰æ£€æµ‹ã€‚æ–¹æ³•åŒ…æ‹¬ä¸“ç”¨LOBåµŒå…¥æ¨¡å—ã€æ‰‹å·¥ç‰¹å¾èåˆã€æ··åˆæŸå¤±ï¼ˆMSEå’Œæœ‰ç›‘ç£å¯¹æ¯”æŸå¤±ï¼‰åŠå¼‚å¸¸æ ·æœ¬è¿‡é‡‡æ ·ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡å‹ä¸‹çš„æ£€æµ‹æ•ˆæœï¼ŒTransformerç±»æ¨¡å‹è¡¨ç°æœ€ä½³ã€‚ç»“è®ºæ˜¾ç¤ºè¯¥æ¡†æ¶å¯æœ‰æ•ˆæŒ–æ˜å¤æ‚å¤šå±‚æ¬¡å¼‚å¸¸ï¼Œæ¨åŠ¨åºåˆ—æ•°æ®å¼‚å¸¸æ£€æµ‹æ–¹æ³•å‘å±•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Decoupled_LOB_Representation_Framework_for_Multilevel_Manipulation_Detection_with_Supervised_Contrastive_Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17077v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      225. [CP4SBI: Local Conformal Calibration of Credible Sets in Simulation-Based Inference]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CP4SBI__Local_Conformal_Calibration_of_Credible_Sets_in_Simulation-Based_Inference.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Federal University of SÃ£o Carlos</span></p>
<p>æœ¬æ–‡æå‡ºCP4SBIï¼Œä¸€ç§å¯åº”ç”¨äºä»»æ„æ¨¡æ‹Ÿå™¨æ¨æ–­ï¼ˆSBIï¼‰åéªŒä¼°è®¡å™¨çš„æ¨¡å‹æ— å…³å‹å…±å½¢æ ¡å‡†æ¡†æ¶ï¼Œåˆ©ç”¨å›å½’æ ‘åˆ†åŒºï¼ˆLoCartï¼‰å’Œæ¡ä»¶åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰ä¸¤ç§ç­–ç•¥ï¼Œä¸ºå„ç§ä¸ç¡®å®šæ€§é‡åŒ–åŒºåŸŸï¼ˆåŒ…æ‹¬HPDã€å¯¹ç§°åŒºé—´ç­‰ï¼‰æä¾›æœ‰é™æ ·æœ¬çš„å±€éƒ¨ä¸æ¡ä»¶è¦†ç›–ç‡ä¿è¯ã€‚å®éªŒè¡¨æ˜ï¼ŒCP4SBIæ˜¾è‘—æå‡äº†ç¥ç»ç½‘ç»œåéªŒä¼°è®¡åŠæ‰©æ•£æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–è´¨é‡ï¼Œä¿è¯äº†æ›´å¯é çš„ç½®ä¿¡åŒºé—´ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CP4SBI_Local_Conformal_Calibration_of_Credible_Sets_in_Simulation-Based_Inference.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17056v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      226. [TabResFlow: A Normalizing Spline Flow Model for Probabilistic Univariate Tabular Regression]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TabResFlow__A_Normalizing_Spline_Flow_Model_for_Probabilistic_Univariate_Tabular_Regression.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Hildesheim</span></p>
<p>æœ¬æ–‡æå‡ºTabResFlowï¼Œä¸€ç§ç”¨äºæ¦‚ç‡å‹å•å˜é‡è¡¨æ ¼å›å½’çš„æ­£åˆ™åŒ–æ ·æ¡æµæ¨¡å‹ï¼Œç»“åˆäº†æ•°å€¼ç‰¹å¾MLPåµŒå…¥ã€ResNetä¸»å¹²ç‰¹å¾æå–å’Œæ¡ä»¶ç¥ç»æ ·æ¡æµï¼ˆRQ-NSFï¼‰å®ç°å¯¹å¤æ‚ç›®æ ‡åˆ†å¸ƒçš„é«˜æ•ˆå»ºæ¨¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTabResFlowåœ¨ä¹ä¸ªå…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šæ¦‚ç‡å»ºæ¨¡æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å®é™…äºŒæ‰‹è½¦å®šä»·ä»»åŠ¡ä¸­å¼•å…¥AURCæŒ‡æ ‡å¹¶å–å¾—æœ€ä¼˜è¡¨ç°ã€‚ç»“è®ºï¼šTabResFlowèƒ½æ›´å‡†ç¡®é«˜æ•ˆåœ°å»ºæ¨¡è¡¨æ ¼æ•°æ®ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œé€‚åˆå¤§è§„æ¨¡å·¥ä¸šåº”ç”¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TabResFlow_A_Normalizing_Spline_Flow_Model_for_Probabilistic_Univariate_Tabular_Regression.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16990v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      227. [Score Matching on Large Geometric Graphs for Cosmology Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Score_Matching_on_Large_Geometric_Graphs_for_Cosmology_Generation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Eindhoven University of Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºscore-matchingçš„ç”Ÿæˆæ¨¡å‹ï¼Œç»“åˆE(3)ç­‰å˜å›¾ç¥ç»ç½‘ç»œï¼Œç”¨äºåœ¨å¤§è§„æ¨¡å‡ ä½•å›¾ï¼ˆå¦‚å®‡å®™æ˜Ÿç³»å›¢ç‚¹äº‘ï¼‰ä¸Šç”Ÿæˆç¬¦åˆç‰©ç†çº¦æŸï¼ˆå¦‚å‘¨æœŸæ€§è¾¹ç•Œã€å‡åŒ€å…ˆéªŒå’Œå¯¹ç§°æ€§ï¼‰çš„ç»“æ„ã€‚æ–¹æ³•åˆ›æ–°æ€§åœ°å¼•å…¥äº†æ‹“æ‰‘æ„ŸçŸ¥å™ªå£°è°ƒåº¦æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†å¤§è§„æ¨¡ç‚¹äº‘ç”Ÿæˆçš„ç‰©ç†ä¸€è‡´æ€§ã€è®¡ç®—æ•ˆç‡å’Œæ‰©å±•æ€§ï¼Œä¸”åœ¨å®‡å®™å­¦ç»“æ„æ¨¡æ‹Ÿä¸Šä¼˜äºç°æœ‰æ‰©æ•£æ¨¡å‹ã€‚ç»“è®ºè¡¨æ˜è¯¥æ–¹æ³•èƒ½é«˜æ•ˆç”Ÿæˆé«˜ä¿çœŸåº¦å®‡å®™å­¦ç»“æ„ï¼Œæ˜¯Nä½“æ¨¡æ‹Ÿçš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Score_Matching_on_Large_Geometric_Graphs_for_Cosmology_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16939v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      228. [Sig-DEG for Distillation: Making Diffusion Models Faster and Lighter]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Sig-DEG_for_Distillation__Making_Diffusion_Models_Faster_and_Lighter.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University College London</span></p>
<p>æœ¬æ–‡æå‡ºSig-DEGï¼ˆSignature-based Differential Equation Generatorï¼‰ï¼Œä¸€ç§åŸºäºéƒ¨åˆ†è·¯å¾„ç­¾åå’Œé«˜é˜¶éšæœºæ³°å‹’å±•å¼€çš„æ‰©æ•£æ¨¡å‹è’¸é¦æ–¹æ³•ã€‚Sig-DEGé€šè¿‡ç›‘ç£å­¦ä¹ åœ¨ç²—æ—¶é—´ç½‘æ ¼ä¸Šè¿‘ä¼¼é€†æ‰©æ•£è¿‡ç¨‹ï¼Œåˆ©ç”¨é€’å½’ç¥ç»ç½‘ç»œå’Œéƒ¨åˆ†ç­¾åå‘é‡å®ç°é«˜æ•ˆæ¨æ–­ï¼Œå¹¶åœ¨æ— éœ€ç»†ç²’åº¦å¸ƒæœ—è·¯å¾„é‡å»ºçš„æƒ…å†µä¸‹ï¼Œæå¤§åŠ é€Ÿäº†ç”Ÿæˆé€Ÿåº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒSig-DEGèƒ½åœ¨å›¾åƒï¼ˆå¦‚MNISTï¼‰å’Œæ—¶é—´åºåˆ—ç­‰å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œä»¥5-10æ­¥ç”Ÿæˆä¿æœ‰é«˜ä¿çœŸåº¦çš„æ ·æœ¬ï¼Œå®ç°50-100å€æ¨ç†åŠ é€Ÿä¸”å‡ ä¹æ— æ€§èƒ½æŸå¤±ï¼Œå±•ç¤ºäº†åŸºäºè·¯å¾„ç­¾åçš„SDEè¿‘ä¼¼åœ¨é«˜æ•ˆç”Ÿæˆå»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Sig-DEG_for_Distillation_Making_Diffusion_Models_Faster_and_Lighter.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17554v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      229. [Bridging Graph and State-Space Modeling for Intensive Care Unit Length of Stay Prediction]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Bridging_Graph_and_State-Space_Modeling_for_Intensive_Care_Unit_Length_of_Stay_Prediction.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">University of Cambridge</span></p>
<p>æœ¬æ–‡æå‡ºäº†S2G-Netï¼Œä¸€ç§ç»“åˆçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆMambaï¼‰ä¸å¤šè§†å›¾å›¾ç¥ç»ç½‘ç»œï¼ˆä¼˜åŒ–ç‰ˆGraphGPSï¼‰çš„åŒé€šè·¯ç¥ç»æ¶æ„ï¼Œç”¨äºé‡ç—‡ç›‘æŠ¤ç—…æˆ¿(ICU)ä½é™¢æ—¶é•¿é¢„æµ‹ã€‚æ–¹æ³•åˆ›æ–°åœ¨äºå¤šæ¨¡æ€ä¸´åºŠæ•°æ®çš„æ—¶é—´åºåˆ—ä¸å¼‚æ„æ‚£è€…ç›¸ä¼¼æ€§å›¾çš„ç»Ÿä¸€å»ºæ¨¡ï¼Œå¹¶å¼•å…¥é«˜æ•ˆçš„å›¾ç»“æ„èåˆå’Œè§£é‡Šæ€§åˆ†æã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒS2G-Netåœ¨MIMIC-IVå¤§è§„æ¨¡æ•°æ®é›†ä¸Šï¼Œåœ¨å„é¡¹æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿåºåˆ—æ¨¡å‹ã€å›¾æ¨¡å‹åŠæ··åˆæ¨¡å‹ï¼Œå…·æœ‰è‰¯å¥½çš„é¢„æµ‹å‡†ç¡®æ€§ã€èµ„æºæ•ˆç‡å’Œæ¨¡å‹ç¨³å¥æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Bridging_Graph_and_State-Space_Modeling_for_Intensive_Care_Unit_Length_of_Stay_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17515v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      230. [GateTS: Versatile and Efficient Forecasting via Attention-Inspired routed Mixture-of-Experts]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GateTS__Versatile_and_Efficient_Forecasting_via_Attention-Inspired_routed_Mixture-of-Experts.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Lviv Polytechnic National University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹GateTSï¼Œå°†ç¨€ç–Mixture-of-Experts (MoE)æ¶æ„ä¸å—æ³¨æ„åŠ›æœºåˆ¶å¯å‘çš„åˆ†è·¯é—¨æ§æœºåˆ¶ç»“åˆï¼Œæ›¿ä»£ä¼ ç»Ÿsoftmaxè·¯ç”±å™¨ï¼Œæ— éœ€è¾…åŠ©è´Ÿè½½å‡è¡¡æŸå¤±ï¼Œå®ç°ä¸“å®¶åˆ©ç”¨çš„è‡ªç„¶å‡è¡¡ã€‚å®éªŒè¯æ˜ï¼ŒGateTSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä»¥æ›´å°‘çš„å‚æ•°è¶…è¶Šæˆ–åª²ç¾ä¸»æµTransformerï¼ˆå¦‚PatchTSTï¼‰å’ŒLSTMï¼Œå…¼å…·é«˜ç²¾åº¦å’Œé«˜è®¡ç®—æ•ˆç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GateTS_Versatile_and_Efficient_Forecasting_via_Attention-Inspired_routed_Mixture-of-Experts.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17478v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      231. [GraphMMP: A Graph Neural Network Model with Mutual Information and Global Fusion for Multimodal Medical Prognosis]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GraphMMP__A_Graph_Neural_Network_Model_with_Mutual_Information_and_Global_Fusion_for_Multimodal_Medi.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Hangzhou Dianzi University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„ä¸¤é˜¶æ®µå¤šæ¨¡æ€åŒ»å­¦é¢„åæ¨¡å‹GraphMMPï¼ŒåŒ…å«äº’ä¿¡æ¯é©±åŠ¨çš„ç‰¹å¾å›¾æ„å»ºæœºåˆ¶å’ŒMambaåºåˆ—æ¨¡å‹çš„å…¨å±€èåˆæ¨¡å—ï¼Œæå‡äº†æ¨¡æ€é—´éšå«ä¿¡æ¯æ•è·èƒ½åŠ›å’Œå…¨å±€æ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒç»“æœåœ¨è‚è„é¢„åå’ŒMETABRICä¹³è…ºç™Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„åˆ†ç±»å‡†ç¡®ç‡ä¸é²æ£’æ€§ï¼ŒéªŒè¯äº†GraphMMPåœ¨å¤šæ¨¡æ€åŒ»å­¦é¢„åä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GraphMMP_A_Graph_Neural_Network_Model_with_Mutual_Information_and_Global_Fusion_for_Multimodal_Medical_Prognosis.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17388v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      232. [Effective Clustering for Large Multi-Relational Graphs]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Effective_Clustering_for_Large_Multi-Relational_Graphs.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Hong Kong Baptist University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸¤ç§å¤šå…³ç³»å›¾èšç±»ç®—æ³•DEMMå’ŒDEMM+ï¼Œåˆ©ç”¨åŸºäºDirichletèƒ½é‡çš„ä¸¤é˜¶æ®µä¼˜åŒ–ç›®æ ‡ï¼Œå°†èŠ‚ç‚¹å±æ€§å’Œå¤šç±»å‹å…³ç³»ç»“æ„æœ‰æ•ˆèåˆã€‚å®éªŒè¡¨æ˜ï¼ŒDEMM+åœ¨å¤šç§çœŸå®å¤§è§„æ¨¡å¤šå…³ç³»å›¾æ•°æ®é›†ä¸Šï¼Œèšç±»è´¨é‡ä¼˜äº20ç§ç°æœ‰æ–¹æ³•ï¼Œä¸”æ•ˆç‡å¤§å¹…æå‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Effective_Clustering_for_Large_Multi-Relational_Graphs.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17376v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      233. [ShaLa: Multimodal Shared Latent Space Modelling]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ShaLa__Multimodal_Shared_Latent_Space_Modelling.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Stevens Institute of Technology</span></p>
<p>è¯¥è®ºæ–‡æå‡ºShaLaæ¡†æ¶ï¼Œé€šè¿‡æ¶æ„æ€§æ¨æ–­æ¨¡å‹å°†å¤šæ¨¡æ€è¾“å…¥ç¼–ç ä¸ºç¡®å®šæ€§ç‰¹å¾å¹¶èåˆä¸ºå…¨å±€è¡¨ç¤ºï¼Œè”åˆæ‰©æ•£æ¨¡å‹ä½œä¸ºå…±äº«æ½œå˜é‡çš„è¡¨è¾¾å…ˆéªŒï¼Œæœ‰æ•ˆè§£å†³å¤šæ¨¡æ€VAEä¸­æ¨æ–­çµæ´»æ€§ä¸è¶³å’Œprior-holeé—®é¢˜ã€‚å®éªŒè¡¨æ˜ShaLaåœ¨å¤šæ¨¡æ€ä¸€è‡´æ€§ã€ç”Ÿæˆè´¨é‡å’Œå¤šè§†è§’å¯æ‰©å±•æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½å®ç°é£æ ¼è¿ç§»å’Œé²æ£’æ¨æ–­ï¼Œé€‚ç”¨äºå¤æ‚å¤šæ¨¡æ€æ•°æ®ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ShaLa_Multimodal_Shared_Latent_Space_Modelling.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17256v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      234. [Provable Generalization in Overparameterized Neural Nets]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Provable_Generalization_in_Overparameterized_Neural_Nets.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Independent</span></p>
<p>æœ¬æ–‡æå‡ºå°†Transformerä¸­æ³¨æ„åŠ›çŸ©é˜µçš„æœ‰æ•ˆç§©ï¼ˆeffective rankï¼‰ä½œä¸ºæ¨¡å‹å®¹é‡çš„æ–°åº¦é‡ï¼Œç†è®ºä¸Šæ¨å¯¼å‡ºåŸºäºæœ‰æ•ˆç§©çš„æ³›åŒ–ç•Œï¼Œå…¶ä¸æ ·æœ¬è§„æ¨¡çš„ä¾èµ–ä¸å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„ç»éªŒç¼©æ”¾å®šå¾‹ä¸€è‡´ã€‚ç»“è®ºæ˜¾ç¤ºï¼Œåœ¨å‚æ•°è¿œè¶…æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå…³æ³¨æ³¨æ„åŠ›çŸ©é˜µçš„è°±å±æ€§èƒ½è·å¾—éå¹³å‡¡çš„æ³›åŒ–ä¿è¯ï¼Œä¸ºè§£é‡Šç°ä»£æ·±åº¦ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›æä¾›äº†æ–°è§†è§’ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Provable_Generalization_in_Overparameterized_Neural_Nets.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17236v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      235. [Learning Short-Term and Long-Term Patterns of High-Order Dynamics in Real-World Networks]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Learning_Short-Term_and_Long-Term_Patterns_of_High-Order_Dynamics_in_Real-World_Networks.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Chung-Ang University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹åŠ¨æ€é«˜é˜¶ç½‘ç»œå…³ç³»å»ºæ¨¡çš„æ–°æ–¹æ³•Lincolnï¼ŒåŒ…æ‹¬åŒäº¤äº’è¶…è¾¹ç¼–ç ï¼ˆBi-interactional hyperedge encodingï¼‰ä»¥æ•æ‰å¿«ç…§å†…é«˜é˜¶å…³ç³»çš„ç»“æ„å’Œæ—¶åºå½±å“ã€å‘¨æœŸæ—¶åºæ³¨å…¥ï¼ˆPeriodic time injectionï¼‰å’Œä¸­é—´èŠ‚ç‚¹è¡¨ç¤ºï¼ˆintermediate node representationï¼‰ä»¥å»ºæ¨¡é•¿å‘¨æœŸé‡å¤æ¨¡å¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLincolnåœ¨åŠ¨æ€è¶…è¾¹é¢„æµ‹ä»»åŠ¡ä¸Šä¼˜äºä¹ç§ä¸»æµæ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆæ•è·é«˜é˜¶å…³ç³»çš„çŸ­æœŸå’Œé•¿æœŸåŠ¨æ€ç‰¹æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Learning_Short-Term_and_Long-Term_Patterns_of_High-Order_Dynamics_in_Real-World_Networks.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17175v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      236. [Scaling Graph Transformers: A Comparative Study of Sparse and Dense Attention]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scaling_Graph_Transformers__A_Comparative_Study_of_Sparse_and_Dense_Attention.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Independent, Munich, Germany</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿæ¯”è¾ƒäº†å›¾Transformerä¸­ç¨ å¯†ï¼ˆDenseï¼‰å’Œç¨€ç–ï¼ˆSparseï¼‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ†æäº†å„è‡ªçš„æŠ€æœ¯å®ç°ã€ä»£è¡¨æ¨¡å‹ï¼ˆå¦‚Graphormerã€SANã€GraphGPSã€Exphormerï¼‰ã€è®¡ç®—å¤æ‚åº¦åŠåœ¨ä¸åŒè§„æ¨¡å›¾ä¸Šçš„å®éªŒè¡¨ç°ã€‚ç»“è®ºæŒ‡å‡ºï¼šç¨ å¯†æ³¨æ„åŠ›é€‚ç”¨äºå°è§„æ¨¡å›¾ä»¥è·å–å…¨å±€ä¿¡æ¯ä½†éš¾ä»¥æ‰©å±•ï¼Œç¨€ç–æ³¨æ„åŠ›åˆ™é€‚åˆå¤§è§„æ¨¡å›¾å¹¶åœ¨æ•ˆç‡ä¸è¡¨ç°é—´å–å¾—è‰¯å¥½å¹³è¡¡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Scaling_Graph_Transformers_A_Comparative_Study_of_Sparse_and_Dense_Attention.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17345v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      237. [ShortListing Model: A Streamlined Simplex Diffusion for Discrete Variable Generation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ShortListing_Model__A_Streamlined_SimplexDiffusion_for_Discrete_Variable_Generation.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tsinghua University</span></p>
<p>æœ¬æ–‡æå‡ºäº†Shortlisting Model (SLM)ï¼Œä¸€ç§åŸºäºsimplexç©ºé—´çš„é«˜æ•ˆç¦»æ•£å˜é‡ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œçµæ„Ÿæ¥è‡ªé€æ­¥å€™é€‰é›†åˆ å‡ã€‚SLMé€šè¿‡åœ¨simplexè´¨å¿ƒä¹‹é—´è¿›è¡Œæ¸è¿›å¼ç”Ÿæˆï¼Œæœ‰æ•ˆé™ä½äº†ç”Ÿæˆå¤æ‚åº¦ï¼Œå¹¶é€šè¿‡çµæ´»çš„æ— åˆ†ç±»å™¨å¼•å¯¼æœºåˆ¶æå‡æ— æ¡ä»¶ç”Ÿæˆæ€§èƒ½ã€‚å®éªŒè¡¨æ˜SLMåœ¨è¯­è¨€å»ºæ¨¡ã€è›‹ç™½è´¨è®¾è®¡åŠDNAåºåˆ—è®¾è®¡ç­‰å¤šé¡¹ç¦»æ•£ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨è›‹ç™½è´¨å’ŒDNAè®¾è®¡ä¸­è¾¾åˆ°æˆ–è¶…è¿‡ç°æœ‰æœ€ä½³æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ShortListing_Model_A_Streamlined_Simplex_Diffusion_for_Discrete_Variable_Generation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18251v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      238. [Aligning the Evaluation of Probabilistic Predictions with Downstream Value]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Aligning_the_Evaluation_of_Probabilistic_Predictions_with_Downstream_Value.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Institute of Computer Science, University of Tartu</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†è¯„ä»·æ¦‚ç‡é¢„æµ‹ä¸å…¶ä¸‹æ¸¸ä»»åŠ¡ä»·å€¼ä¹‹é—´å¯¹é½çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œåŠ æƒè¯„åˆ†è§„åˆ™çš„æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ åŠ æƒå‡½æ•°è‡ªåŠ¨å¯¹é½é¢„æµ‹è¯„ä»·ä¸ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ï¼Œæ— éœ€ä¸“å®¶æŒ‡å®šæƒé‡å‡½æ•°ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨åˆæˆå’ŒçœŸå®å›å½’ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆç¼©å°è¯„ä»·ä¸å®é™…ä¸‹æ¸¸æ•ˆç”¨çš„å·®è·ï¼Œæå‡è¯„ä»·æŒ‡æ ‡ä¸å®é™…å†³ç­–è¡¨ç°çš„ä¸€è‡´æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Aligning_the_Evaluation_of_Probabilistic_Predictions_with_Downstream_Value.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18166v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      239. [PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PCR-CA__Parallel_Codebook_Representations_with_Contrastive_Alignment_for_Multiple-Category_App_Recom.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Microsoft Store</span></p>
<p>è¯¥è®ºæ–‡æå‡ºPCR-CAæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€æ–‡æœ¬åµŒå…¥å’Œå¹¶è¡Œç æœ¬VQ-AEæ¨¡å—ï¼Œå¯¹åº”ç”¨å†…å®¹è¿›è¡Œç¦»æ•£åŒ–è¯­ä¹‰ç¼–ç ï¼ŒåŒæ—¶ä½¿ç”¨å¯¹æ¯”å¯¹é½æŸå¤±åœ¨ç”¨æˆ·å’Œç‰©å“å±‚é¢æ¡¥æ¥è¯­ä¹‰å’ŒååŒç‰¹å¾ï¼Œå¹¶é€šè¿‡åŒè·¯æ³¨æ„åŠ›æœºåˆ¶èåˆIDç‰¹å¾ä¸è¯­ä¹‰ç‰¹å¾ï¼Œæœ‰æ•ˆæå‡å¤šåˆ†ç±»å’Œé•¿å°¾åº”ç”¨çš„CTRé¢„æµ‹è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPCR-CAåœ¨å¤§å‹æ•°æ®é›†ä¸ŠAUCæå‡0.76%ï¼Œé•¿å°¾åº”ç”¨æå‡2.15%ï¼Œçº¿ä¸ŠA&#x2F;Bå®éªŒCTRæå‡10.52%ï¼ŒCVRæå‡16.30%ï¼Œå·²åœ¨å¾®è½¯å•†åº—å…¨é¢éƒ¨ç½²ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PCR-CA_Parallel_Codebook_Representations_with_Contrastive_Alignment_for_Multiple-Category_App_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18130v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      240. [Frozen in Time: Parameter-Efficient Time Series Transformers via Reservoir-Induced Feature Expansion and Fixed Random Dynamics]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Frozen_in_Time__Parameter-Efficient_Time_Series_Transformers_via_Reservoir-Induced_Feature_Expansion.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">IIT Roorkee</span></p>
<p>è¯¥è®ºæ–‡æå‡ºFreezeTSTæ¨¡å‹ï¼Œé€šè¿‡åœ¨Transformeræ—¶é—´åºåˆ—é¢„æµ‹æ¶æ„ä¸­äº¤æ›¿æ’å…¥å†»ç»“çš„éšæœºç‰¹å¾å—ï¼ˆreservoir blocksï¼‰ä¸å¯è®­ç»ƒå±‚ï¼Œå®ç°éçº¿æ€§è®°å¿†æ‰©å±•ä¸”ä¸å¢åŠ ä¼˜åŒ–è´Ÿæ‹…ã€‚ç†è®ºåˆ†æè¯æ˜è¯¥ç»“æ„å…·å¤‡æ¢¯åº¦ç¨³å®šæ€§å’Œå¯æ§é•¿æ—¶è®°å¿†ï¼Œå®éªŒç»“æœè¡¨æ˜FreezeTSTåœ¨ä¸ƒä¸ªé•¿åºåˆ—é¢„æµ‹åŸºå‡†ä¸Šä»¥æ›´å°‘å‚æ•°å’Œè®­ç»ƒæ—¶é—´ï¼Œä¾ç„¶è¾¾åˆ°æˆ–è¶…è¿‡ä¸»æµæ–¹æ³•è¡¨ç°ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Frozen_in_Time_Parameter-Efficient_Time_Series_Transformers_via_Reservoir-Induced_Feature_Expansion_and_Fixed_Random_Dynamics.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18057v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      241. [Dynamic Fusion Multimodal Network for SpeechWellness Detection]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dynamic_Fusion_Multimodal_Network_for_SpeechWellness_Detection.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Northwestern Polytechnical University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§å¤šåˆ†æ”¯å¤šæ¨¡æ€ç½‘ç»œï¼Œé€šè¿‡åŠ¨æ€èåˆæœºåˆ¶é›†æˆäº†æ—¶åŸŸã€æ—¶é¢‘åŸŸå£°å­¦ç‰¹å¾å’Œæ–‡æœ¬è¯­ä¹‰ç‰¹å¾ï¼Œç”¨äºé’å°‘å¹´è‡ªæ€é£é™©è¯­éŸ³æ£€æµ‹ã€‚é€šè¿‡å¼•å…¥å¯å­¦ä¹ åŠ æƒçš„åŠ¨æ€èåˆå—å’Œç®€åŒ–ç‰ˆWav2vec 2.0åŠBERTï¼Œå¤§å¹…å‡å°‘æ¨¡å‹å‚æ•°ï¼Œæé«˜äº†èåˆæ•ˆæœå’Œè®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•å‚æ•°é‡å‡å°‘78%ï¼Œå‡†ç¡®ç‡æå‡5%ï¼Œä¼˜äºå®˜æ–¹åŸºçº¿ï¼Œè¯å®äº†å¤šæ¨¡æ€èåˆå’ŒåŠ¨æ€åŠ æƒç­–ç•¥åœ¨è¯­éŸ³å¿ƒç†å¥åº·æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Dynamic_Fusion_Multimodal_Network_for_SpeechWellness_Detection.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18051v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      242. [Training Transformers for Mesh-Based Simulations]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Training_Transformers_for_Mesh-Based_Simulations.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Mines Paris - PSL University</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„æ–°å‹å›¾ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”¨äºå¤§è§„æ¨¡ç½‘æ ¼ç‰©ç†æ¨¡æ‹Ÿï¼Œåˆ©ç”¨é‚»æ¥çŸ©é˜µä½œä¸ºæ³¨æ„åŠ›æ©ç ï¼Œå¹¶é€šè¿‡æ‰©å±•é‚»æ¥çŸ©é˜µï¼ˆè†¨èƒ€æ»‘åŠ¨çª—å£ã€å…¨å±€æ³¨æ„åŠ›ã€éšæœºè¿æ¥ï¼‰æå‡è¿œç¨‹ä¾èµ–æ•è·èƒ½åŠ›ã€‚å¤§é‡å®éªŒåŠæ¶ˆèåˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç‰©ç†ä»¿çœŸæ•°æ®é›†ä¸Šè¾ƒç°æœ‰ä¸»æµæ–¹æ³•å…·å¤‡æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œæ¨¡å‹æ›´å°ï¼Œå°¤å…¶åœ¨å¤æ‚æµä½“åŠ¨åŠ›å­¦ä»»åŠ¡ä¸­æ€§èƒ½æå‡æ˜¾è‘—ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Training_Transformers_for_Mesh-Based_Simulations.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18037v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      243. [Enhancing Differentially Private Linear Regression via Public Second-Moment]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Enhancing_Differentially_Private_Linear_Regression_via_Public_Second-Moment.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Northwest University</span></p>
<p>è¯¥è®ºæ–‡æå‡ºåˆ©ç”¨å…¬ä¼—æ•°æ®çš„äºŒé˜¶çŸ©é˜µï¼ˆsecond-moment matrixï¼‰å¯¹ç§æœ‰æ•°æ®è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œä»¥å¢å¼ºå·®åˆ†éšç§çº¿æ€§å›å½’çš„æ•ˆç”¨å’Œç¨³å®šæ€§ï¼ˆDP-OLSEï¼‰ï¼Œå¹¶æ¨å¯¼ç†è®ºè¯¯å·®ç•Œé™ã€‚ç»“è®ºè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜å›å½’ç²¾åº¦ä¸é²æ£’æ€§ï¼Œå°¤å…¶åœ¨æ•°æ®åˆ†å¸ƒä¸è‰¯æˆ–é«˜ç»´åœºæ™¯ä¸‹æ•ˆæœçªå‡ºï¼Œå®éªŒç»“æœä¹Ÿåœ¨åˆæˆå’ŒçœŸå®æ•°æ®ä¸Šå¾—åˆ°éªŒè¯ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Enhancing_Differentially_Private_Linear_Regression_via_Public_Second-Moment.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18001v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      244. [A Novel Framework for Uncertainty Quantification via Proper Scores for Classification and Beyond]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Novel_Framework_for_Uncertainty_Quantification_via_Proper_Scores_for_Classification_and_Beyond.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Johann Wolfgang Goethe-UniversitÃ¤t</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºProper Scoresçš„é€šç”¨ä¸ç¡®å®šæ€§é‡åŒ–ç†è®ºæ¡†æ¶ï¼Œç»Ÿä¸€äº†å›å½’ã€åˆ†ç±»åŠç”Ÿæˆæ¨¡å‹ç­‰å¤šç§ä»»åŠ¡çš„ä¸ç¡®å®šæ€§åº¦é‡ã€‚æ–¹æ³•ä¸Šï¼Œä½œè€…é€šè¿‡æ³›åŒ–åå·®-æ–¹å·®åˆ†è§£å’Œå‡½æ•°Bregmanæ•£åº¦ï¼Œç³»ç»Ÿæ€§åœ°è¿æ¥äº†æœ¬ä½“ï¼ˆepistemicï¼‰ä¸ç¡®å®šæ€§ã€å†…ç¦€ï¼ˆaleatoricï¼‰ä¸ç¡®å®šæ€§ã€æ¨¡å‹æ ¡å‡†ä¸Proper Scoresï¼Œæå‡ºäº†é€‚ç”¨äºå›¾åƒã€éŸ³é¢‘ã€è‡ªç„¶è¯­è¨€ç”Ÿæˆç­‰å¤šé¢†åŸŸçš„æ ¸åˆ†æ•°ï¼ˆkernel scoreï¼‰åŠå…¶ç›¸å…³ä¼°è®¡å™¨ï¼Œå¹¶é¦–æ¬¡å°†æ ¡å‡†-é”åº¦åˆ†è§£æ¨å¹¿åˆ°è¶…è¶Šåˆ†ç±»çš„åœºæ™¯ã€‚ç»“è®ºæ˜¯ï¼Œè¯¥æ¡†æ¶ç†è®ºä¸å®è·µç»“åˆï¼Œæå‡äº†ç¥ç»ç½‘ç»œç­‰ç°ä»£æ¨¡å‹åœ¨å¤šä»»åŠ¡ä¸­çš„ä¸ç¡®å®šæ€§é‡åŒ–ä¸è¯„ä¼°ç²¾åº¦ï¼Œç›¸å…³æ ¸ç†µä¸ç¡®å®šæ€§åº¦é‡åœ¨å¤§è¯­è¨€æ¨¡å‹ç­‰åº”ç”¨ä¸­ä¼˜äºç°æœ‰SOTAåŸºçº¿ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Novel_Framework_for_Uncertainty_Quantification_via_Proper_Scores_for_Classification_and_Beyond.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17872v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      245. [Spectrum Prediction in the Fractional Fourier Domain with Adaptive Filtering]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Spectrum_Prediction_in_the_Fractional_Fourier_Domain_with_Adaptive_Filtering.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Nanjing University of Aeronautics and Astronautics</span></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSFFPçš„é¢‘è°±é¢„æµ‹æ¡†æ¶ï¼Œç»“åˆè‡ªé€‚åº”åˆ†æ•°é˜¶å‚…é‡Œå¶å˜æ¢ï¼ˆFrFTï¼‰å¯¹é¢‘è°±æ•°æ®è¿›è¡ŒåŸŸå˜æ¢ï¼Œåˆ©ç”¨æ··åˆæ»¤æ³¢ç­–ç•¥åœ¨åˆ†æ•°é˜¶å‚…é‡Œå¶åŸŸä¸­è‡ªé€‚åº”å»å™ªï¼Œå¹¶é€šè¿‡å¤å€¼ç¥ç»ç½‘ç»œé¢„æµ‹è¶‹åŠ¿åˆ†é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒSFFPåœ¨çœŸå®é¢‘è°±æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰é¢‘è°±å’Œä¸€èˆ¬æ—¶åºé¢„æµ‹æ–¹æ³•ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Spectrum_Prediction_in_the_Fractional_Fourier_Domain_with_Adaptive_Filtering.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17868v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      246. [FasterVoiceGrad: Faster One-step Diffusion-Based Voice Conversion with Adversarial Diffusion Conversion Distillation]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FasterVoiceGrad__Faster_One-step_Diffusion-Based_Voice_Conversion_with_Adversarial_Diffusion_Convers.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">NTT, Inc., Japan</span></p>
<p>è¯¥è®ºæ–‡æå‡ºFasterVoiceGradï¼Œä¸€ç§é€šè¿‡å¯¹æ‰©æ•£æ¨¡å‹å’Œå†…å®¹ç¼–ç å™¨è¿›è¡ŒåŒæ—¶è’¸é¦çš„å•æ­¥æ‰©æ•£è¯­éŸ³è½¬æ¢æ–¹æ³•ï¼Œæ ¸å¿ƒå¼•å…¥äº†å¯¹æŠ—æ€§æ‰©æ•£è½¬æ¢è’¸é¦ï¼ˆADCDï¼‰ï¼Œåœ¨è½¬æ¢è¿‡ç¨‹ä¸­è”åˆåˆ©ç”¨å¯¹æŠ—è®­ç»ƒå’Œåˆ†æ•°è’¸é¦ã€‚å®éªŒè¡¨æ˜FasterVoiceGradåœ¨ä¿æŒè¯­éŸ³è½¬æ¢æ€§èƒ½çš„åŒæ—¶ï¼ŒGPUåŠ é€Ÿæ¯”FastVoiceGradå¿«6.6å€ï¼ŒCPUå¿«1.8å€ï¼Œæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/FasterVoiceGrad_Faster_One-step_Diffusion-Based_Voice_Conversion_with_Adversarial_Diffusion_Conversion_Distillation.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17867v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      247. [Ada-TransGNN: An Air Quality Prediction Model Based On Adaptive Graph Convolutional Networks]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Ada-TransGNN__An_Air_Quality_Prediction_Model_Based_On_Adaptive_Graph_Convolutional_Networks.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">East China University of Science and Technology</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„è‡ªé€‚åº”å›¾å·ç§¯ç¥ç»ç½‘ç»œï¼ˆAda-TransGNNï¼‰ç”¨äºç©ºæ°”è´¨é‡é¢„æµ‹ã€‚æ–¹æ³•é€šè¿‡åŒ…å«å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å’Œå›¾å·ç§¯ç½‘ç»œçš„ååŒæ—¶ç©ºæ¨¡å—ã€å®è§‚&#x2F;å¾®è§‚è‡ªé€‚åº”å›¾ç»“æ„å­¦ä¹ æ¨¡å—å’Œè¾…åŠ©ä»»åŠ¡å­¦ä¹ æ¨¡å—ï¼ˆå¼•å…¥Moranç³»æ•°ï¼‰æ¥åŠ¨æ€å»ºæ¨¡ç›‘æµ‹ç‚¹ä¹‹é—´çš„å¤æ‚æ—¶ç©ºä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çŸ­æœŸå’Œé•¿æœŸç©ºæ°”è´¨é‡é¢„æµ‹ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰ä¸»æµæ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Ada-TransGNN_An_Air_Quality_Prediction_Model_Based_On_Adaptive_Graph_Convolutional_Networks.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17702v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      248. [Copyright Protection for 3D Molecular Structures with Watermarking]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Copyright_Protection_for_3D_Molecular_Structures_with_Watermarking.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">City University of Hong Kong</span></p>
<p>æœ¬æ–‡æå‡ºäº†é¦–ä¸ªç”¨äºä¸‰ç»´åˆ†å­ç»“æ„çš„é²æ£’æ°´å°æ–¹æ³•ï¼Œé€šè¿‡åŸºäºæ·±åº¦å­¦ä¹ çš„ç¼–ç å™¨å°†æ°´å°åµŒå…¥åˆ†å­åŸå­çº§åˆ«ç»“æ„ï¼Œç»“åˆä¸å˜ç‰¹å¾æå–ä¸å¤šç»´ç¼©æ”¾ï¼ˆMDSï¼‰ç­–ç•¥æå‡å¯¹ä»¿å°„å˜æ¢ï¼ˆæ—‹è½¬ã€å¹³ç§»ã€åå°„ï¼‰çš„é²æ£’æ€§ï¼Œå¹¶é€šè¿‡åŠ¨æ€å¹³è¡¡è®­ç»ƒç­–ç•¥ä¼˜åŒ–æ°´å°åµŒå…¥ä¸åˆ†å­å±æ€§çš„æƒè¡¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…åœ¨QM9å’ŒGEOM-DRUGç­‰åŸºå‡†æ•°æ®é›†åŠGeoBFNã€GeoLDMç­‰ç”Ÿæˆæ¨¡å‹ä¸‹å®ç°äº†é«˜è¾¾95%ä»¥ä¸Šæ°´å°æå–å‡†ç¡®ç‡ï¼Œè€Œä¸”å¯¹åˆ†å­ç»“æ„å’Œä¸‹æ¸¸å¯¹æ¥ä»»åŠ¡çš„å½±å“æå°ï¼Œæœ‰æ•ˆä¿éšœäº†åˆ†å­çŸ¥è¯†äº§æƒä¸”ä¸å½±å“å…¶ç§‘å­¦åº”ç”¨ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Copyright_Protection_for_3D_Molecular_Structures_with_Watermarking.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17690v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      249. [Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networks]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Text_Meets_Topology__Rethinking_Out-of-distribution_Detection_in_Text-Rich_Networks.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">The University of Queensland</span></p>
<p>æœ¬æ–‡æå‡ºTextTopoOODè¯„æµ‹æ¡†æ¶ç³»ç»Ÿæ€§æ¶µç›–æ–‡æœ¬å±æ€§ã€ç»“æ„ã€ä¸»é¢˜å’Œé¢†åŸŸå¤šç»´åº¦åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ£€æµ‹åœºæ™¯ï¼Œå¹¶æå‡ºTNT-OODæ–¹æ³•ï¼Œé€šè¿‡ç»“æ„æ„ŸçŸ¥ç¼–ç å™¨ã€è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆåŠè¶…ç½‘ç»œç”ŸæˆèŠ‚ç‚¹ç‰¹å®šæŠ•å½±å‚æ•°ï¼Œæœ‰æ•ˆå¯¹é½èŠ‚ç‚¹çš„æ–‡æœ¬ä¸æ‹“æ‰‘ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜TNT-OODåœ¨11ä¸ªæ•°æ®é›†å››ç±»OODåœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†æ–¹æ³•åœ¨æ–‡æœ¬ä¸°å¯Œç½‘ç»œåˆ†å¸ƒå¤–æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§å’ŒTextTopoOODè¯„æµ‹æ¡†æ¶çš„æŒ‘æˆ˜æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Text_Meets_Topology_Rethinking_Out-of-distribution_Detection_in_Text-Rich_Networks.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17689v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      250. [On the Edge of Memorization in Diffusion Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/On_the_Edge_of_Memorization_in_Diffusion_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">TTIC</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç†è®ºä¸å®éªŒç›¸ç»“åˆçš„â€œè®°å¿†-æ³›åŒ–å®éªŒå®¤â€ï¼Œç”¨äºç³»ç»Ÿç ”ç©¶æ‰©æ•£æ¨¡å‹çš„è®°å¿†ä¸æ³›åŒ–è¡Œä¸ºã€‚ä½œè€…é€šè¿‡åœ¨é«˜ç»´é«˜æ–¯æ··åˆæ¨¡å‹ä¸‹ï¼Œåˆ†ææ¨¡å‹å‚æ•°é‡ã€æ•°æ®å¤æ‚åº¦ç­‰å› ç´ å¯¹æ‰©æ•£æ¨¡å‹æ˜¯å¦ä¼šè®°å¿†è®­ç»ƒæ ·æœ¬è¿˜æ˜¯èƒ½æ³›åŒ–ç”Ÿæˆæ–°æ ·æœ¬çš„å½±å“ï¼Œæå‡ºå¹¶éªŒè¯äº†åŸºäºè®­ç»ƒæŸå¤±å·®å¼‚å¯ä»¥å‡†ç¡®é¢„æµ‹æ¨¡å‹ç”±æ³›åŒ–è½¬å‘è®°å¿†çš„ä¸´ç•Œç‚¹ã€‚ç»“è®ºè¡¨æ˜ï¼šåœ¨å—æ§æ¡ä»¶ä¸‹ï¼Œæ‰©æ•£æ¨¡å‹çš„è®°å¿†-æ³›åŒ–ç›¸å˜ç‚¹å¯è¢«ç†è®ºå‡†ç¡®é¢„æµ‹ï¼Œä¸ºåç»­ç†è®ºå’Œå®é™…æ‰©æ•£æ¨¡å‹å®‰å…¨åº”ç”¨ç ”ç©¶æä¾›äº†å¯éªŒè¯å·¥å…·ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/On_the_Edge_of_Memorization_in_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17679v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      251. [Characterizing the Behavior of Training Mamba-based State Space Models on GPUs]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Characterizing_the_Behavior_of_Training_Mamba-based_State_Space_Models_on_GPUs.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Advanced Micro Devices (AMD)</span></p>
<p>æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†åŸºäºMambaçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰åœ¨GPUè®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®ç®—å­ã€ç®—æœ¯å¼ºåº¦ã€å†…å­˜ä¸ç¼“å­˜è¡Œä¸ºä»¥åŠç®—å­ç“¶é¢ˆï¼Œæ¶µç›–è¯­è¨€ã€è§†è§‰å’Œå›¾ç­‰å¤šé¢†åŸŸæ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºSSMç®—å­ä¸ä»…å ç”¨å¤§é‡æ‰§è¡Œæ—¶é—´ä¸”ä¸»è¦å—é™äºå†…å­˜å¸¦å®½ï¼Œéœ€è½¯ç¡¬ä»¶ååŒä¼˜åŒ–ä»¥æå‡æ¨¡å‹å¯æ‰©å±•æ€§ä¸æ€§èƒ½ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Characterizing_the_Behavior_of_Training_Mamba-based_State_Space_Models_on_GPUs.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17649v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      252. [Longitudinal Progression Prediction of Alzheimerâ€™s Disease with Tabular Foundation Model]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Longitudinal_Progression_Prediction_of_Alzheimer's_Disease_with_Tabular_Foundation_Model.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Emory University</span></p>
<p>æœ¬æ–‡æå‡ºäº†L2C-TabPFNæ–¹æ³•ï¼Œç»“åˆçºµå‘åˆ°æ¨ªæˆªé¢ï¼ˆL2Cï¼‰ç‰¹å¾è½¬æ¢ä¸é¢„è®­ç»ƒçš„Tabular Foundation Modelï¼ˆTabPFNï¼‰ï¼Œç”¨äºTADPOLEæ•°æ®é›†ä¸Šé˜¿å°”èŒ¨æµ·é»˜ç—…å¤šæ¨¡æ€ä¸´åºŠæ•°æ®çš„è¿›å±•é¢„æµ‹ã€‚æ–¹æ³•é€šè¿‡L2Cå°†ç—…äººå†å²æ•°æ®è½¬ä¸ºå®šé•¿ç‰¹å¾å‘é‡ï¼ŒTabPFNåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ è¿›è¡Œè¯Šæ–­ä¸ç”Ÿç‰©æ ‡å¿—ç‰©å›å½’ã€‚å®éªŒè¡¨æ˜ï¼ŒL2C-TabPFNåœ¨è„‘å®¤ä½“ç§¯é¢„æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä¼˜ç»“æœï¼Œä¼˜äºä¼ ç»ŸXGBoostæ¨¡å‹Frogï¼Œè€Œè¯Šæ–­ä¸è®¤çŸ¥åˆ†æ•°é¢„æµ‹è¡¨ç°ç•¥é€Šä¸€ç­¹ã€‚ç»“è®ºï¼šL2C-TabPFNèƒ½æ›´å¥½æ•æ‰ç»“æ„æ€§è„‘å½±åƒå˜åŒ–ï¼Œæå‡é˜¿å°”èŒ¨æµ·é»˜ç—…è¿›å±•é¢„æµ‹ç²¾åº¦ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Longitudinal_Progression_Prediction_of_Alzheimer%E2%80%99s_Disease_with_Tabular_Foundation_Model.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17630v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      253. [Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Quantum_Graph_Attention_Network__A_Novel_Quantum_Multi-Head_Attention_Mechanism_for_Graph_Learning.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Korea Advanced Institute of Science and Technology (KAIST)</span></p>
<p>æœ¬æ–‡æå‡ºäº†Quantum Graph Attention Network (QGAT)ï¼Œå°†å˜åˆ†é‡å­ç”µè·¯èåˆåˆ°å›¾ç¥ç»ç½‘ç»œçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œé‡‡ç”¨å¹…åº¦ç¼–ç å’Œå¼ºçº ç¼ é‡å­å±‚ï¼Œä½¿ç”¨å•ä¸€é‡å­ç”µè·¯å¹¶è¡Œç”Ÿæˆå¤šå¤´æ³¨æ„åŠ›ï¼Œæå¤§å‡å°‘å‚æ•°é‡å¹¶æå‡éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒQGATåœ¨èŠ‚ç‚¹åˆ†ç±»å’Œé“¾è·¯é¢„æµ‹ç­‰ä»»åŠ¡ä¸­å¯¹æ¯”GAT&#x2F;GATv2è¡¨ç°æ›´ä¼˜ï¼Œä¸”åœ¨ç‰¹å¾å’Œç»“æ„å™ªå£°ä¸‹å…·æœ‰æ›´å¼ºé²æ£’æ€§ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰æ¶æ„ï¼Œå±•ç¤ºäº†é‡å­å¢å¼ºæ³¨æ„åŠ›æœºåˆ¶åœ¨å¤æ‚å›¾å­¦ä¹ ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Quantum_Graph_Attention_Network_A_Novel_Quantum_Multi-Head_Attention_Mechanism_for_Graph_Learning.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17622v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      254. [The Statistical Fairness-Accuracy Frontier]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_Statistical_Fairness-Accuracy_Frontier.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Rice University</span></p>
<p>æœ¬æ–‡ç ”ç©¶æœ‰é™æ ·æœ¬æ¡ä»¶ä¸‹å…¬å¹³æ€§ä¸å‡†ç¡®æ€§ï¼ˆFAï¼‰å‰æ²¿çš„ç»Ÿè®¡ç‰¹æ€§ï¼Œæå‡ºé’ˆå¯¹ä¸¤ç»„çº¿æ€§å›å½’é—®é¢˜çš„æœ€ä¼˜ï¼ˆminimaxï¼‰ä¼°è®¡å™¨ï¼Œåˆ†åˆ«é’ˆå¯¹å·²çŸ¥ä¸æœªçŸ¥åæ–¹å·®çŸ©é˜µä¸¤ç§åœºæ™¯ï¼Œå®šé‡åˆ†ææœ‰é™æ ·æœ¬å¯¹å…¬å¹³-å‡†ç¡®æ€§æƒè¡¡æ›²çº¿çš„å½±å“ï¼Œå¹¶æ¨å¯¼é‡‡æ ·åˆ†é…ä¸ä¼°è®¡è¯¯å·®çš„æœ€ä¼˜ç­–ç•¥ã€‚ç»“è®ºæ˜¾ç¤ºï¼Œæœ‰é™æ•°æ®å¯¼è‡´ä¸åŒç¾¤ä½“é£é™©çš„éå¯¹ç§°å½±å“ï¼Œå°¤å…¶åœ¨ç¾¤ä½“å¼‚è´¨æ€§å¤§æ—¶æ›´æ˜¾è‘—ï¼Œå¼ºè°ƒå®é™…è®¾è®¡ä¸­éœ€è€ƒè™‘ç»Ÿè®¡åå·®å¯¹å…¬å¹³-å‡†ç¡®æ€§å¹³è¡¡çš„æ‰­æ›²ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/The_Statistical_Fairness-Accuracy_Frontier.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17561v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      255. [Consciousness as a Functor]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Consciousness_as_a_Functor.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Adobe Research</span></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°æ€§çš„æ„è¯†ç†è®ºï¼Œå°†æ„è¯†å»ºæ¨¡ä¸ºåœ¨èŒƒç•´è®ºæ„ä¹‰ä¸‹çš„å‡½å­ï¼ˆCFï¼‰ï¼Œé€šè¿‡èŒƒç•´ä¸å‡½å­ã€ç…¤ä»£æ•°å’Œæ‹“æ‰‘æ–¯ç†è®ºï¼Œä»æ— æ„è¯†è®°å¿†åˆ°æ„è¯†è®°å¿†çš„ä¿¡æ¯ä¼ é€’è¿›è¡Œå»ºæ¨¡ï¼Œæå‡ºäº†å¤šæ¨¡æ€é€šç”¨Mitchell-BÃ©nabouè¯­è¨€åµŒå…¥ï¼ˆMUMBLEï¼‰ä½œä¸ºâ€œæ€æƒ³çš„å†…éƒ¨è¯­è¨€â€ï¼Œå¹¶ç”¨é€šç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆURLï¼‰ä¸ç½‘ç»œç»æµæ¨¡å‹æè¿°ä¿¡æ¯åœ¨ä¸åŒè®°å¿†æ¨¡å—é—´çš„æµåŠ¨ã€‚ç»“è®ºè®¤ä¸ºè¯¥ç†è®ºä¸ºæ„è¯†çš„ç»“æ„åŒ–å’Œå¯è®¡ç®—å»ºæ¨¡æä¾›äº†é€šç”¨çš„æ•°å­¦æ¡†æ¶ï¼Œä¸”èƒ½ç»Ÿä¸€æè¿°æ„è¯†ä¸æ— æ„è¯†é—´çš„äº¤äº’ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Consciousness_as_a_Functor.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3> 
<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17742v1" 
       style="text-decoration: none; color: inherit;" 
       onmouseover="this.style.textDecoration='underline';" 
       onmouseout="this.style.textDecoration='none';">
      256. [EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Models]
</a> [[PDF]](http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/EEG-FM-Bench__A_Comprehensive_Benchmark_for_the_Systematic_Evaluation_of_EEG_Foundation_Models.pdf)
</h3>

<p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* æ–œä½“ */
    font-weight: bold !important;      /* åŠ ç²— */
">Tongji University</span></p>
<p>æœ¬è®ºæ–‡æå‡ºEEG-FM-Benchï¼Œè¿™æ˜¯é¦–ä¸ªç³»ç»ŸåŒ–ã€æ ‡å‡†åŒ–è¯„æµ‹EEGåŸºç¡€æ¨¡å‹ï¼ˆEEG-FMsï¼‰çš„ç»¼åˆæ€§åŸºå‡†ï¼Œæ¶µç›–14ä¸ªæ•°æ®é›†ä¸10ç§å…¸å‹è„‘ç”µèŒƒå¼ï¼Œé›†æˆç»Ÿä¸€çš„æ•°æ®å¤„ç†ã€ä¸‰ç§ä¸‹æ¸¸å¾®è°ƒç­–ç•¥åŠå®šé‡&#x2F;å®šæ€§åˆ†ææµç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰EEG-FMåœ¨å†»ç»“éª¨å¹²æ—¶æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œç»“åˆå¤šä»»åŠ¡è®­ç»ƒã€ç²¾ç»†åŒ–æ—¶ç©ºç‰¹å¾å»ºæ¨¡åŠè„‘ç§‘å­¦å…ˆéªŒå¯æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½å’Œæ³›åŒ–æ€§ã€‚è®ºæ–‡ä¸ºEEGåŸºç¡€æ¨¡å‹çš„å…¬å¹³æ¯”è¾ƒä¸å¯å¤ç°ç ”ç©¶æä¾›äº†å¹³å°ï¼Œæ¨åŠ¨é¢†åŸŸè¿›æ­¥ã€‚</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/EEG-FM-Bench_A_Comprehensive_Benchmark_for_the_Systematic_Evaluation_of_EEG_Foundation_Models.jpg" srcset="/img/loading.gif" lazyload alt="è®ºæ–‡é…å›¾" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/27æŠ¥çº¸</div>
      <div>http://ywfhhh.github.io/2025/08/2025-08-27_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>ä½œè€…</div>
          <div>å…·èº«ADT</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>å‘å¸ƒäº</div>
          <div>2025å¹´8æœˆ27æ—¥</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>è®¸å¯åè®®</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - ç½²å">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-26_article/" title="2025/08/26æŠ¥çº¸">
                        <span class="hidden-mobile">2025/08/26æŠ¥çº¸</span>
                        <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"æ¬¢è¿äº¤æµè®¨è®º...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>ç›®å½•</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        æ€»è®¿é—®é‡ 
        <span id="busuanzi_value_site_pv"></span>
         æ¬¡
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        æ€»è®¿å®¢æ•° 
        <span id="busuanzi_value_site_uv"></span>
         äºº
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- ä¸»é¢˜çš„å¯åŠ¨é¡¹ï¼Œå°†å®ƒä¿æŒåœ¨æœ€åº•éƒ¨ -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div>
  </noscript>
</body>
</html>
