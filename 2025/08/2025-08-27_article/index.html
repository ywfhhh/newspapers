

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="具身ADT">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;27报纸">
<meta property="og:url" content="http://ywfhhh.github.io/2025/08/2025-08-27_article/index.html">
<meta property="og:site_name" content="最新前沿论文分享">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GRASP_Geospatial_pixel_Reasoning_viA_Structured_Policy_learning.jpg">
<meta property="article:published_time" content="2025-08-27T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-27T03:33:35.755Z">
<meta property="article:author" content="具身ADT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GRASP_Geospatial_pixel_Reasoning_viA_Structured_Policy_learning.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/27报纸 - 最新前沿论文分享</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ywfhhh.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文库</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/27报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-27 00:00" pubdate>
          星期三, 八月 27日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          38k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/27报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/27报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-27<br>📄 <strong>发现论文数量</strong>：256  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-GRASP-Geospatial-pixel-Reasoning-viA-Structured-Policy-learningpdf"><a href="#1-GRASP-Geospatial-pixel-Reasoning-viA-Structured-Policy-learningpdf" class="headerlink" title="1. GRASP: Geospatial pixel Reasoning viA Structured Policy learningpdf"></a>1. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17102v1">GRASP: Geospatial pixel Reasoning viA Structured Policy learning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GRASP__Geospatial_pixel_Reasoning_viA_Structured_Policy_learning.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出GRASP框架，通过多模态大语言模型（MLLM）输出与任务相关的框框和正样本点，作为提示传递给预训练分割模型，最终生成遥感图像的分割掩码。与以往需像素级监督的方案不同，GRASP完全采用强化学习（GRPO算法）仅用框和点的奖励信号优化，无需掩码监督，极大降低标注成本并提升泛化能力。实验表明，GRASP在内外域数据集上均达SOTA，内域提升约4%，外域最高提升54%，表现出极强的泛化和稳健性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GRASP_Geospatial_pixel_Reasoning_viA_Structured_Policy_learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-Solving-the-Min-Max-Multiple-Traveling-Salesmen-Problem-via-Learning-Based-Path-Generation-and-Optimal-Splittingpdf"><a href="#2-Solving-the-Min-Max-Multiple-Traveling-Salesmen-Problem-via-Learning-Based-Path-Generation-and-Optimal-Splittingpdf" class="headerlink" title="2. Solving the Min-Max Multiple Traveling Salesmen Problem via Learning-Based Path Generation and Optimal Splittingpdf"></a>2. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17087v1">Solving the Min-Max Multiple Traveling Salesmen Problem via Learning-Based Path Generation and Optimal Splitting</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Solving_the_Min-Max_Multiple_Traveling_Salesmen_Problem_via_Learning-Based_Path_Generation_and_Optim.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本文提出了Generate-and-Split (GaS) 框架，通过LSTM增强的强化学习模型生成城市访问路径，并结合确定性近似最优的分割算法，将路径分配给多位销售员以最小化最长旅行路径。实验结果表明，GaS在解质量和泛化能力上显著优于现有学习型方法，尤其在不同规模和分布的数据上表现更佳。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Solving_the_Min-Max_Multiple_Traveling_Salesmen_Problem_via_Learning-Based_Path_Generation_and_Optimal_Splitting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-Reinforcement-Guided-Hyper-Heuristic-Hyperparameter-Optimization-for-Fair-and-Explainable-Spiking-Neural-Network-Based-Financial-Fraud-Detectionpdf"><a href="#3-Reinforcement-Guided-Hyper-Heuristic-Hyperparameter-Optimization-for-Fair-and-Explainable-Spiking-Neural-Network-Based-Financial-Fraud-Detectionpdf" class="headerlink" title="3. Reinforcement-Guided Hyper-Heuristic Hyperparameter Optimization for Fair and Explainable Spiking Neural Network-Based Financial Fraud Detectionpdf"></a>3. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16915v1">Reinforcement-Guided Hyper-Heuristic Hyperparameter Optimization for Fair and Explainable Spiking Neural Network-Based Financial Fraud Detection</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Reinforcement-Guided_Hyper-Heuristic_Hyperparameter_Optimization_for_Fair_and_Explainable_Spiking_Ne.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Khulna University of Engineering &amp; Technology (KUET)</span></p>
<p>本文提出了一种结合皮层脉冲神经网络（CSNPC）、基于Q-learning的强化引导超启发式优化器（RHOSS）和可解释性AI（XAI）技术的金融欺诈检测框架。该方法通过RHOSS动态优化SNN超参数，嵌入MoSSTI框架，利用显著性归因和神经元脉冲活动分析实现模型可解释与公平性，实验证明在BAF数据集下模型在5% FPR约束下召回率达90.8%，公平性指标超98%，显著优于现有方法。结论表明该方法在提升脉冲神经网络公平性、性能和可解释性上效果突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Reinforcement-Guided_Hyper-Heuristic_Hyperparameter_Optimization_for_Fair_and_Explainable_Spiking_Neural_Network-Based_Financial_Fraud_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-Dream-to-Chat-Model-based-Reinforcement-Learning-on-Dialogues-with-User-Belief-Modelingpdf"><a href="#4-Dream-to-Chat-Model-based-Reinforcement-Learning-on-Dialogues-with-User-Belief-Modelingpdf" class="headerlink" title="4. Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modelingpdf"></a>4. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16876v1">Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dream_to_Chat__Model-based_Reinforcement_Learning_on_Dialogues_with_User_Belief_Modeling.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Geely AI Lab</span></p>
<p>本文提出DreamCUB框架，将模型驱动的强化学习（MBRL）应用于对话系统，并引入用户信念建模，包括情感、情绪和意图识别。关键方法是构建对话世界模型（DWM），通过POMDP理论和信息瓶颈优化实现用户隐状态推理，并与对话策略联合训练。实验表明，预训练的DWM在情感与情绪识别任务上达到SOTA水平，DreamCUB在日常及同理心对话中均显著提升情感预测与回复质量，泛化能力强。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Dream_to_Chat_Model-based_Reinforcement_Learning_on_Dialogues_with_User_Belief_Modeling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-ReviBranch-Deep-Reinforcement-Learning-for-Branch-and-Bound-with-Revived-Trajectoriespdf"><a href="#5-ReviBranch-Deep-Reinforcement-Learning-for-Branch-and-Bound-with-Revived-Trajectoriespdf" class="headerlink" title="5. ReviBranch: Deep Reinforcement Learning for Branch-and-Bound with Revived Trajectoriespdf"></a>5. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17452v1">ReviBranch: Deep Reinforcement Learning for Branch-and-Bound with Revived Trajectories</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ReviBranch__Deep_Reinforcement_Learning_for_Branch-and-Bound_with_Revived_Trajectories.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hong Kong Baptist University</span></p>
<p>该论文提出了ReviBranch，一个针对混合整数线性规划（MILP）分支定界算法的深度强化学习框架，采用Encoder-Revival-Decoder结构，通过历史轨迹复原和图-序列解码器捕捉分支决策与状态的时空依赖，并引入重要性加权奖励重分配机制，将稀疏奖励转化为密集反馈。实验结果表明，该方法在大规模实例上优于现有强化学习方法，实现了平均B&amp;B节点数降低4.0%、LP迭代次数减少2.2%，具有较强的泛化性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ReviBranch_Deep_Reinforcement_Learning_for_Branch-and-Bound_with_Revived_Trajectories.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-Rectified-Robust-Policy-Optimization-for-Model-Uncertain-Constrained-Reinforcement-Learning-without-Strong-Dualitypdf"><a href="#6-Rectified-Robust-Policy-Optimization-for-Model-Uncertain-Constrained-Reinforcement-Learning-without-Strong-Dualitypdf" class="headerlink" title="6. Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Dualitypdf"></a>6. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17448v1">Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Rectified_Robust_Policy_Optimization_for_Model-Uncertain_Constrained_Reinforcement_Learning_without_.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Maryland, College Park</span></p>
<p>本文针对存在模型不确定性的鲁棒约束强化学习（RL）问题，首次理论性地证明了该类问题一般不满足强对偶性，指出传统的原始-对偶方法可能无法获得最优可行策略。为此，作者提出了一种仅基于原始问题的算法RRPO（Rectified Robust Policy Optimization），直接在原始空间上进行策略优化，并在温和假设下给出全局收敛性理论保证，实验验证RRPO在不同环境下具有更强的鲁棒性和安全性，能有效规避最坏情况下的约束违背。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Rectified_Robust_Policy_Optimization_for_Model-Uncertain_Constrained_Reinforcement_Learning_without_Strong_Duality.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modelingpdf"><a href="#7-TreePO-Bridging-the-Gap-of-Policy-Optimization-and-Efficacy-and-Inference-Efficiency-with-Heuristic-Tree-based-Modelingpdf" class="headerlink" title="7. TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modelingpdf"></a>7. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17445v1">TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TreePO__Bridging_the_Gap_of_Policy_Optimization_and_Efficacy_and_Inference_Efficiency_with_Heuristic.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">M-A-P</span></p>
<p>该论文提出了TreePO框架，将大语言模型的序列生成过程转化为树结构搜索，结合分段采样与分层优势估计，实现高效的探索与精确的信用分配。实验显示，TreePO在多项推理基准上在保持模型性能的同时，显著提升了训练稳定性并减少了22%~43%的GPU计算消耗，能够有效扩展RL在复杂推理任务中的应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TreePO_Bridging_the_Gap_of_Policy_Optimization_and_Efficacy_and_Inference_Efficiency_with_Heuristic_Tree-based_Modeling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-MetaFed-Advancing-Privacy-Performance-and-Sustainability-in-Federated-Metaverse-Systemspdf"><a href="#8-MetaFed-Advancing-Privacy-Performance-and-Sustainability-in-Federated-Metaverse-Systemspdf" class="headerlink" title="8. MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systemspdf"></a>8. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17341v1">MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MetaFed__Advancing_Privacy,_Performance,_and_Sustainability_in_Federated_Metaverse_Systems.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kırıkkale University</span></p>
<p>本文提出了MetaFed框架，将多智能体强化学习（MARL）驱动的动态资源编排、同态加密与差分隐私结合的联邦学习，以及碳感知调度集成于元宇宙环境下，实现隐私保护、高效性能和环境可持续性。实验证明，MetaFed在MNIST和CIFAR-10等基准数据集上能在维持高准确率和低通信延迟的同时，显著减少碳排放（相较传统方法最高降幅达25%），为现实元宇宙应用提供可扩展、绿色且隐私合规的解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MetaFed_Advancing_Privacy_Performance_and_Sustainability_in_Federated_Metaverse_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-Federated-Reinforcement-Learning-for-Runtime-Optimization-of-AI-Applications-in-Smart-Eyewearspdf"><a href="#9-Federated-Reinforcement-Learning-for-Runtime-Optimization-of-AI-Applications-in-Smart-Eyewearspdf" class="headerlink" title="9. Federated Reinforcement Learning for Runtime Optimization of AI Applications in Smart Eyewearspdf"></a>9. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17262v1">Federated Reinforcement Learning for Runtime Optimization of AI Applications in Smart Eyewears</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Federated_Reinforcement_Learning_for_Runtime_Optimization_of_AI_Applications_in_Smart_Eyewears.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Politecnico di Milano</span></p>
<p>该论文提出了一种基于联邦强化学习（FRL）的三层DNN分层分割与任务卸载优化框架，针对智能眼镜（SEW）在有限算力、能耗和网络波动下的AI应用运行时优化。方法采用同步与异步FRL策略，分布式多代理通过深度Q网络（DQN）协同训练，实现多目标（延迟、能耗、费用）自适应决策。实验表明，FRL方法稳定性高、泛化能力强、适应多种AI模型（如YOLOv5&#x2F;YOLOv8），且在异步环境下也能保持低延迟违规率，优于传统分割基线算法。结论：FRL可在动态环境下显著提升可穿戴AI设备的实时性与稳定性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Federated_Reinforcement_Learning_for_Runtime_Optimization_of_AI_Applications_in_Smart_Eyewears.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-GPG-HT-Generalized-Policy-Gradient-with-History-Aware-Decision-Transformer-for-Probabilistic-Path-Planningpdf"><a href="#10-GPG-HT-Generalized-Policy-Gradient-with-History-Aware-Decision-Transformer-for-Probabilistic-Path-Planningpdf" class="headerlink" title="10. GPG-HT: Generalized Policy Gradient with History-Aware Decision Transformer for Probabilistic Path Planningpdf"></a>10. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17218v1">GPG-HT: Generalized Policy Gradient with History-Aware Decision Transformer for Probabilistic Path Planning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GPG-HT__Generalized_Policy_Gradient_with_History-Aware_Decision_Transformer_for_Probabilistic_Path_P.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sichuan University</span></p>
<p>本文提出GPG-HT框架，将历史感知决策Transformer与广义策略梯度（GPG）结合，针对具有依赖关系的随机交通网络中的概率路径规划问题。该方法通过Transformer建模车辆历史轨迹和剩余时间预算，采用GPG优化实现非马尔可夫环境下高效路径决策，并在Sioux Falls Network等基准上显著提升了准时到达概率，优于现有基线方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GPG-HT_Generalized_Policy_Gradient_with_History-Aware_Decision_Transformer_for_Probabilistic_Path_Planning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-Reinforcement-Learning-enhanced-Online-Adaptive-Clinical-Decision-Support-via-Digital-Twin-powered-Policy-and-Treatment-Effect-optimized-Rewardpdf"><a href="#11-Reinforcement-Learning-enhanced-Online-Adaptive-Clinical-Decision-Support-via-Digital-Twin-powered-Policy-and-Treatment-Effect-optimized-Rewardpdf" class="headerlink" title="11. Reinforcement Learning enhanced Online Adaptive Clinical Decision Support via Digital Twin powered Policy and Treatment Effect optimized Rewardpdf"></a>11. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17212v1">Reinforcement Learning enhanced Online Adaptive Clinical Decision Support via Digital Twin powered Policy and Treatment Effect optimized Reward</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Reinforcement_Learning_enhanced_Online_Adaptive_Clinical_Decision_Support_via_Digital_Twin_powered_P.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Houston</span></p>
<p>本文提出了一种结合强化学习、数字孪生和治疗效果优化的新型在线自适应临床决策支持系统。方法流程包括：离线用批约束强化学习（BCQ）和数字孪生训练策略和环境，在线阶段采用Q网络集成判定不确定性，结合主动学习与安全规则实现高效低风险的人机协作决策。实验显示，该系统在合成临床环境中实现了低延迟、高安全性、专家查询率低和收益优于主流基线方法。结论：该系统能在保证安全的前提下快速适应分布变化，显著降低专家负担并提升临床决策效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Reinforcement_Learning_enhanced_Online_Adaptive_Clinical_Decision_Support_via_Digital_Twin_powered_Policy_and_Treatment_Effect_optimized_Reward.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-ANO-Faster-is-Better-in-Noisy-Landscapespdf"><a href="#12-ANO-Faster-is-Better-in-Noisy-Landscapespdf" class="headerlink" title="12. ANO: Faster is Better in Noisy Landscapespdf"></a>12. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18258v1">ANO: Faster is Better in Noisy Landscapes</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ANO___Faster_is_Better_in_Noisy_Landscape.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent Researcher</span></p>
<p>本文提出了Ano，一种新的优化器，通过将更新的方向（由动量平滑）与步长（由瞬时梯度幅值决定）解耦，增强了对噪声和非平稳梯度的鲁棒性，保留了一阶自适应方法的高效与简洁。此外，提出了Anolog，通过对动量系数采用对数调度进一步提升超参数鲁棒性。理论上，Ano在非凸随机优化下具有O(K^{-1&#x2F;4})的收敛率，并在深度强化学习和高噪声NLP任务中显著优于主流优化器，同时在标准视觉任务上也具备竞争力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ANO_Faster_is_Better_in_Noisy_Landscapes.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generationpdf"><a href="#13-Visual-CoG-Stage-Aware-Reinforcement-Learning-with-Chain-of-Guidance-for-Text-to-Image-Generationpdf" class="headerlink" title="13. Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generationpdf"></a>13. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18032v1">Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Visual-CoG__Stage-Aware_Reinforcement_Learning_with_Chain_of_Guidance_for_Text-to-Image_Generation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Alibaba Group</span></p>
<p>本文提出了一种针对文本到图像生成的强化学习新框架Visual-CoG，通过语义推理、过程细化和结果评估三个阶段，结合阶段性奖励机制实现全流程即时优化。实验结果表明，该方法在GenEval、T2I-CompBench和VisCog-Bench等多项基准上显著提升了多属性和模糊指令下的生成效果，尤其增强了模型的推理与语义对齐能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Visual-CoG_Stage-Aware_Reinforcement_Learning_with_Chain_of_Guidance_for_Text-to-Image_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-Multi-layer-Abstraction-for-Nested-Generation-of-Options-MANGO-in-Hierarchical-Reinforcement-Learningpdf"><a href="#14-Multi-layer-Abstraction-for-Nested-Generation-of-Options-MANGO-in-Hierarchical-Reinforcement-Learningpdf" class="headerlink" title="14. Multi-layer Abstraction for Nested Generation of Options (MANGO) in Hierarchical Reinforcement Learningpdf"></a>14. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17751v1">Multi-layer Abstraction for Nested Generation of Options (MANGO) in Hierarchical Reinforcement Learning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multi-layer_Abstraction_for_Nested_Generation_of_Options_(MANGO)_in_Hierarchical_Reinforcement_Learn.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Human Inspired Technology Research Center, Università di Padova</span></p>
<p>本文提出了MANGO（一种多层抽象的嵌套选项生成层次强化学习框架），通过将复杂任务分解为多层抽象，每层定义抽象状态空间并用选项模块化轨迹，实现了跨层选项嵌套和高效重用。实验表明，MANGO在稀疏奖励环境下显著提升了样本效率和泛化能力，并增强了决策过程的可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Multi-layer_Abstraction_for_Nested_Generation_of_Options_%28MANGO%29_in_Hierarchical_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="15-M3DMap-Object-aware-Multimodal-3D-Mapping-for-Dynamic-Environmentspdf"><a href="#15-M3DMap-Object-aware-Multimodal-3D-Mapping-for-Dynamic-Environmentspdf" class="headerlink" title="15. M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environmentspdf"></a>15. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17044v1">M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/M3DMap__Object-aware_Multimodal_3D_Mapping_for_Dynamic_Environments.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Moscow Institute of Physics and Technology</span></p>
<p>本文提出了M3DMap，一种面向动态环境的对象感知多模态3D建图的模块化方法，整合了神经网络实现的多模态对象分割与跟踪、可训练的里程计估计、3D地图构建与更新以及多模态数据检索模块。实验和理论证明，该方法在机器人和自动驾驶等多种下游任务（如目标识别、定位、导航和操作）中能显著提升性能，尤其利用多模态数据与基础模型融合带来更优识别和建图效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/M3DMap_Object-aware_Multimodal_3D_Mapping_for_Dynamic_Environments.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-A-Rapid-Iterative-Trajectory-Planning-Method-for-Automated-Parking-through-Differential-Flatnesspdf"><a href="#16-A-Rapid-Iterative-Trajectory-Planning-Method-for-Automated-Parking-through-Differential-Flatnesspdf" class="headerlink" title="16. A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatnesspdf"></a>16. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17038v1">A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatness</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Rapid_Iterative_Trajectory_Planning_Method_for_Automated_Parking_through_Differential_Flatness.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Laboratory of Industrial Control Technology, Zhejiang University</span></p>
<p>本文提出了一种基于微分平坦性的快速迭代轨迹规划（RITP）方法，用于自动泊车场景中的路径-速度分解。该方法通过并行计算和显式碰撞检测实现高效、实时的碰撞规避，利用多项式路径规划确保车辆运动学模型约束，并在换挡点引入末端平滑约束提升控制可行性。仿真和实车实验结果表明，RITP方法在保证强控制可行性和碰撞无的同时，显著优于现有方法的计算效率和轨迹跟踪性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Rapid_Iterative_Trajectory_Planning_Method_for_Automated_Parking_through_Differential_Flatness.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-DualReg-Dual-Space-Filtering-and-Reinforcement-for-Rigid-Registrationpdf"><a href="#17-DualReg-Dual-Space-Filtering-and-Reinforcement-for-Rigid-Registrationpdf" class="headerlink" title="17. DualReg: Dual-Space Filtering and Reinforcement for Rigid Registrationpdf"></a>17. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17034v1">DualReg: Dual-Space Filtering and Reinforcement for Rigid Registration</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DualReg__Dual-Space_Filtering_and_Reinforcement_for_Rigid_Registration.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本文提出DualReg，一种面向刚性点云配准的高效鲁棒的双空间优化框架。方法上，首先设计了一种结合单点RANSAC快速过滤和三点RANSAC概率加权采样的特征对应点渐进过滤算法，随后以过滤后的特征对应点为锚点，构建局部几何代理点集，提出特征空间与几何空间协同的优化目标并高效迭代求解刚性变换。实验结果表明，该方法在3DMatch、3DLoMatch和KITTI等数据集上显著提升配准精度和速度，部分场景下相比SOTA方法提升32倍速度并保持高精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/DualReg_Dual-Space_Filtering_and_Reinforcement_for_Rigid_Registration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-WEBSIGHT-A-Vision-First-Architecture-for-Robust-Web-Agentspdf"><a href="#18-WEBSIGHT-A-Vision-First-Architecture-for-Robust-Web-Agentspdf" class="headerlink" title="18. WEBSIGHT: A Vision-First Architecture for Robust Web Agentspdf"></a>18. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16987v1">WEBSIGHT: A Vision-First Architecture for Robust Web Agents</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/WebSight__A_Vision-First_Architecture_for_Robust_Web_Agents.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>WEBSIGHT提出了一种以视觉为核心的自主网页代理架构，通过视觉-语言模型WEBSIGHT-7B，仅依赖网页截图实现UI元素识别与交互，完全摆脱HTML或DOM输入。该系统采用多代理框架（规划、推理、视觉动作、验证），通过模块化协作和短期记忆机制实现人类式网页导航，在Showdown Clicks和WebVoyager基准上超越多项业界方案，任务准确率达97.14%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/WEBSIGHT_A_Vision-First_Architecture_for_Robust_Web_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-LLM-based-Human-like-Traffic-Simulation-for-Self-driving-Testspdf"><a href="#19-LLM-based-Human-like-Traffic-Simulation-for-Self-driving-Testspdf" class="headerlink" title="19. LLM-based Human-like Traffic Simulation for Self-driving Testspdf"></a>19. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16962v1">LLM-based Human-like Traffic Simulation for Self-driving Tests</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LLM-based_Human-like_Traffic_Simulation_for_Self-driving_Tests.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>该论文提出HDSim，一种结合认知理论与大语言模型(LLM)的高真实交通仿真框架，通过分层驾驶员风格模型和感知介导行为影响机制（PMBI）生成多样且可扩展的人类驾驶行为。实验证明，HDSim可提升自动驾驶系统在仿真测试中发现安全隐患的能力（失败检测提升达68%），且生成的事故与真实世界高度一致。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LLM-based_Human-like_Traffic_Simulation_for_Self-driving_Tests.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-Drive-As-You-Like-Strategy-Level-Motion-Planning-Based-on-A-Multi-Head-Diffusion-Modelpdf"><a href="#20-Drive-As-You-Like-Strategy-Level-Motion-Planning-Based-on-A-Multi-Head-Diffusion-Modelpdf" class="headerlink" title="20. Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Modelpdf"></a>20. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16947v1">Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Model</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Drive_As_You_Like__Strategy-Level_Motion_Planning_Based_on_A_Multi-Head_Diffusion_Model.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University Malaysia</span></p>
<p>本文提出了一种基于多头扩散模型（M-Diffusion Planner）的交互式自动驾驶运动规划框架，通过DiT变体实现多策略输出，并结合GRPO（Group Relative Policy Optimization）进行策略层面微调，使各输出头学习不同驾驶风格。推理时引入大语言模型（LLM）作为语义解析器，将自然语言指令映射为策略标识，实现了用户指令驱动的实时多策略切换。实验证明该方法在nuPlan基准上获得SOTA性能，支持多样化、偏好化的实时轨迹生成。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Drive_As_You_Like_Strategy-Level_Motion_Planning_Based_on_A_Multi-Head_Diffusion_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-A-Workflow-for-Map-Creation-in-Autonomous-Vehicle-Simulationspdf"><a href="#21-A-Workflow-for-Map-Creation-in-Autonomous-Vehicle-Simulationspdf" class="headerlink" title="21. A Workflow for Map Creation in Autonomous Vehicle Simulationspdf"></a>21. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16856v1">A Workflow for Map Creation in Autonomous Vehicle Simulations</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Workflow_for_Map_Creation_in_Autonomous_Vehicle_Simulations.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ontario Tech University</span></p>
<p>本文提出了一种面向自动驾驶仿真的地图创建自定义工作流，利用OpenStreetMap数据和轻量级开源工具（如OSM2World、CloudCompare与Point Cloud Library）自动生成3D网格、点云和Lanelet2地图，简化了AWSIM与Autoware等仿真平台的高精度地图集成流程。实验在安省理工大学停车场环境验证了该方法的有效性，结论认为该流程能高效、低资源地支持仿真测试，未来将引入SLAM和增强兼容性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Workflow_for_Map_Creation_in_Autonomous_Vehicle_Simulations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-NinA-Normalizing-Flows-in-Action-Training-VLA-Models-with-Normalizing-Flowspdf"><a href="#22-NinA-Normalizing-Flows-in-Action-Training-VLA-Models-with-Normalizing-Flowspdf" class="headerlink" title="22. NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flowspdf"></a>22. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16845v1">NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/NinA__Normalizing_Flows_in_Action._Training_VLA_Models_with_Normalizing_Flows.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AIRI</span></p>
<p>该论文提出NinA，将传统Vision-Language-Action (VLA)架构中的扩散模型动作解码器替换为Normalizing Flow（NF）模型，支持高效单步采样，通过MLP和Transformer两种结构实现，极大提升推理速度并减少参数量。实验证明，NinA在LIBERO机器人基准上可与扩散模型表现相当，但推理速度快7-10倍，模型更小，适合实时机器人控制。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/NinA_Normalizing_Flows_in_Action._Training_VLA_Models_with_Normalizing_Flows.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-Variational-Shape-Inference-for-Grasp-Diffusion-on-SE-3-pdf"><a href="#23-Variational-Shape-Inference-for-Grasp-Diffusion-on-SE-3-pdf" class="headerlink" title="23. Variational Shape Inference for Grasp Diffusion on SE(3)pdf"></a>23. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17482v1">Variational Shape Inference for Grasp Diffusion on SE(3)</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Variational_Shape_Inference_for_Grasp_Diffusion_on_SE(3).pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Purdue University</span></p>
<p>本文提出了一种结合变分形状推断与扩散模型的方法，实现了基于SE(3)流形的多模态机械手抓取姿态生成。具体流程为：先利用基于PointNet的变分自动编码器对点云进行形状隐空间建模，获得稳健的几何特征，再用这些特征引导扩散模型实现抓取姿态的生成，并引入基于神经SDF的测试时抓取优化以提升抓取质量。实验结果表明，该方法在ACRONYM数据集上抓取成功率提升6.3%，且对点云稀疏和噪声具备强鲁棒性，在现实场景下实现了零样本迁移，抓取性能优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Variational_Shape_Inference_for_Grasp_Diffusion_on_SE%283%29.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-SoK-Cybersecurity-Assessment-of-Humanoid-Ecosystempdf"><a href="#24-SoK-Cybersecurity-Assessment-of-Humanoid-Ecosystempdf" class="headerlink" title="24. SoK: Cybersecurity Assessment of Humanoid Ecosystempdf"></a>24. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17481v1">SoK: Cybersecurity Assessment of Humanoid Ecosystem</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SoK__Cybersecurity_Assessment_of_Humanoid_Ecosystem.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ben-Gurion University of the Negev</span></p>
<p>该论文提出了针对人形机器人生态系统的系统性安全评估方法，构建了七层安全模型，将39类攻击与35类防御机制映射到不同架构层，并开发了RISK-MAP定量安全评估体系（含蒙特卡洛仿真），用于评估和比较不同人形机器人的安全水平。实证分析揭示了三种主流平台在物理层和上层防护上的系统性漏洞，结论强调系统化、多层次安全防护对人形机器人安全部署至关重要。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SoK_Cybersecurity_Assessment_of_Humanoid_Ecosystem.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-A-Synthetic-Dataset-for-Manometry-Recognition-in-Robotic-Applicationspdf"><a href="#25-A-Synthetic-Dataset-for-Manometry-Recognition-in-Robotic-Applicationspdf" class="headerlink" title="25. A Synthetic Dataset for Manometry Recognition in Robotic Applicationspdf"></a>25. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17468v1">A Synthetic Dataset for Manometry Recognition in Robotic Applications</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Synthetic_Dataset_for_Manometry_Recognition_in_Robotic_Applications.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of São Paulo</span></p>
<p>该论文提出并验证了一种用于工业场景（如海上石油平台）中压力计识别的混合数据生成管线，结合了BlenderProc程序化渲染与NVIDIA Cosmos-Predict2 AI驱动视频扩展，生成高质量合成图像和视频。实验表明，基于真实与合成数据1:1混合训练的YOLO检测模型在准确率和训练收敛速度均优于仅用真实数据，合成数据可有效缓解数据采集难题并提升工业机器人感知性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Synthetic_Dataset_for_Manometry_Recognition_in_Robotic_Applications.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-Optimizing-Grasping-in-Legged-Robots-A-Deep-Learning-Approach-to-Loco-Manipulationpdf"><a href="#26-Optimizing-Grasping-in-Legged-Robots-A-Deep-Learning-Approach-to-Loco-Manipulationpdf" class="headerlink" title="26. Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulationpdf"></a>26. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17466v1">Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Optimizing_Grasping_in_Legged_Robots__A_Deep_Learning_Approach_to_Loco-Manipulation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Federal University of Uberlandia</span></p>
<p>本文提出了一种基于深度学习的四足机器人抓取优化方法，核心为在Genesis仿真环境中生成抓取数据集，通过融合RGB-D图像、分割掩码和法线图训练U-Net结构卷积神经网络，预测像素级抓取质量热力图。该模型在Boston Dynamics Spot实物平台实现端到端自主导航、识别、抓取，验证了仿真到现实（sim-to-real）策略的有效性，提升了机器人在非结构化环境下的抓取精度与适应性。结论证实所提系统能在现实环境中实现高效灵活的物体抓取，但模型泛化性和感知鲁棒性有待进一步提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Optimizing_Grasping_in_Legged_Robots_A_Deep_Learning_Approach_to_Loco-Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-4D-Visual-Pre-training-for-Robot-Learningpdf"><a href="#27-4D-Visual-Pre-training-for-Robot-Learningpdf" class="headerlink" title="27. 4D Visual Pre-training for Robot Learningpdf"></a>27. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17230v1">4D Visual Pre-training for Robot Learning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/4D_Visual_Pre-training_for_Robot_Learning.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出了一种面向机器人操控的4D视觉预训练框架（FVP），通过将历史帧点云和机器人动作作为条件，利用扩散模型预测未来点云，从而预训练3D视觉表征。实验表明，FVP能显著提升3D模仿学习（如DP3、RISE）和视觉-语言-动作大模型（如RDT-1B）在多种现实机器人任务中的成功率和泛化能力，优于现有2D&#x2F;3D预训练方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/4D_Visual_Pre-training_for_Robot_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agentspdf"><a href="#28-From-reactive-to-cognitive-brain-inspired-spatial-intelligence-for-embodied-agentspdf" class="headerlink" title="28. From reactive to cognitive: brain-inspired spatial intelligence for embodied agentspdf"></a>28. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17198v1">From reactive to cognitive: brain-inspired spatial intelligence for embodied agents</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/From_reactive_to_cognitive__brain-inspired_spatial_intelligence_for_embodied_agents.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本论文提出BSC-Nav框架，结合生物启发的结构化空间记忆（包括地标、路径和地图知识）与多模态大语言模型（MLLM），实现了具备认知空间智能的自主体。方法通过显式构建地标记忆和认知地图，并利用工作记忆模块实现分层检索，以支持多模态、多粒度的导航、指令跟随、问答和移动操作任务。实验显示，BSC-Nav在多项仿真和真实环境下的导航与操作任务中，表现出色的泛化能力与效率，显著超越现有方法，推动了机器人从反应式向认知空间智能的转变。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/From_reactive_to_cognitive_brain-inspired_spatial_intelligence_for_embodied_agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-LodeStar-Long-horizon-Dexterity-via-Synthetic-Data-Augmentation-from-Human-Demonstrationspdf"><a href="#29-LodeStar-Long-horizon-Dexterity-via-Synthetic-Data-Augmentation-from-Human-Demonstrationspdf" class="headerlink" title="29. LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrationspdf"></a>29. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17547v1">LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LodeStar__Long-horizon_Dexterity_via_Synthetic_Data_Augmentation_from_Human_Demonstrations.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California San Diego</span></p>
<p>本论文提出LODESTAR系统，通过结合视觉语言基础模型自动分割人类演示为技能片段，并利用残差强化学习在仿真中合成多样化技能数据，实现从少量人类演示中学习鲁棒的长时序灵巧操作。Skill Routing Transformer策略将这些技能高效串联，实现机器人在现实环境中更强的任务完成率和泛化能力，实验在三个复杂真实任务上平均提升成功率25%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LodeStar_Long-horizon_Dexterity_via_Synthetic_Data_Augmentation_from_Human_Demonstrations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-Scene-Agnostic-Traversability-Labeling-and-Estimation-via-a-Multimodal-Self-supervised-Frameworkpdf"><a href="#30-Scene-Agnostic-Traversability-Labeling-and-Estimation-via-a-Multimodal-Self-supervised-Frameworkpdf" class="headerlink" title="30. Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Frameworkpdf"></a>30. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18249v1">Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scene-Agnostic_Traversability_Labeling_and_Estimation_via_a_Multimodal_Self-supervised_Framework.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了一种多模态自监督框架，结合足迹、LiDAR和相机信息，通过视觉基础模型与几何先验自动生成像素级可通行性标签，并采用语义-几何解耦的双流神经网络实现多模态可通行性估计。实验结果表明，该方法在多种环境下自动标注和可通行性估计均显著优于现有自监督方法，IoU提升1.6–3.5%，能有效助力下游机器人导航任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Scene-Agnostic_Traversability_Labeling_and_Estimation_via_a_Multimodal_Self-supervised_Framework.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-Arnold-a-generalist-muscle-transformer-policypdf"><a href="#31-Arnold-a-generalist-muscle-transformer-policypdf" class="headerlink" title="31. Arnold: a generalist muscle transformer policypdf"></a>31. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18066v1">Arnold: a generalist muscle transformer policy</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Arnold__a_generalist_muscle_transformer_policy.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">École Polytechnique Fédérale de Lausanne (EPFL)</span></p>
<p>该论文提出Arnold，一种基于Transformer的大规模通用肌肉控制策略，能同时处理14项涉及不同人体部位的生物力学任务，包括精细操作与行走。Arnold通过组合传感-运动词汇表、行为克隆（OBC）、PPO微调和自蒸馏，统一学习多任务多体态控制，结果显示在多数任务上达到甚至超越专家级表现，并能高效适应新任务，但其学习到的肌肉协同主要为任务特定，缺乏通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Arnold_a_generalist_muscle_transformer_policy.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-PerPilot-Personalizing-VLM-based-Mobile-Agents-via-Memory-and-Explorationpdf"><a href="#32-PerPilot-Personalizing-VLM-based-Mobile-Agents-via-Memory-and-Explorationpdf" class="headerlink" title="32. PerPilot: Personalizing VLM-based Mobile Agents via Memory and Explorationpdf"></a>32. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18040v1">PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PerPilot__Personalizing_VLM-based_Mobile_Agents_via_Memory_and_Exploration.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northwestern Polytechnical University</span></p>
<p>该论文提出PerPilot，一个基于大语言模型（LLM）的可插拔框架，通过个性化感知模块识别并提取用户指令中的个性化元素，利用记忆检索与推理探索两种方式自动补全和执行个性化指令，并构建了首个个性化移动场景数据集PerInstruct。实验表明，PerPilot大幅提升了多种移动代理系统在个性化场景下的任务成功率，且随使用次数提升个性化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PerPilot_Personalizing_VLM-based_Mobile_Agents_via_Memory_and_Exploration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-MODELING-AND-CONTROL-FRAMEWORK-FOR-AUTONOMOUS-SPACE-MANIPULATOR-HANDOVER-OPERATIONSpdf"><a href="#33-MODELING-AND-CONTROL-FRAMEWORK-FOR-AUTONOMOUS-SPACE-MANIPULATOR-HANDOVER-OPERATIONSpdf" class="headerlink" title="33. MODELING AND CONTROL FRAMEWORK FOR AUTONOMOUS SPACE MANIPULATOR HANDOVER OPERATIONSpdf"></a>33. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18039v1">MODELING AND CONTROL FRAMEWORK FOR AUTONOMOUS SPACE MANIPULATOR HANDOVER OPERATIONS</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Modeling_and_Control_Framework_for_Autonomous_Space_Manipulator_Handover_Operations.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cincinnati</span></p>
<p>本文提出了适用于空间在轨服务、组装和制造场景的高自由度双臂空间机械臂系统动力学建模方法，并对基于非线性模型预测控制（NMPC）与传统PID控制的轨迹跟踪性能进行了对比。仿真结果表明，两种控制器均能实现自主机器人间接力传递任务，但NMPC表现出更优的轨迹跟踪精度和动态响应，验证了该方法在空间机器人自主操作中的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MODELING_AND_CONTROL_FRAMEWORK_FOR_AUTONOMOUS_SPACE_MANIPULATOR_HANDOVER_OPERATIONS.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-No-Need-to-Look-Locating-and-Grasping-Objects-by-a-Robot-Arm-Covered-with-Sensitive-Skinpdf"><a href="#34-No-Need-to-Look-Locating-and-Grasping-Objects-by-a-Robot-Arm-Covered-with-Sensitive-Skinpdf" class="headerlink" title="34. No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skinpdf"></a>34. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17986v1">No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/No_Need_to_Look!_Locating_and_Grasping_Objects_by_a_Robot_Arm_Covered_with_Sensitive_Skin.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Czech Technical University in Prague</span></p>
<p>本论文提出了一种完全基于触觉反馈、无需视觉输入的机器人抓取方案，核心创新在于用覆盖全臂的人工皮肤进行粗略扫描，结合末端力&#x2F;扭矩传感器实现精确定位与抓取。系统在真实和仿真环境中评估，结果显示对不同形状、位置及多物体场景均有较高成功率，且速度为只用末端触觉的6倍，适用于视觉感知受限场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/No_Need_to_Look%21_Locating_and_Grasping_Objects_by_a_Robot_Arm_Covered_with_Sensitive_Skin.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-Neural-Algorithmic-Reasoners-informed-Large-Language-Model-for-Multi-Agent-Path-Findingpdf"><a href="#35-Neural-Algorithmic-Reasoners-informed-Large-Language-Model-for-Multi-Agent-Path-Findingpdf" class="headerlink" title="35. Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Findingpdf"></a>35. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17971v1">Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Neural_Algorithmic_Reasoners_informed_Large_Language_Model_for_Multi-Agent_Path_Finding.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">State Key Laboratory of Complex &amp; Critical Software Environment, Beihang University</span></p>
<p>本文提出LLM-NAR框架，将图神经网络（GNN）为核心的神经算法推理器（NAR）与大型语言模型（LLM）通过跨注意力机制融合，针对多智能体路径规划（MAPF）任务。方法包括基于场景的LLM提示工程、GNN预训练表征空间信息、以及跨注意力融合，极大提升了MAPF任务中LLM的空间理解与决策能力。实验表明，无论在仿真还是实际机器人实验中，LLM-NAR在成功率和路径效率上均显著优于现有LLM方法，且训练步骤和执行时间均大幅减少。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Neural_Algorithmic_Reasoners_informed_Large_Language_Model_for_Multi-Agent_Path_Finding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-Egocentric-Instruction-oriented-Affordance-Prediction-via-Large-Multimodal-Modelpdf"><a href="#36-Egocentric-Instruction-oriented-Affordance-Prediction-via-Large-Multimodal-Modelpdf" class="headerlink" title="36. Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Modelpdf"></a>36. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17922v1">Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Egocentric_Instruction-oriented_Affordance_Prediction_via_Large_Multimodal_Model.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xidian University</span></p>
<p>本文提出了一个以任务&#x2F;指令为导向的仿生视角可供性预测新任务，强调同一物体在不同操作指令下的可供性应动态变化。为此，作者构建了包含近1.5万个对象-指令-可供性三元组的大规模EIVA数据集，并开发了基于大规模多模态模型（LMMs）的“search against verifiers”自反式推理框架，通过Actor与Verifier的交互迭代优化可供性预测，显著提升了LMM在实际机器人操作场景下的泛化和指令适应能力。实验结果显示，所提方法在新任务上为指令导向的可供性预测建立了强基线，并在真实机器人仿真中获得了优异表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Egocentric_Instruction-oriented_Affordance_Prediction_via_Large_Multimodal_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-Physical-Embodiment-Enables-Information-Processing-Beyond-Explicit-Sensing-in-Active-Matterpdf"><a href="#37-Physical-Embodiment-Enables-Information-Processing-Beyond-Explicit-Sensing-in-Active-Matterpdf" class="headerlink" title="37. Physical Embodiment Enables Information Processing Beyond Explicit Sensing in Active Matterpdf"></a>37. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17921v1">Physical Embodiment Enables Information Processing Beyond Explicit Sensing in Active Matter</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Physical_Embodiment_Enables_Information_Processing_Beyond_Explicit_Sensing_in_Active_Matter.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Leipzig University</span></p>
<p>该论文提出了一种通过物理体现（embodiment）实现信息处理的新范式，不依赖显式传感，仅利用自身动力学与环境交互进行适应。研究中，作者通过强化学习（PPO算法）实时控制自驱动微型机器人（光驱动自热泳粒子），实验验证其可在未知流场扰动下，仅凭位置和位移反馈学习到对抗流场的导航策略，实现复杂环境中的自主导航。结论：物理体现可作为微观机器人信息处理的资源，无需复杂传感器即可适应静态及动态流场，为微型自主系统和仿生计算开辟新路径。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Physical_Embodiment_Enables_Information_Processing_Beyond_Explicit_Sensing_in_Active_Matter.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-CubeDN-Real-time-Drone-Detection-in-3D-Space-from-Dual-mmWave-Radar-Cubespdf"><a href="#38-CubeDN-Real-time-Drone-Detection-in-3D-Space-from-Dual-mmWave-Radar-Cubespdf" class="headerlink" title="38. CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubespdf"></a>38. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17831v1">CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubes</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CubeDN__Real-time_Drone_Detection_in_3D_Space_from_Dual_mmWave_Radar_Cubes.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University College London</span></p>
<p>该论文提出了一种基于双毫米波雷达的3D无人机检测系统CubeDN，采用创新的4D雷达数据融合和3D卷积神经网络，实现了对不同尺寸无人机的实时检测、定位与分类。实验显示CubeDN在复杂环境下平均精度达95%、召回率85%，且追踪误差低至分米级，显著优于现有方法，并具备10Hz实时推理能力，适用于实际无人机监控场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CubeDN_Real-time_Drone_Detection_in_3D_Space_from_Dual_mmWave_Radar_Cubes.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-AgentRAN-An-Agentic-AI-Architecture-for-Autonomous-Control-of-Open-6G-Networkspdf"><a href="#39-AgentRAN-An-Agentic-AI-Architecture-for-Autonomous-Control-of-Open-6G-Networkspdf" class="headerlink" title="39. AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networkspdf"></a>39. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17778v1">AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networks</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AgentRAN__An_Agentic_AI_Architecture_for_Autonomous_Control_of_Open_6G_Networks.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northeastern University</span></p>
<p>该论文提出AgentRAN，一种面向开放6G网络的AI原生agent架构，利用大型语言模型（LLM）驱动的智能体，通过自然语言（NL）意图解释和分解，实现无线接入网（RAN）中跨时空和协议层的自治控制。方法包括多层次agent协作、AI-RAN Factory自动合成新agent及持续自学习机制，实验在真实5G测试床中验证了系统能根据NL意图动态优化上行调度和功控，显著提升了网络自适应性和可解释性。结论：AgentRAN能够实现无线网络基于自然语言自适应、高效与透明的自动化控制，推动AI-for-RAN愿景落地。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/AgentRAN_An_Agentic_AI_Architecture_for_Autonomous_Control_of_Open_6G_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-DroneKey-Drone-3D-Pose-Estimation-in-Image-Sequences-using-Gated-Key-representation-and-Pose-adaptive-Learningpdf"><a href="#40-DroneKey-Drone-3D-Pose-Estimation-in-Image-Sequences-using-Gated-Key-representation-and-Pose-adaptive-Learningpdf" class="headerlink" title="40. DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learningpdf"></a>40. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17746v1">DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DroneKey__Drone_3D_Pose_Estimation_in_Image_Sequences_using_Gated_Key-representation_and_Pose-adapti.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chonnam National University</span></p>
<p>该论文提出了DroneKey框架，通过结合transformer编码器的中间和紧凑特征表示，并采用门控加权求和方式，实现了无人机螺旋桨关键点的高精度检测，并通过PnP求解器完成无人机3D姿态估计。提出的姿态自适应Mahalanobis损失函数提升了极端姿态下关键点预测的稳定性和准确性。实验结果显示，方法在关键点检测和3D姿态估计上均优于现有方法，具备实时性和高准确率，适用于反无人机系统。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/DroneKey_Drone_3D_Pose_Estimation_in_Image_Sequences_using_Gated_Key-representation_and_Pose-adaptive_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-MEVITA-Open-Source-Bipedal-Robot-Assembled-from-E-Commerce-Components-via-Sheet-Metal-Weldingpdf"><a href="#41-MEVITA-Open-Source-Bipedal-Robot-Assembled-from-E-Commerce-Components-via-Sheet-Metal-Weldingpdf" class="headerlink" title="41. MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Weldingpdf"></a>41. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17684v1">MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Welding</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MEVITA__Open-Source_Bipedal_Robot_Assembled_from_E-Commerce_Components_via_Sheet_Metal_Welding.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Tokyo</span></p>
<p>本文提出了一种新型开源双足机器人MEVITA，采用电商可采购的金属部件，通过钣金焊接技术极大减少零件数量，实现了简易组装。该机器人结合强化学习仿真与Sim-to-Real迁移，在多种环境下实现了稳定步行，验证了结构与控制方法的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MEVITA_Open-Source_Bipedal_Robot_Assembled_from_E-Commerce_Components_via_Sheet_Metal_Welding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-SEBVS-Synthetic-Event-based-Visual-Servoing-for-Robot-Navigation-and-Manipulationpdf"><a href="#42-SEBVS-Synthetic-Event-based-Visual-Servoing-for-Robot-Navigation-and-Manipulationpdf" class="headerlink" title="42. SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulationpdf"></a>42. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17643v1">SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SEBVS__Synthetic_Event-based_Visual_Servoing_for_Robot_Navigation_and_Manipulation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Arizona State University</span></p>
<p>该论文提出了SEBVS，一个集成于Gazebo仿真的开源ROS2包，实现了基于v2e的视频转事件流仿真，支持机器人在模拟环境下无专用硬件地获取事件流。作者设计了融合RGB和事件帧的Transformer架构，用于端到端学习导航与操作策略（ERPNav和ERPArm），通过行为克隆训练并在两项任务（移动机器人跟踪与机械臂抓取）上评测。实验结果显示，融合事件流的策略在追踪精度、抓取成功率和鲁棒性上均优于仅用RGB或仅用事件流的基线模型，验证了事件视觉在机器人实时控制中的优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SEBVS_Synthetic_Event-based_Visual_Servoing_for_Robot_Navigation_and_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-GWM-Towards-Scalable-Gaussian-World-Models-for-Robotic-Manipulationpdf"><a href="#43-GWM-Towards-Scalable-Gaussian-World-Models-for-Robotic-Manipulationpdf" class="headerlink" title="43. GWM: Towards Scalable Gaussian World Models for Robotic Manipulationpdf"></a>43. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17600v1">GWM: Towards Scalable Gaussian World Models for Robotic Manipulation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GWM__Towards_Scalable_Gaussian_World_Models_for_Robotic_Manipulation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了Gaussian World Model (GWM)，通过结合3D Gaussian Splatting表示、3D变分自编码器（VAE）和潜变量扩散Transformer（DiT），实现对机器人操作中未来场景的高精度三维动态建模。GWM不仅提升了仿真和真实场景下的动作条件三维视频预测能力，还显著增强了模仿学习与基于模型的强化学习中的策略学习表现，实验显示在多项任务中超越现有方法，具备较强的数据扩展性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GWM_Towards_Scalable_Gaussian_World_Models_for_Robotic_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-FlowVLA-Thinking-in-Motion-with-a-Visual-Chain-of-Thoughtpdf"><a href="#44-FlowVLA-Thinking-in-Motion-with-a-Visual-Chain-of-Thoughtpdf" class="headerlink" title="44. FlowVLA: Thinking in Motion with a Visual Chain of Thoughtpdf"></a>44. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18269v1">FlowVLA: Thinking in Motion with a Visual Chain of Thought</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FlowVLA__Thinking_in_Motion_with_a_Visual_Chain_of_Thought.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HKUST(GZ)</span></p>
<p>本文提出了Visual Chain of Thought (Visual CoT) 预训练框架，通过引入中间光流（motion dynamics）预测步骤（vt → ft → vt+1），在单一自回归Transformer中实现显式运动推理与视觉表征解耦。最终，FlowVLA在机器人操作基准上取得了最优表现，并显著提升了样本效率，验证了显式运动推理对提升感知与控制桥接能力的价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/FlowVLA_Thinking_in_Motion_with_a_Visual_Chain_of_Thought.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-The-Effects-of-Communication-Delay-on-Human-Performance-and-Neurocognitive-Responses-in-Mobile-Robot-Teleoperationpdf"><a href="#45-The-Effects-of-Communication-Delay-on-Human-Performance-and-Neurocognitive-Responses-in-Mobile-Robot-Teleoperationpdf" class="headerlink" title="45. The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperationpdf"></a>45. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18074v1">The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_Effects_of_Communication_Delay_on_Human_Performance_and_Neurocognitive_Responses_in_Mobile_Robot.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>本文通过人机回路实验，结合移动机器人行为数据和脑电（EEG）信号，系统分析通信延迟（0-500ms）对远程操作任务中的人类操作性能与神经认知响应的影响。实验发现，在200-300ms延迟范围内，任务效率和准确性首次出现显著下降，EEG分析揭示了与延迟高度相关的脑区特征（额叶θ&#x2F;β波、顶叶α波），并界定了人类对延迟的早期感知阈值（100-200ms）和认知资源分配极限（400ms）。结论为远程操作系统延迟补偿机制和安全参数设计提供了神经认知依据。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/The_Effects_of_Communication_Delay_on_Human_Performance_and_Neurocognitive_Responses_in_Mobile_Robot_Teleoperation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-Integration-of-Computer-Vision-with-Adaptive-Control-for-Autonomous-Driving-Using-ADOREpdf"><a href="#46-Integration-of-Computer-Vision-with-Adaptive-Control-for-Autonomous-Driving-Using-ADOREpdf" class="headerlink" title="46. Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADOREpdf"></a>46. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17985v1">Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADORE</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Integration_of_Computer_Vision_with_Adaptive_Control_for_Autonomous_Driving_Using_ADORE.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Siegen</span></p>
<p>该论文提出将YOLOv8深度学习视觉模型与ADORE自适应控制框架相结合，通过CARLA仿真平台和ROS桥实现感知、决策与控制模块的实时协同。实验结果表明，系统在不同天气条件下能高精度检测交通标志并快速适应车速变化，提升了自动驾驶安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Integration_of_Computer_Vision_with_Adaptive_Control_for_Autonomous_Driving_Using_ADORE.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-A-holistic-perception-system-of-internal-and-external-monitoring-for-ground-autonomous-vehicles-AutoTRUST-paradigmpdf"><a href="#47-A-holistic-perception-system-of-internal-and-external-monitoring-for-ground-autonomous-vehicles-AutoTRUST-paradigmpdf" class="headerlink" title="47. A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigmpdf"></a>47. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17969v1">A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_holistic_perception_system_of_internal_and_external_monitoring_for_ground_autonomous_vehicles__Aut.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Industrial Systems Institute, ATHENA Research Center</span></p>
<p>该论文提出了AutoTRUST框架，融合了车内多摄像头情感识别、驾驶员分心检测、人脸识别、智能空气质量与热舒适监测，以及基于LiDAR的高效外部环境感知（低成本LiDAR超分辨率语义分割），还集成了虚拟助手，全部在嵌入式设备上实现。实车部署与实验显示，该系统能在多样环境下实时、高效提升自动驾驶车辆的安全性、舒适度和体验，具备良好可扩展性和准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_holistic_perception_system_of_internal_and_external_monitoring_for_ground_autonomous_vehicles_AutoTRUST_paradigm.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="48-Structural-Damage-Detection-Using-AI-Super-Resolution-and-Visual-Language-Modelpdf"><a href="#48-Structural-Damage-Detection-Using-AI-Super-Resolution-and-Visual-Language-Modelpdf" class="headerlink" title="48. Structural Damage Detection Using AI Super Resolution and Visual Language Modelpdf"></a>48. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17130v1">Structural Damage Detection Using AI Super Resolution and Visual Language Model</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Structural_Damage_Detection_Using_AI_Super_Resolution_and_Visual_Language_Model.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Dakota State University</span></p>
<p>该论文提出了一种结合无人机航拍、AI视频超分辨率（Video Restoration Transformer, VRT）与视觉语言模型（Gemma3:27b）的结构损伤检测框架。方法流程包括低分辨率灾难现场视频增强、结构损伤识别与四类损伤分级（无&#x2F;轻微、中度、重度、完全毁坏），并在土耳其地震和美国龙卷风数据集上验证，分类准确率达84.5%。结论显示该系统能快速、低成本地提升灾害损伤评估的精度与响应效率，且普通用户也可使用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Structural_Damage_Detection_Using_AI_Super_Resolution_and_Visual_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-CE-RS-SBCIT-A-Novel-Channel-Enhanced-Hybrid-CNN–Transformer-with-Residual-Spatial-and-Boundary-Aware-Learning-for-Brain-Tumor-MRI-Analysispdf"><a href="#49-CE-RS-SBCIT-A-Novel-Channel-Enhanced-Hybrid-CNN–Transformer-with-Residual-Spatial-and-Boundary-Aware-Learning-for-Brain-Tumor-MRI-Analysispdf" class="headerlink" title="49. CE-RS-SBCIT: A Novel Channel-Enhanced Hybrid CNN–Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysispdf"></a>49. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17128v1">CE-RS-SBCIT: A Novel Channel-Enhanced Hybrid CNN–Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysis</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CE-RS-SBCIT_A_Novel_Channel_Enhanced_Hybrid_CNN_Transformer_with_Residual,_Spatial,_and_Boundary-Awa.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ibadat International University</span></p>
<p>本论文提出了CE-RS-SBCIT混合诊断框架，结合残差学习与空间学习CNN、边界与平滑操作的定制Transformer模块，并创新引入通道增强与空间注意力机制，有效融合局部细粒度和全局语境特征。实验证明该方法在多类脑肿瘤MRI分类中准确率达98.3%，敏感性、F1分数和精度均显著优于现有CNN、ViT及混合模型，具备高效、稳健和泛化的医学图像自动诊断潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CE-RS-SBCIT_A_Novel_Channel-Enhanced_Hybrid_CNN%E2%80%93Transformer_with_Residual_Spatial_and_Boundary-Aware_Learning_for_Brain_Tumor_MRI_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-Proximal-Vision-Transformer-Enhancing-Feature-Representation-through-Two-Stage-Manifold-Geometrypdf"><a href="#50-Proximal-Vision-Transformer-Enhancing-Feature-Representation-through-Two-Stage-Manifold-Geometrypdf" class="headerlink" title="50. Proximal Vision Transformer: Enhancing Feature Representation through Two-Stage Manifold Geometrypdf"></a>50. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17081v1">Proximal Vision Transformer: Enhancing Feature Representation through Two-Stage Manifold Geometry</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Proximal_Vision_Transformer__Enhancing_Feature_Representation_through_Two-Stage_Manifold_Geometry.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">North Carolina State University</span></p>
<p>该论文提出了Proximal Vision Transformer（ViT+Prox）框架，将Vision Transformer的自注意力机制构造的流形切丛与可学习的近端算子结合，实现了局部与全局几何特征的统一优化。通过在ViT输出后引入近端迭代，定义切丛截面并投影数据至基空间，有效提升了分类准确率、特征分布的结构化表现，尤其在高分辨率视觉任务上表现显著。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Proximal_Vision_Transformer_Enhancing_Feature_Representation_through_Two-Stage_Manifold_Geometry.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-SSG-DiT-A-Spatial-Signal-Guided-Framework-for-Controllable-Video-Generationpdf"><a href="#51-SSG-DiT-A-Spatial-Signal-Guided-Framework-for-Controllable-Video-Generationpdf" class="headerlink" title="51. SSG-DiT: A Spatial Signal Guided Framework for Controllable Video Generationpdf"></a>51. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17062v1">SSG-DiT: A Spatial Signal Guided Framework for Controllable Video Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SSG-Dit__A_Spatial_Signal_Guided_Framework_for_Controllable_Video_Generation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>该论文提出了SSG-DiT框架，用于可控视频生成，采用两阶段流程：首先通过预训练CLIP模型提取中间特征并融合，生成语义相关的空间视觉提示；然后利用轻量型SSG-Adapter，将视觉提示与文本条件注入冻结的Diffusion Transformer（DiT）主干，实现精准视频内容控制。实验结果表明，SSG-DiT在VBench基准的空间关系控制和一致性方面超过现有方法，显著提升了视频生成的语义一致性与细节还原能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SSG-DiT_A_Spatial_Signal_Guided_Framework_for_Controllable_Video_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-REGEN-Real-Time-Photorealism-Enhancement-in-Games-via-a-Dual-Stage-Generative-Network-Frameworkpdf"><a href="#52-REGEN-Real-Time-Photorealism-Enhancement-in-Games-via-a-Dual-Stage-Generative-Network-Frameworkpdf" class="headerlink" title="52. REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Frameworkpdf"></a>52. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17061v1">REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/REGEN__Real-Time_Photorealism_Enhancement_in_Games_via_a_Dual-Stage_Generative_Network_Framework.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Aristotle University of Thessaloniki</span></p>
<p>该论文提出了REGEN框架，通过先用鲁棒的非实时无监督Im2Im生成高质量光照真实感对齐数据，然后将任务转化为更简单的有监督配对Im2Im任务，利用轻量级Pix2PixHD模型实现游戏画面实时光照真实感增强。实验在GTA V与Cityscapes数据集上验证，REGEN在保持与SOTA方法相当的视觉质量的同时，推理速度提升32倍，可实时应用于游戏画面增强。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/REGEN_Real-Time_Photorealism_Enhancement_in_Games_via_a_Dual-Stage_Generative_Network_Framework.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-PVNet-Point-Voxel-Interaction-LiDAR-Scene-Upsampling-Via-Diffusion-Modelspdf"><a href="#53-PVNet-Point-Voxel-Interaction-LiDAR-Scene-Upsampling-Via-Diffusion-Modelspdf" class="headerlink" title="53. PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Modelspdf"></a>53. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17050v1">PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PVNet__Point-Voxel_Interaction_LiDAR_Scene_Upsampling_Via_Diffusion_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Telecom Guizhou Branch</span></p>
<p>该论文提出PVNet，一种基于扩散模型的点-体素交互框架，实现室外LiDAR点云的无密集监督场景级插值。方法包括分类器无关指导的DDPM，体素补全模块和点-体素交互模块，有效提升复杂场景的点云密度和环境感知。实验表明，在多个数据集和任意插值倍率下均达到了最优性能，显著提升了3D场景感知和理解能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PVNet_Point-Voxel_Interaction_LiDAR_Scene_Upsampling_Via_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-F4-ITS-Fine-grained-Feature-Fusion-for-Food-Image-Text-Searchpdf"><a href="#54-F4-ITS-Fine-grained-Feature-Fusion-for-Food-Image-Text-Searchpdf" class="headerlink" title="54. F4-ITS: Fine-grained Feature Fusion for Food Image-Text Searchpdf"></a>54. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17037v1">F4-ITS: Fine-grained Feature Fusion for Food Image-Text Search</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/F4-ITS__Fine-grained_Feature_Fusion_for_Food_Image-Text_Search.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HyperVerge</span></p>
<p>该论文提出了一种无训练、由视觉语言模型（VLM）指导的多模态特征融合框架F4-ITS，用于提升食物图像与文本描述的检索性能。其主要方法包括图像与VLM生成文本描述的单向和双向融合，以及基于食材级别特征的检索候选重排序。实验结果显示，该方法显著提升了检索准确率，尤其在资源受限环境下小模型也能达到甚至超过大模型。结论认为F4-ITS能有效改善细粒度食物图像文本检索，并可推广至其它多模态检索任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/F4-ITS_Fine-grained_Feature_Fusion_for_Food_Image-Text_Search.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-Contrastive-Prompt-Clustering-for-Weakly-Supervised-Semantic-Segmentationpdf"><a href="#55-Contrastive-Prompt-Clustering-for-Weakly-Supervised-Semantic-Segmentationpdf" class="headerlink" title="55. Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentationpdf"></a>55. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17009v1">Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Contrastive_Prompt_Clustering_for_Weakly_Supervised_Semantic_Segmentation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong-Liverpool University</span></p>
<p>该论文提出了一种新颖的弱监督语义分割框架CPC，利用大型语言模型（LLM）自动生成类别聚类，作为语义先验引入ViT主干网络，并设计了类感知的patch级对比损失以增强类别内一致性和区分类间边界。实验在PASCAL VOC 2012与MS COCO 2014数据集上验证了CPC在弱监督分割任务中的领先性能，显著提升了伪标签质量和分割精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Contrastive_Prompt_Clustering_for_Weakly_Supervised_Semantic_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-HiCache-Training-free-Acceleration-of-Diffusion-Models-via-Hermite-Polynomial-based-Feature-Cachingpdf"><a href="#56-HiCache-Training-free-Acceleration-of-Diffusion-Models-via-Hermite-Polynomial-based-Feature-Cachingpdf" class="headerlink" title="56. HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Cachingpdf"></a>56. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16984v1">HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HiCache__Training-free_Acceleration_of_Diffusion_Models_via_Hermite_Polynomial-based_Feature_Caching.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出HiCache，一种无需重新训练的扩散模型加速框架，通过对特征演化的高斯分布性进行理论验证，将Hermite多项式引入特征缓存预测，并设计了双缩放机制以提升数值稳定性和预测精度。实验表明，HiCache在文本生成图像、视频生成、超分辨率等任务上实现最高6.24倍加速，并在生成质量上优于同类方法，证明了按数据统计性质选择预测基函数的理论和实践价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/HiCache_Training-free_Acceleration_of_Diffusion_Models_via_Hermite_Polynomial-based_Feature_Caching.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-Combating-Digitally-Altered-Images-Deepfake-Detectionpdf"><a href="#57-Combating-Digitally-Altered-Images-Deepfake-Detectionpdf" class="headerlink" title="57. Combating Digitally Altered Images: Deepfake Detectionpdf"></a>57. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16975v1">Combating Digitally Altered Images: Deepfake Detection</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Combating_Digitally_Altered_Images__Deepfake_Detection.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Amrita School of Computing</span></p>
<p>该论文提出了一种基于Vision Transformer（ViT）模型的深度伪造（Deepfake）图像检测方法，采用OpenForensics数据集并结合多种数据增强技术以提升模型对多样化图像篡改的鲁棒性。实验结果表明，该模型在测试集上达到了≥99%的分类准确率，表现出极高的检测性能和实际应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Combating_Digitally_Altered_Images_Deepfake_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-Hierarchical-Contextual-Grounding-LVLM-Enhancing-Fine-Grained-Visual-Language-Understanding-with-Robust-Groundingpdf"><a href="#58-Hierarchical-Contextual-Grounding-LVLM-Enhancing-Fine-Grained-Visual-Language-Understanding-with-Robust-Groundingpdf" class="headerlink" title="58. Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Groundingpdf"></a>58. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16974v1">Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Grounding</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Hierarchical_Contextual_Grounding_LVLM__Enhancing_Fine-Grained_Visual-Language_Understanding_with_Ro.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhongkai University of Agriculture and Engineering</span></p>
<p>该论文提出了层次化上下文感知视觉语言大模型（HCG-LVLM），通过全球上下文感知层和细粒度局部定位层相结合，提升视觉-语言模型在精细视觉推理和精准区域定位方面的能力。方法包括局部细节增强模块和语义一致性验证器，有效减少幻觉现象并提升模型在GQA、A-OKVQA和RefCOCO等数据集上的准确率，结论证实该结构在细粒度视觉语言理解中优于现有主流模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Hierarchical_Contextual_Grounding_LVLM_Enhancing_Fine-Grained_Visual-Language_Understanding_with_Robust_Grounding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-Disentangling-Polysemantic-Neurons-with-a-Null-Calibrated-Polysemanticity-Index-and-Causal-Patch-Interventionspdf"><a href="#59-Disentangling-Polysemantic-Neurons-with-a-Null-Calibrated-Polysemanticity-Index-and-Causal-Patch-Interventionspdf" class="headerlink" title="59. Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventionspdf"></a>59. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16950v1">Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventions</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Disentangling_Polysemantic_Neurons_with_a_Null-Calibrated_Polysemanticity_Index_and_Causal_Patch_Int.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">BITS Pilani</span></p>
<p>本论文提出了Polysemanticity Index (PSI)——一种结合几何聚类质量、类别标签对齐与CLIP开放词汇语义区分度的多证据、显著性校准指标，用于系统量化和发现神经网络中的多语义神经元。通过对ResNet-50在Tiny-ImageNet数据集上的实验，PSI不仅能区分真实结构与噪声，还揭示了深层网络更易出现多语义性，并通过因果性patch-swap实验证明了发现原型的功能相关性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Disentangling_Polysemantic_Neurons_with_a_Null-Calibrated_Polysemanticity_Index_and_Causal_Patch_Interventions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-HieroAction-Hierarchically-Guided-VLM-for-Fine-Grained-Action-Analysispdf"><a href="#60-HieroAction-Hierarchically-Guided-VLM-for-Fine-Grained-Action-Analysispdf" class="headerlink" title="60. HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysispdf"></a>60. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16942v1">HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysis</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HieroAction__Hierarchically_Guided_VLM_for_Fine-Grained_Action_Analysis.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>该论文提出HieroAction，一种结合视觉-语言模型的分层动作分析方法，包括“逐步动作推理”（SAR）与“分层策略学习”（HPL）。SAR模块将复杂动作分解为观察、识别、评估与结论四阶段，HPL利用多维奖励函数通过强化学习优化分阶段动作质量评分。实验显示，HieroAction在FineDive、FineFS和LOGO数据集上实现了高准确率和可解释性，显著优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/HieroAction_Hierarchically_Guided_VLM_for_Fine-Grained_Action_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-LGE-Guided-Cross-Modality-Contrastive-Learning-for-Gadolinium-Free-Cardiomyopathy-Screening-in-Cine-CMRpdf"><a href="#61-LGE-Guided-Cross-Modality-Contrastive-Learning-for-Gadolinium-Free-Cardiomyopathy-Screening-in-Cine-CMRpdf" class="headerlink" title="61. LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMRpdf"></a>61. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16927v1">LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMR</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LGE-Guided_Cross-Modality_Contrastive_Learning_for_Gadolinium-Free_Cardiomyopathy_Screening_in_Cine_.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ShanghaiTech University</span></p>
<p>本文提出了CC-CMR框架，通过对比学习和跨模态特征对齐，将LGE（晚期钆增强）序列的病理特征隐式编码进无钆cine CMR序列中，利用Video Swin Transformer进行特征提取，并通过特征交互模块联合优化分类与特征对齐，同时引入不确定性感知机制动态调整目标权重。实验结果显示，该方法在多中心数据上显著提升了无钆条件下心肌病筛查的准确性（94.3%），优于现有模型，具备临床可行性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LGE-Guided_Cross-Modality_Contrastive_Learning_for_Gadolinium-Free_Cardiomyopathy_Screening_in_Cine_CMR.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-MDD-A-Dataset-for-Text-and-Music-Conditioned-Duet-Dance-Generationpdf"><a href="#62-MDD-A-Dataset-for-Text-and-Music-Conditioned-Duet-Dance-Generationpdf" class="headerlink" title="62. MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generationpdf"></a>62. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16911v1">MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MDD__A_Dataset_for_Text-and-Music_Conditioned_Duet_Dance_Generation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Purdue University</span></p>
<p>该论文提出了MDD，一个结合文本、音乐和动作数据的大规模多模态双人舞蹈生成数据集。MDD涵盖15种舞蹈类型，包含620分钟高质量动作捕捉数据和超1万条细粒度文本注释，并支持两个新任务：Text-to-Duet（文本+音乐条件下生成双人舞动作）和Text-to-Dance Accompaniment（文本+音乐+领舞动作条件下生成跟随者动作）。实验表明，MDD极大提升了多模态交互式舞蹈生成的基准和模型泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MDD_A_Dataset_for_Text-and-Music_Conditioned_Duet_Dance_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-Generating-Synthetic-Contrast-Enhanced-Chest-CT-Images-from-Non-Contrast-Scans-Using-Slice-Consistent-Brownian-Bridge-Diffusion-Networkpdf"><a href="#63-Generating-Synthetic-Contrast-Enhanced-Chest-CT-Images-from-Non-Contrast-Scans-Using-Slice-Consistent-Brownian-Bridge-Diffusion-Networkpdf" class="headerlink" title="63. Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Networkpdf"></a>63. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16897v1">Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Network</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Generating_Synthetic_Contrast-Enhanced_Chest_CT_Images_from_Non-Contrast_Scans_Using_Slice-Consisten.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Saskatchewan</span></p>
<p>本文提出了一种基于Slice-Consistent Brownian Bridge Diffusion Model (SC-BBDM)的桥扩散方法，可从非增强CT生成合成的增强胸部CT影像。通过引入风格键条件（SKC）和切片间轨迹对齐（ISTA）机制，结合严格的预处理（配准和分割掩码），该方法在Coltea-Lung数据集上对比基线模型，在保持解剖结构和增强对比度方面表现更优。结论表明该方法在临床相关区域提升了图像保真度和空间一致性，有望提高无造影剂CT的临床可用性并降低医疗成本。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Generating_Synthetic_Contrast-Enhanced_Chest_CT_Images_from_Non-Contrast_Scans_Using_Slice-Consistent_Brownian_Bridge_Diffusion_Network.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-A-Lightweight-Convolution-and-Vision-Transformer-integrated-model-with-Multi-scale-Self-attention-Mechanismpdf"><a href="#64-A-Lightweight-Convolution-and-Vision-Transformer-integrated-model-with-Multi-scale-Self-attention-Mechanismpdf" class="headerlink" title="64. A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanismpdf"></a>64. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16884v1">A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanism</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Lightweight_Convolution_and_Vision_Transformer_integrated_model_with_Multi-scale_Self-attention_Me.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sichuan University</span></p>
<p>本文提出了一种融合卷积神经网络（CNN）和视觉Transformer（ViT）的轻量级模型SAEViT，采用多尺度稀疏自注意力机制（SAA）和通道交互前馈网络（CIFFN），以提升特征表达能力并降低计算成本。实验表明，SAEViT在ImageNet-1K分类、COCO目标检测和ADE20K语义分割等主流视觉任务上实现了高准确率和高推理速度，优于同类轻量级模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Lightweight_Convolution_and_Vision_Transformer_integrated_model_with_Multi-scale_Self-attention_Mechanism.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-Multimodal-Medical-Endoscopic-Image-Analysis-via-Progressive-Disentangle-aware-Contrastive-Learningpdf"><a href="#65-Multimodal-Medical-Endoscopic-Image-Analysis-via-Progressive-Disentangle-aware-Contrastive-Learningpdf" class="headerlink" title="65. Multimodal Medical Endoscopic Image Analysis via Progressive Disentangle-aware Contrastive Learningpdf"></a>65. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16882v1">Multimodal Medical Endoscopic Image Analysis via Progressive Disentangle-aware Contrastive Learning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multimodal_Medical_Endoscopic_Image_Analysis_via_Progressive_Disentangle-aware_Contrastive_Learning.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</span></p>
<p>该论文提出了一种基于“对齐-解耦-融合”(Align-Disentangle-Fusion)机制的多模态表示学习框架，用于2D内镜医学图像分割，集成了白光成像(WLI)与窄带成像(NBI)。方法包括多尺度分布对齐、多阶段特征解耦与解耦感知对比学习，实现了模态差异消除和鲁棒的语义融合。实验表明，该方法在多个临床数据集上分割精度优于现有方法，提升了模型泛化和实际应用价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Multimodal_Medical_Endoscopic_Image_Analysis_via_Progressive_Disentangle-aware_Contrastive_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-Do-Multimodal-LLMs-See-Sentiment-pdf"><a href="#66-Do-Multimodal-LLMs-See-Sentiment-pdf" class="headerlink" title="66. Do Multimodal LLMs See Sentiment?pdf"></a>66. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16873v1">Do Multimodal LLMs See Sentiment?</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Do_Multimodal_LLMs_See_Sentiment_.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universidade Tecnologica Federal do Paraná</span></p>
<p>本文提出MLLMsent框架，系统性探索多模态大语言模型（如MiniGPT-4、GPT-4o mini、DeepSeek-VL2-Tiny）在图像情感分析中的推理能力。方法包含三种流程：直接用MLLM对图片情感分类；利用MLLM自动生成图片描述并用LLM分析情感；对LLM在情感标注图像描述上进行微调。实验证明，微调后模型在主流基准数据集上显著优于词典、CNN、Transformer等传统方法，在跨数据集测试中也表现出优异的泛化能力。结论：MLLMsent在视觉情感分析任务上达到新SOTA，并提升了解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Do_Multimodal_LLMs_See_Sentiment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-Delta-SVD-Efficient-Compression-for-Personalized-Text-to-Image-Modelspdf"><a href="#67-Delta-SVD-Efficient-Compression-for-Personalized-Text-to-Image-Modelspdf" class="headerlink" title="67. Delta-SVD: Efficient Compression for Personalized Text-to-Image Modelspdf"></a>67. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16863v1">Delta-SVD: Efficient Compression for Personalized Text-to-Image Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Delta-SVD__Efficient_Compression_for_Personalized_Text-to-Image_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University</span></p>
<p>本文提出Delta-SVD，一种针对DreamBooth等个性化文本到图像扩散模型的训练后、免训练压缩方法。该方法通过对微调后与预训练模型权重的差分（delta weights）进行逐层SVD分解，并采用能量阈值自适应截断，得到低秩近似表示，实现大幅度减小存储空间、完全保留原有模型结构且无需额外训练。实验表明，在大幅压缩存储需求的同时，Delta-SVD几乎不损失图像生成质量（在CLIP、SSIM、FID等指标上与全量微调模型相当），显著优于LoRA和8bit量化等方法，适合高效部署和分发大量个性化扩散模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Delta-SVD_Efficient_Compression_for_Personalized_Text-to-Image_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-Dual-Orthogonal-Guidance-for-Robust-Diffusion-based-Handwritten-Text-Generationpdf"><a href="#68-Dual-Orthogonal-Guidance-for-Robust-Diffusion-based-Handwritten-Text-Generationpdf" class="headerlink" title="68. Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generationpdf"></a>68. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17017v1">Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dual_Orthogonal_Guidance_for_Robust_Diffusion-based_Handwritten_Text_Generation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Luleå University of Technology</span></p>
<p>本文提出了一种新的采样指导策略Dual Orthogonal Guidance (DOG)，用于提升扩散模型在手写文本生成任务中的内容清晰度和风格多样性。DOG通过负向扰动条件的正交投影引导生成过程，结合三角调度机制控制指导强度，无需额外训练即可集成现有模型。在DiffusionPen和One-DM等主流模型上的实验表明，DOG方法能显著增强生成文本的可读性和风格多样性，尤其在面对训练集中未见过的词或风格时表现优异。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Dual_Orthogonal_Guidance_for_Robust_Diffusion-based_Handwritten_Text_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-RPD-Diff-Region-Adaptive-Physics-Guided-Diffusion-Model-for-Visibility-Enhancement-under-Dense-and-Non-Uniform-Hazepdf"><a href="#69-RPD-Diff-Region-Adaptive-Physics-Guided-Diffusion-Model-for-Visibility-Enhancement-under-Dense-and-Non-Uniform-Hazepdf" class="headerlink" title="69. RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Hazepdf"></a>69. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16956v1">RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Haze</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/RPD-Diff__Region-Adaptive_Physics-Guided_Diffusion_Model_for_Visibility_Enhancement_under_Dense_and_.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Intelligent Systems Engineering, Shenzhen Campus of Sun Yat-Sen University</span></p>
<p>该论文提出了RPD-Diff，一种区域自适应物理引导去雾扩散模型，结合物理引导的中间状态目标（PIST）和雾感知去噪步长预测器（HADTP），以解决密集和非均匀雾条件下的单幅图像去雾问题。实验表明RPD-Diff在四个真实世界数据集上均优于现有方法，显著提升了图像细节和色彩还原能力，适用于复杂真实场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/RPD-Diff_Region-Adaptive_Physics-Guided_Diffusion_Model_for_Visibility_Enhancement_under_Dense_and_Non-Uniform_Haze.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-HunyuanVideo-Foley-Multimodal-Diffusion-with-Representation-Alignment-for-High-Fidelity-Foley-Audio-Generationpdf"><a href="#70-HunyuanVideo-Foley-Multimodal-Diffusion-with-Representation-Alignment-for-High-Fidelity-Foley-Audio-Generationpdf" class="headerlink" title="70. HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generationpdf"></a>70. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16930v1">HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HunyuanVideo-Foley__Multimodal_Diffusion_with_Representation_Alignment_for_High-Fidelity_Foley_Audio.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent Hunyuan</span></p>
<p>本文提出HunyuanVideo-Foley，一个端到端文本-视频到音频生成框架，结合自动化高质量数据管道、表示对齐损失（REPA）和多模态扩散Transformer，实现音频与视觉动态和语义精准对齐。结果表明，该方法在音频保真度、视语对齐、时间同步和分布匹配等方面均超越现有方法，达到视频语义驱动音频生成的SOTA水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/HunyuanVideo-Foley_Multimodal_Diffusion_with_Representation_Alignment_for_High-Fidelity_Foley_Audio_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-TinySR-Pruning-Diffusion-for-Real-World-Image-Super-Resolutionpdf"><a href="#71-TinySR-Pruning-Diffusion-for-Real-World-Image-Super-Resolutionpdf" class="headerlink" title="71. TinySR: Pruning Diffusion for Real-World Image Super-Resolutionpdf"></a>71. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17434v1">TinySR: Pruning Diffusion for Real-World Image Super-Resolution</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TinySR__Pruning_Diffusion_for_Real-World_Image_Super-Resolution.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了TinySR，一种针对真实场景图像超分辨率（Real-ISR）的高效紧凑扩散模型，通过动态分块激活和扩展-腐蚀策略实现深度剪枝，并结合VAE通道剪枝、注意力移除和轻量化卷积等组件优化，有效压缩模型体积和计算量。实验表明，TinySR在保持感知质量的同时，实现了5.68倍加速和83%参数量减少，显著优于同类方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TinySR_Pruning_Diffusion_for_Real-World_Image_Super-Resolution.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-OmniMRI-A-Unified-Vision–Language-Foundation-Model-for-Generalist-MRI-Interpretationpdf"><a href="#72-OmniMRI-A-Unified-Vision–Language-Foundation-Model-for-Generalist-MRI-Interpretationpdf" class="headerlink" title="72. OmniMRI: A Unified Vision–Language Foundation Model for Generalist MRI Interpretationpdf"></a>72. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17524v1">OmniMRI: A Unified Vision–Language Foundation Model for Generalist MRI Interpretation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/OmniMRI__A_Unified_Vision--Language_Foundation_Model_for_Generalist_MRI_Interpretation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Athinoula A. Martinos Center for Biomedical Imaging, Harvard Medical School</span></p>
<p>OmniMRI提出了一种统一的视觉-语言基础模型，针对MRI临床流程中的图像重建、分割、异常检测、诊断建议和报告生成任务，采用多阶段训练（自监督视觉预训练、视觉-语言对齐、多模态预训练和多任务指令微调），融合了60个公开数据集的大规模异构MRI数据，实现了跨任务和跨解剖部位的泛化。实验表明，OmniMRI在单一架构下可实现多种任务的高质量输出，有效整合了影像理解与临床语义推理，为医学影像自动化和智能化奠定了基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/OmniMRI_A_Unified_Vision%E2%80%93Language_Foundation_Model_for_Generalist_MRI_Interpretation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-Social-MAE-A-Transformer-Based-Multimodal-Autoencoder-for-Face-and-Voicepdf"><a href="#73-Social-MAE-A-Transformer-Based-Multimodal-Autoencoder-for-Face-and-Voicepdf" class="headerlink" title="73. Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voicepdf"></a>73. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17502v1">Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Social-MAE__A_Transformer-Based_Multimodal_Autoencoder_for_Face_and_Voice.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Numediart Institute, University of Mons</span></p>
<p>该论文提出了Social-MAE模型，将Transformer架构与自监督Masked Autoencoder方法相结合，专为多帧输入的音视频社交数据设计，通过在大规模人类社交数据集（VoxCeleb2）上预训练，实现了高时序视觉保真度。实验结果表明，Social-MAE在情感识别、笑声检测等多模态社交任务上取得了SOTA性能，验证了领域自适应预训练的重要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Social-MAE_A_Transformer-Based_Multimodal_Autoencoder_for_Face_and_Voice.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-Multimodal-Representation-Learning-Conditioned-on-Semantic-Relationspdf"><a href="#74-Multimodal-Representation-Learning-Conditioned-on-Semantic-Relationspdf" class="headerlink" title="74. Multimodal Representation Learning Conditioned on Semantic Relationspdf"></a>74. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17497v1">Multimodal Representation Learning Conditioned on Semantic Relations</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multimodal_Representation_Learning_Conditioned_on_Semantic_Relations.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Emory University</span></p>
<p>本文提出了RCML框架，将语义关系作为条件引入多模态对比学习，通过构造自然语言关系描述的多对多训练样本，并采用关系引导的跨注意力机制，实现多模态特征在特定语义上下文下的提取与对齐。实验结果显示，RCML在七个产品领域的检索和分类任务中均优于主流基线模型，证明了利用语义关系促进多模态表征学习的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Multimodal_Representation_Learning_Conditioned_on_Semantic_Relations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-Multi-Level-LVLM-Guidance-for-Untrimmed-Video-Action-Recognitionpdf"><a href="#75-Multi-Level-LVLM-Guidance-for-Untrimmed-Video-Action-Recognitionpdf" class="headerlink" title="75. Multi-Level LVLM Guidance for Untrimmed Video Action Recognitionpdf"></a>75. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17442v1">Multi-Level LVLM Guidance for Untrimmed Video Action Recognition</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multi-Level_LVLM_Guidance_for_Untrimmed_Video_Action_Recognition.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kunming University of Science and Technology</span></p>
<p>本文提出了Event-Contextualized Video Transformer (ECVT) 框架，通过引入大型视觉-语言模型（LVLM）生成的多层次语义描述（全局事件提示和时序子事件提示），以引导视频Transformer对未剪辑视频中的动作进行识别和定位。多层次文本提示与视频特征融合，利用自适应门控、高级语义融合、跨模态注意力和事件图模块进行时序校准，提升了模型对复杂动作时序结构和事件逻辑的理解。实验结果显示，ECVT在ActivityNet v1.3和THUMOS14数据集上均取得了当前最优性能，显著优于现有主流方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Multi-Level_LVLM_Guidance_for_Untrimmed_Video_Action_Recognition.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-An-LLM-LVLM-Driven-Agent-for-Iterative-and-Fine-Grained-Image-Editingpdf"><a href="#76-An-LLM-LVLM-Driven-Agent-for-Iterative-and-Fine-Grained-Image-Editingpdf" class="headerlink" title="76. An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editingpdf"></a>76. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17435v1">An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/An_LLM-LVLM_Driven_Agent_for_Iterative_and_Fine-Grained_Image_Editing.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kunming University of Science and Technology</span></p>
<p>本文提出RefineEdit-Agent，一个无训练、以大语言模型(LLM)和视觉-语言大模型(LVLM)为核心的智能体框架，实现复杂、多轮、细粒度的图像编辑。方法包括LVLM解析指令和场景、LLM分解目标和工具选择、执行编辑、LVLM反馈评估，形成闭环迭代优化。实验在新构建的LongBench-T2I-Edit基准（含九大视觉维度、500组复杂指令）上，RefineEdit-Agent在编辑保真度和上下文一致性方面显著优于主流方法，平均得分3.67，展现出强大多轮细粒度编辑能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/An_LLM-LVLM_Driven_Agent_for_Iterative_and_Fine-Grained_Image_Editing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-Lightweight-Joint-Optimization-of-General-Purpose-Vision-Language-Models-and-Retrievers-for-Medical-Diagnosispdf"><a href="#77-Lightweight-Joint-Optimization-of-General-Purpose-Vision-Language-Models-and-Retrievers-for-Medical-Diagnosispdf" class="headerlink" title="77. Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosispdf"></a>77. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17394v1">Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosis</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Lightweight_Joint_Optimization_of_General-Purpose_Vision-Language_Models_and_Retrievers_for_Medical_.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science and Engineering, The Hebrew University of Jerusalem</span></p>
<p>本论文提出JOMED方法，实现通用大视觉语言模型（LVLM）与多模态检索器的联合优化，用于医学图像分类与视觉问答。该方法采用一般性主干网络，通过轻量级微调，首先用冻结检索器对LVLM进行带检索增强的微调，然后冻结LVLM对检索器进行KL散度损失优化，提升检索内容与下游任务的关联性。实验表明，在多项医学影像分类与视觉问答任务上，JOMED在无医学预训练情况下取得与医学预训练模型相当甚至更优的效果，尤其在不同检索内容导致预测不一致的挑战性场景下显著提升了模型性能。结论是：联合优化的轻量级方法，在医学诊断任务实现了高效且具竞争力的表现，并为未来优化检索-推理流程提供了方向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Lightweight_Joint_Optimization_of_General-Purpose_Vision-Language_Models_and_Retrievers_for_Medical_Diagnosis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-Neural-Proteomics-Fields-for-Super-resolved-Spatial-Proteomics-Predictionpdf"><a href="#78-Neural-Proteomics-Fields-for-Super-resolved-Spatial-Proteomics-Predictionpdf" class="headerlink" title="78. Neural Proteomics Fields for Super-resolved Spatial Proteomics Predictionpdf"></a>78. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17389v1">Neural Proteomics Fields for Super-resolved Spatial Proteomics Prediction</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Neural_Proteomics_Fields_for_Super-resolved_Spatial_Proteomics_Prediction.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Artificial Intelligence, University of Chinese Academy of Sciences</span></p>
<p>该论文提出Neural Proteomics Fields (NPF)，一种针对测序型空间蛋白组（seq-SP）超分辨率预测的深度学习框架。NPF采用空间建模模块和形态建模模块，通过位置频谱编码和病理图像特征融合，实现蛋白表达在连续空间上的重建。实验结果显示，NPF在参数量更少的情况下达到业界领先的预测性能，并首次建立了开放基准数据集，推动空间蛋白组研究发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Neural_Proteomics_Fields_for_Super-resolved_Spatial_Proteomics_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-DiCache-Let-Diffusion-Model-Determine-Its-Own-Cachepdf"><a href="#79-DiCache-Let-Diffusion-Model-Determine-Its-Own-Cachepdf" class="headerlink" title="79. DiCache: Let Diffusion Model Determine Its Own Cachepdf"></a>79. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17356v1">DiCache: Let Diffusion Model Determine Its Own Cache</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DiCache__Let_Diffusion_Model_Determine_Its_Own_Cache.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了一种无需训练、模型无关的自适应缓存加速方法DiCache，用于提升扩散模型（如WAN 2.1、HunyuanVideo和Flux）的推理速度和生成质量。其核心技术包括在线浅层探针剖面方案，可实时根据浅层特征变化估算缓存误差自适应决定缓存时机，以及动态缓存轨迹对齐，通过浅层特征轨迹组合多步缓存以精确逼近当前特征。实验表明，DiCache在多种主流视觉生成任务上相较现有方法实现了更高的推理效率和更优的视觉保真度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/DiCache_Let_Diffusion_Model_Determine_Its_Own_Cache.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-PosBridge-Multi-View-Positional-Embedding-Transplant-for-Identity-Aware-Image-Editingpdf"><a href="#80-PosBridge-Multi-View-Positional-Embedding-Transplant-for-Identity-Aware-Image-Editingpdf" class="headerlink" title="80. PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editingpdf"></a>80. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17302v1">PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editing</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PosBridge__Multi-View_Positional_Embedding_Transplant_for_Identity-Aware_Image_Editing.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Electro-Communications</span></p>
<p>本文提出了PosBridge，一种训练无关的图像编辑框架，通过移植参考图像的位置信嵌入和Corner Centered Layout，实现了高保真、结构一致的目标对象插入。实验表明，PosBridge在语义一致性、外观逼真度和生成鲁棒性上均优于现有主流方法，且可选的LoRA模块进一步提升细节表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PosBridge_Multi-View_Positional_Embedding_Transplant_for_Identity-Aware_Image_Editing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoningpdf"><a href="#81-Explain-Before-You-Answer-A-Survey-on-Compositional-Visual-Reasoningpdf" class="headerlink" title="81. Explain Before You Answer: A Survey on Compositional Visual Reasoningpdf"></a>81. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17298v1">Explain Before You Answer: A Survey on Compositional Visual Reasoning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Explain_Before_You_Answer__A_Survey_on_Compositional_Visual_Reasoning.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Monash University</span></p>
<p>本文系统综述了2023-2025年间260余篇关于组合式视觉推理（Compositional Visual Reasoning, CVR）的前沿论文，梳理了从单体视觉-语言模型到链式思维和自主体VLM的五阶段范式演进，详细分析了CVR在认知对齐、可解释性、泛化性及数据效率等方面的优势。结论指出，组合式视觉推理通过显式中间推理步骤提升了视觉推理系统的透明性、鲁棒性及通用性，并提出了未来在世界模型集成、人机协同推理及更细致评测协议上的研究方向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Explain_Before_You_Answer_A_Survey_on_Compositional_Visual_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-Quickly-Tuning-Foundation-Models-for-Image-Segmentationpdf"><a href="#82-Quickly-Tuning-Foundation-Models-for-Image-Segmentationpdf" class="headerlink" title="82. Quickly Tuning Foundation Models for Image Segmentationpdf"></a>82. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17283v1">Quickly Tuning Foundation Models for Image Segmentation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Quickly_Tuning_Foundation_Models_for_Image_Segmentation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Freiburg</span></p>
<p>本文提出QTT-SEG方法，通过基于元学习的性能与成本预测器结合贝叶斯优化，实现对SAM（Segment Anything Model）在图像分割任务下的快速自动化微调。QTT-SEG能在200万以上配置空间中高效搜索，在13个分割数据集上，显著优于SAM零样本表现，并在绝大多数二分类任务下超过AutoGluon Multimodal的表现，且收敛速度快。结论表明QTT-SEG具备高效、鲁棒、稳定的模型适应能力，尤其在多领域分割任务下表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Quickly_Tuning_Foundation_Models_for_Image_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-ERF-BA-TFD-A-Multimodal-Model-for-Audio-Visual-Deepfake-Detectionpdf"><a href="#83-ERF-BA-TFD-A-Multimodal-Model-for-Audio-Visual-Deepfake-Detectionpdf" class="headerlink" title="83. ERF-BA-TFD+: A Multimodal Model for Audio-Visual Deepfake Detectionpdf"></a>83. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17282v1">ERF-BA-TFD+: A Multimodal Model for Audio-Visual Deepfake Detection</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ERF-BA-TFD+__A_Multimodal_Model_for_Audio-Visual_Deepfake_Detection.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Lanzhou University</span></p>
<p>该论文提出了一种名为ERF-BA-TFD+的多模态深度伪造检测模型，通过融合增强感受野（ERF）模块与音视频特征，利用MViTv2视觉编码器和BYOL-A音频编码器提取特征，并采用CRATrans跨模态重建注意力机制对时序一致性进行检测，实现了对长时长、音视频不同步等复杂情况的深度伪造内容精确检测与定位。在DDL-AV数据集上的实验结果显示，该方法在准确率、召回率等指标上均优于现有方法，并在国际竞赛中获得第一名，验证了其实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ERF-BA-TFD%2B_A_Multimodal_Model_for_Audio-Visual_Deepfake_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-MTNet-Learning-Modality-Aware-Representation-with-Transformer-for-RGBT-Trackingpdf"><a href="#84-MTNet-Learning-Modality-Aware-Representation-with-Transformer-for-RGBT-Trackingpdf" class="headerlink" title="84. MTNet: Learning Modality-Aware Representation with Transformer for RGBT Trackingpdf"></a>84. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17280v1">MTNet: Learning Modality-Aware Representation with Transformer for RGBT Tracking</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MTNet__Learning_modality-aware_representation_with_transformer_for_RGBT_tracking.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本文提出了一种基于Transformer的模态感知RGBT目标跟踪方法MTNet。方法创新性地设计了模态感知网络（包括通道聚合与分布模块CADM和空间相似性感知模块SSPM），利用混合Transformer融合网络实现多模态特征的全局依赖建模，并通过三分支预测头和动态模板更新策略提升定位精度与稳健性。实验在三大RGBT基准上均超越现有最先进方法，并实现实时速度，验证了方法的有效性和优越性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MTNet_Learning_Modality-Aware_Representation_with_Transformer_for_RGBT_Tracking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-SEER-VAR-Semantic-Egocentric-Environment-Reasoner-for-Vehicle-Augmented-Realitypdf"><a href="#85-SEER-VAR-Semantic-Egocentric-Environment-Reasoner-for-Vehicle-Augmented-Realitypdf" class="headerlink" title="85. SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Realitypdf"></a>85. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17255v1">SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SEER-VAR__Semantic_Egocentric_Environment_Reasoner_for_Vehicle_Augmented_Reality.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Eberhard-Karls-Universitat Tübingen</span></p>
<p>该论文提出SEER-VAR框架，集成深度引导的语义分解、双分支上下文感知SLAM和GPT驱动的推荐系统，实现车辆驾驶场景下车内外环境的分离和实时定位，并基于视觉-语言理解生成情境相关AR信息叠加。实验表明，SEER-VAR在多场景下实现了稳健的空间对齐和高感知一致性的AR渲染，提升了驾驶员感知与体验。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SEER-VAR_Semantic_Egocentric_Environment_Reasoner_for_Vehicle_Augmented_Reality.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-PersPose-3D-Human-Pose-Estimation-with-Perspective-Encoding-and-Perspective-Rotationpdf"><a href="#86-PersPose-3D-Human-Pose-Estimation-with-Perspective-Encoding-and-Perspective-Rotationpdf" class="headerlink" title="86. PersPose: 3D Human Pose Estimation with Perspective Encoding and Perspective Rotationpdf"></a>86. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17239v1">PersPose: 3D Human Pose Estimation with Perspective Encoding and Perspective Rotation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PersPose__3D_Human_Pose_Estimation_with_Perspective_Encoding_and_Perspective_Rotation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southern University of Science and Technology</span></p>
<p>本文提出了PersPose框架，创新性地引入了Perspective Encoding（PE）模块，将裁剪图像的相机内参编码为2D映射，并Perspective Rotation（PR）模块将人体居中以减少透视失真，二者联合输入到CNN中进行3D人体姿态估计。实验结果表明，PersPose在3DPW、MPI-INF-3DHP和Human3.6M等数据集上均取得了当前最优性能，验证了方法在变化透视关系下的鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PersPose_3D_Human_Pose_Estimation_with_Perspective_Encoding_and_Perspective_Rotation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-Multi-Agent-Visual-Language-Reasoning-for-Comprehensive-Highway-Scene-Understandingpdf"><a href="#87-Multi-Agent-Visual-Language-Reasoning-for-Comprehensive-Highway-Scene-Understandingpdf" class="headerlink" title="87. Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understandingpdf"></a>87. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17205v1">Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Multi-Agent_Visual-Language_Reasoning_for_Comprehensive_Highway_Scene_Understanding.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Georgia</span></p>
<p>该论文提出了一种多智能体视觉-语言推理框架，结合大型通用视觉语言模型（如GPT-4o）利用领域知识生成任务特定的Chain-of-Thought（CoT）提示，引导小型高效VLM（如Qwen2.5-VL-7B）对短视频和多模态数据进行推理，实现对天气分类、路面湿滑评估和交通拥堵检测等多任务综合场景理解。实验结果表明，该框架在多样的交通和环境条件下表现出强健且高效的多任务推理能力，大幅提升准确率，并易于集成进现有交通摄像头系统，有助于提升道路安全和自动化监测能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Multi-Agent_Visual-Language_Reasoning_for_Comprehensive_Highway_Scene_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-MMCIG-Multimodal-Cover-Image-Generation-for-Text-only-Documents-and-Its-Dataset-Construction-via-Pseudo-labelingpdf"><a href="#88-MMCIG-Multimodal-Cover-Image-Generation-for-Text-only-Documents-and-Its-Dataset-Construction-via-Pseudo-labelingpdf" class="headerlink" title="88. MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labelingpdf"></a>88. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17199v1">MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labeling</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MMCIG__Multimodal_Cover_Image_Generation_for_Text-only_Documents_and_Its_Dataset_Construction_via_Ps.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chungnam National University</span></p>
<p>本文提出了MMCIG任务：针对仅含文本的文档，自动生成精炼摘要和与之视觉一致的封面图片。核心方法为多模态伪标签数据集构建，结合图像与图片说明的独立排序，筛选高一致性样本，实验和人工评测均表明该方法构建的数据集更高质量，模型生成的图片与摘要高度匹配。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MMCIG_Multimodal_Cover_Image_Generation_for_Text-only_Documents_and_Its_Dataset_Construction_via_Pseudo-labeling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-Advancing-Weakly-Supervised-Change-Detection-in-Satellite-Images-via-Adversarial-Class-Promptingpdf"><a href="#89-Advancing-Weakly-Supervised-Change-Detection-in-Satellite-Images-via-Adversarial-Class-Promptingpdf" class="headerlink" title="89. Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Promptingpdf"></a>89. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17186v1">Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Advancing_Weakly-Supervised_Change_Detection_in_Satellite_Images_via_Adversarial_Class_Prompting.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wuhan University</span></p>
<p>本文提出了一种名为Adversarial Class Prompting（AdvCP）的新方法，专注于卫星图像中的弱监督变化检测（WSCD），核心流程包括对未变场景施加对抗性标签扰动以挖掘伪变化噪声样本，并结合全局在线原型进行对抗样本校正。实验表明，AdvCP可无缝集成到多种主流WSCD方法中，在多个遥感变化检测数据集上显著提升IoU和F1分数，且无需增加推理开销。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Advancing_Weakly-Supervised_Change_Detection_in_Satellite_Images_via_Adversarial_Class_Prompting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-VROOM-Visual-Reconstruction-over-Onboard-Multiviewpdf"><a href="#90-VROOM-Visual-Reconstruction-over-Onboard-Multiviewpdf" class="headerlink" title="90. VROOM - Visual Reconstruction over Onboard Multiviewpdf"></a>90. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17172v1">VROOM - Visual Reconstruction over Onboard Multiview</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/VROOM_-_Visual_Reconstruction_over_Onboard_Multiview.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Berkeley</span></p>
<p>该论文提出VROOM系统，利用F1赛车车载摄像头视频，通过集成DROID-SLAM、AnyCam和Monst3r等方法，结合分块、分辨率调整和掩码预处理，实现赛道及车辆3D重建。实验表明，VROOM在复杂动态环境下能够部分还原赛道及车辆轨迹，验证了在真实场景下用车载视频实现可扩展4D重建的可行性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/VROOM_-_Visual_Reconstruction_over_Onboard_Multiview.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-Robust-Point-Cloud-Registration-via-Geometric-Overlapping-Guided-Rotation-Searchpdf"><a href="#91-Robust-Point-Cloud-Registration-via-Geometric-Overlapping-Guided-Rotation-Searchpdf" class="headerlink" title="91. Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Searchpdf"></a>91. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17427v1">Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Robust_Point_Cloud_Registration_via_Geometric_Overlapping_Guided_Rotation_Search.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>本文提出了一种基于几何最大重叠（GMOR）的点云配准算法，通过两阶段的旋转分支定界（BnB）搜索实现刚体变换的全局最优估计。方法利用Chasles定理将刚体变换分解为旋转轴搜索（采用立方映射和区间戳算法进行1D范围最大查询）和旋转角度搜索（基于扫线和线段树进行2D最大重叠查询），确保多项式时间和线性空间复杂度。实验表明，该方法在多个真实与合成数据集上配准精度和效率优于现有主流方法。结论：GMOR在点云配准任务中兼具高效性和鲁棒性，适用于轻量级视觉和机器人应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Robust_Point_Cloud_Registration_via_Geometric_Overlapping_Guided_Rotation_Search.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-Constrained-Prompt-Enhancement-for-Improving-Zero-Shot-Generalization-of-Vision-Language-Modelspdf"><a href="#92-Constrained-Prompt-Enhancement-for-Improving-Zero-Shot-Generalization-of-Vision-Language-Modelspdf" class="headerlink" title="92. Constrained Prompt Enhancement for Improving Zero-Shot Generalization of Vision-Language Modelspdf"></a>92. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17417v1">Constrained Prompt Enhancement for Improving Zero-Shot Generalization of Vision-Language Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Constrained_Prompt_Enhancement_for_Improving_Zero-Shot_Generalization_of_Vision-Language_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tianjin University</span></p>
<p>该论文提出了约束性提示增强（CPE）方法，通过拓扑引导同义语义生成（TGSSG）和类别无关判别区域选择（CADRS），分别构建全面的文本提示和紧凑的视觉提示，并结合测试时自适应（TTA）与最优传输（OT）实现视觉-文本集合对集合匹配。实验表明，该方法在多个零样本图像与视频任务上优于现有方法，显著提升视觉语言模型的零样本泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Constrained_Prompt_Enhancement_for_Improving_Zero-Shot_Generalization_of_Vision-Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-FoundDiff-Foundational-Diffusion-Model-for-Generalizable-Low-Dose-CT-Denoisingpdf"><a href="#93-FoundDiff-Foundational-Diffusion-Model-for-Generalizable-Low-Dose-CT-Denoisingpdf" class="headerlink" title="93. FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoisingpdf"></a>93. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17299v1">FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FoundDiff__Foundational_Diffusion_Model_for_Generalizable_Low-Dose_CT_Denoising.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本论文提出了FoundDiff，一种面向低剂量CT（LDCT）去噪的基础扩散模型，采用两阶段策略：首先利用对比语言-图像预训练模型（DA-CLIP）感知并编码剂量和解剖信息，通过剂量排序和解剖判别损失获得语义嵌入；然后将这些嵌入通过新型DACB模块集成到扩散模型（DA-Diff）中，实现自适应去噪。实验表明该方法在多个数据集、不同剂量和解剖区域均优于现有方法，并对未见剂量具有优良泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/FoundDiff_Foundational_Diffusion_Model_for_Generalizable_Low-Dose_CT_Denoising.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-Follow-My-Hold-Hand-Object-Interaction-Reconstruction-through-Geometric-Guidancepdf"><a href="#94-Follow-My-Hold-Hand-Object-Interaction-Reconstruction-through-Geometric-Guidancepdf" class="headerlink" title="94. Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidancepdf"></a>94. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18213v1">Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Follow_My_Hold__Hand-Object_Interaction_Reconstruction_through_Geometric_Guidance.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Max Planck Institute for Informatics</span></p>
<p>本论文提出FollowMyHold，一种基于扩散模型的单张RGB图像手持物体三维重建方法。方法利用多模态几何引导，包括法线、深度、轮廓一致性和手-物体交互约束，通过分阶段优化（先手、后物体、再联合），在推理过程中引导3D生成模型，实现高质量且物理合理的手-物体三维重建。实验结果表明，该方法在OakInk、Arctic和DexYCB等基准数据集上显著优于现有生成式方法，具备更高的准确率、稳健性和广泛泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Follow_My_Hold_Hand-Object_Interaction_Reconstruction_through_Geometric_Guidance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-Explain-and-Monitor-Deep-Learning-Models-for-Computer-Vision-using-Obz-AIpdf"><a href="#95-Explain-and-Monitor-Deep-Learning-Models-for-Computer-Vision-using-Obz-AIpdf" class="headerlink" title="95. Explain and Monitor Deep Learning Models for Computer Vision using Obz AIpdf"></a>95. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18188v1">Explain and Monitor Deep Learning Models for Computer Vision using Obz AI</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Explain_and_Monitor_Deep_Learning_Models_for_Computer_Vision_using_Obz_AI.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Informatics, University of Warsaw</span></p>
<p>本文提出了Obz AI，一个集成的全栈软件平台，专为深度学习视觉模型的可解释性和可监控性而设计，涵盖Python库、后端数据库和前端仪表盘。Obz AI支持主流XAI算法（如Grad-CAM、Saliency Maps、Attention Map等）与图像特征提取和异常检测方法，并实现了模型预测与解释的实时可视化和数据管理。结论是Obz AI首次为CV领域提供了集成化的解释与监控工具，显著提升了模型部署的透明度与可靠性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Explain_and_Monitor_Deep_Learning_Models_for_Computer_Vision_using_Obz_AI.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-Scene-Aware-Vectorized-Memory-Multi-Agent-Framework-with-Cross-Modal-Differentiated-Quantization-VLMs-for-Visually-Impaired-Assistancepdf"><a href="#96-Scene-Aware-Vectorized-Memory-Multi-Agent-Framework-with-Cross-Modal-Differentiated-Quantization-VLMs-for-Visually-Impaired-Assistancepdf" class="headerlink" title="96. Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistancepdf"></a>96. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18177v1">Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scene-Aware_Vectorized_Memory_Multi-Agent_Framework_with_Cross-Modal_Differentiated_Quantization_VLM.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Electronic Science and Technology of China</span></p>
<p>本文提出了跨模态差异化量化策略，专为视觉-语言模型（VLM）设计，通过对视觉编码器和跨模态模块分别量化，有效将19B参数模型的内存需求从38GB降至16GB，性能下降仅2.05%；构建了场景感知矢量化记忆多智能体系统，集成场景分类、矢量记忆和多模态交互，实现历史场景知识的高效检索，显著提升对视障用户的环境理解和助导航能力。实验表明，该系统在主流基准MMBench与OCR-VQA上保持高准确率，并将语音响应延迟降至2.83-3.52秒，大幅优于传统方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Scene-Aware_Vectorized_Memory_Multi-Agent_Framework_with_Cross-Modal_Differentiated_Quantization_VLMs_for_Visually_Impaired_Assistance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-Annotation-Free-Open-Vocabulary-Segmentation-for-Remote-Sensing-Imagespdf"><a href="#97-Annotation-Free-Open-Vocabulary-Segmentation-for-Remote-Sensing-Imagespdf" class="headerlink" title="97. Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Imagespdf"></a>97. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18067v1">Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Annotation-Free_Open-Vocabulary_Segmentation_for_Remote-Sensing_Images.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xi’an Jiaotong University</span></p>
<p>本文提出SegEarth-OV框架，实现遥感图像的无标注开放词汇语义分割，核心方法包括SimFeatUp通用特征上采样模块（基于参数化联合双边上采样和内容保持网络，恢复VLM下采样失真的空间细节）和全局偏置消除操作（通过减去VLM的[CLS] token抑制全局语义泄漏，提升像素级判别力），无需针对特定任务微调。为支持SAR等多模态遥感数据，提出AlignEarth跨模态蒸馏策略，通过光学-SAR配对图像将光学VLM知识高效迁移到SAR编码器，解决SAR领域大模型难以训练和标注昂贵难题。实验表明，SegEarth-OV在多种光学和SAR遥感分割任务上均显著优于现有SOTA方法，首次验证了遥感领域开放词汇分割的可行性与通用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Annotation-Free_Open-Vocabulary_Segmentation_for_Remote-Sensing_Images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-AQ-PCDSys-An-Adaptive-Quantized-Planetary-Crater-Detection-System-for-Autonomous-Space-Explorationpdf"><a href="#98-AQ-PCDSys-An-Adaptive-Quantized-Planetary-Crater-Detection-System-for-Autonomous-Space-Explorationpdf" class="headerlink" title="98. AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Explorationpdf"></a>98. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18025v1">AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AQ-PCDSys__An_Adaptive_Quantized_Planetary_Crater_Detection_System_for_Autonomous_Space_Exploration.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Manipal University Jaipur</span></p>
<p>本文提出了AQ-PCDSys，一种针对资源受限的行星探测任务设计的自适应量化陨石坑检测系统。方法上，该系统结合了量化感知训练（QAT）的高效量化神经网络架构、自适应多传感器融合模块（AMF）与多尺度检测头，能够智能融合光学影像与数字高程模型数据，通过自适应权重机制动态调整不同传感器特征的贡献，实现了在低功耗硬件上的高精度实时检测。结论表明，AQ-PCDSys在兼顾精度、延迟和功耗的同时，为行星自主探测平台提供了理论上高鲁棒性的边缘AI感知解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/AQ-PCDSys_An_Adaptive_Quantized_Planetary_Crater_Detection_System_for_Autonomous_Space_Exploration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-Propose-and-Rectify-A-Forensics-Driven-MLLM-Framework-for-Image-Manipulation-Localizationpdf"><a href="#99-Propose-and-Rectify-A-Forensics-Driven-MLLM-Framework-for-Image-Manipulation-Localizationpdf" class="headerlink" title="99. Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localizationpdf"></a>99. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17976v1">Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Propose_and_Rectify__A_Forensics-Driven_MLLM_Framework_for_Image_Manipulation_Localization.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">City University of Hong Kong</span></p>
<p>本文提出了一种结合多模态大语言模型（MLLM）和低层次取证特征分析的Propose-Rectify图像篡改检测与定位框架。方法首先用经过取证适配的LLaVA模型生成基于语义理解的初步篡改区域提议，再通过多尺度取证特征（如SRM、Bayar、Sobel、Noiseprint++）在Forensics Rectification Module中系统验证和修正这些提议，并在增强分割模块内融合取证线索以提高分割精度。实验显示该方法在多个数据集上实现了最优的检测准确率和定位精度，具备出色的泛化性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Propose_and_Rectify_A_Forensics-Driven_MLLM_Framework_for_Image_Manipulation_Localization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-SAIL-Recon-Large-SfM-by-Augmenting-Scene-Regression-with-Localizationpdf"><a href="#100-SAIL-Recon-Large-SfM-by-Augmenting-Scene-Regression-with-Localizationpdf" class="headerlink" title="100. SAIL-Recon: Large SfM by Augmenting Scene Regression with Localizationpdf"></a>100. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17972v1">SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SAIL-Recon__Large_SfM_by_Augmenting_Scene_Regression_with_Localization.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>该论文提出SAIL-Recon方法，通过在结构光恢复（SfM）Transformer回归网络中引入视觉定位模块，实现大规模无序图像集合的高效三维重建。其流程为：首先从采样的锚点图像构建神经场景表示，再基于该表示实现对所有输入图像的位姿与场景结构回归。实验显示，该方法在TUM-RGBD、CO3Dv2和Tanks &amp; Temples等数据集上，在相机位姿估计与新视角合成任务上效率和精度均超越传统与现有学习型SfM方法，具备大规模场景扩展能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SAIL-Recon_Large_SfM_by_Augmenting_Scene_Regression_with_Localization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="101-Generative-Feature-Imputing-—-A-Technique-for-Error-resilient-Semantic-Communicationpdf"><a href="#101-Generative-Feature-Imputing-—-A-Technique-for-Error-resilient-Semantic-Communicationpdf" class="headerlink" title="101. Generative Feature Imputing — A Technique for Error-resilient Semantic Communicationpdf"></a>101. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17957v1">Generative Feature Imputing — A Technique for Error-resilient Semantic Communication</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Generative_Feature_Imputing_-_A_Technique_for_Error-resilient_Semantic_Communication.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Hong Kong</span></p>
<p>本文提出了一种生成式特征补全（Generative Feature Imputing, GFI）框架，用于提升数字语义通信系统在图像传输中的抗误差能力。方法包括空间误差集中分组、基于扩散模型的特征生成修复以及语义感知功率分配，分别实现了特征误差的空间局部化、丢失特征的高效重构和对语义重要信息的动态保护。实验表明，该框架在低信噪比和块衰落条件下，较Deep Joint Source-Channel Coding (DJSCC)和JPEG2000等传统方法，在感知质量和语义准确性方面均有显著提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Generative_Feature_Imputing_%E2%80%94_A_Technique_for_Error-resilient_Semantic_Communication.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="102-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimationpdf"><a href="#102-Gaze-into-the-Heart-A-Multi-View-Video-Dataset-for-rPPG-and-Health-Biomarkers-Estimationpdf" class="headerlink" title="102. Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimationpdf"></a>102. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17924v1">Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Gaze_into_the_Heart__A_Multi-View_Video_Dataset_for_rPPG_and_Health_Biomarkers_Estimation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sber AI Lab</span></p>
<p>本文提出了MCD-rPPG，一个包含600名受试者、3600段多视角视频、同步PPG信号及丰富生物健康指标（如心电、血压、血氧等）的公开大规模数据集，并开发了基于面部视频的高效多任务rPPG神经网络模型，实现了实时健康参数估计。实验结果表明，该模型在跨数据集上具备良好的泛化性能与推理效率，推动了AI医疗助手和远程健康监测的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Gaze_into_the_Heart_A_Multi-View_Video_Dataset_for_rPPG_and_Health_Biomarkers_Estimation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="103-EndoUFM-Utilizing-Foundation-Models-for-Monocular-Depth-Estimation-of-Endoscopic-Imagespdf"><a href="#103-EndoUFM-Utilizing-Foundation-Models-for-Monocular-Depth-Estimation-of-Endoscopic-Imagespdf" class="headerlink" title="103. EndoUFM: Utilizing Foundation Models for Monocular Depth Estimation of Endoscopic Imagespdf"></a>103. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17916v1">EndoUFM: Utilizing Foundation Models for Monocular Depth Estimation of Endoscopic Images</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/EndoUFM__Utilizing_Foundation_Models_for_Monocular_depth_estimation_of_endoscopic_images.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本论文提出了EndoUFM，一种融合Depth Anything和Segment Anything两大视觉基础模型的自监督单目内窥镜深度估计框架。方法包括引入随机向量低秩适应（RVLoRA）与基于深度可分离卷积的残差模块（Res-DSC），并结合掩码引导的平滑损失以提升端到端深度一致性。实验在四个公开数据集上验证了方法的领先性能，结果显示其在精度和鲁棒性方面均优于现有方法，且模型参数量小，适于临床部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/EndoUFM_Utilizing_Foundation_Models_for_Monocular_Depth_Estimation_of_Endoscopic_Images.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="104-ISALUX-Illumination-and-Semantics-Aware-Transformer-Employing-Mixture-of-Experts-for-Low-Light-Image-Enhancementpdf"><a href="#104-ISALUX-Illumination-and-Semantics-Aware-Transformer-Employing-Mixture-of-Experts-for-Low-Light-Image-Enhancementpdf" class="headerlink" title="104. ISALUX: Illumination and Semantics-Aware Transformer Employing Mixture of Experts for Low Light Image Enhancementpdf"></a>104. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17885v1">ISALUX: Illumination and Semantics-Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ISALux__Illumination_and_Segmentation_Aware_Transformer_Employing_Mixture_of_Experts_for_Low_Light_I.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Manchester</span></p>
<p>该论文提出ISALux，一种基于Vision Transformer（ViT）的低光照图像增强方法，创新性地融合了照明和语义先验。其核心技术包括混合照明与语义感知多头自注意力（HISA-MSA）模块与专家混合（MoE）前馈网络，利用低秩适应（LoRA）提升泛化能力。实验表明，ISALux在多个基准数据集上实现了比现有技术更优的增强质量和更高的效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ISALUX_Illumination_and_Semantics-Aware_Transformer_Employing_Mixture_of_Experts_for_Low_Light_Image_Enhancement.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="105-Edge-Enhanced-Vision-Transformer-Framework-for-Accurate-AI-Generated-Image-Detectionpdf"><a href="#105-Edge-Enhanced-Vision-Transformer-Framework-for-Accurate-AI-Generated-Image-Detectionpdf" class="headerlink" title="105. Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detectionpdf"></a>105. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17877v1">Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detection</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Edge-Enhanced_Vision_Transformer_Framework_for_Accurate_AI-Generated_Image_Detection.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Uttara University</span></p>
<p>本文提出了一种结合细粒度结构分析与全局特征提取的AI生成图像检测框架：通过对Vision Transformer（ViT）进行特定数据集的微调，捕获高层语义特征，并设计了边缘差异分析模块，将原始与高斯模糊后的图像边缘进行对比，计算边缘方差分数，有效区分真实与AI生成图像。实验结果显示，该方法在CIFAKE等多个数据集上均优于主流CNN和ViT模型，最高准确率达97.75%，F1分数97.77%，且方法轻量、可解释，适用于图像和视频内容的数字取证与自动化验证。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Edge-Enhanced_Vision_Transformer_Framework_for_Accurate_AI-Generated_Image_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="106-Diffusion-Based-Data-Augmentation-for-Medical-Image-Segmentationpdf"><a href="#106-Diffusion-Based-Data-Augmentation-for-Medical-Image-Segmentationpdf" class="headerlink" title="106. Diffusion-Based Data Augmentation for Medical Image Segmentationpdf"></a>106. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17844v1">Diffusion-Based Data Augmentation for Medical Image Segmentation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Diffusion-Based_Data_Augmentation_for_Medical_Image_Segmentation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本论文提出DiffAug框架，将文本引导的扩散模型生成和自动分割验证结合，用于医学图像中罕见异常的合成数据增强。方法采用医学文本描述和空间掩膜指导扩散模型在正常图像中生成病变，并通过潜空间分割网络实时验证生成样本的空间准确性，仅保留高质量样本用于训练。实验表明，DiffAug在CVC-ClinicDB、Kvasir-SEG和REFUGE2三个医学影像分割基准上提升Dice系数8-10%，并将对小型息肉等难检测异常的漏检率降低至28%。结论：该方法高效生成高质量合成数据，有效缓解医学影像中的类别不平衡问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Diffusion-Based_Data_Augmentation_for_Medical_Image_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="107-UniSino-Physics-Driven-Foundational-Model-for-Universal-CT-Sinogram-Standardizationpdf"><a href="#107-UniSino-Physics-Driven-Foundational-Model-for-Universal-CT-Sinogram-Standardizationpdf" class="headerlink" title="107. UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardizationpdf"></a>107. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17816v1">UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/UniSino__Physics-Driven_Foundational_Model_for_Universal_CT_Sinogram_Standardization.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanchang University</span></p>
<p>该论文提出了UniSino，一种基于物理知识的通用CT正投影数据（sinogram）标准化基础模型。方法包括SinoVAE变分自编码器进行高频与全频特征压缩，结合条件扩散模型在投影域进行伪影&#x2F;噪声抑制与重建，并引入物理约束损失函数提升物理一致性和泛化能力。实验结果显示，UniSino在多种伪影类型、混合伪影及多数据集上标准化表现显著优于U-Net、ViT、CycleGAN、DDPM等主流方法，具备很强的泛化和鲁棒性。结论：UniSino可统一处理多种CT原始数据伪影，为医学影像预处理带来高效通用解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/UniSino_Physics-Driven_Foundational_Model_for_Universal_CT_Sinogram_Standardization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="108-CEIDM-A-Controlled-Entity-and-Interaction-Diffusion-Model-for-Enhanced-Text-to-Image-Generationpdf"><a href="#108-CEIDM-A-Controlled-Entity-and-Interaction-Diffusion-Model-for-Enhanced-Text-to-Image-Generationpdf" class="headerlink" title="108. CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generationpdf"></a>108. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17760v1">CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CEIDM__A_Controlled_Entity_and_Interaction_Diffusion_Model_for_Enhanced_Text-to-Image_Generation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">College of Computer Science and Technology, National University of Defense Technology</span></p>
<p>本文提出了一种名为CEIDM的文本到图像生成方法，基于扩散模型并引入了实体与交互的双重控制。方法上，利用大语言模型挖掘隐式交互关系、聚类及偏移交互动作特征，并设计实体控制网络实现语义引导掩码与多尺度卷积特征融合，从而提升实体和交互的精确控制。实验表明，CEIDM在实体与交互可控性和生成图像质量上均优于现有方法，特别适用于复杂场景与细节处理。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CEIDM_A_Controlled_Entity_and_Interaction_Diffusion_Model_for_Enhanced_Text-to-Image_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="109-SUPERGEN-An-Efficient-Ultra-high-resolution-Video-Generation-System-with-Sketching-and-Tilingpdf"><a href="#109-SUPERGEN-An-Efficient-Ultra-high-resolution-Video-Generation-System-with-Sketching-and-Tilingpdf" class="headerlink" title="109. SUPERGEN: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tilingpdf"></a>109. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17756v1">SUPERGEN: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SuperGen__An_Efficient_Ultra-high-resolution_Video_Generation_System_with_Sketching_and_Tiling.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Rice University</span></p>
<p>本文提出SUPERGEN系统，一种高效、训练无关的超高分辨率视频生成框架。其方法包括两阶段生成流程：首先利用预训练扩散模型生成低分辨率全局结构草图，然后通过分块细粒度的局部精化，实现高分辨率视频生成。该系统还引入区域自适应缓存和通信最小化的分块并行机制，有效减少内存和计算成本并提升扩展性。实验表明，SUPERGEN在多个基准上获得最高质量，同时在不降低视频质量的前提下，实现了高达6.2倍的生成速度提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SUPERGEN_An_Efficient_Ultra-high-resolution_Video_Generation_System_with_Sketching_and_Tiling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="110-Few-shot-Human-Action-Anomaly-Detection-via-a-Unified-Contrastive-Learning-Frameworkpdf"><a href="#110-Few-shot-Human-Action-Anomaly-Detection-via-a-Unified-Contrastive-Learning-Frameworkpdf" class="headerlink" title="110. Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Frameworkpdf"></a>110. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17726v1">Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Few-shot_Human_Action_Anomaly_Detection_via_a_Unified_Contrastive_Learning_Framework.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Toyama</span></p>
<p>本文提出了一种统一的对比学习框架用于小样本人体动作异常检测（HAAD），通过对多个类别动作数据进行对比学习，构建类别无关的表征空间，并利用基于扩散模型（HumanMAC）的生成式运动增强策略，在训练和推理阶段生成多样且真实的动作样本，提升模型泛化性和鲁棒性。实验结果在HumanAct12数据集上验证了该方法在见类和未见类场景下均优于现有方法，平均AUC提升2.6%，实现了无需类别重训练的高效、可扩展异常检测。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Few-shot_Human_Action_Anomaly_Detection_via_a_Unified_Contrastive_Learning_Framework.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="111-Instant-Preference-Alignment-for-Text-to-Image-Diffusion-Modelspdf"><a href="#111-Instant-Preference-Alignment-for-Text-to-Image-Diffusion-Modelspdf" class="headerlink" title="111. Instant Preference Alignment for Text-to-Image Diffusion Modelspdf"></a>111. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17718v1">Instant Preference Alignment for Text-to-Image Diffusion Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Instant_Preference_Alignment_for_Text-to-Image_Diffusion_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">New Laboratory of Pattern Recognition, CASIA</span></p>
<p>本论文提出了一种训练无关、实时的用户偏好对齐文本到图像（T2I）生成框架，基于多模态大语言模型（MLLM）先验，将任务分解为偏好理解和偏好引导两部分。具体方法通过MLLM自动从参考图像中提取多维偏好关键词，并设计结构化指令扩展原始提示，结合全局关键词控制和局部区域自注意力调制，实现对扩散模型的精准偏好引导，并支持多轮交互式实时优化。实验表明，该方法在Viper等数据集上显著优于现有方法，实现了更细粒度、动态的用户偏好对齐图像生成。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Instant_Preference_Alignment_for_Text-to-Image_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="112-CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolutionpdf"><a href="#112-CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolutionpdf" class="headerlink" title="112. CATformer: Contrastive Adversarial Transformer for Image Super-Resolutionpdf"></a>112. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17708v1">CATformer: Contrastive Adversarial Transformer for Image Super-Resolution</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CATformer__Contrastive_Adversarial_Transformer_for_Image_Super-Resolution.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Duke University</span></p>
<p>该论文提出CATformer，一种融合扩散式特征精细化、对抗学习和对比学习的双分支Transformer架构，用于图像超分辨率。CATformer通过主扩散Transformer和噪声鲁棒辅助Transformer分支提取互补特征，并结合残差密集块进行高质量图像重建。实验证明该方法在CelebA-HQ、DIV2K和Urban100等数据集上提升了结构相似性、感知质量和推理效率，优于当前主流扩散、Transformer和GAN方法，但在小规模数据集上PSNR略低。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CATformer_Contrastive_Adversarial_Transformer_for_Image_Super-Resolution.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="113-Robustness-Feature-Adapter-for-Efficient-Adversarial-Trainingpdf"><a href="#113-Robustness-Feature-Adapter-for-Efficient-Adversarial-Trainingpdf" class="headerlink" title="113. Robustness Feature Adapter for Efficient Adversarial Trainingpdf"></a>113. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17680v1">Robustness Feature Adapter for Efficient Adversarial Training</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Robustness_Feature_Adapter_for_Efficient_Adversarial_Training.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Dongguan University of Technology</span></p>
<p>该论文提出了一种高效的对抗训练方法，核心为在特征空间内引入鲁棒性特征适配器（RFA），通过外部模块分离鲁棒与非鲁棒特征，并利用双分类器与三元组损失提高泛化能力。结果显示，该方法显著提升了模型对未知攻击的鲁棒性，降低了过拟合风险，且在不同模型架构（如CNN和ViT）及大规模对抗训练中均具备极高效率和灵活性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Robustness_Feature_Adapter_for_Efficient_Adversarial_Training.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="114-Citizen-Centered-Climate-Intelligence-Operationalizing-Open-Tree-Data-for-Urban-Cooling-and-Eco-Routing-in-Indian-Citiespdf"><a href="#114-Citizen-Centered-Climate-Intelligence-Operationalizing-Open-Tree-Data-for-Urban-Cooling-and-Eco-Routing-in-Indian-Citiespdf" class="headerlink" title="114. Citizen Centered Climate Intelligence: Operationalizing Open Tree Data for Urban Cooling and Eco-Routing in Indian Citiespdf"></a>114. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17648v1">Citizen Centered Climate Intelligence: Operationalizing Open Tree Data for Urban Cooling and Eco-Routing in Indian Cities</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Citizen_Centered_Climate_Intelligence__Operationalizing_Open_Tree_Data_for_Urban_Cooling_and_Eco-Rou.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Institute of Technology Tiruchirappalli</span></p>
<p>本文提出了一个以市民为中心的城市气候智能框架，通过智能手机AI分割辅助的树木测量工具、基于遥感的局部冷却效能分析以及整合多源环境指标的生态路径规划引擎，实现市民参与的环境数据采集、分析与行动闭环。结论认为，该方法可提升城市绿化干预的科学性和透明度，并推动城市气候适应和环境治理的民主化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Citizen_Centered_Climate_Intelligence_Operationalizing_Open_Tree_Data_for_Urban_Cooling_and_Eco-Routing_in_Indian_Cities.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="115-Finding-Outliers-in-a-Haystack-Anomaly-Detection-for-Large-Pointcloud-Scenespdf"><a href="#115-Finding-Outliers-in-a-Haystack-Anomaly-Detection-for-Large-Pointcloud-Scenespdf" class="headerlink" title="115. Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenespdf"></a>115. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17634v1">Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenes</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Finding_Outliers_in_a_Haystack__Anomaly_Detection_for_Large_Pointcloud_Scenes.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Adelaide</span></p>
<p>本文提出了一种面向大规模点云场景的开放集分割与异常检测新方法，核心在于结合Mamba架构的长距离依赖建模能力和重建自编码器，先重建场景默认上下文，再利用原始与重建点云的差异进行异常检测。实验显示，该方法在KITTI和ECLAIR两个大型点云数据集上显著提升了异常检测性能，尤其在检测未知异常物体方面优于现有方法，但对已知物体分割仍有提升空间。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Finding_Outliers_in_a_Haystack_Anomaly_Detection_for_Large_Pointcloud_Scenes.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="116-Improving-Interpretability-in-Alzheimer’s-Prediction-via-Joint-Learning-of-ADAS-Cog-Scorespdf"><a href="#116-Improving-Interpretability-in-Alzheimer’s-Prediction-via-Joint-Learning-of-ADAS-Cog-Scorespdf" class="headerlink" title="116. Improving Interpretability in Alzheimer’s Prediction via Joint Learning of ADAS-Cog Scorespdf"></a>116. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17619v1">Improving Interpretability in Alzheimer’s Prediction via Joint Learning of ADAS-Cog Scores</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Improving_Interpretability_in_Alzheimer's_Prediction_via_Joint_Learning_of_ADAS-Cog_Scores.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universiti Brunei Darussalam</span></p>
<p>该论文提出了一种基于多任务学习（MTL）的框架，融合Vision Transformer（ViT）和Swin Transformer从MRI影像中提取特征，并结合纵向临床评分，同时预测阿尔兹海默症ADAS-Cog全局分数及其13项子分数。实验结果表明，子分数的联合学习可提升全局分数预测的准确性，并揭示模型对部分认知域（如词语回忆等）的高度依赖，但也发现部分关键子分数预测不稳定，主要因模型过度依赖临床特征而忽视MRI特征，未来需改进多模态融合与损失自适应权重以提升模型稳健性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Improving_Interpretability_in_Alzheimer%E2%80%99s_Prediction_via_Joint_Learning_of_ADAS-Cog_Scores.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="117-JCo-MVTON-Jointly-Controllable-Multi-Modal-Diffusion-Transformer-for-Mask-Free-Virtual-Try-onpdf"><a href="#117-JCo-MVTON-Jointly-Controllable-Multi-Modal-Diffusion-Transformer-for-Mask-Free-Virtual-Try-onpdf" class="headerlink" title="117. JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-onpdf"></a>117. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17614v1">JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-on</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/JCo-MVTON__Jointly_Controllable_Multi-Modal_Diffusion_Transformer_for_Mask-Free_Virtual_Try-on.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">DAMO Academy, Alibaba Group</span></p>
<p>本论文提出JCo-MVTON，一种基于Multi-Modal Diffusion Transformer (MM-DiT)的无掩码虚拟试衣方法，通过多模态条件融合（包括参考图像和服装图像）并在自注意力层引入条件分支和掩码机制，实现高质量、稳定的服装更换。该方法结合创新的数据生成与筛选流程，有效提升了模型在多样场景下的可控性和泛化能力，在DressCode等公开基准及真实应用中表现优异，显著超过主流学术与商业系统。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/JCo-MVTON_Jointly_Controllable_Multi-Modal_Diffusion_Transformer_for_Mask-Free_Virtual_Try-on.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="118-A-Weighted-Vision-Transformer-Based-Multi-Task-Learning-Framework-for-Predicting-ADAS-Cog-Scorespdf"><a href="#118-A-Weighted-Vision-Transformer-Based-Multi-Task-Learning-Framework-for-Predicting-ADAS-Cog-Scorespdf" class="headerlink" title="118. A Weighted Vision Transformer-Based Multi-Task Learning Framework for Predicting ADAS-Cog Scorespdf"></a>118. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17613v1">A Weighted Vision Transformer-Based Multi-Task Learning Framework for Predicting ADAS-Cog Scores</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Weighted_Vision_Transformer-Based_Multi-Task_Learning_Framework_for_Predicting_ADAS-Cog_Scores.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universiti Brunei Darussalam</span></p>
<p>本文提出了一种基于加权视觉Transformer（ViT）的多任务学习（MTL）框架，用于利用基线MRI影像预测阿尔茨海默病ADAS-Cog量表的13个分项得分及其总分。方法通过对与全局评分高度相关的分项（Q1、Q4、Q8）赋予更高损失权重，提升模型对重要认知域的关注度，并在不同临床分组（MCI和CN）中系统分析权重策略对预测效果的影响。实验表明，强权重策略在MCI组表现更优，适度权重则更适合CN组。结论指出，合理的任务加权可提升预测准确性和模型解释性，而均匀权重策略会低估关键分项，限制模型泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Weighted_Vision_Transformer-Based_Multi-Task_Learning_Framework_for_Predicting_ADAS-Cog_Scores.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="119-TinyGiantVLM-A-Lightweight-Vision-Language-Architecture-for-Spatial-Reasoning-under-Resource-Constraintspdf"><a href="#119-TinyGiantVLM-A-Lightweight-Vision-Language-Architecture-for-Spatial-Reasoning-under-Resource-Constraintspdf" class="headerlink" title="119. TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraintspdf"></a>119. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17595v1">TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TinyGiantVLM__A_Lightweight_Vision-Language_Architecture_for_Spatial_Reasoning_under_Resource_Constr.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science, VNU-HCM</span></p>
<p>该论文提出了TinyGiantVLM，一种轻量级、模块化的视觉-语言双阶段框架，通过预训练视觉主干提取RGB和深度的全局与区域特征，并利用Mixture-of-Experts(MoE)融合模块实现多模态空间推理。在AI City Challenge 2025的工业仓储空间推理任务中，TinyGiantVLM以64M参数基线模型获得第五名，兼顾准确性与计算效率，MoE变体进一步提升性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TinyGiantVLM_A_Lightweight_Vision-Language_Architecture_for_Spatial_Reasoning_under_Resource_Constraints.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="120-HERO-Hierarchical-Extrapolation-and-Refresh-for-Efficient-World-Modelspdf"><a href="#120-HERO-Hierarchical-Extrapolation-and-Refresh-for-Efficient-World-Modelspdf" class="headerlink" title="120. HERO: Hierarchical Extrapolation and Refresh for Efficient World Modelspdf"></a>120. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17588v1">HERO: Hierarchical Extrapolation and Refresh for Efficient World Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HERO__Hierarchical_Extrapolation_and_Refresh_for_Efficient_World_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了一种无需训练的分层加速框架HERO，用于提升世界模型推理效率。方法上，HERO针对多模态世界模型的分层特性，在浅层采用patch-wise refresh动态重算部分token，在深层用线性外推直接估算中间特征，避免冗余计算并兼容FlashAttention。实验证明，HERO在保持高质量输出的同时，实现了1.73倍推理加速，优于现有扩散模型加速方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/HERO_Hierarchical_Extrapolation_and_Refresh_for_Efficient_World_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="121-GSVisLoc-Generalizable-Visual-Localization-for-Gaussian-Splatting-Scene-Representationspdf"><a href="#121-GSVisLoc-Generalizable-Visual-Localization-for-Gaussian-Splatting-Scene-Representationspdf" class="headerlink" title="121. GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representationspdf"></a>121. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18242v1">GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GSVisLoc__Generalizable_Visual_Localization_for_Gaussian_Splatting_Scene_Representations.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Weizmann Institute of Science</span></p>
<p>本文提出GSVisLoc，一种基于3D Gaussian Splatting（3DGS）场景表示的通用视觉定位方法。其核心流程包括：用KPConv编码器对3D高斯点云进行编码与降采样，图像侧用深度学习提取多尺度特征，通过交错自注意力与交叉注意力实现3D-2D特征对齐，先粗后细匹配，并结合PnP+RANSAC及GS-CPR精化姿态估计。结论表明，该方法在多项公开数据集上超越以往3DGS定位方法，并具备跨场景泛化能力，无需额外参考图片或重训练即可应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GSVisLoc_Generalizable_Visual_Localization_for_Gaussian_Splatting_Scene_Representations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="122-Disentangling-the-Factors-of-Convergence-between-Brains-and-Computer-Vision-Modelspdf"><a href="#122-Disentangling-the-Factors-of-Convergence-between-Brains-and-Computer-Vision-Modelspdf" class="headerlink" title="122. Disentangling the Factors of Convergence between Brains and Computer Vision Modelspdf"></a>122. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18226v1">Disentangling the Factors of Convergence between Brains and Computer Vision Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Disentangling_the_Factors_of_Convergence_between_Brains_and_Computer_Vision_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Meta AI</span></p>
<p>本论文通过系统性地训练并比较多种不同结构、训练量和数据类型的自监督视觉Transformer（DINOv3），量化其对自然图像的表征与人脑fMRI和MEG响应的相似性，采用编码分数、空间分数和时间分数三种度量。结果表明，模型规模、训练数据量和图像类型这三大因素均独立且交互地提升AI模型与人脑在视觉表征上的相似性，且这种对齐展现出与人脑发育相似的先后顺序，揭示了架构与经验共同作用于人工神经网络视觉认知能力的形成。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Disentangling_the_Factors_of_Convergence_between_Brains_and_Computer_Vision_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="123-Incorporating-Pre-trained-Diffusion-Models-in-Solving-the-Schrodinger-Bridge-Problempdf"><a href="#123-Incorporating-Pre-trained-Diffusion-Models-in-Solving-the-Schrodinger-Bridge-Problempdf" class="headerlink" title="123. Incorporating Pre-trained Diffusion Models in Solving the Schrödinger Bridge Problempdf"></a>123. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18095v1">Incorporating Pre-trained Diffusion Models in Solving the Schrödinger Bridge Problem</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Incorporating_Pre-trained_Diffusion_Models_in_Solving_the_Schr%C3%B6dinger_Bridge_Problem.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了三种重参数化技术（IPMM、IPTM、IPFM），将预训练扩散模型（SGMs）与Schrödinger Bridge（SB）问题统一起来，以极大地加速和稳定SB模型的训练，并提出创新的初始化策略，利用强大的预训练SGMs显著提升SB模型的训练效率和生成性能。实验结果表明，该方法在合成数据、图像生成和无配对图像迁移等任务中在训练速度、稳定性和生成质量方面均优于现有SB方法，并能有效提升预训练SGMs的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Incorporating_Pre-trained_Diffusion_Models_in_Solving_the_Schr%C3%B6dinger_Bridge_Problem.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="124-SCOUT-Semi-supervised-Camouflaged-Object-Detection-by-Utilizing-Text-and-Adaptive-Data-Selectionpdf"><a href="#124-SCOUT-Semi-supervised-Camouflaged-Object-Detection-by-Utilizing-Text-and-Adaptive-Data-Selectionpdf" class="headerlink" title="124. SCOUT: Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selectionpdf"></a>124. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17843v1">SCOUT: Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selection</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SCOUT__Semi-supervised_Camouflaged_Object_Detection_by_Utilizing_Text_and_Adaptive_Data_Selection.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiamen University</span></p>
<p>该论文提出SCOUT，一种创新的半监督伪装目标检测模型，包含自适应数据增强与选择（ADAS）模块和文本融合（TFM）模块。ADAS通过对未标注数据的对抗性增强与评分，主动选择有价值样本进行标注，TFM结合图像和伪装相关文本信息进行特征融合，并提出RefTextCOD新数据集。实验结果显示SCOUT在四个主流测试集上优于现有方法，显著提升分割精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SCOUT_Semi-supervised_Camouflaged_Object_Detection_by_Utilizing_Text_and_Adaptive_Data_Selection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="125-Towards-Trustworthy-Breast-Tumor-Segmentation-in-Ultrasound-using-Monte-Carlo-Dropout-and-Deep-Ensembles-for-Epistemic-Uncertainty-Estimationpdf"><a href="#125-Towards-Trustworthy-Breast-Tumor-Segmentation-in-Ultrasound-using-Monte-Carlo-Dropout-and-Deep-Ensembles-for-Epistemic-Uncertainty-Estimationpdf" class="headerlink" title="125. Towards Trustworthy Breast Tumor Segmentation in Ultrasound using Monte Carlo Dropout and Deep Ensembles for Epistemic Uncertainty Estimationpdf"></a>125. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17768v1">Towards Trustworthy Breast Tumor Segmentation in Ultrasound using Monte Carlo Dropout and Deep Ensembles for Epistemic Uncertainty Estimation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Towards_Trustworthy_Breast_Tumor_Segmentation_in_Ultrasound_using_Monte_Carlo_Dropout_and_Deep_Ensem.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kwame Nkrumah University of Science and Technology</span></p>
<p>该论文提出了一种结合Monte Carlo Dropout和Deep Ensembles用于不确定性估计的改进Residual Encoder U-Net，用于乳腺超声图像的自动分割，并系统清理和评估BUSI数据集以避免数据泄漏。实验结果表明，该方法在乳腺病变超声基准数据集上取得了最新的分割准确率，并且能够通过不确定性估计有效标识模型低置信度区域，对实际临床部署具有重要意义。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Towards_Trustworthy_Breast_Tumor_Segmentation_in_Ultrasound_using_Monte_Carlo_Dropout_and_Deep_Ensembles_for_Epistemic_Uncertainty_Estimation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="126-Beyond-Play-Pause-Untwist-–-Turning-GPT-4o’s-Spatial-Weakness-into-a-Strength-for-In-Depth-Interactive-Video-Learningpdf"><a href="#126-Beyond-Play-Pause-Untwist-–-Turning-GPT-4o’s-Spatial-Weakness-into-a-Strength-for-In-Depth-Interactive-Video-Learningpdf" class="headerlink" title="126. Beyond Play &amp; Pause: Untwist – Turning GPT-4o’s Spatial Weakness into a Strength for In-Depth Interactive Video Learningpdf"></a>126. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17160v1">Beyond Play &amp; Pause: Untwist – Turning GPT-4o’s Spatial Weakness into a Strength for In-Depth Interactive Video Learning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Beyond_Play_and_Pause__Turning_GPT-4o_Spatial_Weakness_into_a_Strength_for_In-Depth_Interactive_Vide.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Clemson University</span></p>
<p>本文提出了Untwist系统，将GPT-4o与计算机视觉技术结合，实现对视频内容的区域化、实时交互式问答。关键方法包括视频转录、关键帧提取与优化、利用框选注释替代坐标输入以提升空间理解，最终显著提升GPT-4o在视频内容定位和解释的准确度。结论显示，注释帧方式大幅优于原始坐标输入，促进了多模态、实时的智能视频学习体验。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Beyond_Play_%26_Pause_Untwist_%E2%80%93_Turning_GPT-4o%E2%80%99s_Spatial_Weakness_into_a_Strength_for_In-Depth_Interactive_Video_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="127-Towards-Safeguarding-LLM-Fine-tuning-APIs-against-Cipher-Attackspdf"><a href="#127-Towards-Safeguarding-LLM-Fine-tuning-APIs-against-Cipher-Attackspdf" class="headerlink" title="127. Towards Safeguarding LLM Fine-tuning APIs against Cipher Attackspdf"></a>127. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17158v1">Towards Safeguarding LLM Fine-tuning APIs against Cipher Attacks</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Towards_Safeguarding_LLM_Fine-tuning_APIs_against_Cipher_Attacks.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent</span></p>
<p>本文提出了Cipher Fine-tuning Robustness Benchmark (CIFR)，用于评估大语言模型（LLM）微调API在面对基于密码编码攻击时的安全防护能力，并提出了一种基于线性探针的监控方法，通过分析模型内部激活有效检测加密恶意输入。实验表明，该探针在多种密码变体和家族上均能实现99%以上的检测准确率，并优于现有前沿监控方法，具备良好的泛化能力和实际部署价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Towards_Safeguarding_LLM_Fine-tuning_APIs_against_Cipher_Attacks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="128-Mind-the-Gap-Time-of-Check-to-Time-of-Use-Vulnerabilities-in-LLM-Enabled-Agentspdf"><a href="#128-Mind-the-Gap-Time-of-Check-to-Time-of-Use-Vulnerabilities-in-LLM-Enabled-Agentspdf" class="headerlink" title="128. Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agentspdf"></a>128. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17155v1">Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Mind_the_Gap__Time-of-Check_to_Time-of-Use_Vulnerabilities_in_LLM-Enabled_Agents.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Oregon State University</span></p>
<p>本文首次系统性研究了大语言模型（LLM）驱动的智能体中的“检查时到使用时（TOCTOU）”安全漏洞，提出TOCTOU-Bench基准测试集，涵盖66项真实用户任务，用于评估此类漏洞。作者借鉴系统安全领域的方法，提出三类防御措施：提示重写、状态完整性监控和工具融合，并通过实验验证其可显著减少漏洞数量和攻击窗口。结论表明综合防御措施可将TOCTOU漏洞发生率从12%降至8%，攻击窗口缩短95%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Mind_the_Gap_Time-of-Check_to_Time-of-Use_Vulnerabilities_in_LLM-Enabled_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="129-MoE-Beyond-Learning-Based-Expert-Activation-Prediction-on-Edge-Devicespdf"><a href="#129-MoE-Beyond-Learning-Based-Expert-Activation-Prediction-on-Edge-Devicespdf" class="headerlink" title="129. MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devicespdf"></a>129. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17137v1">MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MoE-Beyond__Learning-Based_Expert_Activation_Prediction_on_Edge_Devices.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Univ. of Pennsylvania</span></p>
<p>MoE-Beyond提出了一种基于学习的专家激活预测方法，将专家选择建模为多标签序列预测任务，并利用轻量级Transformer在6600万专家激活轨迹上训练，实现了专家激活的高准确率和高缓存命中率。实验结果表明，该方法在边缘设备上可将GPU缓存命中率从17%提升至72%，显著优于现有启发式方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MoE-Beyond_Learning-Based_Expert_Activation_Prediction_on_Edge_Devices.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="130-PowerChain-Automating-Distribution-Grid-Analysis-with-Agentic-AI-Workflowspdf"><a href="#130-PowerChain-Automating-Distribution-Grid-Analysis-with-Agentic-AI-Workflowspdf" class="headerlink" title="130. PowerChain: Automating Distribution Grid Analysis with Agentic AI Workflowspdf"></a>130. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17094v1">PowerChain: Automating Distribution Grid Analysis with Agentic AI Workflows</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PowerChain__Automating_Distribution_Grid_Analysis_with_Agentic_AI_Workflows.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Vermont</span></p>
<p>本文提出了PowerChain系统，结合基于Agent的AI编排架构和大语言模型（LLM）函数调用，实现对配电网分析任务的自动化。方法通过上下文学习，动态生成和执行与电力系统相关的函数序列，无需模型微调即可处理复杂的未见任务。实验表明，PowerChain在真实电力数据上能有效生成与专家水平相当的工作流，并显著降低了小型电力公司的技术门槛和成本。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PowerChain_Automating_Distribution_Grid_Analysis_with_Agentic_AI_Workflows.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="131-Linguistic-Neuron-Overlap-Patterns-to-Facilitate-Cross-lingual-Transfer-on-Low-resource-Languagespdf"><a href="#131-Linguistic-Neuron-Overlap-Patterns-to-Facilitate-Cross-lingual-Transfer-on-Low-resource-Languagespdf" class="headerlink" title="131. Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languagespdf"></a>131. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17078v1">Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Linguistic_Neuron_Overlap_Patterns_to_Facilitate_Cross-lingual_Transfer_on_Low-resource_Languages.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Information Science and Technology, Beijing Foreign Studies University</span></p>
<p>本文提出BridgeX-ICL方法，通过探究大语言模型（LLMs）中不同语言之间的神经元重叠模式，利用MUSE字典构建探测数据，激活并量化语言重叠神经元，并基于HSIC指标选择最优桥接语言，提升低资源语言的零样本跨语言推理能力。实验表明，该方法在15对语言和两项跨语言任务中平均提升准确率6%，揭示了LLMs多语言内部机制并验证了桥接神经元促进跨语言迁移的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Linguistic_Neuron_Overlap_Patterns_to_Facilitate_Cross-lingual_Transfer_on_Low-resource_Languages.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="132-GRAID-Synthetic-Data-Generation-with-Geometric-Constraints-and-Multi-Agentic-Reflection-for-Harmful-Content-Detectionpdf"><a href="#132-GRAID-Synthetic-Data-Generation-with-Geometric-Constraints-and-Multi-Agentic-Reflection-for-Harmful-Content-Detectionpdf" class="headerlink" title="132. GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detectionpdf"></a>132. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17057v1">GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detection</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GRAID__Synthetic_Data_Generation_with_Geometric_Constraints_and_Multi-Agentic_Reflection_for_Harmful.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Capital One</span></p>
<p>本论文提出了GRAID，一种结合几何约束和多智能体反思机制的数据增强流水线，利用大型语言模型（LLM）进行有害文本分类任务的数据扩充。方法包括通过受控LLM生成几何均衡样本，随后采用多智能体反思流程引入风格多样性和边界样本，实验结果表明GRAID可提升主流文本分类器在有害内容检测任务上的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GRAID_Synthetic_Data_Generation_with_Geometric_Constraints_and_Multi-Agentic_Reflection_for_Harmful_Content_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="133-Unveiling-the-Latent-Directions-of-Reflection-in-Large-Language-Modelspdf"><a href="#133-Unveiling-the-Latent-Directions-of-Reflection-in-Large-Language-Modelspdf" class="headerlink" title="133. Unveiling the Latent Directions of Reflection in Large Language Modelspdf"></a>133. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16989v1">Unveiling the Latent Directions of Reflection in Large Language Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Unveiling_the_Latent_Directions_of_Reflection_in_Large_Language_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Taiwan University</span></p>
<p>本文提出了一种基于激活操控（activation steering）的方法，系统性分析和解释大语言模型（LLM）中的反思（reflection）机制，将反思划分为无反思、内在反思和触发反思三类，并通过构建隐空间方向向量揭示不同反思水平间的潜在过渡。实验表明，激活操控不仅能发现新的反思触发指令，还能在推理时直接增强或抑制反思行为，且抑制反思比增强更容易。结论指出，反思能力与特定激活模式高度相关，该机制既可用于提升模型安全性，也可能被恶意利用以规避安全措施。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Unveiling_the_Latent_Directions_of_Reflection_in_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="134-ReFactX-Scalable-Reasoning-with-Reliable-Facts-via-Constrained-Generationpdf"><a href="#134-ReFactX-Scalable-Reasoning-with-Reliable-Facts-via-Constrained-Generationpdf" class="headerlink" title="134. ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generationpdf"></a>134. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16983v1">ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ReFactX__Scalable_Reasoning_with_Reliable_Facts_via_Constrained_Generation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Milano-Bicocca</span></p>
<p>本文提出ReFactX方法，通过受约束生成与预构建前缀树索引，使LLM在无需外部检索器或辅助模型情况下，直接高效接入超大规模知识库，提升知识问答任务中的事实性与可扩展性。实验表明，该方法在800M事实规模下仅增加约1%延迟，在多个QA基准上可提升准确率至90%以上，并显著减少幻觉现象。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ReFactX_Scalable_Reasoning_with_Reliable_Facts_via_Constrained_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="135-Robust-Diagram-Reasoning-A-Framework-for-Enhancing-LVLM-Performance-on-Visually-Perturbed-Scientific-Diagramspdf"><a href="#135-Robust-Diagram-Reasoning-A-Framework-for-Enhancing-LVLM-Performance-on-Visually-Perturbed-Scientific-Diagramspdf" class="headerlink" title="135. Robust Diagram Reasoning: A Framework for Enhancing LVLM Performance on Visually Perturbed Scientific Diagramspdf"></a>135. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16972v1">Robust Diagram Reasoning: A Framework for Enhancing LVLM Performance on Visually Perturbed Scientific Diagrams</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Robust_Diagram_Reasoning__A_Framework_for_Enhancing_LVLM_Performance_on_Visually_Perturbed_Scientifi.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Taiyuan University of Science and Technology</span></p>
<p>本文提出了Robust Diagram Reasoning (RDR) 框架，通过自适应多视图与一致性验证机制（AMCV），对科学图表生成多种扰动视图，在推理阶段并行推理并进行自纠正，以提升大视觉语言模型（LVLM）在含噪、模糊、遮挡等视觉退化下的鲁棒性。实验基于新构建的SciDiagram-Robust数据集，结果显示RDR框架能显著提升LVLM的扰动鲁棒性，超越现有开源和闭源模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Robust_Diagram_Reasoning_A_Framework_for_Enhancing_LVLM_Performance_on_Visually_Perturbed_Scientific_Diagrams.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="136-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoningpdf"><a href="#136-Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoningpdf" class="headerlink" title="136. Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoningpdf"></a>136. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16949v1">Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Breaking_the_Exploration_Bottleneck__Rubric-Scaffolded_Reinforcement_Learning_for_General_LLM_Reason.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出了RuscaRL，一种利用checklist风格rubrics作为显式探索支架和可验证奖励的强化学习框架，专为提升LLM在开放任务中的推理能力。方法包括在生成阶段通过分组差异和逐步衰减的方式提供rubric指导，训练阶段用多维rubric打分作为奖励信号。实验结果显示RuscaRL在多项医学和STEM基准上显著优于现有RL方法，提升小模型能力，部分指标超越GPT-4.1和OpenAI-o3。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Breaking_the_Exploration_Bottleneck_Rubric-Scaffolded_Reinforcement_Learning_for_General_LLM_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="137-Limitations-of-refinement-methods-for-weak-to-strong-generalizationpdf"><a href="#137-Limitations-of-refinement-methods-for-weak-to-strong-generalizationpdf" class="headerlink" title="137. Limitations of refinement methods for weak to strong generalizationpdf"></a>137. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17018v1">Limitations of refinement methods for weak to strong generalization</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Limitations_of_refinement_methods_for_weak_to_strong_generalization.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Michigan</span></p>
<p>本文采用广泛的概率框架，分析了弱训练和标签修正方法在大语言模型（LLM）弱到强泛化任务中的表现，证明二者都存在不可消除的误差，无法达到理论最优。结论指出，现有弱到强泛化技术（如弱训练和标签修正）无法一致收敛于目标函数，强调未来需开发兼具可实践性和理论最优性的替代方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Limitations_of_refinement_methods_for_weak_to_strong_generalization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="138-Attention-Layers-Add-Into-Low-Dimensional-Residual-Subspacespdf"><a href="#138-Attention-Layers-Add-Into-Low-Dimensional-Residual-Subspacespdf" class="headerlink" title="138. Attention Layers Add Into Low-Dimensional Residual Subspacespdf"></a>138. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16929v1">Attention Layers Add Into Low-Dimensional Residual Subspaces</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Attention_Layers_Add_Into_Low-Dimensional_Residual_Subspaces.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Innovation Institute</span></p>
<p>该论文发现Transformer中的注意力层输出被限制在一个低维子空间，表明约60%的方向包含99%的方差，且这种低秩结构是由输出投影矩阵引起并导致稀疏字典学习中大量dead feature。文章提出Active Subspace Initialization方法，将稀疏自编码器的特征初始化到激活空间的主子空间，显著减少dead feature数量（从87%降至1%以下）并提升重建质量，该方法可推广到其他稀疏字典学习模型，有效提升大语言模型的可解释性和效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Attention_Layers_Add_Into_Low-Dimensional_Residual_Subspaces.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="139-TextOnly-A-Unified-Function-Portal-for-Text-Related-Functions-on-Smartphonespdf"><a href="#139-TextOnly-A-Unified-Function-Portal-for-Text-Related-Functions-on-Smartphonespdf" class="headerlink" title="139. TextOnly: A Unified Function Portal for Text-Related Functions on Smartphonespdf"></a>139. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16926v1">TextOnly: A Unified Function Portal for Text-Related Functions on Smartphones</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TextOnly__A_Unified_Function_Portal_for_Text-Related_Functions_on_Smartphones.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了TextOnly，一种集成了大语言模型（LLM）和BERT模型的统一功能入口，用户可通过在单一文本框中输入原始文本即可访问多应用中的文本相关功能。方法包括利用LLM提供通用知识和冷启动能力，BERT模型实现用户个性化和快速推断，并采用上下文信息提升预测准确率。实验证明，TextOnly在真实用户中取得了71.35%的Top-1准确率，支持更多功能且输入更精简，用户满意度高，且系统能持续自我优化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TextOnly_A_Unified_Function_Portal_for_Text-Related_Functions_on_Smartphones.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="140-Beyond-Emotion-Recognition-A-Multi-Turn-Multimodal-Emotion-Understanding-and-Reasoning-Benchmarkpdf"><a href="#140-Beyond-Emotion-Recognition-A-Multi-Turn-Multimodal-Emotion-Understanding-and-Reasoning-Benchmarkpdf" class="headerlink" title="140. Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmarkpdf"></a>140. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16859v1">Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Beyond_Emotion_Recognition__A_Multi-Turn_Multimodal_Emotion_Understanding_and_Reasoning_Benchmark.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hefei University of Technology</span></p>
<p>本论文提出了MTMEUR多轮多模态情感理解与推理基准，涵盖1451个真实场景视频及5101个递进式多维情感问题，并设计多智能体框架分别聚焦背景、角色、事件和决策信息协作推理。实验表明该方法提升了多模态大模型对复杂情感推理任务的表现，但现有MLLM在负面情感推理上仍存在显著挑战，MTMEUR可作为未来模型改进的重要资源。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Beyond_Emotion_Recognition_A_Multi-Turn_Multimodal_Emotion_Understanding_and_Reasoning_Benchmark.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="141-RADAR-A-Reasoning-Guided-Attribution-Framework-for-Explainable-Visual-Data-Analysispdf"><a href="#141-RADAR-A-Reasoning-Guided-Attribution-Framework-for-Explainable-Visual-Data-Analysispdf" class="headerlink" title="141. RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysispdf"></a>141. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16850v1">RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysis</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/RADAR__A_Reasoning-Guided_Attribution_Framework_for_Explainable_Visual_Data_Analysis.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Massachusetts Institute of Technology</span></p>
<p>该论文提出RADAR框架，通过多模态大语言模型（如InternLM-XComposer2）生成图表问题的逐步数学推理，并实现答案及推理过程的可视化归因。方法包括构建包含问题、答案、推理、归因注释的高质量数据集，并采用Partial-LoRA视觉自适应技术，通过滑动窗口与多模态特征对齐生成图表区域归因。结论：RADAR归因准确率提升15%，自动推理与归因显著提高多模态模型的可信度和解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/RADAR_A_Reasoning-Guided_Attribution_Framework_for_Explainable_Visual_Data_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="142-Quantifying-Sycophancy-as-Deviations-from-Bayesian-Rationality-in-LLMspdf"><a href="#142-Quantifying-Sycophancy-as-Deviations-from-Bayesian-Rationality-in-LLMspdf" class="headerlink" title="142. Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMspdf"></a>142. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16846v1">Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Quantifying_Sycophancy_as_Deviations_from_Bayesian_Rationality_in_LLMs.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northeastern University</span></p>
<p>该论文提出用贝叶斯框架量化大语言模型(LLM)中的谄媚（sycophancy）现象，将其视为偏离贝叶斯理性行为的表现，区分合理与非理性的信念更新。作者在三类任务（对话预测、道德判断、文化可接受性）上，利用不同模型和概率判断方法，系统评估了用户视角下LLM的谄媚程度及对贝叶斯误差的影响。结论显示：LLM普遍非贝叶斯理性，谄媚显著加剧了模型对用户倾向结果的概率偏移，并在多任务&#x2F;模型下增加了贝叶斯误差，体现出谄媚对模型推理的负面影响。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Quantifying_Sycophancy_as_Deviations_from_Bayesian_Rationality_in_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="143-Natural-Language-Satisfiability-Exploring-the-Problem-Distribution-and-Evaluating-Transformer-based-Language-Modelspdf"><a href="#143-Natural-Language-Satisfiability-Exploring-the-Problem-Distribution-and-Evaluating-Transformer-based-Language-Modelspdf" class="headerlink" title="143. Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Modelspdf"></a>143. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17153v1">Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Natural_Language_Satisfiability__Exploring_the_Problem_Distribution_and_Evaluating_Transformer-based.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Department of Computer Science, University of Manchester</span></p>
<p>本文提出以自然语言为表达的可满足性问题，系统构建涵盖多种语言片段和多种计算复杂度的数据集，并实证评估T5、DeBERTa等主流Transformer语言模型在归纳、推理和零样本自然语言可满足性判定上的表现。结果表明，模型在复杂度更高的语言片段上准确率明显下降，且难以泛化和学习底层逻辑推理规则，需进一步研究提升模型的逻辑推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Natural_Language_Satisfiability_Exploring_the_Problem_Distribution_and_Evaluating_Transformer-based_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="144-Token-Homogenization-under-Positional-Biaspdf"><a href="#144-Token-Homogenization-under-Positional-Biaspdf" class="headerlink" title="144. Token Homogenization under Positional Biaspdf"></a>144. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17126v1">Token Homogenization under Positional Bias</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Token_Homogenization_under_Positional_Bias.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HSE University</span></p>
<p>本论文系统提出和实证分析了大语言模型（LLM）中token同质化（token homogenization）现象及其与位置偏置（positional bias）的关系。通过多种新颖与已有的度量方法（如Effective Rank、Maximum Explainable Variance、Schatten范数、MAUVE分数等），在LLaMA-3、Gemma和Qwen模型上，作者发现随着层数加深且输入中关键信息出现显著位置偏置时，token表征趋于同质化，说明位置偏置会显著加剧token信息的丧失。结论认为，位置偏置与token同质化密切相关，这一现象对模型解释性、性能及压缩有重要启示。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Token_Homogenization_under_Positional_Bias.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="145-Zero-shot-Multimodal-Document-Retrieval-via-Cross-modal-Question-Generationpdf"><a href="#145-Zero-shot-Multimodal-Document-Retrieval-via-Cross-modal-Question-Generationpdf" class="headerlink" title="145. Zero-shot Multimodal Document Retrieval via Cross-modal Question Generationpdf"></a>145. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17079v1">Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Zero-shot_Multimodal_Document_Retrieval_via_Cross-modal_Question_Generation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Yonsei University</span></p>
<p>该论文提出PREMIR框架，通过多模态大语言模型(MLLM)生成跨模态预问题(preQs)，将文档分解为细粒度多模态token级表示，构建共享嵌入空间，实现多模态文档的零样本检索。PREMIR在闭域和多语言基准任务中，显著优于现有强基线，无需额外训练，强泛化且适用于真实场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Zero-shot_Multimodal_Document_Retrieval_via_Cross-modal_Question_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="146-KL-Regularised-Q-Learning-A-Token-level-Action-Value-perspective-on-Online-RLHFpdf"><a href="#146-KL-Regularised-Q-Learning-A-Token-level-Action-Value-perspective-on-Online-RLHFpdf" class="headerlink" title="146. KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHFpdf"></a>146. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17000v1">KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/KL-Regularised_Q-Learning__A_Token-level_Action-Value_perspective_on_Online_RLHF.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cambridge</span></p>
<p>该论文提出KL-regularised Q-Learning (KLQ)算法，作为适用于大语言模型从人类反馈中强化学习（LM-RLHF）的新型token级动作值方法。KLQ通过λ-return回归和特殊的动作值函数分解实现高效训练，并理论上证明了其与PPO方法在特定条件下等价。实验结果表明，KLQ在摘要和对话任务中对齐目标与PPO相当，但在LLM评判中表现更优，具有更清晰的理论动机。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/KL-Regularised_Q-Learning_A_Token-level_Action-Value_perspective_on_Online_RLHF.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="147-TriagerX-Dual-Transformers-for-Bug-Triaging-Tasks-with-Content-and-Interaction-Based-Rankingspdf"><a href="#147-TriagerX-Dual-Transformers-for-Bug-Triaging-Tasks-with-Content-and-Interaction-Based-Rankingspdf" class="headerlink" title="147. TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankingspdf"></a>147. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16860v1">TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TriagerX__Dual_Transformers_for_Bug_Triaging_Tasks_with_Content_and_Interaction_Based_Rankings.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Calgary</span></p>
<p>本文提出了一种用于自动化Bug分派的混合框架TriagerX，通过集成两个预训练语言模型（PLM）的双transformer架构（CBR）和基于开发者历史交互的排序方法（IBR），增强了对bug内容和开发者经验的综合建模。实验结果显示，TriagerX在多个公开和工业数据集上相较于九种SOTA基线模型，在Top-1和Top-3准确率上均提升10%以上，工业部署反馈也验证了其实用性和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TriagerX_Dual_Transformers_for_Bug_Triaging_Tasks_with_Content_and_Interaction_Based_Rankings.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="148-In-Context-Algorithm-Emulation-in-Fixed-Weight-Transformerspdf"><a href="#148-In-Context-Algorithm-Emulation-in-Fixed-Weight-Transformerspdf" class="headerlink" title="148. In-Context Algorithm Emulation in Fixed-Weight Transformerspdf"></a>148. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17550v1">In-Context Algorithm Emulation in Fixed-Weight Transformers</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/In-Context_Algorithm_Emulation_in_Fixed-Weight_Transformers.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Center for Foundation Models and Generative AI, Northwestern University</span></p>
<p>本文提出并严格证明了：即使是最小化的、冻结权重的Transformer架构（仅含softmax self-attention层，无需前馈网络和参数更新），也能通过适当设计输入prompt，实现对一大类经典算法（如梯度下降、线性&#x2F;岭回归）的高精度模拟。关键方法是在prompt中编码算法参数，诱导注意力机制完成所需计算，理论与实验均验证了通过改变prompt可在无需再训练的前提下切换和模拟不同算法。结论：Transformer模型本质上可作为可通过prompt编程的通用算法库，这为in-context learning提供了清晰的理论基础，并对理解大模型泛化能力及prompt设计具有指导意义。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/In-Context_Algorithm_Emulation_in_Fixed-Weight_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="149-Activation-Transport-Operatorspdf"><a href="#149-Activation-Transport-Operatorspdf" class="headerlink" title="149. Activation Transport Operatorspdf"></a>149. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17540v1">Activation Transport Operators</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Activation_Transport_Operators.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cambridge</span></p>
<p>本文提出了Activation Transport Operators（ATO），这是一种在变换器解码器模型中，通过线性映射预测下游残差流向量的新方法。利用稀疏自动编码器（SAE）进行特征空间投影，并通过回归与因果验证评估特征是否线性传递或由非线性层重新合成。实验结果表明，特征在线性通道中主要在相邻层间传递，随着层距增加线性传递能力减弱，ATO方法为LLM安全、调试和特征流动映射提供实用工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Activation_Transport_Operators.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="150-Efficient-Zero-Shot-Long-Document-Classification-by-Reducing-Context-Through-Sentence-Rankingpdf"><a href="#150-Efficient-Zero-Shot-Long-Document-Classification-by-Reducing-Context-Through-Sentence-Rankingpdf" class="headerlink" title="150. Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Rankingpdf"></a>150. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17490v1">Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Ranking</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Efficient_Zero-Shot_Long_Document_Classification_by_Reducing_Context_Through_Sentence_Ranking.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Pune Institute of Computer Technology</span></p>
<p>该论文提出了一种高效的零样本长文档分类方法，通过TF-IDF句子排序实现上下文压缩，无需修改Transformer模型结构。该方法将短文本训练模型（如BERT）直接应用于长文档，通过筛选最具信息性的句子，显著减少输入长度和推理时间而保持分类准确率，验证于MahaNews马拉地语长新闻数据集，最高可提升推理效率35%，部分情况下精度超过全文基线。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Efficient_Zero-Shot_Long_Document_Classification_by_Reducing_Context_Through_Sentence_Ranking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="151-MahaParaphrase-A-Marathi-Paraphrase-Detection-Corpus-and-BERT-based-Modelspdf"><a href="#151-MahaParaphrase-A-Marathi-Paraphrase-Detection-Corpus-and-BERT-based-Modelspdf" class="headerlink" title="151. MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Modelspdf"></a>151. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17444v1">MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MahaParaphrase__A_Marathi_Paraphrase_Detection_Corpus_and_BERT-based_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Pune Institute of Computer Technology</span></p>
<p>该论文提出了MahaParaphrase数据集，这是首个包含8000对马拉地语句子（等量的同义与非同义句对）并经过人工标注的高质量释义检测语料。作者基于词重叠度将数据分为五个桶，并评测了Muril、mBERT、IndicBERT和MahaBERT等多种BERT模型，结果显示MahaBERT在该任务中表现最佳（F1&#x3D;88.7%）。结论：MahaParaphrase为马拉地语自然语言处理提供了重要资源，推动了低资源语言释义检测的研究。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MahaParaphrase_A_Marathi_Paraphrase_Detection_Corpus_and_BERT-based_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="152-Large-Language-Models-as-Universal-Predictors-An-Empirical-Study-on-Small-Tabular-Datasetspdf"><a href="#152-Large-Language-Models-as-Universal-Predictors-An-Empirical-Study-on-Small-Tabular-Datasetspdf" class="headerlink" title="152. Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasetspdf"></a>152. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17391v1">Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Large_Language_Models_as_Universal_Predictors__An_Empirical_Study_on_Small_Tabular_Datasets.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute for Language and Speech Processing, Athena Research Center</span></p>
<p>本文系统评估了主流大语言模型（如GPT-5、GPT-4o、Gemini-2.5-Flash等）在小规模结构化表格数据上的分类、回归和聚类任务表现，通过少样本提示（few-shot prompting）实现零训练预测。实验发现，LLMs在分类任务中可作为有效零训练基线，表现接近传统机器学习模型，但在回归和聚类任务中表现欠佳，主要受限于连续数值预测能力和无监督聚类中的上下文学习缺失。结论：LLMs适合于表格分类任务的快速原型开发，但在回归与聚类任务仍有限制。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Large_Language_Models_as_Universal_Predictors_An_Empirical_Study_on_Small_Tabular_Datasets.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="153-GRAPH-R1-Incentivizing-the-Zero-Shot-Graph-Learning-Capability-in-LLMs-via-Explicit-Reasoningpdf"><a href="#153-GRAPH-R1-Incentivizing-the-Zero-Shot-Graph-Learning-Capability-in-LLMs-via-Explicit-Reasoningpdf" class="headerlink" title="153. GRAPH-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoningpdf"></a>153. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17387v1">GRAPH-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Graph-R1__Incentivizing_the_Zero-Shot_Graph_Learning_Capability_in_LLMs_via_Explicit_Reasoning.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>该论文提出了一种完全不依赖GNN的图学习方法GRAPH-R1，将节点分类、链路预测和图分类等任务转化为大语言模型（LLM）可处理的文本推理问题。通过构建包含详细推理链条的新型数据集，并结合任务特定的推理模板和基于强化学习的优化，GRAPH-R1实现在跨任务、跨领域下的零样本泛化能力，并显著优于现有方法，推理过程具备可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GRAPH-R1_Incentivizing_the_Zero-Shot_Graph_Learning_Capability_in_LLMs_via_Explicit_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="154-Mimicking-the-Physicist’s-Eye-A-VLM-centric-Approach-for-Physics-Formula-Discoverypdf"><a href="#154-Mimicking-the-Physicist’s-Eye-A-VLM-centric-Approach-for-Physics-Formula-Discoverypdf" class="headerlink" title="154. Mimicking the Physicist’s Eye: A VLM-centric Approach for Physics Formula Discoverypdf"></a>154. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17380v1">Mimicking the Physicist’s Eye: A VLM-centric Approach for Physics Formula Discovery</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Mimicking_the_Physicist's_Eye_A_VLM-centric_Approach_for_Physics_Formula_Discovery.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">UNC–Chapel Hill</span></p>
<p>本文提出VIPER-R1，一种视觉-语言多模态模型，通过Motion Structure Induction（MSI）与Causal Chain-of-Thought（C-CoT）监督训练，结合Reward-Guided Symbolic Calibration（RGSC）强化学习，实现物理公式的自动发现。模型推理阶段利用外部符号回归工具进行Symbolic Residual Realignment（SR²），将理论公式与实际数据对齐。实验结果显示VIPER-R1在PhysSymbol数据集结构匹配和精度方面均超过SOTA基线，实现了更精确和可解释的物理规律发现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Mimicking_the_Physicist%E2%80%99s_Eye_A_VLM-centric_Approach_for_Physics_Formula_Discovery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="155-Evolving-Collective-Cognition-in-Human–Agent-Hybrid-Societies-How-Agents-Form-Stances-and-Boundariespdf"><a href="#155-Evolving-Collective-Cognition-in-Human–Agent-Hybrid-Societies-How-Agents-Form-Stances-and-Boundariespdf" class="headerlink" title="155. Evolving Collective Cognition in Human–Agent Hybrid Societies: How Agents Form Stances and Boundariespdf"></a>155. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17366v1">Evolving Collective Cognition in Human–Agent Hybrid Societies: How Agents Form Stances and Boundaries</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Evolving_Collective_Cognition_in_Human-Agent_Hybrid_Societies__How_Agents_Form_Stances_and_Boundarie.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">William &amp; Mary</span></p>
<p>该论文提出了CMASE（Computational Multi-Agent Society Experiment）框架，将生成式多智能体建模与虚拟民族志方法结合，系统研究了人-智能体混合社会中立场分化和社会边界形成的机制。通过三项实验证明，智能体可基于语言互动自发形成立场并重塑社会结构，对人类干预表现出不同响应，结论指出，智能体的社会结构不由预设身份决定，而是在互动中动态生成，提示未来AI社会建模应关注内生认知机制与互动动力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Evolving_Collective_Cognition_in_Human%E2%80%93Agent_Hybrid_Societies_How_Agents_Form_Stances_and_Boundaries.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="156-Trust-Me-I-Know-This-Function-Hijacking-LLM-Static-Analysis-using-Biaspdf"><a href="#156-Trust-Me-I-Know-This-Function-Hijacking-LLM-Static-Analysis-using-Biaspdf" class="headerlink" title="156. Trust Me, I Know This Function: Hijacking LLM Static Analysis using Biaspdf"></a>156. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17361v1">Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Trust_Me,_I_Know_This_Function__Hijacking_LLM_Static_Analysis_using_Bias.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ben Gurion University of the Negev</span></p>
<p>该论文提出并系统性分析了一种针对大语言模型（LLM）代码静态分析的全新攻击方式——Familiar Pattern Attack（FPA）。方法上，作者通过自动化黑盒算法，利用LLM对熟悉编程模式的抽象偏见，实现了对多种主流模型（GPT-4o、Claude 3.5、Gemini 2.0）和多种编程语言的跨模型、跨语言迁移攻击。结论表明，这一攻击方式可以在不影响实际代码运行的前提下，误导LLM对代码行为的判断，且难以通过提示工程、数据去重或简单防御策略消除，对自动化代码审计、代码安全及敏感逻辑防护带来广泛挑战和安全隐患。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Trust_Me_I_Know_This_Function_Hijacking_LLM_Static_Analysis_using_Bias.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="157-Agentic-AI-for-Software-thoughts-from-Software-Engineering-communitypdf"><a href="#157-Agentic-AI-for-Software-thoughts-from-Software-Engineering-communitypdf" class="headerlink" title="157. Agentic AI for Software: thoughts from Software Engineering communitypdf"></a>157. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17343v1">Agentic AI for Software: thoughts from Software Engineering community</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Agentic_AI_for_Software__thoughts_from_Software_Engineering_community.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Singapore</span></p>
<p>本文探讨了以大语言模型（LLM）为核心驱动的AI自主代理在软件工程中的应用，强调不仅限于代码生成，还包括自动化的问题解决、意图推断和代码验证。作者以AutoCodeRover为例，提出通过程序表示和分析工具推断开发者意图，实现更可信的自动化软件工程，并指出未来AI代理将在代码验证和安全审计中发挥关键作用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Agentic_AI_for_Software_thoughts_from_Software_Engineering_community.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="158-Capturing-Legal-Reasoning-Paths-from-Facts-to-Law-in-Court-Judgments-using-Knowledge-Graphspdf"><a href="#158-Capturing-Legal-Reasoning-Paths-from-Facts-to-Law-in-Court-Judgments-using-Knowledge-Graphspdf" class="headerlink" title="158. Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphspdf"></a>158. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17340v1">Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphs</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Capturing_Legal_Reasoning_Paths_from_Facts_to_Law_in_Court_Judgments_using_Knowledge_Graphs.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Tokyo</span></p>
<p>本文提出了一种利用结构化知识图谱（LKG）对日本行政法院判决中的法律推理路径进行建模的方法。该方法通过三步流程：1）使用GPT-4o提示抽取事实、法律规范、法律应用和条文节点；2）规范法律条文引用并构建节点间关系；3）基于知识图谱实现从事实到法律条文的检索任务。实验结果表明，LKG方法在法律条文检索准确率和召回率上显著优于仅用大语言模型和检索增强方法，能够更清晰地揭示和解释司法推理结构。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Capturing_Legal_Reasoning_Paths_from_Facts_to_Law_in_Court_Judgments_using_Knowledge_Graphs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="159-DropLoRA-Sparse-Low-Rank-Adaptation-for-Parameter-Efficient-Fine-Tuningpdf"><a href="#159-DropLoRA-Sparse-Low-Rank-Adaptation-for-Parameter-Efficient-Fine-Tuningpdf" class="headerlink" title="159. DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuningpdf"></a>159. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17337v1">DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/DropLoRA__Sparse_Low-Rank_Adaptation_for_Parameter-Efficient_Fine-Tuning.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>该论文提出了一种新的参数高效微调方法DropLoRA，通过在LoRA的两个低秩矩阵之间插入动态剪枝模块，模拟低秩子空间学习，从而提升微调大语言模型（LLM）时的性能。实验结果显示，DropLoRA在常识推理、数学推理、代码生成和指令跟随等多项LLM任务上均优于LoRA及其他主流PEFT方法，且不增加训练和推理成本。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/DropLoRA_Sparse_Low-Rank_Adaptation_for_Parameter-Efficient_Fine-Tuning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="160-Omne-R1-Learning-to-Reason-with-Memory-for-Multi-hop-Question-Answeringpdf"><a href="#160-Omne-R1-Learning-to-Reason-with-Memory-for-Multi-hop-Question-Answeringpdf" class="headerlink" title="160. Omne-R1: Learning to Reason with Memory for Multi-hop Question Answeringpdf"></a>160. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17330v1">Omne-R1: Learning to Reason with Memory for Multi-hop Question Answering</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Omne-R1__Learning_to_Reason_with_Memory_for_Multi-hop_Question_Answering.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tanka Inc.</span></p>
<p>Omne-R1提出了一种多阶段训练框架，结合两轮强化学习和一次监督微调，集成大型语言模型推理能力，实现对无模式知识图谱的多跳问答。通过自动构建领域无关知识图谱和生成多跳QA数据，方法在复杂3+跳问题上取得显著性能提升，具备良好跨领域泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Omne-R1_Learning_to_Reason_with_Memory_for_Multi-hop_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="161-CultranAI-at-PalmX-2025-Data-Augmentation-for-Cultural-Knowledge-Representationpdf"><a href="#161-CultranAI-at-PalmX-2025-Data-Augmentation-for-Cultural-Knowledge-Representationpdf" class="headerlink" title="161. CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representationpdf"></a>161. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17324v1">CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CultranAI_at_PalmX_2025__Data_Augmentation_for_Cultural_Knowledge_Representation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Qatar University</span></p>
<p>该论文提出了CultranAI系统，通过数据增强和LoRA微调提升大型语言模型在阿拉伯文化知识表达任务中的表现。方法包括合并PalmX、Palm及新扩展的22K多项选择题数据集，采用LoRA和QLoRA对Fanar-1-9B-Instruct模型进行微调，实验显示数据增强显著提升了模型在文化相关任务的准确率，并在PalmX竞赛中取得第5名。结论：多源数据增强和适当的微调技术能够提升LLM对阿拉伯文化知识的理解和表达能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CultranAI_at_PalmX_2025_Data_Augmentation_for_Cultural_Knowledge_Representation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="162-Chinese-Court-Simulation-with-LLM-Based-Agent-Systempdf"><a href="#162-Chinese-Court-Simulation-with-LLM-Based-Agent-Systempdf" class="headerlink" title="162. Chinese Court Simulation with LLM-Based Agent Systempdf"></a>162. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17322v1">Chinese Court Simulation with LLM-Based Agent System</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Chinese_Court_Simulation_with_LLM-Based_Agent_System.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出SimCourt框架，基于中国真实庭审流程，利用具备记忆、规划和反思能力的LLM法庭多角色智能体（如法官、检察官、律师等），结合法律条文和案例检索工具，自动模拟五阶段完整庭审过程。实验表明SimCourt在判决预测和过程表现上优于现有人类及自动化基线，显示在法律教育和司法支持中的应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Chinese_Court_Simulation_with_LLM-Based_Agent_System.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="163-CoViPAL-Layer-wise-Contextualized-Visual-Token-Pruning-for-Large-Vision-Language-Modelspdf"><a href="#163-CoViPAL-Layer-wise-Contextualized-Visual-Token-Pruning-for-Large-Vision-Language-Modelspdf" class="headerlink" title="163. CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Modelspdf"></a>163. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17243v1">CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CoViPAL__Layer-wise_Contextualized_Visual_Token_Pruning_for_Large_Vision-Language_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science, Wuhan University</span></p>
<p>该论文提出了CoViPAL，一种层级上下文化的视觉token剪枝方法，通过Plug-and-Play Pruning Module（PPM）轻量级分类器，在多层次上预测并移除冗余视觉token，从而减少大型视觉-语言模型（LVLMs）在推理阶段的计算和内存开销。结果显示，CoViPAL能在不显著降低性能的情况下，最多剪枝75%的视觉token，使预填充时间最多缩短60%，并超越了主流训练&#x2F;非训练型剪枝方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CoViPAL_Layer-wise_Contextualized_Visual_Token_Pruning_for_Large_Vision-Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="164-Module-Aware-Parameter-Efficient-Machine-Unlearning-on-Transformerspdf"><a href="#164-Module-Aware-Parameter-Efficient-Machine-Unlearning-on-Transformerspdf" class="headerlink" title="164. Module-Aware Parameter-Efficient Machine Unlearning on Transformerspdf"></a>164. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17233v1">Module-Aware Parameter-Efficient Machine Unlearning on Transformers</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Module-Aware_Parameter-Efficient_Machine_Unlearning_on_Transformers.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出了MAPE-Unlearn模块感知参数高效机器遗忘方法，利用可学习掩码在Transformer的heads和filters模块定位影响关键参数，通过模块级稀疏更新实现高效数据遗忘，并可集成多种主流遗忘算法。实验表明，MAPE-Unlearn在各类Transformer模型和任务中实现了优异的遗忘效果和模型鲁棒性，特别是在连续遗忘和对抗重学攻击场景下表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Module-Aware_Parameter-Efficient_Machine_Unlearning_on_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="165-SSFO-Self-Supervised-Faithfulness-Optimization-for-Retrieval-Augmented-Generationpdf"><a href="#165-SSFO-Self-Supervised-Faithfulness-Optimization-for-Retrieval-Augmented-Generationpdf" class="headerlink" title="165. SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generationpdf"></a>165. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17225v1">SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SSFO__Self-Supervised_Faithfulness_Optimization_for_Retrieval-Augmented_Generation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>本文提出了SSFO（一种自监督的信实性优化方法），通过对比大语言模型在有无检索上下文条件下的响应，自动生成偏好数据对并应用Direct Preference Optimization（DPO）进行对齐训练，无需人工标注或强AI监督，从而提升RAG系统的信实性并减少幻觉。结论表明，SSFO在多数据集、多语言环境下显著提升了上下文信实性，优于现有方法，并且几乎不损害模型的指令跟随能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SSFO_Self-Supervised_Faithfulness_Optimization_for_Retrieval-Augmented_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="166-Exposing-Privacy-Risks-in-Graph-Retrieval-Augmented-Generationpdf"><a href="#166-Exposing-Privacy-Risks-in-Graph-Retrieval-Augmented-Generationpdf" class="headerlink" title="166. Exposing Privacy Risks in Graph Retrieval-Augmented Generationpdf"></a>166. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17222v1">Exposing Privacy Risks in Graph Retrieval-Augmented Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Exposing_Privacy_Risks_in_Graph_Retrieval-Augmented_Generation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Pennsylvania State University</span></p>
<p>本论文系统分析了Graph Retrieval-Augmented Generation（Graph RAG）系统在数据隐私方面的风险，提出并实施专门的数据提取攻击，比较了与传统RAG系统的结构化信息泄漏差异，并探讨了防御机制。结论显示，尽管Graph RAG可减少原始文本泄漏，但其结构化实体和关系信息极易被提取，现有简单防御手段难以有效防护，亟需针对结构特性的隐私保护技术。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Exposing_Privacy_Risks_in_Graph_Retrieval-Augmented_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="167-How-to-make-Medical-AI-Systems-safer-Simulating-Vulnerabilities-and-Threats-in-Multimodal-Medical-RAG-Systemspdf"><a href="#167-How-to-make-Medical-AI-Systems-safer-Simulating-Vulnerabilities-and-Threats-in-Multimodal-Medical-RAG-Systemspdf" class="headerlink" title="167. How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG Systemspdf"></a>167. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17215v1">How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG Systems</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/How_to_make_Medical_AI_Systems_safer__Simulating_Vulnerabilities,_and_Threats_in_Multimodal_Medical_.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Warwick</span></p>
<p>本论文提出MedThreatRAG框架，系统性地模拟和评估针对多模态医学RAG系统的数据投毒攻击，包括文本攻击、视觉攻击和跨模态冲突注入（CMCI），通过在半开放式知识库中注入对抗性图文对，干扰检索、重排序和生成流程。实验结果显示，MedThreatRAG能显著降低主流医学视觉语言模型（如LLaVA-Med-1.5）在IU-Xray和MIMIC-CXR数据集上的F1分数，最大降幅达27.66%，暴露了医学RAG系统在多模态一致性和安全性上的根本缺陷。结论强调需采用更具威胁感知和多模态一致性校验的防御机制，以保障医疗AI的可靠性和安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/How_to_make_Medical_AI_Systems_safer_Simulating_Vulnerabilities_and_Threats_in_Multimodal_Medical_RAG_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="168-Large-Language-Model-Based-Automatic-Formulation-for-Stochastic-Optimization-Modelspdf"><a href="#168-Large-Language-Model-Based-Automatic-Formulation-for-Stochastic-Optimization-Modelspdf" class="headerlink" title="168. Large Language Model-Based Automatic Formulation for Stochastic Optimization Modelspdf"></a>168. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17200v1">Large Language Model-Based Automatic Formulation for Stochastic Optimization Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Large_Language_Model-Based_Automatic_Formulation_for_Stochastic_Optimization_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Ohio State University</span></p>
<p>本论文首次系统性研究了大语言模型（LLM），尤其是ChatGPT，在将自然语言描述自动转化为随机优化模型（如联合与个体机会约束模型、两阶段随机线性规划SLP-2）中的表现。作者提出多智能体协作、链式思维、多样化提示工程，并设计了结构化软评分体系，较好地度量了生成模型的结构质量和部分正确性。实验表明，GPT-4-Turbo配合cot_s_instructions及agentic提示方法能高效且准确地完成随机优化建模任务，减少错误，推动基于语言的智能建模流程发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Large_Language_Model-Based_Automatic_Formulation_for_Stochastic_Optimization_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="169-BudgetThinker-Empowering-Budget-aware-LLM-Reasoning-with-Control-Tokenspdf"><a href="#169-BudgetThinker-Empowering-Budget-aware-LLM-Reasoning-with-Control-Tokenspdf" class="headerlink" title="169. BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokenspdf"></a>169. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17196v1">BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/BudgetThinker__Empowering_Budget-aware_LLM_Reasoning_with_Control_Tokens.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute for AI Industry Research (AIR), Tsinghua University</span></p>
<p>BudgetThinker提出了一种通过在推理过程中周期性插入剩余预算提示控制token的方法，实现了对大语言模型（LLM）推理长度的精细控制。该方法采用两阶段训练流程，先通过有监督微调(SFT)让模型熟悉预算约束，再用引入长度奖励的强化学习优化模型，使其兼顾准确性与预算遵循。实验证明，BudgetThinker在多个数学推理基准测试中，在不同预算下均优于现有高效推理方法，能更好地兼顾推理准确率和预算控制，适合资源受限与实时场景部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/BudgetThinker_Empowering_Budget-aware_LLM_Reasoning_with_Control_Tokens.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="170-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMspdf"><a href="#170-PosterGen-Aesthetic-Aware-Paper-to-Poster-Generation-via-Multi-Agent-LLMspdf" class="headerlink" title="170. PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMspdf"></a>170. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17188v1">PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PosterGen__Aesthetic-Aware_Paper-to-Poster_Generation_via_Multi-Agent_LLMs.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stony Brook University</span></p>
<p>本文提出了PosterGen，一种基于多智能体大语言模型（LLM）的学术论文自动海报生成框架。方法上，PosterGen通过解析、策展、空间布局、风格化和渲染等专用智能体协作，结合设计美学原则（叙事结构、三栏布局、配色、字体层级），自动生成兼具内容准确性和视觉美感的学术海报。结论实验表明，PosterGen在内容忠实度与设计美观性方面均显著优于现有方法，生成海报无需大量人工优化，具备实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PosterGen_Aesthetic-Aware_Paper-to-Poster_Generation_via_Multi-Agent_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="171-LLM-Assertiveness-can-be-Mechanistically-Decomposed-into-Emotional-and-Logical-Componentspdf"><a href="#171-LLM-Assertiveness-can-be-Mechanistically-Decomposed-into-Emotional-and-Logical-Componentspdf" class="headerlink" title="171. LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Componentspdf"></a>171. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17182v1">LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LLM_Assertiveness_can_be_Mechanistically_Decomposed_into_Emotional_and_Logical_Components.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Cardiff University</span></p>
<p>该论文通过对Llama-3.2模型在人工标注的自信表达数据集上的微调，利用机制解释方法分析模型层级激活，发现高自信表达在模型内部可分解为情感和逻辑两个正交子成分。结论指出两类子成分在模型预测中产生不同因果影响，为解释LLM过度自信行为及干预机制提供了新思路。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LLM_Assertiveness_can_be_Mechanistically_Decomposed_into_Emotional_and_Logical_Components.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="172-MaRVL-QA-A-Benchmark-for-Mathematical-Reasoning-over-Visual-Landscapespdf"><a href="#172-MaRVL-QA-A-Benchmark-for-Mathematical-Reasoning-over-Visual-Landscapespdf" class="headerlink" title="172. MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapespdf"></a>172. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17180v1">MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MaRVL-QA__A_Benchmark_for_Mathematical_Reasoning_over_Visual_Landscapes.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Waymo</span></p>
<p>本文提出MaRVL-QA基准，专为定量评估多模态大语言模型（MLLMs）在数学与空间推理中的能力，包含拓扑计数和变换识别两大任务，并通过严格的函数库和多重过滤生成高质量问答对。实验结果显示，即便是最先进的MLLMs在抽象数学可视化推理任务上仍存在显著短板，表明当前模型缺乏系统化空间与程序化推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MaRVL-QA_A_Benchmark_for_Mathematical_Reasoning_over_Visual_Landscapes.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="173-Evaluating-Retrieval-Augmented-Generation-Strategies-for-Large-Language-Models-in-Travel-Mode-Choice-Predictionpdf"><a href="#173-Evaluating-Retrieval-Augmented-Generation-Strategies-for-Large-Language-Models-in-Travel-Mode-Choice-Predictionpdf" class="headerlink" title="173. Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Predictionpdf"></a>173. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17527v1">Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Prediction</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Evaluating_Retrieval-Augmented_Generation_Strategies_for_Large_Language_Models_in_Travel_Mode_Choice.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Texas at Austin</span></p>
<p>该论文提出了将检索增强生成（RAG）与大语言模型（LLM）结合，用于旅行方式选择预测，开发了包括零样本预测、基础RAG、平衡检索、交叉编码器重排序等多种策略，并在GPT-4o、o3、o4-mini三种架构上进行系统比较。结果显示，RAG显著提升了预测准确率，尤其是GPT-4o结合平衡检索和重排序准确率达80.8%，优于传统统计和机器学习方法，同时LLM具备更强泛化能力，结论强调检索策略需与模型能力匹配以发挥最大潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Evaluating_Retrieval-Augmented_Generation_Strategies_for_Large_Language_Models_in_Travel_Mode_Choice_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="174-MoE-Inference-Bench-Performance-Evaluation-of-Mixture-of-Expert-Large-Language-and-Vision-Modelspdf"><a href="#174-MoE-Inference-Bench-Performance-Evaluation-of-Mixture-of-Expert-Large-Language-and-Vision-Modelspdf" class="headerlink" title="174. MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Modelspdf"></a>174. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17467v1">MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MoE-Inference-Bench__Performance_Evaluation_of_Mixture_of_Expert_Large_Language_and_Vision_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Argonne National Laboratory</span></p>
<p>该论文提出了MoE-Inference-Bench基准套件，系统评估多种Mixture of Experts（MoE）大语言模型和视觉语言模型在不同硬件（主要为Nvidia H100 GPU）及多种算法优化（如量化、剪枝、推理并行、推测解码和Fused MoE）下的推理性能。实验表明，超参数配置和硬件优化对MoE推理效率影响显著，FP8量化和单专家激活可提升吞吐量20-80%，视觉语言模型推理延迟显著高于纯文本模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MoE-Inference-Bench_Performance_Evaluation_of_Mixture_of_Expert_Large_Language_and_Vision_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="175-AdaptiveK-Sparse-Autoencoders-Dynamic-Sparsity-Allocation-for-Interpretable-LLM-Representationspdf"><a href="#175-AdaptiveK-Sparse-Autoencoders-Dynamic-Sparsity-Allocation-for-Interpretable-LLM-Representationspdf" class="headerlink" title="175. AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for Interpretable LLM Representationspdf"></a>175. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17320v1">AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for Interpretable LLM Representations</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AdaptiveK_Sparse_Autoencoders__Dynamic_Sparsity_Allocation_for_Interpretable_LLM_Representations.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文提出了AdaptiveK Sparse Autoencoder（AdaptiveK SAE）框架，通过线性探针预测输入文本的复杂度，根据复杂度动态调整稀疏自编码器的激活特征数，实现对大语言模型（LLM）内部表示的自适应分解。实验表明，该方法在多个主流LLM（Pythia-70M、Pythia-160M、Gemma-2-2B）上，相比于固定稀疏度方法，在重构精度、解释方差及余弦相似度等指标上均取得更优表现，并大幅减少超参数调优成本。结论：AdaptiveK SAE可显著提升LLM可解释性与效能，具备良好的扩展性和实用价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/AdaptiveK_Sparse_Autoencoders_Dynamic_Sparsity_Allocation_for_Interpretable_LLM_Representations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="176-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generationpdf"><a href="#176-ClaimGen-CN-A-Large-scale-Chinese-Dataset-for-Legal-Claim-Generationpdf" class="headerlink" title="176. ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generationpdf"></a>176. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17234v1">ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ClaimGen-CN__A_Large-scale_Chinese_Dataset_for_Legal_Claim_Generation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本论文提出ClaimGen-CN，首个大规模中文民事案件法律诉请生成数据集，涵盖100类案由，从真实判决文档中抽取原告事实与诉请，并提出基于事实性与清晰度的细粒度评价指标。此外，作者对主流通用及法律领域大模型在零样本法律诉请生成任务上的表现进行了系统评测，发现当前大模型在事实准确性和表达清晰度上存在显著不足。结论指出，该数据集为法律AI研究提供了基础，未来需针对法律文本生成特性改进大模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ClaimGen-CN_A_Large-scale_Chinese_Dataset_for_Legal_Claim_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="177-MMTok-Multimodal-Coverage-Maximization-for-Efficient-Inference-of-VLMspdf"><a href="#177-MMTok-Multimodal-Coverage-Maximization-for-Efficient-Inference-of-VLMspdf" class="headerlink" title="177. MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMspdf"></a>177. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18264v1">MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/MMTok__Multimodal_Coverage_Maximization_for_Efficient_Inference_of_VLMs.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Arizona State University</span></p>
<p>该论文提出MMTok方法，通过最大化覆盖的多模态子集选择策略，联合利用视觉和文本Token的相似性，以贪心算法近似求解子模块覆盖优化问题，实现Vision-Language Model (VLM) 高效推理。实验表明，MMTok在多个基准数据集和不同VLM模型上，能在极大减少视觉Token数量的同时保持原有性能（如在POPE数据集上加速1.87倍，保留98.7%性能），优于单模态剪枝方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/MMTok_Multimodal_Coverage_Maximization_for_Efficient_Inference_of_VLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="178-Type-Compliant-Adaptation-Cascades-Adapting-Programmatic-LM-Workflows-to-Datapdf"><a href="#178-Type-Compliant-Adaptation-Cascades-Adapting-Programmatic-LM-Workflows-to-Datapdf" class="headerlink" title="178. Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Datapdf"></a>178. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18244v1">Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Type-Compliant_Adaptation_Cascades__Adapting_Programmatic_LM_Workflows_to_Data.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Google</span></p>
<p>本论文提出Type-Compliant Adaptation Cascades (tacs) 框架，将复杂LLM工作流建模为可梯度训练的类型化概率程序，通过参数高效的适配和严格类型约束，实现端到端结构化任务适应。实验证明，tacs在结构化推理任务上性能显著优于主流prompt优化方法，尤其在模型容量较小或任务结构要求严格时提升更大。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Type-Compliant_Adaptation_Cascades_Adapting_Programmatic_LM_Workflows_to_Data.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="179-Flash-Sparse-Attention-An-Alternative-Efficient-Implementation-of-Native-Sparse-Attention-Kernelpdf"><a href="#179-Flash-Sparse-Attention-An-Alternative-Efficient-Implementation-of-Native-Sparse-Attention-Kernelpdf" class="headerlink" title="179. Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernelpdf"></a>179. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18224v1">Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Flash_Sparse_Attention__An_Alternative_Efficient_Implementation_of_Native_Sparse_Attention_Kernel.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>本文提出Flash Sparse Attention (FSA)，通过重构Native Sparse Attention (NSA)的内核循环顺序，实现了对主流LLM在不同GQA分组下的高效稀疏注意力计算。FSA采用KV块外循环和查询token内循环，并引入非连续内存访问优化、在线softmax及累加机制，有效消除NSA在GPU上的padding低效。实验表明，FSA在训练与推理端显著提升系统效率，最高可达3.5倍核级速度提升，并保持准确性。结论：FSA为长上下文LLM提供了更高效的稀疏注意力系统实现，显著加速训练和推理。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Flash_Sparse_Attention_An_Alternative_Efficient_Implementation_of_Native_Sparse_Attention_Kernel.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="180-Unraveling-the-Cognitive-Patterns-of-Large-Language-Models-through-Module-Communitiespdf"><a href="#180-Unraveling-the-Cognitive-Patterns-of-Large-Language-Models-through-Module-Communitiespdf" class="headerlink" title="180. Unraveling the Cognitive Patterns of Large Language Models through Module Communitiespdf"></a>180. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18192v1">Unraveling the Cognitive Patterns of Large Language Models through Module Communities</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Unraveling_the_cognitive_patterns_of_Large_Language_Models_through_module_communities.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Rensselaer Polytechnic Institute</span></p>
<p>本文提出了一种基于网络科学的方法，通过构建多重网络将LLM的架构模块、认知技能和数据集联系起来，采用Louvain社区发现等技术，揭示了大型语言模型内部模块与认知技能之间的对应关系及其分布式特性。结果表明，LLM的认知能力依赖于模块间的动态交互和分布式学习，而非严格的功能模块化，分布式微调策略优于严格的局部微调。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Unraveling_the_Cognitive_Patterns_of_Large_Language_Models_through_Module_Communities.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="181-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answeringpdf"><a href="#181-ST-Raptor-LLM-Powered-Semi-Structured-Table-Question-Answeringpdf" class="headerlink" title="181. ST-Raptor: LLM-Powered Semi-Structured Table Question Answeringpdf"></a>181. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18190v1">ST-Raptor: LLM-Powered Semi-Structured Table Question Answering</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ST-Raptor__LLM-Powered_Semi-Structured_Table_Question_Answering.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出ST-Raptor，一种基于大语言模型的半结构化表格问答框架，通过引入分层正交树（HO-Tree）对复杂表格结构进行建模，并设计原子树操作实现问题分解与操作管道生成，结合两阶段验证机制提升答案准确性和鲁棒性。实验证明，ST-Raptor在SSTQA等基准数据集上较现有方法最高提升20%准确率，有效解决半结构化表格解析和问答难题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ST-Raptor_LLM-Powered_Semi-Structured_Table_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="182-Leveraging-Large-Language-Models-for-Accurate-Sign-Language-Translation-in-Low-Resource-Scenariospdf"><a href="#182-Leveraging-Large-Language-Models-for-Accurate-Sign-Language-Translation-in-Low-Resource-Scenariospdf" class="headerlink" title="182. Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenariospdf"></a>182. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18183v1">Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Leveraging_Large_Language_Models_for_Accurate_Sign_Language_Translation_in_Low-Resource_Scenarios.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Catania</span></p>
<p>该论文提出AulSign，一种结合Retriever、LLM与Sign Mapper模块的手语翻译方法，利用动态prompt和上下文学习，通过将手语与自然语言紧凑描述关联，实现低资源场景下的高效翻译。实验在ASL和LIS数据集上表明，AulSign在spoken-to-sign和sign-to-spoken任务中均优于现有方法，特别适用于数据稀缺环境，并提升了可解释性和可用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Leveraging_Large_Language_Models_for_Accurate_Sign_Language_Translation_in_Low-Resource_Scenarios.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="183-ADLOCO-ADAPTIVE-BATCHING-SIGNIFICANTLY-IMPROVES-COMMUNICATIONS-EFFICIENCY-AND-CONVERGENCE-FOR-LARGE-LANGUAGE-MODELSpdf"><a href="#183-ADLOCO-ADAPTIVE-BATCHING-SIGNIFICANTLY-IMPROVES-COMMUNICATIONS-EFFICIENCY-AND-CONVERGENCE-FOR-LARGE-LANGUAGE-MODELSpdf" class="headerlink" title="183. ADLOCO: ADAPTIVE BATCHING SIGNIFICANTLY IMPROVES COMMUNICATIONS EFFICIENCY AND CONVERGENCE FOR LARGE LANGUAGE MODELSpdf"></a>183. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18182v1">ADLOCO: ADAPTIVE BATCHING SIGNIFICANTLY IMPROVES COMMUNICATIONS EFFICIENCY AND CONVERGENCE FOR LARGE LANGUAGE MODELS</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AdLoCo__adaptive_batching_significantly_improves_communications_efficiency_and_convergence_for_Large.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Moscow Institute of Physics and Technology</span></p>
<p>本文提出AdLoCo方法，通过结合多实例训练（MIT）、自适应批量调整的DiLoCo和梯度累积切换机制，实现了对大语言模型分布式训练的通信效率和收敛速度的显著提升。实验和理论分析均表明，AdLoCo在硬件资源利用、通信开销和训练收敛性方面优于现有方法，尤其适用于资源受限环境。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ADLOCO_ADAPTIVE_BATCHING_SIGNIFICANTLY_IMPROVES_COMMUNICATIONS_EFFICIENCY_AND_CONVERGENCE_FOR_LARGE_LANGUAGE_MODELS.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="184-SEAM-Semantically-Equivalent-Across-Modalities-Benchmark-for-Vision-Language-Modelspdf"><a href="#184-SEAM-Semantically-Equivalent-Across-Modalities-Benchmark-for-Vision-Language-Modelspdf" class="headerlink" title="184. SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Modelspdf"></a>184. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18179v1">SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/SEAM__Semantically_Equivalent_Across_Modalities_Benchmark_for_Vision-Language_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Toronto</span></p>
<p>该论文提出了SEAM基准，专门针对视觉-语言模型（VLMs）跨模态一致性评价，构造了语义等价的文本和图像输入，涵盖国际象棋、化学、音乐和图论四大领域，保证不同模态间信息完全等价。通过对21个主流VLMs的系统评测，发现所有模型在视觉和语言模态间存在显著性能差距，且跨模态一致性低，主要由文本符号化和视觉感知失误导致。结论是当前VLMs尚无法实现模态无关的统一推理，SEAM为后续模型改进和对比提供了严谨基准。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SEAM_Semantically_Equivalent_Across_Modalities_Benchmark_for_Vision-Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="185-The-Computational-Complexity-of-Satisfiability-in-State-Space-Modelspdf"><a href="#185-The-Computational-Complexity-of-Satisfiability-in-State-Space-Modelspdf" class="headerlink" title="185. The Computational Complexity of Satisfiability in State Space Modelspdf"></a>185. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18162v1">The Computational Complexity of Satisfiability in State Space Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_Computational_Complexity_of_Satisfiability_in_State_Space_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Kassel</span></p>
<p>本文系统分析了State Space Models (SSM)的可满足性判定问题（ssmSAT），揭示其在一般情况下为不可判定，但在实际应用常见的有限上下文长度和定宽量化算术两种约束下可判定，并给出了具体的复杂性界限（如NP-Complete、NEXPTIME、PSPACE-Complete、EXPSPACE等）。结论指出，SSM的形式化验证面临显著的计算复杂性挑战，即使在受限环境下也不容忽视。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/The_Computational_Complexity_of_Satisfiability_in_State_Space_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="186-Learning-from-Few-Samples-A-Novel-Approach-for-High-Quality-Malcode-Generationpdf"><a href="#186-Learning-from-Few-Samples-A-Novel-Approach-for-High-Quality-Malcode-Generationpdf" class="headerlink" title="186. Learning from Few Samples: A Novel Approach for High-Quality Malcode Generationpdf"></a>186. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18148v1">Learning from Few Samples: A Novel Approach for High-Quality Malcode Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Learning_from_Few_Samples__A_Novel_Approach_for_High-Quality_Malcode_Generation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huazhong University of Science and Technology</span></p>
<p>本文提出了GANGRL-LLM框架，将生成对抗网络（GANs）与大语言模型（LLMs）结合，通过协同训练，利用判别器的输出作为奖励信号，指导生成器在数据稀缺场景下生成高质量的恶意代码。实验结果表明，该方法在SQL注入攻击代码生成和检测上均表现优异，并具备良好的迁移能力，能有效提升入侵检测系统在样本有限情况下的防御水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Learning_from_Few_Samples_A_Novel_Approach_for_High-Quality_Malcode_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="187-The-AI-Data-Scientistpdf"><a href="#187-The-AI-Data-Scientistpdf" class="headerlink" title="187. The AI Data Scientistpdf"></a>187. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18113v1">The AI Data Scientist</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_AI_Data_Scientist.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mohamed bin Zayed University of Artificial Intelligence</span></p>
<p>本文提出了AI Data Scientist，一种由大型语言模型（LLM）驱动的自治代理系统，通过六个专用子代理自动完成数据清洗、假设生成与验证、特征工程、建模及业务决策建议，实现从原始数据到可操作洞察的全流程自动化。实验表明，该方法在多领域真实数据集上优于人工基线，结果更具可解释性，显著提升数据科学效率与决策质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/The_AI_Data_Scientist.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="188-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Codepdf"><a href="#188-A-S-E-A-Repository-Level-Benchmark-for-Evaluating-Security-in-AI-Generated-Codepdf" class="headerlink" title="188. A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Codepdf"></a>188. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18106v1">A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A.S.E__A_Repository-Level_Benchmark_for_Evaluating_Security_in_AI-Generated_Code.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent</span></p>
<p>该论文提出A.S.E基准，专为评估大语言模型在真实代码仓库中的安全性设计，通过收集带有CVE的开源项目，采用语义和结构变换确保数据多样性，结合专家制定的规则与Docker容器实现可复现的安全、质量和稳定性评估。实验证明，Claude-3.7-Sonnet整体表现最佳，开源模型与闭源模型安全差距缩小，且“快思考”策略优于“慢思考”在仓库级安全修复中。结论表明A.S.E基准能有效推动LLM安全代码生成技术进步和评估。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A.S.E_A_Repository-Level_Benchmark_for_Evaluating_Security_in_AI-Generated_Code.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="189-Detecting-and-Characterizing-Planning-in-Language-Modelspdf"><a href="#189-Detecting-and-Characterizing-Planning-in-Language-Modelspdf" class="headerlink" title="189. Detecting and Characterizing Planning in Language Modelspdf"></a>189. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18098v1">Detecting and Characterizing Planning in Language Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Detecting_and_Characterizing_Planning_in_Language_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Massachusetts Amherst</span></p>
<p>本文提出了一套基于因果和形式化标准（未来令牌编码Future-Token Encoding和前驱影响Precursor Influence）的半自动管道，用于检测和分析大语言模型（LLM）在生成任务中是否进行计划（planning）而非即兴生成。实验表明，Gemma-2-2B在不同任务中会在规划和即兴之间切换，且指令微调能提升模型选择正确计划的能力，但并非产生规划行为的根本来源。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Detecting_and_Characterizing_Planning_in_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="190-Teaching-LLMs-to-Think-Mathematically-A-Critical-Study-of-Decision-Making-via-Optimizationpdf"><a href="#190-Teaching-LLMs-to-Think-Mathematically-A-Critical-Study-of-Decision-Making-via-Optimizationpdf" class="headerlink" title="190. Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimizationpdf"></a>190. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18091v1">Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Teaching_LLMs_to_Think_Mathematically__A_Critical_Study_of_Decision-Making_via_Optimization.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Princess Sumaya University for Technology</span></p>
<p>本文系统回顾和元分析了当前大语言模型（LLM）在数学建模与优化决策中的能力，包括学习方法、数据集设计、评估指标及提示策略，并通过设计针对计算机网络领域的实验，比较了多种主流LLM在自动生成优化模型时的表现。结果表明LLM在解析自然语言和生成符号化优化公式方面取得了进展，但在准确性、可扩展性和可解释性方面仍存在显著不足，未来需通过结构化数据集、领域微调、多代理模块化、神经符号混合等方法提升其数学推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Teaching_LLMs_to_Think_Mathematically_A_Critical_Study_of_Decision-Making_via_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="191-Named-Entity-Recognition-of-Historical-Text-via-Large-Language-Modelpdf"><a href="#191-Named-Entity-Recognition-of-Historical-Text-via-Large-Language-Modelpdf" class="headerlink" title="191. Named Entity Recognition of Historical Text via Large Language Modelpdf"></a>191. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18090v1">Named Entity Recognition of Historical Text via Large Language Model</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Named_Entity_Recognition_of_Historical_Text_via_Large_Language_Model.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Bologna</span></p>
<p>本论文提出基于大语言模型（LLM）的零样本和少样本提示策略，针对多语言历史文本的命名实体识别（NER）任务，设计了包括随机、词汇重叠和嵌入相似度的示例检索方法，并在HIPE-2022多语言历史数据集上进行系统实验。实验结果显示，少样本提示（尤其是单示例）显著优于零样本，且简单的示例选择策略已能带来提升，但整体性能仍低于有监督SOTA模型，提示LLM提示在低资源历史语料上具备可行性和实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Named_Entity_Recognition_of_Historical_Text_via_Large_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="192-HyST-LLM-Powered-Hybrid-Retrieval-over-Semi-Structured-Tabular-Datapdf"><a href="#192-HyST-LLM-Powered-Hybrid-Retrieval-over-Semi-Structured-Tabular-Datapdf" class="headerlink" title="192. HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Datapdf"></a>192. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18048v1">HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/HyST__LLM-Powered_Hybrid_Retrieval_over_Semi-Structured_Tabular_Data.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">PrompTart LAB, MODULABS</span></p>
<p>该论文提出HyST框架，通过LLM解析自然语言查询，将结构化约束转换为元数据过滤，并利用语义嵌入搜索处理非结构化部分，实现对半结构化表格数据的高精度检索。在STaRK Amazon基准下，HyST在所有检索指标上均优于传统方法，证明结构化过滤与语义检索结合能有效提升推荐系统的准确性和可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/HyST_LLM-Powered_Hybrid_Retrieval_over_Semi-Structured_Tabular_Data.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="193-Language-Models-Coupled-with-Metacognition-Can-Outperform-Reasoning-Modelspdf"><a href="#193-Language-Models-Coupled-with-Metacognition-Can-Outperform-Reasoning-Modelspdf" class="headerlink" title="193. Language Models Coupled with Metacognition Can Outperform Reasoning Modelspdf"></a>193. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17959v1">Language Models Coupled with Metacognition Can Outperform Reasoning Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Language_Models_Coupled_with_Metacognition_Can_Outperform_Reasoning_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IBM Research</span></p>
<p>本文提出了一种通用的SOFAI-LM架构，将大语言模型（LLM）与大推理模型（LRM）通过无训练的元认知反馈模块耦合，实现了LLM的自我纠错和逐步改进，必要时再调用LRM。实验表明，在图着色和代码调试两个领域，SOFAI-LM不仅在准确率上可与独立的LRM媲美甚至超越，而且大幅降低推理时间，提升了整体解题效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Language_Models_Coupled_with_Metacognition_Can_Outperform_Reasoning_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="194-Understanding-Subword-Compositionality-of-Large-Language-Modelspdf"><a href="#194-Understanding-Subword-Compositionality-of-Large-Language-Modelspdf" class="headerlink" title="194. Understanding Subword Compositionality of Large Language Modelspdf"></a>194. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17953v1">Understanding Subword Compositionality of Large Language Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Understanding_Subword_Compositionality_of_Large_Language_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Copenhagen</span></p>
<p>本论文系统性探究了大型语言模型（LLM）中子词组合的机制，从结构相似性、语义可分解性和形式保持性三方面，基于六个主流LLM的多组实验，采用Procrustes分析与探针任务，揭示不同模型在子词组合策略上存在三类截然不同的结构动态。结论表明：大部分LLM的子词组合结构与简单加法高度同构，语义信息跨层次保存良好，但形式信息的保持模式因模型而异，且差异主要源自预训练阶段。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Understanding_Subword_Compositionality_of_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="195-Debiasing-Multilingual-LLMs-in-Cross-lingual-Latent-Spacepdf"><a href="#195-Debiasing-Multilingual-LLMs-in-Cross-lingual-Latent-Spacepdf" class="headerlink" title="195. Debiasing Multilingual LLMs in Cross-lingual Latent Spacepdf"></a>195. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17948v1">Debiasing Multilingual LLMs in Cross-lingual Latent Space</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Debiasing_Multilingual_LLMs_in_Cross-lingual_Latent_Space.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Copenhagen</span></p>
<p>本文提出在跨语种对齐的潜空间中对多语言大模型进行去偏处理，而不是直接在模型原始表示上操作。通过在TED演讲平行语料上训练自编码器构建跨语种潜空间，并在该空间应用SentDebias和INLP去偏技术，实验表明此方法显著提升了去偏性能和跨语种迁移能力，最高可减少65%的偏见分数。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Debiasing_Multilingual_LLMs_in_Cross-lingual_Latent_Space.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="196-AMELIA-A-FAMILY-OF-MULTI-TASK-END-TO-END-LANGUAGE-MODELS-FOR-ARGUMENTATIONpdf"><a href="#196-AMELIA-A-FAMILY-OF-MULTI-TASK-END-TO-END-LANGUAGE-MODELS-FOR-ARGUMENTATIONpdf" class="headerlink" title="196. AMELIA: A FAMILY OF MULTI-TASK END-TO-END LANGUAGE MODELS FOR ARGUMENTATIONpdf"></a>196. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17926v1">AMELIA: A FAMILY OF MULTI-TASK END-TO-END LANGUAGE MODELS FOR ARGUMENTATION</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AMELIA__A_Family_of_Multi-task_End-to-end_Language_Models_for_Argumentation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universite Claude Bernard Lyon 1</span></p>
<p>该论文提出AMELIA，一系列面向论证挖掘的多任务端到端大语言模型。方法上，作者首先将19个主流论证挖掘数据集标准化整合，覆盖八类论证任务，并基于Meta Llama-3.1-8B-Instruct模型，系统评估了单任务微调、多任务微调和模型合并（如DELLA方法）三种训练策略。实验结果表明，单任务微调可显著提升各任务表现，多任务微调可实现跨任务迁移且无性能损失，模型合并则在节约算力的同时兼顾多任务能力。结论：多任务微调在多项论证任务上创造了新的性能标杆，模型合并为实际部署提供了高效可扩展的方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/AMELIA_A_FAMILY_OF_MULTI-TASK_END-TO-END_LANGUAGE_MODELS_FOR_ARGUMENTATION.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="197-Riemannian-Optimization-for-LoRA-on-the-Stiefel-Manifoldpdf"><a href="#197-Riemannian-Optimization-for-LoRA-on-the-Stiefel-Manifoldpdf" class="headerlink" title="197. Riemannian Optimization for LoRA on the Stiefel Manifoldpdf"></a>197. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17901v1">Riemannian Optimization for LoRA on the Stiefel Manifold</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Riemannian_Optimization_for_LoRA_on_the_Stiefel_Manifold.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Opt-AI Inc.</span></p>
<p>该论文提出Stiefel-LoRA方法，将LoRA中B矩阵的优化从传统的Euclidean空间迁移到Stiefel流形，通过强制正交性约束实现参数高效与表达能力提升。实验结果表明，该几何优化方法在多种LLM规模和NLP任务上均显著优于AdamW，尤其提升了表现和收敛速度，揭示了几何约束对PEFT设计的重要性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Riemannian_Optimization_for_LoRA_on_the_Stiefel_Manifold.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="198-UniAPO-Unified-Multimodal-Automated-Prompt-Optimizationpdf"><a href="#198-UniAPO-Unified-Multimodal-Automated-Prompt-Optimizationpdf" class="headerlink" title="198. UniAPO: Unified Multimodal Automated Prompt Optimizationpdf"></a>198. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17890v1">UniAPO: Unified Multimodal Automated Prompt Optimization</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/UniAPO__Unified_Multimodal_Automated_Prompt_Optimization.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ByteDance</span></p>
<p>本文提出了UniAPO——首个面向多模态自动化提示优化的统一框架，采用EM算法思想将反馈建模和提示优化解耦，并设计了长短期记忆机制，有效缓解视觉token膨胀和缺乏过程级监督等多模态自动提示优化难题。实验表明，UniAPO在文本、图像和视频任务上均大幅优于现有方法，具备强泛化与高效率，推动多模态大模型的提示优化向更稳健、可扩展方向发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/UniAPO_Unified_Multimodal_Automated_Prompt_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="199-VISA-Group-wise-Visual-Token-Selection-and-Aggregation-via-Graph-Summarization-for-Efficient-MLLMs-Inferencepdf"><a href="#199-VISA-Group-wise-Visual-Token-Selection-and-Aggregation-via-Graph-Summarization-for-Efficient-MLLMs-Inferencepdf" class="headerlink" title="199. VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inferencepdf"></a>199. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17857v1">VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/VISA__Group-wise_Visual_Token_Selection_and_Aggregation_via_Graph_Summarization_for_Efficient_MLLMs_.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xiamen University</span></p>
<p>该论文提出了一种新方法VISA，用于高效多模态大语言模型（MLLMs）推理中的视觉token压缩。方法包括基于图的视觉token聚合（VTA）模块，通过语义相似性构建token图并将被移除token的信息汇聚到保留token，以及分组的token选择策略（GTS），利用文本token注意力引导分组内的视觉token保留与移除，实现逐步稳定信息压缩。实验表明，VISA在LLaVA-1.5、LLaVA-NeXT和Video-LLaVA等模型和多项基准测试上，在加速推理的同时保持或提升了模型性能，压缩率高时依然优于同类方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/VISA_Group-wise_Visual_Token_Selection_and_Aggregation_via_Graph_Summarization_for_Efficient_MLLMs_Inference.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="200-Group-Expectation-Policy-Optimization-for-Stable-Heterogeneous-Reinforcement-Learning-in-LLMspdf"><a href="#200-Group-Expectation-Policy-Optimization-for-Stable-Heterogeneous-Reinforcement-Learning-in-LLMspdf" class="headerlink" title="200. Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMspdf"></a>200. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17850v1">Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMs</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Group_Expectation_Policy_Optimization_for_Stable_Heterogeneous_Reinforcement_Learning_in_LLMs.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peng Cheng Laboratory</span></p>
<p>本文提出了HeteroRL框架与GEPO算法，专为异构分布式环境下大语言模型的数学推理训练设计。HeteroRL实现了采样和参数学习的异步解耦，GEPO通过引入样本级重要性权重及组期望平滑机制，有效减少高延迟下的梯度方差，提升训练稳定性和性能。实验证明，在极端网络延迟下GEPO性能下降不超过3%，显著优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Group_Expectation_Policy_Optimization_for_Stable_Heterogeneous_Reinforcement_Learning_in_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="201-FAIRGAMER-Evaluating-Biases-in-the-Application-of-Large-Language-Models-to-Video-Gamespdf"><a href="#201-FAIRGAMER-Evaluating-Biases-in-the-Application-of-Large-Language-Models-to-Video-Gamespdf" class="headerlink" title="201. FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Gamespdf"></a>201. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17825v1">FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Games</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FAIRGAMER__Evaluating_Biases_in_the_Application_of_Large_Language_Models_to_Video_Games.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Cyber Security, University of Chinese Academy of Sciences</span></p>
<p>该论文提出了FAIRGAMER基准，专用于评估大语言模型（LLM）在视频游戏场景中的社会与文化偏见，涵盖NPC决策、对抗玩家和场景生成三大任务，并设计了Dlstd指标来量化模型偏见对游戏平衡的影响。实验显示，当前主流LLM均存在明显偏见，部分对游戏公平性和玩家体验造成严重影响，尤其Grok-3偏见最显著，只有极少模型在个别任务上表现无明显偏见；结论指出LLM固有偏见会显著破坏游戏平衡，未来需多角度消除偏见以提升可靠性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/FAIRGAMER_Evaluating_Biases_in_the_Application_of_Large_Language_Models_to_Video_Games.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="202-SCALABLE-ENGINE-AND-THE-PERFORMANCE-OF-DIFFERENT-LLM-MODELS-IN-A-SLURM-BASED-HPC-ARCHITECTUREpdf"><a href="#202-SCALABLE-ENGINE-AND-THE-PERFORMANCE-OF-DIFFERENT-LLM-MODELS-IN-A-SLURM-BASED-HPC-ARCHITECTUREpdf" class="headerlink" title="202. SCALABLE ENGINE AND THE PERFORMANCE OF DIFFERENT LLM MODELS IN A SLURM BASED HPC ARCHITECTUREpdf"></a>202. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17814v1">SCALABLE ENGINE AND THE PERFORMANCE OF DIFFERENT LLM MODELS IN A SLURM BASED HPC ARCHITECTURE</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scalable_Engine_and_the_Performance_of_Different_LLM_Models_in_a_SLURM_based_HPC_architecture.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">AImotion Bavaria</span></p>
<p>该论文提出了一种基于SLURM的高性能计算（HPC）架构，用于在多节点集群中高效部署和推理多种大语言模型（LLM），核心方法包括动态资源调度、容器化微服务集成、REST API接口、异步批量推理与多步“裁判团”输出优化。实验表明，该架构在小模型下可支持高并发且低延迟推理，而大模型在高负载下易饱和，但整体架构具备良好的可扩展性、鲁棒性和灵活性，适合大规模LLM推理任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/SCALABLE_ENGINE_AND_THE_PERFORMANCE_OF_DIFFERENT_LLM_MODELS_IN_A_SLURM_BASED_HPC_ARCHITECTURE.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="203-Proximal-Supervised-Fine-Tuningpdf"><a href="#203-Proximal-Supervised-Fine-Tuningpdf" class="headerlink" title="203. Proximal Supervised Fine-Tuningpdf"></a>203. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17784v1">Proximal Supervised Fine-Tuning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Proximal_Supervised_Fine-Tuning.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了Proximal Supervised Fine-Tuning (PSFT)，受TRPO和PPO等强化学习算法启发，将信任域约束引入大模型监督微调过程，通过clipped surrogate objective方式限制策略漂移，提升模型泛化能力，避免熵塌缩。实验表明，PSFT在数学和人类价值对齐等多种任务中，在域内具备与传统SFT相当性能，域外泛化更优，并为后续RL优化打下更坚实基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Proximal_Supervised_Fine-Tuning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="204-ISACL-Internal-State-Analyzer-for-Copyrighted-Training-Data-Leakagepdf"><a href="#204-ISACL-Internal-State-Analyzer-for-Copyrighted-Training-Data-Leakagepdf" class="headerlink" title="204. ISACL: Internal State Analyzer for Copyrighted Training Data Leakagepdf"></a>204. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17767v1">ISACL: Internal State Analyzer for Copyrighted Training Data Leakage</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ISACL__Internal_State_Analyzer_for_Copyrighted_Training_Data_Leakage.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">City University of Hong Kong</span></p>
<p>本文提出了ISACL框架，通过分析大语言模型（LLM）在生成文本前的内部状态，预测其是否存在训练数据泄漏风险。该方法结合神经网络分类器与RAG（检索增强生成）系统，利用FAISS和SQLite高效检索和比对受版权保护的参考材料，实现实时、可扩展的风险评估。实验结果显示，ISACL在多种LLM（如Llama和Mistral）及不同模型规模下均能高效准确地检测泄漏风险，显著优于传统基于输出内容的检测方法，提升了合规性与检测效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ISACL_Internal_State_Analyzer_for_Copyrighted_Training_Data_Leakage.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="205-Randomly-Removing-50-of-Dimensions-in-Text-Embeddings-has-Minimal-Impact-on-Retrieval-and-Classification-Taskspdf"><a href="#205-Randomly-Removing-50-of-Dimensions-in-Text-Embeddings-has-Minimal-Impact-on-Retrieval-and-Classification-Taskspdf" class="headerlink" title="205. Randomly Removing 50% of Dimensions in Text Embeddings has Minimal Impact on Retrieval and Classification Taskspdf"></a>205. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17744v1">Randomly Removing 50% of Dimensions in Text Embeddings has Minimal Impact on Retrieval and Classification Tasks</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Randomly_Removing_50%_of_Dimensions_in_Text_Embeddings_has_Minimal_Impact_on_Retrieval_and_Classific.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Mannheim</span></p>
<p>本文系统性地分析了在6种主流文本编码器和26个下游任务中，随机移除文本嵌入的50%维度对检索和分类任务性能的影响。通过对嵌入空间的几何属性（如各向异性、维度冗余、异常维度）与下游表现的关联分析，发现许多嵌入维度对性能有负面影响，随机移除维度仅造成极小性能下降，且某些情况性能还提升。结论是，当前主流文本嵌入器对表示空间利用低效，存在大量“降级维度”，未来可通过优化模型结构或目标函数进一步提升嵌入质量和下游任务表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Randomly_Removing_50%25_of_Dimensions_in_Text_Embeddings_has_Minimal_Impact_on_Retrieval_and_Classification_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="206-Speculative-Safety-Aware-Decodingpdf"><a href="#206-Speculative-Safety-Aware-Decodingpdf" class="headerlink" title="206. Speculative Safety-Aware Decodingpdf"></a>206. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17739v1">Speculative Safety-Aware Decoding</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Speculative_Safety-Aware_Decoding.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science &amp; Technology, Beijing Institute of Technology</span></p>
<p>本文提出了一种轻量级解码时防御方法——Speculative Safety-Aware Decoding (SSD)，结合了小型具安全属性语言模型的推测采样与大模型动态切换解码策略，通过匹配率量化越狱风险并在实用性和安全性间自适应切换解码分布。实验表明，SSD可有效增强大语言模型的深度安全对抗能力，保持有用性同时提升推理效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Speculative_Safety-Aware_Decoding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="207-Database-Normalization-via-Dual-LLM-Self-Refinementpdf"><a href="#207-Database-Normalization-via-Dual-LLM-Self-Refinementpdf" class="headerlink" title="207. Database Normalization via Dual-LLM Self-Refinementpdf"></a>207. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17693v1">Database Normalization via Dual-LLM Self-Refinement</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Database_Normalization_via_Dual-LLM_Self-Refinement.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sungshin Women’s University</span></p>
<p>该论文提出了Miffie框架，利用双大语言模型（LLM）自我优化架构，实现数据库规范化自动化。其方法采用GPT-4进行规范化生成，o1-mini进行验证，通过任务特定的零样本提示和反馈迭代提升准确率与效率。实验表明，Miffie能够高效准确地规范化复杂数据库模式，显著减少人工操作。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Database_Normalization_via_Dual-LLM_Self-Refinement.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="208-LLM-based-Agentic-Reasoning-Frameworks-A-Survey-from-Methods-to-Scenariospdf"><a href="#208-LLM-based-Agentic-Reasoning-Frameworks-A-Survey-from-Methods-to-Scenariospdf" class="headerlink" title="208. LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenariospdf"></a>208. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17692v1">LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/LLM-based_Agentic_Reasoning_Frameworks__A_Survey_from_Methods_to_Scenarios.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Jiaotong University</span></p>
<p>本文系统梳理了基于大语言模型（LLM）的Agentic推理框架，提出了统一的三层分类体系：单体智能体方法、工具集成方法与多智能体方法，并以形式化语言描述各类推理流程。通过对科学发现、医疗、软件工程、社会经济仿真等领域的应用梳理，揭示了不同Agentic框架的核心机制、适用场景及评估实践。结论认为该体系有助于理清LLM-Agent的发展脉络，促进标准化和安全演进。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/LLM-based_Agentic_Reasoning_Frameworks_A_Survey_from_Methods_to_Scenarios.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="209-Unlearning-as-Ablation-Toward-a-Falsifiable-Benchmark-for-Generative-Scientific-Discoverypdf"><a href="#209-Unlearning-as-Ablation-Toward-a-Falsifiable-Benchmark-for-Generative-Scientific-Discoverypdf" class="headerlink" title="209. Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discoverypdf"></a>209. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17681v1">Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discovery</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Unlearning_as_Ablation__Toward_a_Falsifiable_Benchmark_for_Generative_Scientific_Discovery.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent Researcher</span></p>
<p>本文提出了一种“unlearning-as-ablation”方法，通过系统性移除大语言模型关于某个科学结果及其所有相关推理链（forget-closure）的知识，再测试模型是否能仅凭剩余公理和工具重新推导该结果。方法明确将unlearning用作检验模型是否具备真实生成性能力而非仅仅记忆，并以数学和算法领域为试点。结论认为该方法可为AI科学发现能力提供可证伪的实验标准，并有助于厘清AI在科学发现中的真实边界。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Unlearning_as_Ablation_Toward_a_Falsifiable_Benchmark_for_Generative_Scientific_Discovery.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="210-Towards-Synthesizing-Normative-Data-for-Cognitive-Assessments-Using-Generative-Multimodal-Large-Language-Modelspdf"><a href="#210-Towards-Synthesizing-Normative-Data-for-Cognitive-Assessments-Using-Generative-Multimodal-Large-Language-Modelspdf" class="headerlink" title="210. Towards Synthesizing Normative Data for Cognitive Assessments Using Generative Multimodal Large Language Modelspdf"></a>210. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17675v1">Towards Synthesizing Normative Data for Cognitive Assessments Using Generative Multimodal Large Language Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Towards_Synthesizing_Normative_Data_for_Cognitive_Assessments_Using_Generative_Multimodal_Large_Lang.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Westminster Schools</span></p>
<p>该论文提出利用生成式多模态大语言模型（如GPT-4o及其mini版本），通过综合和基础提示词策略生成认知评估中的标准化文本反应，实现对真实患者语料的模拟。研究发现，综合性提示词和强模型能生成更真实且可区分认知状态（正常、轻度认知障碍、阿尔茨海默症）的语料，BERTScore和LLM-as-a-Judge评价方法更有效。结论认为此方法可为认知测试快速生成标准化基线数据，降低传统数据收集成本并有助于新测试开发。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Towards_Synthesizing_Normative_Data_for_Cognitive_Assessments_Using_Generative_Multimodal_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="211-Dynamic-Embedding-of-Hierarchical-Visual-Features-for-Efficient-Vision-Language-Fine-Tuningpdf"><a href="#211-Dynamic-Embedding-of-Hierarchical-Visual-Features-for-Efficient-Vision-Language-Fine-Tuningpdf" class="headerlink" title="211. Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuningpdf"></a>211. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17638v1">Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dynamic_Embedding_of_Hierarchical_Visual_Features_for_Efficient_Vision-Language_Fine-Tuning.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National University of Defense Technology</span></p>
<p>本文提出了DEHVF方法，通过动态嵌入和融合视觉编码器的多层次特征，将其直接注入大语言模型（LLM）各层的前馈网络（FFN），实现视觉-语言信息在同语义粒度上的高效对齐。实验表明，DEHVF在ScienceQA和COCO Caption等主流视觉-语言基准上，比现有高效微调方法有更高准确率和更优训练推理速度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Dynamic_Embedding_of_Hierarchical_Visual_Features_for_Efficient_Vision-Language_Fine-Tuning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="212-Steering-When-Necessary-Flexible-Steering-Large-Language-Models-with-Backtrackingpdf"><a href="#212-Steering-When-Necessary-Flexible-Steering-Large-Language-Models-with-Backtrackingpdf" class="headerlink" title="212. Steering When Necessary: Flexible Steering Large Language Models with Backtrackingpdf"></a>212. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17621v1">Steering When Necessary: Flexible Steering Large Language Models with Backtracking</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Steering_When_Necessary__Flexible_Steering_Large_Language_Models_with_Backtracking.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University</span></p>
<p>本文提出了Flexible Activation Steering with Backtracking (FASB) 框架，通过跟踪大型语言模型（LLM）生成过程中的内部状态，动态判断是否需要干预及干预强度，结合回溯机制对已偏离目标行为的token进行再生成，实现更精准的模型行为引导。实验证明FASB在TruthfulQA及六个多项选择数据集上优于现有方法，显著提升了模型的真实性和多项选择准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Steering_When_Necessary_Flexible_Steering_Large_Language_Models_with_Backtracking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="213-ChartMaster-Advancing-Chart-to-Code-Generation-with-Real-World-Charts-and-Chart-Similarity-Reinforcement-Learningpdf"><a href="#213-ChartMaster-Advancing-Chart-to-Code-Generation-with-Real-World-Charts-and-Chart-Similarity-Reinforcement-Learningpdf" class="headerlink" title="213. ChartMaster: Advancing Chart-to-Code Generation with Real-World Charts and Chart Similarity Reinforcement Learningpdf"></a>213. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17608v1">ChartMaster: Advancing Chart-to-Code Generation with Real-World Charts and Chart Similarity Reinforcement Learning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ChartMaster__Advancing_Chart-to-Code_Generation_with_Real-World_Charts_and_Chart_Similarity_Reinforc.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China University of Technology</span></p>
<p>该论文提出ChartMaster框架，通过引入ReChartPrompt自动化管道从arXiv论文收集真实世界图表，构建高多样性ReChartPrompt-240K数据集，并设计ChartSimRL强化学习算法，结合视觉和属性相似性奖励提升图表与代码的对齐效果。实验表明，ChartMaster在多项chart-to-code生成基准上接近GPT-4o水平，显著优于现有开源7B模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ChartMaster_Advancing_Chart-to-Code_Generation_with_Real-World_Charts_and_Chart_Similarity_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="214-Exploring-Efficient-Learning-of-Small-BERT-Networks-with-LoRA-and-DoRApdf"><a href="#214-Exploring-Efficient-Learning-of-Small-BERT-Networks-with-LoRA-and-DoRApdf" class="headerlink" title="214. Exploring Efficient Learning of Small BERT Networks with LoRA and DoRApdf"></a>214. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17586v1">Exploring Efficient Learning of Small BERT Networks with LoRA and DoRA</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Exploring_Efficient_Learning_of_Small_BERT_Networks_with_LoRA_and_DoRA.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>本论文针对小型BERT模型（minBERT），提出并实现了LoRA和DoRA两种低秩适应方法，并结合自动混合精度（AMP）训练，系统基准测试其在多任务（情感分析、复述检测、语义相似性）场景下的效率和性能。实验结果表明，LoRA和DoRA在保持模型性能的同时显著降低了显存和计算消耗，且对于小模型，低秩（rank&#x3D;1）分解即可达到最佳性价比，AMP和LoRA结合效果最优。结论指出，低秩适应技术不仅适用于大模型，对小型语言模型同样有效，并为资源受限环境下的模型微调提供了可行途径。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Exploring_Efficient_Learning_of_Small_BERT_Networks_with_LoRA_and_DoRA.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="215-TradingGroup-A-Multi-Agent-Trading-System-with-Self-Reflection-and-Data-Synthesispdf"><a href="#215-TradingGroup-A-Multi-Agent-Trading-System-with-Self-Reflection-and-Data-Synthesispdf" class="headerlink" title="215. TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesispdf"></a>215. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17565v1">TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TradingGroup__A_Multi-Agent_Trading_System_with_Self-Reflection_and_Data-Synthesis.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of New South Wales</span></p>
<p>该论文提出TradingGroup，一个集成了新闻情感分析、财报解读、股票趋势预测、交易风格适配及决策的多智能体金融交易系统。核心创新包括多层次自反机制、动态风险管理模块，以及自动数据合成与标注流程，用于生成高质量后训练数据并微调LLM。实验结果显示，TradingGroup在五个真实股票数据集上的收益和风险表现均优于现有规则、机器学习、强化学习以及LLM交易策略，验证了系统及合成数据的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TradingGroup_A_Multi-Agent_Trading_System_with_Self-Reflection_and_Data-Synthesis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="216-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiencypdf"><a href="#216-InternVL3-5-Advancing-Open-Source-Multimodal-Models-in-Versatility-Reasoning-and-Efficiencypdf" class="headerlink" title="216. InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiencypdf"></a>216. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18265v1">InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/InternVL3.5__Advancing_Open-Source_Multimodal_Models_in_Versatility,_Reasoning,_and_Efficiency.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai AI Laboratory</span></p>
<p>本文提出了InternVL3.5家族，主打多模态大模型在通用能力、推理能力和推理效率上的提升。方法上，创新性地引入Cascade Reinforcement Learning（级联强化学习）两阶段流程（离线RL高效收敛+在线RL精细对齐），并提出Visual Resolution Router (ViR)用于动态视觉分辨率选择，以及Decoupled Vision-Language Deployment (DvD)实现异构设备分布式推理，加速大模型多模态推理。结论显示，InternVL3.5在多模态理解、推理、文本、Agent等任务上取得领先开源水平，推理能力相比前代提升16%、推理速度提升4倍，显著缩小与商业模型如GPT-5的差距。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/InternVL3.5_Advancing_Open-Source_Multimodal_Models_in_Versatility_Reasoning_and_Efficiency.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="217-Test-Time-Scaling-Strategies-for-Generative-Retrieval-in-Multimodal-Conversational-Recommendationspdf"><a href="#217-Test-Time-Scaling-Strategies-for-Generative-Retrieval-in-Multimodal-Conversational-Recommendationspdf" class="headerlink" title="217. Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendationspdf"></a>217. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18132v1">Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Test-Time_Scaling_Strategies_for_Generative_Retrieval_in_Multimodal_Conversational_Recommendations.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Academia Sinica</span></p>
<p>本文提出了一种集成多模态大语言模型（MLLM）生成式检索器与推理时动态再排序（Test-Time Reranking, TTR）机制的新框架，用于多轮对话中的多模态商品推荐检索。方法流程包括用户意图推断、多模态生成式检索（基于语义ID）、以及TTR机制对候选商品进行推理时再排序，以更好地契合用户动态变化的检索意图。实验结果表明，TTR机制显著提升了多轮多模态检索的准确率，在三个基准数据集上平均提升MRR 14.5分、nDCG@1 10.6分，验证了该方法的有效性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Test-Time_Scaling_Strategies_for_Generative_Retrieval_in_Multimodal_Conversational_Recommendations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="218-ILRE-Intermediate-Layer-Retrieval-for-Context-Compression-in-Causal-Language-Modelspdf"><a href="#218-ILRE-Intermediate-Layer-Retrieval-for-Context-Compression-in-Causal-Language-Modelspdf" class="headerlink" title="218. ILRE: Intermediate Layer Retrieval for Context Compression in Causal Language Modelspdf"></a>218. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17892v1">ILRE: Intermediate Layer Retrieval for Context Compression in Causal Language Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ILRe__Intermediate_Layer_Retrieval_for_Context_Compression_in_Causal_Language_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">China Merchants Bank</span></p>
<p>本文提出了一种用于大语言模型长上下文推理的无训练上下文压缩方法ILRe，通过指定离线中间解码层，仅对该层进行流式分块预填充和查询-上下文注意力检索，并采用多池化核分配策略保持语义完整性。实验结果表明ILRe在降低计算复杂度和内存占用的同时，能以接近甚至超越全上下文的准确率完成长文本推理任务，实现百倍推理加速。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ILRE_Intermediate_Layer_Retrieval_for_Context_Compression_in_Causal_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="219-AVAM-Universal-Training-free-Adaptive-Visual-Anchoring-Embedded-into-Multimodal-Large-Language-Model-for-Multi-image-Question-Answeringpdf"><a href="#219-AVAM-Universal-Training-free-Adaptive-Visual-Anchoring-Embedded-into-Multimodal-Large-Language-Model-for-Multi-image-Question-Answeringpdf" class="headerlink" title="219. AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answeringpdf"></a>219. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17860v1">AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/AVAM__Universal_Training-free_Adaptive_Visual_Anchoring_Embedded_into_Multimodal_Large_Language_Mode.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hunan University</span></p>
<p>本文提出了一种训练无关的自适应视觉锚定（Adaptive Visual Anchoring）策略，通过基于跨模态响应图自适应压缩视觉Token，提取与问题相关的关键视觉区域，有效过滤多图像场景下的冗余信息。并引入协同解码机制，动态融合全局与压缩视觉信息，在多模态大语言模型（MLLM）多图像问答任务中显著提升准确率和效率，经主流MLLMs和多基准测试验证方法的通用性和有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/AVAM_Universal_Training-free_Adaptive_Visual_Anchoring_Embedded_into_Multimodal_Large_Language_Model_for_Multi-image_Question_Answering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="220-F2RVLM-Boosting-Fine-grained-Fragment-Retrieval-for-Multi-Modal-Long-form-Dialogue-with-Vision-Language-Modelpdf"><a href="#220-F2RVLM-Boosting-Fine-grained-Fragment-Retrieval-for-Multi-Modal-Long-form-Dialogue-with-Vision-Language-Modelpdf" class="headerlink" title="220. F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Modelpdf"></a>220. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17714v1">F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/F2RVLM__Boosting_Fine-grained_Fragment_Retrieval_for_Multi-Modal_Long-form_Dialogue_with_Vision_Lang.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent Inc</span></p>
<p>本文提出了F2RVLM模型，针对长篇多模态对话中的细粒度片段检索任务（FFR），通过两阶段训练：首先利用监督微调注入片段级检索知识，随后采用基于GRPO的强化学习，结合多目标奖励（语义精度、相关性和上下文一致性），并引入难度感知课程采样逐步提升模型推理能力。实验结果表明，F2RVLM在大规模MLDR合成数据和真实微信对话数据集上均大幅优于主流视觉语言模型，显著提升了多模态对话检索的准确性和片段一致性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/F2RVLM_Boosting_Fine-grained_Fragment_Retrieval_for_Multi-Modal_Long-form_Dialogue_with_Vision_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="221-Weights-Rotated-Preference-Optimization-for-Large-Language-Modelspdf"><a href="#221-Weights-Rotated-Preference-Optimization-for-Large-Language-Modelspdf" class="headerlink" title="221. Weights-Rotated Preference Optimization for Large Language Modelspdf"></a>221. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17637v1">Weights-Rotated Preference Optimization for Large Language Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Weights-Rotated_Preference_Optimization_for_Large_Language_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Information Engineering, Chinese Academy of Sciences</span></p>
<p>本文针对直接偏好优化（DPO）在大语言模型（LLM）对齐中的“奖励投机”问题，提出了权重旋转偏好优化（RoPO）算法。RoPO方法结合了输出层KL散度的隐式约束与中间隐藏层的多粒度正交矩阵显式约束，防止神经元塌陷带来的表达冗余，从而保留预训练和微调阶段习得的知识与表达能力。实验表明，RoPO在AlpacaEval 2和MT-Bench等基准上显著提升了对齐性能，减少了重复和知识遗忘，且只需0.015%的可训练参数，有效缓解了DPO的奖励投机问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Weights-Rotated_Preference_Optimization_for_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="222-Stop-Spinning-Wheels-Mitigating-LLM-Overthinking-via-Mining-Patterns-for-Early-Reasoning-Exitpdf"><a href="#222-Stop-Spinning-Wheels-Mitigating-LLM-Overthinking-via-Mining-Patterns-for-Early-Reasoning-Exitpdf" class="headerlink" title="222. Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exitpdf"></a>222. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17627v1">Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Stop_Spinning_Wheels__Mitigating_LLM_Overthinking_via_Mining_Patterns_for_Early_Reasoning_Exit.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Computing Technology, Chinese Academy of Sciences</span></p>
<p>本文提出通过分析大语言模型（LLM）推理过程中的思维长度与内容长度关系，将推理划分为三个阶段，并首次明确提出“推理完成点”（RCP）的概念。通过CatBoost特征分析，作者挖掘出识别RCP的稳定信号，并提出一套高效的启发式规则（RCPD），能够低开销地准确检测RCP，实现推理早停。实验结果表明，该方法可在保持甚至提升推理准确率的同时，平均减少30%以上的Token消耗，有效缓解了LLM的过度思考问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Stop_Spinning_Wheels_Mitigating_LLM_Overthinking_via_Mining_Patterns_for_Early_Reasoning_Exit.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="223-Enhancing-Knowledge-Tracing-through-Leakage-Free-and-Recency-Aware-Embeddingspdf"><a href="#223-Enhancing-Knowledge-Tracing-through-Leakage-Free-and-Recency-Aware-Embeddingspdf" class="headerlink" title="223. Enhancing Knowledge Tracing through Leakage-Free and Recency-Aware Embeddingspdf"></a>223. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17092v1">Enhancing Knowledge Tracing through Leakage-Free and Recency-Aware Embeddings</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Enhancing_Knowledge_Tracing_through_Leakage-Free_and_Recency-Aware_Embeddings.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Applied Sciences Karlsruhe</span></p>
<p>本文提出了一种简单高效的知识追踪（KT）输入嵌入改进方法，采用MASK标签机制防止多知识点（KC）问题中的标签泄漏，并首次引入基于可学习傅里叶特征的Recency Encoding用于编码知识点的最近出现距离。这些方法可直接集成到主流KT模型（如DKT、AKT、SAKT）中，不增加显著计算复杂度，并在多个基准数据集上显著提升了预测准确率。结论是：MASK标签和Recency Encoding能有效提升KT模型性能，特别是在多KC问题数据集上，且优于现有防泄漏方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Enhancing_Knowledge_Tracing_through_Leakage-Free_and_Recency-Aware_Embeddings.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="224-A-Decoupled-LOB-Representation-Framework-for-Multilevel-Manipulation-Detection-with-Supervised-Contrastive-Learningpdf"><a href="#224-A-Decoupled-LOB-Representation-Framework-for-Multilevel-Manipulation-Detection-with-Supervised-Contrastive-Learningpdf" class="headerlink" title="224. A Decoupled LOB Representation Framework for Multilevel Manipulation Detection with Supervised Contrastive Learningpdf"></a>224. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17086v1">A Decoupled LOB Representation Framework for Multilevel Manipulation Detection with Supervised Contrastive Learning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Decoupled_LOB_Representation_Framework_for_Multilevel_Manipulation_Detection_with_Supervised_Contr.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southern University of Science and Technology</span></p>
<p>本文提出一种新颖的分级限价订单簿（LOB）表示学习框架，结合级联LOB表示流水线和有监督对比学习，用于多层次金融市场操纵（如欺骗交易）检测。方法包括专用LOB嵌入模块、手工特征融合、混合损失（MSE和有监督对比损失）及异常样本过采样，显著提升了多模型下的检测效果，Transformer类模型表现最佳。结论显示该框架可有效挖掘复杂多层次异常，推动序列数据异常检测方法发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Decoupled_LOB_Representation_Framework_for_Multilevel_Manipulation_Detection_with_Supervised_Contrastive_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="225-CP4SBI-Local-Conformal-Calibration-of-Credible-Sets-in-Simulation-Based-Inferencepdf"><a href="#225-CP4SBI-Local-Conformal-Calibration-of-Credible-Sets-in-Simulation-Based-Inferencepdf" class="headerlink" title="225. CP4SBI: Local Conformal Calibration of Credible Sets in Simulation-Based Inferencepdf"></a>225. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17077v1">CP4SBI: Local Conformal Calibration of Credible Sets in Simulation-Based Inference</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/CP4SBI__Local_Conformal_Calibration_of_Credible_Sets_in_Simulation-Based_Inference.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Federal University of São Carlos</span></p>
<p>本文提出CP4SBI，一种可应用于任意模拟器推断（SBI）后验估计器的模型无关型共形校准框架，利用回归树分区（LoCart）和条件分布函数（CDF）两种策略，为各种不确定性量化区域（包括HPD、对称区间等）提供有限样本的局部与条件覆盖率保证。实验表明，CP4SBI显著提升了神经网络后验估计及扩散模型的不确定性量化质量，保证了更可靠的置信区间。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/CP4SBI_Local_Conformal_Calibration_of_Credible_Sets_in_Simulation-Based_Inference.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="226-TabResFlow-A-Normalizing-Spline-Flow-Model-for-Probabilistic-Univariate-Tabular-Regressionpdf"><a href="#226-TabResFlow-A-Normalizing-Spline-Flow-Model-for-Probabilistic-Univariate-Tabular-Regressionpdf" class="headerlink" title="226. TabResFlow: A Normalizing Spline Flow Model for Probabilistic Univariate Tabular Regressionpdf"></a>226. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17056v1">TabResFlow: A Normalizing Spline Flow Model for Probabilistic Univariate Tabular Regression</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/TabResFlow__A_Normalizing_Spline_Flow_Model_for_Probabilistic_Univariate_Tabular_Regression.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Hildesheim</span></p>
<p>本文提出TabResFlow，一种用于概率型单变量表格回归的正则化样条流模型，结合了数值特征MLP嵌入、ResNet主干特征提取和条件神经样条流（RQ-NSF）实现对复杂目标分布的高效建模。实验结果显示，TabResFlow在九个公开基准数据集上概率建模性能优于现有方法，在实际二手车定价任务中引入AURC指标并取得最优表现。结论：TabResFlow能更准确高效地建模表格数据中的不确定性，适合大规模工业应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/TabResFlow_A_Normalizing_Spline_Flow_Model_for_Probabilistic_Univariate_Tabular_Regression.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="227-Score-Matching-on-Large-Geometric-Graphs-for-Cosmology-Generationpdf"><a href="#227-Score-Matching-on-Large-Geometric-Graphs-for-Cosmology-Generationpdf" class="headerlink" title="227. Score Matching on Large Geometric Graphs for Cosmology Generationpdf"></a>227. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16990v1">Score Matching on Large Geometric Graphs for Cosmology Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Score_Matching_on_Large_Geometric_Graphs_for_Cosmology_Generation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Eindhoven University of Technology</span></p>
<p>本文提出了一种基于score-matching的生成模型，结合E(3)等变图神经网络，用于在大规模几何图（如宇宙星系团点云）上生成符合物理约束（如周期性边界、均匀先验和对称性）的结构。方法创新性地引入了拓扑感知噪声调度机制，显著提升了大规模点云生成的物理一致性、计算效率和扩展性，且在宇宙学结构模拟上优于现有扩散模型。结论表明该方法能高效生成高保真度宇宙学结构，是N体模拟的可行替代方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Score_Matching_on_Large_Geometric_Graphs_for_Cosmology_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="228-Sig-DEG-for-Distillation-Making-Diffusion-Models-Faster-and-Lighterpdf"><a href="#228-Sig-DEG-for-Distillation-Making-Diffusion-Models-Faster-and-Lighterpdf" class="headerlink" title="228. Sig-DEG for Distillation: Making Diffusion Models Faster and Lighterpdf"></a>228. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16939v1">Sig-DEG for Distillation: Making Diffusion Models Faster and Lighter</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Sig-DEG_for_Distillation__Making_Diffusion_Models_Faster_and_Lighter.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University College London</span></p>
<p>本文提出Sig-DEG（Signature-based Differential Equation Generator），一种基于部分路径签名和高阶随机泰勒展开的扩散模型蒸馏方法。Sig-DEG通过监督学习在粗时间网格上近似逆扩散过程，利用递归神经网络和部分签名向量实现高效推断，并在无需细粒度布朗路径重建的情况下，极大加速了生成速度。实验表明，Sig-DEG能在图像（如MNIST）和时间序列等多模态任务中，以5-10步生成保有高保真度的样本，实现50-100倍推理加速且几乎无性能损失，展示了基于路径签名的SDE近似在高效生成建模中的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Sig-DEG_for_Distillation_Making_Diffusion_Models_Faster_and_Lighter.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="229-Bridging-Graph-and-State-Space-Modeling-for-Intensive-Care-Unit-Length-of-Stay-Predictionpdf"><a href="#229-Bridging-Graph-and-State-Space-Modeling-for-Intensive-Care-Unit-Length-of-Stay-Predictionpdf" class="headerlink" title="229. Bridging Graph and State-Space Modeling for Intensive Care Unit Length of Stay Predictionpdf"></a>229. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17554v1">Bridging Graph and State-Space Modeling for Intensive Care Unit Length of Stay Prediction</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Bridging_Graph_and_State-Space_Modeling_for_Intensive_Care_Unit_Length_of_Stay_Prediction.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Cambridge</span></p>
<p>本文提出了S2G-Net，一种结合状态空间模型（Mamba）与多视图图神经网络（优化版GraphGPS）的双通路神经架构，用于重症监护病房(ICU)住院时长预测。方法创新在于多模态临床数据的时间序列与异构患者相似性图的统一建模，并引入高效的图结构融合和解释性分析。实验结果显示，S2G-Net在MIMIC-IV大规模数据集上，在各项指标上显著优于传统序列模型、图模型及混合模型，具有良好的预测准确性、资源效率和模型稳健性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Bridging_Graph_and_State-Space_Modeling_for_Intensive_Care_Unit_Length_of_Stay_Prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="230-GateTS-Versatile-and-Efficient-Forecasting-via-Attention-Inspired-routed-Mixture-of-Expertspdf"><a href="#230-GateTS-Versatile-and-Efficient-Forecasting-via-Attention-Inspired-routed-Mixture-of-Expertspdf" class="headerlink" title="230. GateTS: Versatile and Efficient Forecasting via Attention-Inspired routed Mixture-of-Expertspdf"></a>230. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17515v1">GateTS: Versatile and Efficient Forecasting via Attention-Inspired routed Mixture-of-Experts</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GateTS__Versatile_and_Efficient_Forecasting_via_Attention-Inspired_routed_Mixture-of-Experts.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Lviv Polytechnic National University</span></p>
<p>本文提出了一种新颖的时间序列预测模型GateTS，将稀疏Mixture-of-Experts (MoE)架构与受注意力机制启发的分路门控机制结合，替代传统softmax路由器，无需辅助负载均衡损失，实现专家利用的自然均衡。实验证明，GateTS在多个数据集上以更少的参数超越或媲美主流Transformer（如PatchTST）和LSTM，兼具高精度和高计算效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GateTS_Versatile_and_Efficient_Forecasting_via_Attention-Inspired_routed_Mixture-of-Experts.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="231-GraphMMP-A-Graph-Neural-Network-Model-with-Mutual-Information-and-Global-Fusion-for-Multimodal-Medical-Prognosispdf"><a href="#231-GraphMMP-A-Graph-Neural-Network-Model-with-Mutual-Information-and-Global-Fusion-for-Multimodal-Medical-Prognosispdf" class="headerlink" title="231. GraphMMP: A Graph Neural Network Model with Mutual Information and Global Fusion for Multimodal Medical Prognosispdf"></a>231. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17478v1">GraphMMP: A Graph Neural Network Model with Mutual Information and Global Fusion for Multimodal Medical Prognosis</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/GraphMMP__A_Graph_Neural_Network_Model_with_Mutual_Information_and_Global_Fusion_for_Multimodal_Medi.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hangzhou Dianzi University</span></p>
<p>该论文提出了一种基于图神经网络（GNN）的两阶段多模态医学预后模型GraphMMP，包含互信息驱动的特征图构建机制和Mamba序列模型的全局融合模块，提升了模态间隐含信息捕获能力和全局感知能力。实验结果在肝脏预后和METABRIC乳腺癌数据集上表现出更高的分类准确率与鲁棒性，验证了GraphMMP在多模态医学预后任务中的有效性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/GraphMMP_A_Graph_Neural_Network_Model_with_Mutual_Information_and_Global_Fusion_for_Multimodal_Medical_Prognosis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="232-Effective-Clustering-for-Large-Multi-Relational-Graphspdf"><a href="#232-Effective-Clustering-for-Large-Multi-Relational-Graphspdf" class="headerlink" title="232. Effective Clustering for Large Multi-Relational Graphspdf"></a>232. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17388v1">Effective Clustering for Large Multi-Relational Graphs</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Effective_Clustering_for_Large_Multi-Relational_Graphs.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hong Kong Baptist University</span></p>
<p>本论文提出了两种多关系图聚类算法DEMM和DEMM+，利用基于Dirichlet能量的两阶段优化目标，将节点属性和多类型关系结构有效融合。实验表明，DEMM+在多种真实大规模多关系图数据集上，聚类质量优于20种现有方法，且效率大幅提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Effective_Clustering_for_Large_Multi-Relational_Graphs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="233-ShaLa-Multimodal-Shared-Latent-Space-Modellingpdf"><a href="#233-ShaLa-Multimodal-Shared-Latent-Space-Modellingpdf" class="headerlink" title="233. ShaLa: Multimodal Shared Latent Space Modellingpdf"></a>233. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17376v1">ShaLa: Multimodal Shared Latent Space Modelling</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ShaLa__Multimodal_Shared_Latent_Space_Modelling.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stevens Institute of Technology</span></p>
<p>该论文提出ShaLa框架，通过架构性推断模型将多模态输入编码为确定性特征并融合为全局表示，联合扩散模型作为共享潜变量的表达先验，有效解决多模态VAE中推断灵活性不足和prior-hole问题。实验表明ShaLa在多模态一致性、生成质量和多视角可扩展性方面优于现有方法，并能实现风格迁移和鲁棒推断，适用于复杂多模态数据。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ShaLa_Multimodal_Shared_Latent_Space_Modelling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="234-Provable-Generalization-in-Overparameterized-Neural-Netspdf"><a href="#234-Provable-Generalization-in-Overparameterized-Neural-Netspdf" class="headerlink" title="234. Provable Generalization in Overparameterized Neural Netspdf"></a>234. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17256v1">Provable Generalization in Overparameterized Neural Nets</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Provable_Generalization_in_Overparameterized_Neural_Nets.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent</span></p>
<p>本文提出将Transformer中注意力矩阵的有效秩（effective rank）作为模型容量的新度量，理论上推导出基于有效秩的泛化界，其与样本规模的依赖与大规模语言模型的经验缩放定律一致。结论显示，在参数远超样本的情况下，关注注意力矩阵的谱属性能获得非平凡的泛化保证，为解释现代深度网络的泛化能力提供了新视角。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Provable_Generalization_in_Overparameterized_Neural_Nets.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="235-Learning-Short-Term-and-Long-Term-Patterns-of-High-Order-Dynamics-in-Real-World-Networkspdf"><a href="#235-Learning-Short-Term-and-Long-Term-Patterns-of-High-Order-Dynamics-in-Real-World-Networkspdf" class="headerlink" title="235. Learning Short-Term and Long-Term Patterns of High-Order Dynamics in Real-World Networkspdf"></a>235. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17236v1">Learning Short-Term and Long-Term Patterns of High-Order Dynamics in Real-World Networks</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Learning_Short-Term_and_Long-Term_Patterns_of_High-Order_Dynamics_in_Real-World_Networks.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chung-Ang University</span></p>
<p>本文提出了一种针对动态高阶网络关系建模的新方法Lincoln，包括双交互超边编码（Bi-interactional hyperedge encoding）以捕捉快照内高阶关系的结构和时序影响、周期时序注入（Periodic time injection）和中间节点表示（intermediate node representation）以建模长周期重复模式。大量实验表明，Lincoln在动态超边预测任务上优于九种主流方法，能有效捕获高阶关系的短期和长期动态特性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Learning_Short-Term_and_Long-Term_Patterns_of_High-Order_Dynamics_in_Real-World_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="236-Scaling-Graph-Transformers-A-Comparative-Study-of-Sparse-and-Dense-Attentionpdf"><a href="#236-Scaling-Graph-Transformers-A-Comparative-Study-of-Sparse-and-Dense-Attentionpdf" class="headerlink" title="236. Scaling Graph Transformers: A Comparative Study of Sparse and Dense Attentionpdf"></a>236. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17175v1">Scaling Graph Transformers: A Comparative Study of Sparse and Dense Attention</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Scaling_Graph_Transformers__A_Comparative_Study_of_Sparse_and_Dense_Attention.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Independent, Munich, Germany</span></p>
<p>本文系统比较了图Transformer中稠密（Dense）和稀疏（Sparse）注意力机制，分析了各自的技术实现、代表模型（如Graphormer、SAN、GraphGPS、Exphormer）、计算复杂度及在不同规模图上的实验表现。结论指出：稠密注意力适用于小规模图以获取全局信息但难以扩展，稀疏注意力则适合大规模图并在效率与表现间取得良好平衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Scaling_Graph_Transformers_A_Comparative_Study_of_Sparse_and_Dense_Attention.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="237-ShortListing-Model-A-Streamlined-Simplex-Diffusion-for-Discrete-Variable-Generationpdf"><a href="#237-ShortListing-Model-A-Streamlined-Simplex-Diffusion-for-Discrete-Variable-Generationpdf" class="headerlink" title="237. ShortListing Model: A Streamlined Simplex Diffusion for Discrete Variable Generationpdf"></a>237. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17345v1">ShortListing Model: A Streamlined Simplex Diffusion for Discrete Variable Generation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/ShortListing_Model__A_Streamlined_SimplexDiffusion_for_Discrete_Variable_Generation.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了Shortlisting Model (SLM)，一种基于simplex空间的高效离散变量生成扩散模型，灵感来自逐步候选集删减。SLM通过在simplex质心之间进行渐进式生成，有效降低了生成复杂度，并通过灵活的无分类器引导机制提升无条件生成性能。实验表明SLM在语言建模、蛋白质设计及DNA序列设计等多项离散生成任务上表现优异，在蛋白质和DNA设计中达到或超过现有最佳方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/ShortListing_Model_A_Streamlined_Simplex_Diffusion_for_Discrete_Variable_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="238-Aligning-the-Evaluation-of-Probabilistic-Predictions-with-Downstream-Valuepdf"><a href="#238-Aligning-the-Evaluation-of-Probabilistic-Predictions-with-Downstream-Valuepdf" class="headerlink" title="238. Aligning the Evaluation of Probabilistic Predictions with Downstream Valuepdf"></a>238. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18251v1">Aligning the Evaluation of Probabilistic Predictions with Downstream Value</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Aligning_the_Evaluation_of_Probabilistic_Predictions_with_Downstream_Value.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Computer Science, University of Tartu</span></p>
<p>该论文提出了评价概率预测与其下游任务价值之间对齐的问题，并提出了一种基于神经网络加权评分规则的数据驱动方法，通过学习加权函数自动对齐预测评价与下游任务的表现，无需专家指定权重函数。实验表明，在合成和真实回归任务中，该方法能有效缩小评价与实际下游效用的差距，提升评价指标与实际决策表现的一致性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Aligning_the_Evaluation_of_Probabilistic_Predictions_with_Downstream_Value.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="239-PCR-CA-Parallel-Codebook-Representations-with-Contrastive-Alignment-for-Multiple-Category-App-Recommendationpdf"><a href="#239-PCR-CA-Parallel-Codebook-Representations-with-Contrastive-Alignment-for-Multiple-Category-App-Recommendationpdf" class="headerlink" title="239. PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendationpdf"></a>239. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18166v1">PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/PCR-CA__Parallel_Codebook_Representations_with_Contrastive_Alignment_for_Multiple-Category_App_Recom.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Microsoft Store</span></p>
<p>该论文提出PCR-CA框架，通过多模态文本嵌入和并行码本VQ-AE模块，对应用内容进行离散化语义编码，同时使用对比对齐损失在用户和物品层面桥接语义和协同特征，并通过双路注意力机制融合ID特征与语义特征，有效提升多分类和长尾应用的CTR预测表现。实验结果显示，PCR-CA在大型数据集上AUC提升0.76%，长尾应用提升2.15%，线上A&#x2F;B实验CTR提升10.52%，CVR提升16.30%，已在微软商店全面部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/PCR-CA_Parallel_Codebook_Representations_with_Contrastive_Alignment_for_Multiple-Category_App_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="240-Frozen-in-Time-Parameter-Efficient-Time-Series-Transformers-via-Reservoir-Induced-Feature-Expansion-and-Fixed-Random-Dynamicspdf"><a href="#240-Frozen-in-Time-Parameter-Efficient-Time-Series-Transformers-via-Reservoir-Induced-Feature-Expansion-and-Fixed-Random-Dynamicspdf" class="headerlink" title="240. Frozen in Time: Parameter-Efficient Time Series Transformers via Reservoir-Induced Feature Expansion and Fixed Random Dynamicspdf"></a>240. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18130v1">Frozen in Time: Parameter-Efficient Time Series Transformers via Reservoir-Induced Feature Expansion and Fixed Random Dynamics</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Frozen_in_Time__Parameter-Efficient_Time_Series_Transformers_via_Reservoir-Induced_Feature_Expansion.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IIT Roorkee</span></p>
<p>该论文提出FreezeTST模型，通过在Transformer时间序列预测架构中交替插入冻结的随机特征块（reservoir blocks）与可训练层，实现非线性记忆扩展且不增加优化负担。理论分析证明该结构具备梯度稳定性和可控长时记忆，实验结果表明FreezeTST在七个长序列预测基准上以更少参数和训练时间，依然达到或超过主流方法表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Frozen_in_Time_Parameter-Efficient_Time_Series_Transformers_via_Reservoir-Induced_Feature_Expansion_and_Fixed_Random_Dynamics.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="241-Dynamic-Fusion-Multimodal-Network-for-SpeechWellness-Detectionpdf"><a href="#241-Dynamic-Fusion-Multimodal-Network-for-SpeechWellness-Detectionpdf" class="headerlink" title="241. Dynamic Fusion Multimodal Network for SpeechWellness Detectionpdf"></a>241. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18057v1">Dynamic Fusion Multimodal Network for SpeechWellness Detection</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Dynamic_Fusion_Multimodal_Network_for_SpeechWellness_Detection.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northwestern Polytechnical University</span></p>
<p>本文提出了一种轻量级多分支多模态网络，通过动态融合机制集成了时域、时频域声学特征和文本语义特征，用于青少年自杀风险语音检测。通过引入可学习加权的动态融合块和简化版Wav2vec 2.0及BERT，大幅减少模型参数，提高了融合效果和计算效率。实验结果显示该方法参数量减少78%，准确率提升5%，优于官方基线，证实了多模态融合和动态加权策略在语音心理健康检测中的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Dynamic_Fusion_Multimodal_Network_for_SpeechWellness_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="242-Training-Transformers-for-Mesh-Based-Simulationspdf"><a href="#242-Training-Transformers-for-Mesh-Based-Simulationspdf" class="headerlink" title="242. Training Transformers for Mesh-Based Simulationspdf"></a>242. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18051v1">Training Transformers for Mesh-Based Simulations</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Training_Transformers_for_Mesh-Based_Simulations.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mines Paris - PSL University</span></p>
<p>本文提出了一种基于Transformer的新型图神经网络架构，用于大规模网格物理模拟，利用邻接矩阵作为注意力掩码，并通过扩展邻接矩阵（膨胀滑动窗口、全局注意力、随机连接）提升远程依赖捕获能力。大量实验及消融分析表明，该方法在多物理仿真数据集上较现有主流方法具备更高的准确性和可扩展性，训练速度更快，模型更小，尤其在复杂流体动力学任务中性能提升显著。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Training_Transformers_for_Mesh-Based_Simulations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="243-Enhancing-Differentially-Private-Linear-Regression-via-Public-Second-Momentpdf"><a href="#243-Enhancing-Differentially-Private-Linear-Regression-via-Public-Second-Momentpdf" class="headerlink" title="243. Enhancing Differentially Private Linear Regression via Public Second-Momentpdf"></a>243. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18037v1">Enhancing Differentially Private Linear Regression via Public Second-Moment</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Enhancing_Differentially_Private_Linear_Regression_via_Public_Second-Moment.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Northwest University</span></p>
<p>该论文提出利用公众数据的二阶矩阵（second-moment matrix）对私有数据进行线性变换，以增强差分隐私线性回归的效用和稳定性（DP-OLSE），并推导理论误差界限。结论表明，该方法能显著提高回归精度与鲁棒性，尤其在数据分布不良或高维场景下效果突出，实验结果也在合成和真实数据上得到验证。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Enhancing_Differentially_Private_Linear_Regression_via_Public_Second-Moment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="244-A-Novel-Framework-for-Uncertainty-Quantification-via-Proper-Scores-for-Classification-and-Beyondpdf"><a href="#244-A-Novel-Framework-for-Uncertainty-Quantification-via-Proper-Scores-for-Classification-and-Beyondpdf" class="headerlink" title="244. A Novel Framework for Uncertainty Quantification via Proper Scores for Classification and Beyondpdf"></a>244. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18001v1">A Novel Framework for Uncertainty Quantification via Proper Scores for Classification and Beyond</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/A_Novel_Framework_for_Uncertainty_Quantification_via_Proper_Scores_for_Classification_and_Beyond.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Johann Wolfgang Goethe-Universität</span></p>
<p>该论文提出了一个基于Proper Scores的通用不确定性量化理论框架，统一了回归、分类及生成模型等多种任务的不确定性度量。方法上，作者通过泛化偏差-方差分解和函数Bregman散度，系统性地连接了本体（epistemic）不确定性、内禀（aleatoric）不确定性、模型校准与Proper Scores，提出了适用于图像、音频、自然语言生成等多领域的核分数（kernel score）及其相关估计器，并首次将校准-锐度分解推广到超越分类的场景。结论是，该框架理论与实践结合，提升了神经网络等现代模型在多任务中的不确定性量化与评估精度，相关核熵不确定性度量在大语言模型等应用中优于现有SOTA基线。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/A_Novel_Framework_for_Uncertainty_Quantification_via_Proper_Scores_for_Classification_and_Beyond.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="245-Spectrum-Prediction-in-the-Fractional-Fourier-Domain-with-Adaptive-Filteringpdf"><a href="#245-Spectrum-Prediction-in-the-Fractional-Fourier-Domain-with-Adaptive-Filteringpdf" class="headerlink" title="245. Spectrum Prediction in the Fractional Fourier Domain with Adaptive Filteringpdf"></a>245. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17872v1">Spectrum Prediction in the Fractional Fourier Domain with Adaptive Filtering</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Spectrum_Prediction_in_the_Fractional_Fourier_Domain_with_Adaptive_Filtering.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Aeronautics and Astronautics</span></p>
<p>该论文提出了一种名为SFFP的频谱预测框架，结合自适应分数阶傅里叶变换（FrFT）对频谱数据进行域变换，利用混合滤波策略在分数阶傅里叶域中自适应去噪，并通过复值神经网络预测趋势分量。实验表明，SFFP在真实频谱数据集上显著优于现有频谱和一般时序预测方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Spectrum_Prediction_in_the_Fractional_Fourier_Domain_with_Adaptive_Filtering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="246-FasterVoiceGrad-Faster-One-step-Diffusion-Based-Voice-Conversion-with-Adversarial-Diffusion-Conversion-Distillationpdf"><a href="#246-FasterVoiceGrad-Faster-One-step-Diffusion-Based-Voice-Conversion-with-Adversarial-Diffusion-Conversion-Distillationpdf" class="headerlink" title="246. FasterVoiceGrad: Faster One-step Diffusion-Based Voice Conversion with Adversarial Diffusion Conversion Distillationpdf"></a>246. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17868v1">FasterVoiceGrad: Faster One-step Diffusion-Based Voice Conversion with Adversarial Diffusion Conversion Distillation</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/FasterVoiceGrad__Faster_One-step_Diffusion-Based_Voice_Conversion_with_Adversarial_Diffusion_Convers.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">NTT, Inc., Japan</span></p>
<p>该论文提出FasterVoiceGrad，一种通过对扩散模型和内容编码器进行同时蒸馏的单步扩散语音转换方法，核心引入了对抗性扩散转换蒸馏（ADCD），在转换过程中联合利用对抗训练和分数蒸馏。实验表明FasterVoiceGrad在保持语音转换性能的同时，GPU加速比FastVoiceGrad快6.6倍，CPU快1.8倍，显著提升推理效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/FasterVoiceGrad_Faster_One-step_Diffusion-Based_Voice_Conversion_with_Adversarial_Diffusion_Conversion_Distillation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="247-Ada-TransGNN-An-Air-Quality-Prediction-Model-Based-On-Adaptive-Graph-Convolutional-Networkspdf"><a href="#247-Ada-TransGNN-An-Air-Quality-Prediction-Model-Based-On-Adaptive-Graph-Convolutional-Networkspdf" class="headerlink" title="247. Ada-TransGNN: An Air Quality Prediction Model Based On Adaptive Graph Convolutional Networkspdf"></a>247. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17867v1">Ada-TransGNN: An Air Quality Prediction Model Based On Adaptive Graph Convolutional Networks</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Ada-TransGNN__An_Air_Quality_Prediction_Model_Based_On_Adaptive_Graph_Convolutional_Networks.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">East China University of Science and Technology</span></p>
<p>本文提出了一种基于Transformer的自适应图卷积神经网络（Ada-TransGNN）用于空气质量预测。方法通过包含多头注意力机制和图卷积网络的协同时空模块、宏观&#x2F;微观自适应图结构学习模块和辅助任务学习模块（引入Moran系数）来动态建模监测点之间的复杂时空依赖关系。实验结果表明，该方法在短期和长期空气质量预测任务上均优于现有主流模型，显著提升了预测准确性和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Ada-TransGNN_An_Air_Quality_Prediction_Model_Based_On_Adaptive_Graph_Convolutional_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="248-Copyright-Protection-for-3D-Molecular-Structures-with-Watermarkingpdf"><a href="#248-Copyright-Protection-for-3D-Molecular-Structures-with-Watermarkingpdf" class="headerlink" title="248. Copyright Protection for 3D Molecular Structures with Watermarkingpdf"></a>248. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17702v1">Copyright Protection for 3D Molecular Structures with Watermarking</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Copyright_Protection_for_3D_Molecular_Structures_with_Watermarking.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">City University of Hong Kong</span></p>
<p>本文提出了首个用于三维分子结构的鲁棒水印方法，通过基于深度学习的编码器将水印嵌入分子原子级别结构，结合不变特征提取与多维缩放（MDS）策略提升对仿射变换（旋转、平移、反射）的鲁棒性，并通过动态平衡训练策略优化水印嵌入与分子属性的权衡。实验表明，该方法不仅在QM9和GEOM-DRUG等基准数据集及GeoBFN、GeoLDM等生成模型下实现了高达95%以上水印提取准确率，而且对分子结构和下游对接任务的影响极小，有效保障了分子知识产权且不影响其科学应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Copyright_Protection_for_3D_Molecular_Structures_with_Watermarking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="249-Text-Meets-Topology-Rethinking-Out-of-distribution-Detection-in-Text-Rich-Networkspdf"><a href="#249-Text-Meets-Topology-Rethinking-Out-of-distribution-Detection-in-Text-Rich-Networkspdf" class="headerlink" title="249. Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networkspdf"></a>249. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17690v1">Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networks</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Text_Meets_Topology__Rethinking_Out-of-distribution_Detection_in_Text-Rich_Networks.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Queensland</span></p>
<p>本文提出TextTopoOOD评测框架系统性涵盖文本属性、结构、主题和领域多维度分布外（OOD）检测场景，并提出TNT-OOD方法，通过结构感知编码器、跨模态注意力融合及超网络生成节点特定投影参数，有效对齐节点的文本与拓扑信息。实验表明TNT-OOD在11个数据集四类OOD场景下显著优于现有方法，证明了方法在文本丰富网络分布外检测中的有效性和TextTopoOOD评测框架的挑战性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Text_Meets_Topology_Rethinking_Out-of-distribution_Detection_in_Text-Rich_Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="250-On-the-Edge-of-Memorization-in-Diffusion-Modelspdf"><a href="#250-On-the-Edge-of-Memorization-in-Diffusion-Modelspdf" class="headerlink" title="250. On the Edge of Memorization in Diffusion Modelspdf"></a>250. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17689v1">On the Edge of Memorization in Diffusion Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/On_the_Edge_of_Memorization_in_Diffusion_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">TTIC</span></p>
<p>本文提出了一个理论与实验相结合的“记忆-泛化实验室”，用于系统研究扩散模型的记忆与泛化行为。作者通过在高维高斯混合模型下，分析模型参数量、数据复杂度等因素对扩散模型是否会记忆训练样本还是能泛化生成新样本的影响，提出并验证了基于训练损失差异可以准确预测模型由泛化转向记忆的临界点。结论表明：在受控条件下，扩散模型的记忆-泛化相变点可被理论准确预测，为后续理论和实际扩散模型安全应用研究提供了可验证工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/On_the_Edge_of_Memorization_in_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="251-Characterizing-the-Behavior-of-Training-Mamba-based-State-Space-Models-on-GPUspdf"><a href="#251-Characterizing-the-Behavior-of-Training-Mamba-based-State-Space-Models-on-GPUspdf" class="headerlink" title="251. Characterizing the Behavior of Training Mamba-based State Space Models on GPUspdf"></a>251. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17679v1">Characterizing the Behavior of Training Mamba-based State Space Models on GPUs</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Characterizing_the_Behavior_of_Training_Mamba-based_State_Space_Models_on_GPUs.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Advanced Micro Devices (AMD)</span></p>
<p>本文系统分析了基于Mamba的状态空间模型（SSM）在GPU训练过程中的关键算子、算术强度、内存与缓存行为以及算子瓶颈，涵盖语言、视觉和图等多领域模型。结果显示SSM算子不仅占用大量执行时间且主要受限于内存带宽，需软硬件协同优化以提升模型可扩展性与性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Characterizing_the_Behavior_of_Training_Mamba-based_State_Space_Models_on_GPUs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="252-Longitudinal-Progression-Prediction-of-Alzheimer’s-Disease-with-Tabular-Foundation-Modelpdf"><a href="#252-Longitudinal-Progression-Prediction-of-Alzheimer’s-Disease-with-Tabular-Foundation-Modelpdf" class="headerlink" title="252. Longitudinal Progression Prediction of Alzheimer’s Disease with Tabular Foundation Modelpdf"></a>252. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17649v1">Longitudinal Progression Prediction of Alzheimer’s Disease with Tabular Foundation Model</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Longitudinal_Progression_Prediction_of_Alzheimer's_Disease_with_Tabular_Foundation_Model.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Emory University</span></p>
<p>本文提出了L2C-TabPFN方法，结合纵向到横截面（L2C）特征转换与预训练的Tabular Foundation Model（TabPFN），用于TADPOLE数据集上阿尔茨海默病多模态临床数据的进展预测。方法通过L2C将病人历史数据转为定长特征向量，TabPFN利用上下文学习进行诊断与生物标志物回归。实验表明，L2C-TabPFN在脑室体积预测任务上取得了最优结果，优于传统XGBoost模型Frog，而诊断与认知分数预测表现略逊一筹。结论：L2C-TabPFN能更好捕捉结构性脑影像变化，提升阿尔茨海默病进展预测精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Longitudinal_Progression_Prediction_of_Alzheimer%E2%80%99s_Disease_with_Tabular_Foundation_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="253-Quantum-Graph-Attention-Network-A-Novel-Quantum-Multi-Head-Attention-Mechanism-for-Graph-Learningpdf"><a href="#253-Quantum-Graph-Attention-Network-A-Novel-Quantum-Multi-Head-Attention-Mechanism-for-Graph-Learningpdf" class="headerlink" title="253. Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learningpdf"></a>253. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17630v1">Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Quantum_Graph_Attention_Network__A_Novel_Quantum_Multi-Head_Attention_Mechanism_for_Graph_Learning.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea Advanced Institute of Science and Technology (KAIST)</span></p>
<p>本文提出了Quantum Graph Attention Network (QGAT)，将变分量子电路融合到图神经网络的注意力机制中，采用幅度编码和强纠缠量子层，使用单一量子电路并行生成多头注意力，极大减少参数量并提升非线性表达能力。实验表明，QGAT在节点分类和链路预测等任务中对比GAT&#x2F;GATv2表现更优，且在特征和结构噪声下具有更强鲁棒性，易于集成到现有架构，展示了量子增强注意力机制在复杂图学习任务中的有效性和广泛适用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Quantum_Graph_Attention_Network_A_Novel_Quantum_Multi-Head_Attention_Mechanism_for_Graph_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="254-The-Statistical-Fairness-Accuracy-Frontierpdf"><a href="#254-The-Statistical-Fairness-Accuracy-Frontierpdf" class="headerlink" title="254. The Statistical Fairness-Accuracy Frontierpdf"></a>254. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17622v1">The Statistical Fairness-Accuracy Frontier</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/The_Statistical_Fairness-Accuracy_Frontier.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Rice University</span></p>
<p>本文研究有限样本条件下公平性与准确性（FA）前沿的统计特性，提出针对两组线性回归问题的最优（minimax）估计器，分别针对已知与未知协方差矩阵两种场景，定量分析有限样本对公平-准确性权衡曲线的影响，并推导采样分配与估计误差的最优策略。结论显示，有限数据导致不同群体风险的非对称影响，尤其在群体异质性大时更显著，强调实际设计中需考虑统计偏差对公平-准确性平衡的扭曲。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/The_Statistical_Fairness-Accuracy_Frontier.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="255-Consciousness-as-a-Functorpdf"><a href="#255-Consciousness-as-a-Functorpdf" class="headerlink" title="255. Consciousness as a Functorpdf"></a>255. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17561v1">Consciousness as a Functor</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/Consciousness_as_a_Functor.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Adobe Research</span></p>
<p>本文提出了一种创新性的意识理论，将意识建模为在范畴论意义下的函子（CF），通过范畴与函子、煤代数和拓扑斯理论，从无意识记忆到意识记忆的信息传递进行建模，提出了多模态通用Mitchell-Bénabou语言嵌入（MUMBLE）作为“思想的内部语言”，并用通用强化学习（URL）与网络经济模型描述信息在不同记忆模块间的流动。结论认为该理论为意识的结构化和可计算建模提供了通用的数学框架，且能统一描述意识与无意识间的交互。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/Consciousness_as_a_Functor.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="256-EEG-FM-Bench-A-Comprehensive-Benchmark-for-the-Systematic-Evaluation-of-EEG-Foundation-Modelspdf"><a href="#256-EEG-FM-Bench-A-Comprehensive-Benchmark-for-the-Systematic-Evaluation-of-EEG-Foundation-Modelspdf" class="headerlink" title="256. EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Modelspdf"></a>256. <a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17742v1">EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Models</a><a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-26/arxiv/EEG-FM-Bench__A_Comprehensive_Benchmark_for_the_Systematic_Evaluation_of_EEG_Foundation_Models.pdf">pdf</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tongji University</span></p>
<p>本论文提出EEG-FM-Bench，这是首个系统化、标准化评测EEG基础模型（EEG-FMs）的综合性基准，涵盖14个数据集与10种典型脑电范式，集成统一的数据处理、三种下游微调策略及定量&#x2F;定性分析流程。实验结果表明，现有EEG-FM在冻结骨干时泛化能力有限，结合多任务训练、精细化时空特征建模及脑科学先验可显著提升模型性能和泛化性。论文为EEG基础模型的公平比较与可复现研究提供了平台，推动领域进步。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-27/EEG-FM-Bench_A_Comprehensive_Benchmark_for_the_Systematic_Evaluation_of_EEG_Foundation_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/27报纸</div>
      <div>http://ywfhhh.github.io/2025/08/2025-08-27_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>具身ADT</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月27日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-26_article/" title="2025/08/26报纸">
                        <span class="hidden-mobile">2025/08/26报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
