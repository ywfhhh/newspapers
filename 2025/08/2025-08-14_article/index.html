

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="具身ADT">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;14报纸">
<meta property="og:url" content="http://ywfhhh.github.io/2025/08/2025-08-14_article/index.html">
<meta property="og:site_name" content="最新前沿论文分享">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Reducing_Cognitive_Load_in_Multi-Agent_Reinforcement_Learning_for_Mathematical_Problem_Solving_Decoupling_Reasoning_and_Code_Generation.jpg">
<meta property="article:published_time" content="2025-08-14T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-19T08:49:04.332Z">
<meta property="article:author" content="具身ADT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Reducing_Cognitive_Load_in_Multi-Agent_Reinforcement_Learning_for_Mathematical_Problem_Solving_Decoupling_Reasoning_and_Code_Generation.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/14报纸 - 最新前沿论文分享</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ywfhhh.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文库</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/14报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-14 00:00" pubdate>
          星期四, 八月 14日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          18k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/14报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/14报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-14<br>📄 <strong>发现论文数量</strong>：117  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Reducing-Cognitive-Load-in-Multi-Agent-Reinforcement-Learning-for-Mathematical-Problem-Solving-Decoupling-Reasoning-and-Code-Generation"><a href="#1-Reducing-Cognitive-Load-in-Multi-Agent-Reinforcement-Learning-for-Mathematical-Problem-Solving-Decoupling-Reasoning-and-Code-Generation" class="headerlink" title="1. Reducing Cognitive Load in Multi-Agent Reinforcement Learning for Mathematical Problem Solving: Decoupling Reasoning and Code Generation"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Reducing_Cognitive_Load_in_Multi-Agent_Reinforcement_Learning_for_Mathematical_Problem_Solving__Deco.pdf">Reducing Cognitive Load in Multi-Agent Reinforcement Learning for Mathematical Problem Solving: Decoupling Reasoning and Code Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Baidu Inc.</span></p>
<p>该论文提出了一种双智能体框架，将数学推理与代码生成任务分离：推理智能体负责逐步分解问题，代码智能体负责代码生成与执行。训练方法结合模仿学习与强化学习，分别优化推理和编程过程，实验结果表明分离角色可降低认知负载并提升数学推理准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Reducing_Cognitive_Load_in_Multi-Agent_Reinforcement_Learning_for_Mathematical_Problem_Solving_Decoupling_Reasoning_and_Code_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-Expert-Guided-Diffusion-Planner-for-Auto-bidding"><a href="#2-Expert-Guided-Diffusion-Planner-for-Auto-bidding" class="headerlink" title="2. Expert-Guided Diffusion Planner for Auto-bidding"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Expert-Guided_Diffusion_Planner_for_Auto-bidding.pdf">Expert-Guided Diffusion Planner for Auto-bidding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou Technology</span></p>
<p>本文提出了一种专家引导的条件扩散模型（EGDB）用于广告系统中的自动竞价。方法上，结合专家轨迹指导、变分自编码器（VAE）建模专家行为、跨注意力扩散模块（EGCD）以及跳步采样加速生成，实现了高效且个性化的竞价策略优化。结论表明，该方法在离线和真实广告场景下均显著提升转化率与收益（转化提升11.29%，收益提升12.35%），并有效降低实时推理资源消耗。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Expert-Guided_Diffusion_Planner_for_Auto-bidding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-P-CAFE-Personalized-Cost-Aware-Incremental-Feature-Selection-For-Electronic-Health-Records"><a href="#3-P-CAFE-Personalized-Cost-Aware-Incremental-Feature-Selection-For-Electronic-Health-Records" class="headerlink" title="3. P-CAFE: Personalized Cost-Aware Incremental Feature Selection For Electronic Health Records"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/P-CAFE__Personalized_Cost-Aware_Incremental_Feature_Selection_For_Electronic_Health_Records.pdf">P-CAFE: Personalized Cost-Aware Incremental Feature Selection For Electronic Health Records</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Bar-Ilan University</span></p>
<p>该论文提出了P-CAFE框架，通过将特征选择建模为马尔可夫决策过程（MDP），结合强化学习代理，实现对电子健康记录（EHR）中多模态、稀疏数据的个性化、成本敏感和在线特征增量选择。方法包括：预训练多模态特征嵌入、多阶段掩码策略、基于成本归一化的信息增益奖励、鲁棒优化和灵活的RL代理适配。实验证明P-CAFE能在相同或更低成本下提升预测性能，更好地适应个体化、资源受限的临床场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/P-CAFE_Personalized_Cost-Aware_Incremental_Feature_Selection_For_Electronic_Health_Records.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="4-OPENCUA-Open-Foundations-for-Computer-Use-Agents"><a href="#4-OPENCUA-Open-Foundations-for-Computer-Use-Agents" class="headerlink" title="4. OPENCUA: Open Foundations for Computer-Use Agents"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/OpenCUA__Open_Foundations_for_Computer-Use_Agents.pdf">OPENCUA: Open Foundations for Computer-Use Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Hong Kong</span></p>
<p>本文提出了OPENCUA，一个全面开源的计算机使用智能体（CUA）基础框架，包括跨操作系统的人机演示采集工具、AGENTNET大规模任务数据集、高效的数据处理与反思式链式推理增强机制，以及多模态视觉语言模型训练流程。实验结果显示，OPENCUA-32B在多个主流基准上远超当前开源模型，并优于OpenAI CUA（GPT-4o），具备优异的跨领域泛化能力和强数据扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/OPENCUA_Open_Foundations_for_Computer-Use_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="5-Spatial-Traces-Enhancing-VLA-Models-with-Spatial-Temporal-Understanding"><a href="#5-Spatial-Traces-Enhancing-VLA-Models-with-Spatial-Temporal-Understanding" class="headerlink" title="5. Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Spatial_Traces__Enhancing_VLA_Models_with_Spatial-Temporal_Understanding.pdf">Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Moscow Institute of Physics and Technology (MIPT)</span></p>
<p>本论文提出了一种名为Spatial Traces的新方法，通过将历史关键点轨迹叠加到深度图上，实现了视觉-语言-动作（VLA）模型对环境空间和时间信息的同时感知。实验表明，该方法在仅用极少训练数据的情况下，在机器人操作任务中较SpatialVLA提升4%、较TraceVLA提升19%的任务完成率，显著增强了模型的空间-时序推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Spatial_Traces_Enhancing_VLA_Models_with_Spatial-Temporal_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-MechaFormer-Sequence-Learning-for-Kinematic-Mechanism-Design-Automation"><a href="#6-MechaFormer-Sequence-Learning-for-Kinematic-Mechanism-Design-Automation" class="headerlink" title="6. MechaFormer: Sequence Learning for Kinematic Mechanism Design Automation"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MechaFormer__Sequence_Learning_for_Kinematic_Mechanism_Design_Automation.pdf">MechaFormer: Sequence Learning for Kinematic Mechanism Design Automation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Autodesk Research</span></p>
<p>该论文提出了MechaFormer，一种基于Transformer的模型，将机械机构设计问题转化为条件序列生成任务，通过输入目标曲线（B-spline控制点），自动生成机构拓扑和几何参数的DSL字符串。方法核心包括统一拓扑选择和参数优化、拓扑&#x2F;旋转多样性采样，以及与局部优化结合的混合策略。实验表明，MechaFormer在路径匹配精度和设计多样性上显著优于现有基线，并能为优化算法提供高质量初始值，提升收敛效率和质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MechaFormer_Sequence_Learning_for_Kinematic_Mechanism_Design_Automation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-Spatial-Temporal-Multi-Scale-Quantization-for-Flexible-Motion-Generation"><a href="#7-Spatial-Temporal-Multi-Scale-Quantization-for-Flexible-Motion-Generation" class="headerlink" title="7. Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Spatial-Temporal_Multi-Scale_Quantization_for_Flexible_Motion_Generation.pdf">Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>本论文提出了一种空间-时间多尺度量化方法（MSQ），通过在空间和时间层面上压缩人体动作序列为多尺度离散token，实现了更高效和灵活的人体动作建模。基于该表示，作者设计了生成式掩码建模模型，支持动作编辑、控制和条件生成，在MotionFix、HumanML3D、HUMANISE等多项基准测试中取得了优于现有方法的性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Spatial-Temporal_Multi-Scale_Quantization_for_Flexible_Motion_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-GeoVLA-Empowering-3D-Representations-in-Vision-Language-Action-Models"><a href="#8-GeoVLA-Empowering-3D-Representations-in-Vision-Language-Action-Models" class="headerlink" title="8. GeoVLA: Empowering 3D Representations in Vision-Language-Action Models"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/GeoVLA__Empowering_3D_Representations_in_Vision-Language-Action_Models.pdf">GeoVLA: Empowering 3D Representations in Vision-Language-Action Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tianjin University</span></p>
<p>GeoVLA提出了一种新颖的视觉-语言-动作(VLA)模型框架，通过引入点云编码器(Point Embedding Network, PEN)和空间感知动作专家(3D-enhanced Action Expert, 3DAE)，实现了2D视觉、语言与3D几何信息的多模态融合，并利用扩散模型生成精准动作序列。大量仿真与真实机器人实验表明，GeoVLA在LIBERO和ManiSkill2基准上取得了SOTA性能，特别在高度、尺度和视角变化等3D泛化场景下显著优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/GeoVLA_Empowering_3D_Representations_in_Vision-Language-Action_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-Large-Scale-Robotic-Material-Handling-Learning-Planning-and-Control"><a href="#9-Large-Scale-Robotic-Material-Handling-Learning-Planning-and-Control" class="headerlink" title="9. Large Scale Robotic Material Handling: Learning, Planning, and Control"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Large_Scale_Robotic_Material_Handling__Learning,_Planning,_and_Control.pdf">Large Scale Robotic Material Handling: Learning, Planning, and Control</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ETH Zurich</span></p>
<p>本文提出了一个用于大型液压物料搬运机器人完全自动化的框架，集成了环境感知、基于强化学习的攻击点规划、路径规划、运动控制等模块。方法采用强化学习实现高效抓取点选择和精准轨迹跟踪控制，通过实地实验验证在高通量堆料管理和高精度卡车装载任务中的精度、安全性和效率，结果表明系统性能达到或超过人类操作员水平。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Large_Scale_Robotic_Material_Handling_Learning%2C_Planning%2C_and_Control.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-Generation-of-Real-time-Robotic-Emotional-Expressions-Learning-from-Human-Demonstration-in-Mixed-Reality"><a href="#10-Generation-of-Real-time-Robotic-Emotional-Expressions-Learning-from-Human-Demonstration-in-Mixed-Reality" class="headerlink" title="10. Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Generation_of_Real-time_Robotic_Emotional_Expressions_Learning_from_Human_Demonstration_in_Mixed_Rea.pdf">Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Honda Research Institute Europe</span></p>
<p>该论文提出了一种通过混合现实（MR）平台采集专家演示数据，并利用flow-matching生成模型实现物理机器人实时多样化情感表达的方法。系统将人类面部表情、头部和手势映射到机器人，并在给定情绪标签和目标物体位置下生成连续驱动动作，实验结果表明该方法能有效生成自然且可识别的多种情感表达。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Generation_of_Real-time_Robotic_Emotional_Expressions_Learning_from_Human_Demonstration_in_Mixed_Reality.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-Unsupervised-Skill-Discovery-as-Exploration-for-Learning-Agile-Locomotion"><a href="#11-Unsupervised-Skill-Discovery-as-Exploration-for-Learning-Agile-Locomotion" class="headerlink" title="11. Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Unsupervised_Skill_Discovery_as_Exploration_for_Learning_Agile_Locomotion.pdf">Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Georgia Institute of Technology</span></p>
<p>本论文提出了一种名为SDAX的学习框架，将无监督技能发现与强化学习相结合，采用双层优化机制自适应调节探索与任务奖励的平衡，无需人工设计奖励或演示数据。实验显示，SDAX能够让四足机器人自主习得如爬行、攀爬、跳跃及墙面反弹等高敏捷运动技能，并成功在真实机器人硬件上部署验证了方法的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Unsupervised_Skill_Discovery_as_Exploration_for_Learning_Agile_Locomotion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-A-Pseudo-Global-Fusion-Paradigm-Based-Cross-View-Network-for-LiDAR-Based-Place-Recognition"><a href="#12-A-Pseudo-Global-Fusion-Paradigm-Based-Cross-View-Network-for-LiDAR-Based-Place-Recognition" class="headerlink" title="12. A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/A_Pseudo_Global_Fusion_Paradigm-Based_Cross-View_Network_for_LiDAR-Based_Place_Recognition.pdf">A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Electronics and Information Engineering, South China Normal University</span></p>
<p>本文提出了一种基于伪全局信息融合范式的跨视角网络，用于提升激光雷达（LiDAR）场所识别（Place Recognition）的准确性。方法上，通过伪全局语义信息引导多模态特征学习，实现早期的上下文信息共享，并引入了流形自适应与成对方差-局部性学习度量（MAPVLM），利用自适应SPD矩阵计算Mahalanobis距离，替代传统欧氏度量。实验结果表明，该方法在NCLT、KITTI和Ford Campus等公开数据集上表现优异，尤其在复杂环境下具有更强的泛化能力和实时性，对机器人和自动驾驶定位具有实际应用价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/A_Pseudo_Global_Fusion_Paradigm-Based_Cross-View_Network_for_LiDAR-Based_Place_Recognition.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors"><a href="#13-Towards-Affordance-Aware-Robotic-Dexterous-Grasping-with-Human-like-Priors" class="headerlink" title="13. Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Towards_Affordance-Aware_Robotic_Dexterous_Grasping_with_Human-like_Priors.pdf">Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wuhan University</span></p>
<p>该论文提出了AffordDex，利用两阶段训练框架结合人类手运动模仿与负性可供性分割（NAA）模块。第一阶段通过模仿学习预训练基线策略以获取自然人类运动先验，第二阶段采用残差模块结合NAA引导强化学习，实现具体物体的功能合理抓取，并通过教师-学生蒸馏提升视觉策略性能。实验结果表明，AffordDex在抓取成功率、姿态自然性及功能合理性方面均优于现有方法，显著提升了多样物体上的泛化抓取能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Towards_Affordance-Aware_Robotic_Dexterous_Grasping_with_Human-like_Priors.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-Visual-Prompting-for-Robotic-Manipulation-with-Annotation-Guided-Pick-and-Place-Using-ACT"><a href="#14-Visual-Prompting-for-Robotic-Manipulation-with-Annotation-Guided-Pick-and-Place-Using-ACT" class="headerlink" title="14. Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Visual_Prompting_for_Robotic_Manipulation_with_Annotation-Guided_Pick-and-Place_Using_ACT.pdf">Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Institute of AIST</span></p>
<p>该论文提出了一种基于注释引导视觉提示和Transformer动作分块（ACT）的感知-动作流水线，实现了机器人在便利店等复杂零售环境中的自适应抓取与放置。方法通过对物体和放置区标注边界框，结合模仿学习算法ACT从人类演示中预测动作序列，简化了场景理解并提升了任务执行的灵活性。实验表明，所提系统在不同复杂度任务下均提升了抓取精度和适应能力，尤其在充分数据下表现出较强的泛化和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Visual_Prompting_for_Robotic_Manipulation_with_Annotation-Guided_Pick-and-Place_Using_ACT.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-Boosting-Action-Information-via-a-Variational-Bottleneck-on-Unlabelled-Robot-Videos"><a href="#15-Boosting-Action-Information-via-a-Variational-Bottleneck-on-Unlabelled-Robot-Videos" class="headerlink" title="15. Boosting Action-Information via a Variational Bottleneck on Unlabelled Robot Videos"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Boosting_Action-Information_via_a_Variational_Bottleneck_on_Unlabelled_Robot_Videos.pdf">Boosting Action-Information via a Variational Bottleneck on Unlabelled Robot Videos</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Chinese Academy of Sciences</span></p>
<p>该论文提出了一种基于变分信息瓶颈（VIB）的无标签机器人视频学习框架，通过最大化潜在动作与真实动作之间的互信息，提升动作信息表达能力。方法包括VIB损失理论推导和端到端编码器-解码器实现，并在MetaWorld和Libero等机器人任务上实验证明能显著提升机器人策略性能和参数效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Boosting_Action-Information_via_a_Variational_Bottleneck_on_Unlabelled_Robot_Videos.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="16-OmniVTLA-Vision-Tactile-Language-Action-Model-with-Semantic-Aligned-Tactile-Sensing"><a href="#16-OmniVTLA-Vision-Tactile-Language-Action-Model-with-Semantic-Aligned-Tactile-Sensing" class="headerlink" title="16. OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/OmniVTLA__Vision-Tactile-Language-Action_Model_with_Semantic-Aligned_Tactile_Sensing.pdf">OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Paxini Tech.</span></p>
<p>该论文提出了OmniVTLA，一种结合视觉-触觉-语言的机器人操作模型，核心创新包括双路径触觉编码器架构（利用预训练视觉Transformer和语义对齐触觉ViT），以及新构建的ObjTac三模态数据集，通过跨模态对比学习实现触觉、视觉和语言的统一语义空间。实验结果表明，OmniVTLA在抓取和放置任务中显著优于现有VLA模型，提升了21.9%（夹爪）和6.2%（灵巧手）的成功率，且大幅优化了操作效率和动作平滑性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/OmniVTLA_Vision-Tactile-Language-Action_Model_with_Semantic-Aligned_Tactile_Sensing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-PADReg-Physics-Aware-Deformable-Registration-Guided-by-Contact-Force-for-Ultrasound-Sequences"><a href="#17-PADReg-Physics-Aware-Deformable-Registration-Guided-by-Contact-Force-for-Ultrasound-Sequences" class="headerlink" title="17. PADReg: Physics-Aware Deformable Registration Guided by Contact Force for Ultrasound Sequences"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/PADReg__Physics-Aware_Deformable_Registration_Guided_by_Contact_Force_for_Ultrasound_Sequences.pdf">PADReg: Physics-Aware Deformable Registration Guided by Contact Force for Ultrasound Sequences</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Automation, Chinese Academy of Sciences</span></p>
<p>本文提出了一种物理感知的多模态超声图像变形配准方法PADReg，利用机器人超声系统采集的实时接触力作为物理约束，通过深度网络联合图像与力信息生成像素级组织刚度图，再结合物理模型生成变形场，实现高准确性与物理一致性的超声配准。实验证明，PADReg在结构一致性和物理合理性方面优于现有方法，并具备实时推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/PADReg_Physics-Aware_Deformable_Registration_Guided_by_Contact_Force_for_Ultrasound_Sequences.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-Developing-a-Calibrated-Physics-Based-Digital-Twin-for-Construction-Vehicles"><a href="#18-Developing-a-Calibrated-Physics-Based-Digital-Twin-for-Construction-Vehicles" class="headerlink" title="18. Developing a Calibrated Physics-Based Digital Twin for Construction Vehicles"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Developing_a_Calibrated_Physics-Based_Digital_Twin_for_Construction_Vehicles.pdf">Developing a Calibrated Physics-Based Digital Twin for Construction Vehicles</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Davis</span></p>
<p>该论文提出了一种面向工程机械（轮式装载机）的高保真数字孪生开发方法，结合基于物理的多体动力学仿真（AGX Dynamics）与真实车辆的多传感器数据，通过离线校准使虚拟模型能准确反映铲斗与土壤交互中的受力情况。实验证明，经过校准后数字孪生在关键力学参数预测上的误差大幅下降，为自动化施工车辆的任务规划、性能优化和智能控制提供了可靠的虚拟环境。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Developing_a_Calibrated_Physics-Based_Digital_Twin_for_Construction_Vehicles.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-Communication-Efficient-Robotic-Mixed-Reality-with-Gaussian-Splatting-Cross-Layer-Optimization"><a href="#19-Communication-Efficient-Robotic-Mixed-Reality-with-Gaussian-Splatting-Cross-Layer-Optimization" class="headerlink" title="19. Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Communication_Efficient_Robotic_Mixed_Reality_with_Gaussian_Splatting_Cross-Layer_Optimization.pdf">Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</span></p>
<p>该论文提出了高效通信的机器人混合现实（RoboMR）系统，通过引入高斯点云渲染（Gaussian Splatting, GS）模型作为“记忆系统”，利用机器人姿态在仿真端生成逼真视图，从而大幅减少图像上传需求。进一步提出GS跨层优化（GSCLO）框架，联合优化内容上传决策与通信功率分配，通过加速惩罚优化（APO）算法有效降低计算复杂度，并在多机器人、低功耗及信道不确定场景下实现鲁棒优化。实验结果显示，该方法在多平台上显著提升了MR图像质量、能效和通信成本（可低至&lt;200bits&#x2F;frame），实现了高稳定性和鲁棒性，验证了GS模型与数据混合对于动态场景的优越性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Communication_Efficient_Robotic_Mixed_Reality_with_Gaussian_Splatting_Cross-Layer_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-Autonomous-Mobile-Plant-Watering-Robot-A-Kinematic-Approach"><a href="#20-Autonomous-Mobile-Plant-Watering-Robot-A-Kinematic-Approach" class="headerlink" title="20. Autonomous Mobile Plant Watering Robot: A Kinematic Approach"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Autonomous_Mobile_Plant_Watering_Robot___A_Kinematic_Approach.pdf">Autonomous Mobile Plant Watering Robot: A Kinematic Approach</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of North Dakota</span></p>
<p>本论文提出了一种新型自主移动植物浇水机器人，集成了6自由度机械臂与四驱底盘，结合YOLOv5实时目标检测与Pl@ntNet-300K数据集实现植物识别，并通过插入土壤湿度传感器精准控制浇水量。系统采用Jetson Nano和Arduino微控制器进行多传感器（深度摄像头、LIDAR）数据融合，实现无预设路径的自主避障与任务追踪。实验结果表明，尽管现有机械臂在扭矩和反馈控制方面尚有不足，机器人已能完成基础土壤测量和浇水任务，未来需加强机械臂性能和整体防水设计，提升商业化应用能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Autonomous_Mobile_Plant_Watering_Robot_A_Kinematic_Approach.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-DEEPFLEET-Multi-Agent-Foundation-Models-for-Mobile-Robots"><a href="#21-DEEPFLEET-Multi-Agent-Foundation-Models-for-Mobile-Robots" class="headerlink" title="21. DEEPFLEET: Multi-Agent Foundation Models for Mobile Robots"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DeepFleet__Multi-Agent_Foundation_Models_for_Mobile_Robots.pdf">DEEPFLEET: Multi-Agent Foundation Models for Mobile Robots</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Amazon Robotics</span></p>
<p>本文提出DEEPFLEET，一套专为大规模移动机器人车队协调和规划设计的基础模型，包括四种架构：机器人中心（RC）、机器人-地面（RF）、图像-地面（IF）和图-地面（GF），分别利用决策变换器、跨注意力、卷积和图神经网络，针对机器人轨迹预测任务进行训练和评估。实验结果表明，RC和GF模型在预测准确性和模型效率上优于其他架构，尤其能有效扩展到更大规模的数据和模型，显示其在仓储机器人群体智能管理中的潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DEEPFLEET_Multi-Agent_Foundation_Models_for_Mobile_Robots.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="22-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer"><a href="#22-Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer" class="headerlink" title="22. Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Training-Free_Text-Guided_Color_Editing_with_Multi-Modal_Diffusion_Transformer.pdf">Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology</span></p>
<p>该论文提出了ColorCtrl，一种无需训练的文本引导颜色编辑方法，利用多模态扩散Transformer（MM-DiT）的注意力机制，通过对注意力图和value token的针对性操作，实现了结构与颜色的解耦，从而可精确、局部地编辑图像或视频中的颜色属性（如物体本色、光源色和环境光），同时保持几何、材质和光与物质交互的物理一致性。实验表明，ColorCtrl在SD3和FLUX.1-dev等主流扩散模型上超越现有训练免疫方法和商用模型（如FLUX.1 Kontext Max和GPT-4o），在编辑质量和一致性方面达到SOTA，并能自然泛化到视频和指令式编辑扩散模型。结论：ColorCtrl实现了高保真、可控、可扩展的颜色编辑，推动了自动化视觉内容编辑的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Training-Free_Text-Guided_Color_Editing_with_Multi-Modal_Diffusion_Transformer.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-Addressing-Bias-in-VLMs-for-Glaucoma-Detection-Without-Protected-Attribute-Supervision"><a href="#23-Addressing-Bias-in-VLMs-for-Glaucoma-Detection-Without-Protected-Attribute-Supervision" class="headerlink" title="23. Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Addressing_Bias_in_VLMs_for_Glaucoma_Detection_Without_Protected_Attribute_Supervision.pdf">Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">West Virginia University</span></p>
<p>本论文提出了一种无需受保护属性标签的视觉-语言模型（VLM）去偏方法，用于自动化青光眼筛查。方法通过无监督聚类推断代理子群体，结合图像-文本对比损失和图像-图像对比损失的梯度相似性动态加权，重点提升表现较差子群体的权重。实验结果显示，在Harvard–FairVLMed数据集上，该方法可在不使用敏感标签的前提下降低Equalized-Odds Distance，提高Equalized Subgroup AUC，显著提升少数群体检测公平性并保持整体判别力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Addressing_Bias_in_VLMs_for_Glaucoma_Detection_Without_Protected_Attribute_Supervision.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-Scaling-Learned-Image-Compression-Models-up-to-1-Billion"><a href="#24-Scaling-Learned-Image-Compression-Models-up-to-1-Billion" class="headerlink" title="24. Scaling Learned Image Compression Models up to 1 Billion"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Scaling_Learned_Image_Compression_Models_up_to_1_Billion.pdf">Scaling Learned Image Compression Models up to 1 Billion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>该论文首次系统性研究了扩展学习型图像压缩模型规模（基于HPCM架构）对压缩性能的影响，并利用5个不同参数规模（从6850万到10亿）的模型实证拟合模型大小和压缩性能之间的幂律关系，即“scaling laws”。实验结果表明，随着模型参数数量增长，压缩效果持续提升，HPCM-1B模型在多个公开数据集上达到了当前最优的率失真表现；这为大规模学习型压缩模型奠定了理论与实践基础，并揭示了压缩与智能之间的潜在联系。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Scaling_Learned_Image_Compression_Models_up_to_1_Billion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-VLM-3D-End-to-End-Vision-Language-Models-for-Open-World-3D-Perception"><a href="#25-VLM-3D-End-to-End-Vision-Language-Models-for-Open-World-3D-Perception" class="headerlink" title="25. VLM-3D: End-to-End Vision-Language Models for Open-World 3D Perception"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/VLM-3D_End-to-End_Vision-Language_Models_for_Open-World_3D_Perception.pdf">VLM-3D: End-to-End Vision-Language Models for Open-World 3D Perception</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>该论文提出了VLM-3D，一种基于Qwen2-VL的端到端视觉语言3D开放世界感知框架，利用LoRA高效微调和创新的两阶段联合损失（语义损失+3D IoU损失），实现了图像和文本的多模态融合，并能直接在LiDAR坐标系下预测3D包围框。实验证明，该方法在nuScenes数据集上提升了12.8%的感知精度，显著增强了自动驾驶场景下的开放集3D目标检测的鲁棒性与实用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/VLM-3D_End-to-End_Vision-Language_Models_for_Open-World_3D_Perception.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-TaoCache-Structure-Maintained-Video-Generation-Acceleration"><a href="#26-TaoCache-Structure-Maintained-Video-Generation-Acceleration" class="headerlink" title="26. TaoCache: Structure-Maintained Video Generation Acceleration"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/TaoCache__Structure-Maintained_Video_Generation_Acceleration.pdf">TaoCache: Structure-Maintained Video Generation Acceleration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huawei Inc.</span></p>
<p>该论文提出了TaoCache，一种用于视频扩散模型的训练无关、即插即用的缓存加速策略。方法通过固定点视角和二阶噪声增量建模，在后期去噪阶段利用余弦相似度与范数比进行跳步推理，从而保持高分辨率结构的一致性。实验表明，TaoCache在Latte-1、OpenSora-Plan v110和Wan2.1等主流DiT模型上，相同加速下视频质量（LPIPS、SSIM、PSNR）显著优于现有缓存方法，同时能与其他加速手段如TeaCache和PAB无缝结合。结论是TaoCache可大幅提升视频生成效率，并保持结构和细节一致性，推动高质量视频扩散模型的实用化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/TaoCache_Structure-Maintained_Video_Generation_Acceleration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-UNISTFORMER-Unified-Spatio-Temporal-Lightweight-Transformer-for-Efficient-Skeleton-Based-Action-Recognition"><a href="#27-UNISTFORMER-Unified-Spatio-Temporal-Lightweight-Transformer-for-Efficient-Skeleton-Based-Action-Recognition" class="headerlink" title="27. UNISTFORMER: Unified Spatio-Temporal Lightweight Transformer for Efficient Skeleton-Based Action Recognition"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/UniSTFormer__Unified_Spatio-Temporal_Lightweight_Transformer_for_Efficient_Skeleton-Based_Action_Rec.pdf">UNISTFORMER: Unified Spatio-Temporal Lightweight Transformer for Efficient Skeleton-Based Action Recognition</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of North Carolina at Charlotte</span></p>
<p>该论文提出了UniSTFormer，一种统一的轻量级时空Transformer框架，通过单一注意力模块同时建模空间和时间信息，并引入多尺度池化融合模块提升对局部和全局动作模式的捕获能力。实验结果显示，该方法在多个骨骼动作识别数据集上参数量和计算量显著低于现有方法，同时保持了竞争性的识别准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/UNISTFORMER_Unified_Spatio-Temporal_Lightweight_Transformer_for_Efficient_Skeleton-Based_Action_Recognition.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-MADPromptS-Unlocking-Zero-Shot-Morphing-Attack-Detection-with-Multiple-Prompt-Aggregation"><a href="#28-MADPromptS-Unlocking-Zero-Shot-Morphing-Attack-Detection-with-Multiple-Prompt-Aggregation" class="headerlink" title="28. MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MADPromptS__Unlocking_Zero-Shot_Morphing_Attack_Detection_with_Multiple_Prompt_Aggregation.pdf">MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fraunhofer IGD</span></p>
<p>本文提出了一种利用多模态基础模型CLIP的零样本学习能力进行人脸变形攻击检测（MAD）的方法，通过设计和聚合多文本提示分别描述每个类别（攻击与真实），无需任何微调即可提升检测性能。实验结果显示，多提示聚合策略能有效提升模型在零样本环境下的识别准确率，甚至超过经过微调的模型，强调了合理提示工程对安全生物识别应用的价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MADPromptS_Unlocking_Zero-Shot_Morphing_Attack_Detection_with_Multiple_Prompt_Aggregation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-Lay2Story-Extending-Diffusion-Transformers-for-Layout-Togglable-Story-Generation"><a href="#29-Lay2Story-Extending-Diffusion-Transformers-for-Layout-Togglable-Story-Generation" class="headerlink" title="29. Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Lay2Story__Extending_Diffusion_Transformers_for_Layout-Togglable_Story_Generation.pdf">Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">JD.com, Inc., Beijing, China</span></p>
<p>本文提出了Lay2Story，一种基于Diffusion Transformers（DiTs）的故事生成模型，通过引入布局条件（如主体位置与描述）实现对主体外观、姿态等细粒度可控的故事图像序列生成。方法创新性地包含全局分支和主体分支，结合大规模高质量布局标注数据集Lay2Story-1M和评测基准Lay2Story-Bench，实验证明在一致性、语义相关性与美学质量方面均优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Lay2Story_Extending_Diffusion_Transformers_for_Layout-Togglable_Story_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-Accelerated-Volumetric-Compression-without-Hierarchies-A-Fourier-Feature-Based-Implicit-Neural-Representation-Approach"><a href="#30-Accelerated-Volumetric-Compression-without-Hierarchies-A-Fourier-Feature-Based-Implicit-Neural-Representation-Approach" class="headerlink" title="30. Accelerated Volumetric Compression without Hierarchies: A Fourier Feature Based Implicit Neural Representation Approach"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Accelerated_Volumetric_Compression_without_Hierarchies__A_Fourier_Feature_Based_Implicit_Neural_Repr.pdf">Accelerated Volumetric Compression without Hierarchies: A Fourier Feature Based Implicit Neural Representation Approach</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">VSB – Technical University of Ostrava</span></p>
<p>该论文提出了一种无需分层结构的体数据压缩方法，结合傅里叶特征编码与选择性体素采样，通过动态体素选择和形态膨胀，显著提升压缩效率和训练速度。实验表明，该方法在保持较高重建质量的同时，训练时间缩短63.7%，实现了14倍压缩率，为医学成像和科学可视化等应用提供了高效的体数据表达方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Accelerated_Volumetric_Compression_without_Hierarchies_A_Fourier_Feature_Based_Implicit_Neural_Representation_Approach.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-Masked-Clustering-Prediction-for-Unsupervised-Point-Cloud-Pre-training"><a href="#31-Masked-Clustering-Prediction-for-Unsupervised-Point-Cloud-Pre-training" class="headerlink" title="31. Masked Clustering Prediction for Unsupervised Point Cloud Pre-training"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Masked_Clustering_Prediction_for_Unsupervised_Point_Cloud_Pre-training.pdf">Masked Clustering Prediction for Unsupervised Point Cloud Pre-training</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Pisa</span></p>
<p>该论文提出了MaskClu，一种结合遮蔽点建模与聚类学习的ViT无监督预训练方法，用于3D点云理解。方法通过图卷积网络重建聚类分配和聚类中心，并引入全局对比学习提升实例级特征表达。实验在分割、检测、分类等多项3D任务上显示MaskClu实现了新的竞争性结果，显著提升了点云语义特征的密集性与泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Masked_Clustering_Prediction_for_Unsupervised_Point_Cloud_Pre-training.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-Geometry-Aware-Global-Feature-Aggregation-for-Real-Time-Indirect-Illumination"><a href="#32-Geometry-Aware-Global-Feature-Aggregation-for-Real-Time-Indirect-Illumination" class="headerlink" title="32. Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Geometry-Aware_Global_Feature_Aggregation_for_Real-Time_Indirect_Illumination.pdf">Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出了一种用于实时间接光照预测的神经网络方法，结合了空间几何特征引导的全局特征聚合模块（基于多头注意力机制）和单色处理设计，能够高效捕捉长距离间接照明。实验表明，该方法在复杂、动态光照场景下具有优异的泛化能力和真实感，推理速度快，优于现有学习型间接光照估计算法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Geometry-Aware_Global_Feature_Aggregation_for_Real-Time_Indirect_Illumination.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-TARA-Token-Aware-LoRA-for-Composable-Personalization-in-Diffusion-Models"><a href="#33-TARA-Token-Aware-LoRA-for-Composable-Personalization-in-Diffusion-Models" class="headerlink" title="33. TARA: Token-Aware LoRA for Composable Personalization in Diffusion Models"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/TARA__Token-Aware_LoRA_for_Composable_Personalization_in_Diffusion_Models.pdf">TARA: Token-Aware LoRA for Composable Personalization in Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</span></p>
<p>本文提出了Token-Aware LoRA (TARA)方法，通过引入Token Focus Masking（TFM）和Token Alignment Loss（TAL）来提升扩散模型在多概念个性化文本到图像生成任务中的表现。TFM利用稀有token掩码约束每个LoRA模块仅影响其对应的token，避免干扰；TAL引导每个稀有token的空间注意力与其概念区域对齐，从而减少特征泄漏，实现无需额外训练即可多概念组合生成。实验结果显示，TARA能够高效地进行多概念推理，有效保持每个概念的视觉一致性，优于现有主流方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/TARA_Token-Aware_LoRA_for_Composable_Personalization_in_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-DiffPose-Animal-A-Language-Conditioned-Diffusion-Framework-for-Animal-Pose-Estimation"><a href="#34-DiffPose-Animal-A-Language-Conditioned-Diffusion-Framework-for-Animal-Pose-Estimation" class="headerlink" title="34. DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DiffPose-Animal__A_Language-Conditioned_Diffusion_Framework_for_Animal_Pose_Estimation.pdf">DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Guanghua Cambridge International School</span></p>
<p>该论文提出了DiffPose-Animal框架，将动物姿态估计建模为一个条件扩散去噪过程，通过融合大语言模型（LLM）生成的全局-局部解剖语义先验和视觉特征，利用跨模态注意力机制指导关键点生成。实验结果显示，该方法在AP-10K、AnimalPose和Animal Kingdom等多个公开动物姿态数据集上取得了优于现有方法的性能，尤其在物种多样、遮挡、关键点缺失等挑战场景下表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DiffPose-Animal_A_Language-Conditioned_Diffusion_Framework_for_Animal_Pose_Estimation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-Designing-Memory-Augmented-AR-Agents-for-Spatiotemporal-Reasoning-in-Personalized-Task-Assistance"><a href="#35-Designing-Memory-Augmented-AR-Agents-for-Spatiotemporal-Reasoning-in-Personalized-Task-Assistance" class="headerlink" title="35. Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Designing_Memory-Augmented_AR_Agents_for_Spatiotemporal_Reasoning_in_Personalized_Task_Assistance.pdf">Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Yonsei University</span></p>
<p>本文提出了一种模块化的记忆增强型增强现实（AR）代理框架，集成了多模态感知、场景图记忆、时空推理和智能执行模块，实现对用户长期行为和偏好的捕捉与个性化辅助。实验路线图和典型应用场景（如个性化烹饪指导）展示了该框架能显著提升AR系统在复杂多步任务中的上下文感知和个性化指导能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Designing_Memory-Augmented_AR_Agents_for_Spatiotemporal_Reasoning_in_Personalized_Task_Assistance.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-Bridging-the-Gap-A-Framework-for-Real-World-Video-Deepfake-Detection-via-Social-Network-Compression-Emulation"><a href="#36-Bridging-the-Gap-A-Framework-for-Real-World-Video-Deepfake-Detection-via-Social-Network-Compression-Emulation" class="headerlink" title="36. Bridging the Gap: A Framework for Real-World Video Deepfake Detection via Social Network Compression Emulation"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Bridging_the_Gap__A_Framework_for_Real-World_Video_Deepfake_Detection_via_Social_Network_Compression.pdf">Bridging the Gap: A Framework for Real-World Video Deepfake Detection via Social Network Compression Emulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Trento</span></p>
<p>本文提出了一个社交网络视频共享仿真框架（SNVSE），通过分析少量上传到社交平台的视频，估算压缩与分辨率参数，从而本地化地模拟YouTube、Facebook等平台对视频的压缩处理过程。实验表明，利用仿真生成的数据微调深度伪造检测模型，其检测性能与使用真实社交媒体分享视频微调的效果相当，有效弥合了实验室环境与真实场景的差距。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Bridging_the_Gap_A_Framework_for_Real-World_Video_Deepfake_Detection_via_Social_Network_Compression_Emulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-Exploring-Palette-based-Color-Guidance-in-Diffusion-Models"><a href="#37-Exploring-Palette-based-Color-Guidance-in-Diffusion-Models" class="headerlink" title="37. Exploring Palette based Color Guidance in Diffusion Models"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Exploring_Palette_based_Color_Guidance_in_Diffusion_Models.pdf">Exploring Palette based Color Guidance in Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">CyberAgent Inc.</span></p>
<p>本论文提出在扩散模型中引入独立的调色板（palette）引导机制，以提升图像生成中色彩方案的可控性。方法上，作者设计了多模态调色板表示模型（仅调色板、文本-调色板、图像-调色板），通过交叉注意力机制将调色板嵌入与文本嵌入融合，为图像生成提供精确色彩指导；并构建了Palette-Text-Image数据集进行定量与用户研究。结果表明，调色板引导能大幅提升生成图像的色彩一致性和控制精度，优于仅文本引导及直接在文本中插入调色板描述的基线方法，且受到专业设计师的高度评价。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Exploring_Palette_based_Color_Guidance_in_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-ROD-RGB-Only-Fast-and-Efficient-Off-road-Freespace-Detection"><a href="#38-ROD-RGB-Only-Fast-and-Efficient-Off-road-Freespace-Detection" class="headerlink" title="38. ROD: RGB-Only Fast and Efficient Off-road Freespace Detection"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/ROD__RGB-Only_Fast_and_Efficient_Off-road_Freespace_Detection.pdf">ROD: RGB-Only Fast and Efficient Off-road Freespace Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Institute of Computing Technology, Chinese Academy of Sciences</span></p>
<p>本论文提出ROD方法，利用预训练的Vision Transformer (ViT)从RGB图像中提取特征，并设计高效轻量级解码器，实现离道路可通行区域的语义分割，无需LiDAR数据。结果显示ROD在ORFD和RELLIS-3D数据集上达到新的SOTA精度，同时推理速度达50FPS，远超多模态融合方法，满足实时应用需求。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/ROD_RGB-Only_Fast_and_Efficient_Off-road_Freespace_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-Efficient-Agent-Optimizing-Planning-Capability-for-Multimodal-Retrieval-Augmented-Generation"><a href="#39-Efficient-Agent-Optimizing-Planning-Capability-for-Multimodal-Retrieval-Augmented-Generation" class="headerlink" title="39. Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented Generation"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Efficient_Agent__Optimizing_Planning_Capability_for_Multimodal_Retrieval_Augmented_Generation.pdf">Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">OPPO Research Institute</span></p>
<p>本论文提出E-Agent框架，通过动态mRAG规划器和任务执行器实现多模态信息检索与生成的高效联动，采用单次规划策略动态选择搜索工具并优化执行流程。论文还构建了RemPlan基准，首次系统评测多模态检索规划能力，并在多项VQA任务上验证E-Agent在准确率和效率方面均优于现有方法，减少冗余搜索37%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Efficient_Agent_Optimizing_Planning_Capability_for_Multimodal_Retrieval_Augmented_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-MonoPartNeRF-Human-Reconstruction-from-Monocular-Video-via-Part-Based-Neural-Radiance-Fields"><a href="#40-MonoPartNeRF-Human-Reconstruction-from-Monocular-Video-via-Part-Based-Neural-Radiance-Fields" class="headerlink" title="40. MonoPartNeRF: Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MonoPartNeRF_Human_Reconstruction_from_Monocular_Video_via_Part-Based_Neural_Radiance_Fields.pdf">MonoPartNeRF: Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Guilin University of Electronic Technology</span></p>
<p>MonoPartNeRF提出了一种基于神经辐射场的单目动态人体高质量重建与渲染方法，通过人体结构分部、双向变形机制及UVT时空建模提升各部件几何一致性与自然关节连接。方法还融合了局部关节姿态嵌入与外观特征注意力机制，有效恢复遮挡区域细节，在ZJU-MoCap和MonoCap等数据集上显著优于现有方法，具备更强的结构一致性、视觉真实感和泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MonoPartNeRF_Human_Reconstruction_from_Monocular_Video_via_Part-Based_Neural_Radiance_Fields.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-MMIF-AMIN-Adaptive-Loss-Driven-Multi-Scale-Invertible-Dense-Network-for-Multimodal-Medical-Image-Fusion"><a href="#41-MMIF-AMIN-Adaptive-Loss-Driven-Multi-Scale-Invertible-Dense-Network-for-Multimodal-Medical-Image-Fusion" class="headerlink" title="41. MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for Multimodal Medical Image Fusion"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MMIF-AMIN__Adaptive_Loss-Driven_Multi-Scale_Invertible_Dense_Network_for_Multimodal_Medical_Image_Fu.pdf">MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for Multimodal Medical Image Fusion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southwest University</span></p>
<p>本文提出了一种新型的多模态医学图像融合方法MMIF-AMIN，采用可逆稠密网络（IDN）实现单一模态特征的无损提取，结合多尺度互补特征提取模块（MCFEM），集成CBAM注意力机制、不同尺寸卷积和Transformer以捕捉多模态间的互补信息。引入自适应损失函数，基于源图像统计信息自动调整参数，避免手动调参并提升模型解释性。实验表明，该方法在多个主流数据集上超越九种SOTA方法，融合效果优异，且具备良好泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MMIF-AMIN_Adaptive_Loss-Driven_Multi-Scale_Invertible_Dense_Network_for_Multimodal_Medical_Image_Fusion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-Unified-and-Semantically-Grounded-Domain-Adaptation-for-Medical-Image-Segmentation"><a href="#42-Unified-and-Semantically-Grounded-Domain-Adaptation-for-Medical-Image-Segmentation" class="headerlink" title="42. Unified and Semantically Grounded Domain Adaptation for Medical Image Segmentation"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Unified_and_Semantically_Grounded_Domain_Adaptation_for_Medical_Image_Segmentation.pdf">Unified and Semantically Grounded Domain Adaptation for Medical Image Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Washington</span></p>
<p>本文提出了一种统一且具备语义基础的无监督领域自适应（UDA）框架，用于医学图像分割，能够同时支持source-accessible与source-free两种设置。方法上，创新性地引入了可解释的、解耦的人体结构与个体几何特征的概率流形，将所有解剖学模板嵌入共享的低维空间，并通过可学习的基分布实现形状记忆和可控变换。结论表明，该方法在多器官、多模态公开数据集上取得了领域内最优性能，尤其在困难的source-free场景下表现出高度的泛化能力和稳健性，同时具备良好的可解释性和领域一致性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Unified_and_Semantically_Grounded_Domain_Adaptation_for_Medical_Image_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-Hierarchical-Visual-Prompt-Learning-for-Continual-Video-Instance-Segmentation"><a href="#43-Hierarchical-Visual-Prompt-Learning-for-Continual-Video-Instance-Segmentation" class="headerlink" title="43. Hierarchical Visual Prompt Learning for Continual Video Instance Segmentation"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Hierarchical_Visual_Prompt_Learning_for_Continual_Video_Instance_Segmentation.pdf">Hierarchical Visual Prompt Learning for Continual Video Instance Segmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Mohamed bin Zayed University of Artificial Intelligence</span></p>
<p>该论文提出了分层视觉提示学习（HVPL）模型以解决持续视频实例分割（CVIS）问题，方法包括任务特定的帧和视频提示、正交梯度校正模块及视频上下文解码器，分别从帧和视频层面缓解旧类别遗忘。实验结果显示，HVPL在多个公开数据集上显著优于现有基线方法，有效缓解了灾难性遗忘并提升了分割性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Hierarchical_Visual_Prompt_Learning_for_Continual_Video_Instance_Segmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-Yan-Foundational-Interactive-Video-Generation"><a href="#44-Yan-Foundational-Interactive-Video-Generation" class="headerlink" title="44. Yan: Foundational Interactive Video Generation"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Yan__Foundational_Interactive_Video_Generation.pdf">Yan: Foundational Interactive Video Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent</span></p>
<p>该论文提出了Yan，一个集成高保真实时仿真（Yan-Sim）、多模态可控生成（Yan-Gen）和多粒度交互编辑（Yan-Edit）的交互式视频生成基础框架。通过3D游戏环境自动采集大规模高质量交互视频数据，Yan采用高压缩3D-VAE、分层自回归文本描述、KV-cache窗口去噪、结构-风格解耦等关键技术，实现1080P&#x2F;60FPS实时仿真、文本&#x2F;图像驱动的无限交互生成，以及基于文本的结构与风格实时编辑。结论认为Yan推动了交互式视频生成从碎片功能向统一、可创作范式迈进，为下一代AI内容创作工具奠定基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Yan_Foundational_Interactive_Video_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-Unlocking-the-Potential-of-Diffusion-Priors-in-Blind-Face-Restoration"><a href="#45-Unlocking-the-Potential-of-Diffusion-Priors-in-Blind-Face-Restoration" class="headerlink" title="45. Unlocking the Potential of Diffusion Priors in Blind Face Restoration"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Unlocking_the_Potential_of_Diffusion_Priors_in_Blind_Face_Restoration.pdf">Unlocking the Potential of Diffusion Priors in Blind Face Restoration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Warwick</span></p>
<p>本文提出FLIPNET，一种基于大规模文本到图像（T2I）扩散模型并结合LoRA微调的统一网络，用于盲人脸部修复。该方法通过切换输入顺序在恢复模式和退化模式间切换，利用BoostHub模块有选择地注入低质量（LQ）特征，并引入BFR导向的面部嵌入以提升身份一致性和细节恢复。实验表明，FLIPNET在合成和真实世界数据集上都优于现有BFR方法，能够实现真实且忠实的人脸修复，同时更好地模拟复杂真实世界退化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Unlocking_the_Potential_of_Diffusion_Priors_in_Blind_Face_Restoration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-Calibration-Attention-Instance-wise-Temperature-Scaling-for-Vision-Transformers"><a href="#46-Calibration-Attention-Instance-wise-Temperature-Scaling-for-Vision-Transformers" class="headerlink" title="46. Calibration Attention: Instance-wise Temperature Scaling for Vision Transformers"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Calibration_Attention__Instance-wise_Temperature_Scaling_for_Vision_Transformers.pdf">Calibration Attention: Instance-wise Temperature Scaling for Vision Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Adelaide</span></p>
<p>本文提出了Calibration Attention (CalAttn)模块，为Vision Transformer (ViT, DeiT, Swin)引入了基于每个样本的动态温度标定机制。方法通过轻量级两层MLP从CLS token自适应地预测温度，并在训练和推理阶段联合优化交叉熵及Brier损失，实现端到端的概率标定。实验结果在CIFAR-10&#x2F;100、MNIST、TinyImageNet和ImageNet1K等多数据集上，CalAttn能让校准误差降低至原方法的1&#x2F;4，并且模型参数增加不到0.1%。结论是CalAttn能高效提升ViT类模型的预测可靠性和概率可信度，且无需后处理或复杂调优，具备架构无关和易集成的优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Calibration_Attention_Instance-wise_Temperature_Scaling_for_Vision_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-Hybrid-Long-and-Short-Range-Flows-for-Point-Cloud-Filtering"><a href="#47-Hybrid-Long-and-Short-Range-Flows-for-Point-Cloud-Filtering" class="headerlink" title="47. Hybrid Long and Short Range Flows for Point Cloud Filtering"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Hybrid_Long_and_Short_Range_Flows_for_Point_Cloud_Filtering.pdf">Hybrid Long and Short Range Flows for Point Cloud Filtering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Western Australia</span></p>
<p>本文提出了一种混合点云滤波方法HybridPF，将短程分数估计（score-based）与长程流（long-range flow）相结合，通过并行的ShortModule和LongModule模块分别建模短程与长程噪声去除轨迹，并用联合损失实现端到端训练。实验表明，该方法在多种合成和真实点云数据上实现了更优的滤波精度和更快的推理速度，有效克服了现有方法的点聚集或噪声残留问题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Hybrid_Long_and_Short_Range_Flows_for_Point_Cloud_Filtering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-QueryCraft-Transformer-Guided-Query-Initialization-for-Enhanced-Human-Object-Interaction-Detection"><a href="#48-QueryCraft-Transformer-Guided-Query-Initialization-for-Enhanced-Human-Object-Interaction-Detection" class="headerlink" title="48. QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/QueryCraft__Transformer-Guided_Query_Initialization_for_Enhanced_Human-Object_Interaction_Detection.pdf">QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">South China University of Technology</span></p>
<p>本文提出QueryCraft框架，通过引入两大模块：知识蒸馏的感知查询解码器（PDQD）、以及基于跨模态Transformer的动作感知交互查询器（ACTOR），赋予DETR类HOI检测模型以语义先验和指导特征学习。实验表明，该方法在HICO-Det和V-COCO等基准上大幅提升了检测性能、泛化能力和训练效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/QueryCraft_Transformer-Guided_Query_Initialization_for_Enhanced_Human-Object_Interaction_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-RealisMotion-Decomposed-Human-Motion-Control-and-Video-Generation-in-the-World-Space"><a href="#49-RealisMotion-Decomposed-Human-Motion-Control-and-Video-Generation-in-the-World-Space" class="headerlink" title="49. RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/RealisMotion__Decomposed_Human_Motion_Control_and_Video_Generation_in_the_World_Space.pdf">RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">DAMO Academy, Alibaba Group</span></p>
<p>本文提出RealisMotion框架，将人体视频生成任务中的主体、背景、运动轨迹和动作进行显式解耦。方法首先在物理世界空间中构建3D坐标系，通过SMPL-X参数及深度估计进行运动编辑，然后基于扩散-Transformer视频生成模型实现主体注入、背景拼接和运动控制信号融合，实现了任意人物在任意背景下按指定轨迹和动作生成自然视频。实验表明，该方法在可控性和视频质量上均达到当前最优。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/RealisMotion_Decomposed_Human_Motion_Control_and_Video_Generation_in_the_World_Space.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-Boosting-Generic-Semi-Supervised-Medical-Image-Segmentation-via-Diverse-Teaching-and-Label-Propagation"><a href="#50-Boosting-Generic-Semi-Supervised-Medical-Image-Segmentation-via-Diverse-Teaching-and-Label-Propagation" class="headerlink" title="50. Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse Teaching and Label Propagation"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Boosting_Generic_Semi-Supervised_Medical_Image_Segmentation_via_Diverse_Teaching_and_Label_Propagati.pdf">Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse Teaching and Label Propagation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了一种通用半监督医学图像分割框架DTLP-Net，融合了多样教师模型、伪标签传播、全局-局部一致性学习及知识蒸馏等关键技术，能够同时应对SSMIS、UMDA和Semi-MDG等任务。实验表明，该方法在五个公开数据集上超越了现有最优方法，显著提升了在标注有限和域迁移场景下的分割性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Boosting_Generic_Semi-Supervised_Medical_Image_Segmentation_via_Diverse_Teaching_and_Label_Propagation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-Training-Kindai-OCR-with-parallel-textline-images-and-self-attention-feature-distance-based-loss"><a href="#51-Training-Kindai-OCR-with-parallel-textline-images-and-self-attention-feature-distance-based-loss" class="headerlink" title="51. Training Kindai OCR with parallel textline images and self-attention feature distance-based loss"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Training_Kindai_OCR_with_parallel_textline_images_and_self-attention_feature_distance-based_loss.pdf">Training Kindai OCR with parallel textline images and self-attention feature distance-based loss</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nguyen Tat Thanh University</span></p>
<p>本文提出通过生成并利用近代（日文）历史文献原图与现代字体合成图组成的平行文本行图像对，结合基于自注意特征距离（欧氏距离和最大均值差异MMD）的目标函数，提升Transformer OCR在样本稀缺条件下的识别能力。实验表明，该方法有效缩小了特征领域差距，在历史文献字符识别中使CER分别下降2.23%和3.94%，显著提升了模型的泛化与判别能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Training_Kindai_OCR_with_parallel_textline_images_and_self-attention_feature_distance-based_loss.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="52-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models"><a href="#52-Time-Is-a-Feature-Exploiting-Temporal-Dynamics-in-Diffusion-Language-Models" class="headerlink" title="52. Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Time_Is_a_Feature__Exploiting_Temporal_Dynamics_in_Diffusion_Language_Models.pdf">Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>本文针对扩散大语言模型（dLLMs）在文本生成过程中存在的“时序振荡”现象，提出了两种利用中间预测提升模型性能的方法：一是无需额外训练的“时序自一致性投票”，通过聚合不同去噪步的预测结果，提升推理准确性；二是基于强化学习的“时序一致性增强”，采用时序语义熵（TSE）作为奖励信号，鼓励模型输出更加稳定一致的结果。实验表明，两种方法结合后在数学推理等数据集上显著提升了dLLMs的准确率和稳定性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Time_Is_a_Feature_Exploiting_Temporal_Dynamics_in_Diffusion_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-BrowseMaster-Towards-Scalable-Web-Browsing-via-Tool-Augmented-Programmatic-Agent-Pair"><a href="#53-BrowseMaster-Towards-Scalable-Web-Browsing-via-Tool-Augmented-Programmatic-Agent-Pair" class="headerlink" title="53. BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/BrowseMaster__Towards_Scalable_Web_Browsing_via_Tool-Augmented_Programmatic_Agent_Pair.pdf">BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出BrowseMaster框架，通过规划-执行器（planner-executor）智能体对，将任务分解为高层次推理和程序化工具执行两部分，提升大规模网页检索的推理深度与搜索广度。实验结果表明，BrowseMaster在多个中英文复杂信息检索基准上大幅超过现有开源与专有智能体，具备强大的信息获取能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/BrowseMaster_Towards_Scalable_Web_Browsing_via_Tool-Augmented_Programmatic_Agent_Pair.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="54-Complex-Logical-Instruction-Generation"><a href="#54-Complex-Logical-Instruction-Generation" class="headerlink" title="54. Complex Logical Instruction Generation"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Complex_Logical_Instruction_Generation.pdf">Complex Logical Instruction Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Texas at Dallas</span></p>
<p>本文提出了LogicIFGen框架，可自动从代码函数生成可验证、逻辑丰富的自然语言指令，并基于此构建了LogicIFEval基准，包含426个复杂逻辑任务。实验发现，当前主流大语言模型在该基准上的指令跟随能力有限，多数准确率低于60%，揭示了LLMs在执行复杂逻辑指令时的显著不足。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Complex_Logical_Instruction_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-SMA-Who-Said-That-Auditing-Membership-Leakage-in-Semi-Black-box-RAG-Controlling"><a href="#55-SMA-Who-Said-That-Auditing-Membership-Leakage-in-Semi-Black-box-RAG-Controlling" class="headerlink" title="55. SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG Controlling"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/SMA__Who_Said_That__Auditing_Membership_Leakage_in_Semi-Black-box_RAG_Controlling.pdf">SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG Controlling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-Sen University</span></p>
<p>本文提出了SMA（Source-aware Membership Audit）框架，通过输入扰动与零阶优化回归，实现在半黑盒环境下对RAG和MRAG系统生成内容的细粒度溯源归因，区分内容是否来自模型预训练语料或外部检索库。实验表明，SMA在文本和多模态检索增强生成任务中显著优于现有方法，实现更高准确性与覆盖率，为生成式系统的数据合规与隐私审核提供实用工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/SMA_Who_Said_That_Auditing_Membership_Leakage_in_Semi-Black-box_RAG_Controlling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-Bridging-Formal-Language-with-Chain-of-Thought-Reasoning-to-Geometry-Problem-Solving"><a href="#56-Bridging-Formal-Language-with-Chain-of-Thought-Reasoning-to-Geometry-Problem-Solving" class="headerlink" title="56. Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Bridging_Formal_Language_with_Chain-of-Thought_Reasoning_to_Geometry_Problem_Solving.pdf">Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shenzhen Research Institute of Big Data</span></p>
<p>本文提出了一种将自然语言链式思维（Chain-of-Thought, CoT）与几何问题求解中的形式语言相结合的混合推理框架GF-Reasoner。该方法在Qwen2.5-VL-7B基础上，利用自动合成的自然-形式交织数据进行有监督微调，并结合几何求解器反馈的强化学习，训练模型在推理过程中交替生成自然语言和可执行的几何形式代码。实验表明，该方法在多个公开基准上几何问题求解准确率提升高达15%，且推理链条更简洁高效，错误率显著降低。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Bridging_Formal_Language_with_Chain-of-Thought_Reasoning_to_Geometry_Problem_Solving.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-Can-We-Trust-AI-to-Govern-AI-Benchmarking-LLM-Performance-on-Privacy-and-AI-Governance-Exams"><a href="#57-Can-We-Trust-AI-to-Govern-AI-Benchmarking-LLM-Performance-on-Privacy-and-AI-Governance-Exams" class="headerlink" title="57. Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy and AI Governance Exams"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Can_We_Trust_AI_to_Govern_AI__Benchmarking_LLM_Performance_on_Privacy_and_AI_Governance_Exams.pdf">Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy and AI Governance Exams</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Superset Labs PBC</span></p>
<p>该论文系统性地评估了十种主流大语言模型（LLM）在隐私和AI治理领域认证考试（CIPP&#x2F;US, CIPM, CIPT, AIGP）上的表现，通过标准化零样本多项选择题测试，比较模型得分与人类专业通过线。研究发现，Gemini 2.5 Pro、OpenAI GPT-5等前沿模型在隐私法规、技术控制以及AI治理方面均超过人类认证标准，表明这些模型已具备在高风险数据治理场景中辅助或自动完成专业任务的能力，但在管理类内容仍有改进空间。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Can_We_Trust_AI_to_Govern_AI_Benchmarking_LLM_Performance_on_Privacy_and_AI_Governance_Exams.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-Attacks-Defenses-Against-LLM-Fingerprinting"><a href="#58-Attacks-Defenses-Against-LLM-Fingerprinting" class="headerlink" title="58. Attacks &amp; Defenses Against LLM Fingerprinting"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Attacks_and_Defenses_Against_LLM_Fingerprinting.pdf">Attacks &amp; Defenses Against LLM Fingerprinting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Oak Ridge National Laboratory</span></p>
<p>本文提出了一种基于强化学习的LLM指纹攻击方法，通过自动优化查询选择，仅用3个查询即可达到93.89%的指纹识别准确率，比随机查询提升14.2%。同时，作者设计了基于语义保持的输出过滤防御机制，利用次级LLM对输出进行语义不变的重写，有效降低指纹攻击成功率，且输出质量保持高语义相似度。实验表明，强化学习优化可提升攻击效率，防御策略在多模型上均可大幅降低被识别概率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Attacks_%26_Defenses_Against_LLM_Fingerprinting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-A-Survey-on-Training-free-Alignment-of-Large-Language-Models"><a href="#59-A-Survey-on-Training-free-Alignment-of-Large-Language-Models" class="headerlink" title="59. A Survey on Training-free Alignment of Large Language Models"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/A_Survey_on_Training-free_Alignment_of_Large_Language_Models.pdf">A Survey on Training-free Alignment of Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Wuhan University</span></p>
<p>本文系统性综述了大语言模型（LLMs）训练无关（training-free, TF）对齐方法，将其分为预解码、解码中和解码后三个阶段，涵盖关键技术如提示工程、隐藏状态调整、对数值计算和输出过滤&#x2F;修正，并分析了在单模态和多模态模型上的应用机制与局限。结论指出，TF对齐方法在效率、通用性和模型安全性上表现出重大优势，但仍面临泛化能力、推理延迟和知识降级等挑战，未来需发展更高效、可控和适应性强的TF对齐技术。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/A_Survey_on_Training-free_Alignment_of_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-Retrospective-Sparse-Attention-for-Efficient-Long-Context-Generation"><a href="#60-Retrospective-Sparse-Attention-for-Efficient-Long-Context-Generation" class="headerlink" title="60. Retrospective Sparse Attention for Efficient Long-Context Generation"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Retrospective_Sparse_Attention_for_Efficient_Long-Context_Generation.pdf">Retrospective Sparse Attention for Efficient Long-Context Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Seoul National University</span></p>
<p>本文提出了一种名为RetroAttention的KV缓存压缩与更新技术，用于大语言模型长文本生成任务。该方法通过引入轻量级输出缓存，将后续解码步骤中新加载的KV条目，回溯性地补充到历史查询的注意力输出中，实现对过去近似注意力错误的持续修正，极大提升了长上下文推理和生成的准确率。实验结果表明，RetroAttention在不增加KV缓存预算的前提下，有效提升了KV利用率（最大1.6倍）和准确率（最高提升21.9%），同时几乎不增加延迟和内存开销。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Retrospective_Sparse_Attention_for_Efficient_Long-Context_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-Intrinsic-Memory-Agents-Heterogeneous-Multi-Agent-LLM-Systems-through-Structured-Contextual-Memory"><a href="#61-Intrinsic-Memory-Agents-Heterogeneous-Multi-Agent-LLM-Systems-through-Structured-Contextual-Memory" class="headerlink" title="61. Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Intrinsic_Memory_Agents__Heterogeneous_Multi-Agent_LLM_Systems_through_Structured_Contextual_Memory.pdf">Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Alan Turing Institute</span></p>
<p>该论文提出了一种名为Intrinsic Memory Agents的多智能体LLM系统框架，通过为每个智能体分配结构化、角色对齐的异构记忆模板，并基于智能体自身输出动态更新，从而提升多智能体协作的连贯性、角色一致性与任务针对性。论文在PDDL基准和复杂数据管道设计任务中进行评测，结果显示该方法在规划任务上性能提升38.6%，在多项协作质量指标上均优于现有方法，尽管会增加token使用量但保持最高token效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Intrinsic_Memory_Agents_Heterogeneous_Multi-Agent_LLM_Systems_through_Structured_Contextual_Memory.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-ColorGPT-Leveraging-Large-Language-Models-for-Multimodal-Color-Recommendation"><a href="#62-ColorGPT-Leveraging-Large-Language-Models-for-Multimodal-Color-Recommendation" class="headerlink" title="62. ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/ColorGPT__Leveraging_Large_Language_Models_for_Multimodal_Color_Recommendation.pdf">ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Tokyo</span></p>
<p>本论文提出了ColorGPT，一种利用大语言模型（LLM）进行多模态颜色推荐的无训练（training-free）方法，针对矢量图形设计中的色彩调色板补全和生成任务。方法通过系统性测试多种颜色表示和提示工程，结合结构化JSON文档描述与相似性检索的上下文示例，实现了调色板自动补全与文本到调色板生成，并在准确率和多样性方面显著优于现有方法。结论显示，LLM具备常识推理能力，可有效辅助复杂设计场景下的颜色推荐与设计工作流。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/ColorGPT_Leveraging_Large_Language_Models_for_Multimodal_Color_Recommendation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-Safe-Semantics-Unsafe-Interpretations-Tackling-Implicit-Reasoning-Safety-in-Large-Vision-Language-Models"><a href="#63-Safe-Semantics-Unsafe-Interpretations-Tackling-Implicit-Reasoning-Safety-in-Large-Vision-Language-Models" class="headerlink" title="63. Safe Semantics, Unsafe Interpretations: Tackling Implicit Reasoning Safety in Large Vision-Language Models"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Safe_Semantics,_Unsafe_Interpretations__Tackling_Implicit_Reasoning_Safety_in_Large_Vision-Language_.pdf">Safe Semantics, Unsafe Interpretations: Tackling Implicit Reasoning Safety in Large Vision-Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出了Implicit Reasoning Safety（IRS）概念，指出大规模视觉-语言模型（LVLMs）在处理多模态输入时存在安全隐患，即单独安全的图像和文本组合后可能导致危险输出。为此，作者构建了首个用于该问题的数据集SSUI，并采用基于In-Context Learning的方法进行安全性提升实验。结果显示，使用SSUI后，LVLMs生成安全有效响应的能力显著提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Safe_Semantics%2C_Unsafe_Interpretations_Tackling_Implicit_Reasoning_Safety_in_Large_Vision-Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-Scaling-Up-Active-Testing-to-Large-Language-Models"><a href="#64-Scaling-Up-Active-Testing-to-Large-Language-Models" class="headerlink" title="64. Scaling Up Active Testing to Large Language Models"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Scaling_Up_Active_Testing_to_Large_Language_Models.pdf">Scaling Up Active Testing to Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Oxford</span></p>
<p>本文提出了一种将主动测试（active testing）方法扩展到大语言模型（LLM）评估的新流程。通过用小型且一次性构建的代理模型（利用in-context learning而无需反复训练）指导数据采集，并可仅依赖代理模型进行数据选择，有效降低了评估大语言模型的计算与标注开销。此外，提出了单次运行下的风险估计误差自举评估方法。实验表明，该方法在多个文本分类与推理任务上，比随机采样大幅提高了评估效率，显著减少了所需标签数，同时保证了误差可控。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Scaling_Up_Active_Testing_to_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-Dynamic-Uncertainty-aware-Multimodal-Fusion-for-Outdoor-Health-Monitoring"><a href="#65-Dynamic-Uncertainty-aware-Multimodal-Fusion-for-Outdoor-Health-Monitoring" class="headerlink" title="65. Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Dynamic_Uncertainty-aware_Multimodal_Fusion_for_Outdoor_Health_Monitoring.pdf">Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">City University of Hong Kong</span></p>
<p>该论文提出了一种名为DUAL-Health的动态不确定性感知多模态融合框架，用于动态和嘈杂环境下的室外健康监测。方法包括：1）通过当前和时序特征分别量化输入噪声和生理信号波动导致的模态不确定性；2）基于不确定性自适应分配每种模态的融合权重，并在Transformer架构中动态调整跨模态注意力，提升对低质量数据的鲁棒性；3）通过模态重建网络将波动的模态分布映射到公共语义空间，实现稳定的跨模态对齐和缺失数据恢复。实验结果表明，DUAL-Health在检测准确率和鲁棒性方面均优于现有多模态大语言模型、深度感知等主流方法，尤其在数据质量退化和模态缺失条件下表现突出。结论：DUAL-Health能在动态室外环境中实现更高效、鲁棒的健康监测。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Dynamic_Uncertainty-aware_Multimodal_Fusion_for_Outdoor_Health_Monitoring.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-P-D-Device-Disaggregated-Large-Language-Model-between-Cloud-and-Devices"><a href="#66-P-D-Device-Disaggregated-Large-Language-Model-between-Cloud-and-Devices" class="headerlink" title="66. P&#x2F;D-Device: Disaggregated Large Language Model between Cloud and Devices"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/P_D-Device__Disaggregated_Large_Language_Model_between_Cloud_and_Devices.pdf">P&#x2F;D-Device: Disaggregated Large Language Model between Cloud and Devices</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huawei Technologies Co., Ltd.</span></p>
<p>本文提出P&#x2F;D-Device系统，实现了大语言模型在云和设备间的预填充与解码分离协作。其核心方法包括云端负责预填充并生成部分token，通过压缩和掩码技术将精炼后的prompt与首个token一起发送到设备，设备利用云端辅助的token平滑过渡长时间预填充，同时可在本地继续解码，算法自动确定最优协同参数。实验表明，该方案能将TTFT（首token延迟）降低至少60%，最大TPOT（每token生成延迟）控制在几十毫秒，云端吞吐提升最高达15倍，且在保证推理质量的同时显著提升响应速度和资源利用率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/P_D-Device_Disaggregated_Large_Language_Model_between_Cloud_and_Devices.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-E3-Rewrite-Learning-to-Rewrite-SQL-for-Executability-Equivalence-and-Efficiency"><a href="#67-E3-Rewrite-Learning-to-Rewrite-SQL-for-Executability-Equivalence-and-Efficiency" class="headerlink" title="67. E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence, and Efficiency"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/E3-Rewrite__Learning_to_Rewrite_SQL_for_Executability,_Equivalence,and_Efficiency.pdf">E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence, and Efficiency</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Soochow University</span></p>
<p>本文提出E3-Rewrite，一种基于大语言模型（LLM）的SQL重写框架，通过注入查询执行计划、混合结构-语义检索和两阶段强化学习（GRPO算法）联合优化可执行性、等价性与效率。实验表明，该方法在多个SQL基准上显著提升了查询效率和等价覆盖率，相较已有方法平均查询时间降低25.6%，成功重写数量提升24.4%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/E3-Rewrite_Learning_to_Rewrite_SQL_for_Executability%2C_Equivalence%2C_and_Efficiency.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-Activation-Steering-for-Bias-Mitigation-An-Interpretable-Approach-to-Safer-LLMs"><a href="#68-Activation-Steering-for-Bias-Mitigation-An-Interpretable-Approach-to-Safer-LLMs" class="headerlink" title="68. Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Activation_Steering_for_Bias_Mitigation__An_Interpretable_Approach_to_Safer_LLMs.pdf">Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Indian Institute of Technology Madras</span></p>
<p>本文提出了一套基于机械解释性的方法，通过在GPT2-large模型内部训练线性探测器（probe）识别和定位性别、种族等偏见的表征，并用激活引导向量（steering vector）在推理阶段主动调整模型生成过程，实时缓解有害内容。实验表明，该方法能有效检测并降低模型输出中的偏见，提升LLM安全性和可控性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Activation_Steering_for_Bias_Mitigation_An_Interpretable_Approach_to_Safer_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-LyS-at-SemEval-2025-Task-8-Zero-Shot-Code-Generation-for-Tabular-QA"><a href="#69-LyS-at-SemEval-2025-Task-8-Zero-Shot-Code-Generation-for-Tabular-QA" class="headerlink" title="69. LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/LyS_at_SemEval_2025_Task_8__Zero-Shot_Code_Generation_for_Tabular_QA.pdf">LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universidade da Coruña</span></p>
<p>该论文提出了一种基于指令的大型语言模型（LLM）的零样本代码生成管道，用于表格问答任务。方法包括三模块：相关列选择、Python代码生成和基于错误反馈的迭代修正，提升了表格数据的问答准确性和系统的适应性。结论显示该方法无需微调即可在SemEval 2025 Tabular QA任务中优于基线模型，但在复杂表结构下准确率有所下降。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/LyS_at_SemEval_2025_Task_8_Zero-Shot_Code_Generation_for_Tabular_QA.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-Prospect-Theory-Fails-for-LLMs-Revealing-Instability-of-Decision-Making-under-Epistemic-Uncertainty"><a href="#70-Prospect-Theory-Fails-for-LLMs-Revealing-Instability-of-Decision-Making-under-Epistemic-Uncertainty" class="headerlink" title="70. Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Prospect_Theory_Fails_for_LLMs__Revealing_Instability_of_Decision-Making_under_Epistemic_Uncertainty.pdf">Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Hong Kong University of Science and Technology</span></p>
<p>该论文提出了一个三阶段实验框架，将行为经济学和语义分析结合，用于评估大型语言模型（LLMs）在不确定性决策中的行为，尤其关注语言中的不确定性表达（epistemic markers）。结果显示，LLMs的决策行为无法稳定地用人类中心的前景理论（Prospect Theory, PT）进行建模，且不同模型对不确定性表达的解释差异大，决策不稳定，结论强调需发展针对AI的决策理论和更好的不确定性校准方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Prospect_Theory_Fails_for_LLMs_Revealing_Instability_of_Decision-Making_under_Epistemic_Uncertainty.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-KFFocus-Highlighting-Keyframes-for-Enhanced-Video-Understanding"><a href="#71-KFFocus-Highlighting-Keyframes-for-Enhanced-Video-Understanding" class="headerlink" title="71. KFFocus: Highlighting Keyframes for Enhanced Video Understanding"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/KFFocus__Highlighting_Keyframes_for_Enhanced_Video_Understanding.pdf">KFFocus: Highlighting Keyframes for Enhanced Video Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本文提出KFFocus，一种针对视频大模型（Vid-LLMs）的混合视频帧与令牌压缩方法，通过借鉴传统视频压缩的关键帧（keyframe）提取与动态令牌凝聚策略，显著减少计算冗余并突出关键时序与语义信息。结合时空建模模块，KFFocus在多项长短视频理解基准上实现了更高效率和准确率，优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/KFFocus_Highlighting_Keyframes_for_Enhanced_Video_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-Integrating-Attention-into-Explanation-Frameworks-for-Language-and-Vision-Transformers"><a href="#72-Integrating-Attention-into-Explanation-Frameworks-for-Language-and-Vision-Transformers" class="headerlink" title="72. Integrating Attention into Explanation Frameworks for Language and Vision Transformers"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Integrating_attention_into_explanation_frameworks_for_language_and_vision_transformers.pdf">Integrating Attention into Explanation Frameworks for Language and Vision Transformers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Norwegian University of Science and Technology</span></p>
<p>本文提出了两种将注意力机制信息整合入主流可解释性AI（XAI）框架的新方法，分别适用于NLP和视觉任务。第一种方法将注意力权重引入Shapley值分解来进行局部特征归因，第二种方法将注意力与概念激活向量结合，计算全球性概念敏感性分数。实验结果显示，注意力权重可有效丰富Transformer模型的可解释性，尤其在计算效率和关注内部机制上表现突出。但纯注意力归因在标准输出相关的XAI指标下略逊于基于输入的归因方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Integrating_Attention_into_Explanation_Frameworks_for_Language_and_Vision_Transformers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning"><a href="#73-Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning" class="headerlink" title="73. Train Long, Think Short: Curriculum Learning for Efficient Reasoning"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Train_Long,_Think_Short__Curriculum_Learning_for_Efficient_Reasoning.pdf">Train Long, Think Short: Curriculum Learning for Efficient Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">King Abdullah University of Science and Technology (KAUST)</span></p>
<p>本文提出了一种基于课程学习的高效推理方法，通过在Group Relative Policy Optimization (GRPO)中引入递减token预算，实现大模型推理链条从探索到压缩的自然过渡。方法融合了正确性、长度效率和格式结构三重奖励信号，实验表明在GSM8K、MATH500等多个数学推理数据集上，课程学习显著提升了准确率和token效率，优于固定预算基线，且在不同数据集和预算调度下均有稳定表现。结论：课程驱动的压缩训练是提升大语言模型推理效率和能力的有效通用范式。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Train_Long%2C_Think_Short_Curriculum_Learning_for_Efficient_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-Compass-Thinker-7B-Technical-Report"><a href="#74-Compass-Thinker-7B-Technical-Report" class="headerlink" title="74. Compass-Thinker-7B Technical Report"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Compass-Thinker-7B_Technical_Report.pdf">Compass-Thinker-7B Technical Report</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shopee</span></p>
<p>本文提出Compass-Thinker-7B，一种基于开源大语言模型，通过精心设计的强化学习（RL）流水线进行训练，采用GRPO变体及多项改进，并构建3万道可验证数学题数据集用于奖励建模。实验表明，该模型在AIME24等多个数学推理基准上表现优异，尤其在AIME24准确率由20%提升到40%，证明了低资源下精细RL流程对小型LLM推理能力的显著提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Compass-Thinker-7B_Technical_Report.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-Oblivionis-A-Lightweight-Learning-and-Unlearning-Framework-for-Federated-Large-Language-Models"><a href="#75-Oblivionis-A-Lightweight-Learning-and-Unlearning-Framework-for-Federated-Large-Language-Models" class="headerlink" title="75. Oblivionis: A Lightweight Learning and Unlearning Framework for Federated Large Language Models"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Oblivionis__A_Lightweight_Learning_and_Unlearning_Framework_for_Federated_Large_Language_Models.pdf">Oblivionis: A Lightweight Learning and Unlearning Framework for Federated Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanyang Technological University</span></p>
<p>本文提出了Oblivionis框架，将联邦学习(FL)与定向遗忘(unlearning)联合建模为双目标优化，实现分布式大模型在不共享原始隐私数据的前提下，支持特定数据或客户贡献的后期删除。方法集成6种主流联邦学习算法与5种遗忘算法，并在TOFU和MUSE等基准上验证，结果显示Oblivionis能在遗忘效果和模型效用间实现稳健平衡，优于本地训练且提升法规合规性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Oblivionis_A_Lightweight_Learning_and_Unlearning_Framework_for_Federated_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-Steering-Towards-Fairness-Mitigating-Political-Bias-in-LLMs"><a href="#76-Steering-Towards-Fairness-Mitigating-Political-Bias-in-LLMs" class="headerlink" title="76. Steering Towards Fairness: Mitigating Political Bias in LLMs"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Steering_Towards_Fairness__Mitigating_Political_Bias_in_LLMs.pdf">Steering Towards Fairness: Mitigating Political Bias in LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Macquarie University</span></p>
<p>本文提出了一种基于对比政治构造（Political Compass Test, PCT）和Steering Vector Ensembles (SVE) 的大语言模型（LLM）去偏框架，通过层级激活抽取与向量注入，对模型内部意识形态偏见进行探测与干预。实验结果显示，SVE在多语言、特别是低资源语言环境下，可在不损失输出流畅度和语境相关性的情况下，实现高达60%的社会偏见降低，优于Individual Steering Vectors (ISV)。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Steering_Towards_Fairness_Mitigating_Political_Bias_in_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-An-Investigation-of-Robustness-of-LLMs-in-Mathematical-Reasoning-Benchmarking-with-Mathematically-Equivalent-Transformation-of-Advanced-Mathematical-Problems"><a href="#77-An-Investigation-of-Robustness-of-LLMs-in-Mathematical-Reasoning-Benchmarking-with-Mathematically-Equivalent-Transformation-of-Advanced-Mathematical-Problems" class="headerlink" title="77. An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/An_Investigation_of_Robustness_of_LLMs_in_Mathematical_Reasoning__Benchmarking_with_Mathematically-E.pdf">An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Illinois Urbana-Champaign</span></p>
<p>该论文提出Generalization-and-Perturbation (GAP) 框架，通过对Putnam竞赛数学题进行数学等价但语言和参数不同的变换，系统评估大型语言模型（LLM）在数学推理上的鲁棒性。作者构建了包含6,306道题目的PutnamGAP数据集，对18个主流LLM进行了零样本测试，结果显示各模型在变量名或参数变换下准确率显著下降，揭示了当前LLM在符号和结构泛化上的不足。结论认为，当前高分模型在面对表面变化时表现脆弱，需重视数据污染防御、训练策略多样化和安全性提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/An_Investigation_of_Robustness_of_LLMs_in_Mathematical_Reasoning_Benchmarking_with_Mathematically-Equivalent_Transformation_of_Advanced_Mathematical_Problems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-3DFroMLLM-3D-Prototype-Generation-only-from-Pretrained-Multimodal-LLMs"><a href="#78-3DFroMLLM-3D-Prototype-Generation-only-from-Pretrained-Multimodal-LLMs" class="headerlink" title="78. 3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/3DFroMLLM__3D_Prototype_Generation_only_from_Pretrained_Multimodal_LLMs.pdf">3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Technology Nuremberg</span></p>
<p>本文提出3DFroMLLM框架，实现了仅依赖预训练多模态大模型（MLLMs），无需额外训练数据或详细用户指令，直接生成包含几何体和部件标签的3D原型。其方法采用Designer、Coder和Visual Inspector三代理反馈循环，自动分解、生成和优化3D模型。实验表明，该方法生成的3D原型用于图像分类预训练较以往方法提升15%，用于CLIP部件分割微调可提升55%（无人工标注）。结论：多模态大模型具备直接生成高质量3D原型并助力下游视觉任务的能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/3DFroMLLM_3D_Prototype_Generation_only_from_Pretrained_Multimodal_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-A-Dual-Axis-Taxonomy-of-Knowledge-Editing-for-LLMs-From-Mechanisms-to-Functions"><a href="#79-A-Dual-Axis-Taxonomy-of-Knowledge-Editing-for-LLMs-From-Mechanisms-to-Functions" class="headerlink" title="79. A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to Functions"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/A_Dual-Axis_Taxonomy_of_Knowledge_Editing_for_LLMs__From_Mechanisms_to_Functions.pdf">A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to Functions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Tehran</span></p>
<p>本文提出了一种针对大型语言模型（LLM）知识编辑的双轴分类体系：一轴为编辑机制（如参数修改、外部记忆、神经元增强、元学习），另一轴为知识类型（事实、时序、概念、常识、社会知识），系统综述了不同机制在多类知识编辑场景下的适用性、评估方法及未来挑战。结论认为，知识编辑为LLM高效、可控地更新内部知识提供了重要手段，但仍需提升理论基础、规模化、多样化知识适应和安全性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/A_Dual-Axis_Taxonomy_of_Knowledge_Editing_for_LLMs_From_Mechanisms_to_Functions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-Interpretable-Reward-Model-via-Sparse-Autoencoder"><a href="#80-Interpretable-Reward-Model-via-Sparse-Autoencoder" class="headerlink" title="80. Interpretable Reward Model via Sparse Autoencoder"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Interpretable_Reward_Model_via_Sparse_Autoencoder.pdf">Interpretable Reward Model via Sparse Autoencoder</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>该论文提出了一种名为Sparse Autoencoder-enhanced Reward Model (SARM)的新架构，将预训练的稀疏自动编码器（SAE）集成到奖励模型中，通过将LLM隐藏激活映射到稀疏、单语义特征空间，使奖励分数可解释且可控。实验结果显示，SARM实现了特征级奖励归因和偏好动态调整，并在对齐任务上优于传统奖励模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Interpretable_Reward_Model_via_Sparse_Autoencoder.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-SciRerankBench-Benchmarking-Rerankers-Towards-Scientific-Retrieval-Augmented-Generated-LLMs"><a href="#81-SciRerankBench-Benchmarking-Rerankers-Towards-Scientific-Retrieval-Augmented-Generated-LLMs" class="headerlink" title="81. SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/SciRerankBench__Benchmarking_Rerankers_Towards_Scientific_Retrieval-Augmented_Generated_LLMs.pdf">SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Computer Network Information Center, Chinese Academy of Sciences</span></p>
<p>本文提出SciRerankBench，这是首个专为科学领域RAG-LLMs中重排序模块（reranker）设计的评测基准，涵盖生物、物理、化学、地理和数学五大学科，基于2.5亿篇学术论文构建4.5K问-上下文-答（Q-C-A）对，包含噪声、语义相似但逻辑无关及反事实三类干扰上下文。系统评测13种主流重排序方法在11个开源LLM下的表现，发现重排序器显著提升RAG-LLMs的检索与答案生成质量，尤其是cross-encoder架构在复杂推理任务中表现最佳，同时最终答案质量仍受限于LLM自身的推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/SciRerankBench_Benchmarking_Rerankers_Towards_Scientific_Retrieval-Augmented_Generated_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-Simulating-Generative-Social-Agents-via-Theory-Informed-Workflow-Design"><a href="#82-Simulating-Generative-Social-Agents-via-Theory-Informed-Workflow-Design" class="headerlink" title="82. Simulating Generative Social Agents via Theory-Informed Workflow Design"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Simulating_Generative_Social_Agents_via_Theory-Informed_Workflow_Design.pdf">Simulating Generative Social Agents via Theory-Informed Workflow Design</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Hong Kong University of Science and Technology (Guangzhou)</span></p>
<p>本文提出了一种基于社会认知理论的生成式社会代理统一设计框架，结合Maslow需求层次理论、计划行为理论和社会学习理论，将动机建模、行动规划和学习三大模块集成于LLM驱动的社会代理中，实现了目标形成、行为规划与行为适应的闭环流程。实验结果表明，该方法在多场景下显著提升社会代理的行为一致性和现实性，偏离真实数据的误差比主流生成式基线低65–80%，三大模块缺失会导致1.5–10倍的性能下降，证明其不可或缺性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Simulating_Generative_Social_Agents_via_Theory-Informed_Workflow_Design.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-A-Survey-on-Parallel-Text-Generation-From-Parallel-Decoding-to-Diffusion-Language-Models"><a href="#83-A-Survey-on-Parallel-Text-Generation-From-Parallel-Decoding-to-Diffusion-Language-Models" class="headerlink" title="83. A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/A_Survey_on_Parallel_Text_Generation__From_Parallel_Decoding_to_Diffusion_Language_Models.pdf">A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本论文系统梳理并分类了并行文本生成的主要方法，包括基于自回归（AR-Based）和非自回归（Non-AR-Based）的多种加速范式，如草稿-验证、分解-填充、多Token预测、一次性生成、掩码生成和编辑式细化。作者不仅从理论上比较了各类方法在速度、质量和资源上的权衡，还分析了不同技术组合的潜力与挑战，指出并行生成有望显著提升大语言模型推理效率，但面临质量-速度权衡和系统优化等难题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/A_Survey_on_Parallel_Text_Generation_From_Parallel_Decoding_to_Diffusion_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-ASPD-Unlocking-Adaptive-Serial-Parallel-Decoding-by-Exploring-Intrinsic-Parallelism-in-LLMs"><a href="#84-ASPD-Unlocking-Adaptive-Serial-Parallel-Decoding-by-Exploring-Intrinsic-Parallelism-in-LLMs" class="headerlink" title="84. ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/ASPD__Unlocking_Adaptive_Serial-Parallel_Decoding_by_Exploring_Intrinsic_Parallelism_in_LLMs.pdf">ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tencent YouTu Lab</span></p>
<p>本文提出了Adaptive Serial-Parallel Decoding (ASPD) 框架，通过自动挖掘大语言模型（LLM）生成内容中的内在并行结构，并引入无侵入式并行数据转换管线、分支不可见注意力掩码和共享位置编码，实现了高效的串行-并行混合解码。实验证明，ASPD显著提升了LLM推理速度（平均加速1.85-3.19倍），在多个基准任务上几乎不损失输出质量，适用于对时延敏感的实际场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/ASPD_Unlocking_Adaptive_Serial-Parallel_Decoding_by_Exploring_Intrinsic_Parallelism_in_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-Entangled-in-Representations-Mechanistic-Investigation-of-Cultural-Biases-in-Large-Language-Models"><a href="#85-Entangled-in-Representations-Mechanistic-Investigation-of-Cultural-Biases-in-Large-Language-Models" class="headerlink" title="85. Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Entangled_in_Representations__Mechanistic_Investigation_of_Cultural_Biases_in_Large_Language_Models.pdf">Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Copenhagen</span></p>
<p>本论文提出了Culturescope方法，通过基于机制可解释性的技术（如激活patching），直接探查大语言模型（LLM）内部层的文化知识表征，并引入文化扁平化分数（CF score）量化不同文化表征的混同与偏差。实验证明主流LLM存在西方主导和文化扁平化的内部偏见，低资源文化因训练数据匮乏受影响较小，提示未来需结合偏见与资源特征提升模型文化理解。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Entangled_in_Representations_Mechanistic_Investigation_of_Cultural_Biases_in_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="86-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them"><a href="#86-BiasGym-Fantastic-Biases-and-How-to-Find-and-Remove-Them" class="headerlink" title="86. BiasGym: Fantastic Biases and How to Find (and Remove) Them"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/BiasGym__Fantastic_Biases_and_How_to_Find_(and_Remove)_Them.pdf">BiasGym: Fantastic Biases and How to Find (and Remove) Them</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Copenhagen</span></p>
<p>本文提出BiasGym，一个针对大型语言模型（LLM）的通用、低成本偏见注入与消除框架。方法包括通过BiasInject模块在模型中注入特定偏见（采用特殊token的微调），然后利用BiasScope模块识别和移除与偏见相关的注意力头，实现针对性去偏见，同时保持模型下游任务性能。实验结果显示，BiasGym能有效减少真实世界和虚构的刻板印象，且不会显著影响问答或指令跟随等任务能力，为LLM安全及可解释性研究提供新工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/BiasGym_Fantastic_Biases_and_How_to_Find_%28and_Remove%29_Them.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-EditMF-Drawing-an-Invisible-Fingerprint-for-Your-Large-Language-Models"><a href="#87-EditMF-Drawing-an-Invisible-Fingerprint-for-Your-Large-Language-Models" class="headerlink" title="87. EditMF: Drawing an Invisible Fingerprint for Your Large Language Models"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/EditMF__Drawing_an_Invisible_Fingerprint_for_Your_Large_Language_Models.pdf">EditMF: Drawing an Invisible Fingerprint for Your Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">China Agricultural University</span></p>
<p>EditMF提出了一种基于模型编辑的无训练指纹嵌入方法，通过将所有权信息加密为虚构作者-小说-主角三元组，并利用因果追踪和零空间更新在大语言模型中嵌入极其隐蔽的专属指纹。该方法无需额外训练，仅需一次黑盒查询即可验证所有权，实验表明其嵌入鲁棒性高，基本无性能损失，计算开销极低，优于现有LoRA与SFT指纹方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/EditMF_Drawing_an_Invisible_Fingerprint_for_Your_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-Silicon-Minds-versus-Human-Hearts-The-Wisdom-of-Crowds-Beats-the-Wisdom-of-AI-in-Emotion-Recognition"><a href="#88-Silicon-Minds-versus-Human-Hearts-The-Wisdom-of-Crowds-Beats-the-Wisdom-of-AI-in-Emotion-Recognition" class="headerlink" title="88. Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of AI in Emotion Recognition"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Silicon_Minds_versus_Human_Hearts__The_Wisdom_of_Crowds_Beats_the_Wisdom_of_AI_in_Emotion_Recognitio.pdf">Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of AI in Emotion Recognition</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Elon University</span></p>
<p>本论文系统评估了多模态大型语言模型（MLLMs，如GPT-4o）在情绪识别任务中的表现，并与不同能力水平的人类进行对比，包括低、中、高水平和群体智慧。方法采用RMET和MRMET标准测试，通过单体和集体（多数投票）方式比较GPT-4o与人类的正确率，并分析了人机协作（增强智能）方案。结论：GPT-4o在个体层面全范围优于人类，但人类群体（“群体智慧”）通过集体判断显著超越AI群体，且人机协作能进一步提升准确率，显示未来情感智能AI最优路径是人机协作。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Silicon_Minds_versus_Human_Hearts_The_Wisdom_of_Crowds_Beats_the_Wisdom_of_AI_in_Emotion_Recognition.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments"><a href="#89-Feedback-Driven-Tool-Use-Improvements-in-Large-Language-Models-via-Automated-Build-Environments" class="headerlink" title="89. Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Feedback-Driven_Tool-Use_Improvements_in_Large_Language_Models_via_Automated_Build_Environments.pdf">Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Fudan University</span></p>
<p>本文提出了一种自动化环境构建流水线，包括场景分解、文档生成、功能整合、复杂度扩展及本地部署，实现了无需依赖外部工具即可大规模生成高质量LLM工具使用训练环境。基于该环境，设计了可验证的奖励机制，对工具调用的精确性与任务完成度进行联合评估，并与RL算法结合实现反馈驱动的模型训练。实验表明，该方法显著提升了不同规模LLM在工具使用上的表现，且不损害模型的通用能力，主要提升源于模型低层MLP参数的更新，从而增强了上下文理解和推理能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Feedback-Driven_Tool-Use_Improvements_in_Large_Language_Models_via_Automated_Build_Environments.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge"><a href="#90-Evaluating-Podcast-Recommendations-with-Profile-Aware-LLM-as-a-Judge" class="headerlink" title="90. Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Evaluating_Podcast_Recommendations_with_Profile-Aware_LLM-as-a-Judge.pdf">Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Spotify</span></p>
<p>该论文提出了一种基于大型语言模型(LLM)的离线评估框架，通过自动生成自然语言用户画像来提升播客推荐系统的个性化评估能力。方法包括从用户近90天收听历史提炼出用户兴趣画像，利用LLM依据画像与播客元数据进行点对点及模型间对比判断，结果显示该画像感知方法在与人工评价一致性上优于直接使用历史数据，提升了可扩展性和解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Evaluating_Podcast_Recommendations_with_Profile-Aware_LLM-as-a-Judge.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-DevNous-An-LLM-Based-Multi-Agent-System-for-Grounding-IT-Project-Management-in-Unstructured-Conversation"><a href="#91-DevNous-An-LLM-Based-Multi-Agent-System-for-Grounding-IT-Project-Management-in-Unstructured-Conversation" class="headerlink" title="91. DevNous: An LLM-Based Multi-Agent System for Grounding IT Project Management in Unstructured Conversation"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DevNous__An_LLM-Based_Multi-Agent_System_for_Grounding_IT_Project_Management_in_Unstructured_Convers.pdf">DevNous: An LLM-Based Multi-Agent System for Grounding IT Project Management in Unstructured Conversation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">International Hellenic University</span></p>
<p>本论文提出了DevNous，一个基于大型语言模型（LLM）的分层多智能体专家系统，专为将IT项目团队中非结构化对话自动翻译为结构化项目管理工件而设计。系统集成于团队聊天环境，通过专用子智能体实现意图识别、任务自动结构化与进展摘要生成，并引入了160轮人机交互合成对话的公开基准数据集。实验证明，DevNous在该新基准上取得了81.3%的turn-level准确率和0.845的多标签F1分数，显著优于单体式基线，验证了多智能体架构和方法的有效性和可行性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DevNous_An_LLM-Based_Multi-Agent_System_for_Grounding_IT_Project_Management_in_Unstructured_Conversation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-IROTE-Human-like-Traits-Elicitation-of-Large-Language-Model-via-In-Context-Self-Reflective-Optimization"><a href="#92-IROTE-Human-like-Traits-Elicitation-of-Large-Language-Model-via-In-Context-Self-Reflective-Optimization" class="headerlink" title="92. IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/IROTE__Human-like_Traits_Elicitation_of_Large_Language_Model_via_In-Context_Self-Reflective_Optimiza.pdf">IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出IROTE，一种基于心理学自我反思理论的无微调大语言模型（LLM）人类特质诱导方法，通过信息瓶颈式目标优化生成文本自我反思，并将其嵌入任务提示中，稳定激发LLM展现特定人类特质。实验结果表明，IROTE在多种主流人类特质体系和复杂任务下均优于现有基线，能显著提升LLM在问卷与下游任务中的特质稳定性与可迁移性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/IROTE_Human-like_Traits_Elicitation_of_Large_Language_Model_via_In-Context_Self-Reflective_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-STELAR-VISION-Self-Topology-Aware-Efficient-Learning-for-Aligned-Reasoning-in-Vision"><a href="#93-STELAR-VISION-Self-Topology-Aware-Efficient-Learning-for-Aligned-Reasoning-in-Vision" class="headerlink" title="93. STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/STELAR-VISION__Self-Topology-Aware_Efficient_Learning_for_Aligned_Reasoning_in_Vision.pdf">STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Carnegie Mellon University</span></p>
<p>本文提出了STELAR-Vision训练框架，通过TopoAug自动生成多样化链式、树式和图式推理拓扑结构，并利用监督微调和强化学习提升视觉-语言模型（VLMs）的多模态推理能力和输出效率。实验结果显示，STELAR-Vision在多个视觉推理基准上显著提升准确率（对基线提升9.7%，对更大模型提升7.3%），并通过Frugal Learning变体在保证准确率的同时将输出长度缩短18.1%，在五个分布外数据集上表现出强泛化能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/STELAR-VISION_Self-Topology-Aware_Efficient_Learning_for_Aligned_Reasoning_in_Vision.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-Imposing-AI-Deceptive-design-patterns-against-sustainability"><a href="#94-Imposing-AI-Deceptive-design-patterns-against-sustainability" class="headerlink" title="94. Imposing AI: Deceptive design patterns against sustainability"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Imposing_AI__Deceptive_design_patterns_against_sustainability.pdf">Imposing AI: Deceptive design patterns against sustainability</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Université de Strasbourg</span></p>
<p>本论文通过收集和分析多家主流数字平台在2024年引入生成式AI功能的界面设计变化，揭示了两类主要的欺骗性设计策略：一是以牺牲原有非AI功能为代价强制推广AI特性，二是通过“魔法”与“助手”隐喻等叙事手法美化并掩盖AI的实际环境影响。结论指出，这些策略不仅降低了用户自主选择权，还加剧了数字基础设施的环境负担，亟需通过“欺骗性设计模式”视角推动相关监管与可持续设计。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Imposing_AI_Deceptive_design_patterns_against_sustainability.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-LLM-driven-Text-to-Table-Generation-through-Sub-Tasks-Guidance-and-Iterative-Refinement"><a href="#95-LLM-driven-Text-to-Table-Generation-through-Sub-Tasks-Guidance-and-Iterative-Refinement" class="headerlink" title="95. LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/LLM_driven_Text-to-Table_Generation_through_Sub-Tasks_Guidance_and_Iterative_Refinement.pdf">LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">IBM Research</span></p>
<p>本文提出了一种LLM驱动的文本到表格生成系统，采用两个关键策略：一是将复杂任务分解为中间子任务（如表头解释、缩写扩展、数据格式解析和实体抽取），二是通过LLM自反馈对生成表格进行逐步细化和迭代优化。实验证明，子任务引导和细粒度的单元格级自反馈显著提升了表格生成的准确性，在Rotowire和LiveSum两个复杂数据集上实现了当前最优性能，同时分析了性能提升与计算成本之间的权衡。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/LLM_driven_Text-to-Table_Generation_through_Sub-Tasks_Guidance_and_Iterative_Refinement.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-Classifier-Language-Models-Unifying-Sparse-Finetuning-and-Adaptive-Tokenization-for-Specialized-Classification-Tasks"><a href="#96-Classifier-Language-Models-Unifying-Sparse-Finetuning-and-Adaptive-Tokenization-for-Specialized-Classification-Tasks" class="headerlink" title="96. Classifier Language Models: Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Classifier_Language_Models__Unifying_Sparse_Finetuning_and_Adaptive_Tokenization_for_Specialized_Cla.pdf">Classifier Language Models: Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Amazon Web Services Inc.</span></p>
<p>本文提出了一种结合稀疏微调与自适应分词的新方法（AdaPT），用于高效适应小型预训练语言模型到特定领域分类任务。该方法通过挖掘任务特定的连续和非连续分词序列，扩展词表并仅微调对任务最敏感的模型参数，不引入新参数。结果表明，该方法在五个语义分类任务上稳定性更高、准确率更优且训练成本减半，优于端到端微调、LoRA、层选择和前缀微调等主流方法，尤其在需行业知识的专门任务中表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Classifier_Language_Models_Unifying_Sparse_Finetuning_and_Adaptive_Tokenization_for_Specialized_Classification_Tasks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-AgriGPT-a-Large-Language-Model-Ecosystem-for-Agriculture"><a href="#97-AgriGPT-a-Large-Language-Model-Ecosystem-for-Agriculture" class="headerlink" title="97. AgriGPT: a Large Language Model Ecosystem for Agriculture"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/AgriGPT__a_Large_Language_Model_Ecosystem_for_Agriculture.pdf">AgriGPT: a Large Language Model Ecosystem for Agriculture</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">College of Computer Science and Technology, Zhejiang University</span></p>
<p>本论文提出了AgriGPT，一个专为农业领域设计的大语言模型生态系统。方法包括多智能体数据引擎构建Agri-342K高质量指令数据集、持续预训练与有监督微调，并创新性地采用三通道检索增强生成（Tri-RAG）框架结合稠密&#x2F;稀疏检索与多跳知识图谱推理，显著提升事实性与推理能力。结论表明，AgriGPT在农业领域适应性和推理能力均超越通用大模型，同时具备良好通用性和多语言迁移能力，有助于推动农业AI普惠应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/AgriGPT_a_Large_Language_Model_Ecosystem_for_Agriculture.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-QoE-Aware-Service-Provision-for-Mobile-AR-Rendering-An-Agent-Driven-Approach"><a href="#98-QoE-Aware-Service-Provision-for-Mobile-AR-Rendering-An-Agent-Driven-Approach" class="headerlink" title="98. QoE-Aware Service Provision for Mobile AR Rendering: An Agent-Driven Approach"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/QoE-Aware_Service_Provision_for_Mobile_AR_Rendering__An_Agent-Driven_Approach.pdf">QoE-Aware Service Provision for Mobile AR Rendering: An Agent-Driven Approach</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xidian University</span></p>
<p>该论文提出了一种基于大语言模型(LLM)的智能代理，实现移动增强现实(MAR)服务端与网络端的跨域协同，提升边缘辅助AR渲染体验。方法包括通过代理和API工具链进行用户特定QoE建模，以及结合Kalman滤波实现自适应资源管理。实验结果表明，该方法在用户QoE建模精度和带宽利用效率方面均优于传统方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/QoE-Aware_Service_Provision_for_Mobile_AR_Rendering_An_Agent-Driven_Approach.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="99-DepressLLM-Interpretable-domain-adapted-language-model-for-depression-detection-from-real-world-narratives"><a href="#99-DepressLLM-Interpretable-domain-adapted-language-model-for-depression-detection-from-real-world-narratives" class="headerlink" title="99. DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives"></a>99. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DepressLLM__Interpretable_domain-adapted_language_model_for_depression_detection_from_real-world_nar.pdf">DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Electronics and Telecommunications Research Institute</span></p>
<p>该论文提出了DepressLLM，一种基于大规模语言模型的抑郁症检测方法，通过在包含幸福与痛苦叙述的真实患者语料上微调主流LLM，并引入SToPS（Score-guided Token Probability Summation）模块，实现了可解释性预测及置信度估算。实验结果表明，DepressLLM在多种异构数据集上表现出优越的分类性能和鲁棒性，且高置信度输出与临床专家判断高度一致，推动了精神健康AI的早期筛查应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DepressLLM_Interpretable_domain-adapted_language_model_for_depression_detection_from_real-world_narratives.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="100-DocThinker-Explainable-Multimodal-Large-Language-Models-with-Rule-based-Reinforcement-Learning-for-Document-Understanding"><a href="#100-DocThinker-Explainable-Multimodal-Large-Language-Models-with-Rule-based-Reinforcement-Learning-for-Document-Understanding" class="headerlink" title="100. DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding"></a>100. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/DocThinker__Explainable_Multimodal_Large_Language_Models_with_Rule-based_Reinforcement_Learning_for_.pdf">DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huazhong University of Science and Technology</span></p>
<p>该论文提出DocThinker，一种基于规则的强化学习(RL)框架，用于多模态大语言模型(MLLM)在文档理解中的推理与可解释性提升。方法通过Group Relative Policy Optimization (GRPO)和多目标奖励机制，动态优化推理策略，生成结构化推理过程、改写问题、关键区域定位和最终答案，有效改善模型的泛化能力和输出的可解释性。实验结果表明DocThinker在多个文档理解与多模态推理任务上优于传统SFT和CoT模板方法，显著提高了模型的透明性、适应性和任务表现。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/DocThinker_Explainable_Multimodal_Large_Language_Models_with_Rule-based_Reinforcement_Learning_for_Document_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="101-LLM-Driven-Adaptive-6G-Ready-Wireless-Body-Area-Networks-Survey-and-Framework"><a href="#101-LLM-Driven-Adaptive-6G-Ready-Wireless-Body-Area-Networks-Survey-and-Framework" class="headerlink" title="101. LLM-Driven Adaptive 6G-Ready Wireless Body Area Networks: Survey and Framework"></a>101. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/LLM-Driven_Adaptive_6G-Ready_Wireless_Body_Area_Networks__Survey_and_Framework.pdf">LLM-Driven Adaptive 6G-Ready Wireless Body Area Networks: Survey and Framework</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Nebraska–Lincoln</span></p>
<p>本文提出了一种以大语言模型（LLM）为认知控制平面的自适应无线体域网（WBAN）框架，能够实时协调路由、物理层选择、微能量采集和后量子安全等多项关键功能。研究通过系统综述分析现有WBAN架构、路由策略与安全机制，指出当前碎片化、静态化等不足，提出LLM驱动的统一自优化方案，在6G和医疗健康场景下实现高可靠、低功耗及量子安全的数据传输。结论认为该方法可显著提升WBAN的智能性、自适应性和安全性，为下一代移动健康系统奠定基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/LLM-Driven_Adaptive_6G-Ready_Wireless_Body_Area_Networks_Survey_and_Framework.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="102-In-Context-Learning-as-Nonparametric-Conditional-Probability-Estimation-Risk-Bounds-and-Optimality"><a href="#102-In-Context-Learning-as-Nonparametric-Conditional-Probability-Estimation-Risk-Bounds-and-Optimality" class="headerlink" title="102. In-Context Learning as Nonparametric Conditional Probability Estimation: Risk Bounds and Optimality"></a>102. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/In-Context_Learning_as_Nonparametric_Conditional_Probability_Estimation__Risk_Bounds_and_Optimality.pdf">In-Context Learning as Nonparametric Conditional Probability Estimation: Risk Bounds and Optimality</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Department of Statistics, Beijing Normal University at Zhuhai</span></p>
<p>本文将In-Context Learning（ICL）建模为非参数条件概率估计问题，提出了一种基于截断Kullback-Leibler (KL) 散度的期望超额风险度量，并针对多分类任务推导了Transformer模型下ICL的精确上、下界。方法上，作者提出新颖的oracle不等式，利用对数似然函数类的统一经验覆盖熵控制泛化误差，并证明多层感知机（MLP）在特定假设下同样能以最优速率实现ICL。结论显示，Transformer和MLP均可在多分类ICL任务中达到统计最优的最小极大收敛速率（只差对数因子），并揭示ICL并非Transformer独有，丰富了大模型ICL理论基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/In-Context_Learning_as_Nonparametric_Conditional_Probability_Estimation_Risk_Bounds_and_Optimality.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="103-Aryabhata-An-exam-focused-language-model-for-JEE-Math"><a href="#103-Aryabhata-An-exam-focused-language-model-for-JEE-Math" class="headerlink" title="103. Aryabhata: An exam-focused language model for JEE Math"></a>103. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Aryabhata__An_exam-focused_language_model_for_JEE_Math.pdf">Aryabhata: An exam-focused language model for JEE Math</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">PhysicsWallah</span></p>
<p>Aryabhata 1.0 is a compact 7B parameter language model specialized for mathematical reasoning in Indian JEE exams, built by merging open-source reasoning models and fine-tuned with curriculum learning on verified chain-of-thought traces. Using supervised fine-tuning with rejection sampling and reinforcement learning with verifiable rewards (A2C), Aryabhata achieves state-of-the-art accuracy on JEE and competitive results on MATH&#x2F;GSM8K, with efficient step-by-step reasoning tailored for educational use.</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Aryabhata_An_exam-focused_language_model_for_JEE_Math.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="104-M2LLM-Multi-view-Molecular-Representation-Learning-with-Large-Language-Models"><a href="#104-M2LLM-Multi-view-Molecular-Representation-Learning-with-Large-Language-Models" class="headerlink" title="104. M2LLM: Multi-view Molecular Representation Learning with Large Language Models"></a>104. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/$_text%7BM%7D^%7B2%7D$LLM__Multi-view_Molecular_Representation_Learning_with_Large_Language_Models.pdf">M2LLM: Multi-view Molecular Representation Learning with Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Griffith University</span></p>
<p>该论文提出M2LLM框架，创新性地融合了分子结构视角、任务视角和规则视角，利用大语言模型（LLM）进行分子表征生成和特征提取，并通过动态多视图融合优化表示，提升分子性质预测准确率。实验表明M2LLM在多项分子属性分类和回归任务上超越现有SOTA方法，展示了LLM在分子表示学习领域的巨大潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/M2LLM_Multi-view_Molecular_Representation_Learning_with_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="105-Prompt-and-Check-Using-Large-Language-Models-to-Evaluate-Communication-Protocol-Compliance-in-Simulation-Based-Training"><a href="#105-Prompt-and-Check-Using-Large-Language-Models-to-Evaluate-Communication-Protocol-Compliance-in-Simulation-Based-Training" class="headerlink" title="105. Prompt-and-Check: Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training"></a>105. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Prompt-and-Check__Using_Large_Language_Models_to_Evaluate_Communication_Protocol_Compliance_in_Simul.pdf">Prompt-and-Check: Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Singapore Polytechnic</span></p>
<p>本文提出Prompt-and-Check方法，利用开源大语言模型（如LLaMA 2&#x2F;3、Mistral 7B）结合语境选择和结构化提示，自动评估仿真训练中交流协议的合规性。实验证明，该方法在本地硬件上无需特定微调即可实现高准确率和合理解释，适用于海事等安全关键领域的自动化评估。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Prompt-and-Check_Using_Large_Language_Models_to_Evaluate_Communication_Protocol_Compliance_in_Simulation-Based_Training.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="106-MIGRATE-Mixed-Policy-GRPO-for-Adaptation-at-Test-Time"><a href="#106-MIGRATE-Mixed-Policy-GRPO-for-Adaptation-at-Test-Time" class="headerlink" title="106. MIGRATE: Mixed-Policy GRPO for Adaptation at Test-Time"></a>106. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/MiGrATe__Mixed-Policy_GRPO_for_Adaptation_at_Test-Time.pdf">MIGRATE: Mixed-Policy GRPO for Adaptation at Test-Time</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Massachusetts Amherst</span></p>
<p>该论文提出MIGRATE方法，通过将Group Relative Policy Optimization（GRPO）与混合策略组（包括on-policy采样、贪心采样和邻域采样）结合，实现了无需外部训练数据的大模型在线测试时自适应搜索优化。实验证明MIGRATE在单词搜索、分子优化和抽象推理（ARC）等三大领域均显著优于现有推理和测试时训练基线，展示出在无监督复杂搜索任务中的卓越性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/MIGRATE_Mixed-Policy_GRPO_for_Adaptation_at_Test-Time.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="107-Securing-Educational-LLMs-A-Generalised-Taxonomy-of-Attacks-on-LLMs-and-DREAD-Risk-Assessment"><a href="#107-Securing-Educational-LLMs-A-Generalised-Taxonomy-of-Attacks-on-LLMs-and-DREAD-Risk-Assessment" class="headerlink" title="107. Securing Educational LLMs: A Generalised Taxonomy of Attacks on LLMs and DREAD Risk Assessment"></a>107. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Securing_Educational_LLMs__A_Generalised_Taxonomy_of_Attacks_on_LLMs_and_DREAD_Risk_Assessment.pdf">Securing Educational LLMs: A Generalised Taxonomy of Attacks on LLMs and DREAD Risk Assessment</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Waikato</span></p>
<p>本文系统梳理了大型语言模型（LLM）在教育领域应用时面临的安全威胁，提出了一套基于攻击复杂度的通用攻击分类法，并结合DREAD风险评估模型，量化分析了50种针对LLM及其基础设施的攻击手法在教育场景下的危害性。研究发现，令牌走私、对抗性提示、直接注入和多步越狱等攻击为关键高危威胁，并提出了风险缓解建议，结论对学术和产业界构建教育安全LLM系统具有指导意义。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Securing_Educational_LLMs_A_Generalised_Taxonomy_of_Attacks_on_LLMs_and_DREAD_Risk_Assessment.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="108-Generative-AI-for-Critical-Infrastructure-in-Smart-Grids-A-Unified-Framework-for-Synthetic-Data-Generation-and-Anomaly-Detection"><a href="#108-Generative-AI-for-Critical-Infrastructure-in-Smart-Grids-A-Unified-Framework-for-Synthetic-Data-Generation-and-Anomaly-Detection" class="headerlink" title="108. Generative AI for Critical Infrastructure in Smart Grids: A Unified Framework for Synthetic Data Generation and Anomaly Detection"></a>108. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Generative_AI_for_Critical_Infrastructure_in_Smart_Grids__A_Unified_Framework_for_Synthetic_Data_Gen.pdf">Generative AI for Critical Infrastructure in Smart Grids: A Unified Framework for Synthetic Data Generation and Anomaly Detection</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Michigan-Dearborn</span></p>
<p>本论文提出了一种面向数字变电站IEC61850协议GOOSE消息的生成式AI（GenAI）异常检测系统，核心包括AATM（高级对抗性流量变异）技术用于协议合规的合成数据生成，实现了高真实度和良好类别平衡的零日攻击样本，并利用基于任务导向对话（ToD）的GenAI框架进行异常检测。实验表明，所提GenAI-ADS（以Anthropic Claude Pro实现）在准确率、精度、F1等多项指标上明显优于传统机器学习模型且无需频繁重训练，对新型攻击有更强检测能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Generative_AI_for_Critical_Infrastructure_in_Smart_Grids_A_Unified_Framework_for_Synthetic_Data_Generation_and_Anomaly_Detection.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="109-OmniLLP-Enhancing-LLM-based-Log-Level-Prediction-with-Context-Aware-Retrieval"><a href="#109-OmniLLP-Enhancing-LLM-based-Log-Level-Prediction-with-Context-Aware-Retrieval" class="headerlink" title="109. OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval"></a>109. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/OmniLLP__Enhancing_LLM-based_Log_Level_Prediction_with_Context-Aware_Retrieval.pdf">OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Queen’s University</span></p>
<p>OmniLLP提出了一种利用语义和开发者归属聚类的上下文检索框架，通过多层聚类（语义+所有权）为大型语言模型（LLMs）提供更相关的in-context示例，实现了高效的日志级别自动预测。实验结果表明，OmniLLP在四个大型开源项目上将AUC提升至0.88-0.96，显著优于随机与单一聚类方法，提高了预测准确性和实际开发中的可用性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/OmniLLP_Enhancing_LLM-based_Log_Level_Prediction_with_Context-Aware_Retrieval.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="110-Towards-Universal-Neural-Inference"><a href="#110-Towards-Universal-Neural-Inference" class="headerlink" title="110. Towards Universal Neural Inference"></a>110. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Towards_Universal_Neural_Inference.pdf">Towards Universal Neural Inference</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Department of Computer Science, UNC Chapel Hill</span></p>
<p>本文提出ASPIRE（Arbitrary Set-based Permutation-Invariant Reasoning Engine），一种结合集合变换器和语义嵌入的通用神经推理框架，能够在异构结构化数据上进行语义对齐和泛化预测。ASPIRE通过集合不变性结构和自然语言特征描述，实现跨数据集、跨领域的零样本、少样本泛化推断，在多个表格数据基准上取得领先性能，并支持主动特征获取任务。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Towards_Universal_Neural_Inference.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="111-Constrained-free-energy-minimization-for-the-design-of-thermal-states-and-stabilizer-thermodynamic-systems"><a href="#111-Constrained-free-energy-minimization-for-the-design-of-thermal-states-and-stabilizer-thermodynamic-systems" class="headerlink" title="111. Constrained free energy minimization for the design of thermal states and stabilizer thermodynamic systems"></a>111. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Constrained_free_energy_minimization_for_the_design_of_thermal_states_and_stabilizer_thermodynamic_s.pdf">Constrained free energy minimization for the design of thermal states and stabilizer thermodynamic systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">École Polytechnique Fédérale de Lausanne (EPFL)</span></p>
<p>本文系统研究了LMPW25算法（包括一阶&#x2F;二阶经典与量子-经典混合算法）在量子热力学中受约束能量最小化问题上的数值表现，涵盖了量子Heisenberg模型和基于稳定子码的热力学系统。论文提出将这些算法用于可控Hamiltonian的基态和热态设计，并首次将量子纠错中的稳定子码与热力学中的非对易守恒量建立桥梁，实现了在热力学系统中编码量子信息的新方法。结论表明，LMPW25算法在上述系统中均能有效收敛，理论与数值结果一致，并为材料分子设计及量子信息编码提供了新工具。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Constrained_free_energy_minimization_for_the_design_of_thermal_states_and_stabilizer_thermodynamic_systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="112-Fre-CW-Targeted-Attack-on-Time-Series-Forecasting-using-Frequency-Domain-Loss"><a href="#112-Fre-CW-Targeted-Attack-on-Time-Series-Forecasting-using-Frequency-Domain-Loss" class="headerlink" title="112. Fre-CW: Targeted Attack on Time Series Forecasting using Frequency Domain Loss"></a>112. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Fre-CW__Targeted_Attack_on_Time_Series_Forecasting_using_Frequency_Domain_Loss.pdf">Fre-CW: Targeted Attack on Time Series Forecasting using Frequency Domain Loss</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiaotong University</span></p>
<p>本文提出了一种基于频域损失的时间序列预测目标攻击方法Fre-CW，通过改进C&amp;W攻击算法，将其引入时间序列预测领域，并融合时间域与频域损失优化对抗样本。实验结果表明，Fre-CW在多个数据集和主流预测模型上均显著提升了目标攻击的有效性和隐蔽性，验证了频域信息在提升攻击性能中的重要作用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Fre-CW_Targeted_Attack_on_Time_Series_Forecasting_using_Frequency_Domain_Loss.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="113-FetFIDS-A-Feature-Embedding-Attention-based-Federated-Network-Intrusion-Detection-Algorithm"><a href="#113-FetFIDS-A-Feature-Embedding-Attention-based-Federated-Network-Intrusion-Detection-Algorithm" class="headerlink" title="113. FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm"></a>113. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/FetFIDS__A_Feature_Embedding_Attention_based_Federated_Network_Intrusion_Detection_Algorithm.pdf">FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Purdue University</span></p>
<p>该论文提出了一种特征嵌入与顺序多头注意力机制结合的Transformer模型，并在联邦学习环境下提升网络入侵检测性能。方法采用CNN特征嵌入替代传统位置嵌入，并通过多轮模型聚合与节点本地训练实现隐私保护和性能提升。实验结果表明，该方法在准确率、精确率、召回率及F1分数上均优于当前主流入侵检测算法，且模型稳定性强，适合实际部署。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/FetFIDS_A_Feature_Embedding_Attention_based_Federated_Network_Intrusion_Detection_Algorithm.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="114-Wavelet-Mixture-of-Experts-for-Time-Series-Forecasting"><a href="#114-Wavelet-Mixture-of-Experts-for-Time-Series-Forecasting" class="headerlink" title="114. Wavelet Mixture of Experts for Time Series Forecasting"></a>114. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Wavelet_Mixture_of_Experts_for_Time_Series_Forecasting.pdf">Wavelet Mixture of Experts for Time Series Forecasting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai University of Engineering Science</span></p>
<p>该论文提出了WaveTS系列时间序列预测模型，结合离散小波变换与MLP以分解时序数据的高低频特征，并通过MoE（Mixture of Experts）通道聚类策略高效处理多通道依赖。实验结果表明，WaveTS系列在八个公开数据集上以更少参数实现了SOTA预测性能，尤其在多通道数据集上表现突出，提升了准确率和计算效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Wavelet_Mixture_of_Experts_for_Time_Series_Forecasting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="115-Differentiated-Information-Mining-A-Semi-supervised-Learning-Framework-for-GNNs"><a href="#115-Differentiated-Information-Mining-A-Semi-supervised-Learning-Framework-for-GNNs" class="headerlink" title="115. Differentiated Information Mining: A Semi-supervised Learning Framework for GNNs"></a>115. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Differentiated_Information_Mining__A_Semi-supervised_Learning_Framework_for_GNNs.pdf">Differentiated Information Mining: A Semi-supervised Learning Framework for GNNs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了DiFac框架，通过从单一信息源中挖掘多种区分性决策因子，并利用一致性过滤和最小置信度排名策略，在半监督场景下提升图神经网络（GNN）泛化能力和鲁棒性。同时引入大规模多模态模型生成的辅助文本知识作为因子，并设计问责评分机制以抑制噪声。实验表明，该方法在低标注率和高噪声条件下显著优于主流基线，尤其在特征维度较高的数据集上效果突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Differentiated_Information_Mining_A_Semi-supervised_Learning_Framework_for_GNNs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="116-Elucidating-Rectified-Flow-with-Deterministic-Sampler-Polynomial-Discretization-Complexity-for-Multi-and-One-step-Models"><a href="#116-Elucidating-Rectified-Flow-with-Deterministic-Sampler-Polynomial-Discretization-Complexity-for-Multi-and-One-step-Models" class="headerlink" title="116. Elucidating Rectified Flow with Deterministic Sampler: Polynomial Discretization Complexity for Multi and One-step Models"></a>116. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Elucidating_Rectified_Flow_with_Deterministic_Sampler__Polynomial_Discretization_Complexity_for_Mult.pdf">Elucidating Rectified Flow with Deterministic Sampler: Polynomial Discretization Complexity for Multi and One-step Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了对Rectified Flow (RF) 基于确定性采样器的多步和一步生成模型的离散化复杂性理论分析，首次在真实的有界支持假设下证明了RF模型可达多项式级别的离散化复杂度。方法上结合了预测-校正框架和Langevin过程作为校正器，并提出了向量扰动引理，理论上优于主流扩散模型。结果显示，无论在多步还是一步生成场景，RF模型在采样复杂性上都优于VP、VE等扩散模型，为RF模型优异的经验表现提供了理论解释。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Elucidating_Rectified_Flow_with_Deterministic_Sampler_Polynomial_Discretization_Complexity_for_Multi_and_One-step_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="117-Diminution-On-Reducing-the-Size-of-Grounding-ASP-Programs"><a href="#117-Diminution-On-Reducing-the-Size-of-Grounding-ASP-Programs" class="headerlink" title="117. Diminution: On Reducing the Size of Grounding ASP Programs"></a>117. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-13/Diminution__On_Reducing_the_Size_of_Grounding_ASP_Programs.pdf">Diminution: On Reducing the Size of Grounding ASP Programs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China (USTC)</span></p>
<p>本文提出了“diminution”概念，即在ASP程序归约前选取Herbrand宇宙的子集以生成更小的ground程序，并形式化定义了diminution、分析了其性质和复杂性，并提出基于域谓词的实现方式。实验表明，该方法平均可减少高达70%的grounding时间、85%的ground文件大小，同时保持原程序解的可扩展性，有效缓解了ASP中的grounding瓶颈。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-14/Diminution_On_Reducing_the_Size_of_Grounding_ASP_Programs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/14报纸</div>
      <div>http://ywfhhh.github.io/2025/08/2025-08-14_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>具身ADT</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月14日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/2025-08-15_article/" title="2025/08/15报纸">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">2025/08/15报纸</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-13_article/" title="2025/08/13报纸">
                        <span class="hidden-mobile">2025/08/13报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<!-- hexo injector body_end start -->
<script>
  function download(url) {
    // 检查 URL 是否有效
    if (!url) {
      console.error('下载 URL 无效');
      return;
    }

    // 创建一个隐藏隐藏的 <a> 元素用于下载
    const a = document.createElement('a');
    a.href = url;
    a.download = url.split('/').pop(); // 设置下载的文件名（默认为 URL 的最后一部分）
    document.body.appendChild(a); // 添加到 DOM
    a.click(); // 触发点击事件
    document.body.removeChild(a); // 下载完成后移除 <a> 元素
  }
</script>
<!-- hexo injector body_end end --></body>
</html>
