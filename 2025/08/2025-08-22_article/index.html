

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="具身ADT">
  <meta name="keywords" content="">
  
    <meta name="description" content="每日最新论文分享">
<meta property="og:type" content="article">
<meta property="og:title" content="2025&#x2F;08&#x2F;22报纸">
<meta property="og:url" content="http://ywfhhh.github.io/2025/08/2025-08-22_article/index.html">
<meta property="og:site_name" content="最新前沿论文分享">
<meta property="og:description" content="每日最新论文分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Compute-Optimal_Scaling_for_Value-Based_Deep_RL.jpg">
<meta property="article:published_time" content="2025-08-22T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-22T04:24:44.834Z">
<meta property="article:author" content="具身ADT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Compute-Optimal_Scaling_for_Value-Based_Deep_RL.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>2025/08/22报纸 - 最新前沿论文分享</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ywfhhh.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Anker Innovations</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>论文库</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/banner.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2025/08/22报纸"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-22 00:00" pubdate>
          星期五, 八月 22日 2025
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          15k 字
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">2025/08/22报纸</h1>
            
            
              <div class="markdown-body">
                
                <center><h1>2025/08/22报纸</h1></center>

<p>📅 <strong>日期</strong>：2025-08-22<br>📄 <strong>发现论文数量</strong>：98  </p>
<h2 id="rl">Reinforcement Learning</h2>


<h3 id="1-Compute-Optimal-Scaling-for-Value-Based-Deep-RL"><a href="#1-Compute-Optimal-Scaling-for-Value-Based-Deep-RL" class="headerlink" title="1. Compute-Optimal Scaling for Value-Based Deep RL"></a>1. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Compute-Optimal_Scaling_for_Value-Based_Deep_RL.pdf">Compute-Optimal Scaling for Value-Based Deep RL</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Berkeley</span></p>
<p>本论文系统研究了在给定计算预算下，如何在在线、基于价值（value-based）的深度强化学习中合理分配模型容量与update-to-data (UTD) 比例，以最大化采样效率。通过大量实证分析，作者提出并解释了TD-overfitting现象，量化了模型容量、UTD比和batch size之间的关系，给出了可预测的计算资源分配法则，并为实际使用者提供了优化采样效率和计算效率的指导规则。结论显示，合理分配模型容量与UTD比可显著提升数据与计算利用率，并且大模型能有效缓解TD-overfitting、支持更大batch size。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Compute-Optimal_Scaling_for_Value-Based_Deep_RL.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="2-HERAKLES-Hierarchical-Skill-Compilation-for-Open-ended-LLM-Agents"><a href="#2-HERAKLES-Hierarchical-Skill-Compilation-for-Open-ended-LLM-Agents" class="headerlink" title="2. HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents"></a>2. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/HERAKLES__Hierarchical_Skill_Compilation_for_Open-ended_LLM_Agents.pdf">HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Inria(Flowers)</span></p>
<p>HERAKLES提出了一种层级强化学习框架，利用大语言模型（LLM）作为高层策略进行目标分解，并将已掌握的复杂目标编译为低层策略的可复用技能。通过在Crafter环境的实验，HERAKLES展现了随目标难度线性扩展、显著提升样本效率，并能强泛化到新目标的能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/HERAKLES_Hierarchical_Skill_Compilation_for_Open-ended_LLM_Agents.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="3-Beyond-ReLU-Chebyshev-DQN-for-Enhanced-Deep-Q-Networks"><a href="#3-Beyond-ReLU-Chebyshev-DQN-for-Enhanced-Deep-Q-Networks" class="headerlink" title="3. Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks"></a>3. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Beyond_ReLU__Chebyshev-DQN_for_Enhanced_Deep_Q-Networks.pdf">Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">K. N. Toosi University of Technology</span></p>
<p>本文提出了一种创新的Chebyshev-DQN（Ch-DQN）架构，将Chebyshev多项式基底集成到DQN框架中以提升特征表达能力，通过在输入层进行正交多项式特征变换，改善了Q函数的近似效果。实验证明，Ch-DQN在CartPole、MountainCar和Acrobot等三个基准环境中表现优异，尤其在复杂任务上显著提高了样本效率和最终性能，验证了Chebyshev基底在深度强化学习中的有效性与理论优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Beyond_ReLU_Chebyshev-DQN_for_Enhanced_Deep_Q-Networks.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="4-A-Comparative-Evaluation-of-Teacher-Guided-Reinforcement-Learning-Techniques-for-Autonomous-Cyber-Operations"><a href="#4-A-Comparative-Evaluation-of-Teacher-Guided-Reinforcement-Learning-Techniques-for-Autonomous-Cyber-Operations" class="headerlink" title="4. A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations"></a>4. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/A_Comparative_Evaluation_of_Teacher-Guided_Reinforcement_Learning_Techniques_for_Autonomous_Cyber_Op.pdf">A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Royal Military College of Canada</span></p>
<p>本文在CybORG模拟环境下实现并比较了四种教师引导的强化学习（RL）方法，包括奖励塑形、动作掩码、辅助损失和特征空间修改，均利用预训练RL代理作为教师。实验表明，辅助损失和动作掩码在提升早期训练表现和加快收敛速度方面最有效，而奖励塑形和特征空间修改效果不明显，说明教师引导RL可显著提升自动化网络安全操作中的训练效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/A_Comparative_Evaluation_of_Teacher-Guided_Reinforcement_Learning_Techniques_for_Autonomous_Cyber_Operations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="robot">Robotics</h2>


<h3 id="5-Virtual-Community-An-Open-World-for-Humans-Robots-and-Society"><a href="#5-Virtual-Community-An-Open-World-for-Humans-Robots-and-Society" class="headerlink" title="5. Virtual Community: An Open World for Humans, Robots, and Society"></a>5. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Virtual_Community__An_Open_World_for_Humans,_Robots,_and_Society.pdf">Virtual Community: An Open World for Humans, Robots, and Society</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">UMass Amherst</span></p>
<p>本文提出了Virtual Community，一个基于通用物理引擎的开放世界人-机-社会模拟平台，能够自动将真实地理空间数据转换为大规模3D场景与嵌入丰富社会关系的多智能体社区，并统一支持人类虚拟体与多类型机器人协作、互动与物理仿真。平台提出了Community Planning与Community Robot两大多智能体开放世界任务，并通过多种基线方法系统评测，展示了当前多智能体高层规划与低层物理协作的挑战性。结论认为该平台为研究人机共存与具身通用智能提供了新方向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Virtual_Community_An_Open_World_for_Humans_Robots_and_Society.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="6-Multiscale-Video-Transformers-for-Class-Agnostic-Segmentation-in-Autonomous-Driving"><a href="#6-Multiscale-Video-Transformers-for-Class-Agnostic-Segmentation-in-Autonomous-Driving" class="headerlink" title="6. Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving"></a>6. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Multiscale_Video_Transformers_for_Class_Agnostic_Segmentation_in_Autonomous_Driving.pdf">Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Ontario Tech University</span></p>
<p>该论文提出了一种多阶段多尺度查询-记忆视频Transformer，用于自动驾驶场景下的类别无关视频分割，仅利用运动线索无需光流输入。方法包含多尺度特征提取、查询-记忆解码器和随机Token Drop机制以提升效率和精度，能在保持高分辨率时减少计算和内存开销。实验表明该方法在DAVIS’16、KITTI和Cityscapes等数据集上显著优于传统多尺度解码器，且具有实时性，适用于安全关键型机器人应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Multiscale_Video_Transformers_for_Class_Agnostic_Segmentation_in_Autonomous_Driving.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="7-EAROL-Environmental-Augmented-Perception-Aware-Planning-and-Robust-Odometry-via-Downward-Mounted-Tilted-LiDAR"><a href="#7-EAROL-Environmental-Augmented-Perception-Aware-Planning-and-Robust-Odometry-via-Downward-Mounted-Tilted-LiDAR" class="headerlink" title="7. EAROL: Environmental Augmented Perception-Aware Planning and Robust Odometry via Downward-Mounted Tilted LiDAR"></a>7. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/EAROL__Environmental_Augmented_Perception-Aware_Planning_and_Robust_Odometry_via_Downward-Mounted_Ti.pdf">EAROL: Environmental Augmented Perception-Aware Planning and Robust Odometry via Downward-Mounted Tilted LiDAR</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Institute of Technology</span></p>
<p>该论文提出了一种创新的无人机系统EAROL，通过将LiDAR倾斜向下安装，结合基于IESKF的紧耦合LiDAR-IMU里程计和分层轨迹-偏航优化算法，实现了在无顶部特征的极端环境中高精度6自由度定位与高效动态目标跟踪。实验证明，该系统在室内外场景中可将跟踪误差降低81%，提升22%环境感知覆盖，并几乎消除垂直漂移，为无人机灾后搜索救援与自主导航提供了强健解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/EAROL_Environmental_Augmented_Perception-Aware_Planning_and_Robust_Odometry_via_Downward-Mounted_Tilted_LiDAR.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="8-Adversarial-Generation-and-Collaborative-Evolution-of-Safety-Critical-Scenarios-for-Autonomous-Vehicles"><a href="#8-Adversarial-Generation-and-Collaborative-Evolution-of-Safety-Critical-Scenarios-for-Autonomous-Vehicles" class="headerlink" title="8. Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles"></a>8. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Adversarial_Generation_and_Collaborative_Evolution_of_Safety-Critical_Scenarios_for_Autonomous_Vehic.pdf">Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本文提出SCENGE框架，通过结合知识检索增强的大语言模型（LLM）推理与多代理轨迹优化，自动生成安全关键场景用于仿真测试自动驾驶系统。方法包括Meta-Scenario Generation阶段利用LLM和结构化驾驶知识推理出可执行的对抗场景，随后Complex Scenario Evolution通过协同优化背景车辆轨迹，最大化场景复杂性与风险。实验结果显示SCENGE在多种强化学习自动驾驶模型上能发现更多严重碰撞案例，并提升模型鲁棒性，且可迁移到不同仿真平台和真实车辆测试，表明其生成场景真实且具关键风险。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Adversarial_Generation_and_Collaborative_Evolution_of_Safety-Critical_Scenarios_for_Autonomous_Vehicles.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="9-Action-Constrained-Imitation-Learning"><a href="#9-Action-Constrained-Imitation-Learning" class="headerlink" title="9. Action-Constrained Imitation Learning"></a>9. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Action-Constrained_Imitation_Learning.pdf">Action-Constrained Imitation Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">National Yang Ming Chiao Tung University</span></p>
<p>本文提出了Action-Constrained Imitation Learning (ACIL)问题，并针对由于模仿者受限于动作约束而导致的occupancy measure失配，提出了基于动态时间规整（DTW）距离的动态时间规整模仿学习（DTWIL）方法。该方法通过模型预测控制（MPC）将专家演示轨迹与受限动作下的代理轨迹对齐，生成代理可行的仿真演示数据，并结合下游仿真学习方法进行策略学习。实验结果表明，DTWIL能有效提升机器人控制等多任务下的模仿性能和样本效率，优于多种基线算法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Action-Constrained_Imitation_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="10-Taming-VR-Teleoperation-and-Learning-from-Demonstration-for-Multi-Task-Bimanual-Table-Service-Manipulation"><a href="#10-Taming-VR-Teleoperation-and-Learning-from-Demonstration-for-Multi-Task-Bimanual-Table-Service-Manipulation" class="headerlink" title="10. Taming VR Teleoperation and Learning from Demonstration for Multi-Task Bimanual Table Service Manipulation"></a>10. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Taming_VR_Teleoperation_and_Learning_from_Demonstration_for_Multi-Task_Bimanual_Table_Service_Manipu.pdf">Taming VR Teleoperation and Learning from Demonstration for Multi-Task Bimanual Table Service Manipulation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出了一种结合VR远程遥操作与模仿学习的方法，用于多任务双臂餐桌服务机器人操作，包括桌布展开、披萨放置和食品盒开合。方法采用VR设备采集人类操作演示，通过ACT算法进行策略学习，实现高效且可靠的任务完成。实验结果表明，该混合策略在ICRA 2025 WBCD竞赛中获得冠军，兼顾了效率和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Taming_VR_Teleoperation_and_Learning_from_Demonstration_for_Multi-Task_Bimanual_Table_Service_Manipulation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="11-Great-GATsBi-Hybrid-Multimodal-Trajectory-Forecasting-for-Bicycles-using-Anticipation-Mechanism"><a href="#11-Great-GATsBi-Hybrid-Multimodal-Trajectory-Forecasting-for-Bicycles-using-Anticipation-Mechanism" class="headerlink" title="11. Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism"></a>11. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Great_GATsBi__Hybrid,_Multimodal,_Trajectory_Forecasting_for_Bicycles_using_Anticipation_Mechanism.pdf">Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ETH Zürich</span></p>
<p>该论文提出了Great GATsBi，一种用于自行车轨迹预测的混合多模态框架，融合了基于物理建模（如车辆运动学和卡尔曼滤波）与基于社会建模（如图注意网络GAT，邻居轨迹感知与衰减机制），实现了对自行车运动中物理和社会交互的系统性建模。实验表明，该模型在短期和长期预测均优于现有方法，显著降低预测偏差和不确定性，提升了自主驾驶和交通安全相关应用的轨迹预测能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Great_GATsBi_Hybrid_Multimodal_Trajectory_Forecasting_for_Bicycles_using_Anticipation_Mechanism.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="12-FBI-Learning-Dexterous-In-hand-Manipulation-with-Dynamic-Visuotactile-Shortcut-Policy"><a href="#12-FBI-Learning-Dexterous-In-hand-Manipulation-with-Dynamic-Visuotactile-Shortcut-Policy" class="headerlink" title="12. FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy"></a>12. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/FBI__Learning_Dexterous_In-hand_Manipulation_with_Dynamic_Visuotactile_Shortcut_Policy.pdf">FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出了一种新颖的动态视触觉融合模仿学习方法——Flow Before Imitation (FBI)，通过动态建模触觉与物体运动的因果关系，将时序点云的运动流与密集触觉特征结合，并利用Transformer融合模块生成一步扩散策略，实现了无需或可选物理触觉传感器的灵巧手内操作。实验显示，在模拟和真实环境的五项任务中，FBI均显著超越现有基线，提升操作成功率，特别在复杂重定位任务中表现突出，且具备鲁棒性和高实时性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/FBI_Learning_Dexterous_In-hand_Manipulation_with_Dynamic_Visuotactile_Shortcut_Policy.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="13-DEXTER-LLM-Dynamic-and-Explainable-Coordination-of-Multi-Robot-Systems-in-Unknown-Environments-via-Large-Language-Models"><a href="#13-DEXTER-LLM-Dynamic-and-Explainable-Coordination-of-Multi-Robot-Systems-in-Unknown-Environments-via-Large-Language-Models" class="headerlink" title="13. DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models"></a>13. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/DEXTER-LLM__Dynamic_and_Explainable_Coordination_of_Multi-Robot_Systems_in_Unknown_Environments_via_.pdf">DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出DEXTER-LLM框架，实现了在未知环境下多机器人系统的动态任务分解与协调。方法包括四大模块：任务理解（自然语言&#x2F;LTL转任务DAG）、基于LLM的多阶段子任务生成、基于优化的子任务分配与调度，以及多级在线自适应与人类验证。实验显示，该方法在动态场景下实现了100%任务完成率，任务效率和计划质量大幅优于现有方法，并有效减少了LLM调用次数。结论：框架在开放环境下具备强泛化性、自适应性和可解释性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/DEXTER-LLM_Dynamic_and_Explainable_Coordination_of_Multi-Robot_Systems_in_Unknown_Environments_via_Large_Language_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="14-D²-LIO-Enhanced-Optimization-for-LiDAR-IMU-Odometry-Considering-Directional-Degeneracy"><a href="#14-D²-LIO-Enhanced-Optimization-for-LiDAR-IMU-Odometry-Considering-Directional-Degeneracy" class="headerlink" title="14. D²-LIO: Enhanced Optimization for LiDAR-IMU Odometry Considering Directional Degeneracy"></a>14. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/D$^2$-LIO__Enhanced_Optimization_for_LiDAR-IMU_Odometry_Considering_Directional_Degeneracy.pdf">D²-LIO: Enhanced Optimization for LiDAR-IMU Odometry Considering Directional Degeneracy</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>该论文提出了一种增强型LiDAR-IMU里程计(D²-LIO)优化框架，通过自适应逐点异常值剔除和IMU-点云协同加权，实现对方向性退化的鲁棒定位。实验表明，该方法在复杂和退化环境下显著提升了定位精度和鲁棒性，优于主流SLAM方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/D%C2%B2-LIO_Enhanced_Optimization_for_LiDAR-IMU_Odometry_Considering_Directional_Degeneracy.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="15-MoVieDrive-Multi-Modal-Multi-View-Urban-Scene-Video-Generation"><a href="#15-MoVieDrive-Multi-Modal-Multi-View-Urban-Scene-Video-Generation" class="headerlink" title="15. MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation"></a>15. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/MoVieDrive__Multi-Modal_Multi-View_Urban_Scene_Video_Generation.pdf">MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huawei Noah’s Ark Lab</span></p>
<p>MoVieDrive提出了一种用于自动驾驶的多模态多视角城市场景视频生成方法，通过设计统一的扩散Transformer模型，融合模态共享和模态特定组件，实现RGB、深度和语义等多模态视频在统一框架下的生成。实验结果表明，该方法在nuScenes数据集上生成的视频在保真度、可控性及多模态一致性方面均优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/MoVieDrive_Multi-Modal_Multi-View_Urban_Scene_Video_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="cv">Computer Vision</h2>


<h3 id="16-Locality-aware-Concept-Bottleneck-Model"><a href="#16-Locality-aware-Concept-Bottleneck-Model" class="headerlink" title="16. Locality-aware Concept Bottleneck Model"></a>16. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Locality-aware_Concept_Bottleneck_Model.pdf">Locality-aware Concept Bottleneck Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Seoul National University</span></p>
<p>本文提出了一种新型的Locality-aware Concept Bottleneck Model (LCBM) 框架，通过结合原型学习和CLIP等基础模型，提升了无标签概念瓶颈模型在图像中概念定位的准确性。该方法为每个概念分配原型，并通过辅助分类任务及与CLIP的相似度约束，使原型与局部区域高度对齐，实验结果显示LCBM在多个数据集上提升了概念定位能力且保持了分类性能。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Locality-aware_Concept_Bottleneck_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="17-Snap-Snap-Taking-Two-Images-to-Reconstruct-3D-Human-Gaussians-in-Milliseconds"><a href="#17-Snap-Snap-Taking-Two-Images-to-Reconstruct-3D-Human-Gaussians-in-Milliseconds" class="headerlink" title="17. Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds"></a>17. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Snap-Snap__Taking_Two_Images_to_Reconstruct_3D_Human_Gaussians_in_Milliseconds.pdf">Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Huazhong University of Science and Technology</span></p>
<p>本文提出了一种名为Snap-Snap的前馈式3D人体重建框架，仅需两张RGB图像（正面和背面）即可在190毫秒内直接预测3D人体高斯点云。方法包括重设计的几何点云预测模型、侧视点云及颜色增强算法，并利用高斯属性回归获得完整可渲染的人体高斯点集合。实验显示该方法在THuman2.0和跨域数据集上达到SOTA性能，并支持低成本设备采集。结论：相比现有方法，Snap-Snap在推理速度、简单性和重建质量上具有显著优势。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Snap-Snap_Taking_Two_Images_to_Reconstruct_3D_Human_Gaussians_in_Milliseconds.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="18-Squeezed-Diffusion-Models"><a href="#18-Squeezed-Diffusion-Models" class="headerlink" title="18. Squeezed Diffusion Models"></a>18. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Squeezed_Diffusion_Models.pdf">Squeezed Diffusion Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Stanford University</span></p>
<p>该论文提出了Squeezed Diffusion Models（SDM），通过在扩散模型训练时基于主成分分析对噪声进行各向异性缩放（即挤压&#x2F;反挤压），并设计了Heisenberg扩散模型和标准SDM两种方案。实验显示，在CIFAR-10&#x2F;100和CelebA-64等图像数据集上，适度反挤压（即主轴方向增加噪声方差）可显著提升生成图像的FID和召回率，表明基于数据结构调整噪声可提升扩散模型的生成质量。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Squeezed_Diffusion_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="19-TransLight-Image-Guided-Customized-Lighting-Control-with-Generative-Decoupling"><a href="#19-TransLight-Image-Guided-Customized-Lighting-Control-with-Generative-Decoupling" class="headerlink" title="19. TransLight: Image-Guided Customized Lighting Control with Generative Decoupling"></a>19. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/TransLight__Image-Guided_Customized_Lighting_Control_with_Generative_Decoupling.pdf">TransLight: Image-Guided Customized Lighting Control with Generative Decoupling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of EIC, Huazhong University of Science and Technology</span></p>
<p>本文提出TransLight，一种首创的图像引导光效转移方法，通过创新的生成式解耦策略，利用两个微调扩散模型，将参考图像中的内容与光效分离，构建百万级图像-内容-光效三元组数据集，并以此训练TransLight。该方法实现了高保真、高自由度的光效转移，支持位置、方向、强度等属性灵活控制，实验结果在光效转移和图像内容保持方面均优于现有技术。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/TransLight_Image-Guided_Customized_Lighting_Control_with_Generative_Decoupling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="20-DINOv3-with-Test-Time-Training-for-Medical-Image-Registration"><a href="#20-DINOv3-with-Test-Time-Training-for-Medical-Image-Registration" class="headerlink" title="20. DINOv3 with Test-Time Training for Medical Image Registration"></a>20. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/DINOv3_with_Test-Time_Training_for_Medical_Image_Registration.pdf">DINOv3 with Test-Time Training for Medical Image Registration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Emory University</span></p>
<p>该论文提出了一个无需训练的医学图像配准方法，利用冻结的DINOv3视觉编码器进行切片特征提取，通过PCA降维在特征空间直接优化变形场，仅在测试时进行两阶段配准。实验结果在腹部MR-CT和心脏MRI数据集上均取得了配准精度和变形规则性的新高，表明该方法无需监督即可适用于多模态医学图像临床应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/DINOv3_with_Test-Time_Training_for_Medical_Image_Registration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="21-Fusing-Monocular-RGB-Images-with-AIS-Data-to-Create-a-6D-Pose-Estimation-Dataset-for-Marine-Vessels"><a href="#21-Fusing-Monocular-RGB-Images-with-AIS-Data-to-Create-a-6D-Pose-Estimation-Dataset-for-Marine-Vessels" class="headerlink" title="21. Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels"></a>21. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Fusing_Monocular_RGB_Images_with_AIS_Data_to_Create_a_6D_Pose_Estimation_Dataset_for_Marine_Vessels.pdf">Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Hamburg</span></p>
<p>该论文提出了一种自动化的6D姿态估计算法，通过融合单目RGB图像中的YOLOX-X目标检测结果与AIS（自动识别系统）数据，实现船舶的3D包围盒和6D姿态标注。核心流程包括目标检测、PnP坐标变换、AIS与检测结果的二分图匹配、视觉引导校正和3D包围盒生成。实验表明，PnP方法投影误差远低于传统单应性方法，YOLOX-X检测准确率达0.80，最终自动生成的BONK-Pose数据集有86.4%标注准确、94.5%为可用标注。结论认为该方法无需人工标注即可高效生成高质量6D姿态数据集，并为后续方法优化和移动采集等场景提供了方向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Fusing_Monocular_RGB_Images_with_AIS_Data_to_Create_a_6D_Pose_Estimation_Dataset_for_Marine_Vessels.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="22-MISSIONHD-Data-Driven-Refinement-of-Reasoning-Graph-Structure-through-Hyperdimensional-Causal-Path-Encoding-and-Decoding"><a href="#22-MISSIONHD-Data-Driven-Refinement-of-Reasoning-Graph-Structure-through-Hyperdimensional-Causal-Path-Encoding-and-Decoding" class="headerlink" title="22. MISSIONHD: Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding"></a>22. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/MissionHD__Data-Driven_Refinement_of_Reasoning_Graph_Structure_through_Hyperdimensional_Causal_Path_.pdf">MISSIONHD: Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Irvine</span></p>
<p>该论文提出MISSIONHD框架，通过超高维计算（HDC）实现数据驱动的知识图谱结构优化（D-GSR），将大语言模型（LLM）生成的推理图与下游视觉任务（如视频异常检测）数据分布对齐。方法包括高效的推理路径编码-解码流程，利用任务信号指导图结构修正，实验证明在UCF-Crime和XD-Violence等视觉基准上显著提升推理模型性能，验证了其作为预处理步骤的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/MISSIONHD_Data-Driven_Refinement_of_Reasoning_Graph_Structure_through_Hyperdimensional_Causal_Path_Encoding_and_Decoding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="23-GSFix3D-Diffusion-Guided-Repair-of-Novel-Views-in-Gaussian-Splatting"><a href="#23-GSFix3D-Diffusion-Guided-Repair-of-Novel-Views-in-Gaussian-Splatting" class="headerlink" title="23. GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting"></a>23. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/GSFix3D__Diffusion-Guided_Repair_of_Novel_Views_in_Gaussian_Splatting.pdf">GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University of Munich</span></p>
<p>GSFix3D提出了一套针对3D Gaussian Splatting重建场景新视角修复的流程，通过定制微调的GSFixer扩散模型，融合3DGS和mesh渲染信息，并引入随机掩码增强，实现对缺失区域的智能修复和伪影消除。实验表明该方法无需大规模数据，仅少量场景微调即可在室内外真实数据和极端视角下取得领先修复效果，对位姿误差也具备较强鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/GSFix3D_Diffusion-Guided_Repair_of_Novel_Views_in_Gaussian_Splatting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="24-AnchorSync-Global-Consistency-Optimization-for-Long-Video-Editing"><a href="#24-AnchorSync-Global-Consistency-Optimization-for-Long-Video-Editing" class="headerlink" title="24. AnchorSync: Global Consistency Optimization for Long Video Editing"></a>24. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/AnchorSync__Global_Consistency_Optimization_for_Long_Video_Editing.pdf">AnchorSync: Global Consistency Optimization for Long Video Editing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出AnchorSync，一种面向长视频编辑的两阶段扩散模型框架。方法首先通过带有可训练双向注意力机制的扩散模型对稀疏锚帧进行联合编辑，确保全局结构一致性，并通过多模态（边缘与光流）ControlNet引导的视频扩散模型进行锚帧间的插值，保证时序平滑和结构连续。实验表明，该方法在长视频的编辑一致性、帧连续性和视觉质量方面明显优于现有方法，可高效实现高质量的可控长视频编辑。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/AnchorSync_Global_Consistency_Optimization_for_Long_Video_Editing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="25-Reliable-Smoke-Detection-via-Optical-Flow-Guided-Feature-Fusion-and-Transformer-Based-Uncertainty-Modeling"><a href="#25-Reliable-Smoke-Detection-via-Optical-Flow-Guided-Feature-Fusion-and-Transformer-Based-Uncertainty-Modeling" class="headerlink" title="25. Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling"></a>25. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Reliable_Smoke_Detection_via_Optical_Flow-Guided_Feature_Fusion_and_Transformer-Based_Uncertainty_Mo.pdf">Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Maulana Azad National Institute of Technology Bhopal</span></p>
<p>本文提出了一种融合单目图像光流特征与外观信息的新型烟雾检测框架，核心包括基于分数阶变分的光流估计（FCDLe-FOV模型）、高斯混合模型分割和两阶段不确定性感知Swin Transformer（TP-UAST）。方法在光流与视觉特征融合的基础上，结合不确定性建模，提升了检测的准确性和可靠性。实验表明，该方法在多项指标上优于现有方法，实现了高鲁棒性和优良的置信度校准，适用于早期火灾预警等场景。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Reliable_Smoke_Detection_via_Optical_Flow-Guided_Feature_Fusion_and_Transformer-Based_Uncertainty_Modeling.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="26-Towards-Skeletal-and-Signer-Noise-Reduction-in-Sign-Language-Production-via-Quaternion-Based-Pose-Encoding-and-Contrastive-Learning"><a href="#26-Towards-Skeletal-and-Signer-Noise-Reduction-in-Sign-Language-Production-via-Quaternion-Based-Pose-Encoding-and-Contrastive-Learning" class="headerlink" title="26. Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning"></a>26. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Towards_Skeletal_and_Signer_Noise_Reduction_in_Sign_Language_Production_via_Quaternion-Based_Pose_En.pdf">Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Université de Lorraine</span></p>
<p>本文提出在基于Progressive Transformers的手语生成模型中，采用骨骼四元数旋转编码与几何损失函数以提升关节角度表达的准确性与清晰度，并引入基于语义相似度的对比损失（利用词汇重叠或SBERT句子嵌入）以减少无关风格和解剖学特征带来的噪音。实验在Phoenix14T数据集上表明，对比损失可提升16%关键点正确率，结合四元数编码后平均骨角误差降低6%，显示骨骼结构建模与语义对比目标能提升模型泛化与表达一致性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Towards_Skeletal_and_Signer_Noise_Reduction_in_Sign_Language_Production_via_Quaternion-Based_Pose_Encoding_and_Contrastive_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="27-GOGS-High-Fidelity-Geometry-and-Relighting-for-Glossy-Objects-via-Gaussian-Surfels"><a href="#27-GOGS-High-Fidelity-Geometry-and-Relighting-for-Glossy-Objects-via-Gaussian-Surfels" class="headerlink" title="27. GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels"></a>27. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/GOGS__High-Fidelity_Geometry_and_Relighting_for_Glossy_Objects_via_Gaussian_Surfels.pdf">GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chengdu University of Information Technology</span></p>
<p>本文提出了GOGS，一种基于2D高斯表面元的两阶段反向渲染框架，首先通过结合基础模型的几何先验和物理渲染（split-sum近似）实现高精度几何重建，然后利用蒙特卡洛重要性采样和可微2D高斯追踪对材料进行分解，并引入球面mipmap方向编码以补偿高频镜面细节。实验表明该方法在几何重建、材料分离和新光照下的真实感渲染方面均优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/GOGS_High-Fidelity_Geometry_and_Relighting_for_Glossy_Objects_via_Gaussian_Surfels.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="28-TINKER-Diffusion’s-Gift-to-3D—Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization"><a href="#28-TINKER-Diffusion’s-Gift-to-3D—Multi-View-Consistent-Editing-From-Sparse-Inputs-without-Per-Scene-Optimization" class="headerlink" title="28. TINKER: Diffusion’s Gift to 3D—Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization"></a>28. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Tinker__Diffusion's_Gift_to_3D--Multi-View_Consistent_Editing_From_Sparse_Inputs_without_Per-Scene_O.pdf">TINKER: Diffusion’s Gift to 3D—Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Zhejiang University</span></p>
<p>该论文提出TINKER，一种无需针对具体场景优化的高保真3D编辑框架，通过利用预训练扩散模型，实现从极少的视角输入（一到两张图片）即可生成多视角一致的编辑结果。其核心方法包括：1）多视角引用编辑器，实现参考驱动的跨视角一致编辑；2）任意视角到视频合成器，结合视频扩散模型的空间-时间先验，实现高质量场景补全和新视角生成。结论：TINKER显著降低了3D内容创建门槛，在编辑、一视多角合成及渲染质量提升任务上表现优异，推动3D编辑的通用化和零样本能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/TINKER_Diffusion%E2%80%99s_Gift_to_3D%E2%80%94Multi-View_Consistent_Editing_From_Sparse_Inputs_without_Per-Scene_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="29-Adversarial-Hospital-Invariant-Feature-Learning-for-WSI-Patch-Classification"><a href="#29-Adversarial-Hospital-Invariant-Feature-Learning-for-WSI-Patch-Classification" class="headerlink" title="29. Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification"></a>29. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Adversarial_Hospital-Invariant_Feature_Learning_for_WSI_Patch_Classification.pdf">Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Texas at Arlington</span></p>
<p>本文系统研究了病理基础模型在不同医院来源的WSI图像中存在的域偏置问题，提出了一种轻量级对抗训练框架，通过引入可训练的投影头、域分类器和梯度反转层（GRL），在不修改主编码器的情况下，有效抑制了特征中的医院特有信息，同时保持疾病分类性能。实验表明，该方法在多中心病理数据集上显著降低了医院可预测性，提升了模型在未知医院场景下的泛化能力，验证了对抗训练在医疗AI公平性和鲁棒性提升中的有效性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Adversarial_Hospital-Invariant_Feature_Learning_for_WSI_Patch_Classification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="30-Seeing-Further-on-the-Shoulders-of-Giants-Knowledge-Inheritance-for-Vision-Foundation-Models"><a href="#30-Seeing-Further-on-the-Shoulders-of-Giants-Knowledge-Inheritance-for-Vision-Foundation-Models" class="headerlink" title="30. Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models"></a>30. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Seeing_Further_on_the_Shoulders_of_Giants__Knowledge_Inheritance_for_Vision_Foundation_Models.pdf">Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sony AI</span></p>
<p>该论文提出了一种名为知识保持与统一（KPU）的模型驱动训练方法，通过在统一的潜空间内对多个预训练视觉模型（如CLIP、DINOv2、GroundingDINO）进行知识对齐，并采用适配器模块融合不同教师模型的特长与泛化能力，实现对视觉基础模型的高效继承和知识整合。实验表明，该方法无需大量标注数据即可在图像分类、目标检测、语义分割和实例分割等多项视觉任务上超越现有主流数据驱动和模型驱动方法，展现了模型集成和知识迁移的新潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Seeing_Further_on_the_Shoulders_of_Giants_Knowledge_Inheritance_for_Vision_Foundation_Models.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="31-Virtual-Multiplex-Staining-for-Histological-Images-using-a-Marker-wise-Conditioned-Diffusion-Model"><a href="#31-Virtual-Multiplex-Staining-for-Histological-Images-using-a-Marker-wise-Conditioned-Diffusion-Model" class="headerlink" title="31. Virtual Multiplex Staining for Histological Images using a Marker-wise Conditioned Diffusion Model"></a>31. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Virtual_Multiplex_Staining_for_Histological_Images_using_a_Marker-wise_Conditioned_Diffusion_Model.pdf">Virtual Multiplex Staining for Histological Images using a Marker-wise Conditioned Diffusion Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea University</span></p>
<p>该论文提出了一种基于条件扩散模型的虚拟多重染色方法，利用预训练的稳定扩散模型骨架，并通过marker-wise一热嵌入实现多marker联合生成，提高模型在大规模标记类型下的可扩展性与知识共享。通过两阶段训练框架，后续像素级损失微调显著提升颜色对比保真度与推理效率，在两个公开医学数据集上验证了方法可高质量生成最多18种marker类型，性能优于现有方法，推动多重染色虚拟化在病理分析中的应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Virtual_Multiplex_Staining_for_Histological_Images_using_a_Marker-wise_Conditioned_Diffusion_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="32-Controllable-Latent-Space-Augmentation-for-Digital-Pathology"><a href="#32-Controllable-Latent-Space-Augmentation-for-Digital-Pathology" class="headerlink" title="32. Controllable Latent Space Augmentation for Digital Pathology"></a>32. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Controllable_Latent_Space_Augmentation_for_Digital_Pathology.pdf">Controllable Latent Space Augmentation for Digital Pathology</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">MICS, CentraleSupelec – Université Paris-Saclay</span></p>
<p>该论文提出了一种名为HistAug的高效生成模型，用于数字病理学中的WSI（Whole Slide Image）分析，通过在视觉特征的潜在空间中进行可控的数据增强。HistAug采用基于Transformer的架构，能够根据不同的图像变换（如色调、腐蚀等）参数，生成与指定增强一致的patch特征嵌入，大大提升了多实例学习（MIL）任务中的模型性能。结论表明，HistAug相比传统patch级增强和现有扩散模型，不仅加速训练且显著减少内存消耗，在多种器官和任务中均优于现有方法，尤其在低数据量场景下效果更突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Controllable_Latent_Space_Augmentation_for_Digital_Pathology.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="33-Safety-Critical-Learning-for-Long-Tail-Events-The-TUM-Traffic-Accident-Dataset"><a href="#33-Safety-Critical-Learning-for-Long-Tail-Events-The-TUM-Traffic-Accident-Dataset" class="headerlink" title="33. Safety-Critical Learning for Long-Tail Events: The TUM Traffic Accident Dataset"></a>33. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Safety-Critical_Learning_for_Long-Tail_Events__The_TUM_Traffic_Accident_Dataset.pdf">Safety-Critical Learning for Long-Tail Events: The TUM Traffic Accident Dataset</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Technical University of Munich</span></p>
<p>本文提出了TUM Traffic Accident数据集，涵盖真实高速公路事故的多模态数据（包括2D&#x2F;3D标注、轨迹等），并提出了Accid3nD事故检测框架，将基于规则与基于YOLOv8深度学习的方法结合，实现了对交通事故的实时检测与分析。实验结果表明该方法在多种环境下具有较高鲁棒性和准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Safety-Critical_Learning_for_Long-Tail_Events_The_TUM_Traffic_Accident_Dataset.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="34-A-Comprehensive-Review-of-Agricultural-Parcel-and-Boundary-Delineation-from-Remote-Sensing-Images-Recent-Progress-and-Future-Perspectives"><a href="#34-A-Comprehensive-Review-of-Agricultural-Parcel-and-Boundary-Delineation-from-Remote-Sensing-Images-Recent-Progress-and-Future-Perspectives" class="headerlink" title="34. A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives"></a>34. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/A_Comprehensive_Review_of_Agricultural_Parcel_and_Boundary_Delineation_from_Remote_Sensing_Images__R.pdf">A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sun Yat-Sen University</span></p>
<p>本论文系统回顾了农业地块与边界从遥感影像自动提取的研究进展，涵盖传统图像处理、传统机器学习和深度学习三类方法，重点剖析了语义分割、目标检测和Transformer等深度学习技术在农业地块边界提取中的应用流程与优势。结论指出，深度学习方法特别是基于Transformer和大模型的视觉方法已成为主流，极大提升了复杂地形下地块提取的准确性和适应性，并展望了多源数据融合、可解释模型和大规模应用的未来发展方向。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/A_Comprehensive_Review_of_Agricultural_Parcel_and_Boundary_Delineation_from_Remote_Sensing_Images_Recent_Progress_and_Future_Perspectives.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="35-WISE-FUSE-Efficient-Whole-Slide-Image-Encoding-via-Coarse-to-Fine-Patch-Selection-with-VLM-and-LLM-Knowledge-Fusion"><a href="#35-WISE-FUSE-Efficient-Whole-Slide-Image-Encoding-via-Coarse-to-Fine-Patch-Selection-with-VLM-and-LLM-Knowledge-Fusion" class="headerlink" title="35. WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion"></a>35. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/WISE-FUSE__Efficient_Whole_Slide_Image_Encoding_via_Coarse-to-Fine_Patch_Selection_with_VLM_and_LLM_.pdf">WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea University</span></p>
<p>WISE-FUSE提出了一种高效编码病理全切片图像的方法，结合跨尺度视觉提示微调（CS-VPT）和基于大规模视觉语言模型（VLM）及大语言模型（LLM）的知识融合。该方法通过低分辨率与高分辨率补丁的协同选择和语义融合，仅处理诊断相关区域，大幅减少计算成本并保持或提升诊断性能。实验结果显示WISE-FUSE能将WSI编码时间缩短三倍以上，同时在多种任务和数据集上取得与全量处理相当或更优的诊断结果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/WISE-FUSE_Efficient_Whole_Slide_Image_Encoding_via_Coarse-to-Fine_Patch_Selection_with_VLM_and_LLM_Knowledge_Fusion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="36-Deep-Skin-Lesion-Segmentation-with-Transformer-CNN-Fusion-Toward-Intelligent-Skin-Cancer-Analysis"><a href="#36-Deep-Skin-Lesion-Segmentation-with-Transformer-CNN-Fusion-Toward-Intelligent-Skin-Cancer-Analysis" class="headerlink" title="36. Deep Skin Lesion Segmentation with Transformer-CNN Fusion: Toward Intelligent Skin Cancer Analysis"></a>36. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Deep_Skin_Lesion_Segmentation_with_Transformer-CNN_Fusion__Toward_Intelligent_Skin_Cancer_Analysis.pdf">Deep Skin Lesion Segmentation with Transformer-CNN Fusion: Toward Intelligent Skin Cancer Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of the Chinese Academy of Sciences</span></p>
<p>该论文提出一种基于改进TransUNet结构的皮肤病变图像高精度分割方法，将Transformer模块与CNN结合以兼顾全局语义建模与局部纹理边缘特征，并引入边界引导注意机制和多尺度上采样路径提升病变边界定位和分割一致性。实验表明，该方法在mIoU、mDice和mAcc等指标上优于主流模型，具备更强的病变识别准确性和鲁棒性，尤其在复杂场景下实现了更优的边界重建。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Deep_Skin_Lesion_Segmentation_with_Transformer-CNN_Fusion_Toward_Intelligent_Skin_Cancer_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="37-Vivid-VR-Distilling-Concepts-from-Text-to-Video-Diffusion-Transformer-for-Photorealistic-Video-Restoration"><a href="#37-Vivid-VR-Distilling-Concepts-from-Text-to-Video-Diffusion-Transformer-for-Photorealistic-Video-Restoration" class="headerlink" title="37. Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration"></a>37. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Vivid-VR__Distilling_Concepts_from_Text-to-Video_Diffusion_Transformer_for_Photorealistic_Video_Rest.pdf">Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Alibaba Group - Taobao &amp; Tmall Group</span></p>
<p>本文提出了一种基于Diffusion Transformer (DiT) 的生成式视频复原方法Vivid-VR，核心在于引入概念蒸馏训练策略，通过预训练T2V模型合成嵌入文本概念的训练样本，缓解传统可控生成模型微调时的分布漂移问题，并保持纹理真实感与时序一致性。此外，设计了轻量级控制特征投影器和双分支ControlNet连接器以提升控制能力和去除退化伪影。实验显示，Vivid-VR在真实世界、合成和AIGC视频基准上优于现有方法，实现了更高的纹理真实感、视觉生动性和时间一致性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Vivid-VR_Distilling_Concepts_from_Text-to-Video_Diffusion_Transformer_for_Photorealistic_Video_Restoration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="38-Ouroboros-Single-step-Diffusion-Models-for-Cycle-consistent-Forward-and-Inverse-Rendering"><a href="#38-Ouroboros-Single-step-Diffusion-Models-for-Cycle-consistent-Forward-and-Inverse-Rendering" class="headerlink" title="38. Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering"></a>38. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Ouroboros__Single-step_Diffusion_Models_for_Cycle-consistent_Forward_and_Inverse_Rendering.pdf">Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Irvine</span></p>
<p>本文提出Ouroboros框架，利用两个单步扩散模型实现正向和逆向渲染的一致性学习，核心方法包括基于单步扩散模型的高效逆向&#x2F;正向渲染、循环一致性训练机制，以及无训练的视频推理扩展。实验表明，该方法在多种场景下大幅提升了渲染速度与一致性，并在图像和视频分解与合成任务中均优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Ouroboros_Single-step_Diffusion_Models_for_Cycle-consistent_Forward_and_Inverse_Rendering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="39-MUSE-Multi-Subject-Unified-Synthesis-via-Explicit-Layout-Semantic-Expansion"><a href="#39-MUSE-Multi-Subject-Unified-Synthesis-via-Explicit-Layout-Semantic-Expansion" class="headerlink" title="39. MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion"></a>39. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/MUSE__Multi-Subject_Unified_Synthesis_via_Explicit_Layout_Semantic_Expansion.pdf">MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing University of Posts and Telecommunications</span></p>
<p>该论文提出MUSE框架，通过显式布局语义扩展和级联交叉注意力（CCA）机制，将布局约束与文本描述无干扰地统一，提升多主体文本到图像生成的空间与身份控制能力。采用渐进式两阶段训练，先优化布局，再细化主体合成，实现零样本、端到端高精度图像生成，在多主体合成和布局控制任务上均超越现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/MUSE_Multi-Subject_Unified_Synthesis_via_Explicit_Layout_Semantic_Expansion.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="40-HyperDiff-Hypergraph-Guided-Diffusion-Model-for-3D-Human-Pose-Estimation"><a href="#40-HyperDiff-Hypergraph-Guided-Diffusion-Model-for-3D-Human-Pose-Estimation" class="headerlink" title="40. HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose Estimation"></a>40. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/HyperDiff__Hypergraph_Guided_Diffusion_Model_for_3D_Human_Pose_Estimation.pdf">HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose Estimation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Nanjing University of Aeronautics and Astronautics</span></p>
<p>该论文提出HyperDiff框架，将扩散模型与多粒度HyperGCN结合用于单目3D人体姿态估计，通过构建关节点、身体部件和整体身体多尺度超图，增强模型对关节高阶关联的建模能力，有效缓解深度歧义与遮挡问题。实验结果显示该方法在Human3.6M和MPI-INF-3DHP等标准数据集上达到了当前最佳性能，同时可灵活调整计算资源以兼顾效率和精度，适用于实时应用。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/HyperDiff_Hypergraph_Guided_Diffusion_Model_for_3D_Human_Pose_Estimation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="41-Disentanglement-in-T-space-for-Faster-and-Distributed-Training-of-Diffusion-Models-with-Fewer-Latent-states"><a href="#41-Disentanglement-in-T-space-for-Faster-and-Distributed-Training-of-Diffusion-Models-with-Fewer-Latent-states" class="headerlink" title="41. Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states"></a>41. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Disentanglement_in_T-space_for_Faster_and_Distributed_Training_of_Diffusion_Models_with_Fewer_Latent.pdf">Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Amazon</span></p>
<p>本论文提出了一种在扩散模型训练中减少潜变量（latent-states）数量的方法，通过精心设计噪声调度，实现仅用极少潜变量甚至单一潜变量即可匹配传统千级潜变量模型的性能。作者进一步提出“T空间完全解耦”，将多个独立训练的单潜变量模型组合用于推断，实现高质量生成图像，并在ImageNet和大规模文本-图像数据集上验证了方法可显著加速训练（速度提升4-6倍），且支持跨地理分布式和异构硬件训练，最终在一天内达到传统方法五天训练效果。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Disentanglement_in_T-space_for_Faster_and_Distributed_Training_of_Diffusion_Models_with_Fewer_Latent-states.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="42-SATURN-Autoregressive-Image-Generation-Guided-by-Scene-Graphs"><a href="#42-SATURN-Autoregressive-Image-Generation-Guided-by-Scene-Graphs" class="headerlink" title="42. SATURN: Autoregressive Image Generation Guided by Scene Graphs"></a>42. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/SATURN__Autoregressive_Image_Generation_Guided_by_Scene_Graphs.pdf">SATURN: Autoregressive Image Generation Guided by Scene Graphs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science, VNU-HCM</span></p>
<p>本文提出SATURN方法，将场景图（scene graph）结构转化为显著性排序的token序列，通过冻结的CLIP和VQ-VAE模型仅微调VAR Transformer，实现结构化信息对自回归图像生成的高效引导。实验表明，SATURN在Visual Genome数据集上FID从56.45降到21.62，IS从16.03升到24.78，显著优于SG2IM与SGDiff，同时准确性和可编辑性提升，无需额外模块或多阶段训练。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/SATURN_Autoregressive_Image_Generation_Guided_by_Scene_Graphs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="43-WeedSense-Multi-Task-Learning-for-Weed-Segmentation-Height-Estimation-and-Growth-Stage-Classification"><a href="#43-WeedSense-Multi-Task-Learning-for-Weed-Segmentation-Height-Estimation-and-Growth-Stage-Classification" class="headerlink" title="43. WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation, and Growth Stage Classification"></a>43. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/WeedSense__Multi-Task_Learning_for_Weed_Segmentation,_Height_Estimation,_and_Growth_Stage_Classifica.pdf">WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation, and Growth Stage Classification</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Southern Illinois University Carbondale</span></p>
<p>该论文提出了WeedSense，一种多任务学习架构，可同时实现杂草语义分割、植株高度估算和生长阶段分类。方法采用双路径UIB编码器和Transformer特征融合的多任务解码器，在自主采集的含有16种杂草全生长周期的新数据集上进行评估。实验结果表明，WeedSense在分割、估高和生长阶段分类上均优于现有方法，并实现实时推理速度与参数量优化。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/WeedSense_Multi-Task_Learning_for_Weed_Segmentation_Height_Estimation_and_Growth_Stage_Classification.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="44-Reconstruction-Using-the-Invisible-Intuition-from-NIR-and-Metadata-for-Enhanced-3D-Gaussian-Splatting"><a href="#44-Reconstruction-Using-the-Invisible-Intuition-from-NIR-and-Metadata-for-Enhanced-3D-Gaussian-Splatting" class="headerlink" title="44. Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting"></a>44. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Reconstruction_Using_the_Invisible__Intuition_from_NIR_and_Metadata_for_Enhanced_3D_Gaussian_Splatti.pdf">Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Korea University</span></p>
<p>本文提出了一种多模态3D高斯泼溅重建框架NIRSplat，结合了近红外(NIR)、RGB影像和植物学元数据，通过跨模态注意力机制与三维点位编码，实现对农业场景下复杂照明、遮挡和视角受限等问题的鲁棒三维重建。实验表明，NIRSplat在重建精度与视觉质量上显著优于现有主流方法，特别适用于农业环境。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Reconstruction_Using_the_Invisible_Intuition_from_NIR_and_Metadata_for_Enhanced_3D_Gaussian_Splatting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="45-FOCUS-Frequency-Optimized-Conditioning-of-DiffUSion-Models-for-mitigating-catastrophic-forgetting-during-Test-Time-Adaptation"><a href="#45-FOCUS-Frequency-Optimized-Conditioning-of-DiffUSion-Models-for-mitigating-catastrophic-forgetting-during-Test-Time-Adaptation" class="headerlink" title="45. FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation"></a>45. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/FOCUS__Frequency-Optimized_Conditioning_of_DiffUSion_Models_for_mitigating_catastrophic_forgetting_d.pdf">FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Centre for Frontier AI Research (CFAR), Agency for Science, Technology and Research (A*STAR)</span></p>
<p>该论文提出FOCUS，一种基于频率优化的扩散模型输入适应方法，通过轻量级Y型频率预测网络（Y-FPN）学习空间自适应频率先验，并利用FrequencyMix频率数据增强训练Y-FPN，条件化扩散模型逆过程以提升语义分割和单目深度估计任务在多种分布偏移和噪声干扰下的适应能力。实验表明，FOCUS在15类腐蚀和三大数据集上取得了最优平均性能，并能与现有模型适应方法协同提升，显著缓解灾难性遗忘。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/FOCUS_Frequency-Optimized_Conditioning_of_DiffUSion_Models_for_mitigating_catastrophic_forgetting_during_Test-Time_Adaptation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="46-MoCHA-former-Moire-Conditioned-Hybrid-Adaptive-Transformer-for-Video-Demoireing"><a href="#46-MoCHA-former-Moire-Conditioned-Hybrid-Adaptive-Transformer-for-Video-Demoireing" class="headerlink" title="46. MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing"></a>46. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/MoCHA-former__Moir%C3%A9-Conditioned_Hybrid_Adaptive_Transformer_for_Video_Demoir%C3%A9ing.pdf">MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chung-Ang University</span></p>
<p>该论文提出了MoCHA-former框架，通过Decoupled Moiré Adaptive Demoiréing (DMAD)模块有效分离并自适应处理视频中的摩尔纹，结合Spatio-Temporal Adaptive Demoiréing (STAD)模块利用时空和频域信息去除大规模和动态变化的摩尔纹。实验结果表明，该方法在RawVDemoiré和VDemoiré数据集上超越了现有方法，提升了去摩尔纹效果并提高了参数效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/MoCHA-former_Moir%C3%A9-Conditioned_Hybrid_Adaptive_Transformer_for_Video_Demoir%C3%A9ing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="47-CTA-Flux-Integrating-Chinese-Cultural-Semantics-into-High-Quality-English-Text-to-Image-Communities"><a href="#47-CTA-Flux-Integrating-Chinese-Cultural-Semantics-into-High-Quality-English-Text-to-Image-Communities" class="headerlink" title="47. CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities"></a>47. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/CTA-Flux__Integrating_Chinese_Cultural_Semantics_into_High-Quality_English_Text-to-Image_Communities.pdf">CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>该论文提出CTA-Flux，通过在Flux文本到图像生成模型中集成中国语义分支，实现高质量中文文本驱动的图像生成。方法采用MMDiT架构，在保持英文模型主干及社区插件兼容性的前提下，设计轻量级中文语言适配模块，并通过两阶段训练（语义对齐与文化特征微调）显著提升中文语境下图像的文化真实性和生成质量。实验证明CTA-Flux可同时支持中英文输入，生成图像在视觉和语义上兼具高质量和文化适配性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/CTA-Flux_Integrating_Chinese_Cultural_Semantics_into_High-Quality_English_Text-to-Image_Communities.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="48-QuadINR-Hardware-Efficient-Implicit-Neural-Representations-Through-Quadratic-Activation"><a href="#48-QuadINR-Hardware-Efficient-Implicit-Neural-Representations-Through-Quadratic-Activation" class="headerlink" title="48. QuadINR: Hardware-Efficient Implicit Neural Representations Through Quadratic Activation"></a>48. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/QuadINR__Hardware-Efficient_Implicit_Neural_Representations_Through_Quadratic_Activation.pdf">QuadINR: Hardware-Efficient Implicit Neural Representations Through Quadratic Activation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Hong Kong</span></p>
<p>该论文提出QuadINR方法，通过采用分段二次激活函数实现硬件高效的隐式神经表示（INR），并提出统一的N级流水线框架便于FPGA和ASIC上的高效实现。实验结果表明，QuadINR在图片和视频重建任务中可在大幅降低硬件消耗的同时提升PSNR最高2.06dB，资源和功耗降低高达97%，显著优于现有方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/QuadINR_Hardware-Efficient_Implicit_Neural_Representations_Through_Quadratic_Activation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="49-Physics-Constrained-Diffusion-Reconstruction-with-Posterior-Correction-for-Quantitative-and-Fast-PET-Imaging"><a href="#49-Physics-Constrained-Diffusion-Reconstruction-with-Posterior-Correction-for-Quantitative-and-Fast-PET-Imaging" class="headerlink" title="49. Physics-Constrained Diffusion Reconstruction with Posterior Correction for Quantitative and Fast PET Imaging"></a>49. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Physics-Constrained_Diffusion_Reconstruction_with_Posterior_Correction_for_Quantitative_and_Fast_PET.pdf">Physics-Constrained Diffusion Reconstruction with Posterior Correction for Quantitative and Fast PET Imaging</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beijing Normal University</span></p>
<p>本论文提出了一种基于条件扩散模型与后验物理校正（PET-DPC）的PET图像重建方法。方法流程为：首先对原始PET列表模式数据进行几何TOF概率图像（GTP-image）生成，再将其作为辅助输入用于条件DDPM训练，在反向扩散采样过程中引入物理后验信息以实现散射和衰减校正。实验在大量临床、物理仿体与模拟数据集上验证，结果显示，PET-DPC在定量准确性、结构细节恢复与重建速度上均优于端到端深度学习模型和部分传统迭代算法，并能良好泛化至分布外数据。该方法显著提升了PET图像的定量可靠性和临床适用性，兼顾速度与精度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Physics-Constrained_Diffusion_Reconstruction_with_Posterior_Correction_for_Quantitative_and_Fast_PET_Imaging.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="50-Learning-Point-Cloud-Representations-with-Pose-Continuity-for-Depth-Based-Category-Level-6D-Object-Pose-Estimation"><a href="#50-Learning-Point-Cloud-Representations-with-Pose-Continuity-for-Depth-Based-Category-Level-6D-Object-Pose-Estimation" class="headerlink" title="50. Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation"></a>50. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Learning_Point_Cloud_Representations_with_Pose_Continuity_for_Depth-Based_Category-Level_6D_Object_P.pdf">Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Graduate Center, CUNY</span></p>
<p>该论文提出HRC-Pose，一种基于深度图的类别级6D物体姿态估计方法。方法创新性地引入分层排序对比学习模块，利用6D姿态感知的层级排序来学习点云的连续表征，并将位姿解耦为旋转和平移分别编码，分别用于最终的姿态回归。实验表明，该方法在REAL275和CAMERA25数据集上性能优于现有深度方法，且推理速度达实时水平，具有良好实际应用潜力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Learning_Point_Cloud_Representations_with_Pose_Continuity_for_Depth-Based_Category-Level_6D_Object_Pose_Estimation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="51-TCFNet-Bidirectional-Face-Bone-Transformation-via-a-Transformer-Based-Coarse-to-Fine-Point-Movement-Network"><a href="#51-TCFNet-Bidirectional-Face-Bone-Transformation-via-a-Transformer-Based-Coarse-to-Fine-Point-Movement-Network" class="headerlink" title="51. TCFNet: Bidirectional Face-Bone Transformation via a Transformer-Based Coarse-to-Fine Point Movement Network"></a>51. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/TCFNet__Bidirectional_face-bone_transformation_via_a_Transformer-based_coarse-to-fine_point_movement.pdf">TCFNet: Bidirectional Face-Bone Transformation via a Transformer-Based Coarse-to-Fine Point Movement Network</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Beihang University</span></p>
<p>本文提出了TCFNet，一种基于Transformer的粗到细点云移动网络框架，针对正颌手术规划中的面部与骨骼点云之间的双向变换问题。方法在第一阶段采用Point Transformer V3提取全局特征并生成粗变形路径，第二阶段引入本地信息聚合网络（LIA-Net），建模点云的局部几何结构以增强局部特征；同时设计了辅助损失用于关键结构的对应。实验表明，TCFNet在各项评估指标及可视化效果上均优于现有SOTA方法，能高效准确处理高密度点云，实现面骨双向转化并提升局部结构重建。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/TCFNet_Bidirectional_Face-Bone_Transformation_via_a_Transformer-Based_Coarse-to-Fine_Point_Movement_Network.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="52-Taming-Transformer-for-Emotion-Controllable-Talking-Face-Generation"><a href="#52-Taming-Transformer-for-Emotion-Controllable-Talking-Face-Generation" class="headerlink" title="52. Taming Transformer for Emotion-Controllable Talking Face Generation"></a>52. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Taming_Transformer_for_Emotion-Controllable_Talking_Face_Generation.pdf">Taming Transformer for Emotion-Controllable Talking Face Generation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Xidian University</span></p>
<p>本文提出一种基于离散表示的自回归框架用于情感可控的语音驱动人脸视频生成。方法包括音频情感与内容特征的解耦、VQ-GAN视觉量化、情感锚点（EA）表示融合以及自回归Transformer建模，实现了音视频跨模态情感融合与高质量情感可控人脸生成。实验结果显示，该方法在MEAD数据集上显著优于现有方法，生成视频的情感准确性和真实性大幅提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Taming_Transformer_for_Emotion-Controllable_Talking_Face_Generation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="53-HandCraft-Dynamic-Sign-Generation-for-Synthetic-Data-Augmentation"><a href="#53-HandCraft-Dynamic-Sign-Generation-for-Synthetic-Data-Augmentation" class="headerlink" title="53. HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation"></a>53. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/HandCraft__Dynamic_Sign_Generation_for_Synthetic_Data_Augmentation.pdf">HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Instituto de Investigación en Informática LIDI - Universidad Nacional de La Plata</span></p>
<p>本文提出了一种基于条件多层感知机（CMLPe）的高效手语动作生成器，用于合成手语关键点序列，并通过两阶段流程（先用合成数据预训练SLR模型，再用真实数据微调）提升手语识别准确率。实验结果表明，该方法在LSFB和DiSPLaY数据集上取得新的SOTA，且合成数据预训练优于传统增强方法，两者结合效果更佳。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/HandCraft_Dynamic_Sign_Generation_for_Synthetic_Data_Augmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="llm">Large Language Model</h2>


<h3 id="54-Long-Chain-of-Thought-Reasoning-Across-Languages"><a href="#54-Long-Chain-of-Thought-Reasoning-Across-Languages" class="headerlink" title="54. Long Chain-of-Thought Reasoning Across Languages"></a>54. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Long_Chain-of-Thought_Reasoning_Across_Languages.pdf">Long Chain-of-Thought Reasoning Across Languages</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Berkeley</span></p>
<p>该论文通过将英文推理数据集自动翻译为法语、日语、拉脱维亚语和斯瓦希里语，系统研究了长链式思维（Long CoT）在多语言环境下的表现，方法包括对Qwen 2.5&#x2F;3模型在多语言推理轨迹上的微调、语言强制和数据集质量与规模的对比实验。结论表明：英文作为枢纽语言对不同语种的推理迁移效用不一，高资源语言本地推理表现与英文相当，中低资源语言则表现分化，多语言预训练缩小但未消除跨语种性能差距，小规模高质量数据对高资源语言足够，而低资源语言需要更大规模数据。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Long_Chain-of-Thought_Reasoning_Across_Languages.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="55-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs"><a href="#55-Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs" class="headerlink" title="55. Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs"></a>55. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Quantization_Meets_dLLMs__A_Systematic_Study_of_Post-training_Quantization_for_Diffusion_LLMs.pdf">Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">NLPR &amp; MAIS, Institute of Automation, CAS</span></p>
<p>本文首次系统研究了后训练量化（PTQ）在扩散型大语言模型（dLLMs）上的应用，分析发现激活异常值广泛存在并是低比特量化的关键挑战。实验比较了多种量化方法（GPTQ、AWQ、SmoothQuant、QuaRot、DuQuant）在不同任务和模型上的表现，结论指出GPTQ和DuQuant在低比特量化下表现优越，任务类型和模型调优方式影响量化鲁棒性，为dLLMs高效部署提供了实用指导。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Quantization_Meets_dLLMs_A_Systematic_Study_of_Post-training_Quantization_for_Diffusion_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="56-MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds"><a href="#56-MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds" class="headerlink" title="56. MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds"></a>56. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/MeshCoder__LLM-Powered_Structured_Mesh_Code_Generation_from_Point_Clouds.pdf">MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Artificial Intelligence Laboratory</span></p>
<p>本文提出了MeshCoder框架，将3D点云重建为可编辑的Blender Python脚本。方法包括设计高表达力的Blender Python API、构建百万级3D对象-代码对大数据集，并训练多模态大语言模型（LLM）将点云编码为结构化代码，实现高精度形状重建和语义理解。实验表明，MeshCoder在shape-to-code任务上表现优异，并显著提升LLM对3D形状的推理能力，且支持便捷的几何与拓扑编辑。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/MeshCoder_LLM-Powered_Structured_Mesh_Code_Generation_from_Point_Clouds.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="57-Universal-and-Transferable-Adversarial-Attack-on-Large-Language-Models-Using-Exponentiated-Gradient-Descent"><a href="#57-Universal-and-Transferable-Adversarial-Attack-on-Large-Language-Models-Using-Exponentiated-Gradient-Descent" class="headerlink" title="57. Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent"></a>57. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Universal_and_Transferable_Adversarial_Attack_on_Large_Language_Models_Using_Exponentiated_Gradient_.pdf">Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Florida State University</span></p>
<p>该论文提出了一种基于指数梯度下降（EGD）的新型对抗攻击方法，通过优化松弛的一次编码，能高效地对大型语言模型（LLMs）进行jailbreak。该方法在无需显式投影的情况下保证概率单纯形约束，并通过理论收敛性证明和大量实验，展示了对多种开源模型和多种数据集的高成功率及优越的迁移性与通用性。结论表明，该方法在成功率和计算效率方面均超越三种主流基线，是当前LLM jailbreak的有效方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Universal_and_Transferable_Adversarial_Attack_on_Large_Language_Models_Using_Exponentiated_Gradient_Descent.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="58-From-Passive-Tool-to-Socio-cognitive-Teammate-A-Conceptual-Framework-for-Agentic-AI-in-Human-AI-Collaborative-Learning"><a href="#58-From-Passive-Tool-to-Socio-cognitive-Teammate-A-Conceptual-Framework-for-Agentic-AI-in-Human-AI-Collaborative-Learning" class="headerlink" title="58. From Passive Tool to Socio-cognitive Teammate: A Conceptual Framework for Agentic AI in Human-AI Collaborative Learning"></a>58. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/From_Passive_Tool_to_Socio-cognitive_Teammate__A_Conceptual_Framework_for_Agentic_AI_in_Human-AI_Col.pdf">From Passive Tool to Socio-cognitive Teammate: A Conceptual Framework for Agentic AI in Human-AI Collaborative Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tsinghua University</span></p>
<p>本文提出APCP（Adaptive instrument, Proactive assistant, Co-learner, Peer collaborator）四级框架，系统刻画了教育场景下AI从被动工具到具备主动性、协作能力的“社交认知队友”演变过程。作者基于社会文化学习理论及CSCL，明确AI在不同层级下的角色分工和互动模式，强调AI虽无法成为具备真实意识的合作伙伴，但可作为高效功能型协作者显著提升协作学习效果，对未来教育AI设计和教学实践具有重要指导意义。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/From_Passive_Tool_to_Socio-cognitive_Teammate_A_Conceptual_Framework_for_Agentic_AI_in_Human-AI_Collaborative_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="59-TransLLM-A-Unified-Multi-Task-Foundation-Framework-for-Urban-Transportation-via-Learnable-Prompting"><a href="#59-TransLLM-A-Unified-Multi-Task-Foundation-Framework-for-Urban-Transportation-via-Learnable-Prompting" class="headerlink" title="59. TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting"></a>59. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/TransLLM__A_Unified_Multi-Task_Foundation_Framework_for_Urban_Transportation_via_Learnable_Prompting.pdf">TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Science and Technology of China</span></p>
<p>本文提出TransLLM，一种结合轻量级时空编码器与大语言模型（LLM）的统一多任务城市交通基础模型框架，采用可学习的实例级prompt路由机制（基于强化学习），动态个性化生成适用于不同输入的提示，并通过多任务输出层提升结构化数据的预测精度。实验表明，TransLLM在交通流预测、电动车充电需求预测和出租车调度等任务上，在有监督与零样本场景下均优于十个主流基线模型，实现了优异的泛化和跨任务适应能力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/TransLLM_A_Unified_Multi-Task_Foundation_Framework_for_Urban_Transportation_via_Learnable_Prompting.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="60-Assessing-the-Quality-and-Security-of-AI-Generated-Code-A-Quantitative-Analysis"><a href="#60-Assessing-the-Quality-and-Security-of-AI-Generated-Code-A-Quantitative-Analysis" class="headerlink" title="60. Assessing the Quality and Security of AI-Generated Code: A Quantitative Analysis"></a>60. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Assessing_the_Quality_and_Security_of_AI-Generated_Code__A_Quantitative_Analysis.pdf">Assessing the Quality and Security of AI-Generated Code: A Quantitative Analysis</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Sonar</span></p>
<p>该论文采用静态分析工具SonarQube对五种主流大语言模型（Claude Sonnet 4、Claude 3.7 Sonnet、GPT-4o、Llama 3.2 90B、OpenCoder 8B）生成的4,442份Java代码进行系统质量与安全性评估。研究流程包括代码生成、功能测试与静态缺陷检测，发现所有模型输出的代码均存在明显的代码异味、bug和安全漏洞，且模型功能表现与代码质量、安全性无明显相关性。结论认为，LLM生成代码在投入生产前需严格验证，静态分析工具是发现潜在缺陷和保障安全的关键手段。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Assessing_the_Quality_and_Security_of_AI-Generated_Code_A_Quantitative_Analysis.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="61-ShizhenGPT-Towards-Multimodal-LLMs-for-Traditional-Chinese-Medicine"><a href="#61-ShizhenGPT-Towards-Multimodal-LLMs-for-Traditional-Chinese-Medicine" class="headerlink" title="61. ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine"></a>61. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/ShizhenGPT__Towards_Multimodal_LLMs_for_Traditional_Chinese_Medicine.pdf">ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong, Shenzhen</span></p>
<p>ShizhenGPT提出了首个针对中医领域的多模态大语言模型，结合大规模中医文本、图像、音频和生理信号，采用两阶段预训练和多模态指令微调，支持望、闻、问、切四诊信息的统一感知与推理。实验显示，ShizhenGPT在中医专业考试、视觉诊断和多模态信号感知等任务上超越同规模主流LLM，部分能力接近超大模型，显著提升了中医AI诊断的全面性和准确性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/ShizhenGPT_Towards_Multimodal_LLMs_for_Traditional_Chinese_Medicine.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="62-ELATE-Evolutionary-Language-model-for-Automated-Time-series-Engineering"><a href="#62-ELATE-Evolutionary-Language-model-for-Automated-Time-series-Engineering" class="headerlink" title="62. ELATE: Evolutionary Language model for Automated Time-series Engineering"></a>62. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/ELATE__Evolutionary_Language_model_for_Automated_Time-series_Engineering.pdf">ELATE: Evolutionary Language model for Automated Time-series Engineering</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">JP Morgan AI Research</span></p>
<p>本论文提出ELATE方法，将大型语言模型（LLM）嵌入到进化优化框架中，用于自动化时序数据的特征工程。其流程包括：LLM根据任务上下文和已有特征建议新的特征变换，通过Granger因果性和互信息等指标评估特征预测力，并结合SHAP特征筛选进行特征库的进化与剪枝。大量实验表明，ELATE在多个时序预测任务上平均提升预测准确率8.4%，同时具备较高可解释性和计算效率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/ELATE_Evolutionary_Language_model_for_Automated_Time-series_Engineering.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="63-OneLoc-Geo-Aware-Generative-Recommender-Systems-for-Local-Life-Service"><a href="#63-OneLoc-Geo-Aware-Generative-Recommender-Systems-for-Local-Life-Service" class="headerlink" title="63. OneLoc: Geo-Aware Generative Recommender Systems for Local Life Service"></a>63. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/OneLoc__Geo-Aware_Generative_Recommender_Systems_for_Local_Life_Service.pdf">OneLoc: Geo-Aware Generative Recommender Systems for Local Life Service</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kuaishou Inc.</span></p>
<p>该论文提出OneLoc，一种面向本地生活服务的端到端生成式推荐系统，结合地理信息建模（geo-aware语义ID、geo-aware自注意力、neighbor-aware prompt）与强化学习优化（地理奖励和GMV奖励）。结果显示OneLoc在Kuaishou上线后显著提升GMV和订单量，并在离线实验和A&#x2F;B测试中优于传统推荐方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/OneLoc_Geo-Aware_Generative_Recommender_Systems_for_Local_Life_Service.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="64-Can-LLM-Agents-Solve-Collaborative-Tasks-A-Study-on-Urgency-Aware-Planning-and-Coordination"><a href="#64-Can-LLM-Agents-Solve-Collaborative-Tasks-A-Study-on-Urgency-Aware-Planning-and-Coordination" class="headerlink" title="64. Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination"></a>64. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Can_LLM_Agents_Solve_Collaborative_Tasks__A_Study_on_Urgency-Aware_Planning_and_Coordination.pdf">Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Universidade Federal de Minas Gerais</span></p>
<p>本文提出并系统评估了基于大型语言模型（LLM）驱动的多智能体在结构化救援任务中的协作能力，方法包括在完全可知的图环境下，采用模块化推理流程和消息通信机制，使智能体进行分工、优先级规划和资源分配。结果显示，部分LLM智能体能实现紧急性感知和任务协作，但在效率和可靠性上仍略逊于确定性启发式基线，主要挑战在于计划幻觉、冗余动作和队友意图感知不足。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Can_LLM_Agents_Solve_Collaborative_Tasks_A_Study_on_Urgency-Aware_Planning_and_Coordination.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="65-Evaluating-Retrieval-Augmented-Generation-vs-Long-Context-Input-for-Clinical-Reasoning-over-EHRs"><a href="#65-Evaluating-Retrieval-Augmented-Generation-vs-Long-Context-Input-for-Clinical-Reasoning-over-EHRs" class="headerlink" title="65. Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs"></a>65. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Evaluating_Retrieval-Augmented_Generation_vs._Long-Context_Input_for_Clinical_Reasoning_over_EHRs.pdf">Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Wisconsin-Madison</span></p>
<p>本文比较了在电子健康记录（EHR）中，基于检索增强生成（RAG）与直接长上下文输入的三种临床推理任务表现。通过对三种主流大语言模型在成像信息抽取、抗生素用药时序生成和诊断识别等任务上实验，发现RAG能以显著更少的输入token达到接近甚至超过长上下文输入的性能，尤其在信息提取类任务中表现突出。结论是RAG在长文档医疗推理任务中即使面对更强大的长上下文LLM，仍具备高效竞争力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Evaluating_Retrieval-Augmented_Generation_vs._Long-Context_Input_for_Clinical_Reasoning_over_EHRs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="66-Synthetic-Adaptive-Guided-Embeddings-SAGE-A-Novel-Knowledge-Distillation-Method"><a href="#66-Synthetic-Adaptive-Guided-Embeddings-SAGE-A-Novel-Knowledge-Distillation-Method" class="headerlink" title="66. Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method"></a>66. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Synthetic_Adaptive_Guided_Embeddings_(SAGE)__A_Novel_Knowledge_Distillation_Method.pdf">Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of North Texas</span></p>
<p>该论文提出一种新型知识蒸馏框架SAGE，通过UMAP降维和邻近采样，动态生成针对学生模型高损失区域的合成训练数据，并采用跳过输入层的矢量空间蒸馏策略，大幅提升训练效率。实验证明，在GLUE多项NLP任务上，SAGE学生模型以更少训练周期达到甚至超越DistilBERT等主流蒸馏模型，尤其在语义推理和情感分析任务表现突出。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Synthetic_Adaptive_Guided_Embeddings_%28SAGE%29_A_Novel_Knowledge_Distillation_Method.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="67-PepThink-R1-LLM-for-Interpretable-Cyclic-Peptide-Optimization-with-CoT-SFT-and-Reinforcement-Learning"><a href="#67-PepThink-R1-LLM-for-Interpretable-Cyclic-Peptide-Optimization-with-CoT-SFT-and-Reinforcement-Learning" class="headerlink" title="67. PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning"></a>67. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/PepThink-R1__LLM_for_Interpretable_Cyclic_Peptide_Optimization_with_CoT_SFT_and_Reinforcement_Learni.pdf">PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Merck &amp; Co., Inc., Rahway, NJ, USA</span></p>
<p>本文提出PepThink-R1，一种将大语言模型（LLM）、链式思维（CoT）监督微调和强化学习（RL）相结合的可解释环肽生成优化框架。该方法通过在分子单体水平进行显式推理，引导LLM产生具有多重药理性质（如脂溶性、稳定性和暴露度）优化的环肽，并利用定制奖励函数在化学有效性和多性质提升间平衡，实现自动探索多样化肽序列。实验表明，PepThink-R1在多目标优化和解释性上显著优于现有通用LLM（如GPT-5）和领域特定基线工具，能够高效生成满足多重性质的创新肽分子。结论：PepThink-R1实现了可解释的多目标肽优化，提升了优化成功率和药物发现的透明度。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/PepThink-R1_LLM_for_Interpretable_Cyclic_Peptide_Optimization_with_CoT_SFT_and_Reinforcement_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="68-Evaluating-Multilingual-and-Code-Switched-Alignment-in-LLMs-via-Synthetic-Natural-Language-Inference"><a href="#68-Evaluating-Multilingual-and-Code-Switched-Alignment-in-LLMs-via-Synthetic-Natural-Language-Inference" class="headerlink" title="68. Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference"></a>68. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Evaluating_Multilingual_and_Code-Switched_Alignment_in_LLMs_via_Synthetic_Natural_Language_Inference.pdf">Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Texas A&amp;M University</span></p>
<p>本文提出了一种通过合成、逻辑驱动的自然语言推理（NLI）框架，系统评估大型语言模型（LLMs）在多语言与代码切换情境下的语义一致性。该方法结合模板生成、自动高质量翻译及混合语输入，能精细控制推理难度，剥离语言噪声，揭示不同LLM在跨语言推理上的薄弱环节。实验发现，代码切换不仅不会降低推理表现，有时还能提升模型的多语言鲁棒性，且语义嵌入分析显示翻译后语句高度聚类，证实语义保持良好。结论指出，当前LLM的跨语言推理存在脆弱性，但代码切换可作为提升多语言推理能力的重要手段。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Evaluating_Multilingual_and_Code-Switched_Alignment_in_LLMs_via_Synthetic_Natural_Language_Inference.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="69-Transplant-Then-Regenerate-A-New-Paradigm-for-Text-Data-Augmentation"><a href="#69-Transplant-Then-Regenerate-A-New-Paradigm-for-Text-Data-Augmentation" class="headerlink" title="69. Transplant Then Regenerate: A New Paradigm for Text Data Augmentation"></a>69. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Transplant_Then_Regenerate__A_New_Paradigm_for_Text_Data_Augmentation.pdf">Transplant Then Regenerate: A New Paradigm for Text Data Augmentation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>本文提出了一种基于大语言模型（LLM）的创新文本数据增强范式LMTransplant，其核心方法是“transplant-then-regenerate”：将原始文本嵌入由LLM扩展生成的上下文中，并利用双向文本续写及遮蔽预测引导LLM生成多样且富有创造性的内容级变体。实验结果显示，LMTransplant在文本分类、问答和命名实体识别等任务上生成的数据提升了下游模型表现，并显著优于现有方法，具备较强的可扩展性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Transplant_Then_Regenerate_A_New_Paradigm_for_Text_Data_Augmentation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="70-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers"><a href="#70-MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers" class="headerlink" title="70. MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers"></a>70. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/MCP-Universe__Benchmarking_Large_Language_Models_with_Real-World_Model_Context_Protocol_Servers.pdf">MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Salesforce AI Research</span></p>
<p>本论文提出了MCP-Universe，这是首个基于真实MCP服务器的综合基准，用于评测大语言模型（LLMs）在连接外部数据源和工具时的现实任务表现。方法包括六大领域的231项任务，采用执行式评估框架（包括格式、静态与动态评估器），分析了主流LLM在长上下文、多工具和跨域任务下的局限性，发现即使最强模型成功率也不足50%。结论：当前LLM在真实MCP环境下存在能力短板，需改进模型与代理设计。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/MCP-Universe_Benchmarking_Large_Language_Models_with_Real-World_Model_Context_Protocol_Servers.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="71-Entropy-Constrained-Strategy-Optimization-in-Urban-Floods-A-Multi-Agent-Framework-with-LLM-and-Knowledge-Graph-Integration"><a href="#71-Entropy-Constrained-Strategy-Optimization-in-Urban-Floods-A-Multi-Agent-Framework-with-LLM-and-Knowledge-Graph-Integration" class="headerlink" title="71. Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration"></a>71. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Entropy-Constrained_Strategy_Optimization_in_Urban_Floods__A_Multi-Agent_Framework_with_LLM_and_Know.pdf">Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">College of Intelligence and Computing, Tianjin University</span></p>
<p>该论文提出H–J框架，融合了大语言模型（LLM）、结构化与非结构化知识检索、熵约束分层策略生成和反馈优化，形成城市洪涝多智能体调度的闭环决策系统。实验结果表明，该方法在三种典型降雨场景下，显著优于规则基线和强化学习方法，在交通通畅度、任务完成率和系统鲁棒性方面均有提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Entropy-Constrained_Strategy_Optimization_in_Urban_Floods_A_Multi-Agent_Framework_with_LLM_and_Knowledge_Graph_Integration.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="72-Adaptively-Robust-LLM-Inference-Optimization-under-Prediction-Uncertainty"><a href="#72-Adaptively-Robust-LLM-Inference-Optimization-under-Prediction-Uncertainty" class="headerlink" title="72. Adaptively Robust LLM Inference Optimization under Prediction Uncertainty"></a>72. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Adaptively_Robust_LLM_Inference_Optimization_under_Prediction_Uncertainty.pdf">Adaptively Robust LLM Inference Optimization under Prediction Uncertainty</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Peking University</span></p>
<p>本文提出了解决大语言模型（LLM）推理调度中的预测不确定性问题，通过机器学习方法预测输出长度区间，并设计了两种调度算法：Amax（利用上界，保守但低效）和Amin（利用下界，自适应更新），并对其竞争比进行了理论分析和证明。实验结果显示，Amin算法在各种预测精度下均表现出高效和稳健的推理调度性能，显著优于传统保守方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Adaptively_Robust_LLM_Inference_Optimization_under_Prediction_Uncertainty.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="73-Semantic-Energy-Detecting-LLM-Hallucination-Beyond-Entropy"><a href="#73-Semantic-Energy-Detecting-LLM-Hallucination-Beyond-Entropy" class="headerlink" title="73. Semantic Energy: Detecting LLM Hallucination Beyond Entropy"></a>73. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Semantic_Energy__Detecting_LLM_Hallucination_Beyond_Entropy.pdf">Semantic Energy: Detecting LLM Hallucination Beyond Entropy</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tianjin University</span></p>
<p>本文提出了Semantic Energy框架，基于对大模型最后一层logits的能量建模，结合语义聚类和玻尔兹曼能量分布，提出一种新的LLM不确定性度量方法。该方法能够克服Semantic Entropy基于概率归一化导致在部分场景下失效的问题，在多个问答基准上显著提升了幻觉检测和不确定性评估能力，平均AUROC提升超13%。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Semantic_Energy_Detecting_LLM_Hallucination_Beyond_Entropy.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="74-NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model"><a href="#74-NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model" class="headerlink" title="74. NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model"></a>74. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/NVIDIA_Nemotron_Nano_2__An_Accurate_and_Efficient_Hybrid_Mamba-Transformer_Reasoning_Model.pdf">NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">NVIDIA</span></p>
<p>本论文提出了Nemotron-Nano-9B-v2，一种混合Mamba-Transformer架构的语言模型，通过将多数自注意力层替换为Mamba-2层，提升推理任务吞吐率，保持同规模模型的先进准确率。其流程包括20万亿token的FP8预训练、连续长上下文扩展、多阶段SFT、GRPO、DPO和RLHF对齐，然后利用Minitron策略剪枝和蒸馏，最终在单块A10G GPU实现128k上下文高效推理。实验表明Nemotron-Nano-9B-v2在多项推理基准上准确率比同级模型更优或持平，且推理吞吐率提升至6倍。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/NVIDIA_Nemotron_Nano_2_An_Accurate_and_Efficient_Hybrid_Mamba-Transformer_Reasoning_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="75-Cognitive-Surgery-The-Awakening-of-Implicit-Territorial-Awareness-in-LLMs"><a href="#75-Cognitive-Surgery-The-Awakening-of-Implicit-Territorial-Awareness-in-LLMs" class="headerlink" title="75. Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs"></a>75. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Cognitive_Surgery__The_Awakening_of_Implicit_Territorial_Awareness_in_LLMs.pdf">Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">China Agricultural University</span></p>
<p>该论文提出了Cognitive Surgery (CoSur) 方法，通过表示提取、领地空间构建、作者身份判别和认知编辑四个模块，激活大语言模型（LLM）的隐性领地意识（ITA），提升其在单文本场景下的自我识别能力。实验表明，CoSur显著提升了三种主流LLM在单文本自我识别任务中的准确率，有效解决了信息损失导致的能力瓶颈。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Cognitive_Surgery_The_Awakening_of_Implicit_Territorial_Awareness_in_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="76-NoteIt-A-System-Converting-Instructional-Videos-to-Interactable-Notes-Through-Multimodal-Video-Understanding"><a href="#76-NoteIt-A-System-Converting-Instructional-Videos-to-Interactable-Notes-Through-Multimodal-Video-Understanding" class="headerlink" title="76. NoteIt: A System Converting Instructional Videos to Interactable Notes Through Multimodal Video Understanding"></a>76. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/NoteIt__A_System_Converting_Instructional_Videos_to_Interactable_Notes_Through_Multimodal_Video_Unde.pdf">NoteIt: A System Converting Instructional Videos to Interactable Notes Through Multimodal Video Understanding</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The University of Hong Kong</span></p>
<p>本文提出NoteIt，一个可将教学视频自动转换为可交互笔记的系统。其方法采用多模态大语言模型（如GPT-4o）为核心，结合视频解析、层次结构提取（章节与步骤）、视觉关键信息抽取（静态与动态关键帧）、笔记生成与自定义交互界面，全面保留视频的结构和多模态信息，并支持用户按需定制笔记表现形式和详细程度。技术评估和用户研究表明，NoteIt在内容准确性和用户满意度方面显著优于现有同类工具，有效提升了视频内容转化为结构化学习资料的效率和体验。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/NoteIt_A_System_Converting_Instructional_Videos_to_Interactable_Notes_Through_Multimodal_Video_Understanding.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="77-Credence-Calibration-Game-Calibrating-Large-Language-Models-through-Structured-Play"><a href="#77-Credence-Calibration-Game-Calibrating-Large-Language-Models-through-Structured-Play" class="headerlink" title="77. Credence Calibration Game? Calibrating Large Language Models through Structured Play"></a>77. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Credence_Calibration_Game__Calibrating_Large_Language_Models_through_Structured_Play.pdf">Credence Calibration Game? Calibrating Large Language Models through Structured Play</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Pennsylvania</span></p>
<p>本文提出了一种基于提示工程的LLM校准新框架，借鉴了人类“Credence Calibration Game”的结构化反馈机制，将模型历史表现和校准状态通过自然语言反馈整合到多轮交互Prompt中，实现无需参数更新的自适应置信度校准。实验在多种主流LLM及QA任务中验证了该方法能显著降低ECE和Brier分数，特别是在指数惩罚机制下校准效果最佳，但有时会略微降低准确率，表明该策略能有效提升模型置信度的可靠性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Credence_Calibration_Game_Calibrating_Large_Language_Models_through_Structured_Play.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="78-Towards-LLM-generated-explanations-for-Component-based-Knowledge-Graph-Question-Answering-Systems"><a href="#78-Towards-LLM-generated-explanations-for-Component-based-Knowledge-Graph-Question-Answering-Systems" class="headerlink" title="78. Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems"></a>78. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Towards_LLM-generated_explanations_for_Component-based_Knowledge_Graph_Question_Answering_Systems.pdf">Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">HTWK Leipzig</span></p>
<p>本文提出了一种为基于组件的知识图谱问答系统生成可解释性解释的方法，通过利用组件输入（SPARQL查询）和输出（RDF三元组）数据流，分别采用模板化（规则驱动）和大型语言模型（LLM）两种方式自动生成自然语言解释，并在Qanary框架下进行了实验与专家评价。实验结果表明，LLM生成的解释在可用性和正确性上优于模板方法，能有效提升系统可解释性，为用户和开发者提供对AI驱动问答系统决策过程的透明理解。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Towards_LLM-generated_explanations_for_Component-based_Knowledge_Graph_Question_Answering_Systems.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="79-PB-IAD-Utilizing-multimodal-foundation-models-for-semantic-industrial-anomaly-detection-in-dynamic-manufacturing-environments"><a href="#79-PB-IAD-Utilizing-multimodal-foundation-models-for-semantic-industrial-anomaly-detection-in-dynamic-manufacturing-environments" class="headerlink" title="79. PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments"></a>79. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/PB-IAD__Utilizing_multimodal_foundation_models_for_semantic_industrial_anomaly_detection_in_dynamic_.pdf">PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Friedrich-Alexander-Universität Erlangen-Nürnberg</span></p>
<p>本文提出PB-IAD框架，利用多模态基础模型（如GPT-4）通过语义化提示工程进行工业异常检测，特别适用于动态、数据稀缺的制造环境。方法通过模块化的提示模板和预处理模块，将领域专家的知识以自然语言融入检测流程，无需数据科学背景即可实现模型快速部署和灵活适配。实验表明，在三种工业场景下，PB-IAD仅凭语义提示即可在数据稀缺情况下超越PatchCore、Isolation Forest等主流算法，尤其在低样本场景下表现优异。结论：基于大模型的语义指令和人机协作，可实现工业异常检测的高效、灵活和可扩展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/PB-IAD_Utilizing_multimodal_foundation_models_for_semantic_industrial_anomaly_detection_in_dynamic_manufacturing_environments.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="80-In2x-at-WMT25-Translation-Task"><a href="#80-In2x-at-WMT25-Translation-Task" class="headerlink" title="80. In2x at WMT25 Translation Task"></a>80. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/In2x_at_WMT25_Translation_Task.pdf">In2x at WMT25 Translation Task</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Duxiaoman</span></p>
<p>该论文提出了In2x，一种以日语为核心的通用大语言模型翻译范式，涵盖数据构建、对齐、奖励模型等流程，强调将英语的优势迁移到日语，并通过多阶段预训练、监督微调和强化学习提升模型在低资源语言中的表现及文化表达能力。结果显示，In2x在WMT25比赛中无需任务特定微调便在日语相关赛道取得第一，显著优于多款主流专有模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/In2x_at_WMT25_Translation_Task.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="81-DuPO-Enabling-Reliable-LLM-Self-Verification-via-Dual-Preference-Optimization"><a href="#81-DuPO-Enabling-Reliable-LLM-Self-Verification-via-Dual-Preference-Optimization" class="headerlink" title="81. DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization"></a>81. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/DuPO__Enabling_Reliable_LLM_Self-Verification_via_Dual_Preference_Optimization.pdf">DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ByteDance Seed</span></p>
<p>该论文提出DuPO框架，通过广义双任务学习分解输入为已知和未知部分，并用LLM同时进行主任务和对偶任务，自动生成自监督反馈信号，无需人工标注或外部奖励。实验表明，DuPO在多语种翻译和数学推理任务显著提升了模型性能，并且可以作为推理时无训练重排序机制，帮助小模型超过大模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/DuPO_Enabling_Reliable_LLM_Self-Verification_via_Dual_Preference_Optimization.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="82-Automated-Optimization-Modeling-through-Expert-Guided-Large-Language-Model-Reasoning"><a href="#82-Automated-Optimization-Modeling-through-Expert-Guided-Large-Language-Model-Reasoning" class="headerlink" title="82. Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning"></a>82. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Automated_Optimization_Modeling_through_Expert-Guided_Large_Language_Model_Reasoning.pdf">Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">ZJU-UIUC Institute, Zhejiang University</span></p>
<p>本文提出ORThought框架，通过专家级优化建模原则结合链式思维推理，实现优化建模流程的自动化。方法包括系统化数据集修正与扩充、新物流领域基准LogiOR、模型与求解双代理协同，以及智能错误诊断修复。实验表明ORThought在多类型复杂优化问题上准确率和效率均优于现有多智能体及推理基线方法，尤其在整数与非线性优化建模上提升显著。结论指出，ORThought显著提升了大模型自动优化建模性能，且保持低计算消耗，但在约束表达和复杂问题上仍具挑战，未来可结合RAG检索增强与人机协作进一步提升。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Automated_Optimization_Modeling_through_Expert-Guided_Large_Language_Model_Reasoning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="83-DEPTH-Hallucination-Free-Relation-Extraction-via-Dependency-Aware-Sentence-Simplification-and-Two-tiered-Hierarchical-Refinement"><a href="#83-DEPTH-Hallucination-Free-Relation-Extraction-via-Dependency-Aware-Sentence-Simplification-and-Two-tiered-Hierarchical-Refinement" class="headerlink" title="83. DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement"></a>83. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/DEPTH__Hallucination-Free_Relation_Extraction_via_Dependency-Aware_Sentence_Simplification_and_Two-t.pdf">DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Shanghai Jiao Tong University</span></p>
<p>该论文提出DEPTH框架，通过依赖句法分析实现面向实体对的句子简化，并结合因果奖励建模的RLHF微调，在关系抽取中显著减少LLM幻觉问题。方法包括两阶段流程：首先用Grounding模块提取依赖路径简化句子并预测局部关系，然后用Refinement模块整合全局语义进行自校正和一致性检查。实验表明DEPTH在六大数据集上将幻觉率降至7.0%，F1得分提升17.2%，有效提升关系抽取的准确性与鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/DEPTH_Hallucination-Free_Relation_Extraction_via_Dependency-Aware_Sentence_Simplification_and_Two-tiered_Hierarchical_Refinement.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="84-ZPD-SCA-Unveiling-the-Blind-Spots-of-LLMs-in-Assessing-Students’-Cognitive-Abilities"><a href="#84-ZPD-SCA-Unveiling-the-Blind-Spots-of-LLMs-in-Assessing-Students’-Cognitive-Abilities" class="headerlink" title="84. ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students’ Cognitive Abilities"></a>84. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/ZPD-SCA__Unveiling_the_Blind_Spots_of_LLMs_in_Assessing_Students'_Cognitive_Abilities.pdf">ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students’ Cognitive Abilities</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Psychology, South China Normal University</span></p>
<p>本文提出ZPD-SCA数据集及多阶段学生认知能力适配性评估任务，专注于评估大语言模型（LLM）对不同学段学生中文阅读理解难度的判断能力。通过60位特级教师标注，实验证明LLM在零样本条件下表现较差，加入上下文示例后准确率显著提升，但依然存在系统性偏差和跨级误判，反映出当前LLM训练中对教育任务的忽视。结论指出需将认知对齐任务纳入LLM训练以提升其教育适应性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/ZPD-SCA_Unveiling_the_Blind_Spots_of_LLMs_in_Assessing_Students%E2%80%99_Cognitive_Abilities.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="85-Organ-Agents-Virtual-Human-Physiology-Simulator-via-LLMs"><a href="#85-Organ-Agents-Virtual-Human-Physiology-Simulator-via-LLMs" class="headerlink" title="85. Organ-Agents: Virtual Human Physiology Simulator via LLMs"></a>85. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Organ-Agents__Virtual_Human_Physiology_Simulator_via_LLMs.pdf">Organ-Agents: Virtual Human Physiology Simulator via LLMs</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tianjin University</span></p>
<p>本文提出Organ-Agents，一种基于大语言模型（LLM）的多智能体框架，用于模拟人体多器官系统的生理动态。方法上，将每个器官系统建模为独立的LLM智能体，先进行系统特异的有监督微调，再通过强化学习驱动的Correlator智能体实现智能体间动态协作，并引入残差补偿机制增强长期模拟稳定性。实验在MIMIC-IV及多中心ICU数据集上显示，模型在各系统均获得低MSE，能准确复现多系统临床事件链，在分布迁移和疾病严重度分层下表现稳健，能生成具有临床可解释性的反事实轨迹，专家评价其轨迹真实且生理一致。结论：Organ-Agents为临床数字孪生、诊断推理与治疗模拟提供了可信赖且可解释的解决方案。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Organ-Agents_Virtual_Human_Physiology_Simulator_via_LLMs.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h2 id="ml">Machine Learning</h2>


<h3 id="86-Multimodal-Quantum-Vision-Transformer-for-Enzyme-Commission-Classification-from-Biochemical-Representations"><a href="#86-Multimodal-Quantum-Vision-Transformer-for-Enzyme-Commission-Classification-from-Biochemical-Representations" class="headerlink" title="86. Multimodal Quantum Vision Transformer for Enzyme Commission Classification from Biochemical Representations"></a>86. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Multimodal_Quantum_Vision_Transformer_for_Enzyme_Commission_Classification_from_Biochemical_Represen.pdf">Multimodal Quantum Vision Transformer for Enzyme Commission Classification from Biochemical Representations</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Purdue University</span></p>
<p>本文提出了多模态量子视觉Transformer（QVT）框架，通过融合蛋白质序列嵌入、量子电子描述符、分子图结构和2D分子图像，实现了酶功能（EC号）分类。方法采用各模态专用编码器和交叉注意力融合，有效捕捉酶的立体电子与结构特征。实验结果显示，QVT模型在多模态融合下Top-1准确率提升至85.1%，显著优于单模态和其他量子机器学习模型，表明融合多种生化信息能提升酶功能预测的准确性和鲁棒性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Multimodal_Quantum_Vision_Transformer_for_Enzyme_Commission_Classification_from_Biochemical_Representations.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="87-AFABench-A-Generic-Framework-for-Benchmarking-Active-Feature-Acquisition"><a href="#87-AFABench-A-Generic-Framework-for-Benchmarking-Active-Feature-Acquisition" class="headerlink" title="87. AFABench: A Generic Framework for Benchmarking Active Feature Acquisition"></a>87. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/AFABench__A_Generic_Framework_for_Benchmarking_Active_Feature_Acquisition.pdf">AFABench: A Generic Framework for Benchmarking Active Feature Acquisition</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Chalmers University of Technology</span></p>
<p>本文提出了AFABench，这是首个用于主动特征获取（AFA）方法的标准化评测框架，支持多种贪心、静态和基于强化学习的特征获取策略。作者在多样的真实与合成数据集上系统实现并评测了这些算法，并引入了新型数据集AFAContext以测试非贪心策略。实验结果显示，虽然非贪心（强化学习）方法在特殊结构数据上有潜在优势，但实际中贪心判别方法在多数真实任务上表现更优且训练更高效，静态方法在部分场景下也具竞争力。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/AFABench_A_Generic_Framework_for_Benchmarking_Active_Feature_Acquisition.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="88-ECHO-Frequency-aware-Hierarchical-Encoding-for-Variable-length-Signal"><a href="#88-ECHO-Frequency-aware-Hierarchical-Encoding-for-Variable-length-Signal" class="headerlink" title="88. ECHO: Frequency-aware Hierarchical Encoding for Variable-length Signal"></a>88. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/ECHO__Frequency-aware_Hierarchical_Encoding_for_Variable-length_Signal.pdf">ECHO: Frequency-aware Hierarchical Encoding for Variable-length Signal</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">School of Computer Science, Wuhan University</span></p>
<p>本文提出了ECHO模型，一种适用于变长机器信号（如声学和振动）的频率感知分层编码基础模型。其方法融合了自适应频带切分、相对频率位置编码和层次化Transformer结构，无需信号填充或分割即可生成高质量嵌入，并在SIREN基准测试（涵盖DCASE多年份挑战与多模态故障数据集）中实现了异常检测和故障识别领域的最新性能。结论表明，ECHO兼具跨领域泛化能力和任务性能，优于现有通用音频模型。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/ECHO_Frequency-aware_Hierarchical_Encoding_for_Variable-length_Signal.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="89-Improving-Fairness-in-Graph-Neural-Networks-via-Counterfactual-Debiasing"><a href="#89-Improving-Fairness-in-Graph-Neural-Networks-via-Counterfactual-Debiasing" class="headerlink" title="89. Improving Fairness in Graph Neural Networks via Counterfactual Debiasing"></a>89. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Improving_Fairness_in_Graph_Neural_Networks_via_Counterfactual_Debiasing.pdf">Improving Fairness in Graph Neural Networks via Counterfactual Debiasing</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tianjin University</span></p>
<p>本文提出了一种基于反事实数据增强的公平性提升方法Fair-ICD，通过在消息传递前利用反事实邻域增强，提升图结构异质性，获得无偏节点表示，并结合对抗判别器减少GNN分类器预测中的偏见。实验结果表明，Fair-ICD在多个基准数据集和三种GNN骨干上显著提升了公平性指标，同时保持高准确率。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Improving_Fairness_in_Graph_Neural_Networks_via_Counterfactual_Debiasing.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="90-Cross-Modality-Controlled-Molecule-Generation-with-Diffusion-Language-Model"><a href="#90-Cross-Modality-Controlled-Molecule-Generation-with-Diffusion-Language-Model" class="headerlink" title="90. Cross-Modality Controlled Molecule Generation with Diffusion Language Model"></a>90. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Cross-Modality_Controlled_Molecule_Generation_with_Diffusion_Language_Model.pdf">Cross-Modality Controlled Molecule Generation with Diffusion Language Model</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Brandeis University</span></p>
<p>该论文提出了CMCM-DLM方法，通过在扩散模型基础上集成结构控制模块（SCM）和属性控制模块（PCM），实现了分两阶段的分子生成流程：先在早期扩散步骤注入结构约束，后期结合化学属性控制，显著提升多模态约束下新分子的生成效率和适应性。实验结果表明，该方法在三大分子数据集上平均提升属性指标16%、结构保真度约79%，为药物发现领域建立了新的多模态分子生成基准。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Cross-Modality_Controlled_Molecule_Generation_with_Diffusion_Language_Model.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="91-Addressing-Graph-Anomaly-Detection-via-Causal-Edge-Separation-and-Spectrum"><a href="#91-Addressing-Graph-Anomaly-Detection-via-Causal-Edge-Separation-and-Spectrum" class="headerlink" title="91. Addressing Graph Anomaly Detection via Causal Edge Separation and Spectrum"></a>91. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Addressing_Graph_Anomaly_Detection_via_Causal_Edge_Separation_and_Spectrum.pdf">Addressing Graph Anomaly Detection via Causal Edge Separation and Spectrum</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Tianjin University</span></p>
<p>本文提出了CES2-GAD，一种基于因果边分离与谱混合滤波的图异常点检测方法。具体做法是首先通过因果干预将原始图拆分为同质和异质边，随后用低通和高通谱滤波分别提取不同频域信号，最后将多种信号表示拼接后输入分类器判断异常节点。实验证明该方法在真实数据集上对异质图异常检测优于现有主流方法。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Addressing_Graph_Anomaly_Detection_via_Causal_Edge_Separation_and_Spectrum.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="92-LeanGeo-Formalizing-Competitional-Geometry-Problems-in-Lean"><a href="#92-LeanGeo-Formalizing-Competitional-Geometry-Problems-in-Lean" class="headerlink" title="92. LeanGeo: Formalizing Competitional Geometry Problems in Lean"></a>92. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/LeanGeo__Formalizing_Competitional_Geometry_problems_in_Lean.pdf">LeanGeo: Formalizing Competitional Geometry Problems in Lean</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Moonshot AI</span></p>
<p>该论文提出了LeanGeo，首个可在Lean 4定理证明器中形式化和求解竞赛级几何问题的统一系统，构建了包含260个高阶几何定理的库，并深度集成Mathlib，实现跨领域严谨推理。此外，作者提出了LeanGeo-Bench基准，包含122道国际数学奥林匹克（IMO）等来源的几何题，并系统评估了多种主流大语言模型在此基准上的表现，发现现有模型在复杂几何推理任务上仍有很大提升空间。结论认为LeanGeo及其基准为自动化几何定理证明和神经符号推理提供了可扩展的测试平台，但当前LLM在此类任务上的能力有限，需进一步研究。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/LeanGeo_Formalizing_Competitional_Geometry_Problems_in_Lean.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="93-Clinical-semantics-for-lung-cancer-prediction"><a href="#93-Clinical-semantics-for-lung-cancer-prediction" class="headerlink" title="93. Clinical semantics for lung cancer prediction"></a>93. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Clinical_semantics_for_lung_cancer_prediction.pdf">Clinical semantics for lung cancer prediction</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Erasmus University Medical Center</span></p>
<p>本论文提出将SNOMED医学术语层次结构映射为低维Poincaré超球体嵌入，结合ResNet和Transformer深度学习模型，用于提升肺癌发病预测。实验结果显示，集成Poincaré嵌入能在大规模电子健康记录数据集上相较于传统欧氏嵌入取得稳定但有限的判别提升，模型校准表现也较优。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Clinical_semantics_for_lung_cancer_prediction.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="94-DualNILM-Energy-Injection-Identification-Enabled-Disaggregation-with-Deep-Multi-Task-Learning"><a href="#94-DualNILM-Energy-Injection-Identification-Enabled-Disaggregation-with-Deep-Multi-Task-Learning" class="headerlink" title="94. DualNILM: Energy Injection Identification Enabled Disaggregation with Deep Multi-Task Learning"></a>94. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/DualNILM__Energy_Injection_Identification_Enabled_Disaggregation_with_Deep_Multi-Task_Learning.pdf">DualNILM: Energy Injection Identification Enabled Disaggregation with Deep Multi-Task Learning</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">The Chinese University of Hong Kong, Shenzhen</span></p>
<p>该论文提出了DualNILM，一个基于深度多任务学习的非侵入式负载监测（NILM）新框架，专门针对家庭光伏等后端注入式能源场景。方法采用Transformer架构，融合序列到点和序列到序列模型，实现了对电器状态识别和能源注入量估计的联合建模。实验覆盖真实实验室数据与合成公共数据集，结果显示DualNILM在复杂注入环境下显著优于传统及主流深度学习方法，具备强泛化能力和实际部署价值。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/DualNILM_Energy_Injection_Identification_Enabled_Disaggregation_with_Deep_Multi-Task_Learning.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="95-Artificial-Intelligence-Based-Multiscale-Temporal-Modeling-for-Anomaly-Detection-in-Cloud-Services"><a href="#95-Artificial-Intelligence-Based-Multiscale-Temporal-Modeling-for-Anomaly-Detection-in-Cloud-Services" class="headerlink" title="95. Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services"></a>95. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Artificial_Intelligence-Based_Multiscale_Temporal_Modeling_for_Anomaly_Detection_in_Cloud_Services.pdf">Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of Southern California</span></p>
<p>本文提出了一种基于Transformer架构并集成多尺度特征感知的异常检测方法，专为云服务高维监控数据设计。方法通过改进的Transformer进行时序建模，并引入多尺度特征构建、注意力加权融合模块，实现对不同时间粒度下异常模式的鲁棒建模。实验结果显示，该方法在精度、召回率、AUC和F1-score等指标上均优于主流模型，且在各种扰动条件下保持稳定性能，验证了其在复杂云环境下的优越性。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Artificial_Intelligence-Based_Multiscale_Temporal_Modeling_for_Anomaly_Detection_in_Cloud_Services.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="96-Detecting-Reading-Induced-Confusion-Using-EEG-and-Eye-Tracking"><a href="#96-Detecting-Reading-Induced-Confusion-Using-EEG-and-Eye-Tracking" class="headerlink" title="96. Detecting Reading-Induced Confusion Using EEG and Eye Tracking"></a>96. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Detecting_Reading-Induced_Confusion_Using_EEG_and_Eye_Tracking.pdf">Detecting Reading-Induced Confusion Using EEG and Eye Tracking</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">University of California, Berkeley</span></p>
<p>本论文提出了一种多模态框架，通过整合脑电（EEG）和眼动追踪，实现自然段落阅读过程中混淆状态的检测。方法包括采集高密度EEG与眼动数据，提取神经与行为特征，并使用XGBoost与CNN进行单模态与多模态分类，融合模型准确率达77.29%，最佳个体准确率89.55%。结论显示，EEG和眼动信号可有效识别阅读引发的混淆，为可穿戴神经适应界面和个性化学习工具提供基础。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Detecting_Reading-Induced_Confusion_Using_EEG_and_Eye_Tracking.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="97-Mamba2-Meets-Silence-Robust-Vocal-Source-Separation-for-Sparse-Regions"><a href="#97-Mamba2-Meets-Silence-Robust-Vocal-Source-Separation-for-Sparse-Regions" class="headerlink" title="97. Mamba2 Meets Silence: Robust Vocal Source Separation for Sparse Regions"></a>97. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Mamba2_Meets_Silence__Robust_Vocal_Source_Separation_for_Sparse_Regions.pdf">Mamba2 Meets Silence: Robust Vocal Source Separation for Sparse Regions</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Kwangwoon University</span></p>
<p>本文提出了BSMamba2模型，用于高效且准确地分离音乐中的人声信号。方法上，BSMamba2将最新的Mamba2状态空间模块与频带分割和双路径结构结合，替换传统Transformer模块，显著提升了对长时序和稀疏人声片段的建模能力。实验表明，该方法在MUSDB18HQ等数据集上表现优异，cSDR达到11.03 dB，优于现有主流模型，并在不同片段长度和稀疏人声情况下均表现出稳定优势。结论：BSMamba2在音频高分辨率处理和人声稀疏分离场景下性能优越，有望推动音频分离领域的发展。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Mamba2_Meets_Silence_Robust_Vocal_Source_Separation_for_Sparse_Regions.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>
<h3 id="98-Generative-AI-Against-Poaching-Latent-Composite-Flow-Matching-for-Wildlife-Conservation"><a href="#98-Generative-AI-Against-Poaching-Latent-Composite-Flow-Matching-for-Wildlife-Conservation" class="headerlink" title="98. Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation"></a>98. <a target="_blank" rel="noopener" href="http://172.16.18.71:5000/pdfs/papers_auto_crawl/2025/2025-08/2025-08-21/arxiv/Generative_AI_Against_Poaching__Latent_Composite_Flow_Matching_for_Wildlife_Conservation.pdf">Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation</a></h3><p><span style="
    font-size: 15px !important; 
    color: #555 !important; 
    background: #f9f9f9 !important; 
    padding: 6px 10px !important; 
    border-radius: 4px !important; 
    margin: 6px 0 8px !important; 
    display: inline-block !important; 
    font-style: italic !important;     /* 斜体 */
    font-weight: bold !important;      /* 加粗 */
">Harvard University</span></p>
<p>该论文提出WILDFLOW框架，首次将生成式AI（基于潜在复合流匹配模型）用于偷猎预测，显式结合生态占据模型与图神经网络以建模复杂空间模式，并通过复合基线初始化提升小样本泛化能力。实验证明WILDFLOW在乌干达两个国家公园的数据集上AUPR分别提升7.0%和10.2%，有效应对观测偏差和数据稀缺难题。</p>
<img src="https://raw.githubusercontent.com/ywfhhh/ywfhhh.picture/refs/heads/main/2025-08-22/Generative_AI_Against_Poaching_Latent_Composite_Flow_Matching_for_Wildlife_Conservation.jpg" srcset="/img/loading.gif" lazyload alt="论文配图" width="300px" style="display: block; margin: 0 auto;" />

<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2025/08/22报纸</div>
      <div>http://ywfhhh.github.io/2025/08/2025-08-22_article/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>具身ADT</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月22日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/2025-08-25_article/" title="2025/08/25报纸">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">2025/08/25报纸</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/2025-08-21_article/" title="2025/08/21报纸">
                        <span class="hidden-mobile">2025/08/21报纸</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appKey":"va2F2XvdFvtuyZh8JscAcxdH","path":"window.location.pathname","placeholder":"欢迎交流讨论...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true,"appid":"opdC4BuOPaDCEVIDXc9OKzUp-gzGzoHsz","appkey":"va2F2XvdFvtuyZh8JscAcxdH","notify":false,"verify":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<!-- hexo injector body_end start --><script src="./custom.js"></script><!-- hexo injector body_end end --></body>
</html>
